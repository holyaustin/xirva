[{"id": "1911.00052", "submitter": "Ewandson Luiz Lameu", "authors": "Ewandson L. Lameu, Fernando S. Borges, Kelly C. Iarosz, Paulo R.\n  Protachevicz, Antonio M. Batista, Chris G. Antonopoulos and Elbert E. N.\n  Macau", "title": "Short-term and spike-timing-dependent plasticities facilitate the\n  formation of modular neural networks", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain has the phenomenal ability to reorganize itself by forming new\nconnections among neurons and by pruning others. The so-called neural or brain\nplasticity facilitates the modification of brain structure and function over\ndifferent time scales. Plasticity might occur due to external stimuli received\nfrom the environment, during recovery from brain injury, or due to\nmodifications within the body and brain itself. In this paper, we study the\ncombined effect of short-term (STP) and spike-timing-dependent plasticities\n(STDP) on the synaptic strength of excitatory coupled Hodgkin-Huxley neurons\nand show that plasticity can facilitate the formation of modular neural\nnetworks with complex topologies that resemble those of networks with\npreferential attachment properties. In particular, we use an STDP rule that\nalters the synaptic coupling intensity based on time intervals between spikes\nof postsynaptic and presynaptic neurons. Previous works have shown that STDP\nmay induce the appearance of directed connections from high to low-frequency\nspiking neurons. On the other hand, STP is attributed to the release of\nneurotransmitters in the synaptic cleft of neurons that alter its synaptic\nefficiency. Our results suggest that the combined effect of STP and STDP with\nhigh recovery time facilitates the formation of connections among neurons with\nsimilar spike frequencies only, a kind of preferential attachment. We then\npursue this further and show that, when starting with all-to-all neural\nconfigurations, depending on the STP recovery time and distribution of neural\nfrequencies, modular neural networks can emerge as a direct result of the\ncombined effect of STP and STDP.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:45:39 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Lameu", "Ewandson L.", ""], ["Borges", "Fernando S.", ""], ["Iarosz", "Kelly C.", ""], ["Protachevicz", "Paulo R.", ""], ["Batista", "Antonio M.", ""], ["Antonopoulos", "Chris G.", ""], ["Macau", "Elbert E. N.", ""]]}, {"id": "1911.00072", "submitter": "Gurpreet Singh Matharoo", "authors": "Gurpreet S. Matharoo and Javeria A. Hashmi", "title": "Spontaneous back-pain alters randomness in functional connections in\n  large scale brain networks: A random matrix perspective", "comments": "18 Pages, 5 Figures", "journal-ref": "Physica A; 2019", "doi": "10.1016/j.physa.2019.123321", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use randomness as a measure to assess the impact of evoked pain on brain\nnetworks. Randomness is defined here as the intrinsic correlations that exist\nbetween different brain regions when the brain is in a task-free state. We use\nfMRI data of three brain states in a set of back pain patients monitored over a\nperiod of 6 months. We find that randomness in the task-free state closely\nfollows the predictions of Gaussian orthogonal ensemble of random matrices.\nHowever, the randomness decreases when the brain is engaged in attending to\npainful inputs in patients suffering with early stages of back pain. A\npersistence of this pattern is observed in the patients that develop chronic\nback pain, while the patients who recover from pain after six months, the\nrandomness no longer varies with the pain task. The study demonstrates the\neffectiveness of random matrix theory in differentiating between resting state\nand two distinct task states within the same patient. Further, it demonstrates\nthat random matrix theory is effective in measuring systematic changes\noccurring in functional connectivity and offers new insights on how acute and\nchronic pain are processed in the brain at a network level.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:37:32 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Matharoo", "Gurpreet S.", ""], ["Hashmi", "Javeria A.", ""]]}, {"id": "1911.00307", "submitter": "Johnatan Aljadeff", "authors": "Johnatan Aljadeff, James D'amour, Rachel E. Field, Robert C. Froemke,\n  Claudia Clopath", "title": "Cortical credit assignment by Hebbian, neuromodulatory and inhibitory\n  plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cortex learns to make associations between stimuli and spiking activity\nwhich supports behaviour. It does this by adjusting synaptic weights. The\ncomplexity of these transformations implies that synapses have to change\nwithout access to the full error information, a problem typically referred to\nas \"credit-assignment\". However, it remains unknown how the cortex solves this\nproblem. We propose that a combination of plasticity rules, 1) Hebbian, 2)\nacetylcholine-dependent and 3) noradrenaline-dependent excitatory plasticity,\ntogether with 4) inhibitory plasticity restoring E/I balance, effectively\nsolves the credit assignment problem. We derive conditions under-which a neuron\nmodel can learn a number of associations approaching its theoretical capacity.\nWe confirm our predictions regarding acetylcholine-dependent and inhibitory\nplasticity by reanalysing experimental data. Our work suggests that detailed\ncortical E/I balance reduces the dimensionality of the problem of associating\ninputs with outputs, thereby allowing imperfect \"supervision\" by\nneuromodulatory systems to guide learning effectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 11:30:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Aljadeff", "Johnatan", ""], ["D'amour", "James", ""], ["Field", "Rachel E.", ""], ["Froemke", "Robert C.", ""], ["Clopath", "Claudia", ""]]}, {"id": "1911.00640", "submitter": "Xiang Zou", "authors": "Xiang Zou, Lie Yao, Donghua Zhao, Liang Chen, Ying Mao", "title": "The Intrinsic Properties of Brain Based on the Network Structure", "comments": "24 pages, 6 figures, 1 supplementary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Brain is a fantastic organ that helps creature adapting to the\nenvironment. Network is the most essential structure of brain, but the\ncapability of a simple network is still not very clear. In this study, we try\nto expound some brain functions only by the network property. Methods: Every\nnetwork can be equivalent to a simplified network, which is expressed by an\nequation set. The dynamic of the equation set can be described by some basic\nequations, which is based on the mathematical derivation. Results (1) In a\nclosed network, the stability is based on the excitatory/inhibitory synapse\nproportion. Spike probabilities in the assembly can meet the solution of a\nnonlinear equation set. (2) Network activity can spontaneously evolve into a\ncertain distribution under different stimulation, which is closely related to\ndecision making. (3) Short memory can be formed by coupling of network\nassemblies. Conclusion: The essential property of a network may contribute to\nsome important brain functions.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 03:47:18 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zou", "Xiang", ""], ["Yao", "Lie", ""], ["Zhao", "Donghua", ""], ["Chen", "Liang", ""], ["Mao", "Ying", ""]]}, {"id": "1911.00716", "submitter": "Martin Vasilev", "authors": "Martin R. Vasilev, Victoria I. Adedeji, Calvin Laursen, Marcin Budka,\n  Timothy J. Slattery", "title": "Do readers use character information when programming return-sweep\n  saccades?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading saccades that occur within a single line of text are guided by the\nsize of letters. However, readers occasionally need to make longer saccades\n(known as return-sweeps) that take their eyes from the end of one line of text\nto the beginning of the next. In this study, we tested whether return-sweep\nsaccades are also guided by font size information and whether this guidance\ndepends on visual acuity of the return-sweep target area. To do this, we\nmanipulated the font size of letters (0.29 vs 0.39 deg. per character) and the\nlength of the first line of text (16 vs 26 deg.). The larger font resulted in\nreturn-sweeps that landed further to the right of the line start and in a\nreduction of under-sweeps compared to the smaller font. This suggests that font\nsize information is used when programming return-sweeps. Return-sweeps in the\nlonger line condition landed further to the right of the line start and the\nproportion of under-sweeps increased compared to the short line condition. This\nlikely reflects an increase in saccadic undershoot error with the increase in\nintended saccade size. Critically, there was no interaction between font size\nand line length. This suggests that when programming return-sweeps, the use of\nfont size information does not depend on visual acuity at the saccade target.\nInstead, it appears that readers rely on global typographic properties of the\ntext in order to maintain an optimal number of characters to the left of their\nfirst fixation on a new line.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 13:48:12 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 10:23:13 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 19:24:20 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Vasilev", "Martin R.", ""], ["Adedeji", "Victoria I.", ""], ["Laursen", "Calvin", ""], ["Budka", "Marcin", ""], ["Slattery", "Timothy J.", ""]]}, {"id": "1911.00775", "submitter": "Erik Fagerholm", "authors": "Erik D. Fagerholm, W.M.C. Foulkes, Yasir Gallero-Salas, Fritjof\n  Helmchen, Karl J. Friston, Rosalyn J. Moran, Robert Leech", "title": "Estimating quantities conserved by virtue of scale invariance in\n  timeseries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to the symmetries of translation in space, rotation in space, and\ntranslation in time, the known laws of physics are not universally invariant\nunder transformation of scale. However, the action can be invariant under\nchange of scale in the special case of a scale free dynamical system that can\nbe described in terms of a Lagrangian, that itself scales inversely with time.\nCrucially, this means symmetries under change of scale can exist in dynamical\nsystems under certain constraints. Our contribution lies in the derivation of a\ngeneralised scale invariant Lagrangian - in the form of a power series\nexpansion - that satisfies these constraints. This generalised Lagrangian\nfurnishes a normal form for dynamic causal models (i.e., state space models\nbased upon differential equations) that can be used to distinguish scale\ninvariance (scale symmetry) from scale freeness in empirical data. We establish\nface validity with an analysis of simulated data and then show how scale\ninvariance can be identified - and how the associated conserved quantities can\nbe estimated - in neuronal timeseries.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:52:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Fagerholm", "Erik D.", ""], ["Foulkes", "W. M. C.", ""], ["Gallero-Salas", "Yasir", ""], ["Helmchen", "Fritjof", ""], ["Friston", "Karl J.", ""], ["Moran", "Rosalyn J.", ""], ["Leech", "Robert", ""]]}, {"id": "1911.01230", "submitter": "Tarek Frikha", "authors": "Tarek Frikha", "title": "Source localization of the EEG human brainwaves activities via all the\n  different mother wavelets families for stationary wavelet transform\n  decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.SY eess.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The source localization of the human brain activities is an important\nresource for the recognition of cognitive state, medical disorders and a better\nunderstanding of the brain in general. In this study, we have compared 51\nmother wavelets from 7 different wavelet families in a Stationary Wavelet\ntransform (SWT) decomposition of an EEG signal. This process includes Haar,\nSymlets, Daubechies, Coiflets, Discrete Meyer, Biorthogonal and reverse\nBiorthogonal wavelet families in extracting five different brainwave sub-bands\nfor a source localization. For this process, we used the Independent Component\nAnalysis (ICA) for feature extraction followed by the Boundary Element Model\n(BEM) and the Equivalent Current Dipole (ECD) for the forward and inverse\nproblem solutions. The evaluation results in investigating the optimal mother\nwavelet for source localization eventually identified the sym 20 mother wavelet\nas the best choice followed by bior 6.8 and coif 5.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:07:35 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Frikha", "Tarek", ""]]}, {"id": "1911.01721", "submitter": "Eirini Troullinou", "authors": "Eirini Troullinou, Grigorios Tsagkatakis, Ganna Palagina, Maria\n  Papadopouli, Stelios Manolis Smirnakis, Panagiotis Tsakalides", "title": "Adversarial dictionary learning for a robust analysis and modelling of\n  spontaneous neuronal activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neuroscience is experiencing rapid growth in the complexity and\nquantity of the recorded neural activity allowing us unprecedented access to\nits dynamics in different brain areas. The objective of this work is to\ndiscover directly from the experimental data rich and comprehensible models for\nbrain function that will be concurrently robust to noise. Considering this task\nfrom the perspective of dimensionality reduction, we develop an innovative,\nrobust to noise dictionary learning framework based on adversarial training\nmethods for the identification of patterns of synchronous firing activity as\nwell as within a time lag. We employ real-world binary datasets describing the\nspontaneous neuronal activity of laboratory mice over time, and we aim to their\nefficient low-dimensional representation. The results on the classification\naccuracy for the discrimination between the clean and the adversarial-noisy\nactivation patterns obtained by an SVM classifier highlight the efficacy of the\nproposed scheme compared to other methods, and the visualization of the\ndictionary's distribution demonstrates the multifarious information that we\nobtain from it.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 11:32:06 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 17:18:05 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Troullinou", "Eirini", ""], ["Tsagkatakis", "Grigorios", ""], ["Palagina", "Ganna", ""], ["Papadopouli", "Maria", ""], ["Smirnakis", "Stelios Manolis", ""], ["Tsakalides", "Panagiotis", ""]]}, {"id": "1911.02301", "submitter": "Kesheng Xu Dr", "authors": "Kesheng Xu and Jean Paul Maidana and Patricio Orio", "title": "Diversity of neuronal activity is provided by hybrid synapses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many experiments have evidenced that electrical and chemical synapses --\nhybrid synapses -- coexist in most organisms and brain structures. The role of\nelectrical and chemical synapse connection in diversity of neural activity\ngeneration has been investigated separately in networks of varying\ncomplexities. Nevertheless, theoretical understanding of hybrid synapses in\ndiverse dynamical states of neural networks for self-organization and\nrobustness still has not been fully studied. Here, we present a model of neural\nnetwork built with hybrid synapses to investigate the emergence of global and\ncollective dynamics states. This neural networks consists of excitatory and\ninhibitory population interacting together. The excitatory population is\nconnected by excitatory synapses in small world topology and its adjacent\nneurons are also connected by gap junctions. The inhibitory population is only\nconnected by chemical inhibitory synapses with all-to-all interaction. Our\nnumerical simulations show that in the balanced networks with absence of\nelectrical coupling, the synchrony states generated by this architecture are\nmainly controlled by heterogeneity among neurons and the balance of its\nexcitatory and inhibitory inputs. In balanced networks with strong electrical\ncoupling, several dynamical states arise from different combinations of\nexcitatory and inhibitory weights. More importantly, we find that these states,\nsuch as synchronous firing, cluster synchrony, and various ripples events,\nemerge by slight modification of chemical coupling weights. For large enough\nelectrical synapse coupling, the whole neural networks become synchronized. Our\nresults pave a way in the study of the dynamical mechanisms and computational\nsignificance of the contribution of mixed synapse in the neural functions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 10:49:00 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Xu", "Kesheng", ""], ["Maidana", "Jean Paul", ""], ["Orio", "Patricio", ""]]}, {"id": "1911.02362", "submitter": "Adam Safron", "authors": "Adam Safron", "title": "Bayesian Analogical Cybernetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued that all of cognition can be understood in terms of\nBayesian inference. It has also been argued that analogy is the core of\ncognition. Here I will propose that these perspectives are fully compatible, in\nthat analogical reasoning can be described in terms of Bayesian inference and\nvice versa, and that both of these positions require a thorough cybernetic\ngrounding in order to fulfill their promise as unifying frameworks for\nunderstanding minds. From the Bayesian perspective of the Free Energy Principle\nand Active Inference framework, thought is constituted by dynamics of cascading\nbelief propagation through the nodes of probabilistic generative models\nspecified by a cortical heterarchy \"rooted\" in action-perception cycles that\nground the mind as an embodied control system for an autonomous agent. From the\nanalogical structure mapping perspective, thought is constituted by the\nalignment and comparison of heterogeneous structural representations. Here I\nwill propose that this core cognitive process for analogical reasoning is\nnaturally implemented by predictive coding mechanisms. However, both Bayesian\ncognitive science and models of cognitive development via analogical reasoning\nrequire rich base domains and priors (or reliably learnable posteriors) from\nwhich they can commence the process of bootstrapping minds. Here in the spirit\nof the work of George Lakoff and Mark Johnson, I propose that embodiment\nprovides many of the inductive biases that are usually described in terms of\ninnate core knowledge. (Please note: this manuscript was written and finalized\nin 2012.)\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 06:17:55 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 04:13:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Safron", "Adam", ""]]}, {"id": "1911.02363", "submitter": "Chi-Ning Chou", "authors": "Chi-Ning Chou, Mien Brabeeba Wang", "title": "ODE-Inspired Analysis for the Biological Version of Oja's Rule in\n  Solving Streaming PCA", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oja's rule [Oja, Journal of mathematical biology 1982] is a well-known\nbiologically-plausible algorithm using a Hebbian-type synaptic update rule to\nsolve streaming principal component analysis (PCA). Computational\nneuroscientists have known that this biological version of Oja's rule converges\nto the top eigenvector of the covariance matrix of the input in the limit.\nHowever, prior to this work, it was open to prove any convergence rate\nguarantee.\n  In this work, we give the first convergence rate analysis for the biological\nversion of Oja's rule in solving streaming PCA. Moreover, our convergence rate\nmatches the information theoretical lower bound up to logarithmic factors and\noutperforms the state-of-the-art upper bound for streaming PCA. Furthermore, we\ndevelop a novel framework inspired by ordinary differential equations (ODE) to\nanalyze general stochastic dynamics. The framework abandons the traditional\nstep-by-step analysis and instead analyzes a stochastic dynamic in one-shot by\ngiving a closed-form solution to the entire dynamic. The one-shot framework\nallows us to apply stopping time and martingale techniques to have a flexible\nand precise control on the dynamic. We believe that this general framework is\npowerful and should lead to effective yet simple analysis for a large class of\nproblems with stochastic dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:01:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 21:32:51 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Chou", "Chi-Ning", ""], ["Wang", "Mien Brabeeba", ""]]}, {"id": "1911.02364", "submitter": "Adam Safron", "authors": "Adam Safron", "title": "Rapid Anxiety Reduction (RAR): A unified theory of humor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here I propose a novel theory in which humor is the feeling of Rapid Anxiety\nReduction (RAR). According to RAR, humor can be expressed in a simple formula:\n-d(A)/dt. RAR has strong correspondences with False Alarm Theory, Benign\nViolation Theory, and Cognitive Debugging Theory, all of which represent either\nspecial cases or partial descriptions at alternative levels of analysis. Some\nevidence for RAR includes physiological similarities between hyperventilation\nand laughter and the fact that smiles often indicate negative affect in\nnon-human primates (e.g. fear grimaces where teeth are exposed as a kind of\ninhibited threat display). In accordance with Benign Violation Theory, if humor\nreliably indicates both a) anxiety induction, b) anxiety reduction, and c) the\ntime-course over which anxiety is reduced, then the intersection of these\nconditions productively constrains inference spaces over latent mental states\nwith respect to the values and capacities of the persons experiencing humor. In\nthis way, humor is a powerful cypher for understanding persons in both\nindividual and social contexts, with far-reaching implications. Finally, if\nhumor can be expressed in such a simple formula with clear ties to\nphenomenology, and yet this discovery regarding such an essential part of the\nhuman experience has remained undiscovered for this long, then this is an\nextremely surprising state of affairs worthy of further investigation. Towards\nthis end, I propose an analogy can be found with consciousness studies, where\nin addition to the \"Hard problem\" of trying to explain humor, we would do well\nto consider a \"Meta-Problem\" of why humor seems so difficult to explain, and\nwhy relatively simple explanations may have eluded us for this long. (Please\nnote: RAR was conceived in 2008, and last majorly updated in 2012.)\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 21:56:22 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 03:51:16 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Safron", "Adam", ""]]}, {"id": "1911.02601", "submitter": "Joaquin Goni", "authors": "Enrico Amico, Kausar Abbas, Duy Anh Duong-Tran, Uttara Tipnis,\n  Meenusree Rajapandian, Evgeny Chumin, Mario Ventresca, Jaroslaw Harezlak,\n  Joaqu\\'in Go\\~ni", "title": "Towards an information theoretical description of communication in brain\n  networks", "comments": "28 pages; 4 figures; 1 table; 2 supplementary figures; 2\n  supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling communication dynamics in the brain is a key challenge in network\nneuroscience. We present here a framework that combines two measurements for\nany system where different communication processes are taking place on top of a\nfixed structural topology: Path Processing Score (PPS) estimates how much the\nbrain signal has changed or has been transformed between any two brain regions\n(source and target); Path Broadcasting Strength (PBS) estimates the propagation\nof the signal through edges adjacent to the path being assessed. We use PPS and\nPBS to explore communication dynamics in large-scale brain networks. We show\nthat brain communication dynamics can be divided into three main 'communication\nregimes' of information transfer: absent communication (no communication\nhappening); relay communication (information is being transferred almost\nintact); transducted communication (the information is being transformed). We\nuse PBS to categorize brain regions based on the way they broadcast\ninformation. Subcortical regions are mainly direct broadcasters to multiple\nreceivers; Temporal and frontal nodes mainly operate as broadcast relay brain\nstations; Visual and somato-motor cortices act as multi-channel transducted\nbroadcasters. This work paves the way towards the field of brain network\ninformation theory by providing a principled methodology to explore\ncommunication dynamics in large-scale brain networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 19:18:17 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:21:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Amico", "Enrico", ""], ["Abbas", "Kausar", ""], ["Duong-Tran", "Duy Anh", ""], ["Tipnis", "Uttara", ""], ["Rajapandian", "Meenusree", ""], ["Chumin", "Evgeny", ""], ["Ventresca", "Mario", ""], ["Harezlak", "Jaroslaw", ""], ["Go\u00f1i", "Joaqu\u00edn", ""]]}, {"id": "1911.02609", "submitter": "Fernando Najman", "authors": "Cecilia Romaro, Fernando Araujo Najman and Morgan Andr\\'e", "title": "A Numerical Study of the Time of Extinction in a Class of Systems of\n  Spiking Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a numerical study of a mathematical model of spiking\nneurons introduced by Ferrari et al. in an article entitled Phase transition\nforinfinite systems of spiking neurons. In this model we have a countable\nnumber of neurons linked together in a network, each of them having a membrane\npotential taking value in the integers, and each of them spiking over time at a\nrate which depends on the membrane potential through some rate function $\\phi$.\nBeside being affected by a spike each neuron can also be affected by leaking.\nAt each of these leak times, which occurs for a given neuron at a fixed rate\n$\\gamma$, the membrane potential of the neuron concerned is spontaneously reset\nto $0$. A wide variety of versions of this model can be considered by choosing\ndifferent graph structures for the network and different activation functions.\nIt was rigorously shown that when the graph structure of the network is the\none-dimensional lattice with a hard threshold for the activation function, this\nmodel presents a phase transition with respect to $\\gamma$, and that it also\npresents a metastable behavior. By the latter we mean that in the sub-critical\nregime the re-normalized time of extinction converges to an exponential random\nvariable of mean 1. It has also been proven that in the super-critical regime\nthe renormalized time of extinction converges in probability to 1. Here, we\ninvestigate numerically a richer class of graph structures and activation\nfunctions. Namely we investigate the case of the two dimensional and the three\ndimensional lattices, as well as the case of a linear function and a sigmoid\nfunction for the activation function. We present numerical evidence that the\nresult of metastability in the sub-critical regime holds for these graphs and\nactivation functions as well as the convergence in probability to $1$ in the\nsuper-critical regime.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 20:02:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Romaro", "Cecilia", ""], ["Najman", "Fernando Araujo", ""], ["Andr\u00e9", "Morgan", ""]]}, {"id": "1911.02728", "submitter": "Meimei Liu", "authors": "Meimei Liu, Zhengwu Zhang and David B. Dunson", "title": "Auto-encoding brain networks with applications to analyzing large-scale\n  brain imaging datasets", "comments": "31 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been huge interest in studying human brain connectomes inferred\nfrom different imaging modalities and exploring their relationship with human\ntraits, such as cognition. Brain connectomes are usually represented as\nnetworks, with nodes corresponding to different regions of interest (ROIs) and\nedges to connection strengths between ROIs. Due to the high-dimensionality and\nnon-Euclidean nature of networks, it is challenging to depict their population\ndistribution and relate them to human traits. Current approaches focus on\nsummarizing the network using either pre-specified topological features or\nprincipal components analysis (PCA). In this paper, building on recent advances\nin deep learning, we develop a nonlinear latent factor model to characterize\nthe population distribution of brain graphs and infer the relationships between\nbrain structural connectomes and human traits. We refer to our method as Graph\nAuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging\ndatasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human\nConnectome Project (HCP) for adults, to understand the structural brain\nconnectome and its relationship with cognition. Numerical results demonstrate\nhuge advantages of GATE over competitors in terms of prediction accuracy,\nstatistical inference and computing efficiency. We found that structural\nconnectomes have a stronger association with a wide range of human cognitive\ntraits than was apparent using previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:51:35 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:38:28 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Meimei", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David B.", ""]]}, {"id": "1911.02731", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Shih-Gu Huang, Tananun Songdechakraiwut, Ian C. Carroll,\n  and H. Hill Goldsmith", "title": "Statistical Analysis of Dynamic Functional Brain Networks in Twins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that functional brain brainwork is dynamic even\nduring rest. A common approach to modeling the brain network in whole brain\nresting-state fMRI is to compute the correlation between anatomical regions via\nsliding windows. However, the direct use of the sample correlation matrices is\nnot reliable due to the image acquisition, processing noises and the use of\ndiscrete windows that often introduce spurious high-frequency fluctuations and\nthe zig-zag pattern in the estimated time-varying correlation measures. To\naddress the problem and obtain more robust correlation estimates, we propose\nthe heat kernel based dynamic correlations. We demonstrate that the proposed\nheat kernel method can smooth out the unwanted high-frequency fluctuations in\ncorrelation estimations and achieve higher accuracy in identifying dynamically\nchanging distinct states. The method is further used in determining if such\ndynamic state change is genetically heritable using a large-scale twin study.\nVarious methodological challenges for analyzing paired twin dynamic networks\nare addressed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:57:11 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 06:50:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chung", "Moo K.", ""], ["Huang", "Shih-Gu", ""], ["Songdechakraiwut", "Tananun", ""], ["Carroll", "Ian C.", ""], ["Goldsmith", "H. Hill", ""]]}, {"id": "1911.02926", "submitter": "Marie Roald", "authors": "Marie Roald, Suchita Bhinge, Chunying Jia, Vince Calhoun, T\\\"ulay\n  Adal{\\i}, Evrim Acar", "title": "Tracing Network Evolution Using the PARAFAC2 Model", "comments": "5 pages, 5 figures, conference", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053902", "report-no": null, "categories": "stat.AP cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing time-evolving networks is a challenging task, but it is\ncrucial for understanding the dynamic behavior of complex systems such as the\nbrain. For instance, how spatial networks of functional connectivity in the\nbrain evolve during a task is not well-understood. A traditional approach in\nneuroimaging data analysis is to make simplifications through the assumption of\nstatic spatial networks. In this paper, without assuming static networks in\ntime and/or space, we arrange the temporal data as a higher-order tensor and\nuse a tensor factorization model called PARAFAC2 to capture underlying patterns\n(spatial networks) in time-evolving data and their evolution. Numerical\nexperiments on simulated data demonstrate that PARAFAC2 can successfully reveal\nthe underlying networks and their dynamics. We also show the promising\nperformance of the model in terms of tracing the evolution of task-related\nfunctional connectivity in the brain through the analysis of functional\nmagnetic resonance imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:45:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Roald", "Marie", ""], ["Bhinge", "Suchita", ""], ["Jia", "Chunying", ""], ["Calhoun", "Vince", ""], ["Adal\u0131", "T\u00fclay", ""], ["Acar", "Evrim", ""]]}, {"id": "1911.02931", "submitter": "Leonardo Novelli", "authors": "Leonardo Novelli, Fatihcan M. Atay, J\\\"urgen Jost, Joseph T. Lizier", "title": "Deriving pairwise transfer entropy from network structure and motifs", "comments": "12 pages, 5 figures", "journal-ref": "Proceedings of the Royal Society A: Mathematical, Physical and\n  Engineering Sciences, 476(2236), 20190779 (2020)", "doi": "10.1098/rspa.2019.0779", "report-no": null, "categories": "cs.IT cs.SI math.IT physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy is an established method for quantifying directed\nstatistical dependencies in neuroimaging and complex systems datasets. The\npairwise (or bivariate) transfer entropy from a source to a target node in a\nnetwork does not depend solely on the local source-target link weight, but on\nthe wider network structure that the link is embedded in. This relationship is\nstudied using a discrete-time linearly-coupled Gaussian model, which allows us\nto derive the transfer entropy for each link from the network topology. It is\nshown analytically that the dependence on the directed link weight is only a\nfirst approximation, valid for weak coupling. More generally, the transfer\nentropy increases with the in-degree of the source and decreases with the\nin-degree of the target, indicating an asymmetry of information transfer\nbetween hubs and low-degree nodes. In addition, the transfer entropy is\ndirectly proportional to weighted motif counts involving common parents or\nmultiple walks from the source to the target, which are more abundant in\nnetworks with a high clustering coefficient than in random networks. Our\nfindings also apply to Granger causality, which is equivalent to transfer\nentropy for Gaussian variables. Moreover, similar empirical results on random\nBoolean networks suggest that the dependence of the transfer entropy on the\nin-degree extends to nonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 14:24:40 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 02:37:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Novelli", "Leonardo", ""], ["Atay", "Fatihcan M.", ""], ["Jost", "J\u00fcrgen", ""], ["Lizier", "Joseph T.", ""]]}, {"id": "1911.03268", "submitter": "Dan Schwartz", "authors": "Dan Schwartz, Mariya Toneva, Leila Wehbe", "title": "Inducing brain-relevant bias in natural language processing models", "comments": "To be published in the proceedings of the 33rd Conference on Neural\n  Information Processing Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Progress in natural language processing (NLP) models that estimate\nrepresentations of word sequences has recently been leveraged to improve the\nunderstanding of language processing in the brain. However, these models have\nnot been specifically designed to capture the way the brain represents language\nmeaning. We hypothesize that fine-tuning these models to predict recordings of\nbrain activity of people reading text will lead to representations that encode\nmore brain-activity-relevant language information. We demonstrate that a\nversion of BERT, a recently introduced and powerful language model, can improve\nthe prediction of brain activity after fine-tuning. We show that the\nrelationship between language and brain activity learned by BERT during this\nfine-tuning transfers across multiple participants. We also show that, for some\nparticipants, the fine-tuned representations learned from both\nmagnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI)\nare better for predicting fMRI than the representations learned from fMRI\nalone, indicating that the learned representations capture\nbrain-activity-relevant information that is not simply an artifact of the\nmodality. While changes to language representations help the model predict\nbrain activity, they also do not harm the model's ability to perform downstream\nNLP tasks. Our findings are notable for research on language understanding in\nthe brain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 23:28:16 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Schwartz", "Dan", ""], ["Toneva", "Mariya", ""], ["Wehbe", "Leila", ""]]}, {"id": "1911.03439", "submitter": "Amir Dehsarvi", "authors": "Amir Dehsarvi, Jennifer Kay South Palomares, Stephen Leslie Smith", "title": "Towards Monitoring Parkinson's Disease Following Drug Treatment: CGP\n  Classification of rs-MRI Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.05378", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: It is commonly accepted that accurate monitoring of\nneurodegenerative diseases is crucial for effective disease management and\ndelivery of medication and treatment. This research develops automatic clinical\nmonitoring techniques for PD, following treatment, using the novel application\nof EAs. Specifically, the research question addressed was: Can accurate\nmonitoring of PD be achieved using EAs on rs-fMRI data for patients prescribed\nModafinil (typically prescribed for PD patients to relieve physical fatigue)?\nMethods: This research develops novel clinical monitoring tools using data from\na controlled experiment where participants were administered Modafinil versus\nplacebo, examining the novel application of EAs to both map and predict the\nfunctional connectivity in participants using rs-fMRI data. Specifically, CGP\nwas used to classify DCM analysis and timeseries data. Results were validated\nwith two other commonly used classification methods (ANN and SVM) and via\nk-fold cross-validation. Results: Findings revealed a maximum accuracy of\n74.57% for CGP. Furthermore, CGP provided comparable performance accuracy\nrelative to ANN and SVM. Nevertheless, EAs enable us to decode the classifier,\nin terms of understanding the data inputs that are used, more easily than in\nANN and SVM. Conclusions: These findings underscore the applicability of both\nDCM analyses for classification and CGP as a novel classification technique for\nbrain imaging data with medical implications for medication monitoring.\nFurthermore, classification of fMRI data for research typically involves\nstatistical modelling techniques being often hypothesis driven, whereas EAs use\ndata-driven explanatory modelling methods resulting in numerous benefits. DCM\nanalysis is novel for classification and advantageous as it provides\ninformation on the causal links between different brain regions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 17:46:50 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Dehsarvi", "Amir", ""], ["Palomares", "Jennifer Kay South", ""], ["Smith", "Stephen Leslie", ""]]}, {"id": "1911.04420", "submitter": "Mohammad Nami", "authors": "Ali-Mohammad Kamali, Mohammad Reza Hossein Tehrani, Seyedeh-Saeedeh\n  Yahyavi, Siavash Baneshi, Zahra Kheradmand-Saadi, Masoume Nazeri, Maryam\n  Poursadeghfard and Mohammad Nami", "title": "Transcranial direct current stimulation to remediate myasthenia gravis\n  symptoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our findings indicated that brain stimulation exerted no effect on the\npatients cognitive functions. The study outcome suggest that tDCS over primary\nmotor cortex may be considered as a potential nonpharmochological treatment\nadd-on in MG. Larger-sized studies need to evaluate the significance of this\napproach is real-life practice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 17:49:35 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Kamali", "Ali-Mohammad", ""], ["Tehrani", "Mohammad Reza Hossein", ""], ["Yahyavi", "Seyedeh-Saeedeh", ""], ["Baneshi", "Siavash", ""], ["Kheradmand-Saadi", "Zahra", ""], ["Nazeri", "Masoume", ""], ["Poursadeghfard", "Maryam", ""], ["Nami", "Mohammad", ""]]}, {"id": "1911.04585", "submitter": "Ralf Engbert", "authors": "Johan Chandra, Andre Kruegel, Ralf Engbert", "title": "Modulation of oculomotor control during reading of mirrored and inverted\n  texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between cognitive and oculomotor processes during reading can\nbe explored when the spatial layout of text deviates from the typical display.\nIn this study, we investigate various eye-movement measures during reading of\ntext with experimentally manipulated layout (word-wise and letter-wise\nmirrored-reversed text as well as inverted and scrambled text). While typical\nfindings (e.g., longer mean fixation times, shorter mean saccades lengths) in\nreading manipulated texts compared to normal texts were reported in earlier\nwork, little is known about changes of oculomotor targeting observed in\nwithin-word landing positions under the above text layouts. Here we carry out\nprecise analyses of landing positions and find substantial changes in the\nso-called launch-site effect in addition to the expected overall slow-down of\nreading performance. Specifically, during reading of our manipulated text\nconditions with reversed letter order (against overall reading direction), we\nfind a reduced launch-site effect, while in all other manipulated text\nconditions, we observe an increased launch-site effect. Our results clearly\nindicate that the oculomotor system is highly adaptive when confronted with\nunusual reading conditions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 22:21:22 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Chandra", "Johan", ""], ["Kruegel", "Andre", ""], ["Engbert", "Ralf", ""]]}, {"id": "1911.04895", "submitter": "Hamdan Awan", "authors": "Hamdan Awan, Kareem Zeid, Raviraj S. Adve, Nigel Wallbridge, Carrol\n  Plummer, and Andrew W. Eckford", "title": "Communication in Plants: Comparison of Multiple Action Potential and\n  Mechanosensitive Signals with Experiments", "comments": "12 Pages, 15 Figures, Accepted for Publication in IEEE Transactions\n  on NanoBioscience", "journal-ref": "IEEE Transactions on NanoBioscience 2019", "doi": "10.1109/TNB.2019.2951289", "report-no": null, "categories": "q-bio.MN cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both action potentials and mechanosensitive signalling are an important\ncommunication mechanisms in plants. Considering an information theoretic\nframework, this paper explores the effective range of multiple action\npotentials for a long chain of cells (i.e., up to 100) in different\nconfigurations, and introduces the study of multiple mechanosensitive\nactivation signals (generated due to a mechanical stimulus) in plants. For both\nthese signals, we find that the mutual information per cell and information\npropagation speed tends to increase up to a certain number of receiver cells.\nHowever, as the number of cells increase beyond 10 to 12, the mutual\ninformation per cell starts to decrease. To validate our model and results, we\ninclude an experimental verification of the theoretical model, using a\nPhytlSigns biosignal amplifier, allowing us to measure the magnitude of the\nvoltage associated with the multiple AP and mechanosensitive activation signals\ninduced by different stimulus in plants. Experimental data is used to calculate\nthe mutual information and information propagation speed, which is compared\nwith corresponding numerical results. Since these signals are used for a\nvariety of important tasks within the plant, understanding them may lead to new\nbioengineering methods for plants.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 14:36:23 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Awan", "Hamdan", ""], ["Zeid", "Kareem", ""], ["Adve", "Raviraj S.", ""], ["Wallbridge", "Nigel", ""], ["Plummer", "Carrol", ""], ["Eckford", "Andrew W.", ""]]}, {"id": "1911.05031", "submitter": "Maxwell Bertolero Dr", "authors": "Maxwell A. Bertolero, Danielle S. Bassett", "title": "On the nature of explanations offered by network science: A perspective\n  from and for practicing neuroscientists", "comments": "This article is part a forthcoming Topics in Cognitive Science\n  Special Issue: \"Levels of Explanation in Cognitive Science: From Molecules to\n  Culture,\" Matteo Colombo and Markus Knauff (Topic Editors)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network neuroscience represents the brain as a collection of regions and\ninter-regional connections. Given its ability to formalize systems-level\nmodels, network neuroscience has generated unique explanations of neural\nfunction and behavior. The mechanistic status of these explanations and how\nthey can contribute to and fit within the field of neuroscience as a whole has\nreceived careful treatment from philosophers. However, these philosophical\ncontributions have not yet reached many neuroscientists. Here we complement\nformal philosophical efforts by providing an applied perspective from and for\nneuroscientists. We discuss the mechanistic status of the explanations offered\nby network neuroscience and how they contribute to, enhance, and interdigitate\nwith other types of explanations in neuroscience. In doing so, we rely on\nphilosophical work concerning the role of causality, scale, and mechanisms in\nscientific explanations. In particular, we make the distinction between an\nexplanation and the evidence supporting that explanation, and we argue for a\nscale-free nature of mechanistic explanations. In the course of these\ndiscussions, we hope to provide a useful applied framework in which network\nneuroscience explanations can be exercised across scales and combined with\nother fields of neuroscience to gain deeper insights into the brain and\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 17:49:10 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Bertolero", "Maxwell A.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1911.05072", "submitter": "Zhe Li", "authors": "Zhe Li, Wieland Brendel, Edgar Y. Walker, Erick Cobos, Taliah\n  Muhammad, Jacob Reimer, Matthias Bethge, Fabian H. Sinz, Xaq Pitkow, Andreas\n  S. Tolias", "title": "Learning From Brains How to Regularize Machines", "comments": "14 pages, 7 figures, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive performance on numerous visual tasks, Convolutional Neural\nNetworks (CNNs) --- unlike brains --- are often highly sensitive to small\nperturbations of their input, e.g. adversarial noise leading to erroneous\ndecisions. We propose to regularize CNNs using large-scale neuroscience data to\nlearn more robust neural features in terms of representational similarity. We\npresented natural images to mice and measured the responses of thousands of\nneurons from cortical visual areas. Next, we denoised the notoriously variable\nneural activity using strong predictive models trained on this large corpus of\nresponses from the mouse visual system, and calculated the representational\nsimilarity for millions of pairs of images from the model's predictions. We\nthen used the neural representation similarity to regularize CNNs trained on\nimage classification by penalizing intermediate representations that deviated\nfrom neural ones. This preserved performance of baseline models when\nclassifying images under standard benchmarks, while maintaining substantially\nhigher performance compared to baseline or control models when classifying\nnoisy images. Moreover, the models regularized with cortical representations\nalso improved model robustness in terms of adversarial attacks. This\ndemonstrates that regularizing with neural data can be an effective tool to\ncreate an inductive bias towards more robust inference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 21:53:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Li", "Zhe", ""], ["Brendel", "Wieland", ""], ["Walker", "Edgar Y.", ""], ["Cobos", "Erick", ""], ["Muhammad", "Taliah", ""], ["Reimer", "Jacob", ""], ["Bethge", "Matthias", ""], ["Sinz", "Fabian H.", ""], ["Pitkow", "Xaq", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "1911.05382", "submitter": "Victor Buendia", "authors": "Victor Buend\\'ia, Serena di Santo, Pablo Villegas, Raffaella Burioni,\n  Miguel A. Mu\\~noz", "title": "Self-organized bistability and its possible relevance for brain dynamics", "comments": "6 figures", "journal-ref": "Phys. Rev. Research 2, 013318 (2020)", "doi": "10.1103/PhysRevResearch.2.013318", "report-no": null, "categories": "cond-mat.stat-mech nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organized bistability (SOB) is the counterpart of 'self-organized\ncriticality' (SOC), for systems tuning themselves to the edge of bistability of\na discontinuous phase transition, rather than to the critical point of a\ncontinuous one. The equations defining the mathematical theory of SOB turn out\nto bear strong resemblance to a (Landau-Ginzburg) theory recently proposed to\nanalyze the dynamics of the cerebral cortex. This theory describes the neuronal\nactivity of coupled mesoscopic patches of cortex, homeostatically regulated by\nshort-term synaptic plasticity. The theory for cortex dynamics entails,\nhowever, some significant differences with respect to SOB, including the lack\nof a (bulk) conservation law, the absence of a perfect separation of timescales\nand, the fact that in the former, but not in the second, there is a parameter\nthat controls the overall system state (in blatant contrast with the very idea\nof self-organization). Here, we scrutinize --by employing a combination of\nanalytical and computational tools-- the analogies and differences between both\ntheories and explore whether in some limit SOB can play an important role to\nexplain the emergence of scale-invariant neuronal avalanches observed\nempirically in the cortex. We conclude that, actually, in the limit of\ninfinitely slow synaptic-dynamics, the two theories become identical, but the\ntimescales required for the self-organization mechanism to be effective do not\nseem to be biologically plausible. We discuss the key differences between\nself-organization mechanisms with/without conservation and with/without\ninfinitely separated timescales. In particular, we introduce the concept of\n'self-organized collective oscillations' and scrutinize the implications of our\nfindings in neuroscience, shedding new light into the problems of scale\ninvariance and oscillations in cortical dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 10:18:32 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:29:14 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Buend\u00eda", "Victor", ""], ["di Santo", "Serena", ""], ["Villegas", "Pablo", ""], ["Burioni", "Raffaella", ""], ["Mu\u00f1oz", "Miguel A.", ""]]}, {"id": "1911.05479", "submitter": "Asim Iqbal", "authors": "Asim Iqbal, Phil Dong, Christopher M Kim, Heeun Jang", "title": "Decoding Neural Responses in Mouse Visual Cortex through a Deep Neural\n  Network", "comments": null, "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN).\n  IEEE, 2019", "doi": "10.1109/IJCNN.2019.8852121", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a code to unravel the population of neural responses that leads to a\ndistinct animal behavior has been a long-standing question in the field of\nneuroscience. With the recent advances in machine learning, it is shown that\nthe hierarchically Deep Neural Networks (DNNs) perform optimally in decoding\nunique features out of complex datasets. In this study, we utilize the power of\na DNN to explore the computational principles in the mammalian brain by\nexploiting the Neuropixel data from Allen Brain Institute. We decode the neural\nresponses from mouse visual cortex to predict the presented stimuli to the\nanimal for natural (bear, trees, cheetah, etc.) and artificial (drifted\ngratings, orientated bars, etc.) classes. Our results indicate that neurons in\nmouse visual cortex encode the features of natural and artificial objects in a\ndistinct manner, and such neural code is consistent across animals. We\ninvestigate this by applying transfer learning to train a DNN on the neural\nresponses of a single animal and test its generalized performance across\nmultiple animals. Within a single animal, DNN is able to decode the neural\nresponses with as much as 100% classification accuracy. Across animals, this\naccuracy is reduced to 91%. This study demonstrates the potential of utilizing\nthe DNN models as a computational framework to understand the neural coding\nprinciples in the mammalian brain.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 05:02:33 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Iqbal", "Asim", ""], ["Dong", "Phil", ""], ["Kim", "Christopher M", ""], ["Jang", "Heeun", ""]]}, {"id": "1911.05663", "submitter": "Rohan Gala", "authors": "Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn,\n  Bosiljka Tasic, Gabe Murphy, Hongkui Zeng, Uygar S\\\"umb\\\"ul", "title": "A coupled autoencoder approach for multi-modal analysis of cell types", "comments": "Main text : 10 pages, 5 figures. Supp text : 6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in high throughput profiling of individual neurons have\nspurred data driven exploration of the idea that there exist natural groupings\nof neurons referred to as cell types. The promise of this idea is that the\nimmense complexity of brain circuits can be reduced, and effectively studied by\nmeans of interactions between cell types. While clustering of neuron\npopulations based on a particular data modality can be used to define cell\ntypes, such definitions are often inconsistent across different\ncharacterization modalities. We pose this issue of cross-modal alignment as an\noptimization problem and develop an approach based on coupled training of\nautoencoders as a framework for such analyses. We apply this framework to a\nPatch-seq dataset consisting of transcriptomic and electrophysiological\nprofiles for the same set of neurons to study consistency of representations\nacross modalities, and evaluate cross-modal data prediction ability. We explore\nthe problem where only a subset of neurons is characterized with more than one\nmodality, and demonstrate that representations learned by coupled autoencoders\ncan be used to identify types sampled only by a single modality.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 00:58:02 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Gala", "Rohan", ""], ["Gouwens", "Nathan", ""], ["Yao", "Zizhen", ""], ["Budzillo", "Agata", ""], ["Penn", "Osnat", ""], ["Tasic", "Bosiljka", ""], ["Murphy", "Gabe", ""], ["Zeng", "Hongkui", ""], ["S\u00fcmb\u00fcl", "Uygar", ""]]}, {"id": "1911.05770", "submitter": "Claire Donnat", "authors": "Claire Donnat, Leonardo Tozzi, Susan Holmes", "title": "Constrained Bayesian ICA for Brain Connectome Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain connectomics is a developing field in neurosciences which strives to\nunderstand cognitive processes and psychiatric diseases through the analysis of\ninteractions between brain regions. However, in the high-dimensional,\nlow-sample, and noisy regimes that typically characterize fMRI data, the\nrecovery of such interactions remains an ongoing challenge: how can we discover\npatterns of co-activity between brain regions that could then be associated to\ncognitive processes or psychiatric disorders? In this paper, we investigate a\nconstrained Bayesian ICA approach which, in comparison to current methods,\nsimultaneously allows (a) the flexible integration of multiple sources of\ninformation (fMRI, DWI, anatomical, etc.), (b) an automatic and parameter-free\nselection of the appropriate sparsity level and number of connected submodules\nand (c) the provision of estimates on the uncertainty of the recovered\ninteractions. Our experiments, both on synthetic and real-life data, validate\nthe flexibility of our method and highlight the benefits of integrating\nanatomical information for connectome inference.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 19:25:47 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Donnat", "Claire", ""], ["Tozzi", "Leonardo", ""], ["Holmes", "Susan", ""]]}, {"id": "1911.05943", "submitter": "Shashwat Shukla", "authors": "Shashwat Shukla, Hideaki Shimazaki, Udayan Ganguly", "title": "Structured Mean-field Variational Inference and Learning in\n  Winner-take-all Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian view of the brain hypothesizes that the brain constructs a\ngenerative model of the world, and uses it to make inferences via Bayes' rule.\nAlthough many types of approximate inference schemes have been proposed for\nhierarchical Bayesian models of the brain, the questions of how these distinct\ninference procedures can be realized by hierarchical networks of spiking\nneurons remains largely unresolved. Based on a previously proposed\nmulti-compartment neuron model in which dendrites perform logarithmic\ncompression, and stochastic spiking winner-take-all (WTA) circuits in which\nfiring probability of each neuron is normalized by activities of other neurons,\nhere we construct Spiking Neural Networks that perform \\emph{structured}\nmean-field variational inference and learning, on hierarchical directed\nprobabilistic graphical models with discrete random variables. In these models,\nwe do away with symmetric synaptic weights previously assumed for\n\\emph{unstructured} mean-field variational inference by learning both the\nfeedback and feedforward weights separately. The resulting online learning\nrules take the form of an error-modulated local Spike-Timing-Dependent\nPlasticity rule. Importantly, we consider two types of WTA circuits in which\nonly one neuron is allowed to fire at a time (hard WTA) or neurons can fire\nindependently (soft WTA), which makes neurons in these circuits operate in\nregimes of temporal and rate coding respectively. We show how the hard WTA\ncircuits can be used to perform Gibbs sampling whereas the soft WTA circuits\ncan be used to implement a message passing algorithm that computes the\nmarginals approximately. Notably, a simple change in the amount of lateral\ninhibition realizes switching between the hard and soft WTA spiking regimes.\nHence the proposed network provides a unified view of the two previously\ndisparate modes of inference and coding by spiking neurons.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 05:31:11 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Shukla", "Shashwat", ""], ["Shimazaki", "Hideaki", ""], ["Ganguly", "Udayan", ""]]}, {"id": "1911.06085", "submitter": "Daniel Backhaus", "authors": "Daniel Backhaus, Ralf Engbert, Lars Oliver Martin Rothkegel, Hans Arne\n  Trukenbrod", "title": "Task-dependence in scene perception: Head unrestrained viewing using\n  mobile eye-tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world scene perception is typically studied in the laboratory using\nstatic picture viewing with restrained head position. Consequently, the\ntransfer of results obtained in this paradigm to real-word scenarios has been\nquestioned. The advancement of mobile eye-trackers and the progress in image\nprocessing, however, permit a more natural experimental setup that, at the same\ntime, maintains the high experimental control from the standard laboratory\nsetting. We investigated eye movements while participants were standing in\nfront of a projector screen and explored images under four specific task\ninstructions. Eye movements were recorded with a mobile eye-tracking device and\nraw gaze data was transformed from head-centered into image-centered\ncoordinates. We observed differences between tasks in temporal and spatial\neye-movement parameters and found that the bias to fixate images near the\ncenter differed between tasks. Our results demonstrate that current mobile\neye-tracking technology and a highly controlled design support the study of\nfine-scaled task dependencies in an experimental setting that permits more\nnatural viewing behavior than the static picture viewing paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 13:21:46 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 14:05:44 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 12:56:06 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Backhaus", "Daniel", ""], ["Engbert", "Ralf", ""], ["Rothkegel", "Lars Oliver Martin", ""], ["Trukenbrod", "Hans Arne", ""]]}, {"id": "1911.06276", "submitter": "Federico Bertoni", "authors": "Federico Bertoni, Giovanna Citti, Alessandro Sarti", "title": "LGN-CNN: a biologically inspired CNN architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a biologically inspired Convolutional Neural\nNetwork (CNN) architecture called LGN-CNN that has a first convolutional layer\ncomposed by a single filter that mimics the role of the Lateral Geniculate\nNucleus (LGN). The first layer of the neural network shows a rotationally\nsymmetric pattern justified by the structure of the net itself that turns up to\nbe an approximation of a Laplacian of Gaussian (LoG). The latter function is in\nturn a good approximation of the receptive profiles (RPs) of the cells in the\nLGN. The analogy with respect to the visual system structure is established,\nemerging directly from the architecture of the neural network. A proof of\nrotation invariance of the first layer is given on a fixed LGN-CNN architecture\nand the computational results are shown. Thus, contrast invariance capability\nof the LGN-CNN is investigated and a comparison between the Retinex effects of\nthe first layer of LGN-CNN and the Retinex effects of a LoG is provided on\ndifferent images. A statistical study is done on the filters of the second\nconvolutional layer with respect to biological data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:00:14 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 15:15:43 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bertoni", "Federico", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1911.06509", "submitter": "Koujin Takeda", "authors": "Shun Kimura, Koujin Takeda", "title": "Improved algorithm for neuronal ensemble inference by Monte Carlo method", "comments": "14 pages, 3 figures", "journal-ref": "Proceedings of NetSci-X 2020, pp.77-90", "doi": "10.1007/978-3-030-38965-9_6", "report-no": null, "categories": "cond-mat.dis-nn cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal ensemble inference is one of the significant problems in the study\nof biological neural networks. Various methods have been proposed for ensemble\ninference from their activity data taken experimentally. Here we focus on\nBayesian inference approach for ensembles with generative model, which was\nproposed in recent work. However, this method requires large computational\ncost, and the result sometimes gets stuck in bad local maximum solution of\nBayesian inference. In this work, we give improved Bayesian inference algorithm\nfor these problems. We modify ensemble generation rule in Markov chain Monte\nCarlo method, and introduce the idea of simulated annealing for hyperparameter\ncontrol. We also compare the performance of ensemble inference between our\nalgorithm and the original one.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 08:16:28 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Kimura", "Shun", ""], ["Takeda", "Koujin", ""]]}, {"id": "1911.06602", "submitter": "Toby St Clere Smithe", "authors": "Toby B. St Clere Smithe", "title": "Radically Compositional Cognitive Concepts", "comments": "6 pages, 2 figures; NeurIPS 2019 Context and Compositionality\n  workshop. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite ample evidence that our concepts, our cognitive architecture, and\nmathematics itself are all deeply compositional, few models take advantage of\nthis structure. We therefore propose a radically compositional approach to\ncomputational neuroscience, drawing on the methods of applied category theory.\nWe describe how these tools grant us a means to overcome complexity and improve\ninterpretability, and supply a rigorous common language for scientific\nmodelling, analogous to the type theories of computer science. As a case study,\nwe sketch how to translate from compositional narrative concepts to neural\ncircuits and back again.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:20:36 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Smithe", "Toby B. St Clere", ""]]}, {"id": "1911.06775", "submitter": "Rich Pang", "authors": "Rich Pang", "title": "A crossover code for high-dimensional composition", "comments": "Presented at NeurIPS 2019 Workshop on Context and Compositionality in\n  Biological and Artificial Neural Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel way to encode compositional information in\nhigh-dimensional (HD) vectors. Inspired by chromosomal crossover, random HD\nvectors are recursively interwoven, with a fraction of one vector's components\nmasked out and replaced by those from another using a context-dependent mask.\nUnlike many HD computing schemes, \"crossover\" codes highly overlap with their\nbase elements' and sub-structures' codes without sacrificing relational\ninformation, allowing fast element readout and decoding by greedy\nreconstruction. Crossover is mathematically tractable and has several\nproperties desirable for robust, flexible representation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:39:08 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Pang", "Rich", ""]]}, {"id": "1911.06921", "submitter": "Joseph Natale", "authors": "Joseph L. Natale, H. George E. Hentschel, Ilya Nemenman", "title": "Precise Spatial Memory in Local Random Networks", "comments": "9 pages, 5 figures; presented at APS March Meeting 2019 conference\n  (http://meetings.aps.org/Meeting/MAR19/Session/R67.10)", "journal-ref": "Phys. Rev. E 102, 022405 (2020)", "doi": "10.1103/PhysRevE.102.022405", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-sustained, elevated neuronal activity persisting on time scales of ten\nseconds or longer is thought to be vital for aspects of working memory,\nincluding brain representations of real space. Continuous-attractor neural\nnetworks, one of the most well-known modeling frameworks for persistent\nactivity, have been able to model crucial aspects of such spatial memory. These\nmodels tend to require highly structured or regular synaptic architectures. In\ncontrast, we elaborate a geometrically-embedded model with a local but\notherwise random connectivity profile which, combined with a global regulation\nof the mean firing rate, produces localized, finely spaced discrete attractors\nthat effectively span a 2D manifold. We demonstrate how the set of attracting\nstates can reliably encode a representation of the spatial locations at which\nthe system receives external input, thereby accomplishing spatial memory via\nattractor dynamics without synaptic fine-tuning or regular structure. We\nmeasure the network's storage capacity and find that the statistics of\nretrievable positions are also equivalent to a full tiling of the plane,\nsomething hitherto achievable only with (approximately) translationally\ninvariant synapses, and which may be of interest in modeling such biological\nphenomena as visuospatial working memory in two dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 00:24:04 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Natale", "Joseph L.", ""], ["Hentschel", "H. George E.", ""], ["Nemenman", "Ilya", ""]]}, {"id": "1911.07012", "submitter": "Lucilla De Arcangelis", "authors": "Damian Berger, Emanuele Varriale, Laurens Michiels van Kessenich, Hans\n  J. Herrmann, Lucilla de Arcangelis", "title": "Three cooperative mechanisms required for recovery after brain damage", "comments": null, "journal-ref": "Scientific Reports 9:15858 (2019)", "doi": "10.1038/s41598-019-50946-y", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke is one of the main causes of human disabilities. Experimental\nobservations indicate that several mechanisms are activated during the recovery\nof functional activity after a stroke. Here we unveil how the brain recovers by\nexplaining the role played by three mechanisms: Plastic adaptation,\nhyperexcitability and synaptogenesis. We consider two different damages in a\nneural network: A diffuse damage that simply causes the reduction of the\neffective system size and a localized damage, a stroke, that strongly alters\nthe spontaneous activity of the system. Recovery mechanisms observed\nexperimentally are implemented both separately and in a combined way.\nInterestingly, each mechanism contributes to the recovery to a limited extent.\nOnly the combined application of all three together is able to recover the\nspontaneous activity of the undamaged system. This explains why the brain\ntriggers independent mechanisms, whose cooperation is the fundamental\ningredient for the system recovery.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 10:54:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Berger", "Damian", ""], ["Varriale", "Emanuele", ""], ["van Kessenich", "Laurens Michiels", ""], ["Herrmann", "Hans J.", ""], ["de Arcangelis", "Lucilla", ""]]}, {"id": "1911.07081", "submitter": "Abir Hadriche", "authors": "Amira Hajjeji and Nawel Jmail and Abir Hadriche and Amal Ncibi and\n  Chokri Ben Amar", "title": "Evaluation of techniques for predicting seizure Build up", "comments": "10 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of electrophysiological signal of scalp: EEG\n(electroencephalography), MEG (magnetoencephalography) and depth (intracerebral\nEEG) IEEG is a way to delimit epileptogenic zone (EZ). These epileptic signals\npresent two different activities (oscillations and spikes) which can be\noverlapped in the time frequency plane. Automatic recognition of epileptic\nseizure occurrence needs several preprocessing steps. In this study, we\nevaluated two filtering techniques: the stationary wavelet transforms (SWT) and\nthe Despikifying in order to extract pre ictal gamma oscillations (bio markers\nof seizure build up). Then, we used a temporal basis set of Jmail et al 2017 as\na preprocessing step to evaluate the performance of both technique. Moreover,\nwe used time-frequency and spatio-temporal mapping of simulated and real data\nfor both techniques in order to predict seizure build up (in time and space).\nWe concluded that SWT can detect the oscillations, but Despikyfying is more\nrobust than SWT in reconstructing pure pre ictal gamma oscillations and hence\nin predicting seizure build up.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 19:03:54 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hajjeji", "Amira", ""], ["Jmail", "Nawel", ""], ["Hadriche", "Abir", ""], ["Ncibi", "Amal", ""], ["Amar", "Chokri Ben", ""]]}, {"id": "1911.07127", "submitter": "Prashant Emani", "authors": "Prashant S. Emani, Jonathan Warrell, Alan Anticevic, Stefan Bekiranov,\n  Michael Gandal, Michael J. McConnell, Guillermo Sapiro, Al\\'an Aspuru-Guzik,\n  Justin Baker, Matteo Bastiani, Patrick McClure, John Murray, Stamatios N\n  Sotiropoulos, Jacob Taylor, Geetha Senthil, Thomas Lehner, Mark B. Gerstein,\n  Aram W. Harrow", "title": "Quantum Computing at the Frontiers of Biological Sciences", "comments": "22 pages, 3 figures, Perspective", "journal-ref": "Nature Methods (2021)", "doi": "10.1038/s41592-020-01004-3", "report-no": null, "categories": "quant-ph q-bio.GN q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The search for meaningful structure in biological data has relied on\ncutting-edge advances in computational technology and data science methods.\nHowever, challenges arise as we push the limits of scale and complexity in\nbiological problems. Innovation in massively parallel, classical computing\nhardware and algorithms continues to address many of these challenges, but\nthere is a need to simultaneously consider new paradigms to circumvent current\nbarriers to processing speed. Accordingly, we articulate a view towards quantum\ncomputation and quantum information science, where algorithms have demonstrated\npotential polynomial and exponential computational speedups in certain\napplications, such as machine learning. The maturation of the field of quantum\ncomputing, in hardware and algorithm development, also coincides with the\ngrowth of several collaborative efforts to address questions across length and\ntime scales, and scientific disciplines. We use this coincidence to explore the\npotential for quantum computing to aid in one such endeavor: the merging of\ninsights from genetics, genomics, neuroimaging and behavioral phenotyping. By\nexamining joint opportunities for computational innovation across fields, we\nhighlight the need for a common language between biological data analysis and\nquantum computing. Ultimately, we consider current and future prospects for the\nemployment of quantum computing algorithms in the biological sciences.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 01:45:58 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Emani", "Prashant S.", ""], ["Warrell", "Jonathan", ""], ["Anticevic", "Alan", ""], ["Bekiranov", "Stefan", ""], ["Gandal", "Michael", ""], ["McConnell", "Michael J.", ""], ["Sapiro", "Guillermo", ""], ["Aspuru-Guzik", "Al\u00e1n", ""], ["Baker", "Justin", ""], ["Bastiani", "Matteo", ""], ["McClure", "Patrick", ""], ["Murray", "John", ""], ["Sotiropoulos", "Stamatios N", ""], ["Taylor", "Jacob", ""], ["Senthil", "Geetha", ""], ["Lehner", "Thomas", ""], ["Gerstein", "Mark B.", ""], ["Harrow", "Aram W.", ""]]}, {"id": "1911.07388", "submitter": "Armin Najarpour Foroushani", "authors": "Armin Najarpour Foroushani, Sujaya Neupane, Pablo De Heredia Pastor,\n  Christopher C. Pack, and Mohamad Sawan", "title": "Spatial Resolution of Local Field Potential Signals in Macaque V4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main challenge for the development of cortical visual prostheses is to\nspatially localize individual spots of light, called phosphenes, by assigning\nappropriate stimulating parameters to implanted electrodes. Imitating the\nnatural responses to phosphene-like stimuli at different positions can help in\ndesigning a systematic procedure to determine these parameters. The key\ncharacteristic of such a system is the ability to discriminate between\nresponses to different positions in the visual field. While most previous\nprosthetic devices have targeted the primary visual cortex, the extrastriate\ncortex has the advantage of covering a large part of the visual field with a\nsmaller amount of cortical tissue, providing the possibility of a more compact\nimplant. Here, we studied how well ensembles of Multiunit activity (MUA) and\nLocal Field Potentials (LFPs) responses from extrastriate cortical visual area\nV4 of a behaving macaque monkey can discriminate between two-dimensional\nspatial positions. We found that despite the large receptive field sizes in V4,\nthe combined responses from multiple sites, whether MUA or LFP, has the\ncapability for fine and coarse discrimination of positions. We identified a\nselection procedure that could significantly increase the discrimination\nperformance while reducing the required number of electrodes. Analysis of noise\ncorrelation in MUA and LFP responses showed that noise correlations in LFP\nresponses carry more information about the spatial positions. Overall, these\nfindings suggest that spatial positions could be localized with patterned\nstimulation in extrastriate area V4.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 01:03:52 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Foroushani", "Armin Najarpour", ""], ["Neupane", "Sujaya", ""], ["Pastor", "Pablo De Heredia", ""], ["Pack", "Christopher C.", ""], ["Sawan", "Mohamad", ""]]}, {"id": "1911.07662", "submitter": "Haiping Huang", "authors": "Haiping Huang", "title": "Variational mean-field theory for training restricted Boltzmann machines\n  with binary synapses", "comments": "9 pages, 2 figures, a mean-field framework proposed for unsupervised\n  learning in RBM with discrete synapses, which was previously out of reach", "journal-ref": "Phys. Rev. E 102, 030301 (2020)", "doi": "10.1103/PhysRevE.102.030301", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning requiring only raw data is not only a fundamental\nfunction of the cerebral cortex, but also a foundation for a next generation of\nartificial neural networks. However, a unified theoretical framework to treat\nsensory inputs, synapses and neural activity together is still lacking. The\ncomputational obstacle originates from the discrete nature of synapses, and\ncomplex interactions among these three essential elements of learning. Here, we\npropose a variational mean-field theory in which the distribution of synaptic\nweights is considered. The unsupervised learning can then be decomposed into\ntwo intertwined steps: a maximization step is carried out as a gradient ascent\nof the lower-bound on the data log-likelihood, in which the synaptic weight\ndistribution is determined by updating variational parameters, and an\nexpectation step is carried out as a message passing procedure on an equivalent\nor dual neural network whose parameter is specified by the variational\nparameters of the weight distribution. Therefore, our framework provides\ninsights on how data (or sensory inputs), synapses and neural activities\ninteract with each other to achieve the goal of extracting statistical\nregularities in sensory inputs. This variational framework is verified in\nrestricted Boltzmann machines with planted synaptic weights and\nhandwritten-digits learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 02:12:08 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 03:58:24 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Huang", "Haiping", ""]]}, {"id": "1911.07986", "submitter": "Jianghong Shi", "authors": "Jianghong Shi and Eric Shea-Brown and Michael A. Buice", "title": "Comparison Against Task Driven Artificial Neural Networks Reveals\n  Functional Organization of Mouse Visual Cortex", "comments": "Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially inspired by features of computation in visual cortex, deep neural\nnetworks compute hierarchical representations of their inputs. While these\nnetworks have been highly successful in machine learning, it remains unclear to\nwhat extent they can aid our understanding of cortical function. Several groups\nhave developed metrics that provide a quantitative comparison between\nrepresentations computed by networks and representations measured in cortex. At\nthe same time, neuroscience is well into an unprecedented phase of large-scale\ndata collection, as evidenced by projects such as the Allen Brain Observatory.\nDespite the magnitude of these efforts, in a given experiment only a fraction\nof units are recorded, limiting the information available about the cortical\nrepresentation. Moreover, only a finite number of stimuli can be shown to an\nanimal over the course of a realistic experiment. These limitations raise the\nquestion of how and whether metrics that compare representations of deep\nnetworks are meaningful on these datasets. Here, we empirically quantify the\ncapabilities and limitations of these metrics due to limited image\npresentations and neuron samples. We find that the comparison procedure is\nrobust to different choices of stimuli set and the level of subsampling that\none might expect in a large-scale brain survey with thousands of neurons. Using\nthese results, we compare the representations measured in the Allen Brain\nObservatory in response to natural image presentations to deep neural network.\nWe show that the visual cortical areas are relatively high order\nrepresentations (in that they map to deeper layers of convolutional neural\nnetworks). Furthermore, we see evidence of a broad, more parallel organization\nrather than a sequential hierarchy, with the primary area VISp(V1) being lower\norder relative to the other areas.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:31:12 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Shi", "Jianghong", ""], ["Shea-Brown", "Eric", ""], ["Buice", "Michael A.", ""]]}, {"id": "1911.08241", "submitter": "Tai Sing Lee", "authors": "Ziniu Wu and Harold Rockwell and Yimeng Zhang and Shiming Tang and Tai\n  Sing Lee", "title": "Complex Sparse Code Priors Improve the Statistical Models of Neurons in\n  Primate Primary Visual Cortex", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System identification techniques---projection pursuit regression models (PPR)\nand convolutional neural networks (CNNs)---provide state-of-the-art performance\nin predicting visual cortical neurons' responses to arbitrary input stimuli.\nHowever, the constituent kernels recovered by these methods, particularly those\nof CNNs, are often noisy and lack coherent structure, making it difficult to\nunderstand the underlying component features of a neuron's receptive field. In\nthis paper, we show that using a dictionary of complex sparse codes, which are\nlearned from natural scenes based on efficient coding theory, as the front-end\nfor PPR and CNNs can improve their performance in neuronal response prediction.\nMore importantly, this approach makes the constituent kernels of these models\nsubstantially more coherent and interpretable. Extensive experimental results\nalso indicate that these interpretable kernels provide important information on\nthe component features of a neuron's receptive field. In addition, we find that\nmodels with a complex sparse code front-end are significantly better than\nmodels with a standard orientation-selective Gabor filter front-end for\nmodeling V1 neurons that have been found to exhibit complex pattern\nselectivity. This observation adds further credence to the sparse coding theory\nas well as empirical findings of complex feature selectivity in V1.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 13:00:22 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 03:40:14 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wu", "Ziniu", ""], ["Rockwell", "Harold", ""], ["Zhang", "Yimeng", ""], ["Tang", "Shiming", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1911.08509", "submitter": "Dami\\'an G. Hern\\'andez", "authors": "Dami\\'an G. Hern\\'andez, Samuel J. Sober and Ilya Nemenman", "title": "Unsupervised Bayesian Ising Approximation for revealing the neural\n  dictionary in songbirds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of deciphering how low-level patterns (action potentials in the\nbrain, amino acids in a protein, etc.) drive high-level biological features\n(sensorimotor behavior, enzymatic function) represents the central challenge of\nquantitative biology. The lack of general methods for doing so from the size of\ndatasets that can be collected experimentally severely limits our understanding\nof the biological world. For example, in neuroscience, some sensory and motor\ncodes have been shown to consist of precisely timed multi-spike patterns.\nHowever, the combinatorial complexity of such pattern codes have precluded\ndevelopment of methods for their comprehensive analysis. Thus, just as it is\nhard to predict a protein's function based on its sequence, we still do not\nunderstand how to accurately predict an organism's behavior based on neural\nactivity. Here we derive a method for solving this class of problems. We\ndemonstrate its utility in an application to neural data, detecting precisely\ntimed spike patterns that code for specific motor behaviors in a songbird vocal\nsystem. Our method detects such codewords with an arbitrary number of spikes,\ndoes so from small data sets, and accounts for dependencies in occurrences of\ncodewords. Detecting such dictionaries of important spike patterns --- rather\nthan merely identifying the timescale on which such patterns exist, as in some\nprior approaches --- opens the door for understanding fine motor control and\nthe neural bases of sensorimotor learning in animals. For example, for the\nfirst time, we identify differences in encoding motor exploration versus\ntypical behavior. Crucially, our method can be used not only for analysis of\nneural systems, but also for understanding the structure of correlations in\nother biological and nonbiological datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 19:11:49 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Hern\u00e1ndez", "Dami\u00e1n G.", ""], ["Sober", "Samuel J.", ""], ["Nemenman", "Ilya", ""]]}, {"id": "1911.08583", "submitter": "Jahan Schad", "authors": "Jahan N. Schad", "title": "Mirror Neuron; A Beautiful Unnecessary Concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mirror neuron theory that has enjoyed continued validations was developed\nwith no particular attention to the phenomenon of the vision. Understandably\nthe perception of vision has always been thought to happen, naturally, as that\nfor any of the other four senses. However, the reality that underlies this\npresumption is by no means obvious; vision perception is based on remote\nsensing of the ecology, fundamentally different form that of the other senses,\nwhich have tactile stimulation origin (contact with matter). While its reality,\nas explicated here, explains why the above presumption is true, it also bears\nheavily on the mirror neuron theory: the revelation of the nature of vision\nmakes mirror neurons unnecessary. The extensive cognitive neurosciences\ninvestigation of primates and humans, over the past three decades, have\nexperimentally validated the theory of mirror neurons which had been put\nforward early in the period (1980s and 1990s) based on the results of cognitive\nresearch experiments on the macaque monkeys. Based on further experimental\nworks, phenomena such as learning, empathy, and some aspects of survival, are\nascribed to the operations of this class of additional neurons. Here I reason\nthat all the results of the efforts of the proponents of the theory can, not\nonly find explanation in the context of the new theory of vision but also\nprovide support for it. This new take of the phenomenon of vision is developed\nbased on the nature of the experimental methods that have succeeded in\ndeveloping some measure of vision for the blinds, and the inferences from the\nvery likely nature of the computational strategy of the brain. I present\nevidence that the mental phenomena, which rendered the claim of the mirror\nneurons, are in essence the results of subjects beings variably touched by\ntheir ecology, through the coherent tactile operation of all senses.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:48:44 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Schad", "Jahan N.", ""]]}, {"id": "1911.08584", "submitter": "Eilif Muller", "authors": "Eilif B. Muller, Philippe Beaudoin", "title": "Neocortical plasticity: an unsupervised cake but no free lunch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fields of artificial intelligence and neuroscience have a long history of\nfertile bi-directional interactions. On the one hand, important inspiration for\nthe development of artificial intelligence systems has come from the study of\nnatural systems of intelligence, the mammalian neocortex in particular. On the\nother, important inspiration for models and theories of the brain have emerged\nfrom artificial intelligence research. A central question at the intersection\nof these two areas is concerned with the processes by which neocortex learns,\nand the extent to which they are analogous to the back-propagation training\nalgorithm of deep networks. Matching the data efficiency, transfer and\ngeneralization properties of neocortical learning remains an area of active\nresearch in the field of deep learning. Recent advances in our understanding of\nneuronal, synaptic and dendritic physiology of the neocortex suggest new\napproaches for unsupervised representation learning, perhaps through a new\nclass of objective functions, which could act alongside or in lieu of\nback-propagation. Such local learning rules have implicit rather than explicit\nobjectives with respect to the training data, facilitating domain adaptation\nand generalization. Incorporating them into deep networks for representation\nlearning could better leverage unlabelled datasets to offer significant\nimprovements in data efficiency of downstream supervised readout learning, and\nreduce susceptibility to adversarial perturbations, at the cost of a more\nrestricted domain of applicability.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 18:32:42 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Muller", "Eilif B.", ""], ["Beaudoin", "Philippe", ""]]}, {"id": "1911.08585", "submitter": "Thomas Mesnard", "authors": "Thomas Mesnard, Gaetan Vignoud, Joao Sacramento, Walter Senn, Yoshua\n  Bengio", "title": "Ghost Units Yield Biologically Plausible Backprop in Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, deep learning has transformed artificial intelligence\nresearch and led to impressive performance in various difficult tasks. However,\nit is still unclear how the brain can perform credit assignment across many\nareas as efficiently as backpropagation does in deep neural networks. In this\npaper, we introduce a model that relies on a new role for a neuronal inhibitory\nmachinery, referred to as ghost units. By cancelling the feedback coming from\nthe upper layer when no target signal is provided to the top layer, the ghost\nunits enables the network to backpropagate errors and do efficient credit\nassignment in deep structures. While considering one-compartment neurons and\nrequiring very few biological assumptions, it is able to approximate the error\ngradient and achieve good performance on classification tasks. Error\nbackpropagation occurs through the recurrent dynamics of the network and thanks\nto biologically plausible local learning rules. In particular, it does not\nrequire separate feedforward and feedback circuits. Different mechanisms for\ncancelling the feedback were studied, ranging from complete duplication of the\nconnectivity by long term processes to online replication of the feedback\nactivity. This reduced system combines the essential elements to have a working\nbiologically abstracted analogue of backpropagation with a simple formulation\nand proofs of the associated results. Therefore, this model is a step towards\nunderstanding how learning and memory are implemented in cortical multilayer\nstructures, but it also raises interesting perspectives for neuromorphic\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:47:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Mesnard", "Thomas", ""], ["Vignoud", "Gaetan", ""], ["Sacramento", "Joao", ""], ["Senn", "Walter", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1911.08589", "submitter": "Jason Platt", "authors": "Jason A. Platt and Anna Miller and Lawson Fuller and Henry D. I.\n  Abarbanel", "title": "Machine Learning Classification Informed by a Functional Biophysical\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel machine learning architecture for classification suggested\nby experiments on olfactory systems. The network separates input stimuli,\nrepresented as spatially distinct currents, via winnerless competition---a\nprocess based on the intrinsic sequential dynamics of the neural system---then\nuses a support vector machine (SVM) to provide precision to the space-time\nseparation of the output. The combined network uses biophysical models of\nneurons and shows high discrimination among inputs and robustness to noise.\nWhile using the SVM alone does not permit determination of the components of\nmixtures of classified inputs, the combined network is able to tell the precise\nconcentrations of the constituent parts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:09:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:23:51 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Platt", "Jason A.", ""], ["Miller", "Anna", ""], ["Fuller", "Lawson", ""], ["Abarbanel", "Henry D. I.", ""]]}, {"id": "1911.09071", "submitter": "Katherine Hermann", "authors": "Katherine L. Hermann, Ting Chen, and Simon Kornblith", "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural\n  Networks", "comments": "NeurIPS'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to\nclassify images by texture rather than by shape. How pervasive is this bias,\nand where does it come from? We find that, when trained on datasets of images\nwith conflicting shape and texture, CNNs learn to classify by shape at least as\neasily as by texture. What factors, then, produce the texture bias in CNNs\ntrained on ImageNet? Different unsupervised training objectives and different\narchitectures have small but significant and largely independent effects on the\nlevel of texture bias. However, all objectives and architectures still lead to\nmodels that make texture-based classification decisions a majority of the time,\neven if shape information is decodable from their hidden representations. The\neffect of data augmentation is much larger. By taking less aggressive random\ncrops at training time and applying simple, naturalistic augmentation (color\ndistortion, noise, and blur), we train models that classify ambiguous images by\nshape a majority of the time, and outperform baselines on out-of-distribution\ntest sets. Our results indicate that apparent differences in the way humans and\nImageNet-trained CNNs process images may arise not primarily from differences\nin their internal workings, but from differences in the data that they see.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:16:38 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 20:48:56 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 22:51:23 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hermann", "Katherine L.", ""], ["Chen", "Ting", ""], ["Kornblith", "Simon", ""]]}, {"id": "1911.09230", "submitter": "Atsushi Masumori", "authors": "Atsushi Masumori, Lana Sinapayen, Takashi Ikegami", "title": "Predictive Coding as Stimulus Avoidance in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding can be regarded as a function which reduces the error\nbetween an input signal and a top-down prediction. If reducing the error is\nequivalent to reducing the influence of stimuli from the environment,\npredictive coding can be regarded as stimulation avoidance by prediction. Our\nprevious studies showed that action and selection for stimulation avoidance\nemerge in spiking neural networks through spike-timing dependent plasticity\n(STDP). In this study, we demonstrate that spiking neural networks with random\nstructure spontaneously learn to predict temporal sequences of stimuli based\nsolely on STDP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 00:54:55 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Masumori", "Atsushi", ""], ["Sinapayen", "Lana", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1911.09288", "submitter": "Tal Golan", "authors": "Tal Golan, Prashant C. Raju, Nikolaus Kriegeskorte", "title": "Controversial stimuli: pitting neural networks against each other as\n  models of human recognition", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences. Nov 2020,\n  201912334", "doi": "10.1073/pnas.1912334117", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinct scientific theories can make similar predictions. To adjudicate\nbetween theories, we must design experiments for which the theories make\ndistinct predictions. Here we consider the problem of comparing deep neural\nnetworks as models of human visual recognition. To efficiently compare models'\nability to predict human responses, we synthesize controversial stimuli: images\nfor which different models produce distinct responses. We applied this approach\nto two visual recognition tasks, handwritten digits (MNIST) and objects in\nsmall natural images (CIFAR-10). For each task, we synthesized controversial\nstimuli to maximize the disagreement among models which employed different\narchitectures and recognition algorithms. Human subjects viewed hundreds of\nthese stimuli, as well as natural examples, and judged the probability of\npresence of each digit/object category in each image. We quantified how\naccurately each model predicted the human judgments. The best performing models\nwere a generative Analysis-by-Synthesis model (based on variational\nautoencoders) for MNIST and a hybrid discriminative-generative Joint Energy\nModel for CIFAR-10. These DNNs, which model the distribution of images,\nperformed better than purely discriminative DNNs, which learn only to map\nimages to labels. None of the candidate models fully explained the human\nresponses. Controversial stimuli generalize the concept of adversarial\nexamples, obviating the need to assume a ground-truth model. Unlike natural\nimages, controversial stimuli are not constrained to the stimulus distribution\nmodels are trained on, thus providing severe out-of-distribution tests that\nreveal the models' inductive biases. Controversial stimuli therefore provide\npowerful probes of discrepancies between models and human perception.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:55:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 04:47:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Golan", "Tal", ""], ["Raju", "Prashant C.", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1911.09309", "submitter": "Zhijie Chen", "authors": "Zhijie Chen, Junchi Yan, Longyuan Li and Xiaokang Yang", "title": "Decoding Spiking Mechanism with Dynamic Learning on Neuron Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main concern in cognitive neuroscience is to decode the overt neural spike\ntrain observations and infer latent representations under neural circuits.\nHowever, traditional methods entail strong prior on network structure and\nhardly meet the demand for real spike data. Here we propose a novel neural\nnetwork approach called Neuron Activation Network that extracts neural\ninformation explicitly from single trial neuron population spike trains. Our\nproposed method consists of a spatiotemporal learning procedure on sensory\nenvironment and a message passing mechanism on population graph, followed by a\nneuron activation process in a recursive fashion. Our model is aimed to\nreconstruct neuron information while inferring representations of neuron\nspiking states. We apply our model to retinal ganglion cells and the\nexperimental results suggest that our model holds a more potent capability in\ngenerating neural spike sequences with high fidelity than the state-of-the-art\nmethods, as well as being more expressive and having potential to disclose\nlatent spiking mechanism. The source code will be released with the final\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 06:56:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chen", "Zhijie", ""], ["Yan", "Junchi", ""], ["Li", "Longyuan", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1911.09415", "submitter": "Reza Mahini Sheikhhosseini", "authors": "Reza Mahini, Peng Xu, Guoliang Chen, Yansong Li, Weiyan Ding, Lei\n  Zhang, Nauman Khalid Qureshi, Asoke K. Nandi, Fengyu Cong", "title": "Optimal Number of Clusters by Measuring Similarity among Topographies\n  for Spatio-temporal ERP Analysis", "comments": "34 Pages, 15 figures, 9 tables, under review in Brain Topography", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Averaging amplitudes over consecutive time samples within a time-window is\nwidely used to calculate the amplitude of an event-related potential (ERP) for\ncognitive neuroscience. Objective determination of the time-window is critical\nfor determining the ERP component. Clustering on the spatio-temporal ERP data\ncan obtain the time-window in which the consecutive time samples topographies\nare expected to be highly similar in practice. However, there exists a\nchallenging problem of determining an optimal number of clusters. Here, we\ndevelop a novel methodology to obtain the optimal number of clusters using\nconsensus clustering on the spatio-temporal ERP data. Various clustering\nmethods, namely, K-means, hierarchical clustering, fuzzy C-means,\nself-organizing map, and diffusion maps spectral clustering are combined in an\nensemble clustering manner to find the most reliable clusters. When a range of\nnumbers of clusters is applied on the spatio-temporal ERP dataset, the optimal\nnumber of clusters should correspond to the cluster of interest within which\nthe average of correlation coefficients between topographies of every two-time\nsample in the time-window is the maximum for an ERP of interest. In our method,\nwe consider fewer cluster maps for analyzing an optimal number of clusters for\nisolating the components of interest in the spatio-temporal ERP. The\nstatistical comparison demonstrates that the present method outperforms other\nconventional approaches. This finding would be practically useful for\ndiscovering the optimal clustering in spatio-temporal ERP, especially when the\ncognitive knowledge about time-window is not clearly defined.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:16:10 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Mahini", "Reza", ""], ["Xu", "Peng", ""], ["Chen", "Guoliang", ""], ["Li", "Yansong", ""], ["Ding", "Weiyan", ""], ["Zhang", "Lei", ""], ["Qureshi", "Nauman Khalid", ""], ["Nandi", "Asoke K.", ""], ["Cong", "Fengyu", ""]]}, {"id": "1911.09451", "submitter": "Josh Merel", "authors": "Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne,\n  Bence \\\"Olveczky", "title": "Deep neuroethology of a virtual rodent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel developments in neuroscience and deep learning have led to mutually\nproductive exchanges, pushing our understanding of real and artificial neural\nnetworks in sensory and cognitive systems. However, this interaction between\nfields is less developed in the study of motor control. In this work, we\ndevelop a virtual rodent as a platform for the grounded study of motor activity\nin artificial models of embodied control. We then use this platform to study\nmotor activity across contexts by training a model to solve four complex tasks.\nUsing methods familiar to neuroscientists, we describe the behavioral\nrepresentations and algorithms employed by different layers of the network\nusing a neuroethological approach to characterize motor activity relative to\nthe rodent's behavior and goals. We find that the model uses two classes of\nrepresentations which respectively encode the task-specific behavioral\nstrategies and task-invariant behavioral kinematics. These representations are\nreflected in the sequential activity and population dynamics of neural\nsubpopulations. Overall, the virtual rodent facilitates grounded collaborations\nbetween deep reinforcement learning and motor neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:08:17 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Merel", "Josh", ""], ["Aldarondo", "Diego", ""], ["Marshall", "Jesse", ""], ["Tassa", "Yuval", ""], ["Wayne", "Greg", ""], ["\u00d6lveczky", "Bence", ""]]}, {"id": "1911.09756", "submitter": "Andrea Arnold", "authors": "Kayleigh Campbell, Laura Staugler, Andrea Arnold", "title": "Estimating Time-Varying Applied Current in the Hodgkin-Huxley Model", "comments": "15 pages, 9 figures", "journal-ref": "Applied Sciences 10 (2020) 550", "doi": "10.3390/app10020550", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic Hodgkin-Huxley model is widely used for understanding the\nelectrophysiological dynamics of a single neuron. While applying a constant\ncurrent to the system results in a single voltage spike, it is possible to\nproduce more interesting dynamics by applying time-varying currents, which may\nnot be experimentally measurable. The aim of this work is to estimate\ntime-varying applied currents of different deterministic forms given noisy\nvoltage data. In particular, we utilize an augmented ensemble Kalman filter\nwith parameter tracking to estimate four different deterministic applied\ncurrents, analyzing how the model dynamics change in each case. We test the\nefficiency of the parameter tracking algorithm in this setting by exploring the\neffects of changing the standard deviation of the parameter drift and the\nfrequency of data available on the resulting time-varying applied current\nestimates and related uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 21:28:28 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Campbell", "Kayleigh", ""], ["Staugler", "Laura", ""], ["Arnold", "Andrea", ""]]}, {"id": "1911.09948", "submitter": "Dominique Beroule", "authors": "Dominique B\\'eroule (LIMSI), Pascale Gisquet-Verrier (Neuro-PSI)", "title": "Decision Making guided by Emotion A computational architecture", "comments": null, "journal-ref": "WCCI 2012 IEEE world congress on computational intelligence, Jun\n  2012, Brisbane, France", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational architecture is presented, in which \"swift and fuzzy\"\nemotional channels guide a \"slow and precise\" decision-making channel. Reported\nneurobiological studies first provide hints on the representation of both\nemotional and cognitive dimensions across brain structures, mediated by the\nneuromodulation system. The related model is based on Guided Propagation\nNetworks, the inner flows of which can be guided through modulation. A\nkey-channel of this model grows from a few emotional cues, and is aimed at\nanticipating the consequences of ongoing possible actions. Current experimental\nresults of a computer simulation show the integrated contribution of several\nemotional influences, as well as issues of accidental all-out emotions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 09:49:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["B\u00e9roule", "Dominique", "", "LIMSI"], ["Gisquet-Verrier", "Pascale", "", "Neuro-PSI"]]}, {"id": "1911.09977", "submitter": "Eirini Troullinou", "authors": "Eirini Troullinou, Grigorios Tsagkatakis, Spyridon Chavlis, Gergely\n  Turi, Wen-Ke Li, Attila Losonczy, Panagiotis Tsakalides, Panayiota Poirazi", "title": "Artificial neural networks in action for an automated cell-type\n  classification of biological neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of different neuronal cell types is critical for understanding\ntheir contribution to brain functions. Yet, automated and reliable\nclassification of neurons remains a challenge, primarily because of their\nbiological complexity. Typical approaches include laborious and expensive\nimmunohistochemical analysis while feature extraction algorithms based on\ncellular characteristics have recently been proposed. The former rely on\nmolecular markers, which are often expressed in many cell types, while the\nlatter suffer from similar issues: finding features that are distinctive for\neach class has proven to be equally challenging. Moreover, both approaches are\ntime consuming and demand a lot of human intervention. In this work we\nestablish the first, automated cell-type classification method that relies on\nneuronal activity rather than molecular or cellular features. We test our\nmethod on a real-world dataset comprising of raw calcium activity signals for\nfour neuronal types. We compare the performance of three different deep\nlearning models and demonstrate that our method can achieve automated\nclassification of neuronal cell types with unprecedented accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:19:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 11:41:40 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 12:11:25 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Troullinou", "Eirini", ""], ["Tsagkatakis", "Grigorios", ""], ["Chavlis", "Spyridon", ""], ["Turi", "Gergely", ""], ["Li", "Wen-Ke", ""], ["Losonczy", "Attila", ""], ["Tsakalides", "Panagiotis", ""], ["Poirazi", "Panayiota", ""]]}, {"id": "1911.10193", "submitter": "Joaquin Goni", "authors": "Meenusree Rajapandian, Enrico Amico, Kausar Abbas, Mario Ventresca,\n  Joaqu\\'in Go\\~ni", "title": "Uncovering differential identifiability in network properties of human\n  brain functional connectomes", "comments": "14 pages, 6 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Identifiability Framework (If) has been shown to improve differential\nidentifiability (reliability across-sessions and -sites, and differentiability\nacross-subjects) of functional connectomes for a variety of fMRI tasks. But\nhaving a robust single session/subject functional connectome is just the\nstarting point to subsequently assess network properties for characterizing\nproperties of integration, segregation and communicability, among others.\nNaturally, one wonders if uncovering identifiability at the connectome level\nalso uncovers identifiability on the derived network properties. This also\nraises the question of where to apply the If framework: on the connectivity\ndata or directly on each network measurement? Our work answers these questions\nby exploring the differential identifiability profiles of network measures when\nIf is applied on 1) the functional connectomes, and 2) directly on derived\nnetwork measurements. Results show that improving across-session reliability of\nFCs also improves reliability of derived network measures. We also find that,\nfor specific network properties, application of If directly on network\nproperties is more effective. Finally, we discover that applying the framework,\neither way, increases task sensitivity of network properties. At a time when\nthe neuroscientific community is focused on subject-level inferences, this\nframework is able to uncover FC fingerprints, which propagates to derived\nnetwork properties.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:59:44 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Rajapandian", "Meenusree", ""], ["Amico", "Enrico", ""], ["Abbas", "Kausar", ""], ["Ventresca", "Mario", ""], ["Go\u00f1i", "Joaqu\u00edn", ""]]}, {"id": "1911.10227", "submitter": "Vyom Raval", "authors": "Vyom Raval, Kevin P. Nguyen, Ashley Gerald, Richard B. Dewey Jr.,\n  Albert Montillo", "title": "Prediction of individual progression rate in Parkinson's disease using\n  clinical measures and biomechanical measures of gait and postural stability", "comments": "5 pages, 4 figures, IEEE ICASSP conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a common neurological disorder characterized by\ngait impairment. PD has no cure, and an impediment to developing a treatment is\nthe lack of any accepted method to predict disease progression rate. The\nprimary aim of this study was to develop a model using clinical measures and\nbiomechanical measures of gait and postural stability to predict an\nindividual's PD progression over two years. Data from 160 PD subjects were\nutilized. Machine learning models, including XGBoost and Feed Forward Neural\nNetworks, were developed using extensive model optimization and\ncross-validation. The highest performing model was a neural network that used a\ngroup of clinical measures, achieved a PPV of 71% in identifying fast\nprogressors, and explained a large portion (37%) of the variance in an\nindividual's progression rate on held-out test data. This demonstrates the\npotential to predict individual PD progression rate and enrich trials by\nanalyzing clinical and biomechanical measures with machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:35:55 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Raval", "Vyom", ""], ["Nguyen", "Kevin P.", ""], ["Gerald", "Ashley", ""], ["Dewey", "Richard B.", "Jr."], ["Montillo", "Albert", ""]]}, {"id": "1911.10559", "submitter": "Tosif Ahamed", "authors": "Tosif Ahamed, Antonio Carlos Costa, Greg J. Stephens", "title": "Capturing the Continuous Complexity of Behavior in C. elegans", "comments": "26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS nlin.CD physics.bio-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Animal behavior is often quantified through subjective, incomplete variables\nthat may mask essential dynamics. Here, we develop a behavioral state space in\nwhich the full instantaneous state is smoothly unfolded as a combination of\nshort-time posture dynamics. Our technique is tailored to multivariate\nobservations and extends previous reconstructions through the use of maximal\nprediction. Applied to high-resolution video recordings of the roundworm\n\\textit{C. elegans}, we discover a low-dimensional state space dominated by\nthree sets of cyclic trajectories corresponding to the worm's basic stereotyped\nmotifs: forward, backward, and turning locomotion. In contrast to this broad\nstereotypy, we find variability in the presence of locally-unstable dynamics,\nand this unpredictability shows signatures of deterministic chaos: a collection\nof unstable periodic orbits together with a positive maximal Lyapunov exponent.\nThe full Lyapunov spectrum is symmetric with positive, chaotic exponents\ndriving variability balanced by negative, dissipative exponents driving\nstereotypy. The symmetry is indicative of damped, driven Hamiltonian dynamics\nunderlying the worm's movement control.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 16:02:54 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ahamed", "Tosif", ""], ["Costa", "Antonio Carlos", ""], ["Stephens", "Greg J.", ""]]}, {"id": "1911.10617", "submitter": "Claas Flint", "authors": "Claas Flint, Katharina F\\\"orster, Sophie A. Koser, Carsten Konrad,\n  Pienie Zwitserlood, Klaus Berger, Marco Hermesdorf, Tilo Kircher, Igor\n  Nenadic, Axel Krug, Bernhard T. Baune, Katharina Dohm, Ronny Redlich, Nils\n  Opel, Volker Arolt, Tim Hahn, Xiaoyi Jiang, Udo Dannlowski, Dominik Grotegerd", "title": "Biological sex classification with structural MRI data shows increased\n  misclassification in transgender women", "comments": "Content adapted to the publication at Neuropsychopharmacology", "journal-ref": "Neuropsychopharmacology 45 (2020) 1758-1765", "doi": "10.1038/s41386-020-0666-3", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transgender individuals (TIs) show brain structural alterations that differ\nfrom their biological sex as well as their perceived gender. To substantiate\nevidence that the brain structure of TIs differs from male and female, we use a\ncombined multivariate and univariate approach. Gray matter segments resulting\nfrom voxel-based morphometry preprocessing of $N = 1753$ cisgender (CG) healthy\nparticipants were used to train ($N=1402$) and validate (20 % hold-out; $N =\n351$) a support-vector machine classifying the biological sex. As a second\nvalidation, we classified $N = 1104$ patients with depression. A third\nvalidation was performed using the matched CG sample of the transgender women\n(TWs) application-sample. Subsequently, the classifier was applied to $N = 26$\nTWs. Finally, we compared brain volumes of CG-men, women and TW-pre/post\ntreatment (cross-sex hormone treatment) in a univariate analysis controlling\nfor sexual orientation, age and total brain volume. The application of our\nbiological sex classifier to the transgender sample resulted in a significantly\nlower true positive rate (TPR) (TPR-male = 56.0 %). The TPR did not differ\nbetween CG-individuals with (TPR-male = 86.9 %) and without depression\n(TPR-male = 88.5 %). The univariate analysis of the transgender\napplication-sample revealed that TW-pre/post treatment show brain structural\ndifferences from CG-women and CG-men in the putamen and insula, as well as the\nwhole-brain analysis. Our results support the hypothesis that brain structure\nin TW differs from brain structure of their biological sex (male) as well as\ntheir perceived gender (female). This finding substantiates evidence that TIs\nshow specific brain structural alterations leading to a different pattern of\nbrain structure than CG-individuals.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 21:50:55 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:47:39 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Flint", "Claas", ""], ["F\u00f6rster", "Katharina", ""], ["Koser", "Sophie A.", ""], ["Konrad", "Carsten", ""], ["Zwitserlood", "Pienie", ""], ["Berger", "Klaus", ""], ["Hermesdorf", "Marco", ""], ["Kircher", "Tilo", ""], ["Nenadic", "Igor", ""], ["Krug", "Axel", ""], ["Baune", "Bernhard T.", ""], ["Dohm", "Katharina", ""], ["Redlich", "Ronny", ""], ["Opel", "Nils", ""], ["Arolt", "Volker", ""], ["Hahn", "Tim", ""], ["Jiang", "Xiaoyi", ""], ["Dannlowski", "Udo", ""], ["Grotegerd", "Dominik", ""]]}, {"id": "1911.10943", "submitter": "Thiparat Chotibut", "authors": "Zuozhu Liu, Thiparat Chotibut, Christopher Hillar, Shaowei Lin", "title": "Biologically Plausible Sequence Learning with Spiking Neural Networks", "comments": "Accepted for publication in the Proceedings of the 34th AAAI\n  Conference on Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the celebrated discrete-time model of nervous activity outlined\nby McCulloch and Pitts in 1943, we propose a novel continuous-time model, the\nMcCulloch-Pitts network (MPN), for sequence learning in spiking neural\nnetworks. Our model has a local learning rule, such that the synaptic weight\nupdates depend only on the information directly accessible by the synapse. By\nexploiting asymmetry in the connections between binary neurons, we show that\nMPN can be trained to robustly memorize multiple spatiotemporal patterns of\nbinary vectors, generalizing the ability of the symmetric Hopfield network to\nmemorize static spatial patterns. In addition, we demonstrate that the model\ncan efficiently learn sequences of binary pictures as well as generative models\nfor experimental neural spike-train data. Our learning rule is consistent with\nspike-timing-dependent plasticity (STDP), thus providing a theoretical ground\nfor the systematic design of biologically inspired networks with large and\nrobust long-range sequence storage capacity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:11:07 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Zuozhu", ""], ["Chotibut", "Thiparat", ""], ["Hillar", "Christopher", ""], ["Lin", "Shaowei", ""]]}, {"id": "1911.11024", "submitter": "Cooper Mellema", "authors": "Cooper J. Mellema, Alex Treacher, Kevin P. Nguyen, Albert Montillo", "title": "Architectural configurations, atlas granularity and functional\n  connectivity with diagnostic value in Autism Spectrum Disorder", "comments": "Presented at ISBI 2020", "journal-ref": "ISBI 2020. Iowa City, IA, USA, April 3-7: IEEE", "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the diagnosis of Autism Spectrum Disorder (ASD) is dependent upon\na subjective, time-consuming evaluation of behavioral tests by an expert\nclinician. Non-invasive functional MRI (fMRI) characterizes brain connectivity\nand may be used to inform diagnoses and democratize medicine. However,\nsuccessful construction of deep learning models from fMRI requires addressing\nkey choices about the model's architecture, including the number of layers and\nnumber of neurons per layer. Meanwhile, deriving functional connectivity (FC)\nfeatures from fMRI requires choosing an atlas with an appropriate level of\ngranularity. Once a model has been built, it is vital to determine which\nfeatures are predictive of ASD and if similar features are learned across atlas\ngranularity levels. To identify aptly suited architectural configurations,\nprobability distributions of the configurations of high versus low performing\nmodels are compared. To determine the effect of atlas granularity, connectivity\nfeatures are derived from atlases with 3 levels of granularity and important\nfeatures are ranked with permutation feature importance. Results show the\nhighest performing models use between 2-4 hidden layers and 16-64 neurons per\nlayer, granularity dependent. Connectivity features identified as important\nacross all 3 atlas granularity levels include FC to the supplementary motor\ngyrus and language association cortex, regions associated with deficits in\nsocial and sensory processing in ASD. Importantly, the cerebellum, often not\nincluded in functional analyses, is also identified as a region whose abnormal\nconnectivity is highly predictive of ASD. Results of this study identify\nimportant regions to include in future studies of ASD, help assist in the\nselection of network architectures, and help identify appropriate levels of\ngranularity to facilitate the development of accurate diagnostic models of ASD.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 16:15:11 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 21:05:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Mellema", "Cooper J.", ""], ["Treacher", "Alex", ""], ["Nguyen", "Kevin P.", ""], ["Montillo", "Albert", ""]]}, {"id": "1911.11326", "submitter": "Doo Seok Jeong", "authors": "Vladimir Kornijcuk, Dohun Kim, Guhyun Kim, Doo Seok Jeong", "title": "Simplified calcium signaling cascade for synaptic plasticity", "comments": "42 pages, 7 figures, Accepted by Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for synaptic plasticity based on a calcium signaling\ncascade. The model simplifies the full signaling pathways from a calcium influx\nto the phosphorylation (potentiation) and dephosphorylation (depression) of\nglutamate receptors that are gated by fictive C1 and C2 catalysts,\nrespectively. This model is based on tangible chemical reactions, including\nfictive catalysts, for long-term plasticity rather than the conceptual theories\ncommonplace in various models, such as preset thresholds of calcium\nconcentration. Our simplified model successfully reproduced the experimental\nsynaptic plasticity induced by different protocols such as (i) a synchronous\npairing protocol and (ii) correlated presynaptic and postsynaptic action\npotentials (APs). Further, the ocular dominance plasticity (or the experimental\nverification of the celebrated Bienenstock--Cooper--Munro theory) was\nreproduced by two model synapses that compete by means of back-propagating APs\n(bAPs). The key to this competition is synapse-specific bAPs with reference to\nbAP-boosting on the physiological grounds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 04:02:34 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Kornijcuk", "Vladimir", ""], ["Kim", "Dohun", ""], ["Kim", "Guhyun", ""], ["Jeong", "Doo Seok", ""]]}, {"id": "1911.11393", "submitter": "Jin Xie", "authors": "Jin Xie, Longfei Wang, Paula Webster, Yang Yao, Jiayao Sun, Shuo Wang\n  and Huihui Zhou", "title": "A Two-stream End-to-End Deep Learning Network for Recognizing Atypical\n  Visual Attention in Autism Spectrum Disorder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements have been widely investigated to study the atypical visual\nattention in Autism Spectrum Disorder (ASD). The majority of these studies have\nbeen focused on limited eye movement features by statistical comparisons\nbetween ASD and Typically Developing (TD) groups, which make it difficult to\naccurately separate ASD from TD at the individual level. The deep learning\ntechnology has been highly successful in overcoming this issue by automatically\nextracting features important for classification through a data-driven learning\nprocess. However, there is still a lack of end-to-end deep learning framework\nfor recognition of abnormal attention in ASD. In this study, we developed a\nnovel two-stream deep learning network for this recognition based on 700 images\nand corresponding eye movement patterns of ASD and TD, and obtained an accuracy\nof 0.95, which was higher than the previous state-of-the-art. We next\ncharacterized contributions to the classification at the single image level and\nnon-linearly integration of this single image level information during the\nclassification. Moreover, we identified a group of pixel-level visual features\nwithin these images with greater impacts on the classification. Together, this\ntwo-stream deep learning network provides us a novel and powerful tool to\nrecognize and understand abnormal visual attention in ASD.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:19:47 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Xie", "Jin", ""], ["Wang", "Longfei", ""], ["Webster", "Paula", ""], ["Yao", "Yang", ""], ["Sun", "Jiayao", ""], ["Wang", "Shuo", ""], ["Zhou", "Huihui", ""]]}, {"id": "1911.11409", "submitter": "Erik Fagerholm", "authors": "Erik D. Fagerholm, Rosalyn J. Moran, Robert Leech", "title": "Seizure intervention via diffusion mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by demonstrating that the neuronal state equation from Dynamic\nCausal Modelling takes on the form of the discretized Fokker-Planck equation\nupon the inclusion of local activity gradients within a network. Using the\nJacobian of this modified neuronal state equation, we show that an initially\nunstable system can be rendered stable via a decrease in diffusivity.\nTherefore, if a node within the system is known to be unstable due to a\ntendency toward epileptic seizures for example, then our results indicate that\nintervention techniques should take the counter-intuitive approach of mirroring\nactivity in the nodes immediately surrounding a pathological region -\neffectively fighting seizures with seizures. Finally, using a two-region system\nas an example, we show that one can also tune diffusivity in such a way as to\nallow for the suppression of oscillatory activity during epileptic seizures.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:53:21 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Fagerholm", "Erik D.", ""], ["Moran", "Rosalyn J.", ""], ["Leech", "Robert", ""]]}, {"id": "1911.11466", "submitter": "Nimrod Sherf", "authors": "Nimrod Sherf, Maoz Shamir", "title": "Multiplexing rhythmic information by spike timing dependent plasticity", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008000", "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rhythmic activity has been associated with a wide range of cognitive\nprocesses. Previous studies have shown that spike-timing-dependent plasticity\ncan facilitate the transfer of rhythmic activity downstream the information\nprocessing pathway. However, STDP has also been known to generate strong\nwinner-take-all like competitions between subgroups of correlated synaptic\ninputs. Consequently, one might expect that STDP would induce strong\ncompetition between different rhythmicity channels thus preventing the\nmultiplexing of information across different frequency channels. This study\nexplored whether STDP facilitates the multiplexing of information across\nmultiple frequency channels, and if so, under what conditions. We investigated\nthe STDP dynamics in the framework of a model consisting of two competing\nsubpopulations of neurons that synapse in a feedforward manner onto a single\npostsynaptic neuron. Each sub-population was assumed to oscillate in an\nindependent manner and in a different frequency band. To investigate the STDP\ndynamics, a mean field Fokker-Planck theory was developed in the limit of the\nslow learning rate. Surprisingly, our theory predicted limited interactions\nbetween the different sub-groups. Our analysis further revealed that the\ninteraction between these channels was mainly mediated by the shared component\nof the mean activity. Next, we generalized these results beyond the simplistic\nmodel using numerical simulations. We found that for a wide range of\nparameters, the system converged to a solution in which the post-synaptic\nneuron responded to both rhythms. Nevertheless, all the synaptic weights\nremained dynamic and did not converge to a fixed point. These findings imply\nthat STDP can support the multiplexing of rhythmic information and demonstrate\nhow functionality can be retained in the face of continuous remodeling of all\nthe synaptic weights.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:37:52 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Sherf", "Nimrod", ""], ["Shamir", "Maoz", ""]]}, {"id": "1911.11691", "submitter": "Siavash Golkar", "authors": "Siavash Golkar", "title": "Emergent Structures and Lifetime Structure Evolution in Artificial\n  Neural Networks", "comments": "Proceedings of NeurIPS workshop on Real Neurons & Hidden Units. 5\n  Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the flexibility of biological neural networks whose connectivity\nstructure changes significantly during their lifetime, we introduce the\nUnstructured Recursive Network (URN) and demonstrate that it can exhibit\nsimilar flexibility during training via gradient descent. We show empirically\nthat many of the different neural network structures commonly used in practice\ntoday (including fully connected, locally connected and residual networks of\ndifferent depths and widths) can emerge dynamically from the same URN. These\ndifferent structures can be derived using gradient descent on a single general\nloss function where the structure of the data and the relative strengths of\nvarious regulator terms determine the structure of the emergent network. We\nshow that this loss function and the regulators arise naturally when\nconsidering the symmetries of the network as well as the geometric properties\nof the input data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 16:51:37 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Golkar", "Siavash", ""]]}, {"id": "1911.12124", "submitter": "Ana Maria Triana", "authors": "A.M. Triana, E. Glerean, J. Saram\\\"aki, O. Korhonen", "title": "Effects of spatial smoothing on group-level differences in functional\n  brain networks", "comments": "17 pages, 4 main figures, 44 supplementary figures, 15 supplementary\n  tables. Supplementary information available in\n  https://doi.org/10.5281/zenodo.3671882", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Brain connectivity with functional Magnetic Resonance Imaging (fMRI) is a\npopular approach for detecting differences between healthy and clinical\npopulations. Before creating a functional brain network, the fMRI time series\nmust undergo several preprocessing steps to control for artifacts and to\nimprove data quality. However, preprocessing may affect the results in an\nundesirable way. Spatial smoothing, for example, is known to alter functional\nnetwork structure. Yet, its effects on group-level network differences remain\nunknown.\n  Here, we investigate the effects of spatial smoothing on the difference\nbetween patients and controls for two clinical conditions: autism spectrum\ndisorder and bipolar disorder, considering fMRI data smoothed with Gaussian\nkernels (0-32 mm).\n  We find that smoothing affects network differences between groups. For\nweighted networks, incrementing the smoothing kernel makes networks more\ndifferent. For thresholded networks, larger smoothing kernels lead to more\nsimilar networks, although this depends on the network density. Smoothing also\nalters the effect sizes of the individual link differences. This is independent\nof the ROI size, but vary with link length.\n  The effects of spatial smoothing are diverse, non-trivial, and difficult to\npredict. This has important consequences: the choice of smoothing kernel\naffects the observed network differences.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 09:52:54 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 17:36:24 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Triana", "A. M.", ""], ["Glerean", "E.", ""], ["Saram\u00e4ki", "J.", ""], ["Korhonen", "O.", ""]]}, {"id": "1911.12128", "submitter": "Johan F. Hoorn", "authors": "Johan F. Hoorn and Johnny K. W. Ho", "title": "Robot Affect: the Amygdala as Bloch Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the design of artificially sentient robots, an obstacle always has been\nthat conventional computers cannot really process information in parallel,\nwhereas the human affective system is capable of producing experiences of\nemotional concurrency (e.g., happy and sad). Another schism that has been in\nthe way is the persistent Cartesian divide between cognition and affect,\nwhereas people easily can reflect on their emotions or have feelings about a\nthought. As an essentially theoretical exercise, we posit that quantum physics\nat the basis of neurology explains observations in cognitive emotion psychology\nfrom the belief that the construct of reality is partially imagined (Im) in the\ncomplex coordinate space C^3. We propose a quantum computational account to\nmixed states of reflection and affect, while transforming known psychological\ndimensions into the actual quantum dynamics of electromotive forces. As a\nprecursor to actual simulations, we show examples of possible robot behaviors,\nusing Einstein-Podolsky-Rosen circuits. Keywords: emotion, reflection,\nmodelling, quantum computing\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:35:49 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 03:29:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Hoorn", "Johan F.", ""], ["Ho", "Johnny K. W.", ""]]}, {"id": "1911.12135", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski, Masato S. Abe, Marcin Koculak, Mihoko\n  Otake-Matsuura", "title": "Cognitive Assessment Estimation from Behavioral Responses in Emotional\n  Faces Evaluation Task -- AI Regression Approach for Dementia Onset Prediction\n  in Aging Societies", "comments": "4 pages, 2 figures, accepted for a presentation at AI for Social Good\n  workshop at NeurIPS (2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a practical health-theme machine learning (ML) application\nconcerning `AI for social good' domain for `Producing Good Outcomes' track. In\nparticular, the solution is concerning the problem of a potential elderly adult\ndementia onset prediction in aging societies. The paper discusses our attempt\nand encouraging preliminary study results of behavioral responses analysis in a\nworking memory-based emotional evaluation experiment. We focus on the\ndevelopment of digital biomarkers for dementia progress detection and\nmonitoring. We present a behavioral data collection concept for a subsequent\nAI-based application together with a range of regression encouraging results of\nMontreal Cognitive Assessment (MoCA) scores in the leave-one-subject-out\ncross-validation setup. The regressor input variables include experimental\nsubject's emotional valence and arousal recognition responses, as well as\nreaction times, together with self-reported education levels and ages, obtained\nfrom a group of twenty older adults taking part in the reported data collection\nproject. The presented results showcase the potential social benefits of\nartificial intelligence application for elderly and establish a step forward to\ndevelop ML approaches, for the subsequent application of simple behavioral\nobjective testing for dementia onset diagnostics replacing subjective MoCA.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 05:47:25 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Abe", "Masato S.", ""], ["Koculak", "Marcin", ""], ["Otake-Matsuura", "Mihoko", ""]]}, {"id": "1911.12549", "submitter": "Liane Gabora", "authors": "Victoria S. Scotney, Jasmine Schwartz, Nicole Carbert, Adam Saab, and\n  Liane Gabora", "title": "The Form of a Half-baked Creative Idea: Empirical Explorations into the\n  Structure of Ill-defined Mental Representations", "comments": "Forthcoming in Acta Psychologica. Note: this draft may not be\n  identical to the version that was accepted by Elsevier Press for publication;\n  51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative thought is conventionally believed to involve searching memory and\ngenerating multiple independent candidate ideas followed by selection and\nrefinement of the most promising. Honing theory, which grew out of the quantum\napproach to describing how concepts interact, posits that what appears to be\ndiscrete, separate ideas are actually different projections of the same\nunderlying mental representation, which can be described as a superposition\nstate, and which may take different outward forms when reflected upon from\ndifferent perspectives. As creative thought proceeds, this representation loses\npotentiality to be viewed from different perspectives and manifest as different\noutcomes. Honing theory yields different predictions from conventional theories\nabout the mental representation of an idea midway through the creative process.\nThese predictions were pitted against one another in two studies: one\nclosed-ended and one open-ended. In the first study, participants were\ninterrupted midway through solving an analogy problem and wrote down what they\nwere thinking in terms of a solution. In the second, participants were\ninstructed to create a painting that expressed their true essence and describe\nhow they conceived of the painting. For both studies, na\\\"ive judges\ncategorized these responses as supportive of either the conventional view or\nthe honing theory view. The results of both studies were significantly more\nconsistent with the predictions of honing theory. Some implications for\ncreative cognition, and cognition in general, are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 06:16:37 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Scotney", "Victoria S.", ""], ["Schwartz", "Jasmine", ""], ["Carbert", "Nicole", ""], ["Saab", "Adam", ""], ["Gabora", "Liane", ""]]}, {"id": "1911.12656", "submitter": "Yuval Harel", "authors": "Yuval Harel, Ron Meir", "title": "Optimal Multivariate Tuning with Neuron-Level and Population-Level\n  Energy Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimality principles have been useful in explaining many aspects of\nbiological systems. In the context of neural encoding in sensory areas,\noptimality is naturally formulated in a Bayesian setting, as neural tuning\nwhich minimizes mean decoding error. Many works optimize Fisher information,\nwhich approximates the Minimum Mean Square Error (MMSE) of the optimal decoder\nfor long encoding time, but may be misleading for short encoding times. We\nstudy MMSE-optimal neural encoding of a multivariate stimulus by uniform\npopulations of spiking neurons, under firing rate constraints for each neuron\nas well as for the entire population. We show that the population-level\nconstraint is essential for the formulation of a well-posed problem having\nfinite optimal tuning widths, and optimal tuning aligns with the principal\ncomponents of the prior distribution. Numerical evaluation of the\ntwo-dimensional case shows that encoding only the dimension with higher\nvariance is optimal for short encoding times. We also compare direct MMSE\noptimization to optimization of several proxies to MMSE, namely Fisher\ninformation, Maximum Likelihood estimation error, and the Bayesian Cram\\'er-Rao\nbound. We find that optimization of these measures yield qualitatively\nmisleading results regarding MMSE-optimal tuning and its dependence on encoding\ntime and energy constraints.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 11:54:29 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Harel", "Yuval", ""], ["Meir", "Ron", ""]]}, {"id": "1911.12755", "submitter": "Yujiang Wang", "authors": "Jona Carmon, Jil Heege, Joe H Necus, Thomas W Owen, Gordon Pipa,\n  Marcus Kaiser, Peter N Taylor, Yujiang Wang", "title": "Reliability and comparability of human brain structural covariance\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural covariance analysis is a widely used structural MRI analysis\nmethod which characterises the co-relations of morphology between brain regions\nover a group of subjects. To our knowledge, little has been investigated in\nterms of the comparability of results between different data sets or the\nreliability of results over the same subjects in different rescan sessions,\nimage resolutions, or FreeSurfer versions.\n  In terms of comparability, our results show substantial differences in the\nstructural covariance matrix between data sets of age- and sex-matched healthy\nhuman adults. These differences persist after site correction, they are\nexacerbated by low sample sizes, and they are most pronounced when using\naverage cortical thickness as a morphological measure. Down-stream graph\ntheoretic analyses further show statistically significant differences.\n  In terms of reliability, substantial differences were also found when\ncomparing repeated scan sessions of the same subjects, and image resolutions\nand FreeSurfer versions of the same image. We could further estimate the\nrelative measurement error and showed that it is largest when using thickness.\nWith simulated data, we argue that cortical thickness is least reliable because\nof larger relative measurement errors.\n  Practically, we make the following recommendations (1) pooling subjects\nacross sites into one group should be avoided, particularly if sites differ in\nimage resolutions, demographics, or preprocessing; (2) surface area and volume\nshould be preferred as morphological measures over cortical thickness; (3) a\nlarge number of subjects should be used to estimate structural covariance; (4)\nmeasurement error should be assessed where repeated measurements are available;\n(5) if combining sites is critical, univariate site-correction is insufficient,\nbut error covariance should be explicitly measured and modelled.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 15:43:03 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 20:38:20 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Carmon", "Jona", ""], ["Heege", "Jil", ""], ["Necus", "Joe H", ""], ["Owen", "Thomas W", ""], ["Pipa", "Gordon", ""], ["Kaiser", "Marcus", ""], ["Taylor", "Peter N", ""], ["Wang", "Yujiang", ""]]}]