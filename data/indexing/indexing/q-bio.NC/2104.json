[{"id": "2104.00145", "submitter": "Becket Ebitz", "authors": "R. Becket Ebitz and Benjamin Y. Hayden", "title": "The population doctrine in cognitive neuroscience", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A major shift is happening within neurophysiology: a population doctrine is\ndrawing level with the single-neuron doctrine that has long dominated the\nfield. Population-level ideas have so far had their greatest impact in motor\nneuroscience, but they hold great promise for resolving open questions in\ncognition as well. Here, we codify the population doctrine and survey recent\nwork that leverages this view to specifically probe cognition. Our discussion\nis organized around five core concepts that provide a foundation for\npopulation-level thinking: (1) state spaces, (2) manifolds, (3) coding\ndimensions, (4) subspaces, and (5) dynamics. The work we review illustrates the\nprogress and promise that population neurophysiology holds for cognitive\nneuroscience$-$for delivering new insight into attention, working memory,\ndecision-making, executive function, learning, and reward processing.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:25:16 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 19:13:23 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ebitz", "R. Becket", ""], ["Hayden", "Benjamin Y.", ""]]}, {"id": "2104.00526", "submitter": "Partha Mitra", "authors": "Partha P Mitra", "title": "Fitting Elephants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textbook wisdom advocates for smooth function fits and implies that\ninterpolation of noisy data should lead to poor generalization. A related\nheuristic is that fitting parameters should be fewer than measurements (Occam's\nRazor). Surprisingly, contemporary machine learning (ML) approaches, cf. deep\nnets (DNNs), generalize well despite interpolating noisy data. This may be\nunderstood via Statistically Consistent Interpolation (SCI), i.e. data\ninterpolation techniques that generalize optimally for big data. In this\narticle we elucidate SCI using the weighted interpolating nearest neighbors\n(wiNN) algorithm, which adds singular weight functions to kNN (k-nearest\nneighbors). This shows that data interpolation can be a valid ML strategy for\nbig data. SCI clarifies the relation between two ways of modeling natural\nphenomena: the rationalist approach (strong priors) of theoretical physics with\nfew parameters and the empiricist (weak priors) approach of modern ML with more\nparameters than data. SCI shows that the purely empirical approach can\nsuccessfully predict. However data interpolation does not provide theoretical\ninsights, and the training data requirements may be prohibitive. Complex animal\nbrains are between these extremes, with many parameters, but modest training\ndata, and with prior structure encoded in species-specific mesoscale circuitry.\nThus, modern ML provides a distinct epistemological approach different both\nfrom physical theories and animal brains.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:50:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Mitra", "Partha P", ""]]}, {"id": "2104.00899", "submitter": "Jessica Elizabeth Taylor", "authors": "Jessica Elizabeth Taylor, Aurelio Cortese, Helen C. Barron, Xiaochuan\n  Pan, Masamichi Sakagami and Dagmar Zeithamova", "title": "How do we generalize?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals are able to generalize information from previous\nexperience so that they can behave appropriately in novel situations -- but\nwhat neural mechanisms give rise to this amazing ability? This Generative\nAdversarial Collaboration (GAC) started out with the aim of arbitrating between\ntwo alternative accounts: (a) memories for past experiences are integrated in\nthe brain to allow for such generalization, versus (b) generalization is\ncomputed on-the-fly using separately stored (unintegrated) memories. The\nmembers of this GAC come from a range of academic backgrounds and across the\ncourse of this collaboration we found that -- although we often use different\nterminology and techniques, and although some of our specific papers may\nprovide evidence one way or the other -- in reality we are generally in\nagreement that both of these accounts (as well as others) might be valid. We\nbelieve that different mechanisms might be weighted according to specific\nconditions or task factors, or even implemented in parallel in the brain. Here,\nwe introduce some of the main mechanisms that have been proposed to underlie\ngeneralization, we discuss the issues currently hindering better synthesis of\ngeneralization research, and we propose the way forward in tackling some of\nthese issues. Finally, we introduce some of our own research questions that\nhave arisen over the course of this GAC, that we think would benefit from\nfuture collaborative efforts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:06:11 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Taylor", "Jessica Elizabeth", ""], ["Cortese", "Aurelio", ""], ["Barron", "Helen C.", ""], ["Pan", "Xiaochuan", ""], ["Sakagami", "Masamichi", ""], ["Zeithamova", "Dagmar", ""]]}, {"id": "2104.01203", "submitter": "Jasmine Nirody", "authors": "Jasmine A Nirody", "title": "Universal features in panarthropod inter-limb coordination during\n  forward walking", "comments": null, "journal-ref": "Integrative and Comparative Biology, 2021 June 28", "doi": "10.1093/icb/icab097", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Terrestrial animals must often negotiate heterogeneous, varying environments.\nAccordingly, their locomotive strategies must adapt to a wide range of terrain,\nas well as to a range of speeds in order to accomplish different behavioral\ngoals. Studies in \\textit{Drosophila} have found that inter-leg coordination\npatterns (ICPs) vary smoothly with walking speed, rather than switching between\ndistinct gaits as in vertebrates (e.g., horses transitioning between trotting\nand galloping). Such a continuum of stepping patterns implies that separate\nneural controllers are not necessary for each observed ICP. Furthermore, the\nspectrum of \\textit{Drosophila} stepping patterns includes all canonical\ncoordination patterns observed during forward walking in insects. This raises\nthe exciting possibility that the controller in \\textit{Drosophila} is common\nto all insects, and perhaps more generally to panarthropod walkers. Here, we\nsurvey and collate data on leg kinematics and inter-leg coordination\nrelationships during forward walking in a range of arthropod species, as well\nas include data from a recent behavioral investigation into the tardigrade\n\\textit{Hypsibius exemplaris}. Using this comparative dataset, we point to\nseveral functional and morphological features that are shared amongst\npanarthropods. The goal of the framework presented in this review is to\nemphasize the importance of comparative functional and morphological analyses\nin understanding the origins and diversification of walking in Panarthropoda.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 19:12:14 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:43:42 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Nirody", "Jasmine A", ""]]}, {"id": "2104.01418", "submitter": "Cees Van Leeuwen", "authors": "Ilias Rentzeperis, Steeve Laquitaine, Cees van Leeuwen", "title": "Adaptive rewiring of random neural networks generates\n  convergent-divergent units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Brain networks are adaptively rewired continually, adjusting their topology\nto bring about functionality and efficiency in sensory, motor and cognitive\ntasks. In model neural network architectures, adaptive rewiring generates\ncomplex, brain-like topologies. Present models, however, cannot account for the\nemergence of complex directed connectivity structures. We tested a biologically\nplausible model of adaptive rewiring in directed networks, based on two\nalgorithms widely used in distributed computing: advection and consensus. When\nboth are used in combination as rewiring criteria, adaptive rewiring shortens\npath length and enhances connectivity. When keeping a balance between advection\nand consensus, adaptive rewiring produces convergent-divergent units consisting\nof convergent hub nodes, which collect inputs from pools of sparsely connected,\nor local, nodes and project them via densely interconnected processing nodes\nonto divergent hubs that broadcast output back to the local pools.\nConvergent-divergent units operate within and between sensory, motor, and\ncognitive brain regions as their connective core, mediating context-sensitivity\nto local network units. By showing how these structures emerge spontaneously in\ndirected networks models, adaptive rewiring offers self-organization as a\nprinciple for efficient information propagation and integration in the brain.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 14:31:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Rentzeperis", "Ilias", ""], ["Laquitaine", "Steeve", ""], ["van Leeuwen", "Cees", ""]]}, {"id": "2104.01474", "submitter": "Mien Brabeeba Wang", "authors": "Mien Brabeeba Wang and Michael M. Halassa", "title": "Thalamocortical contribution to solving credit assignment in neural\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal brains evolved to optimize behavior in dynamically changing\nenvironments, selecting actions that maximize future rewards. A large body of\nexperimental work indicates that such optimization changes the wiring of neural\ncircuits, appropriately mapping environmental input onto behavioral outputs. A\nmajor unsolved scientific question is how optimal wiring adjustments, which\nmust target the connections responsible for rewards, can be accomplished when\nthe relation between sensory inputs, action taken, environmental context with\nrewards is ambiguous. The computational problem of properly targeting cues,\ncontexts and actions that lead to reward is known as structural, contextual and\ntemporal credit assignment respectively. In this review, we survey prior\napproaches to these three types of problems and advance the notion that the\nbrain's specialized neural architectures provide efficient solutions. Within\nthis framework, the thalamus with its cortical and basal ganglia interactions\nserve as a systems-level solution to credit assignment. Specifically, we\npropose that thalamocortical interaction is the locus of meta-learning where\nthe thalamus provides cortical control functions that parametrize the cortical\nactivity association space. By selecting among these control functions, the\nbasal ganglia hierarchically guide thalamocortical plasticity across two\ntimescales to enable meta-learning. The faster timescale establishes contextual\nassociations to enable rapid behavioral flexibility while the slower one\nenables generalization to new contexts. Incorporating different thalamic\ncontrol functions under this framework clarifies how thalamocortical-basal\nganglia interactions may simultaneously solve the three credit assignment\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 20:08:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Mien Brabeeba", ""], ["Halassa", "Michael M.", ""]]}, {"id": "2104.01489", "submitter": "Daniel Yamins", "authors": "Rosa Cao and Daniel Yamins", "title": "Explanatory models in neuroscience: Part 2 -- constraint-based\n  intelligibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computational modeling plays an increasingly important role in neuroscience,\nhighlighting the philosophical question of how computational models explain. In\nthe context of neural network models for neuroscience, concerns have been\nraised about model intelligibility, and how they relate (if at all) to what is\nfound in the brain. We claim that what makes a system intelligible is an\nunderstanding of the dependencies between its behavior and the factors that are\ncausally responsible for that behavior. In biological systems, many of these\ndependencies are naturally \"top-down\": ethological imperatives interact with\nevolutionary and developmental constraints under natural selection. We describe\nhow the optimization techniques used to construct NN models capture some key\naspects of these dependencies, and thus help explain why brain systems are as\nthey are -- because when a challenging ecologically-relevant goal is shared by\na NN and the brain, it places tight constraints on the possible mechanisms\nexhibited in both kinds of systems. By combining two familiar modes of\nexplanation -- one based on bottom-up mechanism (whose relation to neural\nnetwork models we address in a companion paper) and the other on top-down\nconstraints, these models illuminate brain function.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:14:01 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:59:48 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cao", "Rosa", ""], ["Yamins", "Daniel", ""]]}, {"id": "2104.01490", "submitter": "Daniel Yamins", "authors": "Rosa Cao and Daniel Yamins", "title": "Explanatory models in neuroscience: Part 1 -- taking mechanistic\n  abstraction seriously", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the recent success of neural network models in mimicking animal\nperformance on visual perceptual tasks, critics worry that these models fail to\nilluminate brain function. We take it that a central approach to explanation in\nsystems neuroscience is that of mechanistic modeling, where understanding the\nsystem is taken to require fleshing out the parts, organization, and activities\nof a system, and how those give rise to behaviors of interest. However, it\nremains somewhat controversial what it means for a model to describe a\nmechanism, and whether neural network models qualify as explanatory.\n  We argue that certain kinds of neural network models are actually good\nexamples of mechanistic models, when the right notion of mechanistic mapping is\ndeployed. Building on existing work on model-to-mechanism mapping (3M), we\ndescribe criteria delineating such a notion, which we call 3M++. These criteria\nrequire us, first, to identify a level of description that is both abstract but\ndetailed enough to be \"runnable\", and then, to construct model-to-brain\nmappings using the same principles as those employed for brain-to-brain mapping\nacross individuals. Perhaps surprisingly, the abstractions required are those\nalready in use in experimental neuroscience, and are of the kind deployed in\nthe construction of more familiar computational models, just as the principles\nof inter-brain mappings are very much in the spirit of those already employed\nin the collection and analysis of data across animals.\n  In a companion paper, we address the relationship between optimization and\nintelligibility, in the context of functional evolutionary explanations. Taken\ntogether, mechanistic interpretations of computational models and the\ndependencies between form and function illuminated by optimization processes\ncan help us to understand why brain systems are built they way they are.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:17:40 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 23:39:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Cao", "Rosa", ""], ["Yamins", "Daniel", ""]]}, {"id": "2104.01532", "submitter": "Thomas Athey", "authors": "Thomas L. Athey, Jacopo Teneggi, Joshua T. Vogelstein, Daniel Tward,\n  Ulrich Mueller, Michael I. Miller", "title": "Fitting Splines to Axonal Arbors Quantifies Relationship between Branch\n  Order and Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.MS math.DG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphology is crucial to identifying neuronal subtypes and understanding\nlearning. It is also implicated in neurological disease. However, standard\nmorphological analysis focuses on macroscopic features such as branching\nfrequency and connectivity between regions, and often neglects the internal\ngeometry of neurons. In this work, we treat neuron trace points as a sampling\nof differentiable curves and fit them with a set of branching B-splines. We\ndesigned our representation with the Frenet-Serret formulas from differential\ngeometry in mind. The Frenet-Serret formulas completely characterize smooth\ncurves, and involve two parameters, curvature and torsion. Our representation\nmakes it possible to compute these parameters from neuron traces in closed\nform. These parameters are defined continuously along the curve, in contrast to\nother parameters like tortuosity which depend on start and end points. We\napplied our method to a dataset of cortical projection neurons traced in two\nmouse brains, and found that the parameters are distributed differently between\nprimary, collateral, and terminal axon branches, thus quantifying geometric\ndifferences between different components of an axonal arbor. The results agreed\nin both brains, further validating our representation. The code used in this\nwork can be readily applied to neuron traces in SWC format and is available in\nour open-source Python package brainlit: http://brainlit.neurodata.io/.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 03:38:42 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:04:09 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 15:19:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Athey", "Thomas L.", ""], ["Teneggi", "Jacopo", ""], ["Vogelstein", "Joshua T.", ""], ["Tward", "Daniel", ""], ["Mueller", "Ulrich", ""], ["Miller", "Michael I.", ""]]}, {"id": "2104.01677", "submitter": "Jo\\~ao Sacramento", "authors": "Nicolas Zucchet and Simon Schug and Johannes von Oswald and Dominic\n  Zhao and Jo\\~ao Sacramento", "title": "A contrastive rule for meta-learning", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning algorithms leverage regularities that are present on a set of\ntasks to speed up and improve the performance of a subsidiary learning process.\nRecent work on deep neural networks has shown that prior gradient-based\nlearning of meta-parameters can greatly improve the efficiency of subsequent\nlearning. Here, we present a biologically plausible meta-learning algorithm\nbased on equilibrium propagation. Instead of explicitly differentiating the\nlearning process, our contrastive meta-learning rule estimates meta-parameter\ngradients by executing the subsidiary process more than once. This avoids\nreversing the learning dynamics in time and computing second-order derivatives.\nIn spite of this, and unlike previous first-order methods, our rule recovers an\narbitrarily accurate meta-parameter update given enough compute. We establish\ntheoretical bounds on its performance and present experiments on a set of\nstandard benchmarks and neural network architectures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:45:41 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 21:04:58 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zucchet", "Nicolas", ""], ["Schug", "Simon", ""], ["von Oswald", "Johannes", ""], ["Zhao", "Dominic", ""], ["Sacramento", "Jo\u00e3o", ""]]}, {"id": "2104.02097", "submitter": "Jan Slovak", "authors": "Temesgen Bihonegn, Sumit Kaushik, Avinash Bansal, Lubomir Vojtisek,\n  Jan Slovak", "title": "Geodesic Fiber Tracking in White Matter using Activation Function", "comments": "11 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The geodesic ray-tracing method has shown its effectiveness for the\nreconstruction of fibers in white matter structure. Based on reasonable metrics\non the spaces of the diffusion tensors, it can provide multiple solutions and\nget robust to noise and curvatures of fibers. The choice of the metric on the\nspaces of diffusion tensors has a significant impact on the outcome of this\nmethod. Our objective is to suggest metrics and modifications of the algorithms\nleading to more satisfactory results in the construction of white matter tracts\nas geodesics. Starting with the DTI modality, we propose to rescale the\ninitially chosen metric on the space of diffusion tensors to increase the\ngeodetic cost in the isotropic regions. This change should be conformal in\norder to preserve the angles between crossing fibers. We also suggest enhancing\nthe methods to be more robust to noise and employing the fourth-order tensor\ndata in order to handle the fiber crossings properly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:04:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bihonegn", "Temesgen", ""], ["Kaushik", "Sumit", ""], ["Bansal", "Avinash", ""], ["Vojtisek", "Lubomir", ""], ["Slovak", "Jan", ""]]}, {"id": "2104.02163", "submitter": "Leonid Rubchinsky", "authors": "Quynh-Anh Nguyen, Leonid L Rubchinsky", "title": "Temporal patterns of synchrony in a pyramidal-interneuron gamma (PING)\n  network", "comments": null, "journal-ref": "Chaos 31, 043134 (2021)", "doi": "10.1063/5.0042451", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization in neural system plays an important role in many brain\nfunctions. Synchronization in the gamma frequency band (30Hz-100Hz) is involved\nin a variety of cognitive phenomena; abnormalities of the gamma synchronization\nare found in schizophrenia and autism spectrum disorder. Frequently, the\nstrength of synchronization is not very high and is intermittent even on short\ntime scales (a few cycles of oscillations). That is, the network exhibits\nintervals of synchronization followed by intervals of desynchronization. Neural\ncircuits dynamics may show different distributions of desynchronization\ndurations even if the synchronization strength is fixed. In this study, we use\na conductance-based neural network exhibiting pyramidal-interneuron (PING)\ngamma rhythm to study the temporal patterning of synchronized neural\noscillations. We found that changes in the synaptic strength (as well as\nchanges in the membrane kinetics) can alter the temporal patterning of\nsynchrony. Moreover, we found that the changes in the temporal pattern of\nsynchrony may be independent of the changes in the average synchrony strength.\nEven though the temporal patterning may vary, there is a tendency for dynamics\nwith short (although potentially numerous) desynchronizations, similar to what\nwas observed in experimental studies of neural activity synchronization in the\nbrain. Recent studies suggested that the short desynchronizations dynamics may\nfacilitate the formation and the break-up of transient neural assemblies. Thus,\nthe results of this study suggest that changes of synaptic strength may alter\nthe temporal patterning of the gamma synchronization as to make the neural\nnetworks more efficient in the formation of neural assemblies and the\nfacilitation of cognitive phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:29:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Nguyen", "Quynh-Anh", ""], ["Rubchinsky", "Leonid L", ""]]}, {"id": "2104.02794", "submitter": "Janaki Raghavan", "authors": "R. Janaki and A. S. Vytheeswaran", "title": "The interplay of inhibitory and electrical synapses results in complex\n  persistent activity", "comments": "Main text: 10 pages, 5 figures; Supplementary Information: 2 pages, 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inhibitory neurons play a crucial role in maintaining persistent neuronal\nactivity. Although connected extensively through electrical synapses\n(gap-junctions), these neurons also exhibit interactions through chemical\nsynapses in certain regions of the brain. When the coupling is sufficiently\nstrong, the effects of these two synaptic modalities combine in a nonlinear\nway. Hence, in this work, we focus on the strong inhibition regime and identify\nthe parametric conditions that result in the emergence of self-sustained\noscillations in systems of coupled excitable neurons, in the presence of a\nbrief sub-threshold stimulus. Our investigation on the dynamics in a minimal\nnetwork of two neurons reveals a rich set of dynamical behaviors viz., periodic\nand various complex oscillations including period-n (n=2,4,8...) dynamics and\nchaos. We further extend our study by considering a system of inhibitory\nneurons arranged in a one-dimensional ring topology and determine the optimal\nconditions for sustained activity. Our work highlights the nonlinear dynamical\nbehavior arising due to the combined effects of gap-junctions and strong\nsynaptic inhibition, which can have potential implications in maintaining\nrobust memory patterns.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:25:58 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Janaki", "R.", ""], ["Vytheeswaran", "A. S.", ""]]}, {"id": "2104.02827", "submitter": "Matthew Singh", "authors": "Matthew F. Singh, Chong Wang, Michael W. Cole, and ShiNung Ching", "title": "Efficient state and parameter estimation for high-dimensional nonlinear\n  system identification with application to MEG brain network modeling", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY q-bio.NC q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  System identification poses a significant bottleneck to characterizing and\ncontrolling complex systems. This challenge is greatest when both the system\nstates and parameters are not directly accessible leading to a dual-estimation\nproblem. Current approaches to such problems are limited in their ability to\nscale with many-parameter systems as often occurs in networks. In the current\nwork, we present a new, computationally efficient approach to treat large\ndual-estimation problems. Our approach consists of directly integrating\npseudo-optimal state estimation (the Extended Kalman Filter) into a\ndual-optimization objective, leaving a differentiable cost/error function of\nonly in terms of the unknown system parameters which we solve using numerical\ngradient/Hessian methods. Intuitively, our approach consists of solving for the\nparameters that generate the most accurate state estimator (Extended Kalman\nFilter). We demonstrate that our approach is at least as accurate in state and\nparameter estimation as joint Kalman Filters (Extended/Unscented), despite\nlower complexity. We demonstrate the utility of our approach by inverting\nanatomically-detailed individualized brain models from human\nmagnetoencephalography (MEG) data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 23:25:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Singh", "Matthew F.", ""], ["Wang", "Chong", ""], ["Cole", "Michael W.", ""], ["Ching", "ShiNung", ""]]}, {"id": "2104.03140", "submitter": "Fabrizio Pittorino", "authors": "Marco Stucchi, Fabrizio Pittorino, Matteo di Volo, Alessandro Vezzani,\n  Raffaella Burioni", "title": "Order symmetry breaking and broad distribution of events in spiking\n  neural networks with continuous membrane potential", "comments": "10 pages, 9 figures; to appear in Chaos, Solitons & Fractals", "journal-ref": null, "doi": "10.1016/j.chaos.2021.110946", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an exactly integrable version of the well-known leaky\nintegrate-and-fire (LIF) model, with continuous membrane potential at the\nspiking event, the c-LIF. We investigate the dynamical regimes of a fully\nconnected network of excitatory c-LIF neurons in the presence of short-term\nsynaptic plasticity. By varying the coupling strength among neurons, we show\nthat a complex chaotic dynamics arises, characterized by scale free avalanches.\nThe origin of this phenomenon in the c-LIF can be related to the order symmetry\nbreaking in neurons spike-times, which corresponds to the onset of a broad\nactivity distribution. Our analysis uncovers a general mechanism through which\nnetworks of simple neurons can be attracted to a complex basin in the phase\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:14:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Stucchi", "Marco", ""], ["Pittorino", "Fabrizio", ""], ["di Volo", "Matteo", ""], ["Vezzani", "Alessandro", ""], ["Burioni", "Raffaella", ""]]}, {"id": "2104.04132", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann,\n  Terrence J. Sejnowski, Christopher Kanan", "title": "Replay in Deep Learning: Current Approaches and Missing Biological\n  Elements", "comments": "Accepted for publication in the MIT Press journal of Neural\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replay is the reactivation of one or more neural patterns, which are similar\nto the activation patterns experienced during past waking experiences. Replay\nwas first observed in biological neural networks during sleep, and it is now\nthought to play a critical role in memory formation, retrieval, and\nconsolidation. Replay-like mechanisms have been incorporated into deep\nartificial neural networks that learn over time to avoid catastrophic\nforgetting of previous knowledge. Replay algorithms have been successfully used\nin a wide range of deep learning methods within supervised, unsupervised, and\nreinforcement learning paradigms. In this paper, we provide the first\ncomprehensive comparison between replay in the mammalian brain and replay in\nartificial neural networks. We identify multiple aspects of biological replay\nthat are missing in deep learning systems and hypothesize how they could be\nutilized to improve artificial neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:19:08 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:01:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Krishnan", "Giri P.", ""], ["Bazhenov", "Maxim", ""], ["Siegelmann", "Hava T.", ""], ["Sejnowski", "Terrence J.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2104.05497", "submitter": "Marco A. Formoso", "authors": "Marco A. Formoso, Andr\\'es Ortiz, Francisco J. Mart\\'inez-Murcia,\n  Nicol\\'as Gallego-Molina, Juan L. Luque", "title": "Modelling Brain Connectivity Networks by Graph Embedding for Dyslexia\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several methods have been developed to extract information from\nelectroencephalograms (EEG). One of them is Phase-Amplitude Coupling (PAC)\nwhich is a type of Cross-Frequency Coupling (CFC) method, consisting in measure\nthe synchronization of phase and amplitude for the different EEG bands and\nelectrodes. This provides information regarding brain areas that are\nsynchronously activated, and eventually, a marker of functional connectivity\nbetween these areas. In this work, intra and inter electrode PAC is computed\nobtaining the relationship among different electrodes used in EEG. The\nconnectivity information is then treated as a graph in which the different\nnodes are the electrodes and the edges PAC values between them. These\nstructures are embedded to create a feature vector that can be further used to\nclassify multichannel EEG samples. The proposed method has been applied to\nclassified EEG samples acquired using specific auditory stimuli in a task\ndesigned for dyslexia disorder diagnosis in seven years old children EEG's. The\nproposed method provides AUC values up to 0.73 and allows selecting the most\ndiscriminant electrodes and EEG bands.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:27:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Formoso", "Marco A.", ""], ["Ortiz", "Andr\u00e9s", ""], ["Mart\u00ednez-Murcia", "Francisco J.", ""], ["Gallego-Molina", "Nicol\u00e1s", ""], ["Luque", "Juan L.", ""]]}, {"id": "2104.05575", "submitter": "Rufin VanRullen", "authors": "Rufin VanRullen and Andrea Alamia", "title": "GAttANet: Global attention agreement for convolutional neural networks", "comments": "Paper accepted to ICANN 2021 - The 30th International Conference on\n  Artificial Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:45:10 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 08:16:07 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["VanRullen", "Rufin", ""], ["Alamia", "Andrea", ""]]}, {"id": "2104.05989", "submitter": "Swapna Sasi", "authors": "Mahak Kothari, Swapna Sasi, Jun Chen, Elham Zareian, Basabdatta Sen\n  Bhattacharya", "title": "Bayesian Optimisation for a Biologically Inspired Population Neural\n  Network", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We have used Bayesian Optimisation (BO) to find hyper-parameters in an\nexisting biologically plausible population neural network. The 8-dimensional\noptimal hyper-parameter combination should be such that the network dynamics\nsimulate the resting state alpha rhythm (8 - 13 Hz rhythms in brain signals).\nEach combination of these eight hyper-parameters constitutes a 'datapoint' in\nthe parameter space. The best combination of these parameters leads to the\nneural network's output power spectral peak being constraint within the alpha\nband. Further, constraints were introduced to the BO algorithm based on\nqualitative observation of the network output time series, so that high\namplitude pseudo-periodic oscillations are removed. Upon successful\nimplementation for alpha band, we further optimised the network to oscillate\nwithin the theta (4 - 8 Hz) and beta (13 - 30 Hz) bands. The changing rhythms\nin the model can now be studied using the identified optimal hyper-parameters\nfor the respective frequency bands. We have previously tuned parameters in the\nexisting neural network by the trial-and-error approach; however, due to time\nand computational constraints, we could not vary more than three parameters at\nonce. The approach detailed here, allows an automatic hyper-parameter search,\nproducing reliable parameter sets for the network.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:48:42 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Kothari", "Mahak", ""], ["Sasi", "Swapna", ""], ["Chen", "Jun", ""], ["Zareian", "Elham", ""], ["Bhattacharya", "Basabdatta Sen", ""]]}, {"id": "2104.05991", "submitter": "Nicol\\'as Gallego", "authors": "Nicol\\'as Gallego-Molina, Marco Formoso, Andr\\'es Ortiz, Francisco J.\n  Mart\\'inez-Murcia, Juan L. Luque", "title": "Temporal EigenPAC for dyslexia diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography signals allow to explore the functional activity of\nthe brain cortex in a non-invasive way. However, the analysis of these signals\nis not straightforward due to the presence of different artifacts and the very\nlow signal-to-noise ratio. Cross-Frequency Coupling (CFC) methods provide a way\nto extract information from EEG, related to the synchronization among frequency\nbands. However, CFC methods are usually applied in a local way, computing the\ninteraction between phase and amplitude at the same electrode. In this work we\nshow a method to compute PAC features among electrodes to study the functional\nconnectivity. Moreover, this has been applied jointly with Principal Component\nAnalysis to explore patterns related to Dyslexia in 7-years-old children. The\ndeveloped methodology reveals the temporal evolution of PAC-based connectivity.\nDirections of greatest variance computed by PCA are called eigenPACs here,\nsince they resemble the classical \\textit{eigenfaces} representation. The\nprojection of PAC data onto the eigenPACs provide a set of features that has\ndemonstrates their discriminative capability, specifically in the Beta-Gamma\nbands.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:51:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gallego-Molina", "Nicol\u00e1s", ""], ["Formoso", "Marco", ""], ["Ortiz", "Andr\u00e9s", ""], ["Mart\u00ednez-Murcia", "Francisco J.", ""], ["Luque", "Juan L.", ""]]}, {"id": "2104.06339", "submitter": "Chiara Mastrogiuseppe", "authors": "Ruben Moreno-Bote and Chiara Mastrogiuseppe", "title": "Deep imagination is a close to optimal policy for planning in large\n  decision trees under limited resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many decisions involve choosing an uncertain course of actions in deep and\nwide decision trees, as when we plan to visit an exotic country for vacation.\nIn these cases, exhaustive search for the best sequence of actions is not\ntractable due to the large number of possibilities and limited time or\ncomputational resources available to make the decision. Therefore, planning\nagents need to balance breadth (exploring many actions at each level of the\ntree) and depth (exploring many levels in the tree) to allocate optimally their\nfinite search capacity. We provide efficient analytical solutions and numerical\nanalysis to the problem of allocating finite sampling capacity in one shot to\nlarge decision trees. We find that in general the optimal policy is to allocate\nfew samples per level so that deep levels can be reached, thus favoring depth\nover breadth search. In contrast, in poor environments and at low capacity, it\nis best to broadly sample branches at the cost of not sampling deeply, although\nthis policy is marginally better than deep allocations. Our results provide a\ntheoretical foundation for the optimality of deep imagination for planning and\nshow that it is a generally valid heuristic that could have evolved from the\nfinite constraints of cognitive systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:31:24 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Moreno-Bote", "Ruben", ""], ["Mastrogiuseppe", "Chiara", ""]]}, {"id": "2104.06937", "submitter": "Wilma Bainbridge", "authors": "Wilma A. Bainbridge", "title": "Shared memories driven by the intrinsic memorability of items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When we experience an event, it feels like our previous experiences, our\ninterpretations of that event (e.g., aesthetics, emotions), and our current\nstate will determine how we will remember it. However, recent work has revealed\na strong sway of the visual world itself in influencing what we remember and\nforget. Certain items -- including certain faces, words, images, and movements\n-- are intrinsically memorable or forgettable across observers, regardless of\nindividual differences. Further, neuroimaging research has revealed that the\nbrain is sensitive to memorability both rapidly and automatically during late\nperception. These strong consistencies in memory across people may reflect the\nbroad organizational principles of our sensory environment, and may reveal how\nthe brain prioritizes information before encoding items into memory. In this\nchapter, I will discuss our current state-of-the-art understanding of\nmemorability for visual information, and what these findings imply about how we\nperceive and remember visual events.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:03:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bainbridge", "Wilma A.", ""]]}, {"id": "2104.07053", "submitter": "Gr\\'egoire Sergeant-Perthuis", "authors": "David Rudrauf, Gr\\'egoire Sergeant-Perthuis, Yvain Tisserand, Teerawat\n  Monnor, Olivier Belli", "title": "Combining the Projective Consciousness Model and Virtual Humans to\n  assess ToM capacity in Virtual Reality: a proof-of-concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relating explicit psychological mechanisms and observable behaviours is a\ncentral aim of psychological and behavioural science. We implemented the\nprinciples of the Projective Consciousness Model into artificial agents\nembodied as virtual humans, as a proof-of-concept for a methodological\nframework aimed at simulating behaviours and assessing underlying psychological\nparameters, in the context of experiments in virtual reality. We focus on\nsimulating the role of Theory of Mind (ToM) in the choice of strategic\nbehaviours of approach and avoidance to optimise the satisfaction of agents'\npreferences. We designed an experiment in a virtual environment that could be\nused with real humans, allowing us to classify behaviours as a function of\norder of ToM, up to the second order. We show that our agents demonstrate\nexpected behaviours with consistent parameters of ToM in this experiment. We\nalso show that the agents can be used to estimate correctly each other order of\nToM. A similar approach could be used with real humans in virtual reality\nexperiments not only to enable human participants to interact with parametric,\nvirtual humans as stimuli, but also as a mean of inference to derive\nmodel-based psychological assessments of the participants.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 23:55:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rudrauf", "David", ""], ["Sergeant-Perthuis", "Gr\u00e9goire", ""], ["Tisserand", "Yvain", ""], ["Monnor", "Teerawat", ""], ["Belli", "Olivier", ""]]}, {"id": "2104.07059", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, L. F. Abbott", "title": "Neural population geometry: An approach for understanding biological and\n  artificial neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in experimental neuroscience have transformed our ability to explore\nthe structure and function of neural circuits. At the same time, advances in\nmachine learning have unleashed the remarkable computational power of\nartificial neural networks (ANNs). While these two fields have different tools\nand applications, they present a similar challenge: namely, understanding how\ninformation is embedded and processed through high-dimensional representations\nto solve complex tasks. One approach to addressing this challenge is to utilize\nmathematical and computational tools to analyze the geometry of these\nhigh-dimensional representations, i.e., neural population geometry. We review\nexamples of geometrical approaches providing insight into the function of\nbiological and artificial neural networks: representation untangling in\nperception, a geometric theory of classification capacity, disentanglement and\nabstraction in cognitive systems, topological representations underlying\ncognitive maps, dynamic untangling in motor systems, and a dynamical approach\nto cognition. Together, these findings illustrate an exciting trend at the\nintersection of machine learning, neuroscience, and geometry, in which neural\npopulation geometry provides a useful population-level mechanistic descriptor\nunderlying task implementation. Importantly, geometric descriptions are\napplicable across sensory modalities, brain regions, network architectures and\ntimescales. Thus, neural population geometry has the potential to unify our\nunderstanding of structure and function in biological and artificial neural\nnetworks, bridging the gap between single neurons, populations and behavior.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:10:34 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:30:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chung", "SueYeon", ""], ["Abbott", "L. F.", ""]]}, {"id": "2104.07062", "submitter": "Po T. Wang", "authors": "Po T. Wang, Colin M. McCrimmon, Susan J. Shaw, Hui Gong, Luis A. Chui,\n  Payam Heydari, Charles Y. Liu, An H. Do, Zoran Nenadic", "title": "Decoding of the Walking States and Step Rates from Cortical\n  Electrocorticogram Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain-computer interfaces (BCIs) have shown promising results in restoring\nmotor function to individuals with spinal cord injury. These systems have\ntraditionally focused on the restoration of upper extremity function; however,\nthe lower extremities have received relatively little attention. Early\nfeasibility studies used noninvasive electroencephalogram (EEG)-based BCIs to\nrestore walking function to people with paraplegia. However, the limited\nspatiotemporal resolution of EEG signals restricted the application of these\nBCIs to elementary gait tasks, such as the initiation and termination of\nwalking. To restore more complex gait functions, BCIs must accurately decode\nadditional degrees of freedom from brain signals. In this study, we used\nsubdurally recorded electrocorticogram (ECoG) signals from able-bodied subjects\nto design a decoder capable of predicting the walking state and step rate\ninformation. We recorded ECoG signals from the motor cortices of two\nindividuals as they walked on a treadmill at different speeds. Our offline\nanalysis demonstrated that the state information could be decoded from >16\nminutes of ECoG data with an unprecedented accuracy of 99.8%. Additionally,\nusing a Bayesian filter approach, we achieved an average correlation\ncoefficient between the decoded and true step rates of 0.934. When combined,\nthese decoders may yield decoding accuracies sufficient to safely operate\npresent-day walking prostheses.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:16:20 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wang", "Po T.", ""], ["McCrimmon", "Colin M.", ""], ["Shaw", "Susan J.", ""], ["Gong", "Hui", ""], ["Chui", "Luis A.", ""], ["Heydari", "Payam", ""], ["Liu", "Charles Y.", ""], ["Do", "An H.", ""], ["Nenadic", "Zoran", ""]]}, {"id": "2104.07257", "submitter": "Jizhao Liu", "authors": "Jizhao Liu, Jing Lian, J C Sprott, Yide Ma", "title": "A Novel Neuron Model of Visual Processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating and imitating the neuronal network of humans or mammals is a\npopular topic that has been explored for many years in the fields of pattern\nrecognition and computer vision. Inspired by neuronal conduction\ncharacteristics in the primary visual cortex of cats, pulse-coupled neural\nnetworks (PCNNs) can exhibit synchronous oscillation behavior, which can\nprocess digital images without training. However, according to the study of\nsingle cells in the cat primary visual cortex, when a neuron is stimulated by\nan external periodic signal, the interspike-interval (ISI) distributions\nrepresent a multimodal distribution. This phenomenon cannot be explained by all\nPCNN models. By analyzing the working mechanism of the PCNN, we present a novel\nneuron model of the primary visual cortex consisting of a continuous-coupled\nneural network (CCNN). Our model inherited the threshold exponential decay and\nsynchronous pulse oscillation property of the original PCNN model, and it can\nexhibit chaotic behavior consistent with the testing results of cat primary\nvisual cortex neurons. Therefore, our CCNN model is closer to real visual\nneural networks. For image segmentation tasks, the algorithm based on CCNN\nmodel has better performance than the state-of-art of visual cortex neural\nnetwork model. The strength of our approach is that it helps neurophysiologists\nfurther understand how the primary visual cortex works and can be used to\nquantitatively predict the temporal-spatial behavior of real neural networks.\nCCNN may also inspire engineers to create brain-inspired deep learning networks\nfor artificial intelligence purposes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:04:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Liu", "Jizhao", ""], ["Lian", "Jing", ""], ["Sprott", "J C", ""], ["Ma", "Yide", ""]]}, {"id": "2104.07445", "submitter": "Cristiano Capone", "authors": "Cristiano Capone, Chiara De Luca, Giulia De Bonis, Elena Pastorelli,\n  Anna Letizia Allegra Mascaro, Francesco Resta, Francesco Pavone, Pier\n  Stanislao Paolucci", "title": "Simulations Approaching Data: Cortical Slow Waves in Inferred Models of\n  the Whole Hemisphere of Mouse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent enhancements in neuroscience, like the development of new and powerful\nrecording techniques of the brain activity combined with the increasing\nanatomical knowledge provided by atlases and the growing understanding of\nneuromodulation principles, allow studying the brain at a whole new level,\npaving the way to the creation of extremely detailed effective network models\ndirectly from observed data. Leveraging the advantages of this integrated\napproach, we propose a method to infer models capable of reproducing the\ncomplex spatio-temporal dynamics of the slow waves observed in the experimental\nrecordings of the cortical hemisphere of a mouse under anesthesia. To reliably\nclaim the good match between data and simulations, we implemented a versatile\nensemble of analysis tools, applicable to both experimental and simulated data\nand capable to identify and quantify the spatio-temporal propagation of waves\nacross the cortex. In order to reproduce the observed slow wave dynamics, we\nintroduced an inference procedure composed of two steps: the inner and the\nouter loop. In the inner loop, the parameters of a mean-field model are\noptimized by likelihood maximization, exploiting the anatomical knowledge to\ndefine connectivity priors. The outer loop explores \"external\" parameters,\nseeking for an optimal match between the simulation outcome and the data,\nrelying on observables (speed, directions, and frequency of the waves) apt for\nthe characterization of cortical slow waves; the outer loop includes a periodic\nneuro-modulation for better reproduction of the experimental recordings. We\nshow that our model is capable to reproduce most of the features of the\nnon-stationary and non-linear dynamics displayed by the biological network.\nAlso, the proposed method allows to infer which are the relevant modifications\nof parameters when the brain state changes, e.g. according to anesthesia\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:25:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Capone", "Cristiano", ""], ["De Luca", "Chiara", ""], ["De Bonis", "Giulia", ""], ["Pastorelli", "Elena", ""], ["Mascaro", "Anna Letizia Allegra", ""], ["Resta", "Francesco", ""], ["Pavone", "Francesco", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "2104.07939", "submitter": "Philipp H\\\"ovel", "authors": "Nikita Gutjahr, Philipp H\\\"ovel, Aline Viol", "title": "Controlling extended criticality via modular connectivity", "comments": "20 pages, 11 figure (9 in main text, 2 in Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Criticality has been conjectured as an integral part of neuronal network\ndynamics. Operating at a critical threshold requires precise parameter tuning\nand a corresponding mechanism remains an open question. Recent studies have\nsuggested that topological features observed in brain networks give rise to a\nGriffiths phase, leading to power-laws in brain activity dynamics and the\noperational benefits of criticality in an extended parameter region. Motivated\nby growing evidence of neural correlates of different states of consciousness,\nwe investigate how topological changes affect the expression of a Griffiths\nphase. We analyze the activity decay in modular networks using a\nSusceptible-Infected-Susceptible propagation model and find that we can control\nthe extension of the Griffiths phase by altering intra- and intermodular\nconnectivity. We find that by adjusting system parameters, we can counteract\nchanges in critical behavior and maintain a stable critical region despite\nchanges in network topology. Our results give insight into how structural\nnetwork properties affect the emergence of a Griffiths phase and how its\nfeatures are linked to established topological network metrics. We discuss how\nthose findings can contribute to understand the observed changes in functional\nbrain networks. Finally, we indicate how our results could be useful in the\nstudy of disease spreading.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:36:00 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gutjahr", "Nikita", ""], ["H\u00f6vel", "Philipp", ""], ["Viol", "Aline", ""]]}, {"id": "2104.08175", "submitter": "Maryam Ghanbari", "authors": "M. Ghanbari, Z. Zhou, L-M. Hsu, Y. Han, Y. Sun, P-T. Yap, H. Zhang, D.\n  Shen", "title": "Altered connectedness of the brain chronnectome during the progression\n  of Alzheimer's disease", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph theory has been extensively used to investigate brain network topology\nand its changes in disease cohorts. However, many graph theoretic\nanalysis-based brain network studies focused on the shortest paths or, more\ngenerally, cost-efficiency. In this work, we use two new concepts,\nconnectedness and 2-connectedness, to measure different global properties\ncompared to the previously widely adopted ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:29:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ghanbari", "M.", ""], ["Zhou", "Z.", ""], ["Hsu", "L-M.", ""], ["Han", "Y.", ""], ["Sun", "Y.", ""], ["Yap", "P-T.", ""], ["Zhang", "H.", ""], ["Shen", "D.", ""]]}, {"id": "2104.08537", "submitter": "Federico Bertoni", "authors": "Federico Bertoni, Noemi Montobbio, Alessandro Sarti and Giovanna Citti", "title": "Emergence of Lie symmetries in functional architectures learned by CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the spontaneous development of symmetries in the early\nlayers of a Convolutional Neural Network (CNN) during learning on natural\nimages. Our architecture is built in such a way to mimic the early stages of\nbiological visual systems. In particular, it contains a pre-filtering step\n$\\ell^0$ defined in analogy with the Lateral Geniculate Nucleus (LGN).\nMoreover, the first convolutional layer is equipped with lateral connections\ndefined as a propagation driven by a learned connectivity kernel, in analogy\nwith the horizontal connectivity of the primary visual cortex (V1). The layer\n$\\ell^0$ shows a rotational symmetric pattern well approximated by a Laplacian\nof Gaussian (LoG), which is a well-known model of the receptive profiles of LGN\ncells. The convolutional filters in the first layer can be approximated by\nGabor functions, in agreement with well-established models for the profiles of\nsimple cells in V1. We study the learned lateral connectivity kernel of this\nlayer, showing the emergence of orientation selectivity w.r.t. the learned\nfilters. We also examine the association fields induced by the learned kernel,\nand show qualitative and quantitative comparisons with known group-based models\nof V1 horizontal connectivity. These geometric properties arise spontaneously\nduring the training of the CNN architecture, analogously to the emergence of\nsymmetries in visual systems thanks to brain plasticity driven by external\nstimuli.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:23:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bertoni", "Federico", ""], ["Montobbio", "Noemi", ""], ["Sarti", "Alessandro", ""], ["Citti", "Giovanna", ""]]}, {"id": "2104.09188", "submitter": "Claudius Gros", "authors": "Claudius Gros", "title": "A devil's advocate view on 'self-organized' brain criticality", "comments": null, "journal-ref": "Journal of Physics: Complexity 2, 031001 (2021)", "doi": "10.1088/2632-072X/abfa0f", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationarity of the constituents of the body and of its functionalities is a\nbasic requirement for life, being equivalent to survival in first place.\nAssuming that the resting state activity of the brain serves essential\nfunctionalities, stationarity entails that the dynamics of the brain needs to\nbe regulated on a time-averaged basis. The combination of recurrent and driving\nexternal inputs must therefore lead to a non-trivial stationary neural\nactivity, a condition which is fulfilled for afferent signals of varying\nstrengths only close to criticality. In this view, the benefits of working\nvicinity of a second-order phase transition, such as signal enhancements, are\nnot the underlying evolutionary drivers, but side effects of the requirement to\nkeep the brain functional in first place. It is hence more appropriate to use\nthe term 'self-regulated' in this context, instead of 'self-organized'.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 10:21:53 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Gros", "Claudius", ""]]}, {"id": "2104.09218", "submitter": "Nikita Novikov", "authors": "Nikita Novikov, Denis Zakharov, Victoria Moiseeva and Boris Gutkin", "title": "Activity stabilization in a population model of working memory by\n  sinusoidal and noisy inputs", "comments": "35 pages, 10 figures, to be published in Frontiers in Neural Circuits", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to mechanistic theories of working memory (WM), information is\nretained as persistent spiking activity of cortical neural networks. Yet, how\nthis activity is related to changes in the oscillatory profile observed during\nWM tasks remains an open issue. We explore joint effects of input gamma-band\noscillations and noise on the dynamics of several firing rate models of WM. The\nconsidered models have a metastable active regime, i.e. they demonstrate\nlong-lasting transient post-stimulus firing rate elevation. We start from a\nsingle excitatory-inhibitory circuit and demonstrate that either gamma-band or\nnoise input could stabilize the active regime, thus supporting WM retention. We\nthen consider a system of two circuits with excitatory intercoupling. We find\nthat fast coupling allows for better stabilization by common noise compared to\nindependent noise and stronger amplification of this effect by in-phase gamma\ninputs compared to anti-phase inputs. Finally, we consider a multi-circuit\nsystem comprised of two clusters, each containing a group of circuits receiving\na common noise input and a group of circuits receiving independent noise. Each\ncluster is associated with its own local gamma generator, so all its circuits\nreceive gamma-band input in the same phase. We find that gamma-band input\ndifferentially stabilizes the activity of the \"common-noise\" groups compared to\nthe \"independent-noise\" groups. If the inter-cluster connections are fast, this\neffect is more pronounced when the gamma-band input is delivered to the\nclusters in the same phase rather than in the anti-phase. Assuming that the\ncommon noise comes from a large-scale distributed WM representation, our\nresults demonstrate that local gamma oscillations can stabilize the activity of\nthe corresponding parts of this representation, with stronger effect for fast\nlong-range connections and synchronized gamma oscillations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:30:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Novikov", "Nikita", ""], ["Zakharov", "Denis", ""], ["Moiseeva", "Victoria", ""], ["Gutkin", "Boris", ""]]}, {"id": "2104.09424", "submitter": "Yu-Juan Sun", "authors": "Yu-Juan Sun and Wei-Min Zhang", "title": "Modeling the Nervous System as An Open Quantum System", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a neural network model of multi-neuron interacting system that\nsimulates neurons to interact each other through the surroundings of neuronal\ncell bodies. We physically model the neuronal cell surroundings, include the\ndendrites, the axons and the synapses as well as the surrounding glial cells,\nas a collection of all kinds of oscillating modes arisen from the electric\ncircuital environment of neuronal action potentials. By analyzing the dynamics\nof this neural model through the master equation approach of open quantum\nsystems, we investigate the collective behavior of neurons. After applying\nstimulations to the neural network, the neuronal collective state is activated\nand shows the action potential behavior. We find that this model can generate\nrandom neuron-neuron interactions and is proper to describe the process of\ninformation transmission in the nervous system physically, which may pave a\npotential route toward understanding the dynamics of nervous system.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:17:09 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 08:56:21 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Sun", "Yu-Juan", ""], ["Zhang", "Wei-Min", ""]]}, {"id": "2104.09447", "submitter": "Guy Ben-Yosef", "authors": "Guy Ben-Yosef, Gabriel Kreiman, Shimon Ullman", "title": "What can human minimal videos tell us about dynamic recognition models?", "comments": "Published as a workshop paper at Bridging AI and Cognitive Science\n  (ICLR 2020). Extended paper was published at Cognition", "journal-ref": null, "doi": "10.1016/j.cognition.2020.104263", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In human vision objects and their parts can be visually recognized from\npurely spatial or purely temporal information but the mechanisms integrating\nspace and time are poorly understood. Here we show that human visual\nrecognition of objects and actions can be achieved by efficiently combining\nspatial and motion cues in configurations where each source on its own is\ninsufficient for recognition. This analysis is obtained by identifying minimal\nvideos: these are short and tiny video clips in which objects, parts, and\nactions can be reliably recognized, but any reduction in either space or time\nmakes them unrecognizable. State-of-the-art deep networks for dynamic visual\nrecognition cannot replicate human behavior in these configurations. This gap\nbetween humans and machines points to critical mechanisms in human dynamic\nvision that are lacking in current models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:53:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ben-Yosef", "Guy", ""], ["Kreiman", "Gabriel", ""], ["Ullman", "Shimon", ""]]}, {"id": "2104.09549", "submitter": "Michael Shvartsman", "authors": "Lucy Owen, Jonathan Browder, Benjamin Letham, Gideon Stocek, Chase\n  Tymms and Michael Shvartsman", "title": "Adaptive Nonparametric Psychophysics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a new set of models and adaptive psychometric testing methods\nfor multidimensional psychophysics. In contrast to traditional adaptive\nstaircase methods like PEST and QUEST, the method is multi-dimensional and does\nnot require a grid over contextual dimensions, retaining sub-exponential\nscaling in the number of stimulus dimensions. In contrast to more recent\nmulti-dimensional adaptive methods, our underlying model does not require a\nparametric assumption about the interaction between intensity and the\nadditional dimensions. In addition, we introduce a new active sampling policy\nthat explicitly targets psychometric detection threshold estimation and does so\nsubstantially faster than policies that attempt to estimate the full\npsychometric function (though it still provides estimates of the function,\nalbeit with lower accuracy). Finally, we introduce AEPsych, a user-friendly\nopen-source package for nonparametric psychophysics that makes these\ntechnically-challenging methods accessible to the broader community.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:19:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Owen", "Lucy", ""], ["Browder", "Jonathan", ""], ["Letham", "Benjamin", ""], ["Stocek", "Gideon", ""], ["Tymms", "Chase", ""], ["Shvartsman", "Michael", ""]]}, {"id": "2104.09743", "submitter": "Nikolaus Kriegeskorte", "authors": "Nikolaus Kriegeskorte, Xue-Xin Wei", "title": "Neural tuning and representational geometry", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A central goal of neuroscience is to understand the representations formed by\nbrain activity patterns and their connection to behavior. The classical\napproach is to investigate how individual neurons encode the stimuli and how\ntheir tuning determines the fidelity of the neural representation. Tuning\nanalyses often use the Fisher information to characterize the sensitivity of\nneural responses to small changes of the stimulus. In recent decades,\nmeasurements of large populations of neurons have motivated a complementary\napproach, which focuses on the information available to linear decoders. The\ndecodable information is captured by the geometry of the representational\npatterns in the multivariate response space. Here we review neural tuning and\nrepresentational geometry with the goal of clarifying the relationship between\nthem. The tuning induces the geometry, but different sets of tuned neurons can\ninduce the same geometry. The geometry determines the Fisher information, the\nmutual information, and the behavioral performance of an ideal observer in a\nrange of psychophysical tasks. We argue that future studies can benefit from\nconsidering both tuning and geometry to understand neural codes and reveal the\nconnections between stimulus, brain activity, and behavior.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:35:35 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kriegeskorte", "Nikolaus", ""], ["Wei", "Xue-Xin", ""]]}, {"id": "2104.09943", "submitter": "Olga Lukyanova", "authors": "Oleg Nikitin, Olga Lukyanova, Alex Kunin", "title": "The principle of weight divergence facilitation for unsupervised pattern\n  recognition in spiking neural networks", "comments": "9 pages, 5 figures, submitted to the conference ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parallels between the signal processing tasks and biological neurons lead to\nan understanding of the principles of self-organized optimization of input\nsignal recognition. In the present paper, we discuss such similarities among\nbiological and technical systems. We propose the addition to the well-known\nSTDP synaptic plasticity rule to directs the weight modification towards the\nstate associated with the maximal difference between the background noise and\ncorrelated signals. The principle of physically constrained weight growth is\nused as a basis for such control of the modification of the weights. It is\nproposed, that biological synaptic straight modification is restricted by the\nexistence and production of bio-chemical 'substances' needed for plasticity\ndevelopment. In this paper, the information about the noise-to-signal ratio is\nused to control such a substances' production and storage and to drive the\nneuron's synaptic pressures towards the state with the best signal-to-noise\nratio. Several experiments with different input signal regimes are considered\nto understand the functioning of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:11:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nikitin", "Oleg", ""], ["Lukyanova", "Olga", ""], ["Kunin", "Alex", ""]]}, {"id": "2104.10070", "submitter": "Natalie Klein", "authors": "Natalie Klein, Joshua H. Siegle, Tobias Teichert, Robert E. Kass", "title": "Cross-population coupling of neural activity based on Gaussian process\n  current source densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because LFPs arise from multiple sources in different spatial locations, they\ndo not easily reveal coordinated activity across neural populations on a\ntrial-to-trial basis. As we show here, however, once disparate source signals\nare decoupled, their trial-to-trial fluctuations become more accessible, and\ncross-population correlations become more apparent. To decouple sources we\nintroduce a general framework for estimation of current source densities\n(CSDs). In this framework, the set of LFPs result from noise being added to the\ntransform of the CSD by a biophysical forward model, while the CSD is\nconsidered to be the sum of a zero-mean, stationary, spatiotemporal Gaussian\nprocess, having fast and slow components, and a mean function, which is the sum\nof multiple time-varying functions distributed across space, each varying\nacross trials. We derived biophysical forward models relevant to the data we\nanalyzed. In simulation studies this approach improved identification of source\nsignals compared to existing CSD estimation methods. Using data recorded from\nprimate auditory cortex, we analyzed trial-to-trial fluctuations in both\nsteady-state and task-evoked signals. We found cortical layer-specific phase\ncoupling between two probes and showed that the same analysis applied directly\nto LFPs did not recover these patterns. We also found task-evoked CSDs to be\ncorrelated across probes, at specific cortical depths. Using data from\nNeuropixels probes in mouse visual areas, we again found evidence for\ndepth-specific phase coupling of areas V1 and LM based on the CSDs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:46:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Klein", "Natalie", ""], ["Siegle", "Joshua H.", ""], ["Teichert", "Tobias", ""], ["Kass", "Robert E.", ""]]}, {"id": "2104.10255", "submitter": "Dushyant Sahoo", "authors": "Dushyant Sahoo and Christos Davatzikos", "title": "Extraction of Hierarchical Functional Connectivity Components in human\n  brain using Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of sparse hierarchical components reflecting patterns of the\nbrain's functional connectivity from rsfMRI data can contribute to our\nunderstanding of the brain's functional organization, and can lead to\nbiomarkers of diseases. However, inter-scanner variations and other confounding\nfactors pose a challenge to the robust and reproducible estimation of\nfunctionally-interpretable brain networks, and especially to reproducible\nbiomarkers. Moreover, the brain is believed to be organized hierarchically, and\nhence single-scale decompositions miss this hierarchy. The paper aims to use\ncurrent advancements in adversarial learning to estimate interpretable\nhierarchical patterns in the human brain using rsfMRI data, which are robust to\n\"adversarial effects\" such as inter-scanner variations. We write the estimation\nproblem as a minimization problem and solve it using alternating updates.\nExtensive experiments on simulation and a real-world dataset show high\nreproducibility of the components compared to other well-known methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:38:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Sahoo", "Dushyant", ""], ["Davatzikos", "Christos", ""]]}, {"id": "2104.10670", "submitter": "Kynan Eng", "authors": "Kynan Eng", "title": "A Note on Relative Consciousness", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes a mathematical formulation for measuring how one system\ncan estimate the consciousness of another. This consciousness estimate is\nalways relative to the observer. The paper shows how this formulation leads to\nsimple resolutions of some key problems of consciousness.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 22:33:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Eng", "Kynan", ""]]}, {"id": "2104.10795", "submitter": "Connor Courtney", "authors": "Connor D. Courtney, Arin Pamukcu, C. Savio Chan", "title": "The external pallidum: think locally, act globally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The globus pallidus (GPe), as part of the basal ganglia, was once described\nas a black box. As its functions were unclear, the GPe has been\nunderappreciated for decades. The advent of molecular tools has sparked a\nresurgence in interest in the GPe. A recent flurry of publications has unveiled\nthe molecular landscape, synaptic organization, and functions of the GPe. It is\nnow clear that the GPe plays multifaceted roles in both motor and non-motor\nfunctions, and is critically implicated in several motor disorders.\nAccordingly, the GPe should no longer be considered as a mere homogeneous relay\nwithin the so-called \"indirect pathway\". Here we summarize the key findings,\nchallenges, consensuses, and disputes from the past few years.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 23:34:56 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Courtney", "Connor D.", ""], ["Pamukcu", "Arin", ""], ["Chan", "C. Savio", ""]]}, {"id": "2104.10829", "submitter": "Tomoki Kurikawa", "authors": "Tomoki Kurikawa", "title": "Transitions among metastable states underlie context-dependent working\n  memories in a multiple timescale network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transitions between metastable states are commonly observed in the neural\nsystem and underlie various cognitive functions such as working memory. In a\nprevious study, we have developed a neural network model with the slow and fast\npopulations, wherein simple Hebb-type learning enables stable and complex\n(e.g., non-Markov) transitions between neural states. This model is distinct\nfrom a network with asymmetric Hebbian connectivity and a network trained with\nsupervised machine learning methods: the former generates simple Markov\nsequences. The latter generates complex but vulnerable sequences against\nperturbation and its learning methods are biologically implausible. By using\nour model, we propose and demonstrate a novel mechanism underlying stable\nworking memories: sequentially stabilizing and destabilizing task-related\nstates in the fast neural dynamics. The slow dynamics maintain a history of the\napplied inputs, e.g., context signals, and enable the task-related states to be\nstabilized in a context-dependent manner. We found that only a single (or a\nfew) state(s) is stabilized in each epoch (i.e., a period in the presence of\nthe context signal and a delayed period) in a working memory task, resulting in\na robust performance against noise and change in a task protocol. These results\nsuggest a simple mechanism underlying complex and stable processing in neural\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 02:09:34 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Kurikawa", "Tomoki", ""]]}, {"id": "2104.10957", "submitter": "Chen-Gia Tsai", "authors": "Chia-Wei Li and Chen-Gia Tsai", "title": "Differential brain connectivity patterns while listening to breakup and\n  rebellious songs: A functional magnetic resonance imaging study", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Song appreciation involves a broad range of mental processes, and different\nneural networks may be activated by different song types. The aim of the\npresent study was to show differential functional connectivity of the\nprefrontal cortices while listening to breakup and rebellious songs. Breakup\nsongs describe romance and longing, whereas rebellious songs convey criticism\nof conventional ideas or socio-cultural norms. We hypothesized that the medial\nand lateral prefrontal cortices may interact with different brain regions in\nresponse to these two song types. Functional magnetic resonance imaging data of\nfifteen participants were collected while they were listening to two complete\nbreakup songs and two complete rebellious songs currently popular in Taiwan.\nThe results showed that listening to the breakup songs, compared to the\nrebellious songs, enhanced the coupling between the medial prefrontal cortex\nand several emotion-related regions, including the thalamus, caudate, amygdala,\nhippocampus, middle orbitofrontal cortex, and right inferior frontal gyrus.\nThis coupling may reflect the neural processes of pain empathy, reward\nprocessing, compassion, and reappraisal in response to longing and sorrow\nexpressed by the breakup songs. Compared to the breakup songs, listening to the\nrebellious songs was associated with enhanced coupling between subregions in\nthe prefrontal and orbitofrontal cortices. These areas might work in concert to\nsupport re-evaluation of conventional ideas or socio-cultural norms as\nsuggested by the rebellious songs. This study advanced our understanding of the\nintegration of brain functions while processing complex information.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:37:07 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 07:19:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Chia-Wei", ""], ["Tsai", "Chen-Gia", ""]]}, {"id": "2104.11666", "submitter": "Geza Odor", "authors": "G\\'eza \\'Odor, Michael Gastner, Jeffrey Kelling, Gustavo Deco", "title": "Modelling the very large-scale connectome", "comments": "23 pages, 10 figures, Review article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review, we discuss critical dynamics of simple nonequilibrium models\non large connectomes, obtained by diffusion MRI, representing the white matter\nof the human brain. In the first chapter, we overview graph theoretical and\ntopological analysis of these networks, pointing out that universality allows\nselecting a representative network, the KKI-18, which has been used for\ndynamical simulation. The critical and sub-critical behaviour of simple, two-\nor three-state threshold models is discussed with special emphasis on\nrare-region effects leading to robust Griffiths Phases (GP). Numerical results\nof synchronization phenomena, studied by the Kuramoto model, are also shown,\nleading to a continuous analog of the GP, termed frustrated synchronization in\nChimera states. The models presented here exhibit avalanche scaling behaviour\nwith exponents in agreement with brain experimental data if local homeostasis\nis provided.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 15:43:05 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["\u00d3dor", "G\u00e9za", ""], ["Gastner", "Michael", ""], ["Kelling", "Jeffrey", ""], ["Deco", "Gustavo", ""]]}, {"id": "2104.11739", "submitter": "Carlo Fulvi Mari Ph.D.", "authors": "Carlo Fulvi Mari", "title": "Memory retrieval dynamics and storage capacity of a modular network\n  model of association cortex with featural decomposition", "comments": "26 pages, 9 figures (PDF), pdflatex. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model network is composed of many modules. Each module represents an\nassembly of recurrently connected neurons of a neocortical functional unit and\noperates as a Hebbian autoassociator, storing a number of local configurations\nof neuronal activity, or features, which it can recall upon cue. The global\nmemory patterns are made of specific combinations of features sparsely\ndistributed across the modules. Intermodular connections are modelled as a\nfinite-connectivity random graph. Any pair of features in any respective pair\nof connected modules is allowed to be involved in several memory patterns; the\nnetwork dynamics is defined in such a way as to overcome the consequent\nambiguity of associations. The effects of a hypothetical mechanism of\nlong-range homeostatic synaptic scaling are also investigated.\n  The dynamical process of cued retrieval almost saturates a natural upper\nbound while producing negligible spurious activation. The extent of finite-size\neffects on storage capacity is quantitatively evaluated. In the limit of\ninfinite size, the functional relationship between storage capacity and number\nof features per module reduces to that which other authors found by methods\nfrom equilibrium statistical mechanics, which suggests that the origin of the\nfunctional form is fundamentally of combinatorial nature. In contrast with its\napparent inevitability at intramodular level, long-range synaptic scaling\nresults to be of minor relevance to both retrieval and storage capacity. A\nconjecture is posited about how statistical fluctuation of coordination number\nacross the network may underpin spontaneous emergence of semantic hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:50:49 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mari", "Carlo Fulvi", ""]]}, {"id": "2104.11844", "submitter": "Brian Corneil", "authors": "Sebastian J Lehmann and Brian D Corneil", "title": "Completing the puzzle: why studies in non-human primates are needed to\n  better understand the effects of non-invasive brain stimulation", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain stimulation is a core method in neuroscience. Numerous non-invasive\nbrain stimulation (NIBS) techniques are currently in use in basic and clinical\nresearch, and recent advances promise the ability to non-invasively access deep\nbrain structures. While encouraging, there is a surprising gap in our\nunderstanding of precisely how NIBS perturbs neural activity throughout an\ninterconnected network, and how such perturbed neural activity ultimately links\nto behaviour. In this review, we will consider why non-human primate (NHP)\nmodels of NIBS are ideally situated to address this gap in knowledge, and will\nconsider why the oculomotor network that moves our line of sight offers a\nparticularly valuable platform in which to empirically test hypothesis\nregarding NIBS-induced changes in brain and behaviour. NHP models of NIBS will\nenable investigation of the complex, dynamic effects of brain stimulation\nacross multiple hierarchically interconnected brain areas, networks, and\neffectors. By establishing such links between brain and behavioural output,\nwork in NHPs can help optimize experimental and therapeutic approaches, improve\nNIBS efficacy, and reduce side-effects of NIBS.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 00:31:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lehmann", "Sebastian J", ""], ["Corneil", "Brian D", ""]]}, {"id": "2104.11852", "submitter": "Tiberiu Tesileanu", "authors": "Tiberiu Tesileanu, Siavash Golkar, Samaneh Nasiri, Anirvan M.\n  Sengupta, Dmitri B. Chklovskii", "title": "Neural circuits for dynamics-based segmentation of time series", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The brain must extract behaviorally relevant latent variables from the\nsignals streamed by the sensory organs. Such latent variables are often encoded\nin the dynamics that generated the signal rather than in the specific\nrealization of the waveform. Therefore, one problem faced by the brain is to\nsegment time series based on underlying dynamics. We present two algorithms for\nperforming this segmentation task that are biologically plausible, which we\ndefine as acting in a streaming setting and all learning rules being local. One\nalgorithm is model-based and can be derived from an optimization problem\ninvolving a mixture of autoregressive processes. This algorithm relies on\nfeedback in the form of a prediction error, and can also be used for\nforecasting future samples. In some brain regions, such as the retina, the\nfeedback connections necessary to use the prediction error for learning are\nabsent. For this case, we propose a second, model-free algorithm that uses a\nrunning estimate of the autocorrelation structure of the signal to perform the\nsegmentation. We show that both algorithms do well when tasked with segmenting\nsignals drawn from autoregressive models with piecewise-constant parameters. In\nparticular, the segmentation accuracy is similar to that obtained from\noracle-like methods in which the ground-truth parameters of the autoregressive\nmodels are known. We provide implementations of our algorithms at\nhttps://github.com/ttesileanu/bio-time-series.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 01:54:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Tesileanu", "Tiberiu", ""], ["Golkar", "Siavash", ""], ["Nasiri", "Samaneh", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2104.12187", "submitter": "Jing Mu", "authors": "Jing Mu, David B. Grayden, Ying Tan, Denny Oetomo", "title": "Frequency Superposition -- A Multi-Frequency Stimulation Method in\n  SSVEP-based BCIs", "comments": "4 pages, 5 figures. This work has been submitted to the IEEE EMBC for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible. arXiv admin note: text\n  overlap with arXiv:2011.05861", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The steady-state visual evoked potential (SSVEP) is one of the most widely\nused modalities in brain-computer interfaces (BCIs) due to its many advantages.\nHowever, the existence of harmonics and the limited range of responsive\nfrequencies in SSVEP make it challenging to further expand the number of\ntargets without sacrificing other aspects of the interface or putting\nadditional constraints on the system. This paper introduces a novel\nmulti-frequency stimulation method for SSVEP and investigates its potential to\neffectively and efficiently increase the number of targets presented. The\nproposed stimulation method, obtained by the superposition of the stimulation\nsignals at different frequencies, is size-efficient, allows single-step target\nidentification, puts no strict constraints on the usable frequency range, can\nbe suited to self-paced BCIs, and does not require specific light sources. In\naddition to the stimulus frequencies and their harmonics, the evoked SSVEP\nwaveforms include frequencies that are integer linear combinations of the\nstimulus frequencies. Results of decoding SSVEPs collected from nine subjects\nusing canonical correlation analysis (CCA) with only the frequencies and\nharmonics as reference, also demonstrate the potential of using such a\nstimulation paradigm in SSVEP-based BCIs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 15:56:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mu", "Jing", ""], ["Grayden", "David B.", ""], ["Tan", "Ying", ""], ["Oetomo", "Denny", ""]]}, {"id": "2104.12249", "submitter": "Hong-Gyu Yoon", "authors": "Hong-Gyu Yoon and Pilwon Kim", "title": "A STDP-based Encoding/Decoding Algorithm for Associative and Composite\n  Data", "comments": "12 pages of main text. Source for simplified MATLAB programs\n  performing two numerical tests presented in this article can be found in the\n  following link: https://github.com/hkyoon94/NRSTDP.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-timing-dependent plasticity(STDP) is a biological process of synaptic\nmodification caused by the difference of firing order and timing between\nneurons. In our separate work, we have rigorously shown that STDP in a network\nof neurons transforms periodic input patterns into a geometrical structure\nnamed memory plane, and a proper memory cue near such structure dynamically\nrevives the stored information. Using these results, in this paper, we\ndemonstrate the following two encoding/decoding algorithms handling practical\ndata. First, we perform an auto-associative memory task with a group of images.\nThe results show that any relevant image to the group can be used as a cue in\norder to reconstruct the original images. The next one deals with the process\nof semantic memory representations that are embedded from sentences. The\nresults show that words can recall multiple sentences simultaneously or one\nexclusively, depending on their grammatical relations. This implies that the\nproposed framework is apt to process multiple groups of associative memories\nwith a composite structure.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 20:26:52 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 00:56:36 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Yoon", "Hong-Gyu", ""], ["Kim", "Pilwon", ""]]}, {"id": "2104.13238", "submitter": "Jakob Jordan", "authors": "Jakob Jordan, Jo\\~ao Sacramento, Willem A.M. Wybo, Mihai A. Petrovici,\n  Walter Senn", "title": "Learning Bayes-optimal dendritic opinion pooling", "comments": "29 pages, 10 figures; Mihai A. Petrovici and Walter Senn share senior\n  authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling different opinions and weighting them according to their reliability\nis conducive to making good decisions. We demonstrate that single cortical\nneurons, through the biophysics of conductance-based coupling, perform such\ncomplex probabilistic computations via their natural dynamics. While the\neffective computation can be described as a feedforward process, the\nimplementation critically relies on the bidirectional current flow along the\ndendritic tree. We suggest that dendritic membrane potentials and conductances\nencode opinions and their associated reliabilities, on which the soma acts as a\ndecision maker. Furthermore, we derive gradient-based plasticity rules,\nallowing neurons to learn to represent desired target distributions and to\nweight afferents according to their reliability. Our theory shows how neurons\nperform Bayes-optimal cue integration. It also explains various experimental\nfindings, both on the system and on the single-cell level, and makes new,\ntestable predictions for intracortical neuron and synapse dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:43:46 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jordan", "Jakob", ""], ["Sacramento", "Jo\u00e3o", ""], ["Wybo", "Willem A. M.", ""], ["Petrovici", "Mihai A.", ""], ["Senn", "Walter", ""]]}, {"id": "2104.13398", "submitter": "Dominik Dold", "authors": "Dominik Dold, Josep Soler Garrido", "title": "SpikE: spike-based embeddings for multi-relational graph data", "comments": "Accepted for publication at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent success of reconciling spike-based coding with the error\nbackpropagation algorithm, spiking neural networks are still mostly applied to\ntasks stemming from sensory processing, operating on traditional data\nstructures like visual or auditory data. A rich data representation that finds\nwide application in industry and research is the so-called knowledge graph - a\ngraph-based structure where entities are depicted as nodes and relations\nbetween them as edges. Complex systems like molecules, social networks and\nindustrial factory systems can be described using the common language of\nknowledge graphs, allowing the usage of graph embedding algorithms to make\ncontext-aware predictions in these information-packed environments. We propose\na spike-based algorithm where nodes in a graph are represented by single spike\ntimes of neuron populations and relations as spike time differences between\npopulations. Learning such spike-based embeddings only requires knowledge about\nspike times and spike time differences, compatible with recently proposed\nframeworks for training spiking neural networks. The presented model is easily\nmapped to current neuromorphic hardware systems and thereby moves inference on\nknowledge graphs into a domain where these architectures thrive, unlocking a\npromising industrial application area for this technology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:00:12 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 09:14:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dold", "Dominik", ""], ["Garrido", "Josep Soler", ""]]}, {"id": "2104.13714", "submitter": "Radoslaw Martin Cichy", "authors": "R.M. Cichy, K. Dwivedi, B. Lahner, A. Lascelles, P. Iamshchinina, M.\n  Graumann, A. Andonian, N.A.R. Murty, K. Kay, G. Roig, A. Oliva", "title": "The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of\n  a World in Motion", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sciences of natural and artificial intelligence are fundamentally\nconnected. Brain-inspired human-engineered AI are now the standard for\npredicting human brain responses during vision, and conversely, the brain\ncontinues to inspire invention in AI. To promote even deeper connections\nbetween these fields, we here release the 2021 edition of the Algonauts Project\nChallenge: How the Human Brain Makes Sense of a World in Motion\n(http://algonauts.csail.mit.edu/). We provide whole-brain fMRI responses\nrecorded while 10 human participants viewed a rich set of over 1,000 short\nvideo clips depicting everyday events. The goal of the challenge is to\naccurately predict brain responses to these video clips. The format of our\nchallenge ensures rapid development, makes results directly comparable and\ntransparent, and is open to all. In this way it facilitates interdisciplinary\ncollaboration towards a common goal of understanding visual intelligence. The\n2021 Algonauts Project is conducted in collaboration with the Cognitive\nComputational Neuroscience (CCN) conference.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:38:31 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cichy", "R. M.", ""], ["Dwivedi", "K.", ""], ["Lahner", "B.", ""], ["Lascelles", "A.", ""], ["Iamshchinina", "P.", ""], ["Graumann", "M.", ""], ["Andonian", "A.", ""], ["Murty", "N. A. R.", ""], ["Kay", "K.", ""], ["Roig", "G.", ""], ["Oliva", "A.", ""]]}, {"id": "2104.14102", "submitter": "Shravan Murlidaran", "authors": "Shravan Murlidaran, William Yang Wang, Miguel P. Eckstein", "title": "Comparing Visual Reasoning in Humans and AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in natural language processing and computer vision have led\nto AI models that interpret simple scenes at human levels. Yet, we do not have\na complete understanding of how humans and AI models differ in their\ninterpretation of more complex scenes. We created a dataset of complex scenes\nthat contained human behaviors and social interactions. AI and humans had to\ndescribe the scenes with a sentence. We used a quantitative metric of\nsimilarity between scene descriptions of the AI/human and ground truth of five\nother human descriptions of each scene. Results show that the machine/human\nagreement scene descriptions are much lower than human/human agreement for our\ncomplex scenes. Using an experimental manipulation that occludes different\nspatial regions of the scenes, we assessed how machines and humans vary in\nutilizing regions of images to understand the scenes. Together, our results are\na first step toward understanding how machines fall short of human visual\nreasoning with complex scenes depicting human behaviors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 04:44:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Murlidaran", "Shravan", ""], ["Wang", "William Yang", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "2104.14264", "submitter": "Vivek Saraswat", "authors": "Vivek Saraswat, Ajinkya Gorad, Anand Naik, Aakash Patil, Udayan\n  Ganguly", "title": "Hardware-Friendly Synaptic Orders and Timescales in Liquid State\n  Machines for Speech Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.NE cs.SD q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Liquid State Machines are brain inspired spiking neural networks (SNNs) with\nrandom reservoir connectivity and bio-mimetic neuronal and synaptic models.\nReservoir computing networks are proposed as an alternative to deep neural\nnetworks to solve temporal classification problems. Previous studies suggest\n2nd order (double exponential) synaptic waveform to be crucial for achieving\nhigh accuracy for TI-46 spoken digits recognition. The proposal of long-time\nrange (ms) bio-mimetic synaptic waveforms is a challenge to compact and power\nefficient neuromorphic hardware. In this work, we analyze the role of synaptic\norders namely: {\\delta} (high output for single time step), 0th (rectangular\nwith a finite pulse width), 1st (exponential fall) and 2nd order (exponential\nrise and fall) and synaptic timescales on the reservoir output response and on\nthe TI-46 spoken digits classification accuracy under a more comprehensive\nparameter sweep. We find the optimal operating point to be correlated to an\noptimal range of spiking activity in the reservoir. Further, the proposed 0th\norder synapses perform at par with the biologically plausible 2nd order\nsynapses. This is substantial relaxation for circuit designers as synapses are\nthe most abundant components in an in-memory implementation for SNNs. The\ncircuit benefits for both analog and mixed-signal realizations of 0th order\nsynapse are highlighted demonstrating 2-3 orders of savings in area and power\nconsumptions by eliminating Op-Amps and Digital to Analog Converter circuits.\nThis has major implications on a complete neural network implementation with\nfocus on peripheral limitations and algorithmic simplifications to overcome\nthem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 11:20:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Saraswat", "Vivek", ""], ["Gorad", "Ajinkya", ""], ["Naik", "Anand", ""], ["Patil", "Aakash", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2104.14364", "submitter": "Mark Humphries", "authors": "Mark D. Humphries and Kevin Gurney", "title": "Making decisions in the dark basement of the brain: A look back at the\n  GPR model of action selection and the basal ganglia", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How does your brain decide what you will do next? Over the past few decades\ncompelling evidence has emerged that the basal ganglia, a collection of nuclei\nin the fore- and mid-brain of all vertebrates, are vital to action selection.\nGurney, Prescott, and Redgrave published an influential computational account\nof this idea in Biological Cybernetics in 2001. Here we take a look back at\nthis pair of papers, outlining the \"GPR\" model contained therein, the context\nof that model's development, and the influence it has had over the past twenty\nyears. Tracing its lineage into models and theories still emerging now, we are\nencouraged that the GPR model is that rare thing, a computational model of a\nbrain circuit whose advances were directly built on by others.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:19:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:13:17 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Humphries", "Mark D.", ""], ["Gurney", "Kevin", ""]]}, {"id": "2104.14610", "submitter": "Nimrod Sherf", "authors": "Nimrod Sherf and Maoz Shamir", "title": "STDP and the distribution of preferred phases in the whisker system", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rats and mice use their whiskers to probe the environment. By rhythmically\nswiping their whiskers back and forth they can detect the existence of an\nobject, locate it, and identify its texture. Localization can be accomplished\nby inferring the position of the whisker. Rhythmic neurons that track the phase\nof the whisking cycle encode information about the azimuthal location of the\nwhisker. These neurons are characterized by preferred phases of firing that are\nnarrowly distributed. Consequently, pooling the rhythmic signal from several\nupstream neurons is expected to result in a much narrower distribution of\npreferred phases in the downstream population, which however has not been\nobserved empirically. Here, we show how spike-timing-dependent plasticity\n(STDP) can provide a solution to this conundrum. We investigated the effect of\nSTDP on the utility of a neural population to transmit rhythmic information\ndownstream using the framework of a modeling study. We found that under a wide\nrange of parameters, STDP facilitated the transfer of rhythmic information\ndespite the fact that all the synaptic weights remained dynamic. As a result,\nthe preferred phase of the downstream neuron was not fixed, but rather drifted\nin time at a drift velocity that depended on the preferred phase, thus inducing\na distribution of preferred phases. We further analyzed how the STDP rule\ngoverns the distribution of preferred phases in the downstream population. This\nlink between the STDP rule and the distribution of preferred phases constitutes\na natural test for our theory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:52:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sherf", "Nimrod", ""], ["Shamir", "Maoz", ""]]}, {"id": "2104.14954", "submitter": "Hamdan Awan", "authors": "Hamdan Awan, Andreani Odysseos, Niovi Nicolaou and Sasitharan\n  Balasubramaniam", "title": "Analysis of Molecular Communications on the Growth Structure of\n  Glioblastoma Multiforme", "comments": "7 pages, 10 Figures- Submitted for possible publication in IEEE\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the influence of intercellular communication on the\ndevelopment and progression of Glioblastoma Multiforme (GBM), a grade IV\nmalignant glioma which is defined by an interplay Grow i.e. self renewal and Go\ni.e. invasiveness potential of multiple malignant glioma stem cells. Firstly,\nwe performed wet lab experiments with U87 malignant glioma cells to study the\nnode-stem growth pattern of GBM. Next we develop a model accounting for the\nstructural influence of multiple transmitter and receiver glioma stem cells\nresulting in the node-stem growth structure of GBM tumour. By using information\ntheory we study different properties associated with this communication model\nto show that the growth of GBM in a particular direction (node to stem) is\nrelated to an increase in mutual information. We further show that information\nflow between glioblastoma cells for different levels of invasiveness vary at\ndifferent points between node and stem. These findings are expected to\ncontribute significantly in the design of future therapeutic mechanisms for\nGBM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:41:48 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Awan", "Hamdan", ""], ["Odysseos", "Andreani", ""], ["Nicolaou", "Niovi", ""], ["Balasubramaniam", "Sasitharan", ""]]}]