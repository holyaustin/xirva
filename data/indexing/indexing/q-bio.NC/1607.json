[{"id": "1607.00435", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Pamela K. Douglas, Ying Nian Wu, Arthur L. Brody, Ariana\n  E. Anderson", "title": "Decoding the Encoding of Functional Brain Networks: an fMRI\n  Classification Comparison of Non-negative Matrix Factorization (NMF),\n  Independent Component Analysis (ICA), and Sparse Coding Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks in fMRI are typically identified using spatial independent\ncomponent analysis (ICA), yet mathematical constraints such as sparse coding\nand positivity both provide alternate biologically-plausible frameworks for\ngenerating brain networks. Non-negative Matrix Factorization (NMF) would\nsuppress negative BOLD signal by enforcing positivity. Spatial sparse coding\nalgorithms ($L1$ Regularized Learning and K-SVD) would impose local\nspecialization and a discouragement of multitasking, where the total observed\nactivity in a single voxel originates from a restricted number of possible\nbrain networks.\n  The assumptions of independence, positivity, and sparsity to encode\ntask-related brain networks are compared; the resulting brain networks for\ndifferent constraints are used as basis functions to encode the observed\nfunctional activity at a given time point. These encodings are decoded using\nmachine learning to compare both the algorithms and their assumptions, using\nthe time series weights to predict whether a subject is viewing a video,\nlistening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects.\n  For classifying cognitive activity, the sparse coding algorithm of $L1$\nRegularized Learning consistently outperformed 4 variations of ICA across\ndifferent numbers of networks and noise levels (p$<$0.001). The NMF algorithms,\nwhich suppressed negative BOLD signal, had the poorest accuracy. Within each\nalgorithm, encodings using sparser spatial networks (containing more\nzero-valued voxels) had higher classification accuracy (p$<$0.001). The success\nof sparse coding algorithms may suggest that algorithms which enforce sparse\ncoding, discourage multitasking, and promote local specialization may capture\nbetter the underlying source processes than those which allow inexhaustible\nlocal processes such as ICA.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 23:48:35 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Xie", "Jianwen", ""], ["Douglas", "Pamela K.", ""], ["Wu", "Ying Nian", ""], ["Brody", "Arthur L.", ""], ["Anderson", "Ariana E.", ""]]}, {"id": "1607.00455", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Robert Keynto, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by Adaptation of 3D Convolutional\n  Network", "comments": "This paper is accepted for publication at IEEE ICIP 2016 conference", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532332", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer\\{'}s disease (AD), is based on classification of\nfeatures extracted from brain images. The features have to accurately capture\nmain AD-related variations of anatomical brain structures, such as, e.g.,\nventricles size, hippocampus shape, cortical thickness, and brain volume. This\npaper proposed to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\nCADDementia MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy. Abilities of\nthe 3D-CNN to generalize the features learnt and adapt to other domains have\nbeen validated on the ADNI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 02:55:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Keynto", "Robert", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00516", "submitter": "Mahmoud Hassan", "authors": "J. Rizkallah, P. Benquet, F. Wendling, M. Khalil, A. Mheich, O. Dufor,\n  M. Hassan", "title": "Brain network modules of meaningful and meaningless objects", "comments": "The 3rd Middle East Conference on Biomedical Engineering (MECBME'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network modularity is a key feature for efficient information processing in\nthe human brain. This information processing is however dynamic and networks\ncan reconfigure at very short time period, few hundreds of millisecond. This\nrequires neuroimaging techniques with sufficient time resolution. Here we use\nthe dense electroencephalography, EEG, source connectivity methods to identify\ncortical networks with excellent time resolution, in the order of millisecond.\nWe identify functional networks during picture naming task. Two categories of\nvisual stimuli were presented, meaningful (tools, animals) and meaningless\n(scrambled) objects.\n  In this paper, we report the reconfiguration of brain network modularity for\nmeaningful and meaningless objects. Results showed mainly that networks of\nmeaningful objects were more modular than those of meaningless objects.\nNetworks of the ventral visual pathway were activated in both cases. However a\nstrong occipitotemporal functional connectivity appeared for meaningful object\nbut not for meaningless object. We believe that this approach will give new\ninsights into the dynamic behavior of the brain networks during fast\ninformation processing.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 14:44:50 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Rizkallah", "J.", ""], ["Benquet", "P.", ""], ["Wendling", "F.", ""], ["Khalil", "M.", ""], ["Mheich", "A.", ""], ["Dufor", "O.", ""], ["Hassan", "M.", ""]]}, {"id": "1607.00556", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Georgy Gimel'farb, Ayman El-Baz", "title": "Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis, playing an important role in preventing progress and\ntreating the Alzheimer's disease (AD), is based on classification of features\nextracted from brain images. The features have to accurately capture main\nAD-related variations of anatomical brain structures, such as, e.g., ventricles\nsize, hippocampus shape, cortical thickness, and brain volume. This paper\nproposes to predict the AD with a deep 3D convolutional neural network\n(3D-CNN), which can learn generic features capturing AD biomarkers and adapt to\ndifferent domain datasets. The 3D-CNN is built upon a 3D convolutional\nautoencoder, which is pre-trained to capture anatomical shape variations in\nstructural brain MRI scans. Fully connected upper layers of the 3D-CNN are then\nfine-tuned for each task-specific AD classification. Experiments on the\n\\emph{ADNI} MRI dataset with no skull-stripping preprocessing have shown our\n3D-CNN outperforms several conventional classifiers by accuracy and robustness.\nAbilities of the 3D-CNN to generalize the features learnt and adapt to other\ndomains have been validated on the \\emph{CADDementia} dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 19:55:56 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Gimel'farb", "Georgy", ""], ["El-Baz", "Ayman", ""]]}, {"id": "1607.00697", "submitter": "Nora Youngs", "authors": "Elizabeth Gross, Nida Kazi Obatake, Nora Youngs", "title": "Neural ideals and stimulus space visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.GR math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural code $\\mathcal{C}$ is a collection of binary vectors of a given\nlength n that record the co-firing patterns of a set of neurons. Our focus is\non neural codes arising from place cells, neurons that respond to geographic\nstimulus. In this setting, the stimulus space can be visualized as subset of\n$\\mathbb{R}^2$ covered by a collection $\\mathcal{U}$ of convex sets such that\nthe arrangement $\\mathcal{U}$ forms an Euler diagram for $\\mathcal{C}$. There\nare some methods to determine whether such a convex realization $\\mathcal{U}$\nexists; however, these methods do not describe how to draw a realization. In\nthis work, we look at the problem of algorithmically drawing Euler diagrams for\nneural codes using two polynomial ideals: the neural ideal, a pseudo-monomial\nideal; and the neural toric ideal, a binomial ideal. In particular, we study\nhow these objects are related to the theory of piercings in information\nvisualization, and we show how minimal generating sets of the ideals reveal\nwhether or not a code is $0$, $1$, or $2$-inductively pierced.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 22:54:11 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Gross", "Elizabeth", ""], ["Obatake", "Nida Kazi", ""], ["Youngs", "Nora", ""]]}, {"id": "1607.00952", "submitter": "Mahmoud Hassan", "authors": "Aya Kabbara, Wassim El Falou, Mohamad Khalil, Fabrice Wendling and\n  Mahmoud Hassan", "title": "Graph analysis of spontaneous brain network using EEG source\n  connectivity", "comments": "International Conference on Bio-engineering for Smart Technologies\n  (BioSMART 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the human brain networks during rest is a topic of great interest.\nSeveral structural and functional studies have previously been conducted to\nstudy the intrinsic brain networks. In this paper, we focus on investigating\nthe human brain network topology using dense Electroencephalography (EEG)\nsource connectivity approach. We applied graph theoretical methods on\nfunctional networks reconstructed from resting state data acquired using EEG in\n14 healthy subjects. Our findings confirmed the existence of sets of brain\nregions considered as functional hubs. In particular, the isthmus cingulate and\nthe orbitofrontal regions reveal high levels of integration. Results also\nemphasize on the critical role of the default mode network (DMN) in enabling an\nefficient communication between brain regions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 16:38:16 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kabbara", "Aya", ""], ["Falou", "Wassim El", ""], ["Khalil", "Mohamad", ""], ["Wendling", "Fabrice", ""], ["Hassan", "Mahmoud", ""]]}, {"id": "1607.00969", "submitter": "Eleonora Russo", "authors": "Eleonora Russo and Daniel Durstewitz", "title": "Cell assemblies at multiple time scales with arbitrary lag\n  constellations", "comments": null, "journal-ref": "eLife 2017;6:e19428", "doi": "10.7554/eLife.19428", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hebb's idea of a cell assembly as the fundamental unit of neural information\nprocessing has dominated neuroscience like no other theoretical concept within\nthe past 60 years. A range of different physiological phenomena, from precisely\nsynchronized spiking to broadly simultaneous rate increases, has been subsumed\nunder this term. Yet progress in this area is hampered by the lack of\nstatistical tools that would enable to extract assemblies with arbitrary\nconstellations of time lags, and at multiple temporal scales, partly due to the\nsevere computational burden. Here we present such a unifying methodological and\nconceptual framework which detects assembly structure at many different time\nscales, levels of precision, and with arbitrary internal organization. Applying\nthis methodology to multiple single unit recordings from various cortical\nareas, we find that there is no universal cortical coding scheme, but that\nassembly structure and precision significantly depends on brain area recorded\nand ongoing task demands.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:35:35 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 14:24:54 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Russo", "Eleonora", ""], ["Durstewitz", "Daniel", ""]]}, {"id": "1607.01010", "submitter": "Evelyn Tang", "authors": "Evelyn Tang, Chad Giusti, Graham Baum, Shi Gu, Eli Pollock, Ari E.\n  Kahn, David Roalf, Tyler M. Moore, Kosha Ruparel, Ruben C. Gur, Raquel E.\n  Gur, Theodore D. Satterthwaite and Danielle S. Bassett", "title": "Developmental increases in white matter network controllability support\n  a growing diversity of brain dynamics", "comments": "In press at Nature Communications", "journal-ref": "Nature Communications, 1252 (2017)", "doi": "10.1038/s41467-017-01254-4", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.CD q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the human brain develops, it increasingly supports coordinated control of\nneural activity. The mechanism by which white matter evolves to support this\ncoordination is not well understood. We use a network representation of\ndiffusion imaging data from 882 youth ages 8 to 22 to show that white matter\nconnectivity becomes increasingly optimized for a diverse range of predicted\ndynamics in development. Notably, stable controllers in subcortical areas are\nnegatively related to cognitive performance. Investigating structural\nmechanisms supporting these changes, we simulate network evolution with a set\nof growth rules. We find that all brain networks are structured in a manner\nhighly optimized for network control, with distinct control mechanisms\npredicted in child versus older youth. We demonstrate that our results cannot\nbe simply explained by changes in network modularity. This work reveals a\npossible mechanism of human brain development that preferentially optimizes\ndynamic network control over static network architecture.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:00:00 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 04:03:51 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 20:34:39 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Tang", "Evelyn", ""], ["Giusti", "Chad", ""], ["Baum", "Graham", ""], ["Gu", "Shi", ""], ["Pollock", "Eli", ""], ["Kahn", "Ari E.", ""], ["Roalf", "David", ""], ["Moore", "Tyler M.", ""], ["Ruparel", "Kosha", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1607.01029", "submitter": "Kieran Fox", "authors": "Kieran C.R. Fox, Jessica R. Andrews-Hanna, Kalina Christoff", "title": "The neurobiology of self-generated thought from cells to systems:\n  Integrating evidence from lesion studies, human intracranial\n  electrophysiology, neurochemistry, and neuroendocrinology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the neural basis of self-generated thought is moving beyond\na simple identification with default network activation toward a more\ncomprehensive view recognizing the role of the frontoparietal control network\nand other areas. A major task ahead is to unravel the functional roles and\ntemporal dynamics of the widely distributed brain regions recruited during\nself-generated thought. We argue that various other neuroscientific methods -\nincluding lesion studies, human intracranial electrophysiology, and\nmanipulation of neurochemistry - have much to contribute to this project. These\ndiverse data have yet to be synthesized with the growing understanding of\nself-generated thought gained from neuroimaging, however. Here, we highlight\nseveral areas of ongoing inquiry and illustrate how evidence from other\nmethodologies corroborates, complements, and clarifies findings from functional\nneuroimaging. Each methodology has particular strengths: functional\nneuroimaging reveals much about the variety of brain areas and networks\nreliably recruited. Lesion studies point to regions critical to generating and\nconsciously experiencing self-generated thought. Human intracranial\nelectrophysiology illuminates how and where in the brain thought is generated\nand where this activity subsequently spreads. Finally, measurement and\nmanipulation of neurotransmitter and hormone levels can clarify what kind of\nneurochemical milieu drives or facilitates self-generated cognition.\nIntegrating evidence from multiple complementary modalities will be a critical\nstep on the way to improving our understanding of the neurobiology of\nfunctional and dysfunctional forms of self-generated thought.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:03:29 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Fox", "Kieran C. R.", ""], ["Andrews-Hanna", "Jessica R.", ""], ["Christoff", "Kalina", ""]]}, {"id": "1607.01706", "submitter": "Shi Gu", "authors": "Shi Gu, Richard F. Betzel, Matthew Cieslak, Philip R. Delio, Scott T.\n  Grafton, Fabio Pasqualetti, Danielle S. Bassett", "title": "Optimal Trajectories of Brain State Transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of neural dynamics stems in part from the complexity of the\nunderlying anatomy. Yet how the organization of white matter architecture\nconstrains how the brain transitions from one cognitive state to another\nremains unknown. Here we address this question from a computational perspective\nby defining a brain state as a pattern of activity across brain regions.\nDrawing on recent advances in network control theory, we model the underlying\nmechanisms of brain state transitions as elicited by the collective control of\nregion sets. Specifically, we examine how the brain moves from a specified\ninitial state (characterized by high activity in the default mode) to a\nspecified target state (characterized by high activity in primary sensorimotor\ncortex) in finite time. Across all state transitions, we observe that the\nsupramarginal gyrus and the inferior parietal lobule consistently acted as\nefficient, low energy control hubs, consistent with their strong anatomical\nconnections to key input areas of sensorimotor cortex. Importantly, both these\nand other regions in the fronto-parietal, cingulo-opercular, and attention\nsystems are poised to affect a broad array of state transitions that cannot\neasily be classified by traditional notions of control common in the\nengineering literature. This theoretical versatility comes with a vulnerability\nto injury. In patients with mild traumatic brain injury, we observe a loss of\nspecificity in putative control processes, suggesting greater susceptibility to\ndamage-induced noise in neurophysiological activity. These results offer\nfundamentally new insights into the mechanisms driving brain state transitions\nin healthy cognition and their alteration following injury.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 16:57:46 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 02:33:35 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 04:18:23 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Gu", "Shi", ""], ["Betzel", "Richard F.", ""], ["Cieslak", "Matthew", ""], ["Delio", "Philip R.", ""], ["Grafton", "Scott T.", ""], ["Pasqualetti", "Fabio", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1607.01746", "submitter": "Tim Kiemel", "authors": "Tim Kiemel, David Logan, John J. Jeka", "title": "Using Perturbations to Probe the Neural Control of Rhythmic Movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small continuous sensory and mechanical perturbations have often been used to\nidentify properties of the closed-loop neural control of posture and other\nsystems that are approximately linear time invariant. Here we extend this\napproach to study the neural control of rhythmic behaviors such as walking. Our\nmethod is based on the theory of linear time periodic systems, with\nmodifications to account for ability of perturbations to reset the phase of a\nrhythmic behavior. We characterize responses to perturbations in the frequency\ndomain using harmonic transfer functions and then convert to the time domain to\nobtain phase-dependent impulse response functions (IRFs) that describe the\nresponse to a small brief perturbation at any phase of the rhythmic behavior.\nIRFs describing responses of kinematic variables and muscle activations\n(measured by EMG) to sensory and mechanical perturbations can be used to infer\nproperties of the plant, the mapping from muscle activation to movement, and of\nneural feedback, the mapping from movement to muscle activation. We illustrate\nour method by applying it to simulated data from a model and experimental data\nof subjects walking on a treadmill perturbed by movement of the visual scene.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 19:08:50 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 20:32:37 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Kiemel", "Tim", ""], ["Logan", "David", ""], ["Jeka", "John J.", ""]]}, {"id": "1607.01959", "submitter": "Shi Gu", "authors": "Shi Gu, Matthew Cieslak, Benjamin Baird, Sarah F. Muldoon, Scott T.\n  Grafton, Fabio Pasqualetti, Danielle S. Bassett", "title": "The Energy Landscape of Neurophysiological Activity Implicit in Brain\n  Network Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical mystery in neuroscience lies in determining how anatomical\nstructure impacts the complex functional dynamics of human thought. How does\nlarge-scale brain circuitry constrain states of neuronal activity and\ntransitions between those states? We address these questions using a maximum\nentropy model of brain dynamics informed by white matter tractography. We\ndemonstrate that the most probable brain states -- characterized by minimal\nenergy -- display common activation profiles across brain areas: local\nspatially-contiguous sets of brain regions reminiscent of cognitive systems are\nco-activated frequently. The predicted activation rate of these systems is\nhighly correlated with the observed activation rate measured in a separate\nresting state fMRI data set, validating the utility of the maximum entropy\nmodel in describing neurophysiologial dynamics. This approach also offers a\nformal notion of the energy of activity within a system, and the energy of\nactivity shared between systems. We observe that within- and between-system\nenergies cleanly separate cognitive systems into distinct categories, optimized\nfor differential contributions to integrated v.s. segregated function. These\nresults support the notion that energetic and structural constraints\ncircumscribe brain dynamics, offering novel insights into the roles that\ncognitive systems play in driving whole-brain activation patterns.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 10:54:53 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Gu", "Shi", ""], ["Cieslak", "Matthew", ""], ["Baird", "Benjamin", ""], ["Muldoon", "Sarah F.", ""], ["Grafton", "Scott T.", ""], ["Pasqualetti", "Fabio", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1607.02695", "submitter": "Tomasz Rutkowski", "authors": "Jair Pereira Junior, Caio Teixeira, and Tomasz M. Rutkowski", "title": "Visual Motion Onset Brain-computer Interface", "comments": "10 pages, 5 figures, submitted to a journal MDPI Computers. arXiv\n  admin note: text overlap with arXiv:1506.04458", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper presents a study of two novel visual motion onset stimulus-based\nbrain-computer interfaces (vmoBCI). Two settings are compared with afferent and\nefferent to a computer screen center motion patterns. Online vmoBCI experiments\nare conducted in an oddball event-related potential (ERP) paradigm allowing for\n\"aha-responses\" decoding in EEG brainwaves. A subsequent stepwise linear\ndiscriminant analysis classification (swLDA) classification accuracy comparison\nis discussed based on two inter-stimulus-interval (ISI) settings of 700 and 150\nms in two online vmoBCI applications with six and eight command settings. A\nresearch hypothesis of classification accuracy non-significant differences with\nvarious ISIs is confirmed based on the two settings of 700 ms and 150 ms, as\nwell as with various numbers of ERP response averaging scenarios. The efferent\nin respect to display center visual motion patterns allowed for a faster\ninterfacing and thus they are recommended as more suitable for the\nno-eye-movements requiring visual BCIs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 05:45:55 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 08:26:59 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Junior", "Jair Pereira", ""], ["Teixeira", "Caio", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1607.02865", "submitter": "Rajani Raman", "authors": "Rajani Raman and Sandip Sarkar", "title": "Significance of Natural Scene Statistics in Understanding the\n  Anisotropies of Perceptual Filling-in at the Blind Spot", "comments": "20 page, 9 figures, Abstract changed, Discussions changed, Additional\n  results with two additional figures, Major Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychophysical experiments reveal our horizontal preference in perceptual\nfilling-in at the blind spot. On the other hand, vertical preference is\nexhibited in the case of tolerance in filling-in. What causes this anisotropy\nin our perception? Building upon the general notion, that the functional\nproperties of the early visual system are shaped by the innate specification as\nwell as the statistics of the environment, we reasoned that the anisotropy in\nfilling-in could be understood in terms of anisotropy in orientation\ndistribution inherent to natural scene statistics. We examined this proposition\nby investigating filling-in of bar stimuli on a Hierarchical Predictive Coding\nmodel network. In response to bar stimuli, the model network, trained with\nnatural images, exhibited anisotropic filling-in performance at the blind spot\nsimilar to reported in psychophysical experiments i.e. horizontal preference in\nfilling-in and vertical preference in tolerance of filling-in. We suggest that\nthe over-representation of horizontal contours in the natural scene contribute\nto the observed horizontal superiority while the broader distribution of\nvertical contours contributes to the observed vertical superiority in\ntolerance. These results indicate that natural scene statistics plays a\nsignificant role in determining the filling-in performance at the blind spot\nand shaping the associated anisotropies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 09:06:30 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 06:53:59 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Raman", "Rajani", ""], ["Sarkar", "Sandip", ""]]}, {"id": "1607.02969", "submitter": "Christoph Simon", "authors": "Sourabh Kumar, Kristine Boone, Jack Tuszynski, Paul E. Barclay,\n  Christoph Simon", "title": "Possible existence of optical communication channels in the brain", "comments": "13+11 pages, 5+7 figures", "journal-ref": "Scientific Reports 6, 36508 (2016)", "doi": "10.1038/srep36508", "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.optics quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that many fundamental questions in neuroscience are still open, it\nseems pertinent to explore whether the brain might use other physical\nmodalities than the ones that have been discovered so far. In particular it is\nwell established that neurons can emit photons, which prompts the question\nwhether these biophotons could serve as signals between neurons, in addition to\nthe well-known electro-chemical signals. For such communication to be targeted,\nthe photons would need to travel in waveguides. Here we show, based on detailed\ntheoretical modeling, that myelinated axons could serve as photonic waveguides,\ntaking into account realistic optical imperfections. We propose experiments,\nboth \\textit{in vivo} and \\textit{in vitro}, to test our hypothesis. We discuss\nthe implications of our results, including the question whether photons could\nmediate long-range quantum entanglement in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 19:37:52 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Kumar", "Sourabh", ""], ["Boone", "Kristine", ""], ["Tuszynski", "Jack", ""], ["Barclay", "Paul E.", ""], ["Simon", "Christoph", ""]]}, {"id": "1607.03502", "submitter": "Manuel J. A. Eugster", "authors": "Manuel J. A. Eugster, Tuukka Ruotsalo, Michiel M. Spap\\'e, Oswald\n  Barral, Niklas Ravaja, Giulio Jacucci, Samuel Kaski", "title": "Natural brain-information interfaces: Recommending information by\n  relevance inferred from human brain signals", "comments": null, "journal-ref": "Scientific Reports 6, Article number: 38580 (2016)", "doi": "10.1038/srep38580", "report-no": null, "categories": "cs.IR cs.HC q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding relevant information from large document collections such as the\nWorld Wide Web is a common task in our daily lives. Estimation of a user's\ninterest or search intention is necessary to recommend and retrieve relevant\ninformation from these collections. We introduce a brain-information interface\nused for recommending information by relevance inferred directly from brain\nsignals. In experiments, participants were asked to read Wikipedia documents\nabout a selection of topics while their EEG was recorded. Based on the\nprediction of word relevance, the individual's search intent was modeled and\nsuccessfully used for retrieving new, relevant documents from the whole English\nWikipedia corpus. The results show that the users' interests towards digital\ncontent can be modeled from the brain signals evoked by reading. The introduced\nbrain-relevance paradigm enables the recommendation of information without any\nexplicit user interaction, and may be applied across diverse\ninformation-intensive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:17:00 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Eugster", "Manuel J. A.", ""], ["Ruotsalo", "Tuukka", ""], ["Spap\u00e9", "Michiel M.", ""], ["Barral", "Oswald", ""], ["Ravaja", "Niklas", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1607.03687", "submitter": "Daniele Marinazzo", "authors": "Frederik van de Steen, Luca Faes, Esin Karahan, Jitkomut Songsiri,\n  Pedro Antonio Valdes Sosa, Daniele Marinazzo", "title": "Critical comments on EEG sensor space dynamical connectivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different analysis techniques have been developed and applied to EEG\nrecordings that allow one to investigate how different brain areas interact.\nOne particular class of methods, based on the linear parametric representation\nof multiple interacting time series, is widely used to study causal\nconnectivity in the brain. However, the results obtained by these methods\nshould be interpreted with great care. The goal of this paper is to show, both\ntheoretically and using simulations, that results obtained by applying causal\nconnectivity measures on the sensor (scalp) time series do not allow\ninterpretation in terms of interacting brain sources. This is because 1) the\nchannel locations cannot be seen as an approximation of a source's anatomical\nlocation and 2) spurious connectivity can occur between sensors. Although many\nmeasures of causal connectivity derived from EEG sensor time series are\naffected by the latter, here we will focus on the well-known time domain index\nof Granger causality (GC) and on the frequency domain directed transfer\nfunction (DTF). Using the state-space framework and designing two simulation\nstudies we show that mixing effects caused by volume conduction can lead to\nspurious connections, detected either by time domain GC or by DTF. Therefore,\nGC/DTF causal connectivity measures should be computed at the source level, or\nderived within analysis frameworks that model the effects of volume conduction.\nSince mixing effects can also occur in the source space, it is advised to\ncombine source space analysis with connectivity measures that are robust to\nmixing.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:54:08 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 13:18:45 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["van de Steen", "Frederik", ""], ["Faes", "Luca", ""], ["Karahan", "Esin", ""], ["Songsiri", "Jitkomut", ""], ["Sosa", "Pedro Antonio Valdes", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1607.04052", "submitter": "Alessandro Torcini Dr", "authors": "S. Luccioli, A. Barzilai, E. Ben-Jacob, P. Bonifazi, and A. Torcini", "title": "Functional Cliques in Developmentally Correlated Neural Networks", "comments": "12 pages, 4 figure, contribution for the Workshop \"Nonlinear Dynamics\n  in Computational Neuroscience: from Physics and Biology to ICT\" held in Turin\n  (Italy) in September 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse random network of excitatory leaky integrate-and-fire\nneurons with short-term synaptic depression. Furthermore to mimic the dynamics\nof a brain circuit in its first stages of development we introduce for each\nneuron correlations among in-degree and out-degree as well as among\nexcitability and the corresponding total degree, We analyze the influence of\nsingle neuron stimulation and deletion on the collective dynamics of the\nnetwork. We show the existence of a small group of neurons capable of\ncontrolling and even silencing the bursting activity of the network. These\nneurons form a functional clique since only their activation in a precise order\nand within specific time windows is capable to ignite population bursts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 09:35:54 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 15:16:31 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Luccioli", "S.", ""], ["Barzilai", "A.", ""], ["Ben-Jacob", "E.", ""], ["Bonifazi", "P.", ""], ["Torcini", "A.", ""]]}, {"id": "1607.04169", "submitter": "Marcelo Mattar", "authors": "Marcelo G Mattar and Sharon L Thompson-Schill and Danielle S Bassett", "title": "The network architecture of value learning", "comments": "27 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value guides behavior. With knowledge of stimulus values and action\nconsequences, behaviors that maximize expected reward can be selected. Prior\nwork has identified several brain structures critical for representing both\nstimuli and their values. Yet, it remains unclear how these structures interact\nwith one another and with other regions of the brain to support the dynamic\nacquisition of value-related knowledge. Here, we use a network neuroscience\napproach to examine how BOLD functional networks change as 20 healthy human\nsubjects learn the values of novel visual stimuli over the course of four\nconsecutive days. We show that connections between regions of the visual,\nfrontal, and cingulate cortices become increasingly stronger as learning\nprogresses, and that these changes are primarily confined to the temporal core\nof the network. These results demonstrate that functional networks dynamically\ntrack behavioral improvement in value judgments, and that interactions between\nnetwork communities form predictive biomarkers of learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 15:40:49 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Mattar", "Marcelo G", ""], ["Thompson-Schill", "Sharon L", ""], ["Bassett", "Danielle S", ""]]}, {"id": "1607.04331", "submitter": "Subhaneil Lahiri", "authors": "Subhaneil Lahiri, Peiran Gao, Surya Ganguli", "title": "Random projections of random manifolds", "comments": "45 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interesting data often concentrate on low dimensional smooth manifolds inside\na high dimensional ambient space. Random projections are a simple, powerful\ntool for dimensionality reduction of such data. Previous works have studied\nbounds on how many projections are needed to accurately preserve the geometry\nof these manifolds, given their intrinsic dimensionality, volume and curvature.\nHowever, such works employ definitions of volume and curvature that are\ninherently difficult to compute. Therefore such theory cannot be easily tested\nagainst numerical simulations to understand the tightness of the proven bounds.\nWe instead study typical distortions arising in random projections of an\nensemble of smooth Gaussian random manifolds. We find explicitly computable,\napproximate theoretical bounds on the number of projections required to\naccurately preserve the geometry of these manifolds. Our bounds, while\napproximate, can only be violated with a probability that is exponentially\nsmall in the ambient dimension, and therefore they hold with high probability\nin cases of practical interest. Moreover, unlike previous work, we test our\ntheoretical bounds against numerical experiments on the actual geometric\ndistortions that typically occur for random projections of random smooth\nmanifolds. We find our bounds are tighter than previous results by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 02:37:47 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Lahiri", "Subhaneil", ""], ["Gao", "Peiran", ""], ["Ganguli", "Surya", ""]]}, {"id": "1607.04618", "submitter": "Kelly Iarosz", "authors": "Fernando da Silva Borges, Paulo Ricardo Protachevicz, Ewandson Luiz\n  Lameu, Robson Conrado Bonetti, Kelly Cristiane Iarosz, Iber\\^e Luiz Caldas,\n  Murilo da Silva Baptista, Antonio Marcos Batista", "title": "Synchronised firing patterns in a random network of adaptive exponential\n  integrate-and-fire", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have studied neuronal synchronisation in a random network of adaptive\nexponential integrate-and-fire neurons. We study how spiking or bursting\nsynchronous behaviour appears as a function of the coupling strength and the\nprobability of connections, by constructing parameter spaces that identify\nthese synchronous behaviours from measurements of the inter-spike interval and\nthe calculation of the order parameter. Moreover, we verify the robustness of\nsynchronisaton by applying an external perturbation to each neuron. The\nsimulations show that bursting synchronisation is more robust than spike\nsynchronisation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 19:19:30 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Borges", "Fernando da Silva", ""], ["Protachevicz", "Paulo Ricardo", ""], ["Lameu", "Ewandson Luiz", ""], ["Bonetti", "Robson Conrado", ""], ["Iarosz", "Kelly Cristiane", ""], ["Caldas", "Iber\u00ea Luiz", ""], ["Baptista", "Murilo da Silva", ""], ["Batista", "Antonio Marcos", ""]]}, {"id": "1607.04886", "submitter": "Marc Howard", "authors": "Marc W. Howard and Karthik H. Shankar", "title": "Neural scaling laws for an uncertain world", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous neural systems must efficiently process information in a wide\nrange of novel environments, which may have very different statistical\nproperties. We consider the problem of how to optimally distribute receptors\nalong a one-dimensional continuum consistent with the following design\nprinciples. First, neural representations of the world should obey a neural\nuncertainty principle---making as few assumptions as possible about the\nstatistical structure of the world. Second, neural representations should\nconvey, as much as possible, equivalent information about environments with\ndifferent statistics. The results of these arguments resemble the structure of\nthe visual system and provide a natural explanation of the behavioral\nWeber-Fechner law, a foundational result in psychology. Because the derivation\nis extremely general, this suggests that similar scaling relationships should\nbe observed not only in sensory continua, but also in neural representations of\n``cognitive' one-dimensional quantities such as time or numerosity.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 15:59:50 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 04:13:42 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Howard", "Marc W.", ""], ["Shankar", "Karthik H.", ""]]}, {"id": "1607.05334", "submitter": "Arash Khodadadi", "authors": "Arash Khodadadi, Pegah Fakhari, Jerome R. Busemeyer", "title": "Learning to Allocate Limited Time to Decisions with Different Expected\n  Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this article is to investigate how human participants allocate\ntheir limited time to decisions with different properties. We report the\nresults of two behavioral experiments. In each trial of the experiments, the\nparticipant must accumulate noisy information to make a decision. The\nparticipants received positive and negative rewards for their correct and\nincorrect decisions, respectively. The stimulus was designed such that\ndecisions based on more accumulated information were more accurate but took\nlonger. Therefore, the total outcome that a participant could achieve during\nthe limited experiments' time depended on her \"decision threshold\", the amount\nof information she needed to make a decision. In the first experiment, two\ntypes of trials were intermixed randomly: hard and easy. Crucially, the hard\ntrials were associated with smaller positive and negative rewards than the easy\ntrials. A cue presented at the beginning of each trial would indicate the type\nof the upcoming trial. The optimal strategy was to adopt a small decision\nthreshold for hard trials. The results showed that several of the participants\ndid not learn this simple strategy. We then investigated how the participants\nadjusted their decision threshold based on the feedback they received in each\ntrial. To this end, we developed and compared 10 computational models for\nadjusting the decision threshold. The models differ in their assumptions on the\nshape of the decision thresholds and the way the feedback is used to adjust the\ndecision thresholds. The results of Bayesian model comparison showed that a\nmodel with time-varying thresholds whose parameters are updated by a\nreinforcement learning algorithm is the most likely model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 21:32:40 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Khodadadi", "Arash", ""], ["Fakhari", "Pegah", ""], ["Busemeyer", "Jerome R.", ""]]}, {"id": "1607.05530", "submitter": "Francesco Fumarola", "authors": "Francesco Fumarola", "title": "Verbal Perception and the Word Length Effect", "comments": "39 pages, 13 figures; data analysis included", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A theoretical framework is proposed for the understanding of verbal\nperception -- the conversion of words into meaning, modeled as a compromise\nbetween lexical demands and contextual constraints -- and the theory is tested\nagainst experiments on short-term memory. The observation that lists of short\nwords are recalled better than lists of long ones has been a long-standing\nsubject of controversy, further complicated by the apparent inversion of the\neffect for mixed lists. In the framework here proposed, these behaviors emerge\nas an effect of the different level of localization of short and long words in\nsemantic space. Events corresponding to the recognition of a nonlocal word have\na clustering property in phase space, which facilitates associative retrieval.\nThe standard word-length effect arises directly from this property, and the\ninverse effect from its breakdown. An analysis of data from the PEERS\nexperiments (Healey and Kahana, 2016) confirms the main predictions of the\ntheory. Further predictions are listed and new experiments are proposed.\nFinally, an interpretation of the above results is presented.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 11:38:19 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 21:58:12 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Fumarola", "Francesco", ""]]}, {"id": "1607.06133", "submitter": "Eugene Pechersky", "authors": "Eugene Pechersky, Guillem Via, Anatoly Yambartsev", "title": "Stochastic Ising model with plastic interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model based on the Ising model with the aim to study\nsynaptic plasticity phenomena in neural networks. It is today well established\nin biology that the synapses or connections between certain types of neurons\nare strengthened when the neurons are co-active, a form of the so called\nsynaptic plasticity. Such mechanism is believed to mediate the formation and\nmaintenance of memories. The proposed model describes some features from that\nphenomenon. Together with the spin-flip dynamics, in our model the coupling\nconstants are also subject to stochastic dynamics, so that they interact with\neach other. The evolution of the system is described by a continuous-time\nMarkov jump process.\n  Keyword Markov chain, Stochastic Ising model, synaptic plasticity, neural\nnetworks, transience\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 21:38:38 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Pechersky", "Eugene", ""], ["Via", "Guillem", ""], ["Yambartsev", "Anatoly", ""]]}, {"id": "1607.06251", "submitter": "\\'Aine Byrne", "authors": "Stephen Coombes and \\'Aine Byrne", "title": "Next generation neural mass models", "comments": "Contribution to the Workshop \"Nonlinear Dynamics in Computational\n  Neuroscience: from Physics and Biology to ICT\" held in Turin (Italy) in\n  September 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural mass models have been actively used since the 1970s to model the\ncoarse grained activity of large populations of neurons and synapses. They have\nproven especially useful in understanding brain rhythms. However, although\nmotivated by neurobiological considerations they are phenomenological in\nnature, and cannot hope to recreate some of the rich repertoire of responses\nseen in real neuronal tissue. In this chapter we consider the $\\theta$-neuron\nmodel that has recently been shown to admit to an exact mean-field description\nfor instantaneous pulsatile interactions. We show that the inclusion of a more\nrealistic synapse model leads to a mean-field model that has many of the\nfeatures of a neural mass model coupled to a further dynamical equation that\ndescribes the evolution of network synchrony. A bifurcation analysis is used to\nuncover the primary mechanism for generating oscillations at the single and two\npopulation level. Numerical simulations also show that the phenomena of event\nrelated synchronisation and desynchronisation are easily realised. Importantly\nunlike its phenomenological counterpart this \\textit{next generation neural\nmass model} is an exact macroscopic description of an underlying microscopic\nspiking neurodynamics, and is a natural candidate for use in future large scale\nhuman brain simulations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 10:09:04 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 14:46:30 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Coombes", "Stephen", ""], ["Byrne", "\u00c1ine", ""]]}, {"id": "1607.06563", "submitter": "Kseniia Kravchuk", "authors": "K. Kravchuk", "title": "Leaky Integrate-and-Fire Neuron under Poisson Stimulation", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single Leaky integrate-and-fire neuron stimulated with Poisson\nprocess. We develop a method, which allows one to obtain the first passage time\nprobability density function without any additional approximations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 06:06:46 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Kravchuk", "K.", ""]]}, {"id": "1607.06754", "submitter": "Mahmoud Hassan", "authors": "N. Nader, M. Hassan, W. Falou, C. Marque and M. Khalil", "title": "A node-wise analysis of the uterine muscle networks for pregnancy\n  monitoring", "comments": "4 pages, 3 figures, accepted in the IEEE EMBC conferance", "journal-ref": null, "doi": "10.1109/EMBC.2016.7590801", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent past years have seen a noticeable increase of interest in the\ncorrelation analysis of electrohysterographic (EHG) signals in the perspective\nof improving the pregnancy monitoring. Here we propose a new approach based on\nthe functional connectivity between multichannel (4x4 matrix) EHG signals\nrecorded from the women abdomen. The proposed pipeline includes i) the\ncomputation of the statistical couplings between the multichannel EHG signals,\nii) the characterization of the connectivity matrices, computed by using the\nimaginary part of the coherence, based on the graph-theory analysis and iii)\nthe use of these measures for pregnancy monitoring. The method was evaluated on\na dataset of EHGs, in order to track the correlation between EHGs collected by\neach electrode of the matrix (called node-wise analysis) and follow their\nevolution along weeks before labor. Results showed that the strength of each\nnode significantly increases from pregnancy to labor. Electrodes located on the\nmedian vertical axis of the uterus seemed to be the more discriminant. We\nspeculate that the network-based analysis can be a very promising tool to\nimprove pregnancy monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 09:28:00 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nader", "N.", ""], ["Hassan", "M.", ""], ["Falou", "W.", ""], ["Marque", "C.", ""], ["Khalil", "M.", ""]]}, {"id": "1607.06980", "submitter": "Siddharth Mehrotra", "authors": "Siddharth Mehrotra, Anuj Shukla, Dipanjan Roy", "title": "Neurophysiological Investigation of Context Modulation based on Musical\n  Stimulus", "comments": null, "journal-ref": "International Conference On Music Perception And Cognition. San\n  Francisco, CA: ICMPC, 2016. 243-246", "doi": "10.13140/RG.2.1.3091.3524", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous studies which suggest that perhaps music is truly the\nlanguage of emotions. Music seems to have an almost willful, evasive quality,\ndefying simple explanation, and indeed requires deeper neurophysiological\ninvestigations to gain a better understanding. The current study makes an\nattempt in that direction to explore the effect of context on music perception.\nTo investigate the same, we measured Galvanic Skin Responses (GSR) and\nself-reported emotion on 18 participants while listening to different Ragas\n(musical stimulus) composed of different Rasa's (emotional expression) in the\ndifferent context (Neutral, Pleasant, and Unpleasant). The IAPS pictures were\nused to induce the emotional context in participants. Our results from this\nstudy suggest that the context can modulate emotional response in music\nperception but only for a shorter time scale. Interestingly, here we\ndemonstrate by combining GSR and self-reports that this effect gradually\nvanishes over time and shows emotional adaptation irrespective of context. The\noverall findings suggest that specific context effects of music perception are\ntransitory in nature and gets saturated on a longer time scale.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 22:55:00 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Mehrotra", "Siddharth", ""], ["Shukla", "Anuj", ""], ["Roy", "Dipanjan", ""]]}, {"id": "1607.07139", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim, Woochang Lim", "title": "Emergence of Sparsely Synchronized Rhythms and Their Responses to\n  External Stimuli in An Inhomogeneous Small-World Complex Neuronal Network", "comments": "arXiv admin note: text overlap with arXiv:1504.03063", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an inhomogeneous small-world network (SWN) composed of inhibitory\nshort-range (SR) and long-range (LR) interneurons. By varying the fraction of\nLR interneurons $p_{long}$, we investigate the effect of network architecture\non emergence of sparsely synchronized rhythms, and make comparison with that in\nthe Watts-Strogatz SWN. Although SR and LR interneurons have the same average\nin- and out-degrees, their betweenness centralities (characterizing the\npotentiality in controlling communication between other interneurons) are\ndistinctly different. Hence, in view of the betweenness, SWNs we consider are\ninhomogeneous, unlike the \"canonical\" Watts-Strogatz SWN with nearly same\nbetweenness centralities. As $p_{long}$ is increased, the average path length\nbecomes shorter, and the load of communication traffic is less concentrated on\nLR interneurons, which leads to better efficiency of global communication\nbetween interneurons. Eventually, when passing a critical value\n$p_{long}^{(c)}$ $(\\simeq 0.16)$, sparsely synchronized rhythms are found to\nemerge. We also consider two cases of external time-periodic stimuli applied to\nsub-groups of LR and SR interneurons, respectively. Dynamical responses (such\nas synchronization suppression and enhancement) to these two cases of stimuli\nare studied and discussed in relation to the betweenness centralities of\nstimulated interneurons, representing the effectiveness for transfer of\nstimulation effect in the whole network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 03:42:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 03:49:00 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "1607.07987", "submitter": "Hosein M. Golshan", "authors": "Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud,\n  Mohammad H. Mahoor", "title": "A Multiple Kernel Learning Approach for Human Behavioral Task\n  Classification using STN-LFP Signal", "comments": "38th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Scociety", "journal-ref": null, "doi": "10.1109/EMBC.2016.7590878", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Brain Stimulation (DBS) has gained increasing attention as an effective\nmethod to mitigate Parkinsons disease (PD) disorders. Existing DBS systems are\nopen-loop such that the system parameters are not adjusted automatically based\non patients behavior. Classification of human behavior is an important step in\nthe design of the next generation of DBS systems that are closed-loop. This\npaper presents a classification approach to recognize such behavioral tasks\nusing the subthalamic nucleus (STN) Local Field Potential (LFP) signals. In our\napproach, we use the time-frequency representation (spectrogram) of the raw LFP\nsignals recorded from left and right STNs as the feature vectors. Then these\nfeatures are combined together via Support Vector Machines (SVM) with Multiple\nKernel Learning (MKL) formulation. The MKL-based classification method is\nutilized to classify different tasks: button press, mouth movement, speech, and\narm movement. Our experiments show that the lp-norm MKL significantly\noutperforms single kernel SVM-based classifiers in classifying behavioral tasks\nof five subjects even using signals acquired with a low sampling rate of 10 Hz.\nThis leads to a lower computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 07:28:40 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Golshan", "Hosein M.", ""], ["Hebb", "Adam O.", ""], ["Hanrahan", "Sara J.", ""], ["Nedrud", "Joshua", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1607.08318", "submitter": "Zachary Kilpatrick PhD", "authors": "Adrian E Radillo, Alan Veliz-Cuba, Kresimir Josic, and Zachary P\n  Kilpatrick", "title": "Evidence accumulation and change rate inference in dynamic environments", "comments": "43 pages, 8 figures, in press", "journal-ref": "Neural Computation (2017)", "doi": null, "report-no": null, "categories": "q-bio.NC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a constantly changing world, animals must account for environmental\nvolatility when making decisions. To appropriately discount older, irrelevant\ninformation, they need to learn the rate at which the environment changes. We\ndevelop an ideal observer model capable of inferring the present state of the\nenvironment along with its rate of change. Key to this computation is an update\nof the posterior probability of all possible changepoint counts. This\ncomputation can be challenging, as the number of possibilities grows rapidly\nwith time. However, we show how the computations can be simplified in the\ncontinuum limit by a moment closure approximation. The resulting\nlow-dimensional system can be used to infer the environmental state and change\nrate with accuracy comparable to the ideal observer. The approximate\ncomputations can be performed by a neural network model via a rate-correlation\nbased plasticity rule. We thus show how optimal observers accumulate evidence\nin changing environments, and map this computation to reduced models which\nperform inference using plausible neural mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:24:55 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 17:46:26 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Radillo", "Adrian E", ""], ["Veliz-Cuba", "Alan", ""], ["Josic", "Kresimir", ""], ["Kilpatrick", "Zachary P", ""]]}, {"id": "1607.08458", "submitter": "Alexandre Gramfort", "authors": "Daniel Strohmeier, Yousra Bekhti, Jens Haueisen, Alexandre Gramfort", "title": "The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG\n  source reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2016.2553445", "report-no": null, "categories": "stat.AP q-bio.NC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source imaging based on magnetoencephalography (MEG) and\nelectroencephalography (EEG) allows for the non-invasive analysis of brain\nactivity with high temporal and good spatial resolution. As the\nbioelectromagnetic inverse problem is ill-posed, constraints are required. For\nthe analysis of evoked brain activity, spatial sparsity of the neuronal\nactivation is a common assumption. It is often taken into account using convex\nconstraints based on the l1-norm. The resulting source estimates are however\nbiased in amplitude and often suboptimal in terms of source selection due to\nhigh correlations in the forward model. In this work, we demonstrate that an\ninverse solver based on a block-separable penalty with a Frobenius norm per\nblock and a l0.5-quasinorm over blocks addresses both of these issues. For\nsolving the resulting non-convex optimization problem, we propose the iterative\nreweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on\niterative reweighted convex surrogate optimization problems, which are solved\nefficiently using a block coordinate descent scheme and an active set strategy.\nWe compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC\napproach based on two MEG data sets. We provide empirical evidence based on\nsimulations and analysis of MEG data that the proposed method improves on the\nstandard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support\nrecovery, and stability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:56:04 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Strohmeier", "Daniel", ""], ["Bekhti", "Yousra", ""], ["Haueisen", "Jens", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1607.08552", "submitter": "Tobias K\\\"uhn", "authors": "Tobias K\\\"uhn, Moritz Helias", "title": "Locking of correlated neural activity to ongoing oscillations", "comments": "57 pages, 12 figures, published version", "journal-ref": "PLoS Comput Biol 13(6): e1005534 (2017)", "doi": "10.1371/journal.pcbi.1005534", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population-wide oscillations are ubiquitously observed in mesoscopic signals\nof cortical activity. In these network states a global oscillatory cycle\nmodulates the propensity of neurons to fire. Synchronous activation of neurons\nhas been hypothesized to be a separate channel of signal processing information\nin the brain. A salient question is therefore if and how oscillations interact\nwith spike synchrony and in how far these channels can be considered separate.\nExperiments indeed showed that correlated spiking co-modulates with the static\nfiring rate and is also tightly locked to the phase of beta-oscillations. While\nthe dependence of correlations on the mean rate is well understood in\nfeed-forward networks, it remains unclear why and by which mechanisms\ncorrelations tightly lock to an oscillatory cycle. We here demonstrate that\nsuch correlated activation of pairs of neurons is qualitatively explained by\nperiodically-driven random networks. We identify the mechanisms by which\ncovariances depend on a driving periodic stimulus. Mean-field theory combined\nwith linear response theory yields closed-form expressions for the\ncyclostationary mean activities and pairwise zero-time-lag covariances of\nbinary recurrent random networks. Two distinct mechanisms cause time-dependent\ncovariances: the modulation of the susceptibility of single neurons (via the\nexternal input and network feedback) and the time-varying variances of single\nunit activities. For some parameters, the effectively inhibitory recurrent\nfeedback leads to resonant covariances even if mean activities show\nnon-resonant behavior. Our analytical results open the question of\ntime-modulated synchronous activity to a quantitative analysis.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 18:09:49 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 09:58:28 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 15:25:12 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["K\u00fchn", "Tobias", ""], ["Helias", "Moritz", ""]]}, {"id": "1607.08840", "submitter": "Christian Donner", "authors": "Christian Donner, Klaus Obermayer, Hideaki Shimazaki", "title": "Approximate Inference for Time-varying Interactions and Macroscopic\n  Dynamics of Neural Populations", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005309", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The models in statistical physics such as an Ising model offer a convenient\nway to characterize stationary activity of neural populations. Such stationary\nactivity of neurons may be expected for recordings from in vitro slices or\nanesthetized animals. However, modeling activity of cortical circuitries of\nawake animals has been more challenging because both spike-rates and\ninteractions can change according to sensory stimulation, behavior, or an\ninternal state of the brain. Previous approaches modeling the dynamics of\nneural interactions suffer from computational cost; therefore, its application\nwas limited to only a dozen neurons. Here by introducing multiple analytic\napproximation methods to a state-space model of neural population activity, we\nmake it possible to estimate dynamic pairwise interactions of up to 60 neurons.\nMore specifically, we applied the pseudolikelihood approximation to the\nstate-space model, and combined it with the Bethe or TAP mean-field\napproximation to make the sequential Bayesian estimation of the model\nparameters possible. The large-scale analysis allows us to investigate dynamics\nof macroscopic properties of neural circuitries underlying stimulus processing\nand behavior. We show that the model accurately estimates dynamics of network\nproperties such as sparseness, entropy, and heat capacity by simulated data,\nand demonstrate utilities of these measures by analyzing activity of monkey V4\nneurons as well as a simulated balanced network of spiking neurons.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 15:03:49 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 16:25:34 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Donner", "Christian", ""], ["Obermayer", "Klaus", ""], ["Shimazaki", "Hideaki", ""]]}, {"id": "1607.08891", "submitter": "Brian Helfer", "authors": "Brian S. Helfer, James R. Williamson, Benjamin A. Miller, Joseph\n  Perricone, Thomas F. Quatieri", "title": "Assessing Functional Neural Connectivity as an Indicator of Cognitive\n  Performance", "comments": "Oral presentation at MLINI 2015 - 5th NIPS Workshop on Machine\n  Learning and Interpretation in Neuroimaging (arXiv:1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/17", "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies in recent years have demonstrated that neural organization and\nstructure impact an individual's ability to perform a given task. Specifically,\nindividuals with greater neural efficiency have been shown to outperform those\nwith less organized functional structure. In this work, we compare the\npredictive ability of properties of neural connectivity on a working memory\ntask. We provide two novel approaches for characterizing functional network\nconnectivity from electroencephalography (EEG), and compare these features to\nthe average power across frequency bands in EEG channels. Our first novel\napproach represents functional connectivity structure through the distribution\nof eigenvalues making up channel coherence matrices in multiple frequency\nbands. Our second approach creates a connectivity network at each frequency\nband, and assesses variability in average path lengths of connected components\nand degree across the network. Failures in digit and sentence recall on single\ntrials are detected using a Gaussian classifier for each feature set, at each\nfrequency band. The classifier results are then fused across frequency bands,\nwith the resulting detection performance summarized using the area under the\nreceiver operating characteristic curve (AUC) statistic. Fused AUC results of\n0.63/0.58/0.61 for digit recall failure and 0.58/0.59/0.54 for sentence recall\nfailure are obtained from the connectivity structure, graph variability, and\nchannel power features respectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:42:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Helfer", "Brian S.", ""], ["Williamson", "James R.", ""], ["Miller", "Benjamin A.", ""], ["Perricone", "Joseph", ""], ["Quatieri", "Thomas F.", ""]]}]