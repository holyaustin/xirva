[{"id": "1010.0302", "submitter": "Marc Barthelemy", "authors": "Marc Barthelemy", "title": "Spatial Networks", "comments": "Review article, revised and augmented version, 86 pages, 86 figures,\n  338 references", "journal-ref": "Physics Reports 499:1-101 (2011)", "doi": "10.1016/j.physrep.2010.11.002", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cs.SI physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are very often organized under the form of networks where\nnodes and edges are embedded in space. Transportation and mobility networks,\nInternet, mobile phone networks, power grids, social and contact networks,\nneural networks, are all examples where space is relevant and where topology\nalone does not contain all the information. Characterizing and understanding\nthe structure and the evolution of spatial networks is thus crucial for many\ndifferent fields ranging from urbanism to epidemiology. An important\nconsequence of space on networks is that there is a cost associated to the\nlength of edges which in turn has dramatic effects on the topological structure\nof these networks. We will expose thoroughly the current state of our\nunderstanding of how the spatial constraints affect the structure and\nproperties of these networks. We will review the most recent empirical\nobservations and the most important models of spatial networks. We will also\ndiscuss various processes which take place on these spatial networks, such as\nphase transitions, random walks, synchronization, navigation, resilience, and\ndisease spread.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 07:55:23 GMT"}, {"version": "v2", "created": "Thu, 4 Nov 2010 11:07:01 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Barthelemy", "Marc", ""]]}, {"id": "1010.0605", "submitter": "R. A. J. van Elburg", "authors": "Ronald A.J. van Elburg", "title": "Stochastic Continuous Time Neurite Branching Models with Tree and\n  Segment Dependent Rates", "comments": "41 pages, 2 figures, revised structure and text improvements", "journal-ref": "Journal of Theoretical Biology, 2011", "doi": "10.1016/j.jtbi.2011.01.039", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a continuous time stochastic neurite branching\nmodel closely related to the discrete time stochastic BES-model. The discrete\ntime BES-model is underlying current attempts to simulate cortical development,\nbut is difficult to analyze. The new continuous time formulation facilitates\nanalytical treatment thus allowing us to examine the structure of the model\nmore closely. We derive explicit expressions for the time dependent\nprobabilities p(\\gamma, t) for finding a tree \\gamma at time t, valid for\narbitrary continuous time branching models with tree and segment dependent\nbranching rates. We show, for the specific case of the continuous time\nBES-model, that as expected from our model formulation, the sums needed to\nevaluate expectation values of functions of the terminal segment number\n\\mu(f(n),t) do not depend on the distribution of the total branching\nprobability over the terminal segments. In addition, we derive a system of\ndifferential equations for the probabilities p(n,t) of finding n terminal\nsegments at time t. For the continuous BES-model, this system of differential\nequations gives direct numerical access to functions only depending on the\nnumber of terminal segments, and we use this to evaluate the development of the\nmean and standard deviation of the number of terminal segments at a time t. For\ncomparison we discuss two cases where mean and variance of the number of\nterminal segments are exactly solvable. Then we discuss the numerical\nevaluation of the S-dependence of the solutions for the continuous time\nBES-model. The numerical results show clearly that higher S values, i.e. values\nsuch that more proximal terminal segments have higher branching rates than more\ndistal terminal segments, lead to more symmetrical trees as measured by three\ntree symmetry indicators.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 14:32:52 GMT"}, {"version": "v2", "created": "Mon, 10 Jan 2011 10:29:21 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["van Elburg", "Ronald A. J.", ""]]}, {"id": "1010.1714", "submitter": "Randall Beer", "authors": "Randall D. Beer and Bryan Daniels", "title": "Saturation Probabilities of Continuous-Time Sigmoidal Networks", "comments": "53 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.CO math.DS nlin.AO q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From genetic regulatory networks to nervous systems, the interactions between\nelements in biological networks often take a sigmoidal or S-shaped form. This\npaper develops a probabilistic characterization of the parameter space of\ncontinuous-time sigmoidal networks (CTSNs), a simple but dynamically-universal\nmodel of such interactions. We describe an efficient and accurate method for\ncalculating the probability of observing effectively M-dimensional dynamics in\nan N-element CTSN, as well as a closed-form but approximate method. We then\nstudy the dependence of this probability on N, M, and the parameter ranges over\nwhich sampling occurs. This analysis provides insight into the overall\nstructure of CTSN parameter space.\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 15:10:38 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Beer", "Randall D.", ""], ["Daniels", "Bryan", ""]]}, {"id": "1010.1960", "submitter": "Vitor Sessak", "authors": "Vitor Sessak", "title": "Inverse problems in spin models", "comments": "PhD thesis defended the 16th September 2010 at the Laboratoire de\n  Physique Th\\'eorique of \\'Ecole Normale Sup\\'erieure in Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent experiments in biology study systems composed of several\ninteracting elements, for example neuron networks. Normally, measurements\ndescribe only the collective behavior of the system, even if in most cases we\nwould like to characterize how its different parts interact. The goal of this\nthesis is to extract information about the microscopic interactions as a\nfunction of their collective behavior for two different cases. First, we will\nstudy a system described by a generalized Ising model. We find explicit\nformulas for the couplings as a function of the correlations and\nmagnetizations. In the following, we will study a system described by a\nHopfield model. In this case, we find not only explicit formula for inferring\nthe patterns, but also an analytical result that allows one to estimate how\nmuch data is necessary for a good inference.\n", "versions": [{"version": "v1", "created": "Sun, 10 Oct 2010 19:42:20 GMT"}], "update_date": "2010-10-12", "authors_parsed": [["Sessak", "Vitor", ""]]}, {"id": "1010.2530", "submitter": "Dante Chialvo", "authors": "Dante R. Chialvo", "title": "Emergent complex neural dynamics", "comments": null, "journal-ref": "Nature Physics 6, 744-750 (2010)", "doi": "10.1038/nphys1803", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large repertoire of spatiotemporal activity patterns in the brain is the\nbasis for adaptive behaviour. Understanding the mechanism by which the brain's\nhundred billion neurons and hundred trillion synapses manage to produce such a\nrange of cortical configurations in a flexible manner remains a fundamental\nproblem in neuroscience. One plausible solution is the involvement of universal\nmechanisms of emergent complex phenomena evident in dynamical systems poised\nnear a critical point of a second-order phase transition. We review recent\ntheoretical and empirical results supporting the notion that the brain is\nnaturally poised near criticality, as well as its implications for better\nunderstanding of the brain.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 22:29:48 GMT"}], "update_date": "2010-10-14", "authors_parsed": [["Chialvo", "Dante R.", ""]]}, {"id": "1010.3009", "submitter": "Leonid Perlovsky", "authors": "Leonid Perlovsky, Marie-Claude Bonniot-Cabanac, Michel Cabanac", "title": "Curiosity and Pleasure", "comments": "12 pages, conference IJCNN 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic decision making received wide attention due to the work of Tversky\nand Kahneman (1981) and inspired multiple studies of irrationality of the human\nmind and a fundamental disregard for knowledge. But what is the source of all\nhuman knowledge, including heuristics? We discuss the hypothesis that\nacquisition of knowledge is a deeply rooted psychological need, a motivational\nmechanism for perception as well as higher cognition. We report experimental\nresults showing that acquisition of knowledge is emotionally pleasing. The\nsatisfaction of curiosity through acquiring knowledge brings pleasure. This\nconfirms the hypothesis that curiosity or need for knowledge is a fundamental\nand ancient motivation on a par with other basic needs, such as sex or food.\nThis paper connects curiosity, knowledge, cognition, emotions, including\naesthetic emotions of the beautiful, mechanisms of drives, high cognitive\nfunctions, minimization of cognitive effort through heuristics, and knowledge\nmaximization. We anticipate our finding to be an important aspect for several\nclassical fields including cognitive dissonance, personality, self, learning,\nand new directions in cognitive science studying emotions related to acquiring\nknowledge, personality types in relation to types of knowledge, relating higher\ncognitive abilities to knowledge-related emotions, and new directions in\naesthetics revealing the cognitive nature of the beautiful and music.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 19:30:08 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Perlovsky", "Leonid", ""], ["Bonniot-Cabanac", "Marie-Claude", ""], ["Cabanac", "Michel", ""]]}, {"id": "1010.3059", "submitter": "Leonid Perlovsky", "authors": "Leonid I. Perlovsky", "title": "SCIENCE AND RELIGION: Scientific Understanding and Mathematical Modeling\n  of Emotions of the Spiritually Sublime", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science strives for a detailed understanding of reality even if this\ndifferentiation threatens individual synthesis, or the wholeness of psyche.\nReligion strives to maintain the wholeness of psyche, even if at the expense of\na detailed understanding of the world and Self. This paper analyzes the\ncognitive forces driving us to achieve both. This analysis leads to\nunderstanding emotions of the religiously sublime, which are the foundations of\nall religions. These seemingly mysterious feelings, which everyone feels, even\nif rarely, even if without noticing them consciously, even if without being\nable to name them properly, today can be explained scientifically. And\npossibly, we may soon be able to measure them in a psychological laboratory.\nThe article briefly reviews new developments in brain imaging that have made\nnew data available, and reviews development and mathematical modeling in\ncognitive theory explaining these previously mysterious feelings. This new\nscientific analysis has overcome another long-standing challenge: reductionism.\nAlthough religious feelings can be scientifically discussed in terms of\nconcrete neural mechanisms and mathematically modeled, but cannot be reduced to\n\"just this or that\" mechanical explanation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 01:17:48 GMT"}], "update_date": "2010-11-13", "authors_parsed": [["Perlovsky", "Leonid I.", ""]]}, {"id": "1010.3063", "submitter": "Jose Acacio de Barros", "authors": "Patrick Suppes, Jose Acacio de Barros, and Gary Oas", "title": "Phase-Oscillator Computations as Neural Models of Stimulus-Response\n  Conditioning and Response Selection", "comments": null, "journal-ref": "Journal of Mathematical Psychology, Volume 56, Issue 2, April\n  2012, Pages 95-117", "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activity of collections of synchronizing neurons can be represented by\nweakly coupled nonlinear phase oscillators satisfying Kuramoto's equations. In\nthis article, we build such neural-oscillator models, partly based on\nneurophysiological evidence, to represent approximately the learning behavior\npredicted and confirmed in three experiments by well-known stochastic learning\nmodels of behavioral stimulus-response theory. We use three Kuramoto\noscillators to model a continuum of responses, and we provide detailed\nnumerical simulations and analysis of the three-oscillator Kuramoto problem,\nincluding an analysis of the stability points for different coupling\nconditions. We show that the oscillator simulation data are well-matched to the\nbehavioral data of the three experiments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 02:03:59 GMT"}, {"version": "v2", "created": "Mon, 18 Oct 2010 01:38:01 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2012 17:33:37 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Suppes", "Patrick", ""], ["de Barros", "Jose Acacio", ""], ["Oas", "Gary", ""]]}, {"id": "1010.3537", "submitter": "Moritz Deger", "authors": "Moritz Helias, Moritz Deger, Stefan Rotter, Markus Diesmann", "title": "The perfect integrator driven by Poisson input and its approximation in\n  the diffusion limit", "comments": "7 pages, 3 figures, v2: corrected authors in reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider the perfect integrator driven by Poisson process\ninput. We derive its equilibrium and response properties and contrast them to\nthe approximations obtained by applying the diffusion approximation. In\nparticular, the probability density in the vicinity of the threshold differs,\nwhich leads to altered response properties of the system in equilibrium.\n", "versions": [{"version": "v1", "created": "Mon, 18 Oct 2010 09:57:29 GMT"}, {"version": "v2", "created": "Mon, 22 Nov 2010 11:18:59 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Helias", "Moritz", ""], ["Deger", "Moritz", ""], ["Rotter", "Stefan", ""], ["Diesmann", "Markus", ""]]}, {"id": "1010.3775", "submitter": "Danielle Bassett", "authors": "Danielle S. Bassett, Nicholas F. Wymbs, Mason A. Porter, Peter J.\n  Mucha, Jean M. Carlson, Scott T. Grafton", "title": "Dynamic reconfiguration of human brain networks during learning", "comments": "Main Text: 19 pages, 4 figures Supplementary Materials: 34 pages, 4\n  figures, 3 tables", "journal-ref": "PNAS 2011, vol. 108, no. 18, 7641-7646", "doi": "10.1073/pnas.1018985108", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn math-ph math.MP nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human learning is a complex phenomenon requiring flexibility to adapt\nexisting brain function and precision in selecting new neurophysiological\nactivities to drive desired behavior. These two attributes -- flexibility and\nselection -- must operate over multiple temporal scales as performance of a\nskill changes from being slow and challenging to being fast and automatic. Such\nselective adaptability is naturally provided by modular structure, which plays\na critical role in evolution, development, and optimal network function. Using\nfunctional connectivity measurements of brain activity acquired from initial\ntraining through mastery of a simple motor skill, we explore the role of\nmodularity in human learning by identifying dynamic changes of modular\norganization spanning multiple temporal scales. Our results indicate that\nflexibility, which we measure by the allegiance of nodes to modules, in one\nexperimental session predicts the relative amount of learning in a future\nsession. We also develop a general statistical framework for the identification\nof modular architectures in evolving systems, which is broadly applicable to\ndisciplines where network adaptability is crucial to the understanding of\nsystem performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Oct 2010 01:30:23 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2011 03:51:53 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Bassett", "Danielle S.", ""], ["Wymbs", "Nicholas F.", ""], ["Porter", "Mason A.", ""], ["Mucha", "Peter J.", ""], ["Carlson", "Jean M.", ""], ["Grafton", "Scott T.", ""]]}, {"id": "1010.4145", "submitter": "Christophe Magnani", "authors": "Christophe Magnani, Daniel Eug\\`ene, Erwin Idoux, L.E. Moore", "title": "Voltage clamp analysis of nonlinear dendritic propertie in prepositus\n  hypoglossi neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonlinear properties of the dendrites in prepositus hypoglossi neurons\nare involved in maintenance of eye position. The biophysical properties of\nthese neurons are essential for the operation of the vestibular neural\nintegrator that converts a head velocity signal to one that controls eye\nposition. A novel method named QSA (quadratic sinusoidal analysis) for voltage\nclamped neurons was used to quantify nonlinear responses that are dominated by\ndendrites. The voltage clamp currents were measured at harmonic and interactive\nfrequencies using specific stimulation frequencies, which act as frequency\nprobes of the intrinsic nonlinear neuronal behavior. These responses to paired\nfrequencies form a matrix that can be reduced by eigendecomposition to provide\na very compact piecewise quadratic analysis at different membrane potentials\nthat otherwise is usually described by complex differential equations involving\na large numbers of parameters and dendritic compartments. Moreover, the QSA\nmatrix can be interpolated to capture most of the nonlinear neuronal behavior\nlike a Volterra kernel. The interpolated quadratic functions of the two major\nprepositus hypoglossi neurons, namely type B and D, are strikingly different. A\nmajor part of the nonlinear responses is due to the persistent sodium\nconductance, which appears to be essential for sustained nonlinear effects\ninduced by NMDA activation and thus would be critical for the operation of the\nneural integrator. Finally, the dominance of the nonlinear responses by the\ndendrites supports the hypothesis that persistent sodium conductance channels\nand NMDA receptors act synergistically to dynamically control the influence of\nindividual synaptic inputs on network behavior.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 09:48:52 GMT"}, {"version": "v2", "created": "Fri, 29 Oct 2010 14:29:24 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Magnani", "Christophe", ""], ["Eug\u00e8ne", "Daniel", ""], ["Idoux", "Erwin", ""], ["Moore", "L. E.", ""]]}, {"id": "1010.4222", "submitter": "Leonid Perlovsky", "authors": "Leonid Perlovsky and Roman Ilin", "title": "Grounded Symbols in the Brain Computational Foundations for Perceptual\n  Symbol System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a mathematical models of grounded symbols in the brain. It also\nserves as a computational foundations for Perceptual Symbol System (PSS). This\ndevelopment requires new mathematical methods of dynamic logic (DL), which have\novercome limitations of classical artificial intelligence and connectionist\napproaches. The paper discusses these past limitations, relates them to\ncombinatorial complexity (exponential explosion) of algorithms in the past, and\nfurther to the static nature of classical logic. The new mathematical theory,\nDL, is a process-logic. A salient property of this process is evolution of\nvague representations into crisp. The paper first applies it to one aspect of\nPSS: situation learning from object perceptions. Then we relate DL to the\nessential PSS mechanisms of concepts, simulators, grounding, productivity,\nbinding, recursion, and to the mechanisms relating grounded and amodal symbols.\nWe discuss DL as a general theory describing the process of cognition on\nmultiple levels of abstraction. We also discuss the implications of this theory\nfor interactions between cognition and language, mechanisms of language\ngrounding, and possible role of language in grounding abstract cognition. The\ndeveloped theory makes experimental predictions, and will impact future\ntheoretical developments in cognitive science, including knowledge\nrepresentation, and perception-cognition interaction. Experimental neuroimaging\nevidence for DL and PSS in brain imaging is discussed as well as future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 15:09:25 GMT"}], "update_date": "2010-10-21", "authors_parsed": [["Perlovsky", "Leonid", ""], ["Ilin", "Roman", ""]]}, {"id": "1010.4517", "submitter": "Jake Bouvrie", "authors": "Jake Bouvrie, Jean-Jacques Slotine", "title": "Synchronization and Redundancy: Implications for Robustness of Neural\n  Learning and Decision Making", "comments": "Preprint, accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and decision making in the brain are key processes critical to\nsurvival, and yet are processes implemented by non-ideal biological building\nblocks which can impose significant error. We explore quantitatively how the\nbrain might cope with this inherent source of error by taking advantage of two\nubiquitous mechanisms, redundancy and synchronization. In particular we\nconsider a neural process whose goal is to learn a decision function by\nimplementing a nonlinear gradient dynamics. The dynamics, however, are assumed\nto be corrupted by perturbations modeling the error which might be incurred due\nto limitations of the biology, intrinsic neuronal noise, and imperfect\nmeasurements. We show that error, and the associated uncertainty surrounding a\nlearned solution, can be controlled in large part by trading off\nsynchronization strength among multiple redundant neural systems against the\nnoise amplitude. The impact of the coupling between such redundant systems is\nquantified by the spectrum of the network Laplacian, and we discuss the role of\nnetwork topology in synchronization and in reducing the effect of noise. A\nrange of situations in which the mechanisms we model arise in brain science are\ndiscussed, and we draw attention to experimental evidence suggesting that\ncortical circuits capable of implementing the computations of interest here can\nbe found on several scales. Finally, simulations comparing theoretical bounds\nto the relevant empirical quantities show that the theoretical estimates we\nderive can be tight.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 16:34:43 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 17:01:04 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Bouvrie", "Jake", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "1010.4683", "submitter": "Mar\\'ia J. C\\'aceres", "authors": "Mar\\'ia J. C\\'aceres, Jos\\'e A. Carrillo and Beno\\^it Perthame", "title": "Analysis of Nonlinear Noisy Integrate\\&Fire Neuron Models: blow-up and\n  steady states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear Noisy Leaky Integrate and Fire (NNLIF) models for neurons networks\ncan be written as Fokker-Planck-Kolmogorov equations on the probability density\nof neurons, the main parameters in the model being the connectivity of the\nnetwork and the noise. We analyse several aspects of the NNLIF model: the\nnumber of steady states, a priori estimates, blow-up issues and convergence\ntoward equilibrium in the linear case. In particular, for excitatory networks,\nblow-up always occurs for initial data concentrated close to the firing\npotential. These results show how critical is the balance between noise and\nexcitatory/inhibitory interactions to the connectivity parameter.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 11:52:38 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["C\u00e1ceres", "Mar\u00eda J.", ""], ["Carrillo", "Jos\u00e9 A.", ""], ["Perthame", "Beno\u00eet", ""]]}, {"id": "1010.4722", "submitter": "Alexander Bershadskii", "authors": "A. Bershadskii and Y. Ikegaya", "title": "Chaotic neuron clock", "comments": null, "journal-ref": "Chaos, Solitons & Fractals 44 (2011) 342-347", "doi": "10.1016/j.chaos.2011.03.001", "report-no": null, "categories": "q-bio.NC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple threshold model of neuron firing (with the neuron membrane\nelectrochemical potential governed by the chaotic Rossler attractor) has been\nanalyzed by mapping the generated irregular spiking time-series into telegraph\nsignals. In this model the fundamental frequency of the chaotic Rossler\nattractor provides (with a period doubling) the strong periodic component of\nthe generated irregular signal. The exponentially decaying broad-band part of\nthe spectrum of the Rossler attractor has been transformed by the threshold\nfiring mechanism into a scaling tale. These results are compared with irregular\nspiking time-series obtained in vitro from a spontaneous activity of\nhippocampal (CA3) singular neurons (rat's brain slice culture). The comparison\nshows good agreement between the model and experimentally obtained spectra.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 14:28:15 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 21:09:22 GMT"}, {"version": "v3", "created": "Sun, 21 Nov 2010 18:15:10 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2011 22:27:46 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Bershadskii", "A.", ""], ["Ikegaya", "Y.", ""]]}, {"id": "1010.5113", "submitter": "Constantinos Siettos", "authors": "Konstantinos G. Spiliotis and Constantinos I. Siettos", "title": "Coarse-Grained Analysis of Microscopic Neuronal Simulators on Networks:\n  Bifurcation and Rare-events computations", "comments": null, "journal-ref": "Neurocomputing, 74, 3576-3589 (2011)", "doi": null, "report-no": null, "categories": "cs.SI nlin.AO physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the Equation-Free approach for mutliscale computations can be\nexploited to extract, in a computational strict and systematic way the emergent\ndynamical attributes, from detailed large-scale microscopic stochastic models,\nof neurons that interact on complex networks. In particular we show how the\nEquation-Free approach can be exploited to perform system-level tasks such as\nbifurcation, stability analysis and estimation of mean appearance times of rare\nevents, bypassing the need for obtaining analytical approximations, providing\nan \"on-demand\" model reduction. Using the detailed simulator as a black-box\ntimestepper, we compute the coarse-grained equilibrium bifurcation diagrams,\nexamine the stability of the solution branches and perform a rare-events\nanalysis with respect to certain characteristics of the underlying network\ntopology such as the connectivity degree\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 13:26:45 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Spiliotis", "Konstantinos G.", ""], ["Siettos", "Constantinos I.", ""]]}, {"id": "1010.5496", "submitter": "Ran Rubin", "authors": "Ran Rubin, Remi Monasson and Haim Sompolinsky", "title": "Theory of spike timing based neural classifiers", "comments": "4 page, 4 figures, Accepted to Physical Review Letters on 19th Oct.\n  2010", "journal-ref": "Phys. Rev. Lett. 105, 218102 (2010)", "doi": "10.1103/PhysRevLett.105.218102", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational capacity of a model neuron, the Tempotron, which\nclassifies sequences of spikes by linear-threshold operations. We use\nstatistical mechanics and extreme value theory to derive the capacity of the\nsystem in random classification tasks. In contrast to its static analog, the\nPerceptron, the Tempotron's solutions space consists of a large number of small\nclusters of weight vectors. The capacity of the system per synapse is finite in\nthe large size limit and weakly diverges with the stimulus duration relative to\nthe membrane and synaptic time constants.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 19:43:44 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Rubin", "Ran", ""], ["Monasson", "Remi", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1010.5602", "submitter": "Pedro Marijuan", "authors": "Pedro C. Marijuan and Jorge Navarro", "title": "The Bonds of Laughter: A Multidisciplinary Inquiry into the Information\n  Processes of Human Laughter", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new core hypothesis on laughter is presented. It has been built by putting\ntogether ideas from several disciplines: neurodynamics, evolutionary\nneurobiology, paleoanthropology, social networks, and communication studies.\nThe hypothesis contributes to ascertain the evolutionary origins of human\nlaughter in connection with its cognitive emotional signaling functions. The\nnew behavioral and neurodynamic tenets introduced about this unusual sound\nfeature of our species justify the ubiquitous presence it has in social\ninteractions and along the life cycle of the individual. Laughter, far from\nbeing a curious evolutionary relic or a rather trivial innate behavior, should\nbe considered as a highly efficient tool for inter-individual problem solving\nand for maintenance of social bonds.\n", "versions": [{"version": "v1", "created": "Wed, 27 Oct 2010 08:29:30 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Marijuan", "Pedro C.", ""], ["Navarro", "Jorge", ""]]}, {"id": "1010.6178", "submitter": "Sander Bohte", "authors": "Sander M. Bohte and Jaldert O. Rombouts", "title": "Fractionally Predictive Spiking Neurons", "comments": "13 pages, 5 figures, in Advances in Neural Information Processing\n  2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experimental work has suggested that the neural firing rate can be\ninterpreted as a fractional derivative, at least when signal variation induces\nneural adaptation. Here, we show that the actual neural spike-train itself can\nbe considered as the fractional derivative, provided that the neural signal is\napproximated by a sum of power-law kernels. A simple standard thresholding\nspiking neuron suffices to carry out such an approximation, given a suitable\nrefractory response. Empirically, we find that the online approximation of\nsignals with a sum of power-law kernels is beneficial for encoding signals with\nslowly varying components, like long-memory self-similar signals. For such\nsignals, the online power-law kernel approximation typically required less than\nhalf the number of spikes for similar SNR as compared to sums of similar but\nexponentially decaying kernels. As power-law kernels can be accurately\napproximated using sums or cascades of weighted exponentials, we demonstrate\nthat the corresponding decoding of spike-trains by a receiving neuron allows\nfor natural and transparent temporal signal filtering by tuning the weights of\nthe decoding kernel.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 10:48:25 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Bohte", "Sander M.", ""], ["Rombouts", "Jaldert O.", ""]]}]