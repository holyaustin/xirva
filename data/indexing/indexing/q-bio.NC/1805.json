[{"id": "1805.00393", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera, Ezequiel Di Paolo", "title": "Integrated Information and Autonomy in the Thermodynamic Limit", "comments": "This paper was published for a conference and it's quite similar to a\n  journal version of the manuscript, also published arXiv:1806.07879", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech nlin.AO physics.bio-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of autonomy is fundamental for understanding biological\norganization and the evolutionary transitions of living systems. Understanding\nhow a system constitutes itself as an individual, cohesive, self-organized\nentity is a fundamental challenge for the understanding of life. However, it is\ngenerally a difficult task to determine whether the system or its environment\nhas generated the correlations that allow an observer to trace the boundary of\na living system as a coherent unit. Inspired by the framework of integrated\ninformation theory, we propose a measure of the level of integration of a\nsystem as the response of a system to partitions that introduce perturbations\nin the interaction between subsystems, without assuming the existence of a\nstationary distribution. With the goal of characterizing transitions in\nintegrated information in the thermodynamic limit, we apply this measure to\nkinetic Ising models of infinite size using mean field techniques. Our findings\nsuggest that, in order to preserve the integration of causal influences of a\nsystem as it grows in size, a living entity must be poised near critical points\nmaximizing its sensitivity to perturbations in the interaction between\nsubsystems. Moreover, we observe how such a measure is able to delimit an agent\nand its environment, being able to characterize simple instances of\nagent-environment asymmetries in which the agent has the ability to modulate\nits coupling with the environment.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 11:19:14 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 13:01:53 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 12:43:43 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 23:14:51 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Aguilera", "Miguel", ""], ["Di Paolo", "Ezequiel", ""]]}, {"id": "1805.00563", "submitter": "Vijay Singh", "authors": "Vijay Singh, Nicolle R. Murphy, Vijay Balasubramanian, Joel D.\n  Mainland", "title": "A competitive binding model predicts nonlinear responses of olfactory\n  receptors to complex mixtures", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1813230116", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In color vision, the quantitative rules for mixing lights to make a target\ncolor are well understood. By contrast, the rules for mixing odorants to make a\ntarget odor remain elusive. A solution to this problem in vision relied on\ncharacterizing receptor responses to different wavelengths of light and\nsubsequently relating these responses to perception. In olfaction,\nexperimentally measuring receptor responses to a representative set of complex\nmixtures is intractable due to the vast number of possibilities. To meet this\nchallenge, we develop a biophysical model that predicts mammalian receptor\nresponses to complex mixtures using responses to single odorants. The dominant\nnonlinearity in our model is competitive binding (CB): only one odorant\nmolecule can attach to a receptor binding site at a time. This simple framework\npredicts receptor responses to mixtures of up to twelve monomolecular odorants\nto within 15\\% of experimental observations and provides a powerful method for\nleveraging limited experimental data. Simple extensions of our model describe\nphenomena such as synergy, overshadowing, and inhibition. We demonstrate that\nthe presence of such interactions can be identified via systematic deviations\nfrom the competitive binding model.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 21:39:19 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 18:58:04 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 01:39:32 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 01:38:40 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Singh", "Vijay", ""], ["Murphy", "Nicolle R.", ""], ["Balasubramanian", "Vijay", ""], ["Mainland", "Joel D.", ""]]}, {"id": "1805.00605", "submitter": "Hao Wang", "authors": "Hao Wang, Jiahui Wang, Xin Yuan Thow, Chengkuo Lee", "title": "The first principle of neural circuit and the general\n  Circuit-Probability theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new neural circuit is proposed by considering the myelin as an inductor.\nThis new neural circuit can explain why the lump-parameter circuit used in\nprevious C-P theory is valid. Meanwhile, it provides a new explanation of the\nbiological function of myelin for neural signal propagation. Furthermore, a new\nmodel for magnetic nerve stimulation is built and all phenomena in magnetic\nnerve stimulation can be well explained. Based on this model, the coil\nstructure can be optimized.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 02:33:05 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 07:21:11 GMT"}, {"version": "v3", "created": "Sat, 2 Jun 2018 03:26:51 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Jiahui", ""], ["Thow", "Xin Yuan", ""], ["Lee", "Chengkuo", ""]]}, {"id": "1805.00786", "submitter": "Don Krieger", "authors": "Don Krieger, Paul Shepard, Ben Zusman, Anirban Jana, David O. Okonkwo", "title": "Seemless Utilization of Heterogeneous XSede Resources to Accelerate\n  Processing of a High Value Functional Neuroimaging Dataset", "comments": "8 pages, 10 figures, supplementary materials. Proceedings of the 2018\n  PEARC Conference, Practice & Experience in Advanced Research Computing,\n  Pittsburgh, PA, July, 2018. arXiv admin note: substantial text overlap with\n  arXiv:1710.05246. author note: The overlap with arXiv:1710.05246 is primarily\n  in \"background\" material which includes a description of the solver and\n  sample scientific findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the technical effort used to process a voluminous high value\nhuman neuroimaging dataset on the Open Science Grid with opportunistic use of\nidle HPC resources to boost computing capacity more than 5-fold. With minimal\nsoftware development effort and no discernable competitive interference with\nother HPC users, this effort delivered 15,000,000 core hours over 7 months.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 13:38:07 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 23:05:48 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 15:14:39 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Krieger", "Don", ""], ["Shepard", "Paul", ""], ["Zusman", "Ben", ""], ["Jana", "Anirban", ""], ["Okonkwo", "David O.", ""]]}, {"id": "1805.01260", "submitter": "Jennifer Stiso", "authors": "Jennifer Stiso, Ankit N. Khambhati, Tommaso Menara, Ari E. Kahn, Joel\n  M. Stein, Sandihitsu R. Das, Richard Gorniak, Joseph Tracy, Brian Litt,\n  Kathryn A. Davis, Fabio Pasqualetti, Timothy Lucas, and Danielle S. Bassett", "title": "White Matter Network Architecture Guides Direct Electrical Stimulation\n  Through Optimal State Transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electrical brain stimulation is currently being investigated as a therapy for\nneurological disease. However, opportunities to optimize such therapies are\nchallenged by the fact that the beneficial impact of focal stimulation on both\nneighboring and distant regions is not well understood. Here, we use network\ncontrol theory to build a model of brain network function that makes\npredictions about how stimulation spreads through the brain's white matter\nnetwork and influences large-scale dynamics. We test these predictions using\ncombined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data\nwho volunteered to participate in an extensive stimulation regimen. We posit a\nspecific model-based manner in which white matter tracts constrain stimulation,\ndefining its capacity to drive the brain to new states, including states\nassociated with successful memory encoding. In a first validation of our model,\nwe find that the true pattern of white matter tracts can be used to more\naccurately predict the state transitions induced by direct electrical\nstimulation than the artificial patterns of null models. We then use a targeted\noptimal control framework to solve for the optimal energy required to drive the\nbrain to a given state. We show that, intuitively, our model predicts larger\nenergy requirements when starting from states that are farther away from a\ntarget memory state. We then suggest testable hypotheses about which structural\nproperties will lead to efficient stimulation for improving memory based on\nenergy requirements. Our work demonstrates that individual white matter\narchitecture plays a vital role in guiding the dynamics of direct electrical\nstimulation, more generally offering empirical support for the utility of\nnetwork control theoretic models of brain response to stimulation.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 12:39:15 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Stiso", "Jennifer", ""], ["Khambhati", "Ankit N.", ""], ["Menara", "Tommaso", ""], ["Kahn", "Ari E.", ""], ["Stein", "Joel M.", ""], ["Das", "Sandihitsu R.", ""], ["Gorniak", "Richard", ""], ["Tracy", "Joseph", ""], ["Litt", "Brian", ""], ["Davis", "Kathryn A.", ""], ["Pasqualetti", "Fabio", ""], ["Lucas", "Timothy", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1805.01277", "submitter": "Gerrit Ecke", "authors": "Gerrit A. Ecke, Fabian A. Mikulasch, Sebastian A. Bruijns, Thede\n  Witschel, Aristides B. Arrenberg and Hanspeter A. Mallot", "title": "Sparse Coding Predicts Optic Flow Specificities of Zebrafish Pretectal\n  Neurons", "comments": "Published Conference Paper from ICANN 2018, Rhodes", "journal-ref": "Artificial Neural Networks and Machine Learning - ICANN 2018.\n  ICANN 2018. Lecture Notes in Computer Science, vol 11141. Springer, Cham", "doi": "10.1007/978-3-030-01424-7_64", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zebrafish pretectal neurons exhibit specificities for large-field optic flow\npatterns associated with rotatory or translatory body motion. We investigate\nthe hypothesis that these specificities reflect the input statistics of natural\noptic flow. Realistic motion sequences were generated using computer graphics\nsimulating self-motion in an underwater scene. Local retinal motion was\nestimated with a motion detector and encoded in four populations of\ndirectionally tuned retinal ganglion cells, represented as two signed input\nvariables. This activity was then used as input into one of two learning\nnetworks: a sparse coding network (competitive learning) and backpropagation\nnetwork (supervised learning). Both simulations develop specificities for optic\nflow which are comparable to those found in a neurophysiological study (Kubo et\nal. 2014), and relative frequencies of the various neuronal responses are best\nmodeled by the sparse coding approach. We conclude that the optic flow neurons\nin the zebrafish pretectum do reflect the optic flow statistics. The predicted\nvectorial receptive fields show typical optic flow fields but also \"Gabor\" and\ndipole-shaped patterns that likely reflect difference fields needed for\nreconstruction by linear superposition.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:10:50 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 12:02:08 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 08:45:02 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 14:30:36 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Ecke", "Gerrit A.", ""], ["Mikulasch", "Fabian A.", ""], ["Bruijns", "Sebastian A.", ""], ["Witschel", "Thede", ""], ["Arrenberg", "Aristides B.", ""], ["Mallot", "Hanspeter A.", ""]]}, {"id": "1805.01552", "submitter": "Don Krieger", "authors": "Don Krieger, Paul Shepard, David O. Okonkwo", "title": "Normative atlases of neuroelectric brain activity and connectivity from\n  a large human cohort", "comments": "37 pages, 13 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalographic (MEG) recordings from a large normative cohort (n =\n619) were processed to extract measures of regional neuroelectric activity. The\noverall objective of the effort was to use these measures to identify normative\nhuman neuroelectric brain function. The aims were (a) to identify and measure\nthe values and range of those neuroelectric properties which are common to the\ncohort, (b) to identify and measure the values and range of those neuroelectric\nproperties which distinguish one individual from another, and (c) to identify\nrelationships of the measures to properties of the individual, e.g. sex,\nbiological age, and sleep symptoms. It is hoped that comparison of the\nresultant established norms to measures from recordings of symptomatic\nindividuals will enable advances is our understanding of pathology.\n  MEG recordings during resting and task conditions were provided by The\nCambridge (UK) Centre for Ageing and Neuroscience Stage 2 cohort study. Referee\nconsensus processing was used to localize and validate neuroelectric currents,\np < 10-12 for each, p < 10-4 for each when corrected for multiple comparisons.\n  Comparisons of regional activity and connectivity within-subjects produced\nprofuse reliable measures detailing differences between individuals, p < 10-8\nfor each comparison, p < 0.005 for each when corrected for a total of 5 x 105\ncomparisons. Cohort-wide regional comparisons (p < 10-8 for each) produced\nprofuse measures which were common to the preponderance of individuals,\ndetailing normative commonalities in brain function. Comparisons of regional\ngray matter (cellular) vs white matter (communication fibers) activity produced\nrobust differences both cohort-wide and for each individual. These results\nvalidate the high spatial resolution of the results and establish the\nunprecedented ability to obtain neuroelectric measures from the white matter.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 21:47:09 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 14:15:41 GMT"}, {"version": "v3", "created": "Sat, 17 Nov 2018 04:03:38 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Krieger", "Don", ""], ["Shepard", "Paul", ""], ["Okonkwo", "David O.", ""]]}, {"id": "1805.01631", "submitter": "James Tee", "authors": "James Tee and Desmond P. Taylor", "title": "Is Information in the Brain Represented in Continuous or Discrete Form?", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of continuous-versus-discrete information representation in the\nbrain is a fundamental yet unresolved question. Historically, most analyses\nassume a continuous representation without considering the discrete\nalternative. Our work explores the plausibility of both, answering the question\nfrom a communications systems engineering perspective. Using Shannon's\ncommunications theory, we posit that information in the brain is represented in\ndiscrete form. We address this hypothesis using 2 approaches. First, we\nidentify the fundamental communication requirements of the brain. Second, we\nestimate the symbol error probability and channel capacity for a continuous\ninformation representation. Our work concludes that information cannot be\ncommunicated and represented reliably in the brain using a continuous\nrepresentation - it has to be in a discrete form. This is a major demarcation\nfrom conventional and current wisdom. We apply this discrete result to the 4\nmajor neural coding hypotheses, and illustrate the use of discrete ISI neural\ncoding in analyzing electrophysiology experimental data. We further posit and\nillustrate a plausible direct link between Weber's Law and discrete neural\ncoding. We end by outlining a number of key research questions on discrete\nneural coding.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 07:24:50 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 19:58:13 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 13:37:30 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 13:46:53 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tee", "James", ""], ["Taylor", "Desmond P.", ""]]}, {"id": "1805.01667", "submitter": "Martin V\\\"olker", "authors": "Martin V\\\"olker, Ji\\v{r}\\'i Hammer, Robin T. Schirrmeister, Joos\n  Behncke, Lukas D.J. Fiederer, Andreas Schulze-Bonhage, Petr Marusi\\v{c},\n  Wolfram Burgard, Tonio Ball", "title": "Intracranial Error Detection via Deep Learning", "comments": "8 pages, 6 figures. Accepted at the 2018 IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have revolutionized the field of machine learning\nand were recently successfully applied to various classification problems in\nnoninvasive electroencephalography (EEG). However, these methods were so far\nonly rarely evaluated for use in intracranial EEG. We employed convolutional\nneural networks (CNNs) to classify and characterize the error-related brain\nresponse as measured in 24 intracranial EEG recordings. Decoding accuracies of\nCNNs were significantly higher than those of a regularized linear discriminant\nanalysis. Using time-resolved deep decoding, it was possible to classify errors\nin various regions in the human brain, and further to decode errors over 200 ms\nbefore the actual erroneous button press, e.g., in the precentral gyrus.\nMoreover, deeper networks performed better than shallower networks in\ndistinguishing correct from error trials in all-channel decoding. In single\nrecordings, up to 100 % decoding accuracy was achieved. Visualization of the\nnetworks' learned features indicated that multivariate decoding on an ensemble\nof channels yields related, albeit non-redundant information compared to\nsingle-channel decoding. In summary, here we show the usefulness of deep\nlearning for both intracranial error decoding and mapping of the\nspatio-temporal structure of the human error processing network.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 08:53:03 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 13:15:38 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 10:57:22 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["V\u00f6lker", "Martin", ""], ["Hammer", "Ji\u0159\u00ed", ""], ["Schirrmeister", "Robin T.", ""], ["Behncke", "Joos", ""], ["Fiederer", "Lukas D. J.", ""], ["Schulze-Bonhage", "Andreas", ""], ["Marusi\u010d", "Petr", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1805.01743", "submitter": "Amirmasoud Ahmadi", "authors": "Amirmasoud Ahmadi, Mahsa Behroozi, Vahid Shalchyan, Mohammad Reza\n  Daliri", "title": "Classification of Epileptic EEG Signals by Wavelet based CFC", "comments": "Electroencephalogram; Wavelet Decomposition; Cross Frequency\n  Coupling;Quadratic Discriminant Analysis; T-test Feature Selection", "journal-ref": "Electrical-Electronics & Biomedical Engineering and Computer\n  Science in 2018 (EBBT 2018)", "doi": "10.1109/EBBT.2018.8391471", "report-no": null, "categories": "eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalogram, an influential equipment for analyzing humans\nactivities and recognition of seizure attacks can play a crucial role in\ndesigning accurate systems which can distinguish ictal seizures from regular\nbrain alertness, since it is the first step towards accomplishing a high\naccuracy computer aided diagnosis system (CAD). In this article a novel\napproach for classification of ictal signals with wavelet based cross frequency\ncoupling (CFC) is suggested. After extracting features by wavelet based CFC,\noptimal features have been selected by t-test and quadratic discriminant\nanalysis (QDA) have completed the Classification.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 12:50:36 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ahmadi", "Amirmasoud", ""], ["Behroozi", "Mahsa", ""], ["Shalchyan", "Vahid", ""], ["Daliri", "Mohammad Reza", ""]]}, {"id": "1805.02599", "submitter": "Jeffrey Shainline", "authors": "Jeffrey M. Shainline, Sonia M. Buckley, Adam N. McCaughan, Manuel\n  Castellanos-Beltran, Christine A. Donnelly, Michael L. Schneider, Richard P.\n  Mirin, and Sae Woo Nam", "title": "Superconducting Optoelectronic Neurons II: Receiver Circuits", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circuits using superconducting single-photon detectors and Josephson\njunctions to perform signal reception, synaptic weighting, and integration are\ninvestigated. The circuits convert photon-detection events into flux quanta,\nthe number of which is determined by the synaptic weight. The current from many\nsynaptic connections is inductively coupled to a superconducting loop that\nimplements the neuronal threshold operation. Designs are presented for synapses\nand neurons that perform integration as well as detect coincidence events for\ntemporal coding. Both excitatory and inhibitory connections are demonstrated.\nIt is shown that a neuron with a single integration loop can receive input from\n1000 such synaptic connections, and neurons of similar design could employ many\nloops for dendritic processing.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 16:17:42 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 17:10:25 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 19:55:10 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Shainline", "Jeffrey M.", ""], ["Buckley", "Sonia M.", ""], ["McCaughan", "Adam N.", ""], ["Castellanos-Beltran", "Manuel", ""], ["Donnelly", "Christine A.", ""], ["Schneider", "Michael L.", ""], ["Mirin", "Richard P.", ""], ["Nam", "Sae Woo", ""]]}, {"id": "1805.02609", "submitter": "Dongwook Kim", "authors": "Dongwook Kim", "title": "The Effect of In vivo-like Synaptic Inputs on Stellate Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous experimental work has shown high-frequency Poisson-distributed\ntrains of combined excitatory and inhibitory conductance- and current-based\nsynaptic inputs reduce amplitude of subthreshold oscillations of SCs. In this\npaper, we investigate the mechanism underlying these phenomena in the context\nof the model. More specially, we studied the effects of both conductance- and\ncurrent-based synaptic inputs at various maximal conductance values on a SC\nmodel. Our numerical simulations show that conductance-based synaptic inputs\nreduce the amplitude of SC's subthreshold oscillations for low enough value of\nthe maximal synaptic conductance value but amplify these oscillations at a\nhigher range. These results contrast with the experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:53:23 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Kim", "Dongwook", ""]]}, {"id": "1805.02611", "submitter": "Qing Hui", "authors": "Mehdi Firouznia, Chen Peng, Qing Hui", "title": "Toward Human-in-the-Loop Supervisory Control for Cyber-Physical Networks", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel approach to include a model of making decision in\nhuman brain into the control loop. Employing the methodology developed in\nmathematical neuroscience, we construct a model that accounts for quality of\nhuman decision in supervisory tasks. We specifically focus on adaptive gain\ntheory and the strategy selection problem. The proposed model is shown to be\ncapable of explaining the change of a strategy from compensatory to heuristics\nin different conditions. We also propose a method to incorporate the effect of\ninternal and external parameters such as stress level and emergencies in the\ndecision model. The model is employed in a supervisory controller that\ndispatches the jobs between autonomy and a human supervisor in an efficient\nway.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 16:43:59 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Firouznia", "Mehdi", ""], ["Peng", "Chen", ""], ["Hui", "Qing", ""]]}, {"id": "1805.02809", "submitter": "Daqing Guo", "authors": "Yangsong Zhang, Erwei Yin, Fali Li, Yu Zhang, Toshihisa Tanaka, Qibin\n  Zhao, Yan Cui, Peng Xu, Dezhong Yao, Daqing Guo", "title": "Two-stage frequency recognition method based on correlated component\n  analysis for SSVEP-based BCI", "comments": "10 pages, 10 figures, submitted to IEEE TNSRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a state-of-the-art method for\nfrequency recognition in steady-state visual evoked potential (SSVEP)-based\nbrain-computer interface (BCI) systems. Various extended methods have been\ndeveloped, and among such methods, a combination method of CCA and\nindividual-template-based CCA (IT-CCA) has achieved excellent performance.\nHowever, CCA requires the canonical vectors to be orthogonal, which may not be\na reasonable assumption for EEG analysis. In the current study, we propose\nusing the correlated component analysis (CORRCA) rather than CCA to implement\nfrequency recognition. CORRCA can relax the constraint of canonical vectors in\nCCA, and generate the same projection vector for two multichannel EEG signals.\nFurthermore, we propose a two-stage method based on the basic CORRCA method\n(termed TSCORRCA). Evaluated on a benchmark dataset of thirty-five subjects,\nthe experimental results demonstrate that CORRCA significantly outperformed\nCCA, and TSCORRCA obtained the best performance among the compared methods.\nThis study demonstrates that CORRCA-based methods have great potential for\nimplementing high-performance SSVEP-based BCI systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 02:50:17 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 09:56:15 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 11:53:06 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhang", "Yangsong", ""], ["Yin", "Erwei", ""], ["Li", "Fali", ""], ["Zhang", "Yu", ""], ["Tanaka", "Toshihisa", ""], ["Zhao", "Qibin", ""], ["Cui", "Yan", ""], ["Xu", "Peng", ""], ["Yao", "Dezhong", ""], ["Guo", "Daqing", ""]]}, {"id": "1805.02890", "submitter": "Nils Berglund", "authors": "Nils Berglund and Christian Kuehn", "title": "Corrigendum to \"Regularity structures and renormalisation of\n  FitzHugh-Nagumo SPDEs in three space dimensions\"", "comments": "24 pages, corrigendum for arXiv:1504.02953, revised version", "journal-ref": "Electron. J. Probab., Volume 24 (2019), paper no. 113, 22 pp", "doi": "10.1214/19-EJP359", "report-no": null, "categories": "math.PR math-ph math.MP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lemma 4.8 in the article [Regularity structures and renormalisation of\nFitzHugh-Nagumo SPDEs in three space dimensions, Electronic J. Probability 21\n(18):1-48 (2016), arXiv:1504.02953] contains a mistake, which implies a weaker\nregularity estimate than the one stated in Proposition 4.11. This does not\naffect the proof of Theorem 2.1, but Theorems 2.2 and 2.3 only follow from the\ngiven proof if either the space dimension $d$ is equal to $2$, or the\nnonlinearity $F(U,V)$ is linear in $V$. To fix this problem and provide a proof\nof Theorems 2.2 and 2.3 valid in full generality, we consider an alternative\nformulation of the fixed-point problem, involving a modified integration\noperator with nonlocal singularity and a slightly different regularity\nstructure. We provide the multilevel Schauder estimates and\nrenormalisation-group analysis required for the fixed-point argument in this\nnew setting.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 08:25:00 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:27:55 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Berglund", "Nils", ""], ["Kuehn", "Christian", ""]]}, {"id": "1805.02995", "submitter": "Qing Hui", "authors": "Yanlin Zhou, Chen Peng, Qing Hui", "title": "A Spiking Neural Dynamical Drift-Diffusion Model on Collective Decision\n  Making with Self-Organized Criticality", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel collective decision making scheme to solve the\nmulti-agent drift-diffusion-model problem with the help of spiking neural\nnetworks. The exponential integrate-and-fire model is used here to capture the\nindividual dynamics of each agent in the system, and we name this new model as\nExponential Decision Making (EDM) model. We demonstrate analytically and\nexperimentally that the gating variable for instantaneous activation follows\nBoltzmann probability distribution, and the collective system reaches\nmeta-stable critical states under the Markov chain premises. With mean field\nanalysis, we derive the global criticality from local dynamics and achieve a\npower law distribution. Critical behavior of EDM exhibits the convergence\ndynamics of Boltzmann distribution, and we conclude that the EDM model inherits\nthe property of self-organized criticality, that the system will eventually\nevolve toward criticality.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 16:56:45 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhou", "Yanlin", ""], ["Peng", "Chen", ""], ["Hui", "Qing", ""]]}, {"id": "1805.03337", "submitter": "William Wilson", "authors": "William H. Wilson", "title": "Multi-scale metrics and self-organizing maps: a computational approach\n  to the structure of sensory maps", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the concept of a bi-scale metric for use in the\ncooperative phase of the self-organizing map (SOM) algorithm. Use of a bi-scale\nmetric allows segmentation of the map into a number of regions, corresponding\nto anticipated cluster structure in the data. Such a situation occurs, for\nexample, in the somatotopic maps which inspired the SOM algo- rithm, where\nclusters of data may correspond to body surface regions whose general structure\nis known. When a bi-scale metric is appropriately applied, issues with map\nneurons that are not activated by any point in the training data are reduced or\neliminated. The paper also presents results of simulation studies on the\nplasticity of bi-scale metric maps when they are retrained af- ter loss of\ngroups of map neurons or after changes in training data (such as would occur in\na somatotopic map when a body surface region like a finger is lost/removed).\nThe paper further considers situations where tri-scale met- rics may be useful,\nand an alternative approach suggested by neurobiology, where some map regions\nadapt more slowly to stimuli because they have a lower learning rate parameter.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 01:05:51 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Wilson", "William H.", ""]]}, {"id": "1805.03592", "submitter": "Wojciech Tarnowski", "authors": "Ewa Gudowska-Nowak, Maciej A. Nowak, Dante R. Chialvo, Jeremi K.\n  Ochab, Wojciech Tarnowski", "title": "From synaptic interactions to collective dynamics in random neuronal\n  networks models: critical role of eigenvectors and transient behavior", "comments": "25 pages + 5 pages of refs, 9 figures", "journal-ref": "Neural Computation 32(2), 395-423 (2020)", "doi": "10.1162/neco_a_01253", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of neuronal interactions is currently at the center of several big\ncollaborative neuroscience projects (including the Human Connectome Project,\nthe Blue Brain Project, the Brainome, etc.) which attempt to obtain a detailed\nmap of the entire brain. Under certain constraints, mathematical theory can\nadvance predictions of the expected neural dynamics based solely on the\nstatistical properties of the synaptic interaction matrix. This work explores\nthe application of free random variables to the study of large synaptic\ninteraction matrices. Besides recovering in a straightforward way known results\non eigenspectra in types of models of neural networks proposed by Rajan and\nAbbott, we extend them to heavy-tailed distributions of interactions. More\nimportantly, we derive analytically the behavior of eigenvector overlaps, which\ndetermine the stability of the spectra. We observe that upon imposing the\nneuronal excitation/inhibition balance, despite the eigenvalues remaining\nunchanged, their stability dramatically decreases due to the strong\nnon-orthogonality of associated eigenvectors. It leads us to the conclusion\nthat the understanding of the temporal evolution of asymmetric neural networks\nrequires considering the entangled dynamics of both eigenvectors and\neigenvalues, which might bear consequences for learning and memory processes in\nthese models. Considering the success of free random variables theory in a wide\nvariety of disciplines, we hope that the results presented here foster the\nadditional application of these ideas in the area of brain sciences.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:40:45 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 17:44:18 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gudowska-Nowak", "Ewa", ""], ["Nowak", "Maciej A.", ""], ["Chialvo", "Dante R.", ""], ["Ochab", "Jeremi K.", ""], ["Tarnowski", "Wojciech", ""]]}, {"id": "1805.03601", "submitter": "Si-Wei Qiu", "authors": "Siwei Qiu, Carson Chow", "title": "Finite size effects for spiking neural networks with spatially dependent\n  coupling", "comments": null, "journal-ref": "Phys. Rev. E 98, 062414 (2018)", "doi": "10.1103/PhysRevE.98.062414", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study finite-size fluctuations in a network of spiking deterministic\nneurons coupled with non-uniform synaptic coupling. We generalize a previously\ndeveloped theory of finite size effects for uniform globally coupled neurons.\nIn the uniform case, mean field theory is well defined by averaging over the\nnetwork as the number of neurons in the network goes to infinity. However, for\nnonuniform coupling it is no longer possible to average over the entire network\nif we are interested in fluctuations at a particular location within the\nnetwork. We show that if the coupling function approaches a continuous function\nin the infinite system size limit then an average over a local neighborhood can\nbe defined such that mean field theory is well defined for a spatially\ndependent field. We then derive a perturbation expansion in the inverse system\nsize around the mean field limit for the covariance of the input to a neuron\n(synaptic drive) and firing rate fluctuations due to dynamical deterministic\nfinite-size effects.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 16:00:31 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 15:52:54 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 19:57:40 GMT"}], "update_date": "2019-01-02", "authors_parsed": [["Qiu", "Siwei", ""], ["Chow", "Carson", ""]]}, {"id": "1805.03749", "submitter": "Catherine Davey E", "authors": "Catherine E. Davey and David B. Grayden and Anthony N. Burkitt", "title": "Emergence of radial orientation selectivity: Effect of cell density\n  changes and eccentricity in a layered network", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work by Linsker revealed how simple cells can emerge in the absence\nof structured environmental input, via a self-organisation learning process. He\nempirically showed the development of spatial-opponent cells driven only by\ninput noise, emerging as a result of structure in the initial synaptic\nconnectivity distribution. To date, a complete set of radial eigenfunctions\nhave not been provided for this multi-layer network. In this paper, the\ncomplete set of eigenfunctions and eigenvalues for a three-layered network is\nfor the first time analytically derived. Initially a simplified learning\nequation is considered for which the homeostatic parameters are set to zero. To\nextend the eigenfunction analysis to the full learning equation, including\nnon-zero homeostatic parameters, a perturbation analysis is used.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 22:43:51 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:57:22 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Davey", "Catherine E.", ""], ["Grayden", "David B.", ""], ["Burkitt", "Anthony N.", ""]]}, {"id": "1805.03792", "submitter": "Catherine Davey E", "authors": "Catherine E Davey and David B Grayden and Anthony N Burkitt", "title": "Impact of axonal delay on structure development in a multi-layered\n  network", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mechanisms underlying how activity in the visual pathway may give rise\nthrough neural plasticity to many of the features observed experimentally in\nthe early stages of visual processing was provided by Linkser in a seminal,\nthree-paper series. Owing to the complexity of multi-layer models, an implicit\nassumption in Linsker's and subsequent papers has been that propagation delay\nis homogeneous and plays little functional role in neural behaviour. We relax\nthis assumption to examine the impact of distance-dependent axonal propagation\ndelay on neural learning. We show that propagation delay induces low-pass\nfiltering by dispersing the arrival times of spikes from presynaptic neurons,\nproviding a natural correlation cancellation mechanism for distal connections.\nThe cut-off frequency decreases as the radial propagation delay within a layer\nincreases relative to propagation delay between the layers, introducing an\nupper limit on temporal resolution. Given that the PSP also acts as a low-pass\nfilter, we show that the effective time constant of each should enable the\nprocessing of similar scales of temporal information. This result has\nimplications for the visual system, in which receptive field size and, thus,\nradial propagation delay, increases with eccentricity. Furthermore, the network\nresponse is frequency dependent since higher frequencies require increased\ninput amplitude to compensate for attenuation. This concords with\nfrequency-dependent contrast sensitivity in the visual system, which changes\nwith eccentricity and receptive field size. We further show that the proportion\nof inhibition relative to excitation is larger where radial propagation delay\nis long relative to inter-laminar propagation delay. We show that the addition\nof propagation delay reduces the range in the cell's on-center size, providing\nstability to variations in homeostatic parameters.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 02:40:57 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 06:49:34 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Davey", "Catherine E", ""], ["Grayden", "David B", ""], ["Burkitt", "Anthony N", ""]]}, {"id": "1805.03886", "submitter": "Giorgio Gosti", "authors": "Viola Folli, Giorgio Gosti, Marco Leonetti, Giancarlo Ruocco", "title": "Effect of dilution in asymmetric recurrent neural networks", "comments": "31 pages, 5 figures", "journal-ref": "Folli, V., Gosti, G., Leonetti, M., Ruocco, G., Effect of dilution\n  in asymmetric recurrent neural networks. Neural Networks (2018)", "doi": "10.1016/J.NEUNET.2018.04.003", "report-no": null, "categories": "cond-mat.dis-nn cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study with numerical simulation the possible limit behaviors of\nsynchronous discrete-time deterministic recurrent neural networks composed of N\nbinary neurons as a function of a network's level of dilution and asymmetry.\nThe network dilution measures the fraction of neuron couples that are\nconnected, and the network asymmetry measures to what extent the underlying\nconnectivity matrix is asymmetric. For each given neural network, we study the\ndynamical evolution of all the different initial conditions, thus\ncharacterizing the full dynamical landscape without imposing any learning rule.\nBecause of the deterministic dynamics, each trajectory converges to an\nattractor, that can be either a fixed point or a limit cycle. These attractors\nform the set of all the possible limit behaviors of the neural network. For\neach network, we then determine the convergence times, the limit cycles'\nlength, the number of attractors, and the sizes of the attractors' basin. We\nshow that there are two network structures that maximize the number of possible\nlimit behaviors. The first optimal network structure is fully-connected and\nsymmetric. On the contrary, the second optimal network structure is highly\nsparse and asymmetric. The latter optimal is similar to what observed in\ndifferent biological neuronal circuits. These observations lead us to\nhypothesize that independently from any given learning model, an efficient and\neffective biologic network that stores a number of limit behaviors close to its\nmaximum capacity tends to develop a connectivity structure similar to one of\nthe optimal networks we found.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:41:27 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Folli", "Viola", ""], ["Gosti", "Giorgio", ""], ["Leonetti", "Marco", ""], ["Ruocco", "Giancarlo", ""]]}, {"id": "1805.03933", "submitter": "Carlos Quintero Quiroz", "authors": "C. Quintero-Quiroz, Luis Montesano, A. J. Pons, M. C. Torrent, J.\n  Garc\\'ia-Ojalvo, and C. Masoller", "title": "Differentiating resting brain states using ordinal symbolic analysis", "comments": "time series analysis, ordinal analysis, EEG, brain dynamics", "journal-ref": null, "doi": "10.1063/1.5036959", "report-no": null, "categories": "physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic methods of analysis are valuable tools for investigating complex\ntime-dependent signals. In particular, the ordinal method defines sequences of\nsymbols according to the ordering in which values appear in a time series. This\nmethod has been shown to yield useful information, even when applied to signals\nwith large noise contamination. Here we use ordinal analysis to investigate the\ntransition between eyes closed (EC) and eyes open (EO) resting states. We\nanalyze two {EEG} datasets (with 71 and 109 healthy subjects) with different\nrecording conditions (sampling rates and the number of electrodes in the\nscalp). Using as diagnostic tools the permutation entropy, the entropy computed\nfrom symbolic transition probabilities, and an asymmetry coefficient (that\nmeasures the asymmetry of the likelihood of the transitions between symbols) we\nshow that ordinal analysis applied to the raw data distinguishes the two brain\nstates. In both datasets, we find that the EO state is characterized by higher\nentropies and lower asymmetry coefficient, as compared to the EC state. Our\nresults thus show that these diagnostic tools have the potential for detecting\nand characterizing changes in time-evolving brain states.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 12:16:00 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Quintero-Quiroz", "C.", ""], ["Montesano", "Luis", ""], ["Pons", "A. J.", ""], ["Torrent", "M. C.", ""], ["Garc\u00eda-Ojalvo", "J.", ""], ["Masoller", "C.", ""]]}, {"id": "1805.04157", "submitter": "Stephen Bonner", "authors": "Nik Khadijah Nik Aznan, Stephen Bonner, Jason D. Connolly, Noura Al\n  Moubayed, Toby P. Breckon", "title": "On the Classification of SSVEP-Based Dry-EEG Signals via Convolutional\n  Neural Networks", "comments": "Accepted as a full paper at the 2018 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC2018)", "journal-ref": null, "doi": "10.1109/SMC.2018.00631", "report-no": null, "categories": "cs.HC eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Convolutional Neural Network (CNN) approach\nfor the classification of raw dry-EEG signals without any data pre-processing.\nTo illustrate the effectiveness of our approach, we utilise the Steady State\nVisual Evoked Potential (SSVEP) paradigm as our use case. SSVEP can be utilised\nto allow people with severe physical disabilities such as Complete Locked-In\nSyndrome or Amyotrophic Lateral Sclerosis to be aided via BCI applications, as\nit requires only the subject to fixate upon the sensory stimuli of interest.\nHere we utilise SSVEP flicker frequencies between 10 to 30 Hz, which we record\nas subject cortical waveforms via the dry-EEG headset. Our proposed end-to-end\nCNN allows us to automatically and accurately classify SSVEP stimulation\ndirectly from the dry-EEG waveforms. Our CNN architecture utilises a common\nSSVEP Convolutional Unit (SCU), comprising of a 1D convolutional layer, batch\nnormalization and max pooling. Furthermore, we compare several deep learning\nneural network variants with our primary CNN architecture, in addition to\ntraditional machine learning classification approaches. Experimental evaluation\nshows our CNN architecture to be significantly better than competing\napproaches, achieving a classification accuracy of 96% whilst demonstrating\nsuperior cross-subject performance and even being able to generalise well to\nunseen subjects whose data is entirely absent from the training process.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:10:02 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 09:47:21 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Aznan", "Nik Khadijah Nik", ""], ["Bonner", "Stephen", ""], ["Connolly", "Jason D.", ""], ["Moubayed", "Noura Al", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1805.04329", "submitter": "Matteo Cinelli", "authors": "M. Cinelli, and I. Echegoyen, and M. Oliveira, and S. Orellana, and T.\n  Gili", "title": "Altered Modularity and Disproportional Integration in Functional\n  Networks are Markers of Abnormal Brain Organization in Schizophrenia", "comments": "This work is the output of the Complexity72h workshop, held at IMT\n  School in Lucca, 7-11 May 2018. https://complexity72h.weebly.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity plays an important role in brain networks' architecture and\ninfluences its dynamics and the ability to integrate and segregate different\nmodules of cerebral regions. Alterations in community structure are associated\nwith several clinical disorders, specially schizophrenia, although its time\nevolution is not clear yet. In the present work, we analyze fMRI functional\nnetworks of $65$ healthy subjects (HC) and $44$ patients of schizophrenia (SZ),\n$28$ of them in a chronic state (CR) of illness, and $16$ at early stage (ES).\nWe find clear differences in edges' weights distribution, networks density,\ncommunity structure consistency and robustness against edge removal. In\ncomparison to healthy subjects, we found that networks from SZ patients\nexhibits wider weight distribution, larger overall connectivity, and are more\nconsistent in the community structure across subjects. We also showed that the\nnetworks of SZ patients tend to be more robust to edge removal than healthy\nsubjects, while having lower network density. In the case of early stages\npatients, we found that their networks exhibit topological features\nconsistently in between the ones obtained from the other two groups, resulting\nin a tendency towards the chronic group state.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 11:25:30 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Cinelli", "M.", ""], ["Echegoyen", "I.", ""], ["Oliveira", "M.", ""], ["Orellana", "S.", ""], ["Gili", "T.", ""]]}, {"id": "1805.04455", "submitter": "Sergio Floquet", "authors": "A J da Silva, S Floquet and R F Lima", "title": "On the distribution of spontaneous potentials intervals in nervous\n  transmission", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in Biophysics teaching consists on how to motivate\nstudents to appreciate the beauty of theoretical formulations. This is crucial\nwhen the system modeling requires numerical calculations to achieve realistic\nresults. In this sense, due to the massive use of software, students often\nbecome a mere users of computational tools without capturing the essence of\nformulation and further problem solution. It is, therefore, necessary for\ninstructors to find innovating ways, allowing students developing of their\nability to deal with mathematical modelling. To address this issue one can\nhighlight the use of Benford's law, thanks to its simple formulation, easy\ncomputational implementation and wide possibility for applications. Indeed,\nthis law enables students to carry out their own data analysis with use of free\nsoftware packages. This law is among the several power or scaling laws found in\nbiological systems. However, to the best of our knowledge, this law has not\nbeen contemplated in Cell Biophysics yet. Beyond its vast applications in many\nfields, neuromuscular junction represents a remarkable substrate for learning\nand teaching of complex system. Thus, in this work, we applied both classical\nand a generalized form of Benford's Law, to examine if electrophysiological\ndata recorded from neuromuscular junction conforms the anomalous number law.\nThe results indicated that nerve-muscle communications conform the generalized\nBenford's law better than the seminal formulation. From our\nelectrophysiological measurements a biological scenario is used to interpret\nthe theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 15:45:40 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["da Silva", "A J", ""], ["Floquet", "S", ""], ["Lima", "R F", ""]]}, {"id": "1805.04566", "submitter": "Zachary Vesoulis", "authors": "Z.A. Vesoulis, P.G. Gamble, S. Jain, N.M. El Ters, S.M. Liao and A.M.\n  Mathur", "title": "WU-NEAT: A clinically validated, open- source MATLAB toolbox for\n  limited-channel neonatal EEG analysis", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: Limited-channel EEG research in neonates is hindered by lack of open,\naccessible analytic tools. To overcome this limitation, we have created the\nWashington University- Neonatal EEG Analysis Toolbox (WU-NEAT), containing two\nof the most commonly used tools, provided in an open-source,\nclinically-validated package running within MATLAB. Methods: The first\nalgorithm is the amplitude-integrated EEG (aEEG), which is generated by\nfiltering, rectifying and time-compressing the original EEG recording, with\nsubsequent semi-logarithmic display. The second algorithm is the spectral edge\nfrequency (SEF), calculated as the critical frequency below which a user-\ndefined proportion of the EEG spectral power is located. The aEEG algorithm was\nvalidated by three experienced reviewers. Reviewers evaluated aEEG recordings\nof fourteen preterm/term infants, displayed twice in random order, once using a\nreference algorithm and again using the WU-NEAT aEEG algorithm. Using standard\nmethodology, reviewers assigned a background pattern classification.\nInter/intra-rater reliability was assessed. For the SEF, calculations were made\nusing the same fourteen recordings, first with the reference and then with the\nWU-NEAT algorithm. Results were compared using Pearson's correlation\ncoefficient. Results: For the aEEG algorithm, intra- and inter-rater\nreliability was 100% and 98%, respectively. For the SEF, the mean (SD) Pearson\ncorrelation coefficient between algorithms was 0.96 (0.04). Conclusion: We have\ndemonstrated a clinically-validated toolbox for generating the aEEG as well as\ncalculating the SEF from EEG data. Open-source access will enable widespread\nuse of common analytic algorithms which are device-independent and not subject\nto obsolescence, thereby facilitating future collaborative research in neonatal\nEEG.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 19:17:34 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Vesoulis", "Z. A.", ""], ["Gamble", "P. G.", ""], ["Jain", "S.", ""], ["Ters", "N. M. El", ""], ["Liao", "S. M.", ""], ["Mathur", "A. M.", ""]]}, {"id": "1805.04619", "submitter": "Stevan Harnad", "authors": "Fernanda P\\'erez-Gay, Tomy Sicotte, Christian Th\\'eriault, Stevan\n  Harnad", "title": "Category learning can alter perception and its neural correlates", "comments": "40 pages, 15 figures, 8 tables", "journal-ref": "PLOS ONE 14(12): e0226000 (2019)", "doi": "10.1371/journal.pone.0226000", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned Categorical Perception (CP) occurs when the members of different\ncategories come to look more dissimilar (between-category separation) and/or\nmembers of the same category come to look more similar (within-category\ncompression) after a new category has been learned. To measure learned CP and\nits physiological correlates we compared dissimilarity judgments and Event\nRelated Potentials (ERPs) before and after learning to sort multi-featured\nvisual textures into two categories by trial and error with corrective\nfeedback. With the same number of trials and feedback, about half the\nparticipants succeeded in learning the categories (learners: criterion 80%\naccuracy) and the rest did not (non-learners). At both lower and higher levels\nof difficulty, successful learners showed significant between-category\nseparation in pairwise dissimilarity judgments after learning compared to\nbefore; their late parietal ERP positivity (LPC, usually interpreted as\ndecisional) also increased and their occipital negativity (N1) (usually\ninterpreted as perceptual) decreased. LPC increased with response accuracy and\nN1 amplitude decreased with between-category separation for the Learners.\nNon-learners showed no significant changes in dissimilarity judgments, LPC or\nN1, within or between categories. This is behavioral and physiological evidence\nthat category learning can alter perception. We sketch a neural net model for\nthis effect.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 23:44:01 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 22:20:51 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 22:54:05 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["P\u00e9rez-Gay", "Fernanda", ""], ["Sicotte", "Tomy", ""], ["Th\u00e9riault", "Christian", ""], ["Harnad", "Stevan", ""]]}, {"id": "1805.04975", "submitter": "Brian Lee", "authors": "Brian C. Lee, Meng Kuan Lin, Yan Fu, Junichi Hata, Michael I. Miller,\n  Partha P. Mitra", "title": "Multimodal Cross-registration and Quantification of Metric Distortions\n  in Whole Brain Histology of Marmoset using Diffeomorphic Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain neuroanatomy using tera-voxel light-microscopic data sets is of\nmuch current interest. A fundamental problem in this field is the mapping of\nindividual brain data sets to a reference space. Previous work has not\nrigorously quantified the distortions in brain geometry from in-vivo to ex-vivo\nbrains due to the tissue processing, which will be important when computing\nproperties such as local cell and process densities at the voxel level in\ncreating reference brain maps. Further, existing approaches focus on\nregistering uni-modal volumetric data; however, given the increasing interest\nin the marmoset model for neuroscience research, it is necessary to\ncross-register multi-modal data sets including MRIs and multiple histological\nseries that can help address individual variations in brain architecture. Here\nwe present a computational approach for same-subject multimodal MRI guided\nreconstruction of a histological series, jointly with diffeomorphic mapping to\na reference atlas. We quantify the scale change during the different stages of\nhistological processing of the brains using the Jacobian determinant of the\ndiffeomorphic transformations involved. There are two major steps in the\nhistology process with associated scale distortions (a) brain perfusion (b)\nhistological sectioning and reassembly. By mapping the final image stacks to\nthe ex-vivo post fixation MRI, we show that tape-transfer histology can be\nreassembled accurately into 3D volumes with a local scale change of 2.0 $\\pm$\n0.4% per axis dimension. In contrast, the perfusion step, as assessed by\nmapping the in-vivo MRIs to the ex-vivo post fixation MRIs, shows a larger\nlocal scale change of 6.9 $\\pm$ 2.1% per axis dimension. This is the first\nsystematic quantification of the local metric distortions associated with\nwhole-brain histological processing, and we expect that the results will\ngeneralize to other species.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 00:36:28 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:26:23 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lee", "Brian C.", ""], ["Lin", "Meng Kuan", ""], ["Fu", "Yan", ""], ["Hata", "Junichi", ""], ["Miller", "Michael I.", ""], ["Mitra", "Partha P.", ""]]}, {"id": "1805.05036", "submitter": "Martin L\\\"angkvist", "authors": "Martin L\\\"angkvist and Amy Loutfi", "title": "A Deep Learning Approach with an Attention Mechanism for Automatic Sleep\n  Stage Classification", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic sleep staging is a challenging problem and state-of-the-art\nalgorithms have not yet reached satisfactory performance to be used instead of\nmanual scoring by a sleep technician. Much research has been done to find good\nfeature representations that extract the useful information to correctly\nclassify each epoch into the correct sleep stage. While many useful features\nhave been discovered, the amount of features have grown to an extent that a\nfeature reduction step is necessary in order to avoid the curse of\ndimensionality. One reason for the need of such a large feature set is that\nmany features are good for discriminating only one of the sleep stages and are\nless informative during other stages. This paper explores how a second feature\nrepresentation over a large set of pre-defined features can be learned using an\nauto-encoder with a selective attention for the current sleep stage in the\ntraining batch. This selective attention allows the model to learn feature\nrepresentations that focuses on the more relevant inputs without having to\nperform any dimensionality reduction of the input data. The performance of the\nproposed algorithm is evaluated on a large data set of polysomnography (PSG)\nnight recordings of patients with sleep-disordered breathing. The performance\nof the auto-encoder with selective attention is compared with a regular\nauto-encoder and previous works using a deep belief network (DBN).\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 07:36:26 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["L\u00e4ngkvist", "Martin", ""], ["Loutfi", "Amy", ""]]}, {"id": "1805.05303", "submitter": "David Papo", "authors": "David Papo", "title": "Neurofeedback: principles, appraisal and outstanding issues", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurofeedback is a form of brain training in which subjects are fed back\ninformation about some measure of their brain activity which they are\ninstructed to modify in a way thought to be functionally advantageous. Over the\nlast twenty years, NF has been used to treat various neurological and\npsychiatric conditions, and to improve cognitive function in various contexts.\nHowever, despite its growing popularity, each of the main steps in NF comes\nwith its own set of often covert assumptions. Here we critically examine some\nconceptual and methodological issues associated with the way general objectives\nand neural targets of NF are defined, and review the neural mechanisms through\nwhich NF may act, and the way its efficacy is gauged. The NF process is\ncharacterised in terms of functional dynamics, and possible ways in which it\nmay be controlled are discussed. Finally, it is proposed that improving NF will\nrequire better understanding of various fundamental aspects of brain dynamics\nand a more precise definition of functional brain activity and brain-behaviour\nrelationships.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 17:24:42 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Papo", "David", ""]]}, {"id": "1805.05682", "submitter": "Begona Diaz", "authors": "Begona Diaz, Helen Blank, and Katharina von Kriegstein", "title": "Task-dependent modulation of the visual sensory thalamus assists\n  visual-speech recognition", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2018.05.032", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The cerebral cortex modulates early sensory processing via feed-back\nconnections to sensory pathway nuclei. The functions of this top-down\nmodulation for human behavior are poorly understood. Here, we show that\ntop-down modulation of the visual sensory thalamus (the lateral geniculate\nbody, LGN) is involved in visual-speech recognition. In two independent\nfunctional magnetic resonance imaging (fMRI) studies, LGN response increased\nwhen participants processed fast-varying features of articulatory movements\nrequired for visual-speech recognition, as compared to temporally more stable\nfeatures required for face identification with the same stimulus material. The\nLGN response during the visual-speech task correlated positively with the\nvisual-speech recognition scores across participants. In addition, the\ntask-dependent modulation was present for speech movements and did not occur\nfor control conditions involving non-speech biological movements. In\nface-to-face communication, visual speech recognition is used to enhance or\neven enable understanding what is said. Speech recognition is commonly\nexplained in frameworks focusing on cerebral cortex areas. Our findings suggest\nthat task-dependent modulation at subcortical sensory stages has an important\nrole for communication: Together with similar findings in the auditory modality\nthe findings imply that task-dependent modulation of the sensory thalami is a\ngeneral mechanism to optimize speech recognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 10:10:59 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 15:32:48 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Diaz", "Begona", ""], ["Blank", "Helen", ""], ["von Kriegstein", "Katharina", ""]]}, {"id": "1805.05696", "submitter": "Luka Ribar", "authors": "Luka Ribar, Rodolphe Sepulchre", "title": "Neuromodulation of Neuromorphic Circuits", "comments": null, "journal-ref": "IEEE Transactions on Circuits and Systems I: Regular Papers, vol.\n  66, no. 8, pp. 3028-3040, Aug. 2019", "doi": "10.1109/TCSI.2019.2907113", "report-no": null, "categories": "q-bio.NC cs.NE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology to enable control of a neuromorphic circuit in\nclose analogy with the physiological neuromodulation of a single neuron. The\nmethodology is general in that it only relies on a parallel interconnection of\nelementary voltage-controlled current sources. In contrast to controlling a\nnonlinear circuit through the parameter tuning of a state-space model, our\napproach is purely input-output. The circuit elements are controlled and\ninterconnected to shape the current-voltage characteristics (I-V curves) of the\ncircuit in prescribed timescales. In turn, shaping those I-V curves determines\nthe excitability properties of the circuit. We show that this methodology\nenables both robust and accurate control of the circuit behavior and resembles\nthe biophysical mechanisms of neuromodulation. As a proof of concept, we\nsimulate a SPICE model composed of MOSFET transconductance amplifiers operating\nin the weak inversion regime.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 10:43:18 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 14:13:15 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 19:49:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ribar", "Luka", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1805.06336", "submitter": "Hamdan Awan", "authors": "Hamdan Awan, Raviraj S. Adve, Nigel Wallbridge, Carrol Plummer and\n  Andrew W. Eckford", "title": "Characterizing Information Propagation in Plants", "comments": "6 pages, 5 Figures, Submitted to IEEE Conference, 2018", "journal-ref": null, "doi": "10.1109/GLOCOM.2018.8647210", "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an electro-chemical based communication model for\nintercellular communication in plants. Many plants, such as Mimosa pudica (the\n\"sensitive plant\"), employ electrochemical signals known as action potentials\n(APs) for communication purposes. In this paper we present a simple model for\naction potential generation. We make use of the concepts from molecular\ncommunication to explain the underlying process of information transfer in a\nplant. Using the information-theoretic analysis, we compute the mutual\ninformation between the input and output in this work. The key aim is to study\nthe variations in the information propagation speed for varying number of plant\ncells for one simple case. Furthermore we study the impact of the AP signal on\nthe mutual information and information propagation speed. We aim to explore\nfurther that how the growth rate in plants can impact the information transfer\nrate and vice versa.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 12:52:50 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Awan", "Hamdan", ""], ["Adve", "Raviraj S.", ""], ["Wallbridge", "Nigel", ""], ["Plummer", "Carrol", ""], ["Eckford", "Andrew W.", ""]]}, {"id": "1805.07061", "submitter": "Yu Terada", "authors": "Yu Terada, Tomoyuki Obuchi, Takuya Isomura, Yoshiyuki Kabashima", "title": "Objective and efficient inference for couplings in neuronal networks", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/ab3219", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring directional couplings from the spike data of networks is desired in\nvarious scientific fields such as neuroscience. Here, we apply a recently\nproposed objective procedure to the spike data obtained from the\nHodgkin--Huxley type models and in vitro neuronal networks cultured in a\ncircular structure. As a result, we succeed in reconstructing synaptic\nconnections accurately from the evoked activity as well as the spontaneous one.\nTo obtain the results, we invent an analytic formula approximately implementing\na method of screening relevant couplings. This significantly reduces the\ncomputational cost of the screening method employed in the proposed objective\nprocedure, making it possible to treat large-size systems as in this study.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 06:19:37 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Terada", "Yu", ""], ["Obuchi", "Tomoyuki", ""], ["Isomura", "Takuya", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1805.07322", "submitter": "Rodrigo Cofre", "authors": "Rodrigo Cofre, Cesar Maldonado, Fernando Rosas", "title": "Large Deviations Properties of Maximum Entropy Markov Chains from Spike\n  Trains", "comments": null, "journal-ref": null, "doi": "10.3390/e20080573", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum entropy Markov chain inference approach to\ncharacterize the collective statistics of neuronal spike trains, focusing on\nthe statistical properties of the inferred model. We review large deviations\ntechniques useful in this context to describe properties of accuracy and\nconvergence in terms of sampling size. We use these results to study the\nstatistical fluctuation of correlations, distinguishability and irreversibility\nof maximum entropy Markov chains. We illustrate these applications using simple\nexamples where the large deviation rate function is explicitly obtained for\nmaximum entropy models of relevance in this field.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 16:43:10 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Cofre", "Rodrigo", ""], ["Maldonado", "Cesar", ""], ["Rosas", "Fernando", ""]]}, {"id": "1805.07343", "submitter": "Rommel Salas", "authors": "Cesar Rommel Salas", "title": "The impact of binaural white noise with oscillations of 100 to 750hz in\n  the short-term visual working memory and the reactivity of alpha and beta\n  cerebral waves", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to some researchers, noise is typically conceived as a detrimental\nfactor in cognitive performance affecting perception, decision making, and\nmotor function. However, in recent studies it is associated with white noise\nwith concentration and calm, therefore, this research seeks to establish the\nimpact of binaural white noise on the performance of short-term visual and\nworking memory, the alpha - beta brain activity, attention - meditation,\nthrough the use of two auditory stimuli with frequency ranges of (100 to 450hz)\nand (100 to 750hz). This study was conducted in the city of Montes Claros, the\nRepublic of Brazil, where seven participants were evaluated (n = 7) with an\naverage age of 36.71, and two age groups (GP1) 21 to 30 and (GP2) 41 50, with\nuniversity studies. Within the experimental process, the short-term visual\nmemory tests were performed using the cognitive assessment battery CAB of\nCogniFit, and the recording of brain activities through the use of monopolar\nelectroencephalogram and the eSense algorithms. With the results obtained and\nthrough the use of statistical tests, we can infer that the binaural white\nnoise with oscillations of 100 to 750 Hz contributed to the performance of\nvisual work memory in the short term\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 13:57:41 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Salas", "Cesar Rommel", ""]]}, {"id": "1805.07528", "submitter": "Shimon Marom", "authors": "Hillel Ori, Eve Marder, Shimon Marom", "title": "Cellular function given parametric variation: excitability in the\n  Hodgkin-Huxley model", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences 2018", "doi": "10.1073/pnas.1808552115", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How is reliable physiological function maintained in cells despite\nconsiderable variability in the values of key parameters of multiple\ninteracting processes that govern that function? Here we use the classic\nHodgkin-Huxley formulation of the squid giant axon action potential to propose\na possible approach to this problem. Although the full Hodgkin-Huxley model is\nvery sensitive to fluctuations that independently occur in its many parameters,\nthe outcome is in fact determined by simple combinations of these parameters\nalong two physiological dimensions: Structural and Kinetic (denoted $S$ and\n$K$). Structural parameters describe the properties of the cell, including its\ncapacitance and the densities of its ion channels. Kinetic parameters are those\nthat describe the opening and closing of the voltage-dependent conductances.\nThe impacts of parametric fluctuations on the dynamics of the system, seemingly\ncomplex in the high dimensional representation of the Hodgkin-Huxley model, are\ntractable when examined within the $S-K$ plane. We demonstrate that slow\ninactivation, a ubiquitous activity-dependent feature of ionic channels, is a\npowerful local homeostatic control mechanism that stabilizes excitability amid\nchanges in structural and kinetic parameters.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 07:10:56 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Ori", "Hillel", ""], ["Marder", "Eve", ""], ["Marom", "Shimon", ""]]}, {"id": "1805.07569", "submitter": "Hannes Rapp", "authors": "Hannes Rapp, Martin Paul Nawrot, Merav Stern", "title": "Reliable counting of weakly labeled concepts by a single spiking neuron\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making an informed, correct and quick decision can be life-saving. It's\ncrucial for animals during an escape behaviour or for autonomous cars during\ndriving. The decision can be complex and may involve an assessment of the\namount of threats present and the nature of each threat. Thus, we should expect\nearly sensory processing to supply classification information fast and\naccurately, even before relying the information to higher brain areas or more\ncomplex system components downstream. Today, advanced convolutional artificial\nneural networks can successfully solve visual detection and classification\ntasks and are commonly used to build complex decision making systems. However,\nin order to perform well on these tasks they require increasingly complex,\n\"very deep\" model structure, which is costly in inference run-time, energy\nconsumption and number of training samples, only trainable on cloud-computing\nclusters. A single spiking neuron has been shown to be able to solve\nrecognition tasks for homogeneous Poisson input statistics, a commonly used\nmodel for spiking activity in the neocortex. When modeled as leaky integrate\nand fire with gradient decent learning algorithm it was shown to posses a\nvariety of complex computational capabilities. Here we improve its\nimplementation. We also account for more natural stimulus generated inputs that\ndeviate from this homogeneous Poisson spiking. The improved gradient-based\nlocal learning rule allows for significantly better and stable generalization.\nWe also show that with its improved capabilities it can count weakly labeled\nconcepts by applying our model to a problem of multiple instance learning (MIL)\nwith counting where labels are only available for collections of concepts. In\nthis counting MNIST task the neuron exploits the improved implementation and\noutperforms conventional ConvNet architecture under similar condtions.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 11:09:27 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 09:09:15 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Rapp", "Hannes", ""], ["Nawrot", "Martin Paul", ""], ["Stern", "Merav", ""]]}, {"id": "1805.07661", "submitter": "Mauricio Girardi-Schappo", "authors": "Mauricio Girardi-Schappo and Marcelo H. R. Tragtenberg", "title": "Measuring neuronal avalanches in disordered systems with absorbing\n  states", "comments": "21 pages, 7 figures", "journal-ref": "Phys. Rev. E 97, 042415, 2018", "doi": "10.1103/PhysRevE.97.042415", "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn cond-mat.stat-mech nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law-shaped avalanche-size distributions are widely used to probe for\ncritical behavior in many different systems, particularly in neural networks.\nThe definition of avalanche is ambiguous. Usually, theoretical avalanches are\ndefined as the activity between a stimulus and the relaxation to an inactive\nabsorbing state. On the other hand, experimental neuronal avalanches are\ndefined by the activity between consecutive silent states. We claim that the\nlatter definition may be extended to some theoretical models to characterize\ntheir power-law avalanches and critical behavior. We study a system in which\nthe separation of driving and relaxation time scales emerges from its\nstructure. We apply both definitions of avalanche to our model. Both yield\npower-law-distributed avalanches that scale with system size in the critical\npoint as expected. Nevertheless, we find restricted power-law-distributed\navalanches outside of the critical region within the experimental procedure,\nwhich is not expected by the standard theoretical definition. We remark that\nthese results are dependent on the model details.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 21:53:53 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Girardi-Schappo", "Mauricio", ""], ["Tragtenberg", "Marcelo H. R.", ""]]}, {"id": "1805.08091", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley, Stephen Grossberg", "title": "Neural computation of surface border ownership and relative surface\n  depth from ambiguous contrast inputs", "comments": null, "journal-ref": "2016, Frontiers in Psychology, 7, 1102", "doi": "10.3389/fpsyg.2016.01102", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segregation of image parts into foreground and background is an important\naspect of the neural computation of 3D scene perception. To achieve such\nsegregation, the brain needs information about border ownership; that is, the\nbelongingness of a contour to a specific surface represented in the image. This\narticle presents psychophysical data derived from 3D percepts of figure and\nground that were generated by presenting 2D images composed of spatially\ndisjoint shapes that pointed inward or outward relative to the continuous\nboundaries that they induced along their collinear edges. The shapes in some\nimages had the same contrast (black or white) with respect to the background\ngray. Other images included opposite contrasts along each induced continuous\nboundary. Psychophysical results show that figure vs ground judgment\nprobabilities in response to these ambiguous displays are determined by the\norientation of contrasts only, not by their relative contrasts, despite the\nfact that many border ownership cells in cortical area V2 respond to a\npreferred relative contrast. The FACADE and 3D LAMINART models are used to\nexplain these data.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 14:47:49 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Grossberg", "Stephen", ""]]}, {"id": "1805.08239", "submitter": "Joshua Glaser", "authors": "Joshua I. Glaser, Ari S. Benjamin, Roozbeh Farhoodi, Konrad P. Kording", "title": "The Roles of Supervised Machine Learning in Systems Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the last several years, the use of machine learning (ML) in neuroscience\nhas been rapidly increasing. Here, we review ML's contributions, both realized\nand potential, across several areas of systems neuroscience. We describe four\nprimary roles of ML within neuroscience: 1) creating solutions to engineering\nproblems, 2) identifying predictive variables, 3) setting benchmarks for simple\nmodels of the brain, and 4) serving itself as a model for the brain. The\nbreadth and ease of its applicability suggests that machine learning should be\nin the toolbox of most systems neuroscientists.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:11:26 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 15:36:21 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Glaser", "Joshua I.", ""], ["Benjamin", "Ari S.", ""], ["Farhoodi", "Roozbeh", ""], ["Kording", "Konrad P.", ""]]}, {"id": "1805.08286", "submitter": "Michael Vaiana", "authors": "Michael Vaiana and Sarah F. Muldoon", "title": "Optimizing state change detection in functional temporal networks\n  through dynamic community detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic community detection provides a coherent description of network\nclusters over time, allowing one to track the growth and death of communities\nas the network evolves. However, modularity maximization, a popular method for\nperforming multilayer community detection, requires the specification of an\nappropriate null model as well as resolution and interlayer coupling\nparameters. Importantly, the ability of the algorithm to accurately detect\ncommunity evolution is dependent on the choice of these parameters. In\nfunctional temporal networks, where evolving communities reflect changing\nfunctional relationships between network nodes, it is especially important that\nthe detected communities reflect any state changes of the system. Here, we\npresent analytical work suggesting that a uniform null model provides improved\nsensitivity to the detection of small evolving communities in temporal\ncorrelation networks. We then propose a method for increasing the sensitivity\nof modularity maximization to state changes in nodal dynamics by modeling\nself-identity links between layers based on the self-similarity of the network\nnodes between layers. This method is more appropriate for functional temporal\nnetworks from both a modeling and mathematical perspective, as it incorporates\nthe dynamic nature of network nodes. We motivate our method based on\napplications in neuroscience where network nodes represent neurons and\nfunctional edges represent similarity of firing patterns in time. Finally, we\nshow that in simulated data sets of neuronal spike trains, updating interlayer\nlinks based on the firing properties of the neurons provides superior community\ndetection of evolving network structure when group of neurons change their\nfiring properties over time.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:46:14 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Vaiana", "Michael", ""], ["Muldoon", "Sarah F.", ""]]}, {"id": "1805.08626", "submitter": "Stefano De Blasi", "authors": "Stefano De Blasi", "title": "Simulation of Large Scale Neural Networks for Evaluation Applications", "comments": "Poster 2018, Prague May 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the complexity of biological neural networks like the human\nbrain is one of the scientific challenges of our century. The organization of\nthe brain can be described at different levels, ranging from small neural\nnetworks to entire brain regions. Existing methods for the description of\nfunctionally or effective connectivity are based on the analysis of relations\nbetween the activities of different neural units by detecting correlations or\ninformation flow. This is a crucial step in understanding neural disorders like\nAlzheimers disease and their causative factors. To evaluate these estimation\nmethods, it is necessary to refer to a neural network with known connectivity,\nwhich is typically unknown for natural biological neural networks. Therefore,\nnetwork simulations, also in silico, are available. In this work, the in silico\nsimulation of large scale neural networks is established and the influence of\ndifferent topologies on the generated patterns of neuronal signals is\ninvestigated. The goal is to develop standard evaluation methods for\nneurocomputational algorithms with a realistic large scale model to enable\nbenchmarking and comparability of different studies.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 15:03:04 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["De Blasi", "Stefano", ""]]}, {"id": "1805.08646", "submitter": "Fanny Grosselin", "authors": "Fanny Grosselin, Xavier Navarro-Sune, Mathieu Raux, Thomas Similowski,\n  Mario Chavez", "title": "CARE-rCortex: a Matlab toolbox for the analysis of\n  CArdio-REspiratory-related activity in the Cortex", "comments": "This manuscript contains 13 pages including 7 color figures", "journal-ref": "J Neurosci Methods. 2018 Aug 13. pii: S0165-0270(18)30247-4. doi:\n  10.1016/j.jneumeth.2018.08.011", "doi": "10.1016/j.jneumeth.2018.08.011", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Although cardio-respiratory (CR) system is generally controlled\nby the autonomic nervous system, interactions between the cortex and these\nprimary functions are receiving an increasing interest in neurosciences. New\nmethod: In general, the timing of such internally paced events (e.g. heartbeats\nor respiratory cycles) may display a large variability. For the analysis of\nsuch CR event-related EEG potentials, a baseline must be correctly associated\nto each cycle of detected events. The open-source toolbox CARE-rCortex provides\nan easy-to-use interface to detect CR events, define baselines, and analyse in\ntime-frequency (TF) domain the CR-based EEG potentials. Results: CARE-rCortex\nprovides some practical tools to detect and validate these CR events. Users can\ndefine baselines time-locked to a phase of respiratory or heart cycle. A\nstatistical test has also been integrated to highlight significant points of\nthe TF maps with respect to the baseline. We illustrate the use of CARE-rCortex\nwith the analysis of two real cardio-respiratory datasets. Comparison with\nexisting methods: Compared to other open-source toolboxes, CARE-rCortex allows\nusers to automatically detect CR events, to define and check baselines for each\ndetected event. Different baseline normalizations can be used in the TF\nanalysis of EEG epochs. Conclusions: The analysis of CR-related EEG activities\ncould provide valuable information about cognitive or pathological brain\nstates. CARE-rCortex runs in Matlab as a plug-in of the EEGLAB software, and it\nis publicly available at https://github.com/FannyGrosselin/CARE-rCortex.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 14:54:03 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 10:52:01 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 09:08:41 GMT"}, {"version": "v4", "created": "Fri, 12 Oct 2018 13:01:49 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Grosselin", "Fanny", ""], ["Navarro-Sune", "Xavier", ""], ["Raux", "Mathieu", ""], ["Similowski", "Thomas", ""], ["Chavez", "Mario", ""]]}, {"id": "1805.08658", "submitter": "Mahmoud Haroun", "authors": "Mahmoud Haroun and Mohamed Salah", "title": "Improving Motor Imagery Based Brain Computer Interfaces Using A Novel\n  Physical Feedback Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, and through an understanding of neuronal system\ncommunication, A novel model serves as an assistive technology for locked-in\npeople suffering from Motor neuronal disease (MND) is proposed. Work was done\nupon the potential of brain wave activity patterns to be detected as electrical\nsignals, classified and translated into commands following Brain Computer\nInterfaces (BCI) constructing paradigm. However, the interface constructed was\nfor the first time a device which can reconstruct this command physically. The\nproject novelty is in the feedback step, where an electromagnets magnetic field\nis used to showcase the command in ferrofluid droplets movement- these moved to\nassigned targets due to rotation of a glass surface desk according to the data\nreceived from the brain. The goal of this project is to address the challenges\nof the inaccurate performance in user-training which is yet the main issues\npreventing BCI from being upgraded into more applicable technology. Tests were\nperformed based on Open ViBE software after uploading recorded files of Motor\nImagery MI tasks and the design requirements tested were the motion speed of\nthe droplet and accuracy of hitting fixed targets. An average speed of 0.469\ncm/s and average accuracy of 81.6% were obtained from the best volume for the\ndroplet. A conclusion to be drawn was that the promise of this other point of\nview on BCI systems to be more Brain-Real World Systems\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 21:58:00 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 21:20:51 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Haroun", "Mahmoud", ""], ["Salah", "Mohamed", ""]]}, {"id": "1805.08889", "submitter": "David Clark", "authors": "David G. Clark, Jesse A. Livezey, Edward F. Chang, Kristofer E.\n  Bouchard", "title": "Spiking Linear Dynamical Systems on Neuromorphic Hardware for Low-Power\n  Brain-Machine Interfaces", "comments": "23 pages, 8 figures; added reference, removed typo in Fig. 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic architectures achieve low-power operation by using many simple\nspiking neurons in lieu of traditional hardware. Here, we develop methods for\nprecise linear computations in spiking neural networks and use these methods to\nmap the evolution of a linear dynamical system (LDS) onto an existing\nneuromorphic chip: IBM's TrueNorth. We analytically characterize, and\nnumerically validate, the discrepancy between the spiking LDS state sequence\nand that of its non-spiking counterpart. These analytical results shed light on\nthe multiway tradeoff between time, space, energy, and accuracy in neuromorphic\ncomputation. To demonstrate the utility of our work, we implemented a\nneuromorphic Kalman filter (KF) and used it for offline decoding of human vocal\npitch from neural data. The neuromorphic KF could be used for low-power\nfiltering in domains beyond neuroscience, such as navigation or robotics.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 22:34:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 23:37:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Clark", "David G.", ""], ["Livezey", "Jesse A.", ""], ["Chang", "Edward F.", ""], ["Bouchard", "Kristofer E.", ""]]}, {"id": "1805.09001", "submitter": "Sizhong Lan", "authors": "Sizhong Lan", "title": "One-to-one Mapping between Stimulus and Neural State: Memory and\n  Classification", "comments": "8 pages, 15 figures, final for AIP Advances", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synaptic strength can be seen as probability to propagate impulse, and\naccording to synaptic plasticity, function could exist from propagation\nactivity to synaptic strength. If the function satisfies constraints such as\ncontinuity and monotonicity, neural network under external stimulus will always\ngo to fixed point, and there could be one-to-one mapping between external\nstimulus and synaptic strength at fixed point. In other words, neural network\n\"memorizes\" external stimulus in its synapses. A biological classifier is\nproposed to utilize this mapping.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 08:08:23 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 16:52:34 GMT"}, {"version": "v3", "created": "Sun, 19 Aug 2018 17:00:10 GMT"}, {"version": "v4", "created": "Mon, 3 Dec 2018 17:19:11 GMT"}, {"version": "v5", "created": "Tue, 8 Jan 2019 05:27:46 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2019 03:07:16 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Lan", "Sizhong", ""]]}, {"id": "1805.09042", "submitter": "James Whittington", "authors": "James C. R. Whittington, Timothy H. Muller, Shirley Mark, Caswell\n  Barry, Timothy E. J. Behrens", "title": "Generalisation of structural knowledge in the hippocampal-entorhinal\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem to understanding intelligence is the concept of\ngeneralisation. This allows previously learnt structure to be exploited to\nsolve tasks in novel situations differing in their particularities. We take\ninspiration from neuroscience, specifically the hippocampal-entorhinal system\nknown to be important for generalisation. We propose that to generalise\nstructural knowledge, the representations of the structure of the world, i.e.\nhow entities in the world relate to each other, need to be separated from\nrepresentations of the entities themselves. We show, under these principles,\nartificial neural networks embedded with hierarchy and fast Hebbian memory, can\nlearn the statistics of memories and generalise structural knowledge. Spatial\nneuronal representations mirroring those found in the brain emerge, suggesting\nspatial cognition is an instance of more general organising principles. We\nfurther unify many entorhinal cell types as basis functions for constructing\ntransition graphs, and show these representations effectively utilise memories.\nWe experimentally support model assumptions, showing a preserved relationship\nbetween entorhinal grid and hippocampal place cells across environments.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 10:32:45 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 12:27:02 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Whittington", "James C. R.", ""], ["Muller", "Timothy H.", ""], ["Mark", "Shirley", ""], ["Barry", "Caswell", ""], ["Behrens", "Timothy E. J.", ""]]}, {"id": "1805.09084", "submitter": "PierGianLuca Porta Mana", "authors": "PierGianLuca Porta Mana, Vahid Rostami, Emiliano Torre, Yasser Roudi", "title": "Maximum-entropy and representative samples of neuronal activity: a\n  dilemma", "comments": "12 pages, 2 figures. V2: added references and updated contact details", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present work shows that the maximum-entropy method can be applied to a\nsample of neuronal recordings along two different routes: (1) apply to the\nsample; or (2) apply to a larger, unsampled neuronal population from which the\nsample is drawn, and then marginalize to the sample. These two routes give\ninequivalent results. The second route can be further generalized to the case\nwhere the size of the larger population is unknown. Which route should be\nchosen? Some arguments are presented in favour of the second. This work also\npresents and discusses probability formulae that relate states of knowledge\nabout a population and its samples, and that may be useful for sampling\nproblems in neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 12:17:49 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 16:49:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mana", "PierGianLuca Porta", ""], ["Rostami", "Vahid", ""], ["Torre", "Emiliano", ""], ["Roudi", "Yasser", ""]]}, {"id": "1805.09133", "submitter": "Supreeth Prajwal Shashikumar", "authors": "Supreeth P. Shashikumar, Amit J. Shah, Gari D. Clifford, and Shamim\n  Nemati", "title": "Detection of Paroxysmal Atrial Fibrillation using Attention-based\n  Bidirectional Recurrent Neural Networks", "comments": "Accepted to the 24th ACM SIGKDD International Conference on Knowledge\n  Discovery and Data Mining (KDD 2018), London, UK, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detection of atrial fibrillation (AF), a type of cardiac arrhythmia, is\ndifficult since many cases of AF are usually clinically silent and undiagnosed.\nIn particular paroxysmal AF is a form of AF that occurs occasionally, and has a\nhigher probability of being undetected. In this work, we present an attention\nbased deep learning framework for detection of paroxysmal AF episodes from a\nsequence of windows. Time-frequency representation of 30 seconds recording\nwindows, over a 10 minute data segment, are fed sequentially into a deep\nconvolutional neural network for image-based feature extraction, which are then\npresented to a bidirectional recurrent neural network with an attention layer\nfor AF detection. To demonstrate the effectiveness of the proposed framework\nfor transient AF detection, we use a database of 24 hour Holter\nElectrocardiogram (ECG) recordings acquired from 2850 patients at the\nUniversity of Virginia heart station. The algorithm achieves an AUC of 0.94 on\nthe testing set, which exceeds the performance of baseline models. We also\ndemonstrate the cross-domain generalizablity of the approach by adapting the\nlearned model parameters from one recording modality (ECG) to another\n(photoplethysmogram) with improved AF detection performance. The proposed high\naccuracy, low false alarm algorithm for detecting paroxysmal AF has potential\napplications in long-term monitoring using wearable sensors.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 20:34:17 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Shashikumar", "Supreeth P.", ""], ["Shah", "Amit J.", ""], ["Clifford", "Gari D.", ""], ["Nemati", "Shamim", ""]]}, {"id": "1805.09176", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley", "title": "Why the brain knows more than we do: non-conscious representations and\n  their role in the construction of conscious experience", "comments": null, "journal-ref": "2011, Brain Sciences,2(1):1-21", "doi": "10.3390/brainsci2010001", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific studies have shown that non-conscious stimuli and representations\ninfluence information processing during conscious experience. In the light of\nsuch evidence, questions about potential functional links between non-conscious\nbrain representations and conscious experience arise. This article discusses a\nneural coding model capable of explaining how statistical learning mechanisms\nin dedicated resonant circuits could generate specific temporal activity traces\nof non-conscious representations in the brain. How reentrant signaling,\ntop-down matching, and statistical coincidence of such activity traces may lead\nto the progressive consolidation of temporal patterns that constitute the\nneural signatures of conscious experience in networks extending across large\ndistances beyond functionally specialized brain regions is then explained.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:52:08 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Dresp-Langley", "Birgitta", ""]]}, {"id": "1805.09603", "submitter": "Chen Beer", "authors": "Chen Beer, Omri Barak", "title": "One step back, two steps forward: interference and learning in recurrent\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks, trained to perform cognitive tasks, have recently\nbeen used as models for neural recordings from animals performing these tasks.\nWhile some progress has been made in performing such comparisons, the evolution\nof network dynamics throughout learning remains unexplored. This is paralleled\nby an experimental focus on recording from trained animals, with few studies\nfollowing neural activity throughout training. In this work, we address this\ngap in the realm of artificial networks by analyzing networks that are trained\nto perform memory and pattern generation tasks. The functional aspect of these\ntasks corresponds to dynamical objects in the fully trained network - a line\nattractor or a set of limit cycles for the two respective tasks. We use these\ndynamical objects as anchors to study the effect of learning on their\nemergence. We find that the sequential nature of learning has major\nconsequences for the learning trajectory and its final outcome. Specifically,\nwe show that Least Mean Squares (LMS), a simple gradient descent suggested as a\nbiologically plausible version of the FORCE algorithm, is constantly obstructed\nby forgetting, which is manifested as the destruction of dynamical objects from\nprevious trials. The degree of interference is determined by the correlation\nbetween different trials. We show which specific ingredients of FORCE avoid\nthis phenomenon. Overall, this difference results in convergence that is orders\nof magnitude slower for LMS. Learning implies accumulating information across\nmultiple trials to form the overall concept of the task. Our results show that\ninterference between trials can greatly affect learning, in a learning rule\ndependent manner. These insights can help design experimental protocols that\nminimize such interference, and possibly infer underlying learning rules by\nobserving behavior and neural activity throughout learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 11:05:12 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 07:06:09 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 14:32:44 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Beer", "Chen", ""], ["Barak", "Omri", ""]]}, {"id": "1805.09774", "submitter": "Dorian Florescu Dr", "authors": "Dorian Florescu, Daniel Coca", "title": "Learning with precise spike times: A new decoding algorithm for liquid\n  state machines", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is extensive evidence that biological neural networks encode\ninformation in the precise timing of the spikes generated and transmitted by\nneurons, which offers several advantages over rate-based codes. Here we adopt a\nvector space formulation of spike train sequences and introduce a new liquid\nstate machine (LSM) network architecture and a new forward orthogonal\nregression algorithm to learn an input-output signal mapping or to decode the\nbrain activity. The proposed algorithm uses precise spike timing to select the\npresynaptic neurons relevant to each learning task. We show that using precise\nspike timing to train the LSM and selecting the readout neurons leads to a\nsignificant increase in performance on binary classification tasks as well as\nin decoding neural activity from multielectrode array recordings, compared with\nwhat is achieved using the standard architecture and training method.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 16:52:59 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 22:50:09 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Florescu", "Dorian", ""], ["Coca", "Daniel", ""]]}, {"id": "1805.09874", "submitter": "Aleksandr Aravkin", "authors": "German Abrevaya, Irina Rish, Aleksandr Y. Aravkin, Guillermo Cecchi,\n  James Kozloski, Pablo Polosecki, Peng Zheng, Silvina Ponce Dawson, Juliana\n  Rhee, David Cox", "title": "Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world data sets, especially in biology, are produced by complex\nnonlinear dynamical systems. In this paper, we focus on brain calcium imaging\n(CaI) of different organisms (zebrafish and rat), aiming to build a model of\njoint activation dynamics in large neuronal populations, including the whole\nbrain of zebrafish. We propose a new approach for capturing dynamics of\ntemporal SVD components that uses the coupled (multivariate) van der Pol (VDP)\noscillator, a nonlinear ordinary differential equation (ODE) model describing\nneural activity, with a new parameter estimation technique that combines\nvariable projection optimization and stochastic search. We show that the\napproach successfully handles nonlinearities and hidden state variables in the\ncoupled VDP. The approach is accurate, achieving 0.82 to 0.94 correlation\nbetween the actual and model-generated components, and interpretable, as VDP's\ncoupling matrix reveals anatomically meaningful positive (excitatory) and\nnegative (inhibitory) interactions across different brain subsystems\ncorresponding to spatial SVD components. Moreover, VDP is comparable to (or\nsometimes better than) recurrent neural networks (LSTM) for (short-term)\nprediction of future brain activity; VDP needs less parameters to train, which\nwas a plus on our small training data. Finally, the overall best predictive\nmethod, greatly outperforming both VDP and LSTM in short- and long-term\npredictive settings on both datasets, was the new hybrid VDP-LSTM approach that\nused VDP to simulate large domain-specific dataset for LSTM pretraining; note\nthat simple LSTM data-augmentation via noisy versions of training data was much\nless effective.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:58:37 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 02:03:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Abrevaya", "German", ""], ["Rish", "Irina", ""], ["Aravkin", "Aleksandr Y.", ""], ["Cecchi", "Guillermo", ""], ["Kozloski", "James", ""], ["Polosecki", "Pablo", ""], ["Zheng", "Peng", ""], ["Dawson", "Silvina Ponce", ""], ["Rhee", "Juliana", ""], ["Cox", "David", ""]]}, {"id": "1805.09999", "submitter": "Yue Cui", "authors": "Yue Cui, Ming Song, Darren M. Lipnicki, Yi Yang, Chuyang Ye, Lingzhong\n  Fan, Jing Sui, Tianzi Jiang, Jianghong He", "title": "Subdivisions of the posteromedial cortex in disorders of consciousness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence suggests that disruptions of the posteromedial cortex (PMC) and\nposteromedial corticothalamic connectivity contribute to disorders of\nconsciousness (DOCs). While most previous studies treated the PMC as a whole,\nthis structure is functionally heterogeneous. The present study investigated\nwhether particular subdivisions of the PMC are specifically associated with\nDOCs. Participants were DOC patients, 21 vegetative state/unresponsive\nwakefulness syndrome (VS/UWS), 12 minimally conscious state (MCS), and 29\nhealthy controls. Individual PMC and thalamus were divided into distinct\nsubdivisions by their fiber tractograpy to each other and default mode regions,\nand white matter integrity and brain activity between/within subdivisions were\nassessed. The thalamus was represented mainly in the dorsal and posterior\nportions of the PMC, and the white matter tracts connecting these subdivisions\nto the thalamus had less integrity in VS/UWS patients than in MCS patients and\nhealthy controls, as well as in patients who did not recover after 12 months\nthan in patients who did. The structural substrates were validated by finding\nimpaired functional fluctuations within this PMC subdivision. This study is the\nfirst to show that tracts from dorsal and posterior subdivisions of the PMC to\nthe thalamus contribute to DOCs.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 06:30:50 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Cui", "Yue", ""], ["Song", "Ming", ""], ["Lipnicki", "Darren M.", ""], ["Yang", "Yi", ""], ["Ye", "Chuyang", ""], ["Fan", "Lingzhong", ""], ["Sui", "Jing", ""], ["Jiang", "Tianzi", ""], ["He", "Jianghong", ""]]}, {"id": "1805.10116", "submitter": "Chiara Gastaldi", "authors": "Chiara Gastaldi, Samuel P. Muscinelli and Wulfram Gerstner", "title": "Optimal stimulation protocol in a bistable synaptic consolidation model", "comments": "23 pages, 6 figures", "journal-ref": "Front. Comput. Neurosci., 13 November 2019", "doi": "10.3389/fncom.2019.00078", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consolidation of synaptic changes in response to neural activity is thought\nto be fundamental for memory maintenance over a timescale of hours. In\nexperiments, synaptic consolidation can be induced by repeatedly stimulating\npresynaptic neurons. However, the effectiveness of such protocols depends\ncrucially on the repetition frequency of the stimulations and the mechanisms\nthat cause this complex dependence are unknown. Here we propose a simple\nmathematical model that allows us to systematically study the interaction\nbetween the stimulation protocol and synaptic consolidation. We show the\nexistence of optimal stimulation protocols for our model and, similarly to LTP\nexperiments, the repetition frequency of the stimulation plays a crucial role\nin achieving consolidation. Our results show that the complex dependence of LTP\non the stimulation frequency emerges naturally from a model which satisfies\nonly minimal bistability requirements.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:54:08 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Gastaldi", "Chiara", ""], ["Muscinelli", "Samuel P.", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "1805.10235", "submitter": "Johanna Senk", "authors": "Johanna Senk, Espen Hagen, Sacha J. van Albada, Markus Diesmann", "title": "Reconciliation of weak pairwise spike-train correlations and highly\n  coherent local field potentials across space", "comments": "44 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic and acute implants of multi-electrode arrays that cover several\nmm$^2$ of neural tissue provide simultaneous access to population signals like\nextracellular potentials and the spiking activity of 100 or more individual\nneurons. While the recorded data may uncover principles of brain function, its\ninterpretation calls for multiscale computational models with corresponding\nspatial dimensions and signal predictions. Such models can facilitate the\nsearch of mechanisms underlying observed spatiotemporal activity patterns in\ncortex. Multi-layer spiking neuron network models of local cortical circuits\ncovering ~1 mm$^2$ have been developed, integrating experimentally obtained\nneuron-type specific connectivity data and reproducing features of in-vivo\nspiking statistics. With forward models, local field potentials (LFPs) can be\ncomputed from the simulated spiking activity. To account for the spatial scale\nof common neural recordings, we extend a local network and LFP model to 4x4\nmm$^2$. The upscaling preserves the neuron densities, and introduces\ndistance-dependent connection probabilities and delays. As detailed\nexperimental connectivity data is partially lacking, we address this\nuncertainty in model parameters by testing parameter combinations within\nbiologically plausible bounds. Based on model predictions of spiking activity\nand LFPs, we find that the upscaling procedure preserves the overall spiking\nstatistics of the original model and reproduces asynchronous irregular spiking\nacross populations and weak pairwise spike-train correlations observed in\nsensory cortex. In contrast with the weak spike-train correlations, the\ncorrelation of LFP signals is strong and distance-dependent, compatible with\nexperimental observations. Enhanced spatial coherence in the low-gamma band may\nexplain the recent experimental report of an apparent band-pass filter effect\nin the spatial reach of the LFP.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 16:23:00 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Senk", "Johanna", ""], ["Hagen", "Espen", ""], ["van Albada", "Sacha J.", ""], ["Diesmann", "Markus", ""]]}, {"id": "1805.10714", "submitter": "Carlo Baldassi", "authors": "Luca Saglietti, Federica Gerace, Alessandro Ingrosso, Carlo Baldassi,\n  Riccardo Zecchina", "title": "From statistical inference to a differential learning rule for\n  stochastic neural networks", "comments": "16 pages, 8 figures + appendix; total: 28 pages, 10 figures", "journal-ref": "Interface Focus 2018 8 20180033; DOI: 10.1098/rsfs.2018.0033.\n  Published 19 October 2018", "doi": "10.1098/rsfs.2018.0033", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic neural networks are a prototypical computational device able to\nbuild a probabilistic representation of an ensemble of external stimuli.\nBuilding on the relationship between inference and learning, we derive a\nsynaptic plasticity rule that relies only on delayed activity correlations, and\nthat shows a number of remarkable features. Our \"delayed-correlations matching\"\n(DCM) rule satisfies some basic requirements for biological feasibility: finite\nand noisy afferent signals, Dale's principle and asymmetry of synaptic\nconnections, locality of the weight update computations. Nevertheless, the DCM\nrule is capable of storing a large, extensive number of patterns as attractors\nin a stochastic recurrent neural network, under general scenarios without\nrequiring any modification: it can deal with correlated patterns, a broad range\nof architectures (with or without hidden neuronal states), one-shot learning\nwith the palimpsest property, all the while avoiding the proliferation of\nspurious attractors. When hidden units are present, our learning rule can be\nemployed to construct Boltzmann machine-like generative models, exploiting the\naddition of hidden neurons in feature extraction and classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 23:30:20 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 11:37:30 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Saglietti", "Luca", ""], ["Gerace", "Federica", ""], ["Ingrosso", "Alessandro", ""], ["Baldassi", "Carlo", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1805.10734", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "A neural network trained to predict future video frames mimics critical\n  properties of biological neuronal responses and perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks take loose inspiration from neuroscience, it is an\nopen question how seriously to take the analogies between artificial deep\nnetworks and biological neuronal systems. Interestingly, recent work has shown\nthat deep convolutional neural networks (CNNs) trained on large-scale image\nrecognition tasks can serve as strikingly good models for predicting the\nresponses of neurons in visual cortex to visual stimuli, suggesting that\nanalogies between artificial and biological neural networks may be more than\nsuperficial. However, while CNNs capture key properties of the average\nresponses of cortical neurons, they fail to explain other properties of these\nneurons. For one, CNNs typically require large quantities of labeled input data\nfor training. Our own brains, in contrast, rarely have access to this kind of\nsupervision, so to the extent that representations are similar between CNNs and\nbrains, this similarity must arise via different training paths. In addition,\nneurons in visual cortex produce complex time-varying responses even to static\ninputs, and they dynamically tune themselves to temporal regularities in the\nvisual environment. We argue that these differences are clues to fundamental\ndifferences between the computations performed in the brain and in deep\nnetworks. To begin to close the gap, here we study the emergent properties of a\npreviously-described recurrent generative network that is trained to predict\nfuture video frames in a self-supervised manner. Remarkably, the model is able\nto capture a wide variety of seemingly disparate phenomena observed in visual\ncortex, ranging from single unit response dynamics to complex perceptual motion\nillusions. These results suggest potentially deep connections between recurrent\npredictive neural network models and the brain, providing new leads that can\nenrich both fields.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 02:15:09 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 02:47:58 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1805.10827", "submitter": "Yanlong Sun", "authors": "Yanlong Sun, Hongbin Wang", "title": "Learning Temporal Structures of Random Patterns", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cornerstone of human statistical learning is the ability to extract\ntemporal regularities / patterns from random sequences. Here we present a\nmethod of computing pattern time statistics with generating functions for\nfirst-order Markov trials and independent Bernoulli trials. We show that the\npattern time statistics cover a wide range of measurements commonly used in\nexisting studies of both human and machine learning of stochastic processes,\nincluding probability of alternation, temporal correlation between pattern\nevents, and related variance / risk measures. Moreover, we show that recurrent\nprocessing and event segmentation by pattern overlap may provide a coherent\nexplanation for the sensitivity of the human brain to the rich statistics and\nthe latent structures in the learning environment.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 09:12:49 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sun", "Yanlong", ""], ["Wang", "Hongbin", ""]]}, {"id": "1805.10892", "submitter": "Eero Satuvuori", "authors": "Eero Satuvuori, Mario Mulansky, Andreas Daffertshofer, Thomas Kreuz", "title": "Using spike train distances to identify the most discriminative neuronal\n  subpopulation", "comments": "14 pages, 9 Figures", "journal-ref": null, "doi": "10.1016/j.jneumeth.2018.09.008", "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Spike trains of multiple neurons can be analyzed following the\nsummed population (SP) or the labeled line (LL) hypothesis. Responses to\nexternal stimuli are generated by a neuronal population as a whole or the\nindividual neurons have encoding capacities of their own. The SPIKE-distance\nestimated either for a single, pooled spike train over a population or for each\nneuron separately can serve to quantify these responses.\n  New Method: For the SP case we compare three algorithms that search for the\nmost discriminative subpopulation over all stimulus pairs. For the LL case we\nintroduce a new algorithm that combines neurons that individually separate\ndifferent pairs of stimuli best.\n  Results: The best approach for SP is a brute force search over all possible\nsubpopulations. However, it is only feasible for small populations. For more\nrealistic settings, simulated annealing clearly outperforms gradient algorithms\nwith only a limited increase in computational load. Our novel LL approach can\nhandle very involved coding scenarios despite its computational ease.\n  Comparison with Existing Methods: Spike train distances have been extended to\nthe analysis of neural populations interpolating between SP and LL coding. This\nincludes parametrizing the importance of distinguishing spikes being fired in\ndifferent neurons. Yet, these approaches only consider the population as a\nwhole. The explicit focus on subpopulations render our algorithms\ncomplimentary.\n  Conclusions: The spectrum of encoding possibilities in neural populations is\nbroad. The SP and LL cases are two extremes for which our algorithms provide\ncorrect identification results.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:40:10 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 09:10:05 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Satuvuori", "Eero", ""], ["Mulansky", "Mario", ""], ["Daffertshofer", "Andreas", ""], ["Kreuz", "Thomas", ""]]}, {"id": "1805.10958", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison, Vincent Adam, Srinivas C. Turaga", "title": "Discrete flow posteriors for variational inference in discrete dynamical\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each training step for a variational autoencoder (VAE) requires us to sample\nfrom the approximate posterior, so we usually choose simple (e.g. factorised)\napproximate posteriors in which sampling is an efficient computation that fully\nexploits GPU parallelism. However, such simple approximate posteriors are often\ninsufficient, as they eliminate statistical dependencies in the posterior.\nWhile it is possible to use normalizing flow approximate posteriors for\ncontinuous latents, some problems have discrete latents and strong statistical\ndependencies. The most natural approach to model these dependencies is an\nautoregressive distribution, but sampling from such distributions is inherently\nsequential and thus slow. We develop a fast, parallel sampling procedure for\nautoregressive distributions based on fixed-point iterations which enables\nefficient and accurate variational inference in discrete state-space latent\nvariable dynamical systems. To optimize the variational bound, we considered\ntwo ways to evaluate probabilities: inserting the relaxed samples directly into\nthe pmf for the discrete distribution, or converting to continuous logistic\nlatent variables and interpreting the K-step fixed-point iterations as a\nnormalizing flow. We found that converting to continuous latent variables gave\nconsiderable additional scope for mismatch between the true and approximate\nposteriors, which resulted in biased inferences, we thus used the former\napproach. Using our fast sampling procedure, we were able to realize the\nbenefits of correlated posteriors, including accurate uncertainty estimates for\none cell, and accurate connectivity estimates for multiple cells, in an order\nof magnitude less time.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:56:26 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Aitchison", "Laurence", ""], ["Adam", "Vincent", ""], ["Turaga", "Srinivas C.", ""]]}, {"id": "1805.10981", "submitter": "Ivan Zubarev", "authors": "Ivan Zubarev, Rasmus Zetter, Hanna-Leena Halme and Lauri Parkkonen", "title": "Adaptive neural network classifier for decoding MEG signals", "comments": "12 pages, 4 figures, 4 tables. keywords: MEG, BCI, real-time,\n  convolutional neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) outperform traditional classification\nmethods in many domains. Recently these methods have gained attention in\nneuroscience and particularly in brain-computer interface (BCI) community.\nHere, we introduce a CNN optimized for classification of brain states from\nmagnetoencephalographic (MEG) measurements. Our CNN design is based on a\ngenerative model of the electromagnetic (EEG and MEG) brain signals and is\nreadily interpretable in neurophysiological terms. We show here that the\nproposed network is able to decode event-related responses as well as\nmodulations of oscillatory brain activity and that it outperforms more complex\nneural networks and traditional classifiers used in the field. Importantly, the\nmodel is robust to inter-individual differences and can successfully generalize\nto new subjects in offline and online classification.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 15:40:44 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 17:47:02 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zubarev", "Ivan", ""], ["Zetter", "Rasmus", ""], ["Halme", "Hanna-Leena", ""], ["Parkkonen", "Lauri", ""]]}, {"id": "1805.11081", "submitter": "Emma Towlson", "authors": "Emma K. Towlson, Petra E. Vertes, Gang Yan, Yee Lian Chew, Denise S.\n  Walker, William R. Schafer, and Albert-Laszlo Barabasi", "title": "Caenorhabditis elegans and the network control framework - FAQs", "comments": "19 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control is essential to the functioning of any neural system. Indeed, under\nhealthy conditions the brain must be able to continuously maintain a tight\nfunctional control between the system's inputs and outputs. One may therefore\nhypothesise that the brain's wiring is predetermined by the need to maintain\ncontrol across multiple scales, maintaining the stability of key internal\nvariables, and producing behaviour in response to environmental cues. Recent\nadvances in network control have offered a powerful mathematical framework to\nexplore the structure-function relationship in complex biological, social, and\ntechnological networks, and are beginning to yield important and precise\ninsights for neuronal systems. The network control paradigm promises a\npredictive, quantitative framework to unite the distinct datasets necessary to\nfully describe a nervous system, and provide mechanistic explanations for the\nobserved structure and function relationships. Here, we provide a thorough\nreview of the network control framework as applied to C. elegans, in the style\nof a FAQ. We present the theoretical, computational, and experimental aspects\nof network control, and discuss its current capabilities and limitations,\ntogether with the next likely advances and improvements. We further present the\nPython code to enable exploration of control principles in a manner specific to\nthis prototypical organism.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 17:49:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Towlson", "Emma K.", ""], ["Vertes", "Petra E.", ""], ["Yan", "Gang", ""], ["Chew", "Yee Lian", ""], ["Walker", "Denise S.", ""], ["Schafer", "William R.", ""], ["Barabasi", "Albert-Laszlo", ""]]}, {"id": "1805.11704", "submitter": "Arna Ghosh", "authors": "Arna Ghosh, Fabien dal Maso, Marc Roig, Georgios D Mitsis and\n  Marie-H\\'el\\`ene Boudrias", "title": "Deep Semantic Architecture with discriminative feature visualization for\n  neuroimage analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging data analysis often involves \\emph{a-priori} selection of data\nfeatures to study the underlying neural activity. Since this could lead to\nsub-optimal feature selection and thereby prevent the detection of subtle\npatterns in neural activity, data-driven methods have recently gained\npopularity for optimizing neuroimaging data analysis pipelines and thereby,\nimproving our understanding of neural mechanisms. In this context, we developed\na deep convolutional architecture that can identify discriminating patterns in\nneuroimaging data and applied it to electroencephalography (EEG) recordings\ncollected from 25 subjects performing a hand motor task before and after a rest\nperiod or a bout of exercise. The deep network was trained to classify subjects\ninto exercise and control groups based on differences in their EEG signals.\nSubsequently, we developed a novel method termed the cue-combination for Class\nActivation Map (ccCAM), which enabled us to identify discriminating\nspatio-temporal features within definite frequency bands (23--33 Hz) and assess\nthe effects of exercise on the brain. Additionally, the proposed architecture\nallowed the visualization of the differences in the propagation of underlying\nneural activity across the cortex between the two groups, for the first time in\nour knowledge. Our results demonstrate the feasibility of using deep network\narchitectures for neuroimaging analysis in different contexts such as, for the\nidentification of robust brain biomarkers to better characterize and\npotentially treat neurological disorders.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 20:55:09 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 20:17:16 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ghosh", "Arna", ""], ["Maso", "Fabien dal", ""], ["Roig", "Marc", ""], ["Mitsis", "Georgios D", ""], ["Boudrias", "Marie-H\u00e9l\u00e8ne", ""]]}, {"id": "1805.11786", "submitter": "Kevin O'Keeffe", "authors": "Kevin P. O'Keeffe, Adam Mahdi", "title": "Bayesian approach to uncertainty quantification for cerebral\n  autoregulation index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebral autoregulation refers to the brain's ability to maintain cerebral\nblood flow at an approximately constant level, despite changes in arterial\nblood pressure. The performance of this mechanism is often assessed using a\nten-scale index called the ARI (autoregulation index). Here, $0$ denotes the\nabsence of, while $9$ denotes the strongest, autoregulation. Current methods to\ncalculate the ARI do not typically provide error estimates. Here, we show how\nthis can be done using a bayesian approach. We use Markov-chain Monte Carlo\nmethods to produce a probability distribution for the ARI, which gives a\nnatural way to estimate error.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 03:09:00 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["O'Keeffe", "Kevin P.", ""], ["Mahdi", "Adam", ""]]}, {"id": "1805.11851", "submitter": "Simone Carlo Surace", "authors": "Simone Carlo Surace, Jean-Pascal Pfister, Wulfram Gerstner, Johanni\n  Brea", "title": "On the choice of metric in gradient-based theories of brain function", "comments": "Revised version; 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea that the brain functions so as to minimize certain costs pervades\ntheoretical neuroscience. Since a cost function by itself does not predict how\nthe brain finds its minima, additional assumptions about the optimization\nmethod need to be made to predict the dynamics of physiological quantities. In\nthis context, steepest descent (also called gradient descent) is often\nsuggested as an algorithmic principle of optimization potentially implemented\nby the brain. In practice, researchers often consider the vector of partial\nderivatives as the gradient. However, the definition of the gradient and the\nnotion of a steepest direction depend on the choice of a metric. Since the\nchoice of the metric involves a large number of degrees of freedom, the\npredictive power of models that are based on gradient descent must be called\ninto question, unless there are strong constraints on the choice of the metric.\nHere we provide a didactic review of the mathematics of gradient descent,\nillustrate common pitfalls of using gradient descent as a principle of brain\nfunction with examples from the literature and propose ways forward to\nconstrain the metric.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 08:21:41 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 21:35:49 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 14:12:10 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Surace", "Simone Carlo", ""], ["Pfister", "Jean-Pascal", ""], ["Gerstner", "Wulfram", ""], ["Brea", "Johanni", ""]]}, {"id": "1805.12005", "submitter": "Roseric Azondekon", "authors": "Roseric Azondekon, Zachary James Harper, and Charles Michael Welzig", "title": "Combined MEG and fMRI Exponential Random Graph Modeling for inferring\n  functional Brain Connectivity", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimated connectomes by the means of neuroimaging techniques have enriched\nour knowledge of the organizational properties of the brain leading to the\ndevelopment of network-based clinical diagnostics. Unfortunately, to date, many\nof those network-based clinical diagnostics tools, based on the mere\ndescription of isolated instances of observed connectomes are noisy estimates\nof the true connectivity network. Modeling brain connectivity networks is\ntherefore important to better explain the functional organization of the brain\nand allow inference of specific brain properties. In this report, we present\npilot results on the modeling of combined MEG and fMRI neuroimaging data\nacquired during an n-back memory task experiment. We adopted a pooled\nExponential Random Graph Model (ERGM) as a network statistical model to capture\nthe underlying process in functional brain networks of 9 subjects MEG and fMRI\ndata out of 32 during a 0-back vs 2-back memory task experiment. Our results\nsuggested strong evidence that all the functional connectomes of the 9 subjects\nhave small world properties. A group level comparison using comparing the\nconditions pairwise showed no significant difference in the functional\nconnectomes across the subjects. Our pooled ERGMs successfully reproduced\nimportant brain properties such as functional segregation and functional\nintegration. However, the ERGMs reproducing the functional segregation of the\nbrain networks discriminated between the 0-back and 2-back conditions while the\nmodels reproducing both properties failed to successfully discriminate between\nboth conditions. Our results are promising and would improve in robustness with\na larger sample size. Nevertheless, our pilot results tend to support previous\nfindings that functional segregation and integration are sufficient to\nstatistically reproduce the main properties of brain network.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 18:59:31 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Azondekon", "Roseric", ""], ["Harper", "Zachary James", ""], ["Welzig", "Charles Michael", ""]]}, {"id": "1805.12491", "submitter": "Christopher Lynn", "authors": "Christopher W. Lynn, Ari E. Kahn, Nathaniel Nyema, and Danielle S.\n  Bassett", "title": "Abstract representations of events arise from mental errors in learning\n  and memory", "comments": "73 pages, 11 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are adept at uncovering abstract associations in the world around\nthem, yet the underlying mechanisms remain poorly understood. Intuitively,\nlearning the higher-order structure of statistical relationships should involve\ncomplex mental processes. Here we propose an alternative perspective: that\nhigher-order associations instead arise from natural errors in learning and\nmemory. Combining ideas from information theory and reinforcement learning, we\nderive a maximum entropy (or minimum complexity) model of people's internal\nrepresentations of the transitions between stimuli. Importantly, our model (i)\naffords a concise analytic form, (ii) qualitatively explains the effects of\ntransition network structure on human expectations, and (iii) quantitatively\npredicts human reaction times in probabilistic sequential motor tasks.\nTogether, these results suggest that mental errors influence our abstract\nrepresentations of the world in significant and predictable ways, with direct\nimplications for the study and design of optimally learnable information\nsources.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 14:30:40 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 15:47:29 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 17:41:13 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lynn", "Christopher W.", ""], ["Kahn", "Ari E.", ""], ["Nyema", "Nathaniel", ""], ["Bassett", "Danielle S.", ""]]}]