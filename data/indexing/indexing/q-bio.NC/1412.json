[{"id": "1412.0291", "submitter": "Viola Priesemann", "authors": "Michael Wibral, Joseph T. Lizier, Viola Priesemann", "title": "Bits from Biology for Computational Intelligence", "comments": null, "journal-ref": "Frontiers in Robotics and AI, 2:5 (2015)", "doi": "10.3389/frobt.2015.00005", "report-no": null, "categories": "q-bio.NC cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational intelligence is broadly defined as biologically-inspired\ncomputing. Usually, inspiration is drawn from neural systems. This article\nshows how to analyze neural systems using information theory to obtain\nconstraints that help identify the algorithms run by such systems and the\ninformation they represent. Algorithms and representations identified\ninformation-theoretically may then guide the design of biologically inspired\ncomputing systems (BICS). The material covered includes the necessary\nintroduction to information theory and the estimation of information theoretic\nquantities from neural data. We then show how to analyze the information\nencoded in a system about its environment, and also discuss recent\nmethodological developments on the question of how much information each agent\ncarries about the environment either uniquely, or redundantly or\nsynergistically together with others. Last, we introduce the framework of local\ninformation dynamics, where information processing is decomposed into component\nprocesses of information storage, transfer, and modification -- locally in\nspace and time. We close by discussing example applications of these measures\nto neural data and other complex systems.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 21:47:15 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Wibral", "Michael", ""], ["Lizier", "Joseph T.", ""], ["Priesemann", "Viola", ""]]}, {"id": "1412.0363", "submitter": "Natasha Cayco Gajic", "authors": "Alex Cayco-Gajic and Joel Zylberberg and Eric Shea-Brown", "title": "Impact of triplet correlations on neural population codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which statistical features of spiking activity matter for how stimuli are\nencoded in neural populations? A vast body of work has explored how firing\nrates in individual cells and correlations in the spikes of cell pairs impact\ncoding. But little is known about how higher-order correlations, which describe\nsimultaneous firing in triplets and larger ensembles of cells, impact encoded\nstimulus information. Here, we take a first step toward closing this gap. We\nvary triplet correlations in small (~10 cell) neural populations while keeping\nsingle cell and pairwise statistics fixed at typically reported values. For\neach value of triplet correlations, we estimate the performance of the neural\npopulation on a two-stimulus discrimination task. We identify a predominant way\nthat such triplet correlations can strongly enhance coding: if triplet\ncorrelations differ for the two stimuli, they skew the response distributions\nof the two stimuli apart from each other, separating them and making them\neasier to distinguish. This coding benefit does not occur when both stimuli\nelicit similar triplet correlations. These results indicate that higher-order\ncorrelations could have a strong effect on population coding. Finally, we\ncalculate how many samples are necessary to accurately measure spiking\ncorrelations of this type, providing an estimate of the necessary recording\ntimes in experiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 07:07:25 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Cayco-Gajic", "Alex", ""], ["Zylberberg", "Joel", ""], ["Shea-Brown", "Eric", ""]]}, {"id": "1412.0595", "submitter": "Naresh Balaji", "authors": "Naresh Balaji, Esin Yavuz, Thomas Nowotny", "title": "Scalability and Optimization Strategies for GPU Enhanced Neural Networks\n  (GeNN)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of spiking neural networks has been traditionally done on\nhigh-performance supercomputers or large-scale clusters. Utilizing the parallel\nnature of neural network computation algorithms, GeNN (GPU Enhanced Neural\nNetwork) provides a simulation environment that performs on General Purpose\nNVIDIA GPUs with a code generation based approach. GeNN allows the users to\ndesign and simulate neural networks by specifying the populations of neurons at\ndifferent stages, their synapse connection densities and the model of\nindividual neurons. In this report we describe work on how to scale synaptic\nweights based on the configuration of the user-defined network to ensure\nsufficient spiking and subsequent effective learning. We also discuss\noptimization strategies particular to GPU computing: sparse representation of\nsynapse connections and occupancy based block-size determination.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:12:54 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Balaji", "Naresh", ""], ["Yavuz", "Esin", ""], ["Nowotny", "Thomas", ""]]}, {"id": "1412.0603", "submitter": "Alexei Koulakov", "authors": "Daniel D. Ferrante, Yi Wei, and Alexei A. Koulakov", "title": "Statistical model of evolution of brain parcellation", "comments": "9 pages, plenty of pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of brain and cortical area sizes [parcellation\nunits (PUs)] obtained for three species: mouse, macaque, and human. We find\nthat the distribution of PU sizes is close to lognormal. We analyze the\nmathematical model of evolution of brain parcellation based on iterative\nfragmentation and specialization. In this model, each existing PU has a\nprobability to be split that depends on PU size only. This model shows that the\nsame evolutionary process may have led to brain parcellation in these three\nspecies. Our model suggests that region-to-region (macro) connectivity is given\nby the outer product form. We show that most experimental data on non-vanishing\nmacaque cortex macroconnectivity (62% for area V1) can be explained by the\nouter product power-law form suggested by our model. We propose a\nmultiplicative Hebbian learning rule for the macroconnectome that could yield\nthe correct scaling of connection strengths between areas. We thus propose a\nuniversal evolutionary model that may have contributed to both brain\nparcellation and mesoscopic level connectivity in mammals.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:28:10 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Ferrante", "Daniel D.", ""], ["Wei", "Yi", ""], ["Koulakov", "Alexei A.", ""]]}, {"id": "1412.0894", "submitter": "Christian Geier", "authors": "Christian Geier, Stephan Bialonski, Christian E. Elger, Klaus Lehnertz", "title": "How important is the seizure onset zone for seizure dynamics?", "comments": "In press (Seizure)", "journal-ref": null, "doi": "10.1016/j.seizure.2014.10.013", "report-no": null, "categories": "q-bio.NC physics.data-an physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Research into epileptic networks has recently allowed deeper\ninsights into the epileptic process. Here we investigated the importance of\nindividual network nodes for seizure dynamics.\n  Methods: We analysed intracranial electroencephalographic recordings of 86\nfocal seizures with different anatomical onset locations. With time-resolved\ncorrelation analyses, we derived a sequence of weighted epileptic networks\nspanning the pre-ictal, ictal, and post-ictal period, and each recording site\nrepresents a network node. We assessed node importance with commonly used\ncentrality indices that take into account different network properties.\n  Results: A high variability of temporal evolution of node importance was\nobserved, both intra- and interindividually. Nevertheless, nodes near and far\noff the seizure onset zone (SOZ) were rated as most important for seizure\ndynamics more often (65% of cases) than nodes from within the SOZ (35% of\ncases).\n  Conclusion: Our findings underline the high relevance of brain outside of the\nSOZ but within the large-scale epileptic network for seizure dynamics.\nKnowledge about these network constituents may elucidate targets for\nindividualised therapeutic interventions that aim at preventing seizure\ngeneration and spread.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 12:44:06 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Geier", "Christian", ""], ["Bialonski", "Stephan", ""], ["Elger", "Christian E.", ""], ["Lehnertz", "Klaus", ""]]}, {"id": "1412.1070", "submitter": "Federico Stella", "authors": "Federico Stella and Alessandro Treves", "title": "The Self-Organization of Grid Cells in 3D", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What sort of grid cells do we expect to see in bats exploring a\nthree-dimensional environment? How long will it take for them to emerge? We\naddress these questions within our self-organization model based on firing-rate\nadaptation. The model indicates that the answer to the first question may be\nsimple, and to the second one rather complex. The mathematical analysis of the\nsimplified version of the model points at asymptotic states resembling FCC and\nHCP crystal structures, which are calculated to be very close to each other in\nterms of cost function. The simulation of the full model, however, shows that\nthe approach to such asymptotic states involves several sub-processes over\ndistinct time scales. The smoothing of the initially irregular multiple fields\nof individual units and their arrangement into hexagonal grids over certain\nbest planes are observed to occur relatively fast, even in large 3D volumes.\nThe correct mutual orientation of the planes, however, and the coordinated\narrangement of different units, take a longer time, with the network showing no\nsign of convergence towards either a pure FCC or HCP ordering.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 20:59:20 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Stella", "Federico", ""], ["Treves", "Alessandro", ""]]}, {"id": "1412.1369", "submitter": "Antonio Batista", "authors": "F. S. Borges, E. L. Lameu, A. M. Batista, K. C. Iarosz, M. S.\n  Baptista, R. L. Viana", "title": "Complementary action of chemical and electrical synapses to perception", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2015.02.098", "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamic range of a cellular automaton model for a neuronal\nnetwork with electrical and chemical synapses. The neural network is separated\ninto two layers, where one layer corresponds to inhibitory, and the other\ncorresponds to excitatory neurons. We randomly distribute electrical synapses\nin the network, in order to analyse the effects on the dynamic range. We verify\nthat electrical synapses have a complementary effect on the enhancement of the\ndynamic range. The enhancement depends on the proportion of electrical\nsynapses, and also depends on the layer where they are distributed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:38:17 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Borges", "F. S.", ""], ["Lameu", "E. L.", ""], ["Batista", "A. M.", ""], ["Iarosz", "K. C.", ""], ["Baptista", "M. S.", ""], ["Viana", "R. L.", ""]]}, {"id": "1412.1713", "submitter": "Zachary Kilpatrick PhD", "authors": "Alan Veliz-Cuba, Harel Shouval, Kresimir Josic, Zachary P. Kilpatrick", "title": "Networks that learn the precise timing of event sequences", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal circuits can learn and replay firing patterns evoked by sequences of\nsensory stimuli. After training, a brief cue can trigger a spatiotemporal\npattern of neural activity similar to that evoked by a learned stimulus\nsequence. Network models show that such sequence learning can occur through the\nshaping of feedforward excitatory connectivity via long term plasticity.\nPrevious models describe how event order can be learned, but they typically do\nnot explain how precise timing can be recalled. We propose a mechanism for\nlearning both the order and precise timing of event sequences. In our recurrent\nnetwork model, long term plasticity leads to the learning of the sequence,\nwhile short term facilitation enables temporally precise replay of events.\nLearned synaptic weights between populations determine the time necessary for\none population to activate another. Long term plasticity adjusts these weights\nso that the trained event times are matched during playback. While we chose\nshort term facilitation as a time-tracking process, we also demonstrate that\nother mechanisms, such as spike rate adaptation, can fulfill this role. We also\nanalyze the impact of trial-to-trial variability, showing how observational\nerrors as well as neuronal noise result in variability in learned event times.\nThe dynamics of the playback process determine how stochasticity is inherited\nin learned sequence timings. Future experiments that characterize such\nvariability can therefore shed light on the neural mechanisms of sequence\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:10:30 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 13:28:29 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Veliz-Cuba", "Alan", ""], ["Shouval", "Harel", ""], ["Josic", "Kresimir", ""], ["Kilpatrick", "Zachary P.", ""]]}, {"id": "1412.1778", "submitter": "Laszlo Kish", "authors": "Eszter A. Kish, Claes-Goran Granqvist, Andras Der, Laszlo B. Kish", "title": "Lognormal distribution of firing time and rate from a single neuron?", "comments": "Accepted for publication in Cognitive Neurodynamics (Springer)", "journal-ref": "Cognitive Neurodynamics vol. 9 (2015) pp. 459-462", "doi": "10.1007/s11571-015-9332-6", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Even a single neuron may be able to produce significant lognormal features in\nits firing statistics due to noise in the charging ion current. A mathematical\nscheme introduced in advanced nanotechnology is relevant for the analysis of\nthis mechanism in the simplest case, the integrate-and-fire model with white\nnoise in the charging ion current.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 15:18:15 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 18:35:24 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Kish", "Eszter A.", ""], ["Granqvist", "Claes-Goran", ""], ["Der", "Andras", ""], ["Kish", "Laszlo B.", ""]]}, {"id": "1412.2622", "submitter": "Alexander K. Vidybida", "authors": "Alexander Vidybida", "title": "Output stream of binding neuron with delayed feedback", "comments": "10 pages, 5 figures, 14-th International Congress of Cybernetics and\n  Systems of WOSC, Wroclaw, September 9-12, 2008, Proceedings, pages 292-302,\n  Oficyna Wydawnicza Politechniki Wroclawskiej, ISBN 978-83-7493-400-8", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binding neuron (BN) whith delayed feedback is considered. The neuron is fed\nexternally with a Poisson stream of intensity $\\lambda$. The neuron's output\nspikes are fed into its input with time delay $\\Delta$. The resulting output\nstream of the BN is not Poissonian, and we look for its interspike intervals\n(ISI) distribution. For BN with threshold 2 an exact mathematical expression as\nfunction of $\\lambda$, $\\Delta$ and BN's internal memory, $\\tau$ is derived for\nthe ISI distribution, and for higher thresholds it is found numerically. The\ndistributions found are characterized with discontinuities of jump type, and\ninclude singularity of Dirac's $\\delta$-function type. It is concluded that\ndelayed feedback presence can radically alter neuronal output firing\nstatistics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:49:58 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Vidybida", "Alexander", ""]]}, {"id": "1412.2818", "submitter": "Fabian Chersi", "authors": "Fabian Chersi", "title": "The hippocampal-striatal circuit for goal-directed and habitual choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  It is now widely accepted that one of the roles of the hippocampus is to\nmaintain episodic spatial representations, while parallel striatal pathways\ncontribute to both declarative and procedural value computations by encoding\ndifferent input-specific outcome predictions. In this paper we investigate the\nuse of these brain mechanisms for action selection, linking them to model-based\nand model-free controllers for decision making. To this aim we propose a\nbiologically inspired computational model that embodies these theories and\nexplains the functioning of the hippocampal-striatal circuit in a rat\nnavigation task. Its main characteristic is to allow the cooperation of\nhabitual and goal-directed behaviors, with the hippocampus primarily involved\nin encoding spatial information and simulating possible navigation paths, and\nthe ventral and dorsal striatum involved in learning stimulus-response\nbehaviors and evaluating the reward expectancies associated to predicted\nlocations and sensed stimuli, respectively. The architecture we present employs\nan unsupervised reinforcement learning rule for the hippocampal-striatal\nnetwork that is able to build a representation of the environment in which\nrewarding sites and informative landmarks produce value gradients that are used\nfor planning and decision making. Additionally, it utilizes an arbitration\nmechanism that balances between exploitation, i.e. stimulus-response behaviors,\nand mental exploration, i.e. motor imagery processes, based on the intensity\nand the variability of the responses of striatal neurons. We interpret these\nresults in light of recent experimental data that show anticipatory activations\nin hippocampal and striatal areas.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 00:19:25 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Chersi", "Fabian", ""]]}, {"id": "1412.2859", "submitter": "James P. Crutchfield", "authors": "Sarah Marzen and James P. Crutchfield", "title": "Circumventing the Curse of Dimensionality in Prediction: Causal\n  Rate-Distortion for Infinite-Order Markov Processes", "comments": "25 pages, 14 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/cn.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG nlin.CD q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive rate-distortion analysis suffers from the curse of dimensionality:\nclustering arbitrarily long pasts to retain information about arbitrarily long\nfutures requires resources that typically grow exponentially with length. The\nchallenge is compounded for infinite-order Markov processes, since conditioning\non finite sequences cannot capture all of their past dependencies. Spectral\narguments show that algorithms which cluster finite-length sequences fail\ndramatically when the underlying process has long-range temporal correlations\nand can fail even for processes generated by finite-memory hidden Markov\nmodels. We circumvent the curse of dimensionality in rate-distortion analysis\nof infinite-order processes by casting predictive rate-distortion objective\nfunctions in terms of the forward- and reverse-time causal states of\ncomputational mechanics. Examples demonstrate that the resulting causal\nrate-distortion theory substantially improves current predictive\nrate-distortion analyses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 05:23:27 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Marzen", "Sarah", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1412.3064", "submitter": "Peteris Daugulis", "authors": "Peteris Daugulis", "title": "Homomorphisms of connectome graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study homomorphisms of connectome graphs. Homomorphisms can be\nstudied as sequences of elementary homomorphisms - folds, which identify pairs\nof vertices. Several fold types are defined. Initial computation results for\nsome connectome graphs are described.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 14:50:50 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Daugulis", "Peteris", ""]]}, {"id": "1412.3151", "submitter": "Vince Grolmusz", "authors": "Balazs Szalkai, Csaba Kerepesi, Balint Varga, Vince Grolmusz", "title": "The Budapest Reference Connectome Server v2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connectomes of different human brains are pairwise distinct: we cannot\ntalk about an abstract \"graph of the brain\". Two typical connectomes, however,\nhave quite a few common graph edges that may describe the same connections\nbetween the same cortical areas. The Budapest Reference Connectome Server Ver.\n2.0 (http://connectome.pitgroup.org) generates the common edges of the\nconnectomes of 96 distinct cortexes, each with 1015 vertices, computed from 96\nMRI data sets of the Human Connectome Project. The user may set numerous\nparameters for the identification and filtering of common edges, and the graphs\nare downloadable in both csv and GraphML formats; both formats carry the\nanatomical annotations of the vertices, generated by the Freesurfer program.\nThe resulting consensus graph is also automatically visualized in a 3D rotating\nbrain model on the website.\n  The consensus graphs, generated with various parameter settings, can be used\nas reference connectomes based on different, independent MRI images, therefore\nthey may serve as reduced-error, low-noise, robust graph representations of the\nhuman brain.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 22:59:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 17:22:17 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Szalkai", "Balazs", ""], ["Kerepesi", "Csaba", ""], ["Varga", "Balint", ""], ["Grolmusz", "Vince", ""]]}, {"id": "1412.3410", "submitter": "Zachary Kilpatrick PhD", "authors": "Daniel Poll and Zachary P. Kilpatrick", "title": "Stochastic motion of bumps in planar neural fields", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.PS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the effects of spatiotemporal noise on stationary pulse solutions\n(bumps) in neural field equations on planar domains. Neural fields are\nintegrodifferential equations whose integral kernel describes the strength and\npolarity of synaptic interactions between neurons at different spatial\nlocations of the network. Fluctuations in neural activity are incorporated by\nmodeling the system as a Langevin equation evolving on a planar domain. Noise\ncauses bumps to wander about the domain in a purely diffusive way. Utilizing a\nsmall noise expansion along with a solvability condition, we can derive an\neffective stochastic equation describing the bump dynamics as two-dimensional\nBrownian motion. The diffusion coefficient can then be computed explicitly. We\nalso show that weak external inputs can pin the bump so it no longer wanders\ndiffusively. Inputs reshape the effective potential that guides the dynamics of\nthe bump position, so it tends to lie near attractors which can be single\npoints or contours in the plane. Perturbative analysis shows the bump position\nevolves as a multivariate Ornstein-Uhlenbeck process whose relaxation constants\nare determined by the shape of the input. Our analytical approximations all\ncompare well to statistics of bump motion in numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:02:59 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 01:23:29 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Poll", "Daniel", ""], ["Kilpatrick", "Zachary P.", ""]]}, {"id": "1412.3889", "submitter": "Zachary Kilpatrick PhD", "authors": "Zachary P. Kilpatrick", "title": "Stochastic synchronization of neural activity waves", "comments": "6 pages, 4 figures", "journal-ref": "Phys. Rev. E 91, 040701 (2015)", "doi": "10.1103/PhysRevE.91.040701", "report-no": null, "categories": "nlin.PS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that waves in distinct layers of a neuronal network can become\nphase-locked by common spatiotemporal noise. This phenomenon is studied for\nstationary bumps, traveling waves, and breathers. A weak noise expansion is\nused to derive an effective equation for the position of the wave in each\nlayer, yielding a stochastic differential equation with multiplicative noise.\nStability of the synchronous state is characterized by a Lyapunov exponent,\nwhich we can compute analytically from the reduced system. Our results extend\nprevious work on limit-cycle oscillators, showing common noise can synchronize\nwaves in a broad class of models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 05:07:52 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 21:26:21 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 21:31:45 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 22:05:42 GMT"}, {"version": "v5", "created": "Sat, 28 Mar 2015 04:52:29 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kilpatrick", "Zachary P.", ""]]}, {"id": "1412.3925", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, INRIA Saclay - Ile de France), Elvis\n  Dohmatob (INRIA Saclay - Ile de France), Bertrand Thirion (NEUROSPIN, INRIA\n  Saclay - Ile de France), Dimitris Samaras, Gael Varoquaux (NEUROSPIN, INRIA\n  Saclay - Ile de France)", "title": "Region segmentation for sparse decompositions: better brain\n  parcellations from rest fMRI", "comments": null, "journal-ref": "Sparsity Techniques in Medical Imaging, Sep 2014, Boston, United\n  States. pp.8", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Images acquired during resting-state provide\ninformation about the functional organization of the brain through measuring\ncorrelations between brain areas. Independent components analysis is the\nreference approach to estimate spatial components from weakly structured data\nsuch as brain signal time courses; each of these components may be referred to\nas a brain network and the whole set of components can be conceptualized as a\nbrain functional atlas. Recently, new methods using a sparsity prior have\nemerged to deal with low signal-to-noise ratio data. However, even when using\nsophisticated priors, the results may not be very sparse and most often do not\nseparate the spatial components into brain regions. This work presents\npost-processing techniques that automatically sparsify brain maps and separate\nregions properly using geometric operations, and compares these techniques\naccording to faithfulness to data and stability metrics. In particular, among\nthreshold-based approaches, hysteresis thresholding and random walker\nsegmentation, the latter improves significantly the stability of both dense and\nsparse models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 09:00:30 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, INRIA Saclay - Ile de France"], ["Dohmatob", "Elvis", "", "INRIA Saclay - Ile de France"], ["Thirion", "Bertrand", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"], ["Samaras", "Dimitris", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"], ["Varoquaux", "Gael", "", "NEUROSPIN, INRIA\n  Saclay - Ile de France"]]}, {"id": "1412.4920", "submitter": "Tim Palmer", "authors": "T.N. Palmer, M. O'Shea", "title": "Neuronal noise as a physical resource for human cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of energy-efficient digital microprocessor is being developed\nwhich is susceptible to thermal noise and consequently operates in\nprobabilistic rather than conventional deterministic mode. Hybrid computing\nsystems which combine probabilistic and deterministic processors can provide\nrobust and efficient tools for computational problems that hitherto would be\nintractable by conventional deterministic algorithm. These developments suggest\na revised perspective on the consequences of ion-channel noise in slender\naxons, often regarded as a hindrance to neuronal computations. It is proposed\nthat the human brain is such an energy-efficient hybrid computational system\nwhose remarkable characteristics emerge from constructive synergies between\nprobabilistic and deterministic modes of operation. In particular, the capacity\nfor intuition and creative problem solving appears to arise naturally from such\na hybrid system. Bearing in mind that physical thermal noise is both pure and\navailable at no cost, our proposal has implications for attempts to emulate the\nenergy-efficient human brain on conventional energy-intensive deterministic\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 08:42:05 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Palmer", "T. N.", ""], ["O'Shea", "M.", ""]]}, {"id": "1412.5004", "submitter": "Aldana Mar\\'ia Gonz\\'alez Montoro", "authors": "Aldana M. Gonz\\'alez Montoro, Ricardo Cao, Nelson Espinosa, Javier\n  Cudeiro, Jorge Mari\\~no", "title": "Bootstrap testing for cross-correlation under low firing activity", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s10827-015-0557-5", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new cross-correlation synchrony index for neural activity is proposed. The\nindex is based on the integration of the kernel estimation of the\ncross-correlation function. It is used to test for the dynamic synchronization\nlevels of spontaneous neural activity under two induced brain states:\nsleep-like and awake-like. Two bootstrap resampling plans are proposed to\napproximate the distribution of the test statistics. The results of the first\nbootstrap method indicate that it is useful to discern significant differences\nin the synchronization dynamics of brain states characterized by a neural\nactivity with low firing rate. The second bootstrap method is useful to unveil\nsubtle differences in the synchronization levels of the awake-like state,\ndepending on the activation pathway.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 14:10:24 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 18:22:07 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Montoro", "Aldana M. Gonz\u00e1lez", ""], ["Cao", "Ricardo", ""], ["Espinosa", "Nelson", ""], ["Cudeiro", "Javier", ""], ["Mari\u00f1o", "Jorge", ""]]}, {"id": "1412.5862", "submitter": "Wolfgang Maass Prof.", "authors": "Zeno Jonke, Stefan Habenschuss, Wolfgang Maass", "title": "A theoretical basis for efficient computations with noisy spiking\n  neurons", "comments": "main paper: 21 pages, 5 figures supplemental paper: 11 pages, no\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network of neurons in the brain apply - unlike processors in our current\ngeneration of computer hardware - an event-based processing strategy, where\nshort pulses (spikes) are emitted sparsely by neurons to signal the occurrence\nof an event at a particular point in time. Such spike-based computations\npromise to be substantially more power-efficient than traditional clocked\nprocessing schemes. However it turned out to be surprisingly difficult to\ndesign networks of spiking neurons that are able to carry out demanding\ncomputations. We present here a new theoretical framework for organizing\ncomputations of networks of spiking neurons. In particular, we show that a\nsuitable design enables them to solve hard constraint satisfaction problems\nfrom the domains of planning - optimization and verification - logical\ninference. The underlying design principles employ noise as a computational\nresource. Nevertheless the timing of spikes (rather than just spike rates)\nplays an essential role in the resulting computations. Furthermore, one can\ndemonstrate for the Traveling Salesman Problem a surprising computational\nadvantage of networks of spiking neurons compared with traditional artificial\nneural networks and Gibbs sampling. The identification of such advantage has\nbeen a well-known open problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 14:12:54 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Jonke", "Zeno", ""], ["Habenschuss", "Stefan", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1412.5909", "submitter": "Peter Erdi", "authors": "P\\'eter \\'Erdi", "title": "Teaching Computational Neuroscience", "comments": "8 pages", "journal-ref": null, "doi": "10.1007/s11571-015-9340-6.", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems and beauty of teaching computational neuroscience are discussed\nby reviewing three new textbooks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 15:52:08 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 17:09:21 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["\u00c9rdi", "P\u00e9ter", ""]]}, {"id": "1412.5931", "submitter": "Philipp H\\\"ovel", "authors": "Vesna Vuksanovi\\'c and Philipp H\\\"ovel", "title": "Dynamic changes in network synchrony reveal resting-state functional\n  networks", "comments": "11 pages, 9 figures, 1 table", "journal-ref": null, "doi": "10.1063/1.4913526", "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental fMRI studies have shown that spontaneous brain activity i.e. in\nthe absence of any external input, exhibit complex spatial and temporal\npatterns of co-activity between segregated brain regions. These so-called\nlarge-scale resting-state functional connectivity networks represent\ndynamically organized neural assemblies interacting with each other in a\ncomplex way. It has been suggested that looking at the dynamical properties of\ncomplex patterns of brain functional co-activity may reveal neural mechanisms\nunderlying the dynamic changes in functional interactions. Here, we examine how\nglobal network dynamics is shaped by different network configurations, derived\nfrom realistic brain functional interactions. We focus on two main dynamics\nmeasures: synchrony and variations in synchrony. Neural activity and the\ninferred hemodynamic response of the network nodes are simulated using system\nof 90 FitzHugh-Nagumo neural models subject to system noise and time-delayed\ninteractions. These models are embedded into the topology of the complex brain\nfunctional interactions, whose architecture is additionally reduced to its main\nstructural pathways. In the simulated functional networks, patterns of\ncorrelated regional activity clearly arise from dynamical properties that\nmaximize synchrony and variations in synchrony. Our results on the fast changes\nof the level of the network synchrony also show how flexible changes in the\nlarge-scale network dynamics could be.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 16:36:01 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Vuksanovi\u0107", "Vesna", ""], ["H\u00f6vel", "Philipp", ""]]}, {"id": "1412.6325", "submitter": "Robert Leech", "authors": "Peter J. Hellyer, Barbara Jachs, Robert Leech, Claudia Clopath", "title": "Local inhibitory plasticity tunes global brain dynamics and allows the\n  emergence of functional brain networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich, spontaneous brain activity has been observed across a range of\ndifferent temporal and spatial scales. These dynamics are thought to be\nimportant t for efficient neural functioning. Experimental evidence suggests\nthat these neural dynamics are maintained across a variety of different\ncognitive states, in response to alterations of the environment and to changes\nin brain configuration (e.g., across individuals, development and in many\nneurological disorders). This suggests that the brain has evolved mechanisms to\nstabilize dynamics and maintain them across a range of situations. Here, we\nemploy a local homeostatic inhibitory plasticity mechanism, balancing\ninhibitory and excitatory activity in a model of macroscopic brain activity\nbased on white-matter structural connectivity. We demonstrate that the addition\nof homeostatic plasticity regulates network activity and allows for the\nemergence of rich, spontaneous dynamics across a range of brain configurations.\nFurthermore, the presence of homeostatic plasticity maximises the overlap\nbetween empirical and simulated patterns of functional connectivity. Therefore,\nthis work presents a simple, local, biologically plausible inhibitory mechanism\nthat allows stable dynamics to emerge in the brain and which facilitates the\nformation of functional connectivity networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 13:08:04 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 00:02:22 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Hellyer", "Peter J.", ""], ["Jachs", "Barbara", ""], ["Leech", "Robert", ""], ["Clopath", "Claudia", ""]]}, {"id": "1412.6341", "submitter": "Peteris Daugulis", "authors": "Peteris Daugulis", "title": "Connectome graphs and maximum flow problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study maximum flow problems for connectome graphs. We suggest a\nfew computational problems: finding vertex pairs with maximal flow, finding new\nedges which would increase the maximal flow. Initial computation results for\nsome publicly available connectome graphs are described.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 13:45:26 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Daugulis", "Peteris", ""]]}, {"id": "1412.6383", "submitter": "Pierre de Buyl", "authors": "Christophe Pouzat, Georgios Is. Detorakis", "title": "SPySort: Neuronal Spike Sorting with Python", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-05", "categories": "cs.CE q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extracellular recordings with multi-electrode arrays is one of the basic\ntools of contemporary neuroscience. These recordings are mostly used to monitor\nthe activities, understood as sequences of emitted action potentials, of many\nindividual neurons. But the raw data produced by extracellular recordings are\nmost commonly a mixture of activities from several neurons. In order to get the\nactivities of the individual contributing neurons, a pre-processing step called\nspike sorting is required. We present here a pure Python implementation of a\nwell tested spike sorting procedure. The latter was designed in a modular way\nin order to favour a smooth transition from an interactive sorting, for\ninstance with IPython, to an automatic one. Surprisingly enough - or sadly\nenough, depending on one's view point -, recoding our now 15 years old\nprocedure into Python was the occasion of major methodological improvements.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:40:00 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Pouzat", "Christophe", ""], ["Detorakis", "Georgios Is.", ""]]}, {"id": "1412.6469", "submitter": "Marc Box", "authors": "Marc Box, Matt W. Jones, Nick Whiteley", "title": "A hidden Markov model for decoding and the analysis of replay in spike\n  trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hidden Markov model that describes variation in an animal's\nposition associated with varying levels of activity in action potential spike\ntrains of individual place cell neurons. The model incorporates a\ncoarse-graining of position, which we find to be a more parsimonious\ndescription of the system than other models. We use a sequential Monte Carlo\nalgorithm for Bayesian inference of model parameters, including the state space\ndimension, and we explain how to estimate position from spike train\nobservations (decoding). We obtain greater accuracy over other methods in the\nconditions of high temporal resolution and small neuronal sample size. We also\npresent a novel, model-based approach to the study of replay: the expression of\nspike train activity related to behaviour during times of motionlessness or\nsleep, thought to be integral to the consolidation of long-term memories. We\ndemonstrate how we can detect the time, information content and compression\nrate of replay events in simulated and real hippocampal data recorded from rats\nin two different environments, and verify the correlation between the times of\ndetected replay events and of sharp wave/ripples in the local field potential.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:08:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Box", "Marc", ""], ["Jones", "Matt W.", ""], ["Whiteley", "Nick", ""]]}, {"id": "1412.6502", "submitter": "Siddharth Pramod", "authors": "Siddharth Pramod, Adam Page, Tinoosh Mohsenin and Tim Oates", "title": "Detecting Epileptic Seizures from EEG Data using Neural Networks", "comments": "This paper has been withdrawn by the authors due to an error\n  discovered in the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of neural networks trained with dropout in predicting\nepileptic seizures from electroencephalographic data (scalp EEG). The input to\nthe neural network is a 126 feature vector containing 9 features for each of\nthe 14 EEG channels obtained over 1-second, non-overlapping windows. The models\nin our experiments achieved high sensitivity and specificity on patient records\nnot used in the training process. This is demonstrated using\nleave-one-out-cross-validation across patient records, where we hold out one\npatient's record as the test set and use all other patients' records for\ntraining; repeating this procedure for all patients in the database.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:00:38 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 01:20:42 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 00:42:42 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2015 15:24:05 GMT"}, {"version": "v5", "created": "Thu, 6 Dec 2018 03:15:14 GMT"}, {"version": "v6", "created": "Mon, 4 Feb 2019 00:41:35 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Pramod", "Siddharth", ""], ["Page", "Adam", ""], ["Mohsenin", "Tinoosh", ""], ["Oates", "Tim", ""]]}, {"id": "1412.6602", "submitter": "Richard Golden Professor", "authors": "Shaurabh Nandy and Richard M. Golden", "title": "Generative Modeling of Hidden Functional Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity refers to the temporal statistical relationship\nbetween spatially distinct brain regions and is usually inferred from the time\nseries coherence/correlation in brain activity between regions of interest. In\nhuman functional brain networks, the network structure is often inferred from\nfunctional magnetic resonance imaging (fMRI) blood oxygen level dependent\n(BOLD) signal. Since the BOLD signal is a proxy for neuronal activity, it is of\ninterest to learn the latent functional network structure. Additionally,\ndespite a core set of observations about functional networks such as\nsmall-worldness, modularity, exponentially truncated degree distributions, and\npresence of various types of hubs, very little is known about the computational\nprinciples which can give rise to these observations. This paper introduces a\nHidden Markov Random Field framework for the purpose of representing,\nestimating, and evaluating latent neuronal functional relationships between\ndifferent brain regions using fMRI data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 04:52:54 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 15:12:30 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Nandy", "Shaurabh", ""], ["Golden", "Richard M.", ""]]}, {"id": "1412.7955", "submitter": "Santosh Vempala", "authors": "Christos H. Papadimitriou and Santosh S. Vempala", "title": "Unsupervised Learning through Prediction in a Model of Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DS q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a primitive called PJOIN, for \"predictive join,\" which combines\nand extends the operations JOIN and LINK, which Valiant proposed as the basis\nof a computational theory of cortex. We show that PJOIN can be implemented in\nValiant's model. We also show that, using PJOIN, certain reasonably complex\nlearning and pattern matching tasks can be performed, in a way that involves\nphenomena which have been observed in cognition and the brain, namely\nmemory-based prediction and downward traffic in the cortical hierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:41:04 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Papadimitriou", "Christos H.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1412.7958", "submitter": "Alexander K. Vidybida", "authors": "Alexander K.Vidybida", "title": "Relation between firing statistics of spiking neuron with instantaneous\n  feedback and without feedback", "comments": "5 pages letter. In this version, section about moments and variances\n  is added, text style and language improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wide class of spiking neuron models, defined by rather general\nset of conditions typical for basic models like leaky integrate and fire, or\nbinding neuron model. A neuron is fed with a point renewal process. A relation\nbetween the three probability density functions (pdf): (i) pdf of input\ninterspike intervals, (ii) pdf of output interspike intervals of a neuron with\ninstantaneous feedback and (iii) pdf for that same neuron without feedback is\nderived. This allows to calculate any of the three pdfs provided the another\ntwo are given. Similar relation between corresponding means and variances is\nderived. The relations are checked exactly for the binding neuron model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 16:50:28 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 12:57:09 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 14:59:51 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Vidybida", "Alexander K.", ""]]}]