[{"id": "2105.00425", "submitter": "Islem Rekik", "authors": "Megi Isallari and Islem Rekik", "title": "Brain Graph Super-Resolution Using Adversarial Graph Neural Network with\n  Application to Functional Brain Connectivity", "comments": "arXiv admin note: text overlap with arXiv:2009.11080", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain image analysis has advanced substantially in recent years with the\nproliferation of neuroimaging datasets acquired at different resolutions. While\nresearch on brain image super-resolution has undergone a rapid development in\nthe recent years, brain graph super-resolution is still poorly investigated\nbecause of the complex nature of non-Euclidean graph data. In this paper, we\npropose the first-ever deep graph super-resolution (GSR) framework that\nattempts to automatically generate high-resolution (HR) brain graphs with N'\nnodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR)\ngraphs with N nodes where N < N'. First, we formalize our GSR problem as a node\nfeature embedding learning task. Once the HR nodes' embeddings are learned, the\npairwise connectivity strength between brain ROIs can be derived through an\naggregation rule based on a novel Graph U-Net architecture. While typically the\nGraph U-Net is a node-focused architecture where graph embedding depends mainly\non node attributes, we propose a graph-focused architecture where the node\nfeature embedding is based on the graph topology. Second, inspired by graph\nspectral theory, we break the symmetry of the U-Net architecture by\nsuper-resolving the low-resolution brain graph structure and node content with\na GSR layer and two graph convolutional network layers to further learn the\nnode embeddings in the HR graph. Third, to handle the domain shift between the\nground-truth and the predicted HR brain graphs, we incorporate adversarial\nregularization to align their respective distributions. Our proposed AGSR-Net\nframework outperformed its variants for predicting high-resolution functional\nbrain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub\nat https://github.com/basiralab/AGSR-Net.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:09:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Isallari", "Megi", ""], ["Rekik", "Islem", ""]]}, {"id": "2105.01223", "submitter": "Luyan Yu", "authors": "Luyan Yu and Thibaud Taillefumier", "title": "Metastable spiking networks in the replica-mean-field limit", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing metastable neural dynamics in finite-size spiking networks\nremains a daunting challenge. We propose to address this challenge in the\nrecently introduced replica-mean-field (RMF) limit. In this limit, networks are\nmade of infinitely many replicas of the finite network of interest, but with\nrandomized interactions across replica. Such randomization renders certain\nexcitatory networks fully tractable at the cost of neglecting activity\ncorrelations, but with explicit dependence on the finite size of the neural\nconstituents. However, metastable dynamics typically unfold in networks with\nmixed inhibition and excitation. Here, we extend the RMF computational\nframework to point-process-based neural network models with exponential\nstochastic intensities, allowing for mixed excitation and inhibition. Within\nthis setting, we show that metastable finite-size networks admit multistable\nRMF limits, which are fully characterized by stationary firing rates.\nTechnically, these stationary rates are determined as solutions to a set of\ndelayed differential equations under certain regularity conditions that any\nphysical solutions shall satisfy. We solve this original problem by combining\nthe resolvent formalism and singular-perturbation theory. Importantly, we find\nthat these rates specify probabilistic pseudo-equilibria which accurately\ncapture the neural variability observed in the original finite-size network. We\nalso discuss the emergence of metastability as a stochastic bifurcation, which\ncan also be interpreted as a static phase transition in the RMF limits. In\nturn, we expect to leverage the static picture of RMF limits to infer purely\ndynamical features of metastable finite-size networks, such as the transition\nrates between pseudo-equilibria.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 00:22:37 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 05:04:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yu", "Luyan", ""], ["Taillefumier", "Thibaud", ""]]}, {"id": "2105.01358", "submitter": "Apoorv Kishore", "authors": "Apoorv Kishore, Vivek Saraswat, Udayan Ganguly", "title": "Simplified Klinokinesis using Spiking Neural Networks for\n  Resource-Constrained Navigation on the Neuromorphic Processor Loihi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  C. elegans shows chemotaxis using klinokinesis where the worm senses the\nconcentration based on a single concentration sensor to compute the\nconcentration gradient to perform foraging through gradient ascent/descent\ntowards the target concentration followed by contour tracking. The biomimetic\nimplementation requires complex neurons with multiple ion channel dynamics as\nwell as interneurons for control. While this is a key capability of autonomous\nrobots, its implementation on energy-efficient neuromorphic hardware like\nIntel's Loihi requires adaptation of the network to hardware-specific\nconstraints, which has not been achieved. In this paper, we demonstrate the\nadaptation of chemotaxis based on klinokinesis to Loihi by implementing\nnecessary neuronal dynamics with only LIF neurons as well as a complete\nspike-based implementation of all functions e.g. Heaviside function and\nsubtractions. Our results show that Loihi implementation is equivalent to the\nsoftware counterpart on Python in terms of performance - both during foraging\nand contour tracking. The Loihi results are also resilient in noisy\nenvironments. Thus, we demonstrate a successful adaptation of chemotaxis on\nLoihi - which can now be combined with the rich array of SNN blocks for SNN\nbased complex robotic control.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 08:26:46 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kishore", "Apoorv", ""], ["Saraswat", "Vivek", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2105.01395", "submitter": "Fabian Pallasdies", "authors": "Fabian Pallasdies, Philipp Norton, Jan-Hendrik Schleimer, Susanne\n  Schreiber", "title": "Neural Optimization: Understanding Trade-offs with Pareto Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nervous systems, like any organismal structure, have been shaped by\nevolutionary processes to increase fitness. The resulting neural 'bauplan' has\nto account for multiple objectives simultaneously, including computational\nfunction as well as additional factors like robustness to environmental changes\nand energetic limitations. Oftentimes these objectives compete and a\nquantification of the relative impact of individual optimization targets is\nnon-trivial. Pareto optimality offers a theoretical framework to decipher\nobjectives and trade-offs between them. We, therefore, highlight Pareto theory\nas a useful tool for the analysis of neurobiological systems, from\nbiophysically-detailed cells to large-scale network structures and behavior.\nThe Pareto approach can help to assess optimality, identify relevant objectives\nand their respective impact, and formulate testable hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:12:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Pallasdies", "Fabian", ""], ["Norton", "Philipp", ""], ["Schleimer", "Jan-Hendrik", ""], ["Schreiber", "Susanne", ""]]}, {"id": "2105.01410", "submitter": "Danko Georgiev", "authors": "Danko D. Georgiev", "title": "Quantum information in neural systems", "comments": "31 pages, 7 figures", "journal-ref": "Symmetry 2021; 13(5): 773", "doi": "10.3390/sym13050773", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the physiological processes in the central nervous system that\nunderlie our conscious experiences has been at the forefront of cognitive\nneuroscience. While the principles of classical physics were long found to be\nunaccommodating for a causally effective consciousness, the inherent\nindeterminism of quantum physics, together with its characteristic dichotomy\nbetween quantum states and quantum observables, provides a fertile ground for\nthe physical modeling of consciousness. Here, we utilize the Schr\\\"odinger\nequation, together with the Planck--Einstein relation between energy and\nfrequency, in order to determine the appropriate quantum dynamical timescale of\nconscious processes. Furthermore, with the help of a simple two-qubit toy model\nwe illustrate the importance of non-zero interaction Hamiltonian for the\ngeneration of quantum entanglement and manifestation of observable correlations\nbetween different measurement outcomes. Employing a quantitative measure of\nentanglement based on Schmidt decomposition, we show that quantum evolution\ngoverned only by internal Hamiltonians for the individual quantum subsystems\npreserves quantum coherence of separable initial quantum states, but eliminates\nthe possibility of any interaction and quantum entanglement. The presence of\nnon-zero interaction Hamiltonian, however, allows for decoherence of the\nindividual quantum subsystems along with their mutual interaction and quantum\nentanglement. The presented results show that quantum coherence of individual\nsubsystems cannot be used for cognitive binding because it is a physical\nmechanism that leads to separability and non-interaction. In contrast, quantum\ninteractions with their associated decoherence of individual subsystems are\ninstrumental for dynamical changes in the quantum entanglement of the composite\nquantum state vector and manifested correlations of different observable\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:43:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Georgiev", "Danko D.", ""]]}, {"id": "2105.01570", "submitter": "Martin Miguel", "authors": "Martin Miguel, Pablo Riera and Diego Fernandez Slezak", "title": "Simple and Cheap Setup for Timing Tapping Responses Synchronized to\n  Auditory Stimuli", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring human capabilities to synchronize in time, adapt to perturbations\nto timing sequences or reproduce time intervals often require experimental\nsetups that allow recording response times with millisecond precision. Most\nsetups present auditory stimuli using either MIDI devices or specialized\nhardware such as Arduino and are often expensive or require calibration and\nadvanced programming skills. Here, we present in detail an experimental setup\nthat only requires an external sound card and minor electronic skills, works on\na conventional PC, is cheaper than alternatives and requires almost no\nprogramming skills. It is intended for presenting any auditory stimuli and\nrecording tapping response times with within 2 milliseconds precision (up to\n-2ms lag). This paper shows why desired accuracy in recording response times\nagainst auditory stimuli is difficult to achieve in conventional computer\nsetups, presents an experimental setup to overcome this and explains in detail\nhow to set it up and use the provided code. Finally, the code for analyzing the\nrecorded tapping responses was evaluated, showing that no spurious or missing\nevents were found in 94% of the analyzed recordings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 21:30:40 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 23:40:23 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Miguel", "Martin", ""], ["Riera", "Pablo", ""], ["Slezak", "Diego Fernandez", ""]]}, {"id": "2105.02716", "submitter": "Hidenori Tanaka", "authors": "Hidenori Tanaka, Daniel Kunin", "title": "Noether's Learning Dynamics: The Role of Kinetic Symmetry Breaking in\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nature, symmetry governs regularities, while symmetry breaking brings\ntexture. Here, we reveal a novel role of symmetry breaking behind efficiency\nand stability in learning, a critical issue in machine learning. Recent\nexperiments suggest that the symmetry of the loss function is closely related\nto the learning performance. This raises a fundamental question. Is such\nsymmetry beneficial, harmful, or irrelevant to the success of learning? Here,\nwe demystify this question and pose symmetry breaking as a new design principle\nby considering the symmetry of the learning rule in addition to the loss\nfunction. We model the discrete learning dynamics using a continuous-time\nLagrangian formulation, in which the learning rule corresponds to the kinetic\nenergy and the loss function corresponds to the potential energy. We identify\nkinetic asymmetry unique to learning systems, where the kinetic energy often\ndoes not have the same symmetry as the potential (loss) function reflecting the\nnon-physical symmetries of the loss function and the non-Euclidean metric used\nin learning rules. We generalize Noether's theorem known in physics to\nexplicitly take into account this kinetic asymmetry and derive the resulting\nmotion of the Noether charge. Finally, we apply our theory to modern deep\nnetworks with normalization layers and reveal a mechanism of implicit adaptive\noptimization induced by the kinetic symmetry breaking.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:36:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tanaka", "Hidenori", ""], ["Kunin", "Daniel", ""]]}, {"id": "2105.02786", "submitter": "Yi Ding", "authors": "Yi Ding, Neethu Robinson, Qiuhao Zeng, Cuntai Guan", "title": "LGGNet: Learning from Local-Global-Graph Representations for\n  Brain-Computer Interface", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose LGG, a neurologically inspired graph neural\nnetwork, to learn local-global-graph representations from\nElectroencephalography (EEG) for a Brain-Computer Interface (BCI). A temporal\nconvolutional layer with multi-scale 1D convolutional kernels and kernel-level\nattention fusion is proposed to learn the temporal dynamics of EEG. Inspired by\nneurological knowledge of cognitive processes in the brain, we propose local\nand global graph-filtering layers to learn the brain activities within and\nbetween different functional areas of the brain to model the complex relations\namong them during the cognitive processes. Under the robust nested\ncross-validation settings, the proposed method is evaluated on the publicly\navailable dataset DEAP, and the classification performance is compared with\nstate-of-the-art methods, such as FBFgMDM, FBTSC, Unsupervised learning,\nDeepConvNet, ShallowConvNet, EEGNet, and TSception. The results show that the\nproposed method outperforms all these state-of-the-art methods, and the\nimprovements are statistically significant (p<0.05) in most cases. The source\ncode can be found at: https://github.com/yi-ding-cs/LGG\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:06:34 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ding", "Yi", ""], ["Robinson", "Neethu", ""], ["Zeng", "Qiuhao", ""], ["Guan", "Cuntai", ""]]}, {"id": "2105.02805", "submitter": "Christoforos Papasavvas", "authors": "Christoforos Papasavvas, Peter Neal Taylor, Yujiang Wang", "title": "Long-term changes in functional connectivity predict responses to\n  intracranial stimulation of the human brain", "comments": "Article, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Targeted electrical stimulation of the brain perturbs neural networks and\nmodulates their rhythmic activity both at the site of stimulation and at remote\nbrain regions. Understanding, or even predicting, this neuromodulatory effect\nis crucial for any therapeutic use of brain stimulation. To this end, we\nanalyzed the stimulation responses in 131 stimulation sessions across 66\npatients with focal epilepsy recorded through intracranial EEG (iEEG). We\nconsidered functional and structural connectivity features as predictors of the\nresponse at every iEEG contact. Taking advantage of multiple recordings over\ndays, we also investigated how slow changes in interictal functional\nconnectivity (FC) ahead of the stimulation relate to stimulation responses. The\nresults reveal that, indeed, this long-term variability of FC exhibits strong\nassociation with the stimulation-induced increases in delta and theta band\npower. Furthermore, we show through cross-validation that long-term variability\nof FC improves prediction of responses above the performance of spatial\npredictors alone. These findings can enhance the patient-specific design of\neffective neuromodulatory protocols for therapeutic interventions.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:51:08 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 09:46:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Papasavvas", "Christoforos", ""], ["Taylor", "Peter Neal", ""], ["Wang", "Yujiang", ""]]}, {"id": "2105.02811", "submitter": "Weikai Li", "authors": "Weikai Li, Yongxiang Tang, Zhengxia Wang, Shuo Hu and Xin Gao", "title": "The Reconfiguration Pattern of Individual Brain Metabolic Connectome for\n  Parkinson's Disease Identification", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Positron Emission Tomography (PET) with 18F-fluorodeoxyglucose\n(18F-FDG) reveals metabolic abnormalities in Parkinson's disease (PD) at a\nsystemic level. Previous metabolic connectome studies derived from groups of\npatients have failed to identify the individual neurophysiological details. We\naim to establish an individual metabolic connectome method to characterize the\naberrant connectivity patterns and topological alterations of the\nindividual-level brain metabolic connectome and their diagnostic value in PD.\nMethods: The 18F-FDG PET data of 49 PD patients and 49 healthy controls (HCs)\nwere recruited. Each individual's metabolic brain network was ascertained using\nthe proposed Jensen-Shannon Divergence Similarity Estimation (JSSE) method. The\nintergroup difference of the individual's metabolic brain network and its\nglobal and local graph metrics were analyzed to investigate the metabolic\nconnectome's alterations. The identification of the PD from HC individuals was\nused by the multiple kernel support vector machine (MK-SVM) to combine the\ninformation from connection and topological metrics. The validation was\nconducted using the nest leave-one-out cross-validation strategy to confirm the\nperformance of the methods. Results: The proposed JSSE metabolic connectome\nmethod showed the most involved metabolic motor networks were PUT-PCG, THA-PCG,\nand SMA pathways in PD, which was similar to the typical group-level method,\nand yielded another detailed individual pathological connectivity in ACG-PCL,\nDCG-PHG and ACG pathways. These aberrant functional network measures exhibited\nan ideal classification performance in the identifying of PD individuals from\nHC individuals at an accuracy of up to 91.84%.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:46:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Weikai", ""], ["Tang", "Yongxiang", ""], ["Wang", "Zhengxia", ""], ["Hu", "Shuo", ""], ["Gao", "Xin", ""]]}, {"id": "2105.03400", "submitter": "Sathish Ande", "authors": "Sathish Ande, Srinivas Avasarala, Ajith Karunarathne, Lopamudra Giri,\n  Soumya Jana", "title": "Correlation in Neuronal Calcium Spiking: Quantification based on\n  Empirical Mutual Information Rate", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.00723", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Quantification of neuronal correlations in neuron populations helps us to\nunderstand neural coding rules. Such quantification could also reveal how\nneurons encode information in normal and disease conditions like Alzheimer's\nand Parkinson's. While neurons communicate with each other by transmitting\nspikes, there would be a change in calcium concentration within the neurons\ninherently. Accordingly, there would be correlations in calcium spike trains\nand they could have heterogeneous memory structures. In this context,\nestimation of mutual information rate in calcium spike trains assumes primary\nsignificance. However, such estimation is difficult with available methods\nwhich would consider longer blocks for convergence without noticing that\nneuronal information changes in short time windows. Against this backdrop, we\npropose a faster method that exploits the memory structures in pair of calcium\nspike trains to quantify mutual information shared between them. Our method has\nshown superior performance with example Markov processes as well as\nexperimental spike trains. Such mutual information rate analysis could be used\nto identify signatures of neuronal behavior in large populations in normal and\nabnormal conditions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:19:02 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ande", "Sathish", ""], ["Avasarala", "Srinivas", ""], ["Karunarathne", "Ajith", ""], ["Giri", "Lopamudra", ""], ["Jana", "Soumya", ""]]}, {"id": "2105.03508", "submitter": "Heejong Bong", "authors": "Heejong Bong (1), Val\\'erie Ventura (1), Eric A. Yttri (3, 4 and 5),\n  Matthew A. Smith (4 and 5) and Robert E. Kass (1, 2 and 5) ((1) Department of\n  Statistics and Data Sciences, Carnegie Mellon University, (2) Machine\n  Learning Department, Carnegie Mellon University, (3) Department of Biological\n  Sciences, Carnegie Mellon University, (4) Department of Biomedical\n  Engineering, Carnegie Mellon University, (5) Neuroscience Institute, Carnegie\n  Mellon University)", "title": "Latent Cross-population Dynamic Time-series Analysis of High-dimensional\n  Neural Recordings", "comments": "19 pages, 9 figures, submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important problem in analysis of neural data is to characterize\ninteractions across brain regions from high-dimensional multiple-electrode\nrecordings during a behavioral experiment. Lead-lag effects indicate possible\ndirectional flows of neural information, but they are often transient,\nappearing during short intervals of time. Such non-stationary interactions can\nbe difficult to identify, but they can be found by taking advantage of the\nreplication structure inherent to many neurophysiological experiments. To\ndescribe non-stationary interactions between replicated pairs of\nhigh-dimensional time series, we developed a method of estimating latent,\nnon-stationary cross-correlation. Our approach begins with an extension of\nprobabilistic CCA to the time series setting, which provides a model-based\ninterpretation of multiset CCA. Because the covariance matrix describing\nnon-stationary dependence is high-dimensional, we assume sparsity of\ncross-correlations within a range of possible interesting lead-lag effects. We\nshow that the method can perform well in realistic settings and we apply it to\n192 simultaneous local field potential (LFP) recordings from prefrontal cortex\n(PFC) and visual cortex (area V4) during a visual memory task. We find lead-lag\nrelationships that are highly plausible, being consistent with related results\nin the literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:13:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bong", "Heejong", "", "3, 4 and 5"], ["Ventura", "Val\u00e9rie", "", "3, 4 and 5"], ["Yttri", "Eric A.", "", "3, 4 and 5"], ["Smith", "Matthew A.", "", "4 and 5"], ["Kass", "Robert E.", "", "1, 2 and 5"]]}, {"id": "2105.03548", "submitter": "Muhammad Abdulla", "authors": "Muhammad U. Abdulla, Ryan S. Phillips, Jonathan E. Rubin", "title": "Dynamics of ramping bursts in a respiratory neuron model", "comments": "23 pages, 10 figures, submitted to Journal of Computational\n  Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive computational and theoretical work has led to the development of\nmutliple mathematical models for bursting in respiratory neurons in the\npre-B\\\"otzinger Complex (pre-B\\\"otC) of the mammalian brainstem. Nonetheless,\nthese previous models have not captured the pre-inspiratory ramping aspects of\nthese neurons' activity patterns, in which relatively slow tonic spiking\ngradually progresses to faster spiking and a full blown burst, with a\ncorresponding gradual development of an underlying plateau potential. In this\nwork, we show that the incorporation of the dynamics of the extracellular\npotassium ion concentration into an existing model for pre-B\\\"otC neuron\nbursting, along with some parameter updates, suffices to induce this ramping\nbehavior. Using fast-slow decomposition, we show that this activity can be\nconsidered as a form of parabolic bursting, but with burst termination at a\nhomoclinic bifurcation rather than as a SNIC bifurcation. We also investigate\nthe parameter-dependence of these solutions and show that the proposed model\nyields a greater dynamic range of burst frequencies, durations, and duty cycles\nthan those produced by other models in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 01:21:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Abdulla", "Muhammad U.", ""], ["Phillips", "Ryan S.", ""], ["Rubin", "Jonathan E.", ""]]}, {"id": "2105.03961", "submitter": "Thomas Schmidt", "authors": "Thomas Schmidt", "title": "Is response priming based on surface color? Response to Skrzypulec\n  (2021)", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skrzypulec (2021) raises the question whether motor activation by masked\ncolor primes is based on the same type of color representation as conscious\nvision. He postulates that the literature on color processing without awareness\nmakes an implicit assumption that \"conscious\" and \"unconscious\" color\nrepresentations have the same properties, in which case priming by masked color\nstimuli would indeed indicate that the same complex representation of surface\ncolor can be conscious as well as unconscious. I review some evidence from\nresponse priming by lightness stimuli in the context of a visual illusion that\nalters the perceived lightness of the primes (Schmidt et al., 2010). Those\nresults clearly show that response priming is not driven by color-constant\ninformation but instead by local image contrast, making it unlikely that rapid\nresponse activation by color primes is based on a color-constant representation\nof surface color.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 15:36:57 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 09:13:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Schmidt", "Thomas", ""]]}, {"id": "2105.04261", "submitter": "Pablo Lanillos", "authors": "Pablo Lanillos, Marcel van Gerven", "title": "Neuroscience-inspired perception-action in robotics: applying active\n  inference for state estimation, control and self-perception", "comments": "Accepted at ICLR 2021 Brain2AI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike robots, humans learn, adapt and perceive their bodies by interacting\nwith the world. Discovering how the brain represents the body and generates\nactions is of major importance for robotics and artificial intelligence. Here\nwe discuss how neuroscience findings open up opportunities to improve current\nestimation and control algorithms in robotics. In particular, how active\ninference, a mathematical formulation of how the brain resists a natural\ntendency to disorder, provides a unified recipe to potentially solve some of\nthe major challenges in robotics, such as adaptation, robustness, flexibility,\ngeneralization and safe interaction. This paper summarizes some experiments and\nlessons learned from developing such a computational model on real embodied\nplatforms, i.e., humanoid and industrial robots. Finally, we showcase the\nlimitations and challenges that we are still facing to give robots human-like\nperception\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:59:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lanillos", "Pablo", ""], ["van Gerven", "Marcel", ""]]}, {"id": "2105.04643", "submitter": "Peter Taylor", "authors": "Peter N Taylor, Christoforos A Papasavvas, Thomas W Owen, Gabrielle M\n  Schroeder, Frances E Hutchings, Fahmida A Chowdhury, Beate Diehl, John S\n  Duncan, Andrew W McEvoy, Anna Miserocchi, Jane de Tisi, Sjoerd B Vos, Matthew\n  C Walker, Yujiang Wang", "title": "Normative brain mapping of interictal intracranial EEG to localise\n  epileptogenic tissue", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification of abnormal electrographic activity is important in a wide\nrange of neurological disorders, including epilepsy for localising\nepileptogenic tissue. However, this identification may be challenging during\nnon-seizure (interictal) periods, especially if abnormalities are subtle\ncompared to the repertoire of possible healthy brain dynamics. Here, we\ninvestigate if such interictal abnormalities become more salient by\nquantitatively accounting for the range of healthy brain dynamics in a\nlocation-specific manner. To this end, we constructed a normative map of brain\ndynamics, in terms of relative band power, from interictal intracranial\nrecordings from 234 subjects (21,598 electrode contacts). We then compared\ninterictal recordings from 62 patients with epilepsy to the normative map to\nidentify abnormal regions. We hypothesised that if the most abnormal regions\nwere spared by surgery, then patients would be more likely to experience\ncontinued seizures post-operatively. We first confirmed that the spatial\nvariations of band power in the normative map across brain regions were\nconsistent with healthy variations reported in the literature. Second, when\naccounting for the normative variations, regions which were spared by surgery\nwere more abnormal than those resected only in patients with persistent\npost-operative seizures (t=-3.6, p=0.0003), confirming our hypothesis. Third,\nwe found that this effect discriminated patient outcomes (AUC=0.75 p=0.0003).\nNormative mapping is a well-established practice in neuroscientific research.\nOur study suggests that this approach is feasible to detect interictal\nabnormalities in intracranial EEG, and of potential clinical value to identify\npathological tissue in epilepsy. Finally, we make our normative intracranial\nmap publicly available to facilitate future investigations in epilepsy and\nbeyond.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:56:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Taylor", "Peter N", ""], ["Papasavvas", "Christoforos A", ""], ["Owen", "Thomas W", ""], ["Schroeder", "Gabrielle M", ""], ["Hutchings", "Frances E", ""], ["Chowdhury", "Fahmida A", ""], ["Diehl", "Beate", ""], ["Duncan", "John S", ""], ["McEvoy", "Andrew W", ""], ["Miserocchi", "Anna", ""], ["de Tisi", "Jane", ""], ["Vos", "Sjoerd B", ""], ["Walker", "Matthew C", ""], ["Wang", "Yujiang", ""]]}, {"id": "2105.05002", "submitter": "Stefan Dasbach", "authors": "Stefan Dasbach, Tom Tetzlaff, Markus Diesmann, Johanna Senk", "title": "Prominent characteristics of recurrent neuronal networks are robust\n  against low synaptic weight resolution", "comments": "39 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of the natural-density, heterogeneous connectivity of\nneuronal network models at relevant spatial scales remains a challenge for\nComputational Neuroscience and Neuromorphic Computing. In particular, the\nmemory demands imposed by the vast number of synapses in brain-scale network\nsimulations constitutes a major obstacle. Limiting the number resolution of\nsynaptic weights appears to be a natural strategy to reduce memory and compute\nload. In this study, we investigate the effects of a limited synaptic-weight\nresolution on the dynamics of recurrent spiking neuronal networks resembling\nlocal cortical circuits, and develop strategies for minimizing deviations from\nthe dynamics of networks with high-resolution synaptic weights. We mimic the\neffect of a limited synaptic weight resolution by replacing normally\ndistributed synaptic weights by weights drawn from a discrete distribution, and\ncompare the resulting statistics characterizing firing rates, spike-train\nirregularity, and correlation coefficients with the reference solution. We show\nthat a naive discretization of synaptic weights generally leads to a distortion\nof the spike-train statistics. Only if the weights are discretized such that\nthe mean and the variance of the total synaptic input currents are preserved,\nthe firing statistics remains unaffected for the types of networks considered\nin this study. For networks with sufficiently heterogeneous in-degrees, the\nfiring statistics can be preserved even if all synaptic weights are replaced by\nthe mean of the weight distribution. We conclude that even for simple networks\nwith non-plastic neurons and synapses, a discretization of synaptic weights can\nlead to substantial deviations in the firing statistics, unless the\ndiscretization is performed with care and guided by a rigorous validation\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:08:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dasbach", "Stefan", ""], ["Tetzlaff", "Tom", ""], ["Diesmann", "Markus", ""], ["Senk", "Johanna", ""]]}, {"id": "2105.05070", "submitter": "Benedetta Mariani", "authors": "Benedetta Mariani, Giorgio Nicoletti, Marta Bisio, Marta Maschietto,\n  Stefano Vassanelli, Samir Suweis", "title": "On the critical signatures of neural activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The critical brain hypothesis has emerged as an attractive framework to\nunderstand the functional implications of the variability in brain activity.\nHowever, the mechanisms behind the observed emergent collective neural patterns\nremain unclear. Here we propose a modeling framework to reconcile apparent\ncontrasting results. We show that the presence of scale-free neuronal\navalanches and the associated crackling-noise relation can be explained by the\npresence of an extrinsic, stochastic-induced mutual information in the system,\nwhereas the intrinsic spatial correlation structure shows the typical features\nof systems close to their critical point. We test our results on data obtained\nfrom the rat's cortex through state-of-the-art multi-array probes.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:22:48 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Mariani", "Benedetta", ""], ["Nicoletti", "Giorgio", ""], ["Bisio", "Marta", ""], ["Maschietto", "Marta", ""], ["Vassanelli", "Stefano", ""], ["Suweis", "Samir", ""]]}, {"id": "2105.05382", "submitter": "Arna Ghosh", "authors": "Luke Y. Prince, Ellen Boven, Roy Henha Eyono, Arna Ghosh, Joe\n  Pemberton, Franz Scherr, Claudia Clopath, Rui Ponte Costa, Wolfgang Maass,\n  Blake A. Richards, Cristina Savin, Katharina Anna Wilmes", "title": "CCN GAC Workshop: Issues with learning in biological recurrent neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This perspective piece came about through the Generative Adversarial\nCollaboration (GAC) series of workshops organized by the Computational\nCognitive Neuroscience (CCN) conference in 2020. We brought together a number\nof experts from the field of theoretical neuroscience to debate emerging issues\nin our understanding of how learning is implemented in biological recurrent\nneural networks. Here, we will give a brief review of the common assumptions\nabout biological learning and the corresponding findings from experimental\nneuroscience and contrast them with the efficiency of gradient-based learning\nin recurrent neural networks commonly used in artificial intelligence. We will\nthen outline the key issues discussed in the workshop: synaptic plasticity,\nneural circuits, theory-experiment divide, and objective functions. Finally, we\nconclude with recommendations for both theoretical and experimental\nneuroscientists when designing new studies that could help to bring clarity to\nthese issues.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:59:40 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Prince", "Luke Y.", ""], ["Boven", "Ellen", ""], ["Eyono", "Roy Henha", ""], ["Ghosh", "Arna", ""], ["Pemberton", "Joe", ""], ["Scherr", "Franz", ""], ["Clopath", "Claudia", ""], ["Costa", "Rui Ponte", ""], ["Maass", "Wolfgang", ""], ["Richards", "Blake A.", ""], ["Savin", "Cristina", ""], ["Wilmes", "Katharina Anna", ""]]}, {"id": "2105.05592", "submitter": "Shimon Ullman", "authors": "Shimon Ullman, Liav Assif, Alona Strugatski, Ben-Zion Vatashsky, Hila\n  Levy, Aviv Netanyahu, Adam Yaari", "title": "Image interpretation by iterative bottom-up top-down processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene understanding requires the extraction and representation of scene\ncomponents together with their properties and inter-relations. We describe a\nmodel in which meaningful scene structures are extracted from the image by an\niterative process, combining bottom-up (BU) and top-down (TD) networks,\ninteracting through a symmetric bi-directional communication between them\n(counter-streams structure). The model constructs a scene representation by the\niterative use of three components. The first model component is a BU stream\nthat extracts selected scene elements, properties and relations. The second\ncomponent (cognitive augmentation) augments the extracted visual representation\nbased on relevant non-visual stored representations. It also provides input to\nthe third component, the TD stream, in the form of a TD instruction,\ninstructing the model what task to perform next. The TD stream then guides the\nBU visual stream to perform the selected task in the next cycle. During this\nprocess, the visual representations extracted from the image can be combined\nwith relevant non-visual representations, so that the final scene\nrepresentation is based on both visual information extracted from the scene and\nrelevant stored knowledge of the world. We describe how a sequence of\nTD-instructions is used to extract from the scene structures of interest,\nincluding an algorithm to automatically select the next TD-instruction in the\nsequence. The extraction process is shown to have favorable properties in terms\nof combinatorial generalization, generalizing well to novel scene structures\nand new combinations of objects, properties and relations not seen during\ntraining. Finally, we compare the model with relevant aspects of the human\nvision, and suggest directions for using the BU-TD scheme for integrating\nvisual and cognitive components in the process of scene understanding.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:10:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ullman", "Shimon", ""], ["Assif", "Liav", ""], ["Strugatski", "Alona", ""], ["Vatashsky", "Ben-Zion", ""], ["Levy", "Hila", ""], ["Netanyahu", "Aviv", ""], ["Yaari", "Adam", ""]]}, {"id": "2105.06057", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim and Woochang Lim", "title": "Dynamical Origin for Winner-Take-All Competition in A Biological Network\n  of The Hippocampal Dentate Gyrus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a biological network of the hippocampal dentate gyrus (DG). The\nDG is a pre-processor for pattern separation which facilitates pattern storage\nand retrieval in the CA3 area of the hippocampus. The main encoding cells in\nthe DG are the granule cells (GCs) which receive the input from the entorhinal\ncortex (EC) and send their output to the CA3. The activation degree of GCs is\nso low (~ 5%). This sparsity has been thought to enhance the pattern\nseparation. We investigate the dynamical origin for winner-take-all (WTA)\ncompetition which leads to sparse activation of the GCs. The whole GCs are\ngrouped into lamellar clusters. In each GC cluster, there is one inhibitory (I)\nbasket cell (BC) along with excitatory (E) GCs. There are three kinds of\nexternal inputs into the GCs; the direct excitatory EC input, the indirect\ninhibitory EC input, mediated by the HIPP (hilar perforant path-associated)\ncells, and the excitatory input from the hilar mossy cells (MCs). The firing\nactivities of the GCs are determined via competition between the external E and\nI inputs. The E-I conductance ratio ${\\cal{R}}_{\\rm E-I}^{\\rm (con)*}$ (given\nby the time average of the external E to I conductances) may represents well\nthe degree of such external E-I input competition. GCs become active when their\n$\\cal{R}_{\\rm E-I}^{\\rm (con)*}$ is larger than a threshold ${\\cal{R}}_{th}^*$,\nand then the mean firing rates of the active GCs are strongly correlated with\n$\\cal{R}_{\\rm E-I}^{\\rm (con)*}$. In each GC cluster, the feedback inhibition\nof the BC may select the winner GCs. GCs with larger $\\cal{R}_{\\rm E-I}^{\\rm\n(con)*}$ than the threshold ${\\cal{R}}_{th}^*$ survive, and they become\nwinners; all the other GCs with smaller $\\cal{R}_{\\rm E-I}^{\\rm (con)*}$ become\nsilent. The WTA competition occurs via competition between the firing activity\nof the GCs and the feedback inhibition from the BC in each GC cluster.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 03:18:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "2105.06418", "submitter": "Marco Antonio Pinto Orellana", "authors": "Marco Antonio Pinto-Orellana and Peyman Mirtaheri and Hugo L. Hammer\n  and Hernando Ombao", "title": "SCAU: Modeling spectral causality for multivariate time series with\n  applications to electroencephalograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electroencephalograms (EEG) are noninvasive measurement signals of electrical\nneuronal activity in the brain. One of the current major statistical challenges\nis formally measuring functional dependency between those complex signals. This\npaper, proposes the spectral causality model (SCAU), a robust linear model,\nunder a causality paradigm, to reflect inter- and intra-frequency modulation\neffects that cannot be identifiable using other methods. SCAU inference is\nconducted with three main steps: (a) signal decomposition into frequency bins,\n(b) intermediate spectral band mapping, and (c) dependency modeling through\nfrequency-specific autoregressive models (VAR). We apply SCAU to study complex\ndependencies during visual and lexical fluency tasks (word generation and\nvisual fixation) in 26 participants' EEGs. We compared the connectivity\nnetworks estimated using SCAU with respect to a VAR model. SCAU networks show a\nclear contrast for both stimuli while the magnitude links also denoted a low\nvariance in comparison with the VAR networks. Furthermore, SCAU dependency\nconnections not only were consistent with findings in the neuroscience\nliterature, but it also provided further evidence on the directionality of the\nspatio-spectral dependencies such as the delta-originated and theta-induced\nlinks in the fronto-temporal brain network.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:50:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pinto-Orellana", "Marco Antonio", ""], ["Mirtaheri", "Peyman", ""], ["Hammer", "Hugo L.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2105.06824", "submitter": "KongFatt Wong-Lin", "authors": "James Fitzgerald and KongFatt Wong-Lin", "title": "Multi-Objective Optimisation of Cortical Spiking Neural Networks With\n  Genetic Algorithms", "comments": "In: 32nd Irish Signals and Systems Conference (ISSC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking neural networks (SNNs) communicate through the all-or-none spiking\nactivity of neurons. However, fitting the large number of SNN model parameters\nto observed neural activity patterns, for example, in biological experiments,\nremains a challenge. Previous work using genetic algorithm (GA) optimisation on\na specific efficient SNN model, using the Izhikevich neuronal model, was\nlimited to a single parameter and objective. This work applied a version of GA,\ncalled non-dominated sorting GA (NSGA-III), to demonstrate the feasibility of\nperforming multi-objective optimisation on the same SNN, focusing on searching\nnetwork connectivity parameters to achieve target firing rates of excitatory\nand inhibitory neuronal types, including across different network connectivity\nsparsity. We showed that NSGA-III could readily optimise for various firing\nrates. Notably, when the excitatory neural firing rates were higher than or\nequal to that of inhibitory neurons, the errors were small. Moreover, when\nconnectivity sparsity was considered as a parameter to be optimised, the\noptimal solutions required sparse network connectivity. We also found that for\nexcitatory neural firing rates lower than that of inhibitory neurons, the\nerrors were generally larger. Overall, we have successfully demonstrated the\nfeasibility of implementing multi-objective GA optimisation on network\nparameters of recurrent and sparse SNN.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:35:39 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Fitzgerald", "James", ""], ["Wong-Lin", "KongFatt", ""]]}, {"id": "2105.07069", "submitter": "Joshua Faskowitz", "authors": "Joshua Faskowitz, Richard F. Betzel, Olaf Sporns", "title": "Edges in Brain Networks: Contributions to Models of Structure and\n  Function", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network models describe the brain as sets of nodes and edges that represent\nits distributed organization. So far, most discoveries in network neuroscience\nhave prioritized insights that highlight distinct groupings and specialized\nfunctional contributions of network nodes. Importantly, these functional\ncontributions are determined and expressed by the web of their\ninterrelationships, formed by network edges. Here, we underscore the important\ncontributions made by brain network edges for understanding distributed brain\norganization. Different types of edges represent different types of\nrelationships, including connectivity and similarity among nodes. Adopting a\nspecific definition of edges can fundamentally alter how we analyze and\ninterpret a brain network. Furthermore, edges can associate into collectives\nand higher-order arrangements, describe time series, and form edge communities\nthat provide insights into brain network topology complementary to the\ntraditional node-centric perspective. Focusing on the edges, and the\nhigher-order or dynamic information they can provide, discloses previously\nunderappreciated aspects of structural and functional network organization.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 21:12:53 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Faskowitz", "Joshua", ""], ["Betzel", "Richard F.", ""], ["Sporns", "Olaf", ""]]}, {"id": "2105.07140", "submitter": "Zijin Gu", "authors": "Zijin Gu, Keith W. Jamison, Meenakshi Khosla, Emily J. Allen, Yihan\n  Wu, Thomas Naselaris, Kendrick Kay, Mert R. Sabuncu, Amy Kuceyeski", "title": "NeuroGen: activation optimized image synthesis for discovery\n  neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional MRI (fMRI) is a powerful technique that has allowed us to\ncharacterize visual cortex responses to stimuli, yet such experiments are by\nnature constructed based on a priori hypotheses, limited to the set of images\npresented to the individual while they are in the scanner, are subject to noise\nin the observed brain responses, and may vary widely across individuals. In\nthis work, we propose a novel computational strategy, which we call NeuroGen,\nto overcome these limitations and develop a powerful tool for human vision\nneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model\nof human vision with a deep generative network to synthesize images predicted\nto achieve a target pattern of macro-scale brain activation. We demonstrate\nthat the reduction of noise that the encoding model provides, coupled with the\ngenerative network's ability to produce images of high fidelity, results in a\nrobust discovery architecture for visual neuroscience. By using only a small\nnumber of synthetic images created by NeuroGen, we demonstrate that we can\ndetect and amplify differences in regional and individual human brain response\npatterns to visual stimuli. We then verify that these discoveries are reflected\nin the several thousand observed image responses measured with fMRI. We further\ndemonstrate that NeuroGen can create synthetic images predicted to achieve\nregional response patterns not achievable by the best-matching natural images.\nThe NeuroGen framework extends the utility of brain encoding models and opens\nup a new avenue for exploring, and possibly precisely controlling, the human\nvisual system.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:36:39 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gu", "Zijin", ""], ["Jamison", "Keith W.", ""], ["Khosla", "Meenakshi", ""], ["Allen", "Emily J.", ""], ["Wu", "Yihan", ""], ["Naselaris", "Thomas", ""], ["Kay", "Kendrick", ""], ["Sabuncu", "Mert R.", ""], ["Kuceyeski", "Amy", ""]]}, {"id": "2105.07196", "submitter": "Jitkomut Songsiri", "authors": "Parinthorn Manomaisaowapak and Jitkomut Songsiri", "title": "Joint learning of multiple Granger causal networks via non-convex\n  regularizations: Inference of group-level brain connectivity", "comments": "23 pages, 9 figures, 3 tables, Comments: Title changed, Math\n  expression corrected in the formulation and algorithm section, More\n  references and discussions are added to explain difficulties in the\n  convergence analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper considers joint learning of multiple sparse Granger graphical\nmodels to discover underlying common and differential Granger causality (GC)\nstructures across multiple time series. This can be applied to drawing\ngroup-level brain connectivity inferences from a homogeneous group of subjects\nor discovering network differences among groups of signals collected under\nheterogeneous conditions. By recognizing that the GC of a single multivariate\ntime series can be characterized by common zeros of vector autoregressive (VAR)\nlag coefficients, a group sparse prior is included in joint regularized\nleast-squares estimations of multiple VAR models. Group-norm regularizations\nbased on group- and fused-lasso penalties encourage a decomposition of multiple\nnetworks into a common GC structure, with other remaining parts defined in\nindividual-specific networks. Prior information about sparseness and sparsity\npatterns of desired GC networks are incorporated as relative weights, while a\nnon-convex group norm in the penalty is proposed to enhance the accuracy of\nnetwork estimation in low-sample settings. Extensive numerical results on\nsimulations illustrated our method's improvements over existing sparse\nestimation approaches on GC network sparsity recovery. Our methods were also\napplied to available resting-state fMRI time series from the ADHD-200 data sets\nto learn the differences of causality mechanisms, called effective brain\nconnectivity, between adolescents with ADHD and typically developing children.\nOur analysis revealed that parts of the causality differences between the two\ngroups often resided in the orbitofrontal region and areas associated with the\nlimbic system, which agreed with clinical findings and data-driven results in\nprevious studies.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 10:29:02 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 03:19:08 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Manomaisaowapak", "Parinthorn", ""], ["Songsiri", "Jitkomut", ""]]}, {"id": "2105.07284", "submitter": "Joseph Monaco", "authors": "Joseph D. Monaco, Kanaka Rajan, Grace M. Hwang", "title": "A brain basis of dynamical intelligence for AI and computational\n  neuroscience", "comments": "Perspective article: 24 pages, 3 figures, 1 display box", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The deep neural nets of modern artificial intelligence (AI) have not achieved\ndefining features of biological intelligence, including abstraction, causal\nlearning, and energy-efficiency. While scaling to larger models has delivered\nperformance improvements for current applications, more brain-like capacities\nmay demand new theories, models, and methods for designing artificial learning\nsystems. Here, we argue that this opportunity to reassess insights from the\nbrain should stimulate cooperation between AI research and theory-driven\ncomputational neuroscience (CN). To motivate a brain basis of neural\ncomputation, we present a dynamical view of intelligence from which we\nelaborate concepts of sparsity in network structure, temporal dynamics, and\ninteractive learning. In particular, we suggest that temporal dynamics, as\nexpressed through neural synchrony, nested oscillations, and flexible\nsequences, provide a rich computational layer for reading and updating\nhierarchical models distributed in long-term memory networks. Moreover,\nembracing agent-centered paradigms in AI and CN will accelerate our\nunderstanding of the complex dynamics and behaviors that build useful world\nmodels. A convergence of AI/CN theories and objectives will reveal dynamical\nprinciples of intelligence for brains and engineered learning systems. This\narticle was inspired by our symposium on dynamical neuroscience and machine\nlearning at the 6th Annual US/NIH BRAIN Initiative Investigators Meeting.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 19:49:32 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:29:51 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Monaco", "Joseph D.", ""], ["Rajan", "Kanaka", ""], ["Hwang", "Grace M.", ""]]}, {"id": "2105.07308", "submitter": "Alexander Ororbia", "authors": "Alexander Ororbia, M. A. Kelly", "title": "Towards a Predictive Processing Implementation of the Common Model of\n  Cognition", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a cognitive architecture that is built from\npowerful yet simple neural models. Specifically, we describe an implementation\nof the common model of cognition grounded in neural generative coding and\nholographic associative memory. The proposed system creates the groundwork for\ndeveloping agents that learn continually from diverse tasks as well as model\nhuman performance at larger scales than what is possible with existant\ncognitive architectures.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 22:55:23 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 21:14:26 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ororbia", "Alexander", ""], ["Kelly", "M. A.", ""]]}, {"id": "2105.07416", "submitter": "Sebastian Goldt", "authors": "Sebastian Goldt, Florent Krzakala, Lenka Zdeborov\\'a, Nicolas Brunel", "title": "Bayesian reconstruction of memories stored in neural networks from their\n  connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of comprehensive synaptic wiring diagrams of large neural circuits\nhas created the field of connectomics and given rise to a number of open\nresearch questions. One such question is whether it is possible to reconstruct\nthe information stored in a recurrent network of neurons, given its synaptic\nconnectivity matrix. Here, we address this question by determining when solving\nsuch an inference problem is theoretically possible in specific attractor\nnetwork models and by providing a practical algorithm to do so. The algorithm\nbuilds on ideas from statistical physics to perform approximate Bayesian\ninference and is amenable to exact analysis. We study its performance on three\ndifferent models and explore the limitations of reconstructing stored patterns\nfrom synaptic connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 12:05:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Goldt", "Sebastian", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""], ["Brunel", "Nicolas", ""]]}, {"id": "2105.07541", "submitter": "Matthew Mizuhara", "authors": "Georgi S. Medvedev and Matthew S. Mizuhara", "title": "Chimeras unfolded", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instability of mixing in the Kuramoto model of coupled phase oscillators\nis the key to understanding a range of spatiotemporal patterns, which feature\nprominently in collective dynamics of systems ranging from neuronal networks,\nto coupled lasers, to power grids. In this paper, we describe a codimension-2\nbifurcation of mixing whose unfolding, in addition to the classical scenario of\nthe onset of synchronization, also explains the formation of clusters and\nchimeras. We use a combination of linear stability analysis and Penrose\ndiagrams to identify and analyze a variety of spatiotemporal patterns including\nstationary and traveling coherent clusters and twisted states, as well as their\ncombinations with regions of incoherent behavior called chimera states. The\nlinear stability analysis is used to estimate of the velocity distribution\nwithin these structures. Penrose diagrams, on the other hand, predict\naccurately the basins of their existence. Furthermore, we show that network\ntopology can endow chimera states with nontrivial spatial organization. In\nparticular, we present twisted chimera states, whose coherent regions are\norganized as stationary or traveling twisted states. The analytical results are\nillustrated with numerical bifurcation diagrams computed for the Kuramoto model\nwith uni-, bi-, and tri-modal frequency distributions and all-to-all and\nnonlocal nearest-neighbor connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 23:01:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Medvedev", "Georgi S.", ""], ["Mizuhara", "Matthew S.", ""]]}, {"id": "2105.08111", "submitter": "Thomas Schumacher", "authors": "Thomas Schumacher", "title": "Livewired Neural Networks: Making Neurons That Fire Together Wire\n  Together", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Until recently, artificial neural networks were typically designed with a\nfixed network structure. Here, I argue that network structure is highly\nrelevant to function, and therefore neural networks should be livewired\n(Eagleman 2020): dynamically rewired to reflect relationships between higher\norder representations of the external environment identified by coincident\nactivations in individual neurons. I discuss how this approach may enable such\nnetworks to build compositional world models that operate on symbols and that\nachieve few-shot learning, capabilities thought by many to be critical to\nhuman-level cognition. Here, I also 1) discuss how such livewired neural\nnetworks maximize the information the environment provides to a model, 2)\nexplore evidence indicating that livewiring is implemented in the brain, guided\nby glial cells, 3) discuss how livewiring may give rise to the associative\nemergent behaviors of brains, and 4) suggest paths for future research using\nlivewired networks to understand and create human-like reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:45:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Schumacher", "Thomas", ""]]}, {"id": "2105.08288", "submitter": "Shuhan Zheng", "authors": "Shuhan Zheng, Zhichao Liang, Youzhi Qu, Qingyuan Wu, Haiyan Wu,\n  Quanying Liu", "title": "Kuramoto model based analysis reveals oxytocin effects on brain network\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The oxytocin effects on large-scale brain networks such as Default Mode\nNetwork (DMN) and Frontoparietal Network (FPN) have been largely studied using\nfMRI data. However, those studies are mainly based on the statistical\ncorrelation or bayesian causality inference, lacking physical and neuroscience\nlevel interpretability. Here, we propose a Kuramoto model physical-based\nframework to investigate oxytocin effects on the phase dynamical neural\ncoupling in DMN and FPN. Tested on fMRI data from 59 participants administrated\nwith either oxytocin or placebo, we demonstrate that oxytocin changes the\ntopology of brain communities in DMN and FPN, leading to higher synchronization\nin the DMN and lower synchronization in the FPN, as well as a higher variance\nof the coupling strength within the DMN and more flexible coupling patterns\nacross time. These results together imply that oxytocin may increase the\ncapability to overcome the dispersion of corresponding intrinsic oscillations\nand yield flexibility in neural synchrony under various social contexts,\nproviding new evidence to account for oxytocin modulated social behaviors. Our\nproposed Kuramoto model-based framework can be a potential tool in network\nneuroscience and offers physical and neural insights into phase dynamics in the\nbrain.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 05:32:07 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 08:26:21 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zheng", "Shuhan", ""], ["Liang", "Zhichao", ""], ["Qu", "Youzhi", ""], ["Wu", "Qingyuan", ""], ["Wu", "Haiyan", ""], ["Liu", "Quanying", ""]]}, {"id": "2105.08810", "submitter": "Nicolas Perez-Nieves", "authors": "Nicolas Perez-Nieves and Dan F.M. Goodman", "title": "Sparse Spiking Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in emulating Spiking Neural Networks (SNNs)\non neuromorphic computing devices due to their low energy consumption. Recent\nadvances have allowed training SNNs to a point where they start to compete with\ntraditional Artificial Neural Networks (ANNs) in terms of accuracy, while at\nthe same time being energy efficient when run on neuromorphic hardware.\nHowever, the process of training SNNs is still based on dense tensor operations\noriginally developed for ANNs which do not leverage the spatiotemporally sparse\nnature of SNNs. We present here the first sparse SNN backpropagation algorithm\nwhich achieves the same or better accuracy as current state of the art methods\nwhile being significantly faster and more memory efficient. We show the\neffectiveness of our method on real datasets of varying complexity\n(Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a\nspeedup in the backward pass of up to 70x, and 40% more memory efficient,\nwithout losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:00:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Perez-Nieves", "Nicolas", ""], ["Goodman", "Dan F. M.", ""]]}, {"id": "2105.08944", "submitter": "Jacob Russin", "authors": "Jacob Russin, Maryam Zolfaghar, Seongmin A. Park, Erie Boorman,\n  Randall C. O'Reilly", "title": "Complementary Structure-Learning Neural Networks for Relational\n  Reasoning", "comments": "7 pages, 4 figures, Accepted to CogSci 2021 for poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The neural mechanisms supporting flexible relational inferences, especially\nin novel situations, are a major focus of current research. In the\ncomplementary learning systems framework, pattern separation in the hippocampus\nallows rapid learning in novel environments, while slower learning in neocortex\naccumulates small weight changes to extract systematic structure from\nwell-learned environments. In this work, we adapt this framework to a task from\na recent fMRI experiment where novel transitive inferences must be made\naccording to implicit relational structure. We show that computational models\ncapturing the basic cognitive properties of these two systems can explain\nrelational transitive inferences in both familiar and novel environments, and\nreproduce key phenomena observed in the fMRI experiment.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:25:21 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Russin", "Jacob", ""], ["Zolfaghar", "Maryam", ""], ["Park", "Seongmin A.", ""], ["Boorman", "Erie", ""], ["O'Reilly", "Randall C.", ""]]}, {"id": "2105.09679", "submitter": "Koujin Takeda", "authors": "Shun Kimura, Keisuke Ota, Koujin Takeda", "title": "Improved Neuronal Ensemble Inference with Generative Model and MCMC", "comments": "23 pages, 8 figures, partially overlapped with arXiv:1911.06509", "journal-ref": "J. Stat. Mech. (2021) 063501", "doi": "10.1088/1742-5468/abffd5", "report-no": null, "categories": "cond-mat.dis-nn cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal ensemble inference is a significant problem in the study of\nbiological neural networks. Various methods have been proposed for ensemble\ninference from experimental data of neuronal activity. Among them, Bayesian\ninference approach with generative model was proposed recently. However, this\nmethod requires large computational cost for appropriate inference. In this\nwork, we give an improved Bayesian inference algorithm by modifying update rule\nin Markov chain Monte Carlo method and introducing the idea of simulated\nannealing for hyperparameter control. We compare the performance of ensemble\ninference between our algorithm and the original one, and discuss the advantage\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 11:37:38 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kimura", "Shun", ""], ["Ota", "Keisuke", ""], ["Takeda", "Koujin", ""]]}, {"id": "2105.10109", "submitter": "Carlos Stein Naves De Brito", "authors": "Carlos S. N. Brito, Wulfram Gerstner", "title": "Correlation-invariant synaptic plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cortical neurons develop receptive fields adapted to the statistics of the\nenvironment. Synaptic plasticity models reproduce some of these response\nproperties, but so far require unrealistic assumptions about the statistics of\nthe incoming sensory signals, such as decorrelated inputs with identical firing\nrates. Here we develop a theory for synaptic plasticity that is invariant to\nsecond-order correlations in the input. Going beyond classical Hebbian\nlearning, we show that metaplastic long-term depression cancels the sensitivity\nto second-order correlation, bringing out selectivity to higher-order\nstatistics. In contrast, alternative stabilization mechanisms, such as\nheterosynaptic depression, increase the sensitivity to input correlations. Our\nsimulations demonstrate how correlation-invariant plasticity models can learn\nlatent patterns despite perturbations in input statistics without the need for\nwhitening. The theory advances our understanding of local unsupervised learning\nin cortical circuits and assigns a precise functional role to synaptic\ndepression mechanisms in pyramidal neurons.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 03:24:38 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Brito", "Carlos S. N.", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "2105.10404", "submitter": "Michael Rosenblum", "authors": "Michael Rosenblum and Arkady Pikovsky and Andrea A. K\\\"uhn and\n  Johannes L. Busch", "title": "Real-time estimation of phase and amplitude with application to neural\n  data", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computation of the instantaneous phase and amplitude via the Hilbert\nTransform is a powerful tool of data analysis. This approach finds many\napplications in various science and engineering branches but is not proper for\ncausal estimation because it requires knowledge of the signal's past and\nfuture. However, several problems require real-time estimation of phase and\namplitude; an illustrative example is phase-locked or amplitude-dependent\nstimulation in neuroscience. In this paper, we discuss and compare three causal\nalgorithms that do not rely on the Hilbert Transform but exploit well-known\nphysical phenomena, the synchronization and the resonance. After testing the\nalgorithms on a synthetic data set, we illustrate their performance computing\nphase and amplitude for the accelerometer tremor measurements and a\nParkinsonian patient's beta-band brain activity.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:21:33 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rosenblum", "Michael", ""], ["Pikovsky", "Arkady", ""], ["K\u00fchn", "Andrea A.", ""], ["Busch", "Johannes L.", ""]]}, {"id": "2105.10415", "submitter": "Sishu Shankar Muni", "authors": "Sishu Shankar Muni, Karthikeyan Rajagopal, Anitha Karthikeyan,\n  Sundaram Arun", "title": "Discrete hybrid Izhikevich neuron model: nodal and network behaviours\n  considering electromagnetic flux coupling", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the dynamics of the improved discretised version of the well known\nIzhikevich neuronmodel under the action of external electromagnetic field. It\nis found that the three-dimensional IZHmap shows rich dynamics. With the\nvariation of the electromagnetic field, period-doubling routeto chaos in a\nrepeating fashion is observed from the bifurcation diagram. Even the forward\nandbackward continuation bifurcation diagram which do not completely overlap\nsuggests that there is multistability in the system. The phenomenon of\nbistability (coexistence of periodic and chaotic attractors) is observed. The\npresence of periodic and chaotic attractor is aided by the maximal Lyapunov\nexponent diagram. The Lyapunov phase diagram of electromagnetic field and\nsynapses current shows a large parameter region of chaotic and periodic\nbehaviors with the presence of unbounded regions as well. The IZH map shows a\nplethora of spiking and bursting patterns such as mixed-mode patterns, tonic\nspiking, phasic spiking, steady spikes, regular spikes, spike bursting,\nperiodic bursting, phasic bursting, chaotic firing, etc with the variation of\nelectromagnetic coupling strength and the synapses current. We also investigate\nthe presence of chimera states in a ring-star, ring, star network of IZH map\nneurons. Chimera states are found in the case of ring-star and ring network\nwhile synchronised clusters were found in the case of star network and are\naided by the spatiotemporal plots, space-time plot, recurrence plots. The rich\ndynamics shown by the discretised IZH map makes it a promising research model\nto study about neurodynamics.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:48:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Muni", "Sishu Shankar", ""], ["Rajagopal", "Karthikeyan", ""], ["Karthikeyan", "Anitha", ""], ["Arun", "Sundaram", ""]]}, {"id": "2105.10461", "submitter": "Jeffrey Krichmar", "authors": "Jeffrey L. Krichmar", "title": "Edelman's Steps Toward a Conscious Artifact", "comments": "7 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2006, during a meeting of a working group of scientists in La Jolla,\nCalifornia at The Neurosciences Institute (NSI), Gerald Edelman described a\nroadmap towards the creation of a Conscious Artifact. As far as I know, this\nroadmap was not published. However, it did shape my thinking and that of many\nothers in the years since that meeting. This short paper, which is based on my\nnotes taken during the meeting, describes the key steps in this roadmap. I\nbelieve it is as groundbreaking today as it was more than 15 years ago.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 00:13:06 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 03:18:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Krichmar", "Jeffrey L.", ""]]}, {"id": "2105.10597", "submitter": "Eric Lucon", "authors": "C\\'eline Duval, Eric Lu\\c{c}on, Christophe Pouzat", "title": "Interacting Hawkes processes with multiplicative inhibition", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present work, we introduce a general class of mean-field interacting\nnonlinear Hawkes processes modelling the reciprocal interactions between two\nneuronal populations, one excitatory and one inhibitory. The model incorporates\ntwo features: inhibition, which acts as a multiplicative factor onto the\nintensity of the excitatory population and additive retroaction from the\nexcitatory neurons onto the inhibitory ones. We give first a detailed analysis\nof the well-posedness of this interacting system as well as its dynamics in\nlarge population. The second aim of the paper is to give a rigorous analysis of\nthe longtime behavior of the mean-field limit process. We provide also\nnumerical evidence that inhibition and retroaction may be responsible for the\nemergence of limit cycles in such system.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 22:57:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Duval", "C\u00e9line", ""], ["Lu\u00e7on", "Eric", ""], ["Pouzat", "Christophe", ""]]}, {"id": "2105.10676", "submitter": "Swapna Sasi", "authors": "Swapna Sasi, Basabdatta Sen Bhattacharya", "title": "Phase Entrainment by Periodic Stimuli In Silico: A Quantitative Study", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a quantitative study of phase entrainment by periodic visual\nstimuli in a biologically inspired neural network. The objective is to\nunderstand the neuronal population dynamics that underlie phase entrainment.\nPhase and frequency entrainment of brain oscillations by external stimuli are\nused as therapeutic treatment in neurological disorders, for example in\nParkinsonian tremor. And yet, the neuronal dynamics underpinning such\nentrainment is not fully understood. Rhythmic sensory stimulation is one way of\nstudying phase synchronisation in the brain. A recent experimental study has\nshown phase entrainment of brain oscillations during steady state visually\nevoked potentials (SSVEP), which are scalp electroencephalogram corresponding\nto periodic rhythmic stimuli. We have simulated SSVEP-like signals\ncorresponding to periodic pulse input to our in silico model. We have used\nphase locking values, normalised Shannon entropy and conditional probability as\nsynchronisation indices showing relative phase synchrony between the neuronal\npopulations as well with the input. The phase synchronisation disappears with\njitter in the input inter-pulse intervals. This would not have been the case if\nthe output signal were to be the superposition of the responses to the\ndifferent input signals. Thus, it may be inferred that the phase\nsynchronisation implies entrainment of the network response by the periodic\ninput. Overall, our study shows the plausibility of using biologically inspired\nin silico models, validated with experimental works, to understand and make\ntestable predictions on brain entrainment as a therapeutic treatment in\nspecific neurological disorders.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:11:21 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sasi", "Swapna", ""], ["Bhattacharya", "Basabdatta Sen", ""]]}, {"id": "2105.11203", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera and Beren Millidge and Alexander Tschantz and\n  Christopher L. Buckley", "title": "How particular is the physics of the Free Energy Principle?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Free Energy Principle (FEP) states that any dynamical system can be\ninterpreted as performing Bayesian inference upon its surrounding environment.\nAlthough the FEP applies in theory to a wide variety of systems, there has been\nalmost no direct exploration of the principle in concrete systems. In this\npaper, we examine in depth the assumptions required to derive the FEP in the\nsimplest possible set of systems - weakly-coupled non-equilibrium linear\nstochastic systems. Specifically, we explore (i) how general are the\nrequirements imposed on the statistical structure of a system and (ii) how\ninformative the FEP is about the behaviour of such systems. We find that this\nstructure, known as a Markov blanket (i.e. a boundary precluding direct\ncoupling between internal and external states) and stringent restrictions on\nits solenoidal flows, both required by the FEP, make it challenging to find\nsystems that fulfil the required assumptions. Suitable systems require an\nabsence of asymmetries in sensorimotor interactions that are highly unusual for\nliving systems. Moreover, we find that a core step in the argument relating the\nbehaviour of a system to variational inference relies on an implicit\nequivalence between the dynamics of the average states with the average of the\ndynamics. This equivalence does not hold in general even for linear systems as\nit requires an effective decoupling from the system's history of interactions.\nThese observations are critical for evaluating the generality and applicability\nof the FEP and point to potential issues in its current form. These issues make\nthe FEP, as it stands, not straightforwardly applicable to the simple linear\nsystems studied here and suggests more development is needed before it could be\napplied to the kind of complex systems which describe living and cognitive\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:17:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Aguilera", "Miguel", ""], ["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Buckley", "Christopher L.", ""]]}, {"id": "2105.12011", "submitter": "Nir Lahav PhD", "authors": "Nir Lahav", "title": "Emergence and Synchronization in Chaotic Oscillators and in the Human\n  Cortical Network", "comments": "PhD dissertation (Bar Ilan university, Physics department, Israel,\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO nlin.CD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When we look at the world around us, we see complex physical systems and\nemergent phenomena. Emergence occurs when a system is observed to have\nproperties that its parts do not have on their own. These properties or\nbehaviors emerge only when the parts interact in a wider whole. Examples of\nemergence can vary from the synchronization of pendulum clocks hanging on the\nsame wall to the phenomenon of life as an emergent property of chemistry. One\nof the most complex systems that exist in nature is the human brain. It\ncontains on average 100 to 200 billion neurons and about 100 trillion synapses\nconnecting them. From this vast neuronal dynamics, the ability to learn and\nstore memory emerges as well as the ability to have complex cognitive skills,\nconscious experience and a sense of self. In this work, we investigated how\ncomplex systems like the human brain and chaotic systems create emergent\nproperties. In order to do so, we used network theory (paper 1), chaos and\nsynchronization theory (paper 2 and 3).\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 00:10:16 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lahav", "Nir", ""]]}, {"id": "2105.12916", "submitter": "Hubert Banville", "authors": "Hubert Banville, Sean U.N. Wood, Chris Aimone, Denis-Alexander\n  Engemann and Alexandre Gramfort", "title": "Robust learning from corrupted EEG with dynamic spatial filtering", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machine learning models using EEG recorded outside of the laboratory\nsetting requires methods robust to noisy data and randomly missing channels.\nThis need is particularly great when working with sparse EEG montages (1-6\nchannels), often encountered in consumer-grade or mobile EEG devices. Neither\nclassical machine learning models nor deep neural networks trained end-to-end\non EEG are typically designed or tested for robustness to corruption, and\nespecially to randomly missing channels. While some studies have proposed\nstrategies for using data with missing channels, these approaches are not\npractical when sparse montages are used and computing power is limited (e.g.,\nwearables, cell phones). To tackle this problem, we propose dynamic spatial\nfiltering (DSF), a multi-head attention module that can be plugged in before\nthe first layer of a neural network to handle missing EEG channels by learning\nto focus on good channels and to ignore bad ones. We tested DSF on public EEG\ndata encompassing ~4,000 recordings with simulated channel corruption and on a\nprivate dataset of ~100 at-home recordings of mobile EEG with natural\ncorruption. Our proposed approach achieves the same performance as baseline\nmodels when no noise is applied, but outperforms baselines by as much as 29.4%\naccuracy when significant channel corruption is present. Moreover, DSF outputs\nare interpretable, making it possible to monitor channel importance in\nreal-time. This approach has the potential to enable the analysis of EEG in\nchallenging settings where channel corruption hampers the reading of brain\nsignals.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 02:33:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Banville", "Hubert", ""], ["Wood", "Sean U. N.", ""], ["Aimone", "Chris", ""], ["Engemann", "Denis-Alexander", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2105.12976", "submitter": "Ohad Felsenstein", "authors": "Ohad Felsenstein and Moshe Abeles", "title": "Spatio-Temporal Investigation of Brain-Wide Sequences", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In \"The Organization of Behavior\" (Hebb, 1949), Hebb suggested that the\npropagation of activity between transiently grouped neurons plays an important\nrole in behavior. Since then, multiple studies have provided evidence\nsupporting Hebb's claim; however, most findings have been found locally in\nconfined brain regions during unimodal tasks. Here we report on brain-wide\nbehavioral-specific sequences in humans performing a multimodal task. To\ninvestigate the structure of these sequences, we used MEG to record brain\nactivity in multiple brain regions simultaneously in participants performing a\nsensory-motor synchronization task. We detected local transient events\ncorresponding to synchronously activating populations of pyramidal neurons and\nsearched for their global organization as spatio-temporal patterns of\nactivation sequences between distant neural populations. We focused our\nanalysis on two types of spatio-temporal patterns: the most frequently\nrepeating patterns and the most discriminative patterns, to concentrate on\npatterns with high relevancy to behavior. The findings revealed that global\ntemporally precise sequences can be found and that these sequences have\npartially stereotypical characteristics, both temporally and spatially, with\nconsistent properties across subjects. By implementing a simplistic\nsingle-trial decoding approach, we found that brain-wide sequences have a\ntemporal precision of 17-31 milliseconds, which resembles the temporal\nprecision found locally in neural assemblies.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:40:30 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Felsenstein", "Ohad", ""], ["Abeles", "Moshe", ""]]}, {"id": "2105.13495", "submitter": "Byung-Hoon Kim M.D. Ph.D.", "authors": "Byung-Hoon Kim, Jong Chul Ye, Jae-Jin Kim", "title": "Learning Dynamic Graph Representation of Brain Connectome with\n  Spatio-Temporal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity (FC) between regions of the brain can be assessed by\nthe degree of temporal correlation measured with functional neuroimaging\nmodalities. Based on the fact that these connectivities build a network,\ngraph-based approaches for analyzing the brain connectome have provided\ninsights into the functions of the human brain. The development of graph neural\nnetworks (GNNs) capable of learning representation from graph structured data\nhas led to increased interest in learning the graph representation of the brain\nconnectome. Although recent attempts to apply GNN to the FC network have shown\npromising results, there is still a common limitation that they usually do not\nincorporate the dynamic characteristics of the FC network which fluctuates over\ntime. In addition, a few studies that have attempted to use dynamic FC as an\ninput for the GNN reported a reduction in performance compared to static FC\nmethods, and did not provide temporal explainability. Here, we propose STAGIN,\na method for learning dynamic graph representation of the brain connectome with\nspatio-temporal attention. Specifically, a temporal sequence of brain graphs is\ninput to the STAGIN to obtain the dynamic graph representation, while novel\nREADOUT functions and the Transformer encoder provide spatial and temporal\nexplainability with attention, respectively. Experiments on the HCP-Rest and\nthe HCP-Task datasets demonstrate exceptional performance of our proposed\nmethod. Analysis of the spatio-temporal attention also provide concurrent\ninterpretation with the neuroscientific knowledge, which further validates our\nmethod. Code is available at https://github.com/egyptdj/stagin\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:06:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Kim", "Byung-Hoon", ""], ["Ye", "Jong Chul", ""], ["Kim", "Jae-Jin", ""]]}, {"id": "2105.13705", "submitter": "Arthur Prat-Carrabin", "authors": "Arthur Prat-Carrabin and Michael Woodford", "title": "Bias and variance of the Bayesian-mean decoder", "comments": "12 pages, 2 figures. Supplementary Materials: 6 pages, 1 figure.\n  Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Perception, in theoretical neuroscience, has been modeled as the encoding of\nexternal stimuli into internal signals, which are then decoded. The Bayesian\nmean is an important decoder, as it is optimal for purposes of both estimation\nand discrimination. We present widely-applicable approximations to the bias and\nto the variance of the Bayesian mean, obtained under the minimal and\nbiologically-relevant assumption that the encoding results from a series of\nindependent, though not necessarily identically-distributed, signals.\nSimulations substantiate the accuracy of our approximations in the small-noise\nregime. The bias of the Bayesian mean comprises two components: one driven by\nthe prior, and one driven by the precision of the encoding. If the encoding is\n'efficient', the two components have opposite effects; their relative strengths\nare determined by the objective that the encoding optimizes. The experimental\nliterature on perception reports both 'Bayesian' biases directed towards prior\nexpectations, and opposite, 'anti-Bayesian' biases. We show that different\ntasks are indeed predicted to yield such contradictory biases, under a\nconsistently-optimal encoding-decoding model. Moreover, we recover Wei and\nStocker's \"law of human perception\", a relation between the bias of the\nBayesian mean and the derivative of its variance, and show how the coefficient\nof proportionality in this law depends on the task at hand. Our results provide\na parsimonious theory of optimal perception under constraints, in which\nencoding and decoding are adapted both to the prior and to the task faced by\nthe observer.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 09:59:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Prat-Carrabin", "Arthur", ""], ["Woodford", "Michael", ""]]}, {"id": "2105.14108", "submitter": "Christian David Marton", "authors": "Christian David Marton, Guillaume Lajoie, Kanaka Rajan", "title": "Efficient and robust multi-task learning in the brain with modular task\n  primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a real-world setting biological agents do not have infinite resources to\nlearn new things. It is thus useful to recycle previously acquired knowledge in\na way that allows for faster, less resource-intensive acquisition of multiple\nnew skills. Neural networks in the brain are likely not entirely re-trained\nwith new tasks, but how they leverage existing computations to learn new tasks\nis not well understood. In this work, we study this question in artificial\nneural networks trained on commonly used neuroscience paradigms. Building on\nrecent work from the multi-task learning literature, we propose two\ningredients: (1) network modularity, and (2) learning task primitives.\nTogether, these ingredients form inductive biases we call structural and\nfunctional, respectively. Using a corpus of nine different tasks, we show that\na modular network endowed with task primitives allows for learning multiple\ntasks well while keeping parameter counts, and updates, low. We also show that\nthe skills acquired with our approach are more robust to a broad range of\nperturbations compared to those acquired with other multi-task learning\nstrategies. This work offers a new perspective on achieving efficient\nmulti-task learning in the brain, and makes predictions for novel neuroscience\nexperiments in which targeted perturbations are employed to explore solution\nspaces.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:07:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Marton", "Christian David", ""], ["Lajoie", "Guillaume", ""], ["Rajan", "Kanaka", ""]]}, {"id": "2105.14409", "submitter": "Niharika S. D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Deana Crocetti, Nicholas\n  Wymbs, Joshua Robinson, Stewart Mostofsky, Archana Venkataraman", "title": "A Matrix Autoencoder Framework to Align the Functional and Structural\n  Connectivity Manifolds as Guided by Behavioral Phenotypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel matrix autoencoder to map functional connectomes from\nresting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor\nImaging (DTI), as guided by subject-level phenotypic measures. Our specialized\nautoencoder infers a low dimensional manifold embedding for the rs-fMRI\ncorrelation matrices that mimics a canonical outer-product decomposition. The\nembedding is simultaneously used to reconstruct DTI tractography matrices via a\nsecond manifold alignment decoder and to predict inter-subject phenotypic\nvariability via an artificial neural network. We validate our framework on a\ndataset of 275 healthy individuals from the Human Connectome Project database\nand on a second clinical dataset consisting of 57 subjects with Autism Spectrum\nDisorder. We demonstrate that the model reliably recovers structural\nconnectivity patterns across individuals, while robustly extracting predictive\nand interpretable brain biomarkers in a cross-validated setting. Finally, our\nframework outperforms several baselines at predicting behavioral phenotypes in\nboth real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:06:12 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 21:59:34 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Crocetti", "Deana", ""], ["Wymbs", "Nicholas", ""], ["Robinson", "Joshua", ""], ["Mostofsky", "Stewart", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2105.14493", "submitter": "Tamer Olmez", "authors": "Nuri Korkan, Tamer Olmez, Zumray Dokur", "title": "Generating Ten BCI Commands Using Four Simple Motor Imageries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The brain computer interface (BCI) systems are utilized for transferring\ninformation among humans and computers by analyzing electroencephalogram (EEG)\nrecordings.The process of mentally previewing a motor movement without\ngenerating the corporal output can be described as motor imagery (MI).In this\nemerging research field, the number of commands is also limited in relation to\nthe number of MI tasks; in the current literature, mostly two or four commands\n(classes) are studied. As a solution to this problem, it is recommended to use\nmental tasks as well as MI tasks. Unfortunately, the use of this approach\nreduces the classification performance of MI EEG signals. The fMRI analyses\nshow that the resources in the brain associated with the motor imagery can be\nactivated independently. It is assumed that the brain activity induced by the\nMI of the combination of body parts corresponds to the superposition of the\nactivities generated during each body parts's simple MI. In this study, in\norder to create more than four BCI commands, we suggest to generate combined MI\nEEG signals artificially by using left hand, right hand, tongue, and feet motor\nimageries in pairs. A maximum of ten different BCI commands can be generated by\nusing four motor imageries in pairs.This study aims to achieve high\nclassification performances for BCI commands produced from four motor imageries\nby implementing a small-sized deep neural network (DNN).The presented method is\nevaluated on the four-class datasets of BCI Competitions III and IV, and an\naverage classification performance of 81.8% is achieved for ten classes. The\nabove assumption is also validated on a different dataset which consists of\nsimple and combined MI EEG signals acquired in real time. Trained with the\nartificially generated combined MI EEG signals, DivFE resulted in an average of\n76.5% success rate for the combined MI EEG signals acquired in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:34:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Korkan", "Nuri", ""], ["Olmez", "Tamer", ""], ["Dokur", "Zumray", ""]]}, {"id": "2105.14587", "submitter": "Tatiana Kozitsina", "authors": "Mikhail Kunavin, Tatiana Kozitsina (Babkina), Mikhail Myagkov, Irina\n  Kozhevnikova, Mikhail Pankov, Ludmila Sokolova", "title": "Bioelectrical brain activity can predict prosocial behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-bio.NC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, people behave in social dilemmas such as proself and prosocial.\nHowever, inside social groups, people have a tendency to choose prosocial\nalternatives due to in-group favoritism. The bioelectrical activity of the\nhuman brain shows the differences between proself and prosocial exist even out\nof a socialized group. Moreover, a group socialization strengthens these\ndifferences. We used EEG System, \"Neuron-Spectrum-4/EPM\" (16 channels), to\ntrack the brain bioelectrical activity during decision making in laboratory\nexperiments with the Prisoner's dilemma game and the short-term socialization\nstage. We compared the spatial distribution of the spectral density during the\ndifferent experimental parts. The noncooperative decision was characterized by\nthe increased values of spectral the beta rhythm in the orbital regions of\nprefrontal cortex. The cooperative choice, on the contrary, was accompanied by\nthe theta-rhythm activation in the central cortex regions in both hemispheres\nand the high-frequency alpha rhythm in the medial regions of the prefrontal\ncortex. People who increased the cooperation level after the socialization\nstage was initially different from the ones who decreased the cooperation in\nterms of the bioelectrical activity. Well-socialized participants differed by\nincreased values of spectral density of theta-diapason and decreased values of\nspectral density of beta-diapason in the middle part of frontal lobe. People\nwho decreased the cooperation level after the socialization stage was\ncharacterized by decreased values of spectral density of alpha rhythm in the\nmiddle and posterior convex regions of both hemispheres.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:35:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kunavin", "Mikhail", "", "Babkina"], ["Kozitsina", "Tatiana", "", "Babkina"], ["Myagkov", "Mikhail", ""], ["Kozhevnikova", "Irina", ""], ["Pankov", "Mikhail", ""], ["Sokolova", "Ludmila", ""]]}, {"id": "2105.15034", "submitter": "Fei Tang", "authors": "Fei Tang, Michael Kopp", "title": "A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]", "comments": "1 page, 8 formulae", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In their recent paper titled \"Large Associative Memory Problem in\nNeurobiology and Machine Learning\" [arXiv:2008.06996] the authors gave a\nbiologically plausible microscopic theory from which one can recover many dense\nassociative memory models discussed in the literature. We show that the layers\nof the recent \"MLP-mixer\" [arXiv:2105.01601] as well as the essentially\nequivalent model in [arXiv:2105.02723] are amongst them.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:13:00 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:14:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tang", "Fei", ""], ["Kopp", "Michael", ""]]}]