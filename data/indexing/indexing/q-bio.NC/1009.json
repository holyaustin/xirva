[{"id": "1009.0077", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Not only a lack of right definitions: Arguments for a shift in\n  information-processing paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Consciousness and Machine Intelligence are not simply new buzzwords\nthat occupy our imagination. Over the last decades, we witness an unprecedented\nrise in attempts to create machines with human-like features and capabilities.\nHowever, despite widespread sympathy and abundant funding, progress in these\nenterprises is far from being satisfactory. The reasons for this are twofold:\nFirst, the notions of cognition and intelligence (usually borrowed from human\nbehavior studies) are notoriously blurred and ill-defined, and second, the\nbasic concepts underpinning the whole discourse are by themselves either\nundefined or defined very vaguely. That leads to improper and inadequate\nresearch goals determination, which I will illustrate with some examples drawn\nfrom recent documents issued by DARPA and the European Commission. On the other\nhand, I would like to propose some remedies that, I hope, would improve the\ncurrent state-of-the-art disgrace.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 02:37:54 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1009.0537", "submitter": "Patrick N. McGraw", "authors": "Patrick McGraw and Michael Menzinger", "title": "Self-Sustaining Oscillations in Complex Networks of Excitable Elements", "comments": "5 pages, 4 figures included, submitted to Phys. Rev. Lett", "journal-ref": null, "doi": "10.1103/PhysRevE.83.037102", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random networks of symmetrically coupled, excitable elements can\nself-organize into coherently oscillating states if the networks contain loops\n(indeed loops are abundant in random networks) and if the initial conditions\nare sufficiently random. In the oscillating state, signals propagate in a\nsingle direction and one or a few network loops are selected as driving loops\nin which the excitation circulates periodically. We analyze the mechanism,\ndescribe the oscillating states, identify the pacemaker loops and explain key\nfeatures of their distribution. This mechanism may play a role in epileptic\nseizures.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 20:29:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["McGraw", "Patrick", ""], ["Menzinger", "Michael", ""]]}, {"id": "1009.0796", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui and Rolando J. Biscay-Lirio", "title": "Dynamic interactions in terms of senders, hubs, and receivers (SHR)\n  using the singular value decomposition of time series: Theory and brain\n  connectivity applications", "comments": "Technical report 2010-September-04, The KEY Institute for Brain-Mind\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.bio-ph q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Understanding of normal and pathological brain function requires the\nidentification and localization of functional connections between specialized\nregions. The availability of high time resolution signals of electric neuronal\nactivity at several regions offers information for quantifying the connections\nin terms of information flow. When the signals cover the whole cortex, the\nnumber of connections is very large, making visualization and interpretation\nvery difficult. We introduce here the singular value decomposition of\ntime-lagged multiple signals, which localizes the senders, hubs, and receivers\n(SHR) of information transmission. Unlike methods that operate on large\nconnectivity matrices, such as correlation thresholding and graph-theoretic\nanalyses, this method operates on the multiple time series directly, providing\n3D brain images that assign a score to each location in terms of its sending,\nrelaying, and receiving capacity. The scope of the method is general and\nencompasses other applications outside the field of brain connectivity.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 01:26:31 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 00:36:11 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay-Lirio", "Rolando J.", ""]]}, {"id": "1009.0867", "submitter": "Adi Taflia", "authors": "Adi Taflia and David Holcman", "title": "Estimating the synaptic current in a multi-conductance AMPA receptor\n  model", "comments": null, "journal-ref": null, "doi": "10.1016/j.bpj.2011.05.032", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pre-synaptic neuron releases diffusing neurotransmitters such as glutamate\nthat activate post-synaptic receptors. The amplitude of the post-synaptic\ncurrent, mostly mediated by glutamatergic (AMPARs) receptors, is a fundamental\nsignal that may generate an action potential. However, although various\nsimulation results \\cite{kullman,Barbour,Raghavachari} have addressed how\nsynapses control the post-synaptic current, it is still unclear how this\ncurrent depends analytically on factors such as the synaptic cleft geometry,\nthe distribution, the number and the multi-conductance state of receptors, the\ngeometry of post-synaptic density (PSD) and the neurotransmitter release\nlocation. To estimate the synaptic current maximal amplitude, we present a\nsemi-analytical model of glutamate diffusing in the synaptic cleft. We modeled\nreceptors as multi-conductance channels and we find that PSD morphological\nchanges can significantly modulate the synaptic current, which is maximally\nreliable (the coefficient of variation is minimal) for an optimal size of the\nPSD, that depends on the vesicular release active zone. The existence of an\noptimal PSD size is related to nonlinear phenomena such as the multi-binding\ncooperativity of the neurotransmitter to the receptors. We conclude that\nchanges in the PSD geometry can sustain a form of synaptic plasticity,\nindependent of a change in the number of receptors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 20:14:57 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Taflia", "Adi", ""], ["Holcman", "David", ""]]}, {"id": "1009.1238", "submitter": "Albrecht Haase", "authors": "Albrecht Haase, Elisa Rigosi, Federica Trona, Gianfranco Anfora,\n  Giorgio Vallortigara, Renzo Antolini, and Claudio Vinegoni", "title": "In-vivo two-photon imaging of the honey bee antennal lobe", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the honey bee's importance as a simple neural model, there is a great\nneed for new functional imaging modalities. Herein we report on the use of\ntwo-photon microscopy for in-vivo functional and morphological imaging of the\nhoney bee's olfactory system focusing on its primary centers, the antennal\nlobes (ALs). Our imaging platform allows for simultaneously obtaining both\nmorphological measurements of the AL and in-vivo calcium recording of neural\nactivities. By applying external odor stimuli to the bee's antennas, we were\nable to record the characteristic odor response maps. Compared to previous\nworks where conventional fluorescence microscopy is used, our approach offers\nall the typical advantages of multi-photon imaging, providing substantial\nenhancement in both spatial and temporal resolutions while minimizing\nphoto-damages and autofluorescence contribution with a four-fold improvement in\nthe functional signal. Moreover, the multi-photon associated extended\npenetration depth allows for functional imaging within profound glomeruli.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 09:26:26 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Haase", "Albrecht", ""], ["Rigosi", "Elisa", ""], ["Trona", "Federica", ""], ["Anfora", "Gianfranco", ""], ["Vallortigara", "Giorgio", ""], ["Antolini", "Renzo", ""], ["Vinegoni", "Claudio", ""]]}, {"id": "1009.1286", "submitter": "Antonio de Candia", "authors": "S. Scarpetta, A. de Candia, F. Giacco", "title": "Storage of phase-coded patterns via STDP in fully-connected and sparse\n  network: a study of the network capacity", "comments": null, "journal-ref": "Front. Syn. Neurosci. 2:32 (2010)", "doi": "10.3389/fnsyn.2010.00032", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the storage and retrieval of phase-coded patterns as stable\ndynamical attractors in recurrent neural networks, for both an analog and a\nintegrate-and-fire spiking model. The synaptic strength is determined by a\nlearning rule based on spike-time-dependent plasticity, with an asymmetric time\nwindow depending on the relative timing between pre- and post-synaptic\nactivity. We store multiple patterns and study the network capacity.\n  For the analog model, we find that the network capacity scales linearly with\nthe network size, and that both capacity and the oscillation frequency of the\nretrieval state depend on the asymmetry of the learning time window. In\naddition to fully-connected networks, we study sparse networks, where each\nneuron is connected only to a small number z << N of other neurons. Connections\ncan be short range, between neighboring neurons placed on a regular lattice, or\nlong range, between randomly chosen pairs of neurons. We find that a small\nfraction of long range connections is able to amplify the capacity of the\nnetwork. This imply that a small-world-network topology is optimal, as a\ncompromise between the cost of long range connections and the capacity\nincrease.\n  Also in the spiking integrate and fire model the crucial result of storing\nand retrieval of multiple phase-coded patterns is observed. The capacity of the\nfully-connected spiking network is investigated, together with the relation\nbetween oscillation frequency of retrieval state and window asymmetry.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 13:23:39 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Scarpetta", "S.", ""], ["de Candia", "A.", ""], ["Giacco", "F.", ""]]}, {"id": "1009.1828", "submitter": "Simon R. Schultz", "authors": "Michael T. Schaub and Simon R. Schultz", "title": "The Ising decoder: reading out the activity of large neural ensembles", "comments": "23 pages 10 figures, in press: Journal of Computational Neuroscience\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising Model has recently received much attention for the statistical\ndescription of neural spike train data. In this paper, we propose and\ndemonstrate its use for building decoders capable of predicting, on a\nmillisecond timescale, the stimulus represented by a pattern of neural\nactivity. After fitting to a training dataset, the Ising decoder can be applied\n\"online\" for instantaneous decoding of test data. While such models can be fit\nexactly using Boltzmann learning, this approach rapidly becomes computationally\nintractable as neural ensemble size increases. We show that several approaches,\nincluding the Thouless-Anderson-Palmer (TAP) mean field approach from\nstatistical physics, and the recently developed Minimum Probability Flow\nLearning (MPFL) algorithm, can be used for rapid inference of model parameters\nin large-scale neural ensembles. Use of the Ising model for decoding, unlike\nother problems such as functional connectivity estimation, requires estimation\nof the partition function. As this involves summation over all possible\nresponses, this step can be limiting. Mean field approaches avoid this problem\nby providing an analytical expression for the partition function. We\ndemonstrate these decoding techniques by applying them to simulated neural\nensemble responses from a mouse visual cortex model, finding an improvement in\ndecoder performance for a model with heterogeneous as opposed to homogeneous\nneural tuning and response properties. Our results demonstrate the practicality\nof using the Ising model to read out, or decode, spatial patterns of activity\ncomprised of many hundreds of neurons.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 17:09:06 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2011 15:28:55 GMT"}, {"version": "v3", "created": "Mon, 23 May 2011 13:30:08 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Schaub", "Michael T.", ""], ["Schultz", "Simon R.", ""]]}, {"id": "1009.2150", "submitter": "Xuhong Liao", "authors": "Liao Xuhong, Xia Qinzhi, Qian Yu, Zhang Lisheng, Hu Gang, Mi Yuanyuan", "title": "Pattern formation in oscillatory complex networks consisting of\n  excitable nodes", "comments": "15 pages, 7 figures, to appear in Phys. Rev. E", "journal-ref": "Phys. Rev. E 83, 056204 (2011)", "doi": "10.1103/PhysRevE.83.056204", "report-no": null, "categories": "nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oscillatory dynamics of complex networks has recently attracted great\nattention. In this paper we study pattern formation in oscillatory complex\nnetworks consisting of excitable nodes. We find that there exist a few center\nnodes and small skeletons for most oscillations. Complicated and seemingly\nrandom oscillatory patterns can be viewed as well-organized target waves\npropagating from center nodes along the shortest paths, and the shortest loops\npassing through both the center nodes and their driver nodes play the role of\noscillation sources. Analyzing simple skeletons we are able to understand and\npredict various essential properties of the oscillations and effectively\nmodulate the oscillations. These methods and results will give insights into\npattern formation in complex networks, and provide suggestive ideas for\nstudying and controlling oscillations in neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Sep 2010 09:20:59 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2011 08:45:47 GMT"}], "update_date": "2011-11-23", "authors_parsed": [["Xuhong", "Liao", ""], ["Qinzhi", "Xia", ""], ["Yu", "Qian", ""], ["Lisheng", "Zhang", ""], ["Gang", "Hu", ""], ["Yuanyuan", "Mi", ""]]}, {"id": "1009.2243", "submitter": "Maurizio De Pitta'", "authors": "Mati Goldberg and Maurizio De Pitt\\`a and Vladislav Volman and Hugues\n  Berry and Eshel Ben-Jacob", "title": "Nonlinear gap junctions enable long-distance propagation of pulsating\n  calcium waves in astrocyte networks", "comments": "Article: 30 pages, 7 figures. Supplementary Material: 11 pages, 6\n  figures", "journal-ref": "PLoS Comput Biol 6(8): e1000909", "doi": "10.1371/journal.pcbi.1000909", "report-no": null, "categories": "q-bio.NC nlin.CD q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new paradigm has recently emerged in brain science whereby communications\nbetween glial cells and neuron-glia interactions should be considered together\nwith neurons and their networks to understand higher brain functions. In\nparticular, astrocytes, the main type of glial cells in the cortex, have been\nshown to communicate with neurons and with each other. They are thought to form\na gap-junction-coupled syncytium supporting cell-cell communication via\npropagating Ca2+ waves. An identified mode of propagation is based on\ncytoplasm-to-cytoplasm transport of inositol trisphosphate (IP3) through gap\njunctions that locally trigger Ca2+ pulses via IP3-dependent Ca2+-induced Ca2+\nrelease. It is, however, currently unknown whether this intracellular route is\nable to support the propagation of long-distance regenerative Ca2+ waves or is\nrestricted to short-distance signaling. Furthermore, the influence of the\nintracellular signaling dynamics on intercellular propagation remains to be\nunderstood. In this work, we propose a model of the gap-junctional route for\nintercellular Ca2+ wave propagation in astrocytes showing that: (1)\nlong-distance regenerative signaling requires nonlinear coupling in the gap\njunctions, and (2) even with nonlinear gap junctions, long-distance\nregenerative signaling is favored when the internal Ca2+ dynamics implements\nfrequency modulation-encoding oscillations with pulsating dynamics, while\namplitude modulation-encoding dynamics tends to restrict the propagation range.\nAs a result, spatially heterogeneous molecular properties and/or weak couplings\nare shown to give rise to rich spatiotemporal dynamics that support complex\npropagation behaviors. These results shed new light on the mechanisms\nimplicated in the propagation of Ca2+ waves across astrocytes and precise the\nconditions under which glial cells may participate in information processing in\nthe brain.\n", "versions": [{"version": "v1", "created": "Sun, 12 Sep 2010 15:44:03 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Goldberg", "Mati", ""], ["De Pitt\u00e0", "Maurizio", ""], ["Volman", "Vladislav", ""], ["Berry", "Hugues", ""], ["Ben-Jacob", "Eshel", ""]]}, {"id": "1009.2290", "submitter": "C.C. Alan Fung", "authors": "C. C. Alan Fung and K. Y. Michael Wong and He Wang and Si Wu", "title": "Attractor Dynamics with Synaptic Depression", "comments": "9 pages, 8 figures. This article has been accepted by NIPS with a\n  poster spotlight presentation at the conference", "journal-ref": "Advances in NIPS vol. 23 (2010)", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal connection weights exhibit short-term depression (STD). The present\nstudy investigates the impact of STD on the dynamics of a continuous attractor\nneural network (CANN) and its potential roles in neural information processing.\nWe find that the network with STD can generate both static and traveling bumps,\nand STD enhances the performance of the network in tracking external inputs. In\nparticular, we find that STD endows the network with slow-decaying plateau\nbehaviors, namely, the network being initially stimulated to an active state\nwill decay to silence very slowly in the time scale of STD rather than that of\nneural signaling. We argue that this provides a mechanism for neural systems to\nhold short-term memory easily and shut off persistent activities naturally.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 03:25:55 GMT"}, {"version": "v2", "created": "Mon, 22 Nov 2010 03:56:11 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Fung", "C. C. Alan", ""], ["Wong", "K. Y. Michael", ""], ["Wang", "He", ""], ["Wu", "Si", ""]]}, {"id": "1009.2855", "submitter": "Jakob Macke Jakob Macke", "authors": "Jakob H Macke, Manfred Opper, Matthias Bethge", "title": "An analytically tractable model of neural population activity in the\n  presence of common input explains higher-order correlations and entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously recorded neurons exhibit correlations whose underlying causes\nare not known. Here, we use a population of threshold neurons receiving\ncorrelated inputs to model neural population recordings. We show analytically\nthat small changes in second-order correlations can lead to large changes in\nhigher correlations, and that these higher-order correlations have a strong\nimpact on the entropy, sparsity and statistical heat capacity of the\npopulation. Remarkably, our findings for this simple model may explain a couple\nof surprising effects recently observed in neural population recordings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Sep 2010 07:34:13 GMT"}, {"version": "v2", "created": "Fri, 17 Sep 2010 13:13:21 GMT"}], "update_date": "2010-09-20", "authors_parsed": [["Macke", "Jakob H", ""], ["Opper", "Manfred", ""], ["Bethge", "Matthias", ""]]}, {"id": "1009.3656", "submitter": "Efstratios Manousakis", "authors": "Efstratios Manousakis", "title": "When perceptual time stands still: Long stable memory in binocular\n  rivalry", "comments": "7 two-column latex pages and 9 eps figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have carried out binocular rivalry experiments with a large number of\nsubjects to obtain high quality statistics on probability distribution of\ndominance duration (PDDD) for two cases where (a) the rival stimulus is\ncontinuously presented and (b) the rival stimulus is periodically removed, with\nstimulus-on and stimulus-off intervals Ton and Toff respectively. It is shown\nthat the PDDD obtained for the latter case can be reproduced to a reasonable\ndegree of approximation by simply using the PDDD of part (a) and slicing it at\npieces of time extent Ton and by introducing intervals of length Toff between\nthe on-intervals where the PDDD is set to zero. This suggests that the\nvariables representing the perceptual state do not change significantly during\nlong blank intervals. We argue that these findings impose challenges to\ntheoretical models which aim at describing visual perception.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 18:36:31 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Manousakis", "Efstratios", ""]]}, {"id": "1009.4172", "submitter": "Joshua Goldwyn", "authors": "Joshua H. Goldwyn, Nikita S. Imennov, Michael Famulare, and Eric\n  Shea-Brown", "title": "On stochastic differential equation models for ion channel noise in\n  Hodgkin-Huxley neurons", "comments": "19 pages, including 6 figures and 3 appendices", "journal-ref": null, "doi": "10.1103/PhysRevE.83.041908", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random transitions of ion channels between conducting and non-conducting\nstates generate a source of internal fluctuations in a neuron, known as channel\nnoise. The standard method for modeling fluctuations in the states of ion\nchannels uses continuous-time Markov chains nonlinearly coupled to a\ndifferential equation for voltage. Beginning with the work of Fox and Lu, there\nhave been attempts to generate simpler models that use stochastic differential\nequation (SDEs) to approximate the stochastic spiking activity produced by\nMarkov chain models. Recent numerical investigations, however, have raised\ndoubts that SDE models can preserve the stochastic dynamics of Markov chain\nmodels.\n  We analyze three SDE models that have been proposed as approximations to the\nMarkov chain model: one that describes the states of the ion channels and two\nthat describe the states of the ion channel subunits. We show that the former\nchannel-based approach can capture the distribution of channel noise and its\neffect on spiking in a Hodgkin-Huxley neuron model to a degree not previously\ndemonstrated, but the latter two subunit-based approaches cannot. Our analysis\nprovides intuitive and mathematical explanations for why this is the case: the\ntemporal correlation in the channel noise is determined by the combinatorics of\nbundling subunits into channels, and the subunit-based approaches do not\ncorrectly account for this structure. Our study therefore confirms and\nelucidates the findings of previous numerical investigations of subunit-based\nSDE models. Moreover, it presents the first evidence that Markov chain models\nof the nonlinear, stochastic dynamics of neural membranes can be accurately\napproximated by SDEs. This finding opens a door to future modeling work using\nSDE techniques to further illuminate the effects of ion channel fluctuations on\nelectrically active cells.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 18:56:58 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Goldwyn", "Joshua H.", ""], ["Imennov", "Nikita S.", ""], ["Famulare", "Michael", ""], ["Shea-Brown", "Eric", ""]]}, {"id": "1009.4516", "submitter": "Magdoom Mohamed Kulam Najmudeen", "authors": "K.N.Magdoom, D.Subramanian, V.S.Chakravarthy, B.Ravindran, Shun-ichi\n  Amari, N. Meenakshisundaram", "title": "Modeling Basal Ganglia for understanding Parkinsonian Reaching Movements", "comments": "Neural Computation, In Press", "journal-ref": "Neural Computation (2011), 23(2), 477-516", "doi": "10.1162/NECO_a_00073", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a computational model that highlights the role of basal ganglia\n(BG) in generating simple reaching movements. The model is cast within the\nreinforcement learning (RL) framework with the correspondence between RL\ncomponents and neuroanatomy as follows: dopamine signal of substantia nigra\npars compacta as the Temporal Difference error, striatum as the substrate for\nthe Critic, and the motor cortex as the Actor. A key feature of this\nneurobiological interpretation is our hypothesis that the indirect pathway is\nthe Explorer. Chaotic activity, originating from the indirect pathway part of\nthe model, drives the wandering, exploratory movements of the arm. Thus the\ndirect pathway subserves exploitation while the indirect pathway subserves\nexploration. The motor cortex becomes more and more independent of the\ncorrective influence of BG, as training progresses. Reaching trajectories show\ndiminishing variability with training. Reaching movements associated with\nParkinson's disease (PD) are simulated by (a) reducing dopamine and (b)\ndegrading the complexity of indirect pathway dynamics by switching it from\nchaotic to periodic behavior. Under the simulated PD conditions, the arm\nexhibits PD motor symptoms like tremor, bradykinesia and undershoot. The model\nechoes the notion that PD is a dynamical disease.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 04:20:43 GMT"}], "update_date": "2011-01-26", "authors_parsed": [["Magdoom", "K. N.", ""], ["Subramanian", "D.", ""], ["Chakravarthy", "V. S.", ""], ["Ravindran", "B.", ""], ["Amari", "Shun-ichi", ""], ["Meenakshisundaram", "N.", ""]]}, {"id": "1009.4958", "submitter": "Carina Curto", "authors": "Carina Curto, Anda Degeratu, Vladimir Itskov", "title": "Flexible Memory Networks", "comments": "Accepted to Bulletin of Mathematical Biology, 11 July 2011", "journal-ref": "Bulletin of Mathematical Biology, Vol 74(3):590-614, 2012", "doi": null, "report-no": null, "categories": "q-bio.NC math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks of neurons in some brain areas are flexible enough to encode new\nmemories quickly. Using a standard firing rate model of recurrent networks, we\ndevelop a theory of flexible memory networks. Our main results characterize\nnetworks having the maximal number of flexible memory patterns, given a\nconstraint graph on the network's connectivity matrix. Modulo a mild\ntopological condition, we find a close connection between maximally flexible\nnetworks and rank 1 matrices. The topological condition is H_1(X;Z)=0, where X\nis the clique complex associated to the network's constraint graph; this\ncondition is generically satisfied for large random networks that are not\noverly sparse. In order to prove our main results, we develop some\nmatrix-theoretic tools and present them in a self-contained section independent\nof the neuroscience context.\n", "versions": [{"version": "v1", "created": "Fri, 24 Sep 2010 23:28:30 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2011 22:34:53 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Curto", "Carina", ""], ["Degeratu", "Anda", ""], ["Itskov", "Vladimir", ""]]}, {"id": "1009.5028", "submitter": "Marius Buliga", "authors": "Marius Buliga", "title": "What is a space? Computations in emergent algebras and the front end\n  visual system", "comments": "comments welcomed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.LO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the help of link diagrams with decorated crossings, I explain\ncomputations in emergent algebras, introduced in arXiv:0907.1520, as the kind\nof computations done in the front end visual system.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 19:27:41 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Buliga", "Marius", ""]]}, {"id": "1009.5198", "submitter": "Yunyun Li", "authors": "Yunyun Li, Gerhard Schmid, Peter Hanggi and Lutz Schimansky-Geier", "title": "Spontaneous spiking in an autaptic Hodgkin-Huxley set up", "comments": "8 pages, 10 figures", "journal-ref": "Phys. Rev. E 82, 061907 (2010)", "doi": "10.1103/PhysRevE.82.061907", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effect of intrinsic channel noise is investigated for the dynamic\nresponse of a neuronal cell with a delayed feedback loop. The loop is based on\nthe so-called autapse phenomenon in which dendrites establish not only\nconnections to neighboring cells but as well to its own axon. The biophysical\nmodeling is achieved in terms of a stochastic Hodgkin-Huxley model containing\nsuch a built in delayed feedback. The fluctuations stem from intrinsic channel\nnoise, being caused by the stochastic nature of the gating dynamics of ion\nchannels. The influence of the delayed stimulus is systematically analyzed with\nrespect to the coupling parameter and the delay time in terms of the interspike\ninterval histograms and the average interspike interval. The delayed feedback\nmanifests itself in the occurrence of bursting and a rich multimodal interspike\ninterval distribution, exhibiting a delay-induced reduction of the spontaneous\nspiking activity at characteristic frequencies. Moreover, a specific\nfrequency-locking mechanism is detected for the mean interspike interval.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 09:33:46 GMT"}, {"version": "v2", "created": "Tue, 28 Sep 2010 07:28:42 GMT"}, {"version": "v3", "created": "Thu, 25 Nov 2010 12:16:17 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Li", "Yunyun", ""], ["Schmid", "Gerhard", ""], ["Hanggi", "Peter", ""], ["Schimansky-Geier", "Lutz", ""]]}, {"id": "1009.5355", "submitter": "Tsvi Tlusty", "authors": "J.-P. Eckmann, Elisha Moses, Olav Stetter, Tsvi Tlusty, Cyrille\n  Zbinden", "title": "Leaders of neuronal cultures in a quorum percolation model", "comments": "Keywords: Neuronal cultures, Graph theory, Activation dynamics,\n  Percolation, Statistical mechanics of networks, Leaders of activity, Quorum.\n  http://www.weizmann.ac.il/complex/tlusty/papers/FrontCompNeuro2010.pdf", "journal-ref": "Front. Comput. Neurosci. 4:132 (2010)", "doi": "10.3389/fncom.2010.00132", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical framework using quorum-percolation for describing\nthe initiation of activity in a neural culture. The cultures are modeled as\nrandom graphs, whose nodes are excitatory neurons with kin inputs and kout\noutputs, and whose input degrees kin = k obey given distribution functions pk.\nWe examine the firing activity of the population of neurons according to their\ninput degree (k) classes and calculate for each class its firing probability\n\\Phi_k(t) as a function of t. The probability of a node to fire is found to be\ndetermined by its in-degree k, and the first-to-fire neurons are those that\nhave a high k. A small minority of high-k classes may be called \"Leaders\", as\nthey form an inter-connected subnetwork that consistently fires much before the\nrest of the culture. Once initiated, the activity spreads from the Leaders to\nthe less connected majority of the culture. We then use the distribution of\nin-degree of the Leaders to study the growth rate of the number of neurons\nactive in a burst, which was experimentally measured to be initially\nexponential. We find that this kind of growth rate is best described by a\npopulation that has an in-degree distribution that is a Gaussian centered\naround k = 75 with width {\\sigma} = 31 for the majority of the neurons, but\nalso has a power law tail with exponent -2 for ten percent of the population.\nNeurons in the tail may have as many as k = 4, 700 inputs. We explore and\ndiscuss the correspondence between the degree distribution and a dynamic\nneuronal threshold, showing that from the functional point of view, structure\nand elementary dynamics are interchangeable. We discuss possible geometric\norigins of this distribution, and comment on the importance of size, or of\nhaving a large number of neurons, in the culture.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 19:01:34 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Eckmann", "J. -P.", ""], ["Moses", "Elisha", ""], ["Stetter", "Olav", ""], ["Tlusty", "Tsvi", ""], ["Zbinden", "Cyrille", ""]]}, {"id": "1009.5473", "submitter": "Paul Merolla", "authors": "Paul Merolla, Tristan Ursell, John Arthur", "title": "The thermodynamic temperature of a rhythmic spiking network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks built from two-state neurons are powerful\ncomputational substrates, whose computational ability is well understood by\nanalogy with statistical mechanics. In this work, we introduce similar\nanalogies in the context of spiking neurons in a fixed time window, where\nexcitatory and inhibitory inputs drawn from a Poisson distribution play the\nrole of temperature. For single neurons with a \"bandgap\" between their inputs\nand the spike threshold, this temperature allows for stochastic spiking. By\nimposing a global inhibitory rhythm over the fixed time windows, we connect\nneurons into a network that exhibits synchronous, clock-like updating akin to\nneural networks. We implement a single-layer Boltzmann machine without learning\nto demonstrate our model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 07:22:08 GMT"}], "update_date": "2010-09-29", "authors_parsed": [["Merolla", "Paul", ""], ["Ursell", "Tristan", ""], ["Arthur", "John", ""]]}, {"id": "1009.6025", "submitter": "Yuriy Pershin", "authors": "Yuriy V. Pershin and Massimiliano Di Ventra", "title": "Neuromorphic, Digital and Quantum Computation with Memory Circuit\n  Elements", "comments": null, "journal-ref": "Proceedings of the IEEE 100(6), 2071-2080 (2012)", "doi": "10.1109/JPROC.2011.2166369", "report-no": null, "categories": "cond-mat.mes-hall q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory effects are ubiquitous in nature and the class of memory circuit\nelements - which includes memristors, memcapacitors and meminductors - shows\ngreat potential to understand and simulate the associated fundamental physical\nprocesses. Here, we show that such elements can also be used in electronic\nschemes mimicking biologically-inspired computer architectures, performing\ndigital logic and arithmetic operations, and can expand the capabilities of\ncertain quantum computation schemes. In particular, we will discuss few\nexamples where the concept of memory elements is relevant to the realization of\nassociative memory in neuronal circuits, spike-timing-dependent plasticity of\nsynapses, digital and field-programmable quantum computing.\n", "versions": [{"version": "v1", "created": "Thu, 30 Sep 2010 01:22:40 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2011 08:52:03 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2011 23:53:31 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Pershin", "Yuriy V.", ""], ["Di Ventra", "Massimiliano", ""]]}]