[{"id": "1905.00231", "submitter": "Jennifer Sorinas", "authors": "Jennifer Sorinas, Jose Manuel Ferr\\'andez and Eduardo Fernandez", "title": "The Psychological and Physiological Part of Emotions: Multimodal\n  Approximation for Valence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to develop more precise and functional affective applications, it is\nnecessary to achieve a balance between the psychology and the engineering\napplied to emotions. Signals from the central and peripheral nervous systems\nhave been used for emotion recognition purposes, however, their operation and\nthe relationship between them remains unknown. In this context, in the present\nwork we have tried to approach the study of the psychobiology of both systems\nin order to generate a computational model for the recognition of emotions in\nthe dimension of valence. To this end, the electroencephalography (EEG) signal,\nelectrocardiography (ECG) signal and skin temperature of 24 subjects have been\nstudied. Each methodology has been evaluated individually, finding\ncharacteristic patterns of positive and negative emotions in each of them.\nAfter feature selection of each methodology, the results of the classification\nshowed that, although the classification of emotions is possible at both\ncentral and peripheral levels, the multimodal approach did not improve the\nresults obtained through the EEG alone. In addition, differences have been\nobserved between cerebral and physiological responses in the processing\nemotions by separating the sample by sex; though, the differences between men\nand women were only notable at the physiological level.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 09:29:53 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Sorinas", "Jennifer", ""], ["Ferr\u00e1ndez", "Jose Manuel", ""], ["Fernandez", "Eduardo", ""]]}, {"id": "1905.00319", "submitter": "Atalanti Mastakouri", "authors": "Atalanti A. Mastakouri, Bernhard Sch\\\"olkopf and Moritz Grosse-Wentrup", "title": "Beta Power May Mediate the Effect of Gamma-TACS on Motor Performance", "comments": "7 pages, 5 figures, in Proceedings of IEEE Engineering in Medicine\n  and Biology Conference, July 2019 (IEEE license notice)", "journal-ref": "2019 41st Annual International Conference of the IEEE Engineering\n  in Medicine and Biology Society (EMBC)", "doi": "10.1109/EMBC.2019.8856416", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcranial alternating current stimulation (tACS) is becoming an important\nmethod in the field of motor rehabilitation because of its ability to\nnon-invasively influence ongoing brain oscillations at arbitrary frequencies.\nHowever, substantial variations in its effect across individuals are reported,\nmaking tACS a currently unreliable treatment tool. One reason for this\nvariability is the lack of knowledge about the exact way tACS entrains and\ninteracts with ongoing brain oscillations. The present crossover stimulation\nstudy on 20 healthy subjects contributes to the understanding of\ncross-frequency effects of gamma (70 Hz) tACS over the contralateral motor\ncortex by providing empirical evidence which is consistent with a role of low-\n(12~-20 Hz) and high- (20-~30 Hz) beta power as a mediator of gamma-tACS on\nmotor performance.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 14:06:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mastakouri", "Atalanti A.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1905.00378", "submitter": "Will Xiao", "authors": "Will Xiao and Gabriel Kreiman", "title": "Gradient-free activation maximization for identifying effective stimuli", "comments": "16 pages, 8 figures, 3 tables", "journal-ref": "PLOS Comp Biol 2020 16(6): e1007973", "doi": "10.1371/journal.pcbi.1007973", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question for understanding brain function is what types of\nstimuli drive neurons to fire. In visual neuroscience, this question has also\nbeen posted as characterizing the receptive field of a neuron. The search for\neffective stimuli has traditionally been based on a combination of insights\nfrom previous studies, intuition, and luck. Recently, the same question has\nemerged in the study of units in convolutional neural networks (ConvNets), and\ntogether with this question a family of solutions were developed that are\ngenerally referred to as \"feature visualization by activation maximization.\"\n  We sought to bring in tools and techniques developed for studying ConvNets to\nthe study of biological neural networks. However, one key difference that\nimpedes direct translation of tools is that gradients can be obtained from\nConvNets using backpropagation, but such gradients are not available from the\nbrain. To circumvent this problem, we developed a method for gradient-free\nactivation maximization by combining a generative neural network with a genetic\nalgorithm. We termed this method XDream (EXtending DeepDream with real-time\nevolution for activation maximization), and we have shown that this method can\nreliably create strong stimuli for neurons in the macaque visual cortex (Ponce\net al., 2019). In this paper, we describe extensive experiments characterizing\nthe XDream method by using ConvNet units as in silico models of neurons. We\nshow that XDream is applicable across network layers, architectures, and\ntraining sets; examine design choices in the algorithm; and provide practical\nguides for choosing hyperparameters in the algorithm. XDream is an efficient\nalgorithm for uncovering neuronal tuning preferences in black-box networks\nusing a vast and diverse stimulus space.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 16:56:57 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Xiao", "Will", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1905.00414", "submitter": "Simon Kornblith", "authors": "Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey\n  Hinton", "title": "Similarity of Neural Network Representations Revisited", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has sought to understand the behavior of neural networks by\ncomparing representations between layers and between different trained models.\nWe examine methods for comparing neural network representations based on\ncanonical correlation analysis (CCA). We show that CCA belongs to a family of\nstatistics for measuring multivariate similarity, but that neither CCA nor any\nother statistic that is invariant to invertible linear transformation can\nmeasure meaningful similarities between representations of higher dimension\nthan the number of data points. We introduce a similarity index that measures\nthe relationship between representational similarity matrices and does not\nsuffer from this limitation. This similarity index is equivalent to centered\nkernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA\ncan reliably identify correspondences between representations in networks\ntrained from different initializations.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:57:26 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 13:26:27 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 16:19:38 GMT"}, {"version": "v4", "created": "Fri, 19 Jul 2019 14:59:45 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Kornblith", "Simon", ""], ["Norouzi", "Mohammad", ""], ["Lee", "Honglak", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1905.01173", "submitter": "Andrija Stajduhar", "authors": "Andrija \\v{S}tajduhar, Tomislav Lipi\\'c, Goran Sedmak, Sven\n  Lon\\v{c}ari\\'c, Milo\\v{s} Juda\\v{s}", "title": "Computational analysis of laminar structure of the human cortex based on\n  local neuron features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel method for analysis and segmentation of\nlaminar structure of the cortex based on tissue characteristics whose change\nacross the gray matter underlies distinctive between cortical layers. We\ndevelop and analyze features of individual neurons to investigate changes in\ncytoarchitectonic differentiation and present a novel high-performance,\nautomated framework for neuron-level histological image analysis. Local tissue\nand cell descriptors such as density, neuron size and other measures are used\nfor development of more complex neuron features used in machine learning model\ntrained on data manually labeled by three human experts. Final neuron layer\nclassifications were obtained by training a separate model for each expert and\ncombining their probability outputs. Importances of developed neuron features\non both global model level and individual prediction level are presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 13:15:54 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 15:19:04 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["\u0160tajduhar", "Andrija", ""], ["Lipi\u0107", "Tomislav", ""], ["Sedmak", "Goran", ""], ["Lon\u010dari\u0107", "Sven", ""], ["Juda\u0161", "Milo\u0161", ""]]}, {"id": "1905.01181", "submitter": "Rodrigo Felipe De Oliveira Pena", "authors": "Rodrigo F.O. Pena, Vinicius Lima, Renan O. Shimoura, Jo\\~ao P. Novato,\n  Antonio C. Roque", "title": "Optimal interplay between synaptic strengths and network structure\n  enhances activity fluctuations and information propagation in hierarchical\n  modular networks", "comments": "19 pages, 7 figures", "journal-ref": "Brain Sciences 2020, 10, 228", "doi": "10.3390/brainsci10040228", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network models of spiking neurons, the joint impact of network structure\nand synaptic parameters on activity propagation is still an open problem. Here\nwe use an information-theoretical approach to investigate activity propagation\nin spiking networks with hierarchical modular topology. We observe that\noptimized pairwise information propagation emerges due to the increase of\neither (i) the global synaptic strength parameter or (ii) the number of modules\nin the network, while the network size remains constant. At the population\nlevel, information propagation of activity among adjacent modules is enhanced\nas the number of modules increases until a maximum value is reached and then\ndecreases, showing that there is an optimal interplay between synaptic strength\nand modularity for population information flow. This is in contrast to\ninformation propagation evaluated among pairs of neurons, which attains maximum\nvalue at the maximum values of these two parameter ranges. By examining the\nnetwork behavior under increase of synaptic strength and number of modules we\nfind that these increases are associated with two different effects: (i)\nincrease of autocorrelations among individual neurons, and (ii) increase of\ncross-correlations among pairs of neurons. The second effect is associated with\nbetter information propagation in the network. Our results suggest roles that\nlink topological features and synaptic strength levels to the transmission of\ninformation in cortical networks.v\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 13:42:13 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 02:56:05 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 23:26:31 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 19:21:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Pena", "Rodrigo F. O.", ""], ["Lima", "Vinicius", ""], ["Shimoura", "Renan O.", ""], ["Novato", "Jo\u00e3o P.", ""], ["Roque", "Antonio C.", ""]]}, {"id": "1905.01342", "submitter": "Jonathan Touboul", "authors": "Jonathan D. Touboul, Charlotte Piette, Laurent Venance, G. Bard\n  Ermentrout", "title": "Noise-induced synchronization and anti-resonance in excitable systems;\n  Implications for information processing in Parkinson's Disease and Deep Brain\n  Stimulation", "comments": null, "journal-ref": "Phys. Rev. X 10, 011073 (2020)", "doi": "10.1103/PhysRevX.10.011073", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical physics of a surprising phenomenon arising in large\nnetworks of excitable elements in response to noise: while at low noise,\nsolutions remain in the vicinity of the resting state and large-noise solutions\nshow asynchronous activity, the network displays orderly, perfectly\nsynchronized periodic responses at intermediate level of noise. We show that\nthis phenomenon is fundamentally stochastic and collective in nature. Indeed,\nfor noise and coupling within specific ranges, an asymmetry in the transition\nrates between a resting and an excited regime progressively builds up, leading\nto an increase in the fraction of excited neurons eventually triggering a chain\nreaction associated with a macroscopic synchronized excursion and a collective\nreturn to rest where this process starts afresh, thus yielding the observed\nperiodic synchronized oscillations. We further uncover a novel anti-resonance\nphenomenon: noise-induced synchronized oscillations disappear when the system\nis driven by periodic stimulation with frequency within a specific range. In\nthat anti-resonance regime, the system is optimal for measures of information\ncapacity. This observation provides a new hypothesis accounting for the\nefficiency of Deep Brain Stimulation therapies in Parkinson's disease, a\nneurodegenerative disease characterized by an increased synchronization of\nbrain motor circuits. We further discuss the universality of these phenomena in\nthe class of stochastic networks of excitable elements with confining coupling,\nand illustrate this universality by analyzing various classical models of\nneuronal networks. Altogether, these results uncover some universal mechanisms\nsupporting a regularizing impact of noise in excitable systems, reveal a novel\nanti-resonance phenomenon in these systems, and propose a new hypothesis for\nthe efficiency of high-frequency stimulation in Parkinson's disease.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 19:02:30 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:34:13 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Touboul", "Jonathan D.", ""], ["Piette", "Charlotte", ""], ["Venance", "Laurent", ""], ["Ermentrout", "G. Bard", ""]]}, {"id": "1905.01917", "submitter": "Ernest Montbrio", "authors": "Bastian Pietras, Federico Devalle, Alex Roxin, Andreas Daffertshofer,\n  Ernest Montbri\\'o", "title": "An exact firing rate model reveals the differential effects of chemical\n  versus electrical synapses in spiking networks", "comments": null, "journal-ref": "Phys. Rev. E 100, 042412 (2019)", "doi": "10.1103/PhysRevE.100.042412", "report-no": null, "categories": "nlin.AO nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical and electrical synapses shape the dynamics of neuronal networks.\nNumerous theoretical studies have investigated how each of these types of\nsynapses contributes to the generation of neuronal oscillations, but their\ncombined effect is less understood. This limitation is further magnified by the\nimpossibility of traditional neuronal mean-field models ---also known as firing\nrate models, or firing rate equations--- to account for electrical synapses.\nHere we introduce a novel firing rate model that exactly describes the mean\nfield dynamics of heterogeneous populations of quadratic integrate-and-fire\n(QIF) neurons with both chemical and electrical synapses. The mathematical\nanalysis of the firing rate model reveals a well-established bifurcation\nscenario for networks with chemical synapses, characterized by a codimension-2\nCusp point and persistent states for strong recurrent excitatory coupling. The\ninclusion of electrical coupling generally implies neuronal synchrony by virtue\nof a supercritical Hopf bifurcation. This transforms the Cusp scenario into a\nbifurcation scenario characterized by three codimension-2 points (Cusp,\nTakens-Bogdanov, and Saddle-Node Separatrix Loop), which greatly reduces the\npossibility for persistent states. This is generic for heterogeneous QIF\nnetworks with both chemical and electrical coupling. Our results agree with\nseveral numerical studies on the dynamics of large networks of heterogeneous\nspiking neurons with electrical and chemical coupling.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 10:40:28 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 18:59:11 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Pietras", "Bastian", ""], ["Devalle", "Federico", ""], ["Roxin", "Alex", ""], ["Daffertshofer", "Andreas", ""], ["Montbri\u00f3", "Ernest", ""]]}, {"id": "1905.01933", "submitter": "Maria Masoliver", "authors": "Maria Masoliver and Cristina Masoller", "title": "Neuronal coupling benefits the encoding of weak periodic signals in\n  symbolic spike patterns", "comments": null, "journal-ref": null, "doi": "10.1016/j.cnsns.2019.105023", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good understanding of how neurons use electrical pulses (i.e, spikes) to\nencode the signal information remains elusive. Analyzing spike sequences\ngenerated by individual neurons and by two coupled neurons (using the\nstochastic FitzHugh-Nagumo model), recent theoretical studies have found that\nthe relative timing of the spikes can encode the signal information. Using a\nsymbolic method to analyze the spike sequence, preferred and infrequent spike\npatterns were detected, whose probabilities vary with both, the amplitude and\nthe frequency of the signal. To investigate if this encoding mechanism is\nplausible also for neuronal ensembles, here we analyze the activity of a group\nof neurons, when they all perceive a weak periodic signal. We find that, as in\nthe case of one or two coupled neurons, the probabilities of the spike\npatterns, now computed from the spike sequences of all the neurons, depend on\nthe signal's amplitude and period, and thus, the patterns' probabilities encode\nthe information of the signal. We also find that the resonances with the period\nof the signal or with the noise level are more pronounced when a group of\nneurons perceive the signal, in comparison with when only one or two coupled\nneurons perceive it. Neuronal coupling is beneficial for signal encoding as a\ngroup of neurons is able to encode a small-amplitude signal, which could not be\nencoded when it is perceived by just one or two coupled neurons. Interestingly,\nwe find that for a group of neurons, just a few connections with one another\ncan significantly improve the encoding of small-amplitude signals. Our findings\nindicate that information encoding in preferred and infrequent spike patterns\nis a plausible mechanism that can be employed by neuronal populations to encode\nweak periodic inputs, exploiting the presence of neural noise.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 11:25:24 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Masoliver", "Maria", ""], ["Masoller", "Cristina", ""]]}, {"id": "1905.02007", "submitter": "Ahmed El Hady", "authors": "Pepe Alcami, Ahmed El Hady", "title": "Axonal Computations", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axons functionally link the somato-dendritic compartment to synaptic\nterminals. Structurally and functionally diverse, they accomplish a central\nrole in determining the delays and reliability with which neuronal ensembles\ncommunicate. By combining their active and passive biophysical properties, they\nensure a plethora of physiological computations. In this review, we revisit the\nbiophysics of generation and propagation of electrical signals in the axon,\ntheir complex interplay, and their rich dynamics. We further place the\ncomputational abilities of axons in the context of intracellular and\nintercellular coupling. We discuss how, by means of sophisticated biophysical\nmechanisms, axons expand the repertoire of axonal computation, and thereby, of\nneural computation.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 12:57:12 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Alcami", "Pepe", ""], ["Hady", "Ahmed El", ""]]}, {"id": "1905.02024", "submitter": "Ilenna Jones", "authors": "Ilenna Simone Jones and Konrad Paul Kording", "title": "Quantifying the role of neurons for behavior is a mediation question", "comments": "4 pages, 2 figures", "journal-ref": "Behav Brain Sci 42 (2019) e233", "doi": "10.1017/S0140525X19001444", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many systems neuroscientists want to understand neurons in terms of\nmediation; we want to understand how neurons are involved in the causal chain\nfrom stimulus to behavior. Unfortunately, most tools are inappropriate for that\nwhile our language takes mediation for granted. Here we discuss the contrast\nbetween our conceptual drive towards mediation and the difficulty of obtaining\nmeaningful evidence.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:15:30 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jones", "Ilenna Simone", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "1905.02136", "submitter": "Emad Ul Haq Qazi", "authors": "Qazi Emad-Ul-Haq, Muhammad Hussain, Hatim Aboalsamh, Saeed Bamatraf,\n  Aamir Saeed Malik, Hafeez Ullah Amin", "title": "A Review on understanding Brain, and Memory Retention and Recall\n  Processes using EEG and fMRI techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human memory -- the learning of new information involves changes at the\nsynaptic level between neurons dedicated for storage of in-formation.\nGenerally, memory is classified as Long-Term Memory and Short-Term Memory. The\nvarious types of the memory and their disorder are widely studied using\nneuroimaging techniques like Electroencephalography (EEG) and functional\nMagnetic Resonance Imaging (fMRI). Brain is effectively occupied with the\ncapabilities of learning, retention and recall. The brain regions (pre-frontal\ncortex, associated hippocampus cortices and their interactions with other\nlobes) involved in memory recall tasks focuses on understanding the memory\nretention and recall processes. However, due to highly complicated and dynamic\nmechanisms of brain, the specific regions where information may reside are not\ncompletely explored. In this research paper, recent memory literature using EEG\nand fMRI studies is reviewed to understand the memory retention and recall\nprocesses as well as the various brain regions associated with these processes.\nA number of stimuli which are reported in previous studies are evaluated and\ndiscussed. Furthermore, the challenges which are being faced by researchers in\nEEG and fMRI methodologies are also presented. Recommendations for the future\nresearch related to memory retention and recall are also discussed at the end.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 13:46:59 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Emad-Ul-Haq", "Qazi", ""], ["Hussain", "Muhammad", ""], ["Aboalsamh", "Hatim", ""], ["Bamatraf", "Saeed", ""], ["Malik", "Aamir Saeed", ""], ["Amin", "Hafeez Ullah", ""]]}, {"id": "1905.02241", "submitter": "Pramod Kumbhar", "authors": "Pramod Kumbhar, Omar Awile, Liam Keegan, Jorge Blanco Alonso, James\n  King, Michael Hines, Felix Sch\\\"urmann", "title": "An optimizing multi-platform source-to-source compiler framework for the\n  NEURON MODeling Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages (DSLs) play an increasingly important role in the\ngeneration of high performing software. They allow the user to exploit specific\nknowledge encoded in the constructs for the generation of code adapted to a\nparticular hardware architecture; at the same time, they make it easier to\ngenerate optimized code for a multitude of platforms as the transformation has\nto be encoded only once. Here, we describe a new code generation framework\n(NMODL) for an existing DSL in the NEURON framework, a widely used software for\nmassively parallel simulation of biophysically detailed brain tissue models.\nExisting NMODL DSL transpilers lack either essential features to generate\noptimized code or capability to parse the diversity of existing models in the\nuser community. Our NMODL framework has been tested against a large number of\npreviously published user models and offers high-level domain-specific\noptimizations and symbolic algebraic simplifications before target code\ngeneration. Furthermore, rich analysis tools are provided allowing the\nscientist to introspect models. NMODL implements multiple SIMD and SPMD targets\noptimized for modern hardware. Benchmarks were performed on Intel Skylake,\nIntel KNL and AMD Naples platforms. When comparing NMODL-generated kernels with\nNEURON we observe a speedup of up to 20x, resulting into overall speedups of\ntwo different production simulations by $\\sim$10x. When compared to a\npreviously published SIMD optimized version that heavily relied on\nauto-vectorization by the compiler still a speedup of up to $\\sim$2x is\nobserved.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 19:20:54 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Kumbhar", "Pramod", ""], ["Awile", "Omar", ""], ["Keegan", "Liam", ""], ["Alonso", "Jorge Blanco", ""], ["King", "James", ""], ["Hines", "Michael", ""], ["Sch\u00fcrmann", "Felix", ""]]}, {"id": "1905.02403", "submitter": "Misha Tsodyks", "authors": "Michelangelo Naim, Mikhail Katkov, Sandro Romani, Misha Tsodyks", "title": "Fundamental Law of Memory Recall", "comments": "9 pages, 2 figures in the main text, 6 pages and 2 figure in\n  supplementary material", "journal-ref": "Phys. Rev. Lett. 124, 018101 (2020)", "doi": "10.1103/PhysRevLett.124.018101", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Free recall of random lists of words is a standard paradigm used to probe\nhuman memory. We proposed an associative search process that can be reduced to\na deterministic walk on random graphs defined by the structure of memory\nrepresentations. This model makes a parameter-free prediction for the average\nnumber of memory items recalled ($RC$) out of $M$ items in memory: $R =\n\\sqrt{3\\pi M/2}$. This prediction was verified in a large-scale crowd-sourced\nfree recall and recognition experiment. We uncovered a novel law of memory\nrecall, indicating that recall operates according to a stereotyped search\nprocess common to all people.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:33:29 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:46:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Naim", "Michelangelo", ""], ["Katkov", "Mikhail", ""], ["Romani", "Sandro", ""], ["Tsodyks", "Misha", ""]]}, {"id": "1905.02405", "submitter": "Elisa Castaldi", "authors": "Elisa Castaldi, Claudia Lunghi and Maria Concetta Morrone", "title": "Neuroplasticity in adult human visual cortex", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Between 1 to 5 out of 100 people worldwide has never experienced normotypic\nvision due to a condition called amblyopia, and about 1 out of 4000 suffer from\ninherited retinal dystrophies that progressively lead them to blindness. While\na wide range of technologies and therapies are being developed to restore\nvision, a fundamental question still remains unanswered: would the adult visual\nbrain retain a sufficient plastic potential to learn how to see after a\nprolonged period of abnormal visual experience? In this review we summarize\nstudies showing that the visual brain of sighted adults retains a type of\ndevelopmental plasticity, called homeostatic plasticity, and this property has\nbeen recently exploited successfully for adult amblyopia recover. Next, we\ndiscuss how the brain circuits reorganizes when visual stimulation is partially\nrestored by means of a bionic eye in late blinds with Retinitis Pigmentosa. The\nprimary visual cortex in these patients slowly became activated by the\nartificial visual stimulation, indicating that sight restoration therapies can\nrely on a considerable degree of spared plasticity in adulthood.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:37:31 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Castaldi", "Elisa", ""], ["Lunghi", "Claudia", ""], ["Morrone", "Maria Concetta", ""]]}, {"id": "1905.02422", "submitter": "Chihye Han", "authors": "Chihye Han, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim", "title": "Representation of White- and Black-Box Adversarial Examples in Deep\n  Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study", "comments": "Copyright 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of brain-inspired deep neural networks (DNNs) in solving\ncomplex, high-level visual tasks has led to rising expectations for their\npotential to match the human visual system. However, DNNs exhibit\nidiosyncrasies that suggest their visual representation and processing might be\nsubstantially different from human vision. One limitation of DNNs is that they\nare vulnerable to adversarial examples, input images on which subtle, carefully\ndesigned noises are added to fool a machine classifier. The robustness of the\nhuman visual system against adversarial examples is potentially of great\nimportance as it could uncover a key mechanistic feature that machine vision is\nyet to incorporate. In this study, we compare the visual representations of\nwhite- and black-box adversarial examples in DNNs and humans by leveraging\nfunctional magnetic resonance imaging (fMRI). We find a small but significant\ndifference in representation patterns for different (i.e. white- versus black-\nbox) types of adversarial examples for both humans and DNNs. However, human\nperformance on categorical judgment is not degraded by noise regardless of the\ntype unlike DNN. These results suggest that adversarial examples may be\ndifferentially represented in the human visual system, but unable to affect the\nperceptual experience.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:10:48 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Han", "Chihye", ""], ["Yoon", "Wonjun", ""], ["Kwon", "Gihyun", ""], ["Nam", "Seungkyu", ""], ["Kim", "Daeshik", ""]]}, {"id": "1905.02532", "submitter": "Yuanhong Tang", "authors": "Lingling An, Yuanhong Tang, Quan Wang, Qingqi Pei, Ran Wei, Huiyuan\n  Duan, Jian K. Liu", "title": "Coding Capacity of Purkinje Cells with Different Schemes of\n  Morphological Reduction", "comments": "In Press. Front. Comput. Neurosci. | doi: 10.3389/fncom.2019.00029", "journal-ref": null, "doi": "10.3389/fncom.2019.00029", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The brain as a neuronal system has very complex structure with large\ndiversity of neuronal types. The most basic complexity is seen from the\nstructure of neuronal morphology, which usually has a complex tree-like\nstructure with dendritic spines distributed in branches. For simulating a\nlarge-scale network with spiking neurons, the simple point neuron, such as\nintegrate-and-fire neuron, is often used. However, recent experimental evidence\nsuggests that the computational ability of a single neuron is largely enhanced\nby its morphological structure, in particular, by various types of dendritic\ndynamics. As morphology reduction of detailed biophysical models is one of\nclassic questions for systems neuroscience, much effort has been taken to\nsimulate a neuron with a few compartments to include the interaction between\nsoma and dendritic spines. Yet, novel reduction methods are still needed to\ndeal with complex dendritic tree. Here by using ten individual Purkinje cells\nof the cerebellum from three species of guinea-pig, mouse and rat, we consider\nfour types of reduction methods and study their effects on the coding capacity\nof Purkinje cells in terms of firing rate, timing coding, spiking pattern, and\nmodulated firing under different stimulation protocols. We find that there is a\nvariation of reduction performance depending on individual cells and species,\nhowever, all reduction methods can preserve, to some degree, firing activity of\nthe full model of Purkinje cell. Therefore, when stimulating large-scale\nnetwork of neurons, one has to choose a proper type of reduced neuronal model\ndepending on the questions addressed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:35:33 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["An", "Lingling", ""], ["Tang", "Yuanhong", ""], ["Wang", "Quan", ""], ["Pei", "Qingqi", ""], ["Wei", "Ran", ""], ["Duan", "Huiyuan", ""], ["Liu", "Jian K.", ""]]}, {"id": "1905.02563", "submitter": "Amine Bohi", "authors": "Xiaoyu Wang, Amine Bohi, Mariam Al Harrach, Mickael Dinomais, Julien\n  Lef\\`evre, Fran\\c{c}ois Rousseau", "title": "On early brain folding patterns using biomechanical growth modeling", "comments": "4 pages, 4 figures, 41st EMB Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Abnormal cortical folding patterns may be related to neurodevelopmental\ndisorders such as lissencephaly and polymicrogyria. In this context,\ncomputational modeling is a powerful tool to provide a better understanding of\nthe early brain folding process. Recent studies based on biomechanical modeling\nhave shown that mechanical forces play a crucial role in the formation of\ncortical convolutions. However, the correlation between simulation results and\nbiological facts, and the effect of physical parameters in these models remain\nunclear. In this paper, we propose a new brain longitudinal length growth model\nto improve brain model growth. In addition, we investigate the effect of the\ninitial cortical thickness on folding patterns, quantifying the folds by the\nsurface-based three-dimensional gyrification index and a spectral analysis of\ngyrification. The results tend to show that the use of such biomechanical\nmodels could highlight the links between neurodevelopmental diseases and\nphysical parameters.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 13:40:18 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Wang", "Xiaoyu", ""], ["Bohi", "Amine", ""], ["Harrach", "Mariam Al", ""], ["Dinomais", "Mickael", ""], ["Lef\u00e8vre", "Julien", ""], ["Rousseau", "Fran\u00e7ois", ""]]}, {"id": "1905.02613", "submitter": "Sarah McIntyre", "authors": "Sarah McIntyre, Athanasia Moungou, Rebecca Boehme, Peder M. Isager,\n  Frances Lau, Ali Israr, Ellen A. Lumpkin, Freddy Abnousi, H{\\aa}kan Olausson", "title": "Affective touch communication in close adult relationships", "comments": "Technical paper accepted for presentation at World Haptics 2019. Data\n  and materials available: https://doi.org/10.17605/OSF.IO/7XRWC", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-personal touch is a powerful aspect of social interaction that we\nexpect to be particularly important for emotional communication. We studied the\ncapacity of closely acquainted humans to signal the meaning of several word\ncues (e.g. gratitude, sadness) using touch sensation alone. Participants\ncommunicated all cues with above chance performance. We show that emotionally\nclose people can accurately signal the meaning of different words through\ntouch, and that performance is affected by the amount of contextual information\navailable. Even with minimal context and feedback, both attention-getting and\nlove were communicated surprisingly well. Neither the type of close\nrelationship, nor self-reported comfort with touch significantly affected\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:33:59 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["McIntyre", "Sarah", ""], ["Moungou", "Athanasia", ""], ["Boehme", "Rebecca", ""], ["Isager", "Peder M.", ""], ["Lau", "Frances", ""], ["Israr", "Ali", ""], ["Lumpkin", "Ellen A.", ""], ["Abnousi", "Freddy", ""], ["Olausson", "H\u00e5kan", ""]]}, {"id": "1905.02636", "submitter": "Sam Blakeman", "authors": "Sam Blakeman and Denis Mareschal", "title": "A Complementary Learning Systems Approach to Temporal Difference\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary Learning Systems (CLS) theory suggests that the brain uses a\n'neocortical' and a 'hippocampal' learning system to achieve complex behavior.\nThese two systems are complementary in that the 'neocortical' system relies on\nslow learning of distributed representations while the 'hippocampal' system\nrelies on fast learning of pattern-separated representations. Both of these\nsystems project to the striatum, which is a key neural structure in the brain's\nimplementation of Reinforcement Learning (RL). Current deep RL approaches share\nsimilarities with a 'neocortical' system because they slowly learn distributed\nrepresentations through backpropagation in Deep Neural Networks (DNNs). An\nongoing criticism of such approaches is that they are data inefficient and lack\nflexibility. CLS theory suggests that the addition of a 'hippocampal' system\ncould address these criticisms. In the present study we propose a novel\nalgorithm known as Complementary Temporal Difference Learning (CTDL), which\ncombines a DNN with a Self-Organising Map (SOM) to obtain the benefits of both\na 'neocortical' and a 'hippocampal' system. Key features of CTDL include the\nuse of Temporal Difference (TD) error to update a SOM and the combination of a\nSOM and DNN to calculate action values. We evaluate CTDL on grid worlds and the\nCart-Pole environment, and show several benefits over the classic Deep\nQ-Network (DQN) approach. These results demonstrate (1) the utility of\ncomplementary learning systems for the evaluation of actions, (2) that the TD\nerror signal is a useful form of communication between the two systems and (3)\nthe biological plausibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:17:20 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Blakeman", "Sam", ""], ["Mareschal", "Denis", ""]]}, {"id": "1905.03972", "submitter": "Zhongqi Tian", "authors": "Zhong-Qi Kyle Tian, Douglas Zhou, David Cai", "title": "Digital System Reconstruction by Pairwise Transfer Entropy", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy (TE) is an attractive model-free method to detect causality\nand infer structural connectivity of general digital systems. However it relies\non high dimensions used in its definition to clearly remove the memory effect\nand distinguish the direct causality from the indirect ones which makes it\nalmost inoperable in practice. In this work, we try to use a low order and\npairwise TE framework with binary data suitably filtered from the recorded\nsignals to avoid the high dimensional problem. Under this setting, we find and\nexplain that the TE values from the connected and unconnected pairs have a\nsignificant difference of magnitude, which can be easily classified by cluster\nmethods. This phenomenon widely and robustly holds over a wide range of systems\nand dynamical regimes. In addition, we find the TE value is quadratically\nrelated to the coupling strength and thus we can establish a quantitative\nmapping between the causal and structural connectivity.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 07:08:33 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Tian", "Zhong-Qi Kyle", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1905.04026", "submitter": "Armelle Rancillac", "authors": "H\\'el\\`ene Geoffroy, Jean Rossier (NDC), Armelle Rancillac (CIRB)", "title": "Impaired neurovascular coupling in the APPxPS1 mouse model of\n  Alzheimer\u00e2\u0080\u0099s disease", "comments": "Current Alzheimer Research, Bentham Science Publishers, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tight coupling between neuronal activity and the local increase of blood\nflow termed neurovascular coupling is essential for normal brain function. This\nmechanism of regulation is compromised in Alzheimer's Disease (AD). In order to\ndetermine whether a purely vascular dysfunction or a neuronal alteration of\nblood vessels diameter control could be responsible for the impaired\nneurovascular coupling observed in AD, blood vessels reactivity in response to\ndifferent pharmacological stimulations was examined in double transgenic\nAPPxPS1 mice model of AD. Blood vessels movements were monitored using infrared\nvideomicroscopy ex vivo, in cortical slices of 8 month-old APPxPS1 and wild\ntype (WT) mice. We quantified vasomotor responses induced either by direct\nblood vessel stimulation with a thromboxane A 2 analogue, the U46619\n(9,11-dideoxy-11a,9a-epoxymethanoprostaglandin F2) or via the stimulation of\ninterneurons with the nicotinic acetylcholine receptor (nAChRs) agonist DMPP\n(1,1-Dimethyl-4-phenylpiperazinium iodide). Using both types of stimulation, no\nsignificant differences were detected for the amplitude of blood vessel\ndiameter changes between the transgenic APPxPS1 mice model of AD and WT mice,\nalthough the kinetics of recovery were slower in APPxPS1 mice. We find that\nactivation of neocortical interneurons with DMPP induced both vasodilation via\nNitric Oxide (NO) release and constriction via Neuropeptide Y (NPY) release.\nHowever, we observed a smaller proportion of reactive blood vessels following a\nneuronal activation in transgenic mice compared with WT mice. Altogether, these\nresults suggest that in this mouse model of AD, deficiency in the cortical\nneurovascular coupling essentially results from a neuronal rather than a\nvascular dysfunction.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:25:43 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Geoffroy", "H\u00e9l\u00e8ne", "", "NDC"], ["Rossier", "Jean", "", "NDC"], ["Rancillac", "Armelle", "", "CIRB"]]}, {"id": "1905.04149", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Xianzhi Wang, Jessica Monaghan, David Mcalpine,\n  Yu Zhang", "title": "A Survey on Deep Learning-based Non-Invasive Brain Signals:Recent\n  Advances and New Frontiers", "comments": "Accepted by Journal of Neural Engineering. Summarized more than 200+\n  brain signal-related papers, systematically covering 8 Brain-Computer\n  Interface (BCI) categories and 10+ deep learning models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) bridges the human's neural world and the outer\nphysical world by decoding individuals' brain signals into commands\nrecognizable by computer devices. Deep learning has lifted the performance of\nbrain-computer interface systems significantly in recent years. In this\narticle, we systematically investigate brain signal types for BCI and related\ndeep learning concepts for brain signal analysis. We then present a\ncomprehensive survey of deep learning techniques used for BCI, by summarizing\nover 230 contributions most published in the past five years. Finally, we\ndiscuss the applied areas, opening challenges, and future directions for deep\nlearning-based BCI.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:04:00 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:26:12 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 13:32:22 GMT"}, {"version": "v4", "created": "Sat, 26 Oct 2019 06:29:35 GMT"}, {"version": "v5", "created": "Wed, 21 Oct 2020 23:44:10 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Monaghan", "Jessica", ""], ["Mcalpine", "David", ""], ["Zhang", "Yu", ""]]}, {"id": "1905.04221", "submitter": "Steven Baete", "authors": "Steven H. Baete, Martijn A. Cloos, Ying-Chia Lin, Dimitris G.\n  Placantonakis, Timothy Shepherd, Fernando E. Boada", "title": "Fingerprinting Orientation Distribution Functions in Diffusion MRI\n  detects smaller crossing angles", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tractography is routinely used to study white matter architecture\nand brain connectivity in vivo. A key step for successful tractography of\nneuronal tracts is the correct identification of tract directions in each\nvoxel. Here we propose a fingerprinting-based methodology to identify these\nfiber directions in Orientation Distribution Functions, dubbed\nODF-Fingerprinting (ODF-FP). In ODF-FP, fiber configurations are selected based\non the similarity between measured ODFs and elements in a pre-computed library.\nIn noisy ODFs, the library matching algorithm penalizes the more complex fiber\nconfigurations. ODF simulations and analysis of bootstrapped partial and\nwhole-brain in vivo datasets show that the ODF-FP approach improves the\ndetection of fiber pairs with small crossing angles while maintaining fiber\ndirection precision, which leads to better tractography results. Rather than\nfocusing on the ODF maxima, the ODF-FP approach uses the whole ODF shape to\ninfer fiber directions to improve the detection of fiber bundles with small\ncrossing angle. The resulting fiber directions aid tractography algorithms in\naccurately displaying neuronal tracts and calculating brain connectivity.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:39:30 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Baete", "Steven H.", ""], ["Cloos", "Martijn A.", ""], ["Lin", "Ying-Chia", ""], ["Placantonakis", "Dimitris G.", ""], ["Shepherd", "Timothy", ""], ["Boada", "Fernando E.", ""]]}, {"id": "1905.04283", "submitter": "Dennis Dimond", "authors": "Dennis Dimond, Rebecca Perry, Giuseppe Iaria, Signe Bray", "title": "Visuospatial short-term memory and dorsal visual gray matter volume", "comments": "22 pages, 2 figures", "journal-ref": "J.Cortex (2019), 113:184-190", "doi": "10.1016/j.cortex.2018.12.007", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual short-term memory (VSTM) is an important cognitive capacity that\nvaries across the healthy adult population and is affected in several\nneurodevelopmental disorders. It has been suggested that neuroanatomy places\nlimits on this capacity through a map architecture that creates competition for\ncortical space. This suggestion has been supported by the finding that primary\nvisual (V1) gray matter volume (GMV) is positively associated with VSTM\ncapacity. However, evidence from neurodevelopmental disorders suggests that the\ndorsal visual stream more broadly is vulnerable and atypical volumes of other\nmap-containing regions may therefore play a role. For example, Turner syndrome\nis associated with concomitantly reduced volume of the right intraparietal\nsulcus (IPS) and deficits in VSTM. As posterior IPS regions (IPS0-2) contains\ntopographic maps, together this suggests that posterior IPS volumes may also\nassociate with VSTM. In this study, we assessed VSTM using two tasks, as well\nas a composite score, and used voxel-based morphometry of T1-weighted magnetic\nresonance images to assess GMV in V1 and right IPS0-2 in 32 healthy young\nadults (16 female). For comparison with previous work, we also assessed\nassociations between VSTM and voxel-wise GMV on a whole-brain basis. We found\nthat total brain volume (TBV) significantly correlated with VSTM, and that\ncorrelations between VSTM and regional GMV were substantially reduced in\nstrength when controlling for TBV. In our whole-brain analysis, we found that\nVSTM was associated with GMV of clusters centered around the right putamen and\nleft Rolandic operculum, though only when TBV was not controlled for. Our\nresults suggest that VSTM ability is unlikely to be accounted for by the volume\nof an individual cortical region and may instead rely on distributed structural\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 17:50:21 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Dimond", "Dennis", ""], ["Perry", "Rebecca", ""], ["Iaria", "Giuseppe", ""], ["Bray", "Signe", ""]]}, {"id": "1905.04367", "submitter": "Dimitrios Moirogiannis", "authors": "Keith Hayton and Dimitrios Moirogiannis", "title": "Emergence of Subcritical Bifurcations in a System of Randomly Coupled\n  Supercritical Andronov-Hopf Oscillators: A Potential Mechanism for Neural\n  Network Type Switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS nlin.AO physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental evidence suggests that the computational state of cortical\nsystems change according to behavioral and stimulus context. However, it is\nstill unknown what mechanisms underlie this adaptive processing in cortical\ncircuitry. In this paper, we present a model of randomly coupled supercritical\nAndronov-Hopf oscillators which can act as an adaptive processor by exhibiting\ndrastically different dynamics depending on the value of a single network\nparameter. Despite being only composed of supercritical subunits, the full\nsystem can exhibit either supercritical or subcritical Andronov-Hopf\nbifurcations. This model might provide a novel mechanism for switching between\nglobally asymptotically stable and nonhyperbolic neural network types in\npattern recognition theory.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 20:09:47 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 15:04:10 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Hayton", "Keith", ""], ["Moirogiannis", "Dimitrios", ""]]}, {"id": "1905.04527", "submitter": "Yiannis F. Contoyiannis", "authors": "Yiannis Contoyiannis and Myron Kampitakis", "title": "Neurobiological reality simulation through an Artificial Neural Network\n  at criticality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial neural network (ANN) based on fundamental principles of physics\ncan simulate the operation of neurobiological reality of membrane potential as\nwell as the properly defined order parameter. This ANN operates in conditions\nof criticality and simulates the behavior of an excitatory biological neuron,\nespecially the relaxation phase where the critical fluctuations of biological\nneuron appear. These critical fluctuations can not be explained by the\nHodgkin-Huxley (H-H) model. A proposal for the origin of these fluctuations is\nbeing discussed.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 13:34:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Contoyiannis", "Yiannis", ""], ["Kampitakis", "Myron", ""]]}, {"id": "1905.04820", "submitter": "Tijl Grootswagers", "authors": "Thomas A. Carlson, Tijl Grootswagers, Amanda K. Robinson", "title": "An introduction to time-resolved decoding analysis for M/EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is constantly processing and integrating information in order\nto make decisions and interact with the world, for tasks from recognizing a\nfamiliar face to playing a game of tennis. These complex cognitive processes\nrequire communication between large populations of neurons. The non-invasive\nneuroimaging methods of electroencephalography (EEG) and magnetoencephalography\n(MEG) provide population measures of neural activity with millisecond precision\nthat allow us to study the temporal dynamics of cognitive processes. However,\nmulti-sensor M/EEG data is inherently high dimensional, making it difficult to\nparse important signal from noise. Multivariate pattern analysis (MVPA) or\n\"decoding\" methods offer vast potential for understanding high-dimensional\nM/EEG neural data. MVPA can be used to distinguish between different conditions\nand map the time courses of various neural processes, from basic sensory\nprocessing to high-level cognitive processes. In this chapter, we discuss the\npractical aspects of performing decoding analyses on M/EEG data as well as the\nlimitations of the method, and then we discuss some applications for\nunderstanding representational dynamics in the human brain.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 01:26:54 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Carlson", "Thomas A.", ""], ["Grootswagers", "Tijl", ""], ["Robinson", "Amanda K.", ""]]}, {"id": "1905.05182", "submitter": "Gregoire Cattan", "authors": "Gijsbrecht Van Veen (GIPSA-VIBS), Alexandre Barachant (CEA-LETI,\n  GIPSA-VIBS), Anton Andreev (GIPSA-Services), Gr\\'egoire Cattan\n  (GIPSA-Services), Pedro Coelho Rodrigues (GIPSA-VIBS), Marco Congedo\n  (GIPSA-VIBS)", "title": "Building Brain Invaders: EEG data of an experimental validation", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.09111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the experimental procedures for a dataset that we have made\npublicly available at https://doi.org/10.5281/zenodo.2649006 in mat and csv\nformats. This dataset contains electroencephalographic (EEG) recordings of 25\nsubjects testing the Brain Invaders (Congedo, 2011), a visual P300\nBrain-Computer Interface inspired by the famous vintage video game Space\nInvaders (Taito, Tokyo, Japan). The visual P300 is an event-related potential\nelicited by a visual stimulation, peaking 240-600 ms after stimulus onset. EEG\ndata were recorded by 16 electrodes in an experiment that took place in the\nGIPSA-lab, Grenoble, France, in 2012 (Van Veen, 2013 and Congedo, 2013). Python\ncode for manipulating the data is available at\nhttps://github.com/plcrodrigues/py.BI.EEG.2012-GIPSA. The ID of this dataset is\nBI.EEG.2012-GIPSA.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:18:52 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Van Veen", "Gijsbrecht", "", "GIPSA-VIBS"], ["Barachant", "Alexandre", "", "CEA-LETI,\n  GIPSA-VIBS"], ["Andreev", "Anton", "", "GIPSA-Services"], ["Cattan", "Gr\u00e9goire", "", "GIPSA-Services"], ["Rodrigues", "Pedro Coelho", "", "GIPSA-VIBS"], ["Congedo", "Marco", "", "GIPSA-VIBS"]]}, {"id": "1905.05272", "submitter": "Martin Hjelm", "authors": "Martin Hjelm", "title": "Human Visual Understanding for Cognition and Manipulation -- A primer\n  for the roboticist", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic research is often built on approaches that are motivated by insights\nfrom self-examination of how we interface with the world. However, given\ncurrent theories about human cognition and sensory processing, it is reasonable\nto assume that the internal workings of the brain are separate from how we\ninterface with the world and ourselves. To amend some of these misconceptions\narising from self-examination this article reviews human visual understanding\nfor cognition and action, specifically manipulation. Our focus is on\nidentifying overarching principles such as the separation into visual\nprocessing for action and cognition, hierarchical processing of visual input,\nand the contextual and anticipatory nature of visual processing for action. We\nalso provide a rudimentary exposition of previous theories about visual\nunderstanding that shows how self-examination can lead down the wrong path. Our\nhope is that the article will provide insights for the robotic researcher that\ncan help them navigate the path of self-examination, give them an overview of\ncurrent theories about human visual processing, as well as provide a source for\nfurther relevant reading.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 20:09:32 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Hjelm", "Martin", ""]]}, {"id": "1905.05675", "submitter": "Radoslaw Martin Cichy", "authors": "Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij Dwivedi,\n  Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakrishnan, Aude\n  Oliva", "title": "The Algonauts Project: A Platform for Communication between the Sciences\n  of Biological and Artificial Intelligence", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last decade, artificial intelligence (AI) models inspired by the brain\nhave made unprecedented progress in performing real-world perceptual tasks like\nobject classification and speech recognition. Recently, researchers of natural\nintelligence have begun using those AI models to explore how the brain performs\nsuch tasks. These developments suggest that future progress will benefit from\nincreased interaction between disciplines. Here we introduce the Algonauts\nProject as a structured and quantitative communication channel for\ninterdisciplinary interaction between natural and artificial intelligence\nresearchers. The project's core is an open challenge with a quantitative\nbenchmark whose goal is to account for brain data through computational models.\nThis project has the potential to provide better models of natural intelligence\nand to gather findings that advance AI. The 2019 Algonauts Project focuses on\nbenchmarking computational models predicting human brain activity when people\nlook at pictures of objects. The 2019 edition of the Algonauts Project is\navailable online: http://algonauts.csail.mit.edu/.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:37:22 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Cichy", "Radoslaw Martin", ""], ["Roig", "Gemma", ""], ["Andonian", "Alex", ""], ["Dwivedi", "Kshitij", ""], ["Lahner", "Benjamin", ""], ["Lascelles", "Alex", ""], ["Mohsenzadeh", "Yalda", ""], ["Ramakrishnan", "Kandan", ""], ["Oliva", "Aude", ""]]}, {"id": "1905.05678", "submitter": "Adam Noel", "authors": "Adam Noel, Shayan Monabbati, Dimitrios Makrakis, Andrew W. Eckford", "title": "Modeling Interference-Free Neuron Spikes with Optogenetic Stimulation", "comments": "12 pages, 11 figures, 7 tables. Submitted for publication. Portions\n  of this work appeared previously as arXiv:1710.11569, which is the conference\n  version of this article", "journal-ref": null, "doi": "10.1109/TMBMC.2020.2981655", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper predicts the ability to externally control the firing times of a\ncortical neuron whose behavior follows the Izhikevich neuron model. The\nIzhikevich neuron model provides an efficient and biologically plausible method\nto track a cortical neuron's membrane potential and its firing times. The\nexternal control is a simple optogenetic model represented by an illumination\nsource that stimulates a saturating or decaying membrane current. This paper\nconsiders firing frequencies that are sufficiently low for the membrane\npotential to return to its resting potential after it fires. The time required\nfor the neuron to charge and for the neuron to recover to the resting potential\nare numerically fitted to functions of the Izhikevich neuron model parameters\nand the peak input current. Results show that simple functions of the model\nparameters and maximum input current can be used to predict the charging and\nrecovery times, even when there are deviations in the actual parameter values.\nFurthermore, the predictions lead to lower bounds on the firing frequency that\ncan be achieved without significant distortion.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:43:27 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 10:38:10 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Noel", "Adam", ""], ["Monabbati", "Shayan", ""], ["Makrakis", "Dimitrios", ""], ["Eckford", "Andrew W.", ""]]}, {"id": "1905.05827", "submitter": "Yejin Kim", "authors": "Yejin Kim, Xiaoqian Jiang, Luyao Chen, Xiaojin Li, Licong Cui", "title": "Discriminative Sleep Patterns of Alzheimer's Disease via Tensor\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": "PMC7153114", "categories": "q-bio.NC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep change is commonly reported in Alzheimer's disease (AD) patients and\ntheir brain wave studies show decrease in dreaming and non-dreaming stages.\nAlthough sleep disturbance is generally considered as a consequence of AD, it\nmight also be a risk factor of AD as new biological evidence shows. Leveraging\nNational Sleep Research Resource (NSRR), we built a unique cohort of 83 cases\nand 331 controls with clinical variables and EEG signals. Supervised tensor\nfactorization method was applied for this temporal dataset to extract\ndiscriminative sleep patterns. Among the 30 patterns extracted, we identified 5\nsignificant patterns (4 patterns for AD likely and 1 pattern for normal ones)\nand their visual patterns provide interesting linkage to sleep with repeated\nwakefulness, insomnia, epileptic seizure, and etc. This study is preliminary\nbut findings are interesting, which is a first step to provide quantifiable\nevidences to measure sleep as a risk factor of AD.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:21:58 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kim", "Yejin", ""], ["Jiang", "Xiaoqian", ""], ["Chen", "Luyao", ""], ["Li", "Xiaojin", ""], ["Cui", "Licong", ""]]}, {"id": "1905.06038", "submitter": "Alberto P\\'erez-Cervera", "authors": "Alberto P\\'erez-Cervera, Tere M. Seara and Gemma Huguet", "title": "Phase-locked states in oscillating neural networks and their role in\n  neural communication", "comments": null, "journal-ref": null, "doi": "10.1016/j.cnsns.2019.104992", "report-no": null, "categories": "q-bio.NC math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of communication through coherence (CTC) proposes that brain\noscillations reflect changes in the excitability of neurons, and therefore the\nsuccessful communication between two oscillating neural populations depends not\nonly on the strength of the signal emitted but also on the relative phases\nbetween them. More precisely, effective communication occurs when the emitting\nand receiving populations are properly phase locked so the inputs sent by the\nemitting population arrive at the phases of maximal excitability of the\nreceiving population. To study this setting, we consider a population rate\nmodel consisting of excitatory and inhibitory cells modelling the receiving\npopulation, and we perturb it with a time-dependent periodic function modelling\nthe input from the emitting population. We consider the stroboscopic map for\nthis system and compute numerically the fixed and periodic points of this map\nand their bifurcations as the amplitude and the frequency of the perturbation\nare varied. From the bifurcation diagram, we identify the phase-locked states\nas well as different regions of bistability. We explore carefully the dynamics\nemphasizing its implications for the CTC theory. In particular, we study how\nthe input gain depends on the timing between the input and the inhibitory\naction of the receiving population. Our results show that naturally an optimal\nphase locking for CTC emerges, and provide a mechanism by which the receiving\npopulation can implement selective communication. Moreover, the presence of\nbistable regions, suggests a mechanism by which different communication regimes\nbetween brain areas can be established without changing the structure of the\nnetwork\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 09:05:23 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 10:51:10 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 09:16:25 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["P\u00e9rez-Cervera", "Alberto", ""], ["Seara", "Tere M.", ""], ["Huguet", "Gemma", ""]]}, {"id": "1905.06225", "submitter": "Hamid R Noori", "authors": "Farzad Fathizadeh, Ekaterina Mitricheva, Rui Kimura, Nikos Logothetis,\n  Hamid Reza Noori", "title": "Signal detection in extracellular neural ensemble recordings using\n  higher criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information processing in the brain is conducted by a concerted action of\nmultiple neural populations. Gaining insights in the organization and dynamics\nof such populations can best be studied with broadband intracranial recordings\nof so-called extracellular field potential, reflecting neuronal spiking as well\nas mesoscopic activities, such as waves, oscillations, intrinsic large\ndeflections, and multiunit spiking activity. Such signals are critical for our\nunderstanding of how neuronal ensembles encode sensory information and how such\ninformation is integrated in the large networks underlying cognition. The\naforementioned principles are now well accepted, yet the efficacy of extracting\ninformation out of the complex neural data, and their employment for improving\nour understanding of neural networks, critically depends on the mathematical\nprocessing steps ranging from simple detection of action potentials in noisy\ntraces - to fitting advanced mathematical models to distinct patterns of the\nneural signal potentially underlying intra-processing of information, e.g.\ninterneuronal interactions. Here, we present a robust strategy for detecting\nsignals in broadband and noisy time series such as spikes, sharp waves and\nmulti-unit activity data that is solely based on the intrinsic statistical\ndistribution of the recorded data. By using so-called higher criticism - a\nsecond-level significance testing procedure comparing the fraction of observed\nsignificances to an expected fraction under the global null - we are able to\ndetect small signals in correlated noisy time-series without prior filtering,\ndenoising or data regression. Results demonstrate the efficiency and\nreliability of the method and versatility over a wide range of experimental\nconditions and suggest the appropriateness of higher criticism to characterize\nneuronal dynamics without prior manipulation of the data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:58:03 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Fathizadeh", "Farzad", ""], ["Mitricheva", "Ekaterina", ""], ["Kimura", "Rui", ""], ["Logothetis", "Nikos", ""], ["Noori", "Hamid Reza", ""]]}, {"id": "1905.06236", "submitter": "Wushi Dong", "authors": "Wushi Dong, Murat Keceli, Rafael Vescovi, Hanyu Li, Corey Adams, Elise\n  Jennings, Samuel Flender, Tom Uram, Venkatram Vishwanath, Nicola Ferrier,\n  Narayanan Kasthuri, Peter Littlewood", "title": "Scaling Distributed Training of Flood-Filling Networks on HPC\n  Infrastructure for Brain Mapping", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping all the neurons in the brain requires automatic reconstruction of\nentire cells from volume electron microscopy data. The flood-filling network\n(FFN) architecture has demonstrated leading performance for segmenting\nstructures from this data. However, the training of the network is\ncomputationally expensive. In order to reduce the training time, we implemented\nsynchronous and data-parallel distributed training using the Horovod library,\nwhich is different from the asynchronous training scheme used in the published\nFFN code. We demonstrated that our distributed training scaled well up to 2048\nIntel Knights Landing (KNL) nodes on the Theta supercomputer. Our trained\nmodels achieved similar level of inference performance, but took less training\ntime compared to previous methods. Our study on the effects of different batch\nsizes on FFN training suggests ways to further improve training efficiency. Our\nfindings on optimal learning rate and batch sizes agree with previous works.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:00:52 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 03:27:23 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2019 17:37:37 GMT"}, {"version": "v4", "created": "Mon, 9 Dec 2019 22:29:23 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Dong", "Wushi", ""], ["Keceli", "Murat", ""], ["Vescovi", "Rafael", ""], ["Li", "Hanyu", ""], ["Adams", "Corey", ""], ["Jennings", "Elise", ""], ["Flender", "Samuel", ""], ["Uram", "Tom", ""], ["Vishwanath", "Venkatram", ""], ["Ferrier", "Nicola", ""], ["Kasthuri", "Narayanan", ""], ["Littlewood", "Peter", ""]]}, {"id": "1905.07153", "submitter": "Claudius Gros", "authors": "Frederike Kubandt, Michael Nowak, Tim Koglin, Claudius Gros, Bulcsu\n  Sandor", "title": "Embodied robots driven by self-organized environmental feedback", "comments": "Adaptive Behavior, in presse", "journal-ref": "Adaptive Behavior, 105971231985562 (2019)", "doi": "10.1177/1059712319855622", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which kind of complex behavior may arise from self-organizing principles? We\ninvestigate this question for the case of snake-like robots composed of\npassively coupled segments, with every segment containing two wheels actuated\nseparately by a single neuron. The robot is self organized both on the level of\nthe individual wheels and with respect to inter-wheel coordination, which\narises exclusively from the mechanical coupling of the individual wheels and\nsegments. For the individual wheel, the generating principle proposed results\nin locomotive states that correspond to self-organized limit cycles of the\nsensorimotor loop.\n  Our robot interacts with the environment by monitoring the state of its\nactuators, that is via propriosensation. External sensors are absent. In a\nstructured environment the robot shows complex emergent behavior that includes\npushing movable blocks around, reversing direction when hitting a wall and\nturning when climbing a slope. On flat grounds the robot wiggles in a\nsnake-like manner, when moving at higher velocities. We also investigate the\nemergence of motor primitives, viz the route to locomotion, which is\ncharacterized by a series of local and global bifurcations in terms of\ndynamical system theory.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 07:56:21 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kubandt", "Frederike", ""], ["Nowak", "Michael", ""], ["Koglin", "Tim", ""], ["Gros", "Claudius", ""], ["Sandor", "Bulcsu", ""]]}, {"id": "1905.07474", "submitter": "Siddharth Siddharth", "authors": "Julia Anna Adrian, Siddharth Siddharth, Syed Zain Ali Baquar,\n  Tzyy-Ping Jung, and Gedeon De\\'ak", "title": "Decision-Making in a Social Multi-Armed Bandit Task: Behavior,\n  Electrophysiology and Pupillometry", "comments": "Accepted for publication in The 41st Annual Meeting of the Cognitive\n  Science Society (CogSci 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding, predicting, and learning from other people's actions are\nfundamental human social-cognitive skills. Little is known about how and when\nwe consider other's actions and outcomes when making our own decisions. We\ndeveloped a novel task to study social influence in decision-making: the social\nmulti-armed bandit task. This task assesses how people learn policies for\noptimal choices based on their own outcomes and another player's (observed)\noutcomes. The majority of participants integrated information gained through\nobservation of their partner similarly as information gained through their own\nactions. This lead to a suboptimal decision-making strategy. Interestingly,\nevent-related potentials time-locked to stimulus onset qualitatively similar\nbut the amplitudes are attenuated in the solo compared to the dyadic version.\nThis might indicate that arousal and attention after receiving a reward are\nsustained when a second agent is present but not when playing alone.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:51:31 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Adrian", "Julia Anna", ""], ["Siddharth", "Siddharth", ""], ["Baquar", "Syed Zain Ali", ""], ["Jung", "Tzyy-Ping", ""], ["De\u00e1k", "Gedeon", ""]]}, {"id": "1905.07562", "submitter": "Feng Qi", "authors": "Feng Qi and Wenchuan Wu", "title": "Human-like machine thinking: Language guided imagination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human thinking requires the brain to understand the meaning of language\nexpression and to properly organize the thoughts flow using the language.\nHowever, current natural language processing models are primarily limited in\nthe word probability estimation. Here, we proposed a Language guided\nimagination (LGI) network to incrementally learn the meaning and usage of\nnumerous words and syntaxes, aiming to form a human-like machine thinking\nprocess. LGI contains three subsystems: (1) vision system that contains an\nencoder to disentangle the input or imagined scenarios into abstract population\nrepresentations, and an imagination decoder to reconstruct imagined scenario\nfrom higher level representations; (2) Language system, that contains a\nbinarizer to transfer symbol texts into binary vectors, an IPS (mimicking the\nhuman IntraParietal Sulcus, implemented by an LSTM) to extract the quantity\ninformation from the input texts, and a textizer to convert binary vectors into\ntext symbols; (3) a PFC (mimicking the human PreFrontal Cortex, implemented by\nan LSTM) to combine inputs of both language and vision representations, and\npredict text symbols and manipulated images accordingly. LGI has incrementally\nlearned eight different syntaxes (or tasks), with which a machine thinking loop\nhas been formed and validated by the proper interaction between language and\nvision system. The paper provides a new architecture to let the machine learn,\nunderstand and use language in a human-like way that could ultimately enable a\nmachine to construct fictitious 'mental' scenario and possess intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 09:23:00 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 07:46:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Qi", "Feng", ""], ["Wu", "Wenchuan", ""]]}, {"id": "1905.07606", "submitter": "Maxwell Bertolero Dr", "authors": "Maxwell A. Bertolero, Ann Sizemore Blevins, Graham L. Baum, Ruben C.\n  Gur, Raquel E. Gur, David R. Roalf, Theodore D. Satterthwaite, Danielle S.\n  Bassett", "title": "The human brain's network architecture is genetically encoded by modular\n  pleiotropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.GN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For much of biology, the manner in which genotype maps to phenotype remains a\nfundamental mystery. The few maps that are known tend to show modular\npleiotropy: sets of phenotypes are determined by distinct sets of genes. One\nkey map that has evaded discovery is that of the human brain's network\narchitecture. Here, we determine the form of this map for gene coexpression and\nsingle nucleotide polymorphisms. We discover that mostly non-overlapping sets\nof genes encode the connectivity of brain network modules (or so-called\ncommunities), suggesting that brain network communities demarcate genetic\ntransitions. We find that these clean boundaries break down at connector hubs,\nwhose integrative connectivity is encoded by pleiotropic genes from mostly\nnon-overlapping sets. Broadly, this study opens fundamentally new directions in\nthe study of genetic encoding of brain development, evolution, and disease.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 15:56:50 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 16:09:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bertolero", "Maxwell A.", ""], ["Blevins", "Ann Sizemore", ""], ["Baum", "Graham L.", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Roalf", "David R.", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1905.07813", "submitter": "Maria Giulia Preti", "authors": "Maria Giulia Preti and Dimitri Van De Ville", "title": "Decoupling of brain function from structure reveals regional behavioral\n  specialization in humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is an assembly of neuronal populations interconnected by structural\npathways. Brain activity is expressed on and constrained by this substrate.\nTherefore, statistical dependencies between functional signals in directly\nconnected areas can be expected higher. However, the degree to which brain\nfunction is bound by the underlying wiring diagram remains a complex question\nthat has been only partially answered. Here, we introduce the\nstructural-decoupling index to quantify the coupling strength between structure\nand function, and we reveal a macroscale gradient from brain regions more\nstrongly coupled, to regions more strongly decoupled, than expected by\nrealistic surrogate data. This gradient spans behavioral domains from\nlower-level sensory function to high-level cognitive ones and shows for the\nfirst time that the strength of structure-function coupling is spatially\nvarying in line with evidence derived from other modalities, such as functional\nconnectivity, gene expression, microstructural properties and temporal\nhierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 21:51:11 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 12:24:55 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Preti", "Maria Giulia", ""], ["Van De Ville", "Dimitri", ""]]}, {"id": "1905.09826", "submitter": "Raphael Sivera", "authors": "Rapha\\\"el Sivera (EPIONE), Herv\\'e Delingette (EPIONE), Marco Lorenzi\n  (EPIONE), Xavier Pennec (EPIONE), Nicholas Ayache (EPIONE)", "title": "A model of brain morphological changes related to aging and Alzheimer's\n  disease from cross-sectional assessments", "comments": "NeuroImage, Elsevier, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we propose a deformation-based framework to jointly model the\ninfluence of aging and Alzheimer's disease (AD) on the brain morphological\nevolution. Our approach combines a spatio-temporal description of both\nprocesses into a generative model. A reference morphology is deformed along\nspecific trajectories to match subject specific morphologies. It is used to\ndefine two imaging progression markers: 1) a morphological age and 2) a disease\nscore. These markers can be computed locally in any brain region. The approach\nis evaluated on brain structural magnetic resonance images (MRI) from the ADNI\ndatabase. The generative model is first estimated on a control population,\nthen, for each subject, the markers are computed for each acquisition. The\nlongitudinal evolution of these markers is then studied in relation with the\nclinical diagnosis of the subjects and used to generate possible morphological\nevolution. In the model, the morphological changes associated with normal aging\nare mainly found around the ventricles, while the Alzheimer's disease specific\nchanges are more located in the temporal lobe and the hippocampal area. The\nstatistical analysis of these markers highlights differences between clinical\nconditions even though the inter-subject variability is quiet high. In this\ncontext, the model can be used to generate plausible morphological trajectories\nassociated with the disease. Our method gives two interpretable scalar imaging\nbiomarkers assessing the effects of aging and disease on brain morphology at\nthe individual and population level. These markers confirm an acceleration of\napparent aging for Alzheimer's subjects and can help discriminate clinical\nconditions even in prodromal stages. More generally, the joint modeling of\nnormal and pathological evolutions shows promising results to describe\nage-related brain diseases over long time scales.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:26:30 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Sivera", "Rapha\u00ebl", "", "EPIONE"], ["Delingette", "Herv\u00e9", "", "EPIONE"], ["Lorenzi", "Marco", "", "EPIONE"], ["Pennec", "Xavier", "", "EPIONE"], ["Ayache", "Nicholas", "", "EPIONE"]]}, {"id": "1905.10010", "submitter": "Lukas Hirsch", "authors": "Lukas Hirsch, Yu Huang, Lucas C Parra", "title": "Segmentation of MRI head anatomy using deep volumetric networks and\n  multiple spatial priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Conventional automated segmentation of the head anatomy in MRI\ndistinguishes different brain and non-brain tissues based on image intensities\nand prior tissue probability maps (TPM). This works well for normal head\nanatomies, but fails in the presence of unexpected lesions. Deep convolutional\nneural networks leverage instead spatial patterns and can learn to segment\nlesions, but often ignore prior probabilities. Approach: We add three sources\nof prior information to a three-dimensional convolutional network, namely,\nspatial priors with a TPM, morphological priors with conditional random fields,\nand spatial context with a wider field-of-view at lower resolution. We train\nand test these networks on 3D images of 43 stroke patients and 4 healthy\nindividuals which have been manually segmented. Results: We demonstrate the\nbenefits of each sources of prior information, and we show that the new\narchitecture, which we call Multiprior network, improves the performance of\nexisting segmentation software, such as SPM, FSL, and DeepMedic for abnormal\nanatomies. The relevance of the different priors was compared and the TPM was\nfound to be most beneficial. The benefit of adding a TPM is generic in that it\ncan boost the performance of established segmentation networks such as the\nDeepMedic and a UNet. We also provide an out-of-sample validation and clinical\napplication of the approach on an additional 47 patients with disorders of\nconsciousness. We make the code and trained networks freely available.\nConclusions: Biomedical images follow imaging protocols that can be leveraged\nas prior information into deep convolutional neural networks to improve\nperformance. The network segmentations match human manual corrections performed\nin 3D, and are comparable in performance to human segmentations obtained from\nscratch in 2D for abnormal brain anatomies.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 02:54:04 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 14:29:22 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 19:22:16 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 16:18:57 GMT"}, {"version": "v5", "created": "Wed, 19 May 2021 14:32:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Hirsch", "Lukas", ""], ["Huang", "Yu", ""], ["Parra", "Lucas C", ""]]}, {"id": "1905.10023", "submitter": "Xieyi Liu", "authors": "Xieyi Liu, Junjun Zhang, Ling Li", "title": "Separation Effect of Early Visual Cortex V1 Under Different Crowding\n  Conditions A TMS Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual crowding makes it difficult to identify the patterns in peripheral\nvision, but the neural mechanism for this phenomenon is still unclear because\nof different opinions. In order to study the separation effect of V1 under\ndifferent crowding conditions, single-pulse transcranial magnetic stimulation\nis applied within the right V1. The experimental design includes two factors:\nTMS intensity (10%, 65%, and 90% of the phosphene threshold) and crowding (high\nand low) conditions. The accuracy results show that there is a strong\ninteraction between crowding condition and TMS condition. When the TMS\nstimulation intensity is lower than the phosphene threshold, more crowding will\nbe perceived under the high crowding condition, and less crowding will be\nperceived under the low crowding condition. The above results conclude that the\nhigh and low crowding condition separate by TMS stimulation. The results\nsupport the assumption that the crowding is related to V1 and occurs in the\nvisual coding phase.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 04:09:24 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Liu", "Xieyi", ""], ["Zhang", "Junjun", ""], ["Li", "Ling", ""]]}, {"id": "1905.10037", "submitter": "Satoshi Nishida", "authors": "Satoshi Nishida, Yusuke Nakano, Antoine Blanc, Naoya Maeda, Masataka\n  Kado, and Shinji Nishimoto", "title": "Brain-mediated Transfer Learning of Convolutional Neural Networks", "comments": null, "journal-ref": "Proc. Thirty-Fourth AAAI Conf. Artif. Intell. (2020) 5281-5288", "doi": "10.1609/aaai.v34i04.5974", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The human brain can effectively learn a new task from a small number of\nsamples, which indicate that the brain can transfer its prior knowledge to\nsolve tasks in different domains. This function is analogous to transfer\nlearning (TL) in the field of machine learning. TL uses a well-trained feature\nspace in a specific task domain to improve performance in new tasks with\ninsufficient training data. TL with rich feature representations, such as\nfeatures of convolutional neural networks (CNNs), shows high generalization\nability across different task domains. However, such TL is still insufficient\nin making machine learning attain generalization ability comparable to that of\nthe human brain. To examine if the internal representation of the brain could\nbe used to achieve more efficient TL, we introduce a method for TL mediated by\nhuman brains. Our method transforms feature representations of audiovisual\ninputs in CNNs into those in activation patterns of individual brains via their\nassociation learned ahead using measured brain responses. Then, to estimate\nlabels reflecting human cognition and behavior induced by the audiovisual\ninputs, the transformed representations are used for TL. We demonstrate that\nour brain-mediated TL (BTL) shows higher performance in the label estimation\nthan the standard TL. In addition, we illustrate that the estimations mediated\nby different brains vary from brain to brain, and the variability reflects the\nindividual variability in perception. Thus, our BTL provides a framework to\nimprove the generalization ability of machine-learning feature representations\nand enable machine learning to estimate human-like cognition and behavior,\nincluding individual variability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 05:15:17 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 10:09:35 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 02:20:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Nishida", "Satoshi", ""], ["Nakano", "Yusuke", ""], ["Blanc", "Antoine", ""], ["Maeda", "Naoya", ""], ["Kado", "Masataka", ""], ["Nishimoto", "Shinji", ""]]}, {"id": "1905.10172", "submitter": "Dr. Alexander Paraskevov", "authors": "A.V. Paraskevov, A.S. Minkin", "title": "Damped oscillations of the probability of random events followed by\n  absolute refractory period", "comments": "additional section has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.PR math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many events are followed by an absolute refractory state, when for some time\nafter the event a repetition of a similar event is impossible. If uniform\nevents, each of which is followed by the same period of absolute\nrefractoriness, occur randomly, as in the Bernoulli scheme, then the event\nprobability as a function of time can exhibit damped transient oscillations\ncaused by a specific initial condition. Here we give an exact analytical\ndescription of the oscillations, with a focus on application within\nneuroscience. The resulting formulas stand out for their relative simplicity,\nenabling analytical calculation of the damping coefficients for the second and\nthird peaks of the event probability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 12:02:17 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 15:05:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Paraskevov", "A. V.", ""], ["Minkin", "A. S.", ""]]}, {"id": "1905.10281", "submitter": "Aline Viol", "authors": "Aline Viol, Vesna Vuksanovi\\'c and Philipp H\\\"ovel", "title": "Information parity in complex networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing interest in complex networks theory results in an ongoing demand\nfor new analytical tools. We propose a novel measure based on information\ntheory that provides a new perspective for a better understanding of networked\nsystems: Termed \"information parity,\" it quantifies the consonance of influence\namong nodes with respect to the whole network architecture. Considering the\nstatistics of geodesic distances, information parity detects how similar a pair\nof nodes can influence and be influenced by the network. This allows us to\nquantify the quality of information gathered by the nodes. To demonstrate the\nmethod's potential, we evaluate a social network and human brain networks. Our\nresults indicate that emerging phenomena like an ideological orientation of\nnodes in a social network is severely influenced by their information parities.\nWe also show that anatomical brain networks have a greater information parity\nin inter-hemispheric correspondent regions placed near the sagittal plane.\nFinally, functional networks have, on average, greater information parity for\ninter-hemispheric correspondent regions in comparison to the whole network. We\nfind that a pair of regions with high information parity exhibits higher\ncorrelation, suggesting that the functional correlations between cortical\nregions can be partially explained by the symmetry of their overall influences\nof the whole brain.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:21:42 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:00:57 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Viol", "Aline", ""], ["Vuksanovi\u0107", "Vesna", ""], ["H\u00f6vel", "Philipp", ""]]}, {"id": "1905.10401", "submitter": "Johannes Zierenberg", "authors": "Johannes Zierenberg, Jens Wilting, Viola Priesemann, Anna Levina", "title": "Tailored ensembles of neural networks optimize sensitivity to stimulus\n  statistics", "comments": "6 pages plus supplemental material", "journal-ref": "Phys. Rev. Research 2, 013115 (2020)", "doi": "10.1103/PhysRevResearch.2.013115", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic range of stimulus processing in living organisms is much larger\nthan a single neural network can explain. For a generic, tunable spiking\nnetwork we derive that while the dynamic range is maximal at criticality, the\ninterval of discriminable intensities is very similar for any network tuning\ndue to coalescence. Compensating coalescence enables adaptation of\ndiscriminable intervals. Thus, we can tailor an ensemble of networks optimized\nto the distribution of stimulus intensities, e.g., extending the dynamic range\narbitrarily. We discuss potential applications in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 18:33:21 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zierenberg", "Johannes", ""], ["Wilting", "Jens", ""], ["Priesemann", "Viola", ""], ["Levina", "Anna", ""]]}, {"id": "1905.10402", "submitter": "Johannes Zierenberg", "authors": "Johannes Zierenberg, Jens Wilting, Viola Priesemann, Anna Levina", "title": "Description of spreading dynamics by microscopic network models and\n  macroscopic branching processes can differ due to coalescence", "comments": "13 pages", "journal-ref": "Phys. Rev. E 101, 022301 (2020)", "doi": "10.1103/PhysRevE.101.022301", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreading processes are conventionally monitored on a macroscopic level by\ncounting the number of incidences over time. The spreading process can then be\nmodeled either on the microscopic level, assuming an underlying interaction\nnetwork, or directly on the macroscopic level, assuming that microscopic\ncontributions are negligible. The macroscopic characteristics of both\ndescriptions are commonly assumed to be identical. In this work, we show that\nthese characteristics of microscopic and macroscopic descriptions can be\ndifferent due to coalescence, i.e., a node being activated at the same time by\nmultiple sources. In particular, we consider a (microscopic) branching network\n(probabilistic cellular automaton) with annealed connectivity disorder, record\nthe macroscopic activity, and then approximate this activity by a (macroscopic)\nbranching process. In this framework, we analytically calculate the effect of\ncoalescence on the collective dynamics. We show that coalescence leads to a\nuniversal non-linear scaling function for the conditional expectation value of\nsuccessive network activity. This allows us to quantify the difference between\nthe microscopic model parameter and established macroscopic estimates. To\novercome this difference, we propose a non-linear estimator that correctly\ninfers the model branching parameter for all system sizes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 18:33:25 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zierenberg", "Johannes", ""], ["Wilting", "Jens", ""], ["Priesemann", "Viola", ""], ["Levina", "Anna", ""]]}, {"id": "1905.10480", "submitter": "Shailaja Akella", "authors": "Shailaja Akella and Jose C. Principe", "title": "Correntropy Based Robust Decomposition of Neuromodulations", "comments": "4 pages, Engineering in Medicine and Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromodulations as observed in the extracellular electrical potential\nrecordings obtained from Electroencephalograms (EEG) manifest as organized,\ntransient patterns that differ statistically from their featureless noisy\nbackground. Leveraging on this statistical dissimilarity, we propose a\nnoniterative robust classification algorithm to isolate, in time, these\nneuromodulations from the temporally disorganized but structured background\nactivity while simultaneously incorporating temporal sparsity of the events.\nSpecifically, we exploit the ability of correntropy to asses higher - order\nmoments as well as imply the degree of similarity between two random variables\nin the joint space regulated by the kernel bandwidth. We test our algorithm on\nDREAMS Sleep Spindle Database and further elaborate on the hyperparameters\nintroduced. Finally, we compare the performance of the algorithm with two\nalgorithms designed on similar ideas; one of which is a quick, simple norm\nbased technique while the other parallels the state-of-the-art Robust Principal\nComponent Analysis (RPCA) to achieve classification. The algorithm is able to\nmatch the performance of the state-of-the-art techniques while saving\ntremendously on computation time and complexity.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:30:36 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Akella", "Shailaja", ""], ["Principe", "Jose C.", ""]]}, {"id": "1905.10629", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Claire Launay and Ruben Coen-Cagli", "title": "Flexibly Regularized Mixture Models and Application to Image\n  Segmentation", "comments": "33 pages ( 30 + 3 for appendix). 11 figures + 1 in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistic finite mixture models are widely used for unsupervised\nclustering. These models can often be improved by adapting them to the topology\nof the data. For instance, in order to classify spatially adjacent data points\nsimilarly, it is common to introduce a Laplacian constraint on the posterior\nprobability that each data point belongs to a class. Alternatively, the mixing\nprobabilities can be treated as free parameters, while assuming Gauss-Markov or\nmore complex priors to regularize those mixing probabilities. However, these\napproaches are constrained by the shape of the prior and often lead to\ncomplicated or intractable inference. Here, we propose a new parametrization of\nthe Dirichlet distribution to flexibly regularize the mixing probabilities of\nover-parametrized mixture distributions. Using the Expectation-Maximization\nalgorithm, we show that our approach allows us to define any linear update rule\nfor the mixing probabilities, including spatial smoothing regularization as a\nspecial case. We then show that this flexible design can be extended to share\nclass information between multiple mixture models. We apply our algorithm to\nartificial and natural image segmentation tasks, and we provide quantitative\nand qualitative comparison of the performance of Gaussian and Student-t\nmixtures on the Berkeley Segmentation Dataset. We also demonstrate how to\npropagate class information across the layers of deep convolutional neural\nnetworks in a probabilistically optimal way, suggesting a new interpretation\nfor feedback signals in biological visual systems. Our flexible approach can be\neasily generalized to adapt probabilistic mixture models to arbitrary data\ntopologies.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 16:55:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:48:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Vacher", "Jonathan", ""], ["Launay", "Claire", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "1905.10831", "submitter": "Neil Oxtoby", "authors": "Agoston Mihalik, Mikael Brudfors, Maria Robu, Fabio S. Ferreira,\n  Hongxiang Lin, Anita Rau, Tong Wu, Stefano B. Blumberg, Baris Kanber, Maira\n  Tariq, Maria Del Mar Estarellas Garcia, Cemre Zor, Daniil I. Nikitichev,\n  Janaina Mourao-Miranda, Neil P. Oxtoby", "title": "ABCD Neurocognitive Prediction Challenge 2019: Predicting individual\n  fluid intelligence scores from structural MRI using probabilistic\n  segmentation and kernel ridge regression", "comments": "Winning entry in the ABCD Neurocognitive Prediction Challenge at\n  MICCAI 2019. 7 pages plus references, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied several regression and deep learning methods to predict fluid\nintelligence scores from T1-weighted MRI scans as part of the ABCD\nNeurocognitive Prediction Challenge (ABCD-NP-Challenge) 2019. We used voxel\nintensities and probabilistic tissue-type labels derived from these as features\nto train the models. The best predictive performance (lowest mean-squared\nerror) came from Kernel Ridge Regression (KRR; $\\lambda=10$), which produced a\nmean-squared error of 69.7204 on the validation set and 92.1298 on the test\nset. This placed our group in the fifth position on the validation leader board\nand first place on the final (test) leader board.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:31:00 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Mihalik", "Agoston", ""], ["Brudfors", "Mikael", ""], ["Robu", "Maria", ""], ["Ferreira", "Fabio S.", ""], ["Lin", "Hongxiang", ""], ["Rau", "Anita", ""], ["Wu", "Tong", ""], ["Blumberg", "Stefano B.", ""], ["Kanber", "Baris", ""], ["Tariq", "Maira", ""], ["Garcia", "Maria Del Mar Estarellas", ""], ["Zor", "Cemre", ""], ["Nikitichev", "Daniil I.", ""], ["Mourao-Miranda", "Janaina", ""], ["Oxtoby", "Neil P.", ""]]}, {"id": "1905.10834", "submitter": "Neil Oxtoby", "authors": "Neil P. Oxtoby, Fabio S. Ferreira, Agoston Mihalik, Tong Wu, Mikael\n  Brudfors, Hongxiang Lin, Anita Rau, Stefano B. Blumberg, Maria Robu, Cemre\n  Zor, Maira Tariq, Maria Del Mar Estarellas Garcia, Baris Kanber, Daniil I.\n  Nikitichev, Janaina Mourao-Miranda", "title": "ABCD Neurocognitive Prediction Challenge 2019: Predicting individual\n  residual fluid intelligence scores from cortical grey matter morphology", "comments": "8 pages plus references, 3 figures, 2 tables. Submission to the ABCD\n  Neurocognitive Prediction Challenge at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predicted residual fluid intelligence scores from T1-weighted MRI data\navailable as part of the ABCD NP Challenge 2019, using morphological similarity\nof grey-matter regions across the cortex. Individual structural covariance\nnetworks (SCN) were abstracted into graph-theory metrics averaged over nodes\nacross the brain and in data-driven communities/modules. Metrics included\ndegree, path length, clustering coefficient, centrality, rich club coefficient,\nand small-worldness. These features derived from the training set were used to\nbuild various regression models for predicting residual fluid intelligence\nscores, with performance evaluated both using cross-validation within the\ntraining set and using the held-out validation set. Our predictions on the test\nset were generated with a support vector regression model trained on the\ntraining set. We found minimal improvement over predicting a zero residual\nfluid intelligence score across the sample population, implying that structural\ncovariance networks calculated from T1-weighted MR imaging data provide little\ninformation about residual fluid intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 16:38:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Oxtoby", "Neil P.", ""], ["Ferreira", "Fabio S.", ""], ["Mihalik", "Agoston", ""], ["Wu", "Tong", ""], ["Brudfors", "Mikael", ""], ["Lin", "Hongxiang", ""], ["Rau", "Anita", ""], ["Blumberg", "Stefano B.", ""], ["Robu", "Maria", ""], ["Zor", "Cemre", ""], ["Tariq", "Maira", ""], ["Garcia", "Maria Del Mar Estarellas", ""], ["Kanber", "Baris", ""], ["Nikitichev", "Daniil I.", ""], ["Mourao-Miranda", "Janaina", ""]]}, {"id": "1905.11110", "submitter": "Zhi-Xuan Tan", "authors": "Zhi-Xuan Tan and Desmond C. Ong", "title": "Bayesian Inference of Social Norms as Shared Constraints on Behavior", "comments": "7 pages, 5 figures, to appear in CogSci 2019, code available at\n  https://github.com/ztangent/norms-cogsci19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People act upon their desires, but often, also act in adherence to implicit\nsocial norms. How do people infer these unstated social norms from others'\nbehavior, especially in novel social contexts? We propose that laypeople have\nintuitive theories of social norms as behavioral constraints shared across\ndifferent agents in the same social context. We formalize inference of norms\nusing a Bayesian Theory of Mind approach, and show that this computational\napproach provides excellent predictions of how people infer norms in two\nscenarios. Our results suggest that people separate the influence of norms and\nindividual desires on others' actions, and have implications for modelling\ngeneralizations of hidden causes of behavior.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:33:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tan", "Zhi-Xuan", ""], ["Ong", "Desmond C.", ""]]}, {"id": "1905.11249", "submitter": "Fabiano Ferrari", "authors": "Jos\\'e C. P. Coninck, Fabiano A. S. Ferrari, Adriane S. Reis, Kelly C.\n  Iarosz, Antonio M. Batista, Ricardo L. Viana", "title": "Network properties of healthy and Alzheimer's brains", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2020.124475", "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small-world structures are often used to describe structural connections in\nthe brain. In this work, we compare the structural connection of cortical areas\nof a healthy brain and a brain affected by Alzheimer's disease with artificial\nsmall-world networks. Based on statistics analysis, we demonstrate that similar\nsmall-world networks can be constructed using Newman-Watts procedure. The\nnetwork quantifiers of both structural matrices are identified inside the\nprobabilistic valley. Despite of similarities between structural connection\nmatrices and sampled small-world networks, increased assortativity can be found\nin the Alzheimer brain. Our results indicate that network quantifiers can be\nhelpful to identify abnormalities in real structural connection matrices.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:07:21 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Coninck", "Jos\u00e9 C. P.", ""], ["Ferrari", "Fabiano A. S.", ""], ["Reis", "Adriane S.", ""], ["Iarosz", "Kelly C.", ""], ["Batista", "Antonio M.", ""], ["Viana", "Ricardo L.", ""]]}, {"id": "1905.11515", "submitter": "Alex Gain", "authors": "Alex Gain, Hava Siegelmann", "title": "Abstraction Mechanisms Predict Generalization in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding problem for Deep Neural Networks (DNNs) is understanding their\npuzzling ability to generalize well. We approach this problem through the\nunconventional angle of \\textit{cognitive abstraction mechanisms}, drawing\ninspiration from recent neuroscience work, allowing us to define the Cognitive\nNeural Activation metric (CNA) for DNNs, which is the correlation between\ninformation complexity (entropy) of given input and the concentration of higher\nactivation values in deeper layers of the network. The CNA is highly predictive\nof generalization ability, outperforming norm-and-margin-based generalization\nmetrics on an extensive evaluation of over 100 dataset-and-network-architecture\ncombinations, especially in cases where additive noise is present and/or\ntraining labels are corrupted. These strong empirical results show the\nusefulness of CNA as a generalization metric, and encourage further research on\nthe connection between information complexity and representations in the deeper\nlayers of networks in order to better understand the generalization\ncapabilities of DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 21:34:34 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 02:59:25 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gain", "Alex", ""], ["Siegelmann", "Hava", ""]]}, {"id": "1905.11594", "submitter": "Yuan Zeng", "authors": "Yuan Zeng and Zubayer Ibne Ferdous and Weixiang Zhang and Mufan Xu and\n  Anlan Yu and Drew Patel and Xiaochen Guo and Yevgeny Berdichevsky and Zhiyuan\n  Yan", "title": "Inference with Hybrid Bio-hardware Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the learning process in brains, biologically plausible\nalgorithms have been explored by modeling the detailed neuron properties and\ndynamics. On the other hand, simplified multi-layer models of neural networks\nhave shown great success on computational tasks such as image classification\nand speech recognition. However, the computational models that can achieve good\naccuracy for these learning applications are very different from the\nbio-plausible models. This paper studies whether a bio-plausible model of a in\nvitro living neural network can be used to perform machine learning tasks and\nachieve good inference accuracy. A novel two-layer bio-hardware hybrid neural\nnetwork is proposed. The biological layer faithfully models variations of\nsynapses, neurons, and network sparsity in in vitro living neural networks. The\nhardware layer is a computational fully-connected layer that tunes parameters\nto optimize for accuracy. Several techniques are proposed to improve the\ninference accuracy of the proposed hybrid neural network. For instance, an\nadaptive pre-processing technique helps the proposed neural network to achieve\ngood learning accuracy for different living neural network sparsity. The\nproposed hybrid neural network with realistic neuron parameters and variations\nachieves a 98.3% testing accuracy for the handwritten digit recognition task on\nthe full MNIST dataset.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 03:38:27 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 16:06:44 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zeng", "Yuan", ""], ["Ferdous", "Zubayer Ibne", ""], ["Zhang", "Weixiang", ""], ["Xu", "Mufan", ""], ["Yu", "Anlan", ""], ["Patel", "Drew", ""], ["Guo", "Xiaochen", ""], ["Berdichevsky", "Yevgeny", ""], ["Yan", "Zhiyuan", ""]]}, {"id": "1905.11758", "submitter": "Dante Chialvo", "authors": "Dante R. Chialvo, Sergio A. Cannas, Dietmar Plenz and Tomas S. Grigera", "title": "Controlling a complex system near its critical point via temporal\n  correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of complex systems exhibit large fluctuations both in space\nand time that often can be attributed to the presence of some kind of critical\nphenomena. Under such critical scenario it is well known that the properties of\nthe correlation functions in space and time are two sides of the same coin.\nHere we test wether systems exhibiting a phase transition could self-tune to\nits critical point taking advantage of such correlation properties. We describe\nresults in three models: the 2D Ising ferromagnetic model, the 3D Vicsek\nflocking model and a small-world neuronal network model. We illustrate how the\nfeedback of the autocorrelation function of the order parameter fluctuations is\nable to shift the system towards its critical point. Since the results rely on\nuniversal properties they are expected to be relevant to a variety of other\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:02:17 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chialvo", "Dante R.", ""], ["Cannas", "Sergio A.", ""], ["Plenz", "Dietmar", ""], ["Grigera", "Tomas S.", ""]]}, {"id": "1905.11833", "submitter": "Mariya Toneva", "authors": "Mariya Toneva and Leila Wehbe", "title": "Interpreting and improving natural-language processing (in machines)\n  with natural language-processing (in the brain)", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks models for NLP are typically implemented without the explicit\nencoding of language rules and yet they are able to break one performance\nrecord after another. This has generated a lot of research interest in\ninterpreting the representations learned by these networks. We propose here a\nnovel interpretation approach that relies on the only processing system we have\nthat does understand language: the human brain. We use brain imaging recordings\nof subjects reading complex natural text to interpret word and sequence\nembeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We\nstudy how their representations differ across layer depth, context length, and\nattention type. Our results reveal differences in the context-related\nrepresentations across these models. Further, in the transformer models, we\nfind an interaction between layer depth and context length, and between layer\ndepth and attention type. We finally hypothesize that altering BERT to better\nalign with brain recordings would enable it to also better understand language.\nProbing the altered BERT using syntactic NLP tasks reveals that the model with\nincreased brain-alignment outperforms the original model. Cognitive\nneuroscientists have already begun using NLP networks to study the brain, and\nthis work closes the loop to allow the interaction between NLP and cognitive\nneuroscience to be a true cross-pollination.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:13:09 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 14:52:22 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 17:40:56 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 16:25:28 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Toneva", "Mariya", ""], ["Wehbe", "Leila", ""]]}, {"id": "1905.12100", "submitter": "Owen Marschall", "authors": "Owen Marschall, Kyunghyun Cho, Cristina Savin", "title": "Using local plasticity rules to train recurrent neural networks", "comments": "Abstract submission to Computational and Systems Neuroscience\n  (Cosyne) 2019, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn useful dynamics on long time scales, neurons must use plasticity\nrules that account for long-term, circuit-wide effects of synaptic changes. In\nother words, neural circuits must solve a credit assignment problem to\nappropriately assign responsibility for global network behavior to individual\ncircuit components. Furthermore, biological constraints demand that plasticity\nrules are spatially and temporally local; that is, synaptic changes can depend\nonly on variables accessible to the pre- and postsynaptic neurons. While\nartificial intelligence offers a computational solution for credit assignment,\nnamely backpropagation through time (BPTT), this solution is wildly\nbiologically implausible. It requires both nonlocal computations and unlimited\nmemory capacity, as any synaptic change is a complicated function of the entire\nhistory of network activity. Similar nonlocality issues plague other approaches\nsuch as FORCE (Sussillo et al. 2009). Overall, we are still missing a model for\nlearning in recurrent circuits that both works computationally and uses only\nlocal updates. Leveraging recent advances in machine learning on approximating\ngradients for BPTT, we derive biologically plausible plasticity rules that\nenable recurrent networks to accurately learn long-term dependencies in\nsequential data. The solution takes the form of neurons with segregated voltage\ncompartments, with several synaptic sub-populations that have different\nfunctional properties. The network operates in distinct phases during which\neach synaptic sub-population is updated by its own local plasticity rule. Our\nresults provide new insights into the potential roles of segregated dendritic\ncompartments, branch-specific inhibition, and global circuit phases in\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 21:32:26 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Marschall", "Owen", ""], ["Cho", "Kyunghyun", ""], ["Savin", "Cristina", ""]]}, {"id": "1905.12176", "submitter": "Eli Shlizerman", "authors": "Kun Su, Eli Shlizerman", "title": "Clustering and Recognition of Spatiotemporal Features through\n  Interpretable Embedding of Sequence to Sequence Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder recurrent neural network models (RNN Seq2Seq) have achieved\ngreat success in ubiquitous areas of computation and applications. It was shown\nto be successful in modeling data with both temporal and spatial dependencies\nfor translation or prediction tasks. In this study, we propose an embedding\napproach to visualize and interpret the representation of data by these models.\nFurthermore, we show that the embedding is an effective method for unsupervised\nlearning and can be utilized to estimate the optimality of model training. In\nparticular, we demonstrate that embedding space projections of the decoder\nstates of RNN Seq2Seq model trained on sequences prediction are organized in\nclusters capturing similarities and differences in the dynamics of these\nsequences. Such performance corresponds to an unsupervised clustering of any\nspatio-temporal features and can be employed for time-dependent problems such\nas temporal segmentation, clustering of dynamic activity, self-supervised\nclassification, action recognition, failure prediction, etc. We test and\ndemonstrate the application of the embedding methodology to time-sequences of\n3D human body poses. We show that the methodology provides a high-quality\nunsupervised categorization of movements.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 01:56:06 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 16:44:40 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Su", "Kun", ""], ["Shlizerman", "Eli", ""]]}, {"id": "1905.12223", "submitter": "Zaid Bin Mahbub", "authors": "Zaid Bin Mahbub, J H Karami, K Siddique-e Rabbani", "title": "Analysis of evoked EMG using wavelet transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph physics.med-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evoked EMG M-responses obtained from the thenar muscle in the palm by\nelectrical stimulation of the median nerve demonstrate a well-established\nsmooth bipolar shape for normal healthy subjects while kinks are observed in\ncertain neurological disorders, particularly in cervical spondylotic\nneuropathy. A first differentiation failed to identify these kinks because of\ncomparable values obtained for normally rising and falling segments of the\nsmooth regions, and due to noise. In this study, the usefulness of the wavelet\ntransform (WT), that provides localized measures of non-stationary signals is\ninvestigated. The Haar WT was used to analyze a total of 36 M-responses\nrecorded from the median nerves of 6 normal subjects (having smooth shape) and\n12 subjects with assumed neurological disorders (having kinks), for two points\nof stimulation on the same nerve. Features in the time-scale representation of\nthe M-responses were studied using WT to distinguish smooth M-responses from\nones with kinks. Variations in the coefficient line of the WT were also studied\nto allow visualization of WT at different scales (inverse of frequency). The\nhigh and low frequency regions in the WT came out distinctively which helped\nidentifications of kinks even of very subtle ones in the M-responses which were\ndifficult to obtain using the differentiated signal. In conclusion, the wavelet\nanalysis may be a technique of choice in identifying kinks in M-responses in\nrelation to time, thus enhancing the accuracy of neurological diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:34:40 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Mahbub", "Zaid Bin", ""], ["Karami", "J H", ""], ["Rabbani", "K Siddique-e", ""]]}, {"id": "1905.12375", "submitter": "Cole Hurwitz", "authors": "Cole L. Hurwitz, Kai Xu, Akash Srivastava, Alessio P. Buccino,\n  Matthias H. Hennig", "title": "Scalable Spike Source Localization in Extracellular Recordings using\n  Amortized Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the positions of neurons in an extracellular recording is useful\nfor investigating functional properties of the underlying neural circuitry. In\nthis work, we present a Bayesian modelling approach for localizing the source\nof individual spikes on high-density, microelectrode arrays. To allow for\nscalable inference, we implement our model as a variational autoencoder and\nperform amortized variational inference. We evaluate our method on both\nbiophysically realistic simulated and real extracellular datasets,\ndemonstrating that it is more accurate than and can improve spike sorting\nperformance over heuristic localization methods such as center of mass.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:30:24 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 22:01:00 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 16:28:54 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Hurwitz", "Cole L.", ""], ["Xu", "Kai", ""], ["Srivastava", "Akash", ""], ["Buccino", "Alessio P.", ""], ["Hennig", "Matthias H.", ""]]}, {"id": "1905.12382", "submitter": "Amir Hossein Ansari", "authors": "Amir Hossein Ansari, Perumpillichira Joseph Cherian, Alexander\n  Caicedo, Anneleen Dereymaeker, Katrien Jansen, Leen De Wispelaere, Charlotte\n  Dielman, Jan Vervisch, Paul Govaert, Maarten De Vos, Gunnar Naulaers, Sabine\n  Van Huffel", "title": "NeoGuard: a public, online learning platform for neonatal seizures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seizures occur in the neonatal period more frequently than other periods of\nlife and usually denote the presence of serious brain dysfunction. The gold\nstandard for detecting seizures is based on visual inspection of continuous\nelectroencephalogram (cEEG) complemented by video analysis, performed by an\nexpert clinical neurophysiologist. Previous studies have reported varying\ndegree of agreement between expert EEG readers, with kappa coefficients ranging\nfrom 0.4 to 0.85, calling into question the validity of visual scoring. This\nvariability in visual scoring of neonatal seizures may be due to factors such\nas reader expertise and the nature of expressed patterns. One of the possible\nreasons for low inter-rater agreement is the absence of any benchmark for the\nEEG readers to be able to compare their opinions. One way to develop this is to\nuse a shared multi-center neonatal seizure database and use the inputs from\nmultiple experts. This will also improve the teaching of trainees, and help to\navoid potential bias from a single expert's opinion. In this paper, we\nintroduce and explain the NeoGuard public learning platform that can be used by\ntrainees, tutors, and expert EEG readers who are interested to test their\nknowledge and learn from neonatal EEG-polygraphic segments scored by several\nexpert EEG readers. For this platform, 1919 clinically relevant segments,\ntotaling 280h, recorded from 71 term neonates in two centers, including a wide\nvariety of seizures and artifacts were used. These segments were scored by 4\nEEG readers from three different centers. Users of this platform can score an\narbitrary number of segments and then test their scoring with the experts'\nopinions. The kappa and joint probability of agreement, is then shown as\ninter-rater agreement metrics between the user and each of the experts. The\nplatform is publicly available at the NeoGuard website (www.neoguard.net).\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:45:38 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ansari", "Amir Hossein", ""], ["Cherian", "Perumpillichira Joseph", ""], ["Caicedo", "Alexander", ""], ["Dereymaeker", "Anneleen", ""], ["Jansen", "Katrien", ""], ["De Wispelaere", "Leen", ""], ["Dielman", "Charlotte", ""], ["Vervisch", "Jan", ""], ["Govaert", "Paul", ""], ["De Vos", "Maarten", ""], ["Naulaers", "Gunnar", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1905.12554", "submitter": "Giorgio Mantica", "authors": "Theophile Caby and Giorgio Mantica", "title": "Extreme value theory of evolving phenomena in complex dynamical systems:\n  firing cascades in a model of neural network", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": "10.1063/1.5120570", "report-no": null, "categories": "q-bio.NC math.DS nlin.CD physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the scope of the dynamical theory of extreme values to cover\nphenomena that do not happen instantaneously, but evolve over a finite, albeit\nunknown at the onset, time interval. We consider complex dynamical systems,\ncomposed of many individual subsystems linked by a network of interactions. As\na specific example of the general theory, a model of neural network, introduced\nto describe the electrical activity of the cerebral cortex, is analyzed in\ndetail: on the basis of this analysis we propose a novel definition of neuronal\ncascade, a physiological phenomenon of primary importance. We derive extreme\nvalue laws for the statistics of these cascades, both from the point of view of\nexceedances (that satisfy critical scaling theory) and of block maxima.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:26:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Caby", "Theophile", ""], ["Mantica", "Giorgio", ""]]}, {"id": "1905.12570", "submitter": "James Yearsley", "authors": "James M Yearsley and Jonathan J Halliwell", "title": "Contextuality in Human Decision Making in the Presence of Direct\n  Influences: A Comment on Basieva et al. (2019)", "comments": "5 pages. V2: Substantial Revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Basieva, Cervantes, Dzhafarov, and Khrennikov (2019)\npresented a series of experiments which they claimed show evidence for\ncontextuality in human judgments. This was based on a set of modified Bell-like\ninequalities designed to rule out effects caused by signalling. In this comment\nwe show that it is, however, possible to construct a non-contextual model which\nexplains the experimental data via direct influences, which we take to mean\nthat a measurement outcome has a (model-specific) causal dependence on other\nmeasurements. We trace the apparent inconsistency to a definition of signalling\nwhich does not account for all possible forms of direct influence. Further, we\ncast doubt on the idea that any experimental data in psychology could provide\nconclusive evidence for contextuality beyond that explainable by direct\ninfluence.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:19:59 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 12:16:36 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Yearsley", "James M", ""], ["Halliwell", "Jonathan J", ""]]}, {"id": "1905.12599", "submitter": "Yanlong Sun", "authors": "Hongbin Wang, Jack W. Smith, Yanlong Sun", "title": "Simulating Cognition with Quantum Computers", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are inherent limits in classical computation for it to serve as an\nadequate model of human cognition. In particular, non-commutativity, while\nubiquitous in physics and psychology, cannot be sufficiently handled. We\npropose that we need a new mathematics that is capable of expressing more\ncomplex mathematical structures to tackle those hard X-problems in cognitive\nscience. A quantum mind approach is advocated and we hypothesize a way in which\nquantum computation might be realized in the brain.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:09:07 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 22:34:59 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Wang", "Hongbin", ""], ["Smith", "Jack W.", ""], ["Sun", "Yanlong", ""]]}, {"id": "1905.12601", "submitter": "Nithin Nagaraj", "authors": "Harikrishnan N B and Nithin Nagaraj", "title": "A Novel Chaos Theory Inspired Neuronal Architecture", "comments": "6 pages, 5 figures. This is a pre-print version of the manuscript\n  which we will be submitting soon to an international conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical success of widely used machine learning (ML) and deep learning\n(DL) algorithms in Artificial Intelligence (AI) community owes to availability\nof large datasets for training and huge computational resources. Despite the\nenormous practical success of AI, these algorithms are only loosely inspired\nfrom the biological brain and do not mimic any of the fundamental properties of\nneurons in the brain, one such property being the chaotic firing of biological\nneurons. This motivates us to develop a novel neuronal architecture where the\nindividual neurons are intrinsically chaotic in nature. By making use of the\ntopological transitivity property of chaos, our neuronal network is able to\nperform classification tasks with very less number of training samples. For the\nMNIST dataset, with as low as $0.1 \\%$ of the total training data, our method\noutperforms ML and matches DL in classification accuracy for up to $7$ training\nsamples/class. For the Iris dataset, our accuracy is comparable with ML\nalgorithms, and even with just two training samples/class, we report an\naccuracy as high as $95.8 \\%$. This work highlights the effectiveness of chaos\nand its properties for learning and paves the way for chaos-inspired neuronal\narchitectures by closely mimicking the chaotic nature of neurons in the brain.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 07:45:57 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["B", "Harikrishnan N", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1905.12937", "submitter": "Jan Melchior", "authors": "Jan Melchior and Mehdi Bayati and Amir Azizi and Sen Cheng and Laurenz\n  Wiskott", "title": "A Hippocampus Model for Online One-Shot Storage of Pattern Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational model based on the CRISP theory (Content\nRepresentation, Intrinsic Sequences, and Pattern completion) of the hippocampus\nthat allows to continuously store pattern sequences online in a one-shot\nfashion. Rather than storing a sequence in CA3, CA3 provides a pre-trained\nsequence that is hetero-associated with the input sequence, which allows the\nsystem to perform one-shot learning. Plasticity on a short time scale therefore\nonly happens in the incoming and outgoing connections of CA3. Stored sequences\ncan later be recalled from a single cue pattern. We identify the pattern\nseparation performed by subregion DG to be necessary for storing sequences that\ncontain correlated patterns. A design principle of the model is that we use a\nsingle learning rule named Hebbiand-escent to train all parts of the system.\nHebbian-descent has an inherent forgetting mechanism that allows the system to\ncontinuously memorize new patterns while forgetting early stored ones. The\nmodel shows a plausible behavior when noisy and new patterns are presented and\nhas a rather high capacity of about 40% in terms of the number of neurons in\nCA3. One notable property of our model is that it is capable of\n`boot-strapping' (improving) itself without external input in a process we\nrefer to as `dreaming'. Besides artificially generated input sequences we also\nshow that the model works with sequences of encoded handwritten digits or\nnatural images. To our knowledge this is the first model of the hippocampus\nthat allows to store correlated pattern sequences online in a one-shot fashion\nwithout a consolidation process, which can instantaneously be recalled later.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 09:51:58 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Melchior", "Jan", ""], ["Bayati", "Mehdi", ""], ["Azizi", "Amir", ""], ["Cheng", "Sen", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1905.13065", "submitter": "Julien Lagarde", "authors": "Julien Lagarde and Nicolas Bouisset", "title": "When is now in a distributed system? Animated motion (could) set the\n  present in brain networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our brains are viewed as interconnected distributed systems. The connections\nbetween distant areas in the brain are significantly delayed. How to obtained\nnow in such networks with delayed interconnections? We aim to show that delayed\ncommunication and interconnectedness of the brain impose an interaction with\nthe environment, assuming that such an access to now, which we label t-present,\nis of use for this system. It is conjectured that for any sensory, motor or\ncognitive functions to work efficiently an updated sort of time origin is\nrequired, and we claim that it is uniquely given by a direct contact with the\nphysical environment. To get such contact autonomously any movement is\nrequired, be it originating in the motion of sensory systems or in goal\ndirected movements. Some limit cases are identified and discussed. Next,\nseveral testable situations are envisioned and available studies in favor of\nthe main theoretical hypothesis are shortly reviewed. Finally, as a proof of\nconcept, an experimental study employing galvanic vestibular stimulation is\npresented and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:12:15 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Lagarde", "Julien", ""], ["Bouisset", "Nicolas", ""]]}, {"id": "1905.13173", "submitter": "Roberto N. Mu\\~noz", "authors": "Roberto N. Mu\\~noz, Angus Leung, Aidan Zecevik, Felix A. Pollock, Dror\n  Cohen, Bruno van Swinderen, Naotsugu Tsuchiya, Kavan Modi", "title": "General anesthesia reduces complexity and temporal asymmetry of the\n  informational structures derived from neural recordings in Drosophila", "comments": "14 pages, 6 figures. Comments welcome; Added time-reversal analysis,\n  updated discussion, new figures (Fig. 5 & Fig. 6) and Tables (Tab. 1)", "journal-ref": "Phys. Rev. Research 2, 023219 (2020)", "doi": "10.1103/PhysRevResearch.2.023219", "report-no": null, "categories": "q-bio.NC physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply techniques from the field of computational mechanics to evaluate the\nstatistical complexity of neural recording data from fruit flies. First, we\nconnect statistical complexity to the flies' level of conscious arousal, which\nis manipulated by general anesthesia (isoflurane). We show that the complexity\nof even single channel time series data decreases under anesthesia. The\nobserved difference in complexity between the two states of conscious arousal\nincreases as higher orders of temporal correlations are taken into account. We\nthen go on to show that, in addition to reducing complexity, anesthesia also\nmodulates the informational structure between the forward- and reverse-time\nneural signals. Specifically, using three distinct notions of temporal\nasymmetry we show that anesthesia reduces temporal asymmetry on\ninformation-theoretic and information-geometric grounds. In contrast to prior\nwork, our results show that: (1) Complexity differences can emerge at very\nshort timescales and across broad regions of the fly brain, thus heralding the\nmacroscopic state of anesthesia in a previously unforeseen manner, and (2) that\ngeneral anesthesia also modulates the temporal asymmetry of neural signals.\nTogether, our results demonstrate that anesthetized brains become both less\nstructured and more reversible.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:57:48 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 06:22:56 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 01:57:40 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Mu\u00f1oz", "Roberto N.", ""], ["Leung", "Angus", ""], ["Zecevik", "Aidan", ""], ["Pollock", "Felix A.", ""], ["Cohen", "Dror", ""], ["van Swinderen", "Bruno", ""], ["Tsuchiya", "Naotsugu", ""], ["Modi", "Kavan", ""]]}, {"id": "1905.13225", "submitter": "Ismael Tito Freire Gonz\\'alez", "authors": "Ismael T. Freire, Xerxes D. Arsiwalla, Jordi-Ysard Puigb\\`o, Paul\n  Verschure", "title": "Modeling Theory of Mind in Multi-Agent Games Using Adaptive Feedback\n  Control", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in cognitive science and AI has been to understand how\nautonomous agents might acquire and predict behavioral and mental states of\nother agents in the course of complex social interactions. How does such an\nagent model the goals, beliefs, and actions of other agents it interacts with?\nWhat are the computational principles to model a Theory of Mind (ToM)? Deep\nlearning approaches to address these questions fall short of a better\nunderstanding of the problem. In part, this is due to the black-box nature of\ndeep networks, wherein computational mechanisms of ToM are not readily\nrevealed. Here, we consider alternative hypotheses seeking to model how the\nbrain might realize a ToM. In particular, we propose embodied and situated\nagent models based on distributed adaptive control theory to predict actions of\nother agents in five different game theoretic tasks (Harmony Game, Hawk-Dove,\nStag-Hunt, Prisoner's Dilemma and Battle of the Exes). Our multi-layer control\nmodels implement top-down predictions from adaptive to reactive layers of\ncontrol and bottom-up error feedback from reactive to adaptive layers. We test\ncooperative and competitive strategies among seven different agent models\n(cooperative, greedy, tit-for-tat, reinforcement-based, rational, predictive\nand other's-model agents). We show that, compared to pure reinforcement-based\nstrategies, probabilistic learning agents modeled on rational, predictive and\nother's-model phenotypes perform better in game-theoretic metrics across tasks.\nOur autonomous multi-agent models capture systems-level processes underlying a\nToM and highlight architectural principles of ToM from a control-theoretic\nperspective.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 18:26:26 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Freire", "Ismael T.", ""], ["Arsiwalla", "Xerxes D.", ""], ["Puigb\u00f2", "Jordi-Ysard", ""], ["Verschure", "Paul", ""]]}, {"id": "1905.13424", "submitter": "Bruno. Cessac", "authors": "B. Cessac", "title": "Linear response in neuronal networks: from neurons dynamics to\n  collective response", "comments": "23 pages, 17 figures, to appear", "journal-ref": null, "doi": "10.1063/1.5111803", "report-no": null, "categories": "q-bio.NC nlin.CD physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review two examples where the linear response of a neuronal network\nsubmitted to an external stimulus can be derived explicitely, including network\nparameters dependence. This is done in a statistical physics-like approach\nwhere one associates to the spontaneous dynamics of the model a natural notion\nof Gibbs distribution inherited from ergodic theory or stochastic processes.\nThese two examples are the Amari-Wilson-Cowan model and a conductance based\nIntegrate and Fire model.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 05:49:15 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 11:10:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Cessac", "B.", ""]]}]