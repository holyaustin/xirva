[{"id": "1001.0036", "submitter": "Cosma Rohilla Shalizi", "authors": "Robert Haslinger, Kristina Lisa Klinkner, and Cosma Rohilla Shalizi", "title": "The Computational Structure of Spike Trains", "comments": "Somewhat different format from journal version but same content", "journal-ref": "Neural Computation, vol. 22 (2010), pp. 121--157", "doi": "10.1162/neco.2009.12-07-678", "report-no": null, "categories": "q-bio.NC cs.IT math.IT nlin.AO physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons perform computations, and convey the results of those computations\nthrough the statistical structure of their output spike trains. Here we present\na practical method, grounded in the information-theoretic analysis of\nprediction, for inferring a minimal representation of that structure and for\ncharacterizing its complexity. Starting from spike trains, our approach finds\ntheir causal state models (CSMs), the minimal hidden Markov models or\nstochastic automata capable of generating statistically identical time series.\nWe then use these CSMs to objectively quantify both the generalizable structure\nand the idiosyncratic randomness of the spike train. Specifically, we show that\nthe expected algorithmic information content (the information needed to\ndescribe the spike train exactly) can be split into three parts describing (1)\nthe time-invariant structure (complexity) of the minimal spike-generating\nprocess, which describes the spike train statistically; (2) the randomness\n(internal entropy rate) of the minimal spike-generating process; and (3) a\nresidual pure noise term not described by the minimal spike-generating process.\nWe use CSMs to approximate each of these quantities. The CSMs are inferred\nnonparametrically from the data, making only mild regularity assumptions, via\nthe causal state splitting reconstruction algorithm. The methods presented here\ncomplement more traditional spike train analyses by describing not only spiking\nprobability and spike train entropy, but also the complexity of a spike train's\nstructure. We demonstrate our approach using both simulated spike trains and\nexperimental data recorded in rat barrel cortex during vibrissa stimulation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 22:12:20 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Haslinger", "Robert", ""], ["Klinkner", "Kristina Lisa", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1001.0446", "submitter": "Benjamin Torben-Nielsen", "authors": "Benjamin Torben-Nielsen and Marylka Uusisaari and Klaus M. Stiefel", "title": "A novel method for determining the phase-response curves of neurons\n  based on minimizing spike-time prediction error", "comments": "PDFLatex 7 A4 pages, 4 figures. New method to estimate the neuronal\n  phase-response curve", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular firing neurons can be seen as oscillators. The phase-response curve\n(PRC) describes how such neurons will respond to small excitatory\nperturbations. Knowledge of the PRC is important as it is associated to the\nexcitability type of neurons and their capability to synchronize in networks.\nIn this work we present a novel method to estimate the PRC from experimental\ndata. We assume that continuous noise signal can be discretized into\nindependent perturbations at evenly spaced phases and predict the next spike\nbased on these independent perturbations. The difference between the predicted\nnext spike made at every discretized phase and the actual next spike time is\nused as the error signal used to optimize the PRC. We test our method on model\ndata and experimentally obtained data and find that the newly developed method\nis robust and reliable method for the estimation of PRCs from experimental\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 10:54:46 GMT"}], "update_date": "2010-01-05", "authors_parsed": [["Torben-Nielsen", "Benjamin", ""], ["Uusisaari", "Marylka", ""], ["Stiefel", "Klaus M.", ""]]}, {"id": "1001.0647", "submitter": "Benjamin Torben-Nielsen", "authors": "Benjamin Torben-Nielsen, Marylka Uusisaari, Klaus M. Stiefel", "title": "A comparison of methods to determine neuronal phase-response curves", "comments": "PDFLatex, 16 pages, 7 figures.", "journal-ref": "Front. Neuroinform. 4:6", "doi": "10.3389/fninf.2010.00006", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phase-response curve (PRC) is an important tool to determine the\nexcitability type of single neurons which reveals consequences for their\nsynchronizing properties. We review five methods to compute the PRC from both\nmodel data and experimental data and compare the numerically obtained results\nfrom each method. The main difference between the methods lies in the\nreliability which is influenced by the fluctuations in the spiking data and the\nnumber of spikes available for analysis. We discuss the significance of our\nresults and provide guidelines to choose the best method based on the available\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 08:19:36 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2010 17:26:36 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2010 14:10:03 GMT"}], "update_date": "2010-03-29", "authors_parsed": [["Torben-Nielsen", "Benjamin", ""], ["Uusisaari", "Marylka", ""], ["Stiefel", "Klaus M.", ""]]}, {"id": "1001.0663", "submitter": "Dimitrije Markovic", "authors": "Dimitrije Markovic, Claudius Gros", "title": "Self-organized chaos through polyhomeostatic optimization", "comments": null, "journal-ref": "Phys. Rev. Lett. 105, 068702 (2010)", "doi": "10.1103/PhysRevLett.105.068702", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of polyhomeostatic control is to achieve a certain target\ndistribution of behaviors, in contrast to polyhomeostatic regulation which aims\nat stabilizing a steady-state dynamical state. We consider polyhomeostasis for\nindividual and networks of firing-rate neurons, adapting to achieve target\ndistributions of firing rates maximizing information entropy. We show that any\nfinite polyhomeostatic adaption rate destroys all attractors in Hopfield-like\nnetwork setups, leading to intermittently bursting behavior and self-organized\nchaos. The importance of polyhomeostasis to adapting behavior in general is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 09:42:01 GMT"}, {"version": "v2", "created": "Fri, 28 May 2010 15:49:50 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Markovic", "Dimitrije", ""], ["Gros", "Claudius", ""]]}, {"id": "1001.1399", "submitter": "Liane Gabora", "authors": "Liane Gabora and Diederik Aerts", "title": "A model of the emergence and evolution of integrated worldviews", "comments": null, "journal-ref": "Gabora, L. & Aerts, D. (2009). A model of the emergence and\n  evolution of integrated worldviews. Journal of Mathematical Psychology, 53,\n  434-451", "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proposed that the ability of humans to flourish in diverse environments\nand evolve complex cultures reflects the following two underlying cognitive\ntransitions. The transition from the coarse-grained associative memory of Homo\nhabilis to the fine-grained memory of Homo erectus enabled limited\nrepresentational redescription of perceptually similar episodes, abstraction,\nand analytic thought, the last of which is modeled as the formation of states\nand of lattices of properties and contexts for concepts. The transition to the\nmodern mind of Homo sapiens is proposed to have resulted from onset of the\ncapacity to spontaneously and temporarily shift to an associative mode of\nthought conducive to interaction amongst seemingly disparate concepts, modeled\nas the forging of conjunctions resulting in states of entanglement. The fruits\nof associative thought became ingredients for analytic thought, and vice versa.\nThe ratio of associative pathways to concepts surpassed a percolation threshold\nresulting in the emergence of a self-modifying, integrated internal model of\nthe world, or worldview.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2010 04:27:17 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Gabora", "Liane", ""], ["Aerts", "Diederik", ""]]}, {"id": "1001.1401", "submitter": "Liane Gabora", "authors": "Steve DiPaola and Liane Gabora", "title": "Incorporating characteristics of human creativity into an evolutionary\n  art algorithm", "comments": null, "journal-ref": "Genetic Programming and Evolvable Machines, 10(2), 97-110 (2009)", "doi": "10.1007/s10710-008-9074-x", "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A perceived limitation of evolutionary art and design algorithms is that they\nrely on human intervention; the artist selects the most aesthetically pleasing\nvariants of one generation to produce the next. This paper discusses how\ncomputer generated art and design can become more creatively human-like with\nrespect to both process and outcome. As an example of a step in this direction,\nwe present an algorithm that overcomes the above limitation by employing an\nautomatic fitness function. The goal is to evolve abstract portraits of Darwin,\nusing our 2nd generation fitness function which rewards genomes that not just\nproduce a likeness of Darwin but exhibit certain strategies characteristic of\nhuman artists. We note that in human creativity, change is less choosing\namongst randomly generated variants and more capitalizing on the associative\nstructure of a conceptual network to hone in on a vision. We discuss how to\nachieve this fluidity algorithmically.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2010 04:36:24 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 00:46:46 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 18:54:45 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["DiPaola", "Steve", ""], ["Gabora", "Liane", ""]]}, {"id": "1001.2474", "submitter": "Michele Thieullen", "authors": "K. Pakdaman, M. Thieullen, G. Wainrib", "title": "Fluid limit theorems for stochastic hybrid systems with application to\n  neuron models", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes limit theorems for a class of stochastic hybrid\nsystems (continuous deterministic dynamic coupled with jump Markov processes)\nin the fluid limit (small jumps at high frequency), thus extending known\nresults for jump Markov processes. We prove a functional law of large numbers\nwith exponential convergence speed, derive a diffusion approximation and\nestablish a functional central limit theorem. We apply these results to neuron\nmodels with stochastic ion channels, as the number of channels goes to\ninfinity, estimating the convergence to the deterministic model. In terms of\nneural coding, we apply our central limit theorems to estimate numerically\nimpact of channel noise both on frequency and spike timing coding.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2010 14:43:56 GMT"}], "update_date": "2010-01-15", "authors_parsed": [["Pakdaman", "K.", ""], ["Thieullen", "M.", ""], ["Wainrib", "G.", ""]]}, {"id": "1001.3246", "submitter": "George F. R. Ellis", "authors": "Leendert A. Remmelzwaal, Jonathan Tapson, George F. R. Ellis", "title": "Salience-Affected Neural Networks", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple neural network model which combines a locally-connected\nfeedforward structure, as is traditionally used to model inter-neuron\nconnectivity, with a layer of undifferentiated connections which model the\ndiffuse projections from the human limbic system to the cortex. This new layer\nmakes it possible to model global effects such as salience, at the same time as\nthe local network processes task-specific or local information. This simple\ncombination network displays interactions between salience and regular\nprocessing which correspond to known effects in the developing brain, such as\nenhanced learning as a result of heightened affect.\n  The cortex biases neuronal responses to affect both learning and memory,\nthrough the use of diffuse projections from the limbic system to the cortex.\nStandard ANNs do not model this non-local flow of information represented by\nthe ascending systems, which are a significant feature of the structure of the\nbrain, and although they do allow associational learning with multiple-trial,\nthey simply don't provide the capacity for one-time learning.\n  In this research we model this effect using an artificial neural network\n(ANN), creating a salience-affected neural network (SANN). We adapt an ANN to\nembody the capacity to respond to an input salience signal and to produce a\nreverse salience signal during testing.\n  This research demonstrates that input combinations similar to the inputs in\nthe training data sets will produce similar reverse salience signals during\ntesting. Furthermore, this research has uncovered a novel method for training\nANNs with a single training iteration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 10:28:11 GMT"}], "update_date": "2010-01-21", "authors_parsed": [["Remmelzwaal", "Leendert A.", ""], ["Tapson", "Jonathan", ""], ["Ellis", "George F. R.", ""]]}, {"id": "1001.3256", "submitter": "Joaquin Torres", "authors": "Juan A. Bonachela, Sebastiano de Franciscis, Joaquin J. Torres, and\n  Miguel A. Munoz", "title": "Self-organization without conservation: Are neuronal avalanches\n  generically critical?", "comments": "28 pages, 11 figures, regular paper", "journal-ref": null, "doi": "10.1088/1742-5468/2010/02/P02015", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experiments on cortical neural networks have revealed the existence of\nwell-defined avalanches of electrical activity. Such avalanches have been\nclaimed to be generically scale-invariant -- i.e. power-law distributed -- with\nmany exciting implications in Neuroscience. Recently, a self-organized model\nhas been proposed by Levina, Herrmann and Geisel to justify such an empirical\nfinding. Given that (i) neural dynamics is dissipative and (ii) there is a\nloading mechanism \"charging\" progressively the background synaptic strength,\nthis model/dynamics is very similar in spirit to forest-fire and earthquake\nmodels, archetypical examples of non-conserving self-organization, which have\nbeen recently shown to lack true criticality. Here we show that cortical neural\nnetworks obeying (i) and (ii) are not generically critical; unless parameters\nare fine tuned, their dynamics is either sub- or super-critical, even if the\npseudo-critical region is relatively broad. This conclusion seems to be in\nagreement with the most recent experimental observations. The main implication\nof our work is that, if future experimental research on cortical networks were\nto support that truly critical avalanches are the norm and not the exception,\nthen one should look for more elaborate (adaptive/evolutionary) explanations,\nbeyond simple self-organization, to account for this.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 10:51:31 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bonachela", "Juan A.", ""], ["de Franciscis", "Sebastiano", ""], ["Torres", "Joaquin J.", ""], ["Munoz", "Miguel A.", ""]]}, {"id": "1001.3872", "submitter": "Jonathan Touboul", "authors": "Jonathan Touboul, Bard Ermentrout, Olivier Faugeras, Bruno Cessac", "title": "Stochastic firing rate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a recent approach to the mean-field limits in neural networks that\ntakes into account the stochastic nature of input current and the uncertainty\nin synaptic coupling. This approach was proved to be a rigorous limit of the\nnetwork equations in a general setting, and we express here the results in a\nmore customary and simpler framework. We propose a heuristic argument to derive\nthese equations providing a more intuitive understanding of their origin. These\nequations are characterized by a strong coupling between the different moments\nof the solutions. We analyse the equations, present an algorithm to simulate\nthe solutions of these mean-field equations, and investigate numerically the\nequations. In particular, we build a bridge between these equations and\nSompolinsky and collaborators approach (1988, 1990), and show how the coupling\nbetween the mean and the covariance function deviates from customary\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2010 20:27:01 GMT"}], "update_date": "2010-01-22", "authors_parsed": [["Touboul", "Jonathan", ""], ["Ermentrout", "Bard", ""], ["Faugeras", "Olivier", ""], ["Cessac", "Bruno", ""]]}, {"id": "1001.4708", "submitter": "Armando Bazzani", "authors": "Armndop Bazzani, Gastone C. Castellani, Leon N. Cooper", "title": "Eigenvalue Distributions for a Class of Covariance Matrices with\n  Applications to Bienenstock-Cooper-Munro Neurons Under Noisy Conditions", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.81.051917", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the effects of noise correlations in the input to, or among, BCM\nneurons using the Wigner semicircular law to construct random,\npositive-definite symmetric correlation matrices and compute their eigenvalue\ndistributions. In the finite dimensional case, we compare our analytic results\nwith numerical simulations and show the effects of correlations on the\nlifetimes of synaptic strengths in various visual environments. These\ncorrelations can be due either to correlations in the noise from the input LGN\nneurons, or correlations in the variability of lateral connections in a network\nof neurons. In particular, we find that for fixed dimensionality, a large noise\nvariance can give rise to long lifetimes of synaptic strengths. This may be of\nphysiological significance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2010 14:57:38 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bazzani", "Armndop", ""], ["Castellani", "Gastone C.", ""], ["Cooper", "Leon N.", ""]]}, {"id": "1001.4845", "submitter": "Sitabhra Sinha", "authors": "Sitabhra Sinha, T Jesan and Nivedita Chatterjee", "title": "Systems biology: From the cell to the brain", "comments": "6 pages, 5 figures", "journal-ref": "Current Trends in Science: Platinum Jubilee Special (Ed. N\n  Mukunda), Bangalore: Indian Academy of Sciences, 2009, pp 199-205", "doi": null, "report-no": null, "categories": "q-bio.MN physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the completion of human genome mapping, the focus of scientists seeking\nto explain the biological complexity of living systems is shifting from\nanalyzing the individual components (such as a particular gene or biochemical\nreaction) to understanding the set of interactions amongst the large number of\ncomponents that results in the different functions of the organism. To this\nend, the area of systems biology attempts to achieve a \"systems-level\"\ndescription of biology by focusing on the network of interactions instead of\nthe characteristics of its isolated parts. In this article, we briefly describe\nsome of the emerging themes of research in \"network\" biology, looking at\ndynamical processes occurring at the two different length scales of within the\ncell and between cells, viz., the intra-cellular signaling network and the\nnervous system. We show that focusing on the systems-level aspects of these\nproblems allows one to observe surprising and illuminating common themes\namongst them.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2010 03:35:20 GMT"}], "update_date": "2010-01-28", "authors_parsed": [["Sinha", "Sitabhra", ""], ["Jesan", "T", ""], ["Chatterjee", "Nivedita", ""]]}, {"id": "1001.5420", "submitter": "Dhagash Mehta", "authors": "William Hanan, Dhagash Mehta, Guillaume Moroz, Sepanda Pouryahya", "title": "Stability and Bifurcation Analysis of Coupled Fitzhugh-Nagumo\n  Oscillators", "comments": "\"Extended abstract\" published in the Joint Conference of ASCM2009 and\n  MACIS2009, Japan, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SC nlin.CD q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons are the central biological objects in understanding how the brain\nworks. The famous Hodgkin-Huxley model, which describes how action potentials\nof a neuron are initiated and propagated, consists of four coupled nonlinear\ndifferential equations. Because these equations are difficult to deal with,\nthere also exist several simplified models, of which many exhibit\npolynomial-like non-linearity. Examples of such models are the Fitzhugh-Nagumo\n(FHN) model, the Hindmarsh-Rose (HR) model, the Morris-Lecar (ML) model and the\nIzhikevich model. In this work, we first prescribe the biologically relevant\nparameter ranges for the FHN model and subsequently study the dynamical\nbehaviour of coupled neurons on small networks of two or three nodes. To do\nthis, we use a computational real algebraic geometry method called the\nDiscriminant Variety (DV) method to perform the stability and bifurcation\nanalysis of these small networks. A time series analysis of the FHN model can\nbe found elsewhere in related work[15].\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2010 15:25:26 GMT"}], "update_date": "2010-02-01", "authors_parsed": [["Hanan", "William", ""], ["Mehta", "Dhagash", ""], ["Moroz", "Guillaume", ""], ["Pouryahya", "Sepanda", ""]]}]