[{"id": "2006.00999", "submitter": "Mihaela Duta", "authors": "Mihaela Duta and Kim Plunkett", "title": "A Neural Network Model of Lexical Competition during Infant Spoken Word\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual world studies show that upon hearing a word in a target-absent visual\ncontext containing related and unrelated items, toddlers and adults briefly\ndirect their gaze towards phonologically related items, before shifting towards\nsemantically and visually related ones. We present a neural network model that\nprocesses dynamic unfolding phonological representations and maps them to\nstatic internal semantic and visual representations. The model, trained on\nrepresentations derived from real corpora, simulates this early phonological\nover semantic/visual preference. Our results support the hypothesis that\nincremental unfolding of a spoken word is in itself sufficient to account for\nthe transient preference for phonological competitors over both unrelated and\nsemantically and visually related ones. Phonological representations mapped\ndynamically in a bottom-up fashion to semantic-visual representations capture\nthe early phonological preference effects reported in a visual world task. The\nsemantic-visual preference observed later in such a trial does not require\ntop-down feedback from a semantic or visual system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:04:11 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Duta", "Mihaela", ""], ["Plunkett", "Kim", ""]]}, {"id": "2006.01001", "submitter": "Guangyu Robert Yang", "authors": "Guangyu Robert Yang, Xiao-Jing Wang", "title": "Artificial neural networks for neuroscientists: A primer", "comments": null, "journal-ref": "Neuron, Volume 107, Issue 6, 23 September 2020, Pages 1048-1070", "doi": "10.1016/j.neuron.2020.09.005", "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks (ANNs) are essential tools in machine learning\nthat have drawn increasing attention in neuroscience. Besides offering powerful\ntechniques for data analysis, ANNs provide a new approach for neuroscientists\nto build models for complex behaviors, heterogeneous neural activity and\ncircuit connectivity, as well as to explore optimization in neural systems, in\nways that traditional models are not designed for. In this pedagogical Primer,\nwe introduce ANNs and demonstrate how they have been fruitfully deployed to\nstudy neuroscientific questions. We first discuss basic concepts and methods of\nANNs. Then, with a focus on bringing this mathematical framework closer to\nneurobiology, we detail how to customize the analysis, structure, and learning\nof ANNs to better address a wide range of challenges in brain research. To help\nthe readers garner hands-on experience, this Primer is accompanied with\ntutorial-style code in PyTorch and Jupyter Notebook, covering major topics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:08:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 16:15:27 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Yang", "Guangyu Robert", ""], ["Wang", "Xiao-Jing", ""]]}, {"id": "2006.01666", "submitter": "Isabel Gonzalo Fonrodona", "authors": "Isabel Gonzalo-Fonrodona and Miguel A. Porras", "title": "Nervous Excitability dynamics in a multisensory syndrome and its\n  similitude with a normal state. Scaling Laws", "comments": "16 pages and 16 figures. A version of this article was published in\n  the reference indicated. arXiv admin note: text overlap with arXiv:0808.1135", "journal-ref": "In: Horizons in Neuroscience Research, Volume 13, Chap. 10, A.\n  Costa and E.Villalba (Eds), 2014 Nova Sci. Publish., Inc., pp 161-189", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of increased number of works published on multisensory and\ncross-modal effects, we review a cortical multisensory syndrome (called central\nsyndrome) associated with a unilateral parieto-occipital lesion in a rather\nunspecific (or multisensory) zone of the cortex.\n  The patients with this syndrome suffered from bilateral and symmetric\nmultisensory disorders dependent on the extent of nervous mass lost and the\nintensity of the stimulus. They also presented cross-modal effects. A key point\nis the similitude of this syndrome with a normal state, since this syndrome\nwould be the result of a scale reduction in brain excitability. The first\nqualities lost when the nervous excitation diminishes are the most complex\nones, following allometric laws proper of a dynamic system.\n  The inverted perception (visual, tactile, auditive) in this syndrome is\ncompared to other cases of visual inversion reported in the literature. We\nfocus on the capability of improving perception by intensifying the stimulus or\nby means of another type of stimulus (cross-modal), muscular effort being one\nof the most efficient and least known means. This capability is greater when\nnervous excitability deficit (lesion) is greater and when the primary stimulus\nis weaker. Thus, in a normal subject, this capability is much weaker although\nperceptible for functions with high excitability demand. We also review the\nproposed scheme of functional cortical gradients whereby the specificity of the\ncortex is distributed with a continuous variation leading to a brain dynamics\nmodel accounting for multisensory or cross-modal interactions. Perception data\n(including cross-modal effects) in this syndrome are fitted using Stevens'\npower law which we relate to the allometric scaling power laws dependent on the\nactive neural mass, which seem to be the laws governing many biological neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:55:42 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Gonzalo-Fonrodona", "Isabel", ""], ["Porras", "Miguel A.", ""]]}, {"id": "2006.01745", "submitter": "Max Ortiz-Catalan", "authors": "Victoria Ashley Lang, Torbj\\\"orn Lundh, and Max Ortiz-Catalan", "title": "Mathematical models for pain: a systematic review", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no single prevailing theory of pain that explains its origin,\nqualities, and alleviation. Although many studies have investigated various\nmolecular targets for pain management, few have attempted to examine the\netiology or working mechanisms of pain through mathematical or computational\ntechniques. In this systematic review, we identified mathematical and\ncomputational approaches for characterizing pain. The databases queried were\nScience Direct and PubMed, yielding 560 articles published prior to January\n1st, 2020. After screening for inclusion of mathematical or computational\nmodels of pain, 31 articles were deemed relevant. Most of the reviewed articles\nutilized classification algorithms to categorize pain and no-pain conditions.\nWe found the literature heavily focused on the application of existing models\nor machine learning algorithms to identify the presence or absence of pain,\nrather than to explore features of pain that may be used for diagnostics and\ntreatment. Although understudied, the development of mathematical models may\naugment the current understanding of pain by providing directions for testable\nhypotheses of its underlying mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 16:16:21 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Lang", "Victoria Ashley", ""], ["Lundh", "Torbj\u00f6rn", ""], ["Ortiz-Catalan", "Max", ""]]}, {"id": "2006.01764", "submitter": "Federico Battiston", "authors": "Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora,\n  Maxime Lucas, Alice Patania, Jean-Gabriel Young, Giovanni Petri", "title": "Networks beyond pairwise interactions: structure and dynamics", "comments": "Accepted for publication in Physics Reports. 109 pages, 47 figures", "journal-ref": "Physics Reports 874, 1 (2020)", "doi": "10.1016/j.physrep.2020.05.004", "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.SI nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of many biological, social and technological systems stems\nfrom the richness of the interactions among their units. Over the past decades,\na great variety of complex systems has been successfully described as networks\nwhose interacting pairs of nodes are connected by links. Yet, in face-to-face\nhuman communication, chemical reactions and ecological systems, interactions\ncan occur in groups of three or more nodes and cannot be simply described just\nin terms of simple dyads. Until recently, little attention has been devoted to\nthe higher-order architecture of real complex systems. However, a mounting body\nof evidence is showing that taking the higher-order structure of these systems\ninto account can greatly enhance our modeling capacities and help us to\nunderstand and predict their emerging dynamical behaviors. Here, we present a\ncomplete overview of the emerging field of networks beyond pairwise\ninteractions. We first discuss the methods to represent higher-order\ninteractions and give a unified presentation of the different frameworks used\nto describe higher-order systems, highlighting the links between the existing\nconcepts and representations. We review the measures designed to characterize\nthe structure of these systems and the models proposed in the literature to\ngenerate synthetic structures, such as random and growing simplicial complexes,\nbipartite graphs and hypergraphs. We introduce and discuss the rapidly growing\nresearch on higher-order dynamical systems and on dynamical topology. We focus\non novel emergent phenomena characterizing landmark dynamical processes, such\nas diffusion, spreading, synchronization and games, when extended beyond\npairwise interactions. We elucidate the relations between higher-order topology\nand dynamical properties, and conclude with a summary of empirical\napplications, providing an outlook on current modeling and conceptual\nfrontiers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 16:44:32 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Battiston", "Federico", ""], ["Cencetti", "Giulia", ""], ["Iacopini", "Iacopo", ""], ["Latora", "Vito", ""], ["Lucas", "Maxime", ""], ["Patania", "Alice", ""], ["Young", "Jean-Gabriel", ""], ["Petri", "Giovanni", ""]]}, {"id": "2006.01923", "submitter": "Vadim Zotev", "authors": "Vadim Zotev and Jerzy Bodurka", "title": "Effects of simultaneous real-time fMRI and EEG neurofeedback in major\n  depressive disorder evaluated with brain electromagnetic tomography", "comments": "15 pages, 9 figures, to appear in NeuroImage: Clinical", "journal-ref": "NeuroImage: Clinical 28 (2020) 102459", "doi": "10.1016/j.nicl.2020.102459", "report-no": null, "categories": "q-bio.NC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we reported an emotion self-regulation study (Zotev et al., 2020),\nin which patients with major depressive disorder (MDD) used simultaneous\nreal-time fMRI and EEG neurofeedback (rtfMRI-EEG-nf) to upregulate two fMRI and\ntwo EEG activity measures, relevant to MDD. The target measures included fMRI\nactivities of the left amygdala and left rostral anterior cingulate cortex, and\nfrontal EEG asymmetries in the alpha band (FAA) and high-beta band (FBA). Here\nwe apply the exact low resolution brain electromagnetic tomography (eLORETA) to\ninvestigate EEG source activities during the rtfMRI-EEG-nf procedure. The\nexploratory analyses reveal significant changes in hemispheric lateralities of\nupper alpha and high-beta current source densities in the prefrontal regions,\nconsistent with upregulation of the FAA and FBA during the rtfMRI-EEG-nf task.\nSimilar laterality changes are observed for current source densities in the\namygdala. Prefrontal upper alpha current density changes show significant\nnegative correlations with anhedonia severity. Changes in prefrontal high-beta\ncurrent density are consistent with reduction in comorbid anxiety. Comparisons\nwith results of previous LORETA studies suggest that the rtfMRI-EEG-nf training\nis beneficial to MDD patients, and may have the ability to correct functional\ndeficiencies associated with anhedonia and comorbid anxiety in MDD.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:13:31 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 20:32:59 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zotev", "Vadim", ""], ["Bodurka", "Jerzy", ""]]}, {"id": "2006.02359", "submitter": "Simon DeDeo", "authors": "Zachary Wojtowicz and Simon DeDeo", "title": "From Probability to Consilience: How Explanatory Values Implement\n  Bayesian Reasoning", "comments": "19 pages, 1 figure, comments welcome", "journal-ref": "Trends in Cognitive Sciences (2020)", "doi": "10.1016/j.tics.2020.09.013", "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in cognitive science has uncovered a diversity of explanatory\nvalues, or dimensions along which we judge explanations as better or worse. We\npropose a Bayesian account of how these values fit together to guide\nexplanation. The resulting taxonomy provides a set of predictors for which\nexplanations people prefer and shows how core values from psychology,\nstatistics, and the philosophy of science emerge from a common mathematical\nframework. In addition to operationalizing the explanatory virtues associated\nwith, for example, scientific argument-making, this framework also enables us\nto reinterpret the explanatory vices that drive conspiracy theories, delusions,\nand extremist ideologies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:11:45 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wojtowicz", "Zachary", ""], ["DeDeo", "Simon", ""]]}, {"id": "2006.02427", "submitter": "Rainer Engelken", "authors": "Rainer Engelken, Fred Wolf, L.F. Abbott", "title": "Lyapunov spectra of chaotic recurrent neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brains process information through the collective dynamics of large neural\nnetworks. Collective chaos was suggested to underlie the complex ongoing\ndynamics observed in cerebral cortical circuits and determine the impact and\nprocessing of incoming information streams. In dissipative systems, chaotic\ndynamics takes place on a subset of phase space of reduced dimensionality and\nis organized by a complex tangle of stable, neutral and unstable manifolds. Key\ntopological invariants of this phase space structure such as attractor\ndimension, and Kolmogorov-Sinai entropy so far remained elusive.\n  Here we calculate the complete Lyapunov spectrum of recurrent neural\nnetworks. We show that chaos in these networks is extensive with a\nsize-invariant Lyapunov spectrum and characterized by attractor dimensions much\nsmaller than the number of phase space dimensions. We find that near the onset\nof chaos, for very intense chaos, and discrete-time dynamics, random matrix\ntheory provides analytical approximations to the full Lyapunov spectrum. We\nshow that a generalized time-reversal symmetry of the network dynamics induces\na point-symmetry of the Lyapunov spectrum reminiscent of the symplectic\nstructure of chaotic Hamiltonian systems. Fluctuating input reduces both the\nentropy rate and the attractor dimension. For trained recurrent networks, we\nfind that Lyapunov spectrum analysis provides a quantification of error\npropagation and stability achieved. Our methods apply to systems of arbitrary\nconnectivity, and we describe a comprehensive set of controls for the accuracy\nand convergence of Lyapunov exponents.\n  Our results open a novel avenue for characterizing the complex dynamics of\nrecurrent neural networks and the geometry of the corresponding chaotic\nattractors. They also highlight the potential of Lyapunov spectrum analysis as\na diagnostic for machine learning applications of recurrent networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:55:43 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Engelken", "Rainer", ""], ["Wolf", "Fred", ""], ["Abbott", "L. F.", ""]]}, {"id": "2006.02741", "submitter": "Ines Hipolito", "authors": "Ines Hipolito, Maxwell Ramstead, Laura Convertino, Anjali Bhat, Karl\n  Friston, Thomas Parr", "title": "Markov Blankets in the Brain", "comments": "25 pages, 5 figures, 1 table, Glossary", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent characterisations of self-organising systems depend upon the presence\nof a Markov blanket: a statistical boundary that mediates the interactions\nbetween what is inside of and outside of a system. We leverage this idea to\nprovide an analysis of partitions in neuronal systems. This is applicable to\nbrain architectures at multiple scales, enabling partitions into single\nneurons, brain regions, and brain-wide networks. This treatment is based upon\nthe canonical micro-circuitry used in empirical studies of effective\nconnectivity, so as to speak directly to practical applications. This depends\nupon the dynamic coupling between functional units, whose form recapitulates\nthat of a Markov blanket at each level. The nuance afforded by partitioning\nneural systems in this way highlights certain limitations of modular\nperspectives of brain function that only consider a single level of\ndescription.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:03:31 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Hipolito", "Ines", ""], ["Ramstead", "Maxwell", ""], ["Convertino", "Laura", ""], ["Bhat", "Anjali", ""], ["Friston", "Karl", ""], ["Parr", "Thomas", ""]]}, {"id": "2006.02949", "submitter": "Dale Zhou", "authors": "Dale Zhou, David M. Lydon-Staley, Perry Zurn, Danielle S. Bassett", "title": "The growth and form of knowledge networks by kinesthetic curiosity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Throughout life, we might seek a calling, companions, skills, entertainment,\ntruth, self-knowledge, beauty, and edification. The practice of curiosity can\nbe viewed as an extended and open-ended search for valuable information with\nhidden identity and location in a complex space of interconnected information.\nDespite its importance, curiosity has been challenging to computationally model\nbecause the practice of curiosity often flourishes without specific goals,\nexternal reward, or immediate feedback. Here, we show how network science,\nstatistical physics, and philosophy can be integrated into an approach that\ncoheres with and expands the psychological taxonomies of specific-diversive and\nperceptual-epistemic curiosity. Using this interdisciplinary approach, we\ndistill functional modes of curious information seeking as searching movements\nin information space. The kinesthetic model of curiosity offers a vibrant\ncounterpart to the deliberative predictions of model-based reinforcement\nlearning. In doing so, this model unearths new computational opportunities for\nidentifying what makes curiosity curious.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:30:41 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Zhou", "Dale", ""], ["Lydon-Staley", "David M.", ""], ["Zurn", "Perry", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2006.03226", "submitter": "Yujie Wu", "authors": "Yujie Wu, Rong Zhao, Jun Zhu, Feng Chen, Mingkun Xu, Guoqi Li, Sen\n  Song, Lei Deng, Guanrui Wang, Hao Zheng, Jing Pei, Youhui Zhang, Mingguo\n  Zhao, and Luping Shi", "title": "Brain-inspired global-local learning incorporated with neuromorphic\n  computing", "comments": "5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two main routes of learning methods exist at present including error-driven\nglobal learning and neuroscience-oriented local learning. Integrating them into\none network may provide complementary learning capabilities for versatile\nlearning scenarios. At the same time, neuromorphic computing holds great\npromise, but still needs plenty of useful algorithms and algorithm-hardware\nco-designs for exploiting the advantages. Here, we report a neuromorphic hybrid\nlearning model by introducing a brain-inspired meta-learning paradigm and a\ndifferentiable spiking model incorporating neuronal dynamics and synaptic\nplasticity. It can meta-learn local plasticity and receive top-down supervision\ninformation for multiscale synergic learning. We demonstrate the advantages of\nthis model in multiple different tasks, including few-shot learning, continual\nlearning, and fault-tolerance learning in neuromorphic vision sensors. It\nachieves significantly higher performance than single-learning methods, and\nshows promise in empowering neuromorphic applications revolution. We further\nimplemented the hybrid model in the Tianjic neuromorphic platform by exploiting\nalgorithm-hardware co-designs and proved that the model can fully utilize\nneuromorphic many-core architecture to develop hybrid computation paradigm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 04:24:19 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 12:01:40 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 01:15:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wu", "Yujie", ""], ["Zhao", "Rong", ""], ["Zhu", "Jun", ""], ["Chen", "Feng", ""], ["Xu", "Mingkun", ""], ["Li", "Guoqi", ""], ["Song", "Sen", ""], ["Deng", "Lei", ""], ["Wang", "Guanrui", ""], ["Zheng", "Hao", ""], ["Pei", "Jing", ""], ["Zhang", "Youhui", ""], ["Zhao", "Mingguo", ""], ["Shi", "Luping", ""]]}, {"id": "2006.03355", "submitter": "Changmin Yu", "authors": "Changmin Yu, Timothy E.J. Behrens and Neil Burgess", "title": "Prediction and Generalisation over Directed Actions by Grid Cells", "comments": "In Proceedings of ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing how the effects of directed actions generalise to new situations\n(e.g. moving North, South, East and West, or turning left, right, etc.) is key\nto rapid generalisation across new situations. Markovian tasks can be\ncharacterised by a state space and a transition matrix and recent work has\nproposed that neural grid codes provide an efficient representation of the\nstate space, as eigenvectors of a transition matrix reflecting diffusion across\nstates, that allows efficient prediction of future state distributions. Here we\nextend the eigenbasis prediction model, utilising tools from Fourier analysis,\nto prediction over arbitrary translation-invariant directed transition\nstructures (i.e. displacement and diffusion), showing that a single set of\neigenvectors can support predictions over arbitrary directed actions via\naction-specific eigenvalues. We show how to define a \"sense of direction\" to\ncombine actions to reach a target state (ignoring task-specific deviations from\ntranslation-invariance), and demonstrate that adding the Fourier\nrepresentations to a deep Q network aids policy learning in continuous control\ntasks. We show the equivalence between the generalised prediction framework and\ntraditional models of grid cell firing driven by self-motion to perform path\nintegration, either using oscillatory interference (via Fourier components as\nvelocity-controlled oscillators) or continuous attractor networks (via analysis\nof the update dynamics). We thus provide a unifying framework for the role of\nthe grid system in predictive planning, sense of direction and path\nintegration: supporting generalisable inference over directed actions across\ndifferent tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:32:22 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 02:05:30 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yu", "Changmin", ""], ["Behrens", "Timothy E. J.", ""], ["Burgess", "Neil", ""]]}, {"id": "2006.03420", "submitter": "Robert Worden", "authors": "R.P. Worden", "title": "Is there a wave excitation in the Thalamus?", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes that the thalamus is the site of a wave excitation, whose\nfunction is to represent the locations of things around the animal. Neurons\ncouple to the wave as transmitters and receivers. The wave acts as an analogue\nrepresentation of local space. This has benefits over a purely neural\nrepresentation of space. Several lines of evidence support this hypothesis;\nboth theoretical, concerning efficient Bayesian inference in the brain, and\nempirical, concerning the neuro-anatomy of the thalamus. Across all species,\nthe most basic function of the brain is to coordinate movements in space. To\nrepresent positions in space only by neural firing rates would be complex and\ninefficient. It is possible that that the brain represents 3D space in a direct\nand natural way, by a 3D wave\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:28:02 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Worden", "R. P.", ""]]}, {"id": "2006.03611", "submitter": "Markus D Schirmer", "authors": "Markus D. Schirmer, Archana Venkataraman, Islem Rekik, Minjeong Kim,\n  Stewart H. Mostofsky, Mary Beth Nebel, Keri Rosch, Karen Seymour, Deana\n  Crocetti, Hassna Irzan, Michael H\\\"utel, Sebastien Ourselin, Neil Marlow,\n  Andrew Melbourne, Egor Levchenko, Shuo Zhou, Mwiza Kunda, Haiping Lu, Nicha\n  C. Dvornek, Juntang Zhuang, Gideon Pinto, Sandip Samal, Jennings Zhang, Jorge\n  L. Bernal-Rusiel, Rudolph Pienaar, Ai Wern Chung", "title": "Neuropsychiatric Disease Classification Using Functional Connectomics --\n  Results of the Connectomics in NeuroImaging Transfer Learning Challenge", "comments": "CNI-TLC was held in conjunction with MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, open-source consortium datasets have spurred the development of new\nand increasingly powerful machine learning approaches in brain connectomics.\nHowever, one key question remains: are we capturing biologically relevant and\ngeneralizable information about the brain, or are we simply overfitting to the\ndata? To answer this, we organized a scientific challenge, the Connectomics in\nNeuroImaging Transfer Learning Challenge (CNI-TLC), held in conjunction with\nMICCAI 2019. CNI-TLC included two classification tasks: (1) diagnosis of\nAttention-Deficit/Hyperactivity Disorder (ADHD) within a pre-adolescent cohort;\nand (2) transference of the ADHD model to a related cohort of Autism Spectrum\nDisorder (ASD) patients with an ADHD comorbidity. In total, 240 resting-state\nfMRI time series averaged according to three standard parcellation atlases,\nalong with clinical diagnosis, were released for training and validation (120\nneurotypical controls and 120 ADHD). We also provided demographic information\nof age, sex, IQ, and handedness. A second set of 100 subjects (50 neurotypical\ncontrols, 25 ADHD, and 25 ASD with ADHD comorbidity) was used for testing.\nModels were submitted in a standardized format as Docker images through ChRIS,\nan open-source image analysis platform. Utilizing an inclusive approach, we\nranked the methods based on 16 different metrics. The final rank was calculated\nusing the rank product for each participant across all measures. Furthermore,\nwe assessed the calibration curves of each method. Five participants submitted\ntheir model for evaluation, with one outperforming all other methods in both\nADHD and ASD classification. However, further improvements are needed to reach\nthe clinical translation of functional connectomics. We are keeping the CNI-TLC\nopen as a publicly available resource for developing and validating new\nclassification methodologies in the field of connectomics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:05:42 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 11:45:57 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Schirmer", "Markus D.", ""], ["Venkataraman", "Archana", ""], ["Rekik", "Islem", ""], ["Kim", "Minjeong", ""], ["Mostofsky", "Stewart H.", ""], ["Nebel", "Mary Beth", ""], ["Rosch", "Keri", ""], ["Seymour", "Karen", ""], ["Crocetti", "Deana", ""], ["Irzan", "Hassna", ""], ["H\u00fctel", "Michael", ""], ["Ourselin", "Sebastien", ""], ["Marlow", "Neil", ""], ["Melbourne", "Andrew", ""], ["Levchenko", "Egor", ""], ["Zhou", "Shuo", ""], ["Kunda", "Mwiza", ""], ["Lu", "Haiping", ""], ["Dvornek", "Nicha C.", ""], ["Zhuang", "Juntang", ""], ["Pinto", "Gideon", ""], ["Samal", "Sandip", ""], ["Zhang", "Jennings", ""], ["Bernal-Rusiel", "Jorge L.", ""], ["Pienaar", "Rudolph", ""], ["Chung", "Ai Wern", ""]]}, {"id": "2006.03643", "submitter": "Kalel Luiz Rossi", "authors": "R. C. Budzinski, K. L. Rossi, B. R. R. Boaretto, T. L. Prado, S. R.\n  Lopes", "title": "Synchronization malleability in neural networks under a\n  distance-dependent coupling", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the synchronization features of a network of spiking neurons\nunder a distance-dependent coupling following a power-law model. The interplay\nbetween topology and coupling strength leads to the existence of different\nspatiotemporal patterns, corresponding to either non-synchronized or\nphase-synchronized states. Particularly interesting is what we call\nsynchronization malleability, in which the system depicts significantly\ndifferent phase synchronization degrees for the same parameters as a\nconsequence of a different ordering of neural inputs. We analyze the functional\nconnectivity of the network by calculating the mutual information between\nneuronal spike trains, allowing us to characterize the structures of\nsynchronization in the network. We show that these structures are dependent on\nthe ordering of the inputs for the parameter regions where the network presents\nsynchronization malleability and we suggest that this is due to a complex\ninterplay between coupling, connection architecture, and individual neural\ninputs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 19:22:07 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 15:26:25 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Budzinski", "R. C.", ""], ["Rossi", "K. L.", ""], ["Boaretto", "B. R. R.", ""], ["Prado", "T. L.", ""], ["Lopes", "S. R.", ""]]}, {"id": "2006.03698", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Aida Davila, Adam Kohn, Ruben Coen-Cagli", "title": "Texture Interpolation for Probing Visual Perception", "comments": "Paper + ref: 12 pages and 7 figures | Supplementary: 16 pages and 16\n  figures Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Texture synthesis models are important tools for understanding visual\nprocessing. In particular, statistical approaches based on neurally relevant\nfeatures have been instrumental in understanding aspects of visual perception\nand of neural coding. New deep learning-based approaches further improve the\nquality of synthetic textures. Yet, it is still unclear why deep texture\nsynthesis performs so well, and applications of this new framework to probe\nvisual perception are scarce. Here, we show that distributions of deep\nconvolutional neural network (CNN) activations of a texture are well described\nby elliptical distributions and therefore, following optimal transport theory,\nconstraining their mean and covariance is sufficient to generate new texture\nsamples. Then, we propose the natural geodesics (ie the shortest path between\ntwo points) arising with the optimal transport metric to interpolate between\narbitrary textures. Compared to other CNN-based approaches, our interpolation\nmethod appears to match more closely the geometry of texture perception, and\nour mathematical framework is better suited to study its statistical nature. We\napply our method by measuring the perceptual scale associated to the\ninterpolation parameter in human observers, and the neural sensitivity of\ndifferent areas of visual cortex in macaque monkeys.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:28:36 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:05:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Vacher", "Jonathan", ""], ["Davila", "Aida", ""], ["Kohn", "Adam", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "2006.03737", "submitter": "Ding Zhou", "authors": "Xue-Xin Wei, Ding Zhou, Andres Grosmark, Zaki Ajabi, Fraser Sparks,\n  Pengcheng Zhou, Mark Brandon, Attila Losonczy, Liam Paninski", "title": "A zero-inflated gamma model for deconvolved calcium imaging traces", "comments": "Accepted for publication in Neurons, Behavior, Data analysis, and\n  Theory", "journal-ref": "Neurons, Behavior, Data Analysis, and Theory, 2020", "doi": "10.1101/637652", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Calcium imaging is a critical tool for measuring the activity of large neural\npopulations. Much effort has been devoted to developing \"pre-processing\" tools\nfor calcium video data, addressing the important issues of e.g., motion\ncorrection, denoising, compression, demixing, and deconvolution. However,\nstatistical modeling of deconvolved calcium signals (i.e., the estimated\nactivity extracted by a pre-processing pipeline) is just as critical for\ninterpreting calcium measurements, and for incorporating these observations\ninto downstream probabilistic encoding and decoding models. Surprisingly, these\nissues have to date received significantly less attention. In this work we\nexamine the statistical properties of the deconvolved activity estimates, and\ncompare probabilistic models for these random signals. In particular, we\npropose a zero-inflated gamma (ZIG) model, which characterizes the calcium\nresponses as a mixture of a gamma distribution and a point mass that serves to\nmodel zero responses. We apply the resulting models to neural encoding and\ndecoding problems. We find that the ZIG model outperforms simpler models (e.g.,\nPoisson or Bernoulli models) in the context of both simulated and real neural\ndata, and can therefore play a useful role in bridging calcium imaging analysis\nmethods with tools for analyzing activity in large neural populations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:29:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wei", "Xue-Xin", ""], ["Zhou", "Ding", ""], ["Grosmark", "Andres", ""], ["Ajabi", "Zaki", ""], ["Sparks", "Fraser", ""], ["Zhou", "Pengcheng", ""], ["Brandon", "Mark", ""], ["Losonczy", "Attila", ""], ["Paninski", "Liam", ""]]}, {"id": "2006.03965", "submitter": "Gasper Begus", "authors": "Ga\\v{s}per Begu\\v{s}", "title": "Generative Adversarial Phonology: Modeling unsupervised phonetic and\n  phonological learning with neural networks", "comments": "Provisionally accepted in Frontiers in Artificial Intelligence", "journal-ref": "Frontiers in Artificial Intelligence 2020", "doi": "10.3389/frai.2020.00044", "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on well-understood dependencies in speech data\ncan provide new insights into how they learn internal representations. This\npaper argues that acquisition of speech can be modeled as a dependency between\nrandom space and generated speech data in the Generative Adversarial Network\narchitecture and proposes a methodology to uncover the network's internal\nrepresentations that correspond to phonetic and phonological properties. The\nGenerative Adversarial architecture is uniquely appropriate for modeling\nphonetic and phonological learning because the network is trained on\nunannotated raw acoustic data and learning is unsupervised without any\nlanguage-specific assumptions or pre-assumed levels of abstraction. A\nGenerative Adversarial Network was trained on an allophonic distribution in\nEnglish. The network successfully learns the allophonic alternation: the\nnetwork's generated speech signal contains the conditional distribution of\naspiration duration. The paper proposes a technique for establishing the\nnetwork's internal representations that identifies latent variables that\ncorrespond to, for example, presence of [s] and its spectral properties. By\nmanipulating these variables, we actively control the presence of [s] and its\nfrication amplitude in the generated outputs. This suggests that the network\nlearns to use latent variables as an approximation of phonetic and phonological\nrepresentations. Crucially, we observe that the dependencies learned in\ntraining extend beyond the training interval, which allows for additional\nexploration of learning representations. The paper also discusses how the\nnetwork's architecture and innovative outputs resemble and differ from\nlinguistic behavior in language acquisition, speech disorders, and speech\nerrors, and how well-understood dependencies in speech data can help us\ninterpret how neural networks learn their representations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 20:31:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Begu\u0161", "Ga\u0161per", ""]]}, {"id": "2006.04039", "submitter": "Benjamin Ambrosio", "authors": "Benjamin Ambrosio, Lai-Sang Young", "title": "Simulating brain rhythms using an ODE with stochastically varying\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain produces rhythms in a variety of frequency bands. Some are likely\nby-products of neuronal processes; others are thought to be top-down. Produced\nentirely naturally, these rhythms have clearly recognizable beats, but they are\nvery far from periodic in the sense of mathematics. They produce signals that\nare broad-band, episodic, wandering in magnitude, in frequency and in phase;\nthe rhythm comes and goes, degrading and regenerating. Rhythms with these\ncharacteristics do not match standard dynamical systems paradigms of\nperiodicity, quasi-periodicity, or periodic motion in the presence of a\nBrownian noise. Thus far they have been satisfactorily reproduced only using\nnetworks of hundreds of integrate-and-fire neurons. In this paper, we tackle\nthe mathematical question of whether signals with these properties can be\ngenerated from simpler dynamical systems. Using an ODE with two variables\ninspired by the FitzHugh-Nagumo model, and varying randomly three parameters\nthat control the magnitude, frequency and degree of degradation, we were able\nto replicate the qualitative characteristics of these natural brain rhythms.\nViewing the two variables as Excitatory and Inhibitory conductances of a\ntypical neuron in a local population, our model produces results that closely\nresemble gamma-band activity in real cortex, including the moment-to-moment\nbalancing of E and I-currents seen in experiments.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 04:30:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ambrosio", "Benjamin", ""], ["Young", "Lai-Sang", ""]]}, {"id": "2006.04120", "submitter": "Thomas Parr", "authors": "Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, Thomas\n  Parr", "title": "Sophisticated Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active inference offers a first principle account of sentient behaviour, from\nwhich special and important cases can be derived, e.g., reinforcement learning,\nactive learning, Bayes optimal inference, Bayes optimal design, etc. Active\ninference resolves the exploitation-exploration dilemma in relation to prior\npreferences, by placing information gain on the same footing as reward or\nvalue. In brief, active inference replaces value functions with functionals of\n(Bayesian) beliefs, in the form of an expected (variational) free energy. In\nthis paper, we consider a sophisticated kind of active inference, using a\nrecursive form of expected free energy. Sophistication describes the degree to\nwhich an agent has beliefs about beliefs. We consider agents with beliefs about\nthe counterfactual consequences of action for states of affairs and beliefs\nabout those latent states. In other words, we move from simply considering\nbeliefs about 'what would happen if I did that' to 'what would I believe about\nwhat would happen if I did that'. The recursive form of the free energy\nfunctional effectively implements a deep tree search over actions and outcomes\nin the future. Crucially, this search is over sequences of belief states, as\nopposed to states per se. We illustrate the competence of this scheme, using\nnumerical simulations of deep decision problems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:18:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Friston", "Karl", ""], ["Da Costa", "Lancelot", ""], ["Hafner", "Danijar", ""], ["Hesp", "Casper", ""], ["Parr", "Thomas", ""]]}, {"id": "2006.04176", "submitter": "Zafeirios Fountas PhD", "authors": "Zafeirios Fountas, Noor Sajid, Pedro A.M. Mediano, Karl Friston", "title": "Deep active inference agents using Monte-Carlo methods", "comments": "To appear in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference is a Bayesian framework for understanding biological\nintelligence. The underlying theory brings together perception and action under\none single imperative: minimizing free energy. However, despite its theoretical\nutility in explaining intelligence, computational implementations have been\nrestricted to low-dimensional and idealized situations. In this paper, we\npresent a neural architecture for building deep active inference agents\noperating in complex, continuous state-spaces using multiple forms of\nMonte-Carlo (MC) sampling. For this, we introduce a number of techniques, novel\nto active inference. These include: i) selecting free-energy-optimal policies\nvia MC tree search, ii) approximating this optimal policy distribution via a\nfeed-forward `habitual' network, iii) predicting future parameter belief\nupdates using MC dropouts and, finally, iv) optimizing state transition\nprecision (a high-end form of attention). Our approach enables agents to learn\nenvironmental dynamics efficiently, while maintaining task performance, in\nrelation to reward-based counterparts. We illustrate this in a new toy\nenvironment, based on the dSprites data-set, and demonstrate that active\ninference agents automatically create disentangled representations that are apt\nfor modeling state transitions. In a more complex Animal-AI environment, our\nagents (using the same neural architecture) are able to simulate future state\ntransitions and actions (i.e., plan), to evince reward-directed navigation -\ndespite temporary suspension of visual input. These results show that deep\nactive inference - equipped with MC methods - provides a flexible framework to\ndevelop biologically-inspired intelligent agents, with applications in both\nmachine learning and cognitive science.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 15:10:42 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 13:17:36 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Fountas", "Zafeirios", ""], ["Sajid", "Noor", ""], ["Mediano", "Pedro A. M.", ""], ["Friston", "Karl", ""]]}, {"id": "2006.04728", "submitter": "Linden Parkes PhD", "authors": "Linden Parkes, Theodore D. Satterthwaite, Danielle S. Bassett", "title": "Towards precise resting-state fMRI biomarkers in psychiatry:\n  synthesizing developments in transdiagnostic research, dimensional models of\n  psychopathology, and normative neurodevelopment", "comments": "Keywords: Psychiatry; biomarker; transdiagnostic; dimensional\n  psychopathology; normative modeling; fingerprint; network science", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Searching for biomarkers has been a chief pursuit of the field of psychiatry.\nToward this end, studies have catalogued candidate resting-state biomarkers in\nnearly all forms of mental disorder. However, it is becoming increasingly clear\nthat these biomarkers lack specificity, limiting their capacity to yield\nclinical impact. We discuss three avenues of research that are overcoming this\nlimitation: (i) the adoption of transdiagnostic research designs, which involve\nstudying and explicitly comparing multiple disorders from distinct diagnostic\naxes of psychiatry; (ii) dimensional models of psychopathology that map the\nfull spectrum of symptomatology and that cut across traditional disorder\nboundaries; and (iii) modeling individuals' unique functional connectomes\nthroughout development. We provide a framework for tying these subfields\ntogether that draws on tools from machine learning and network science.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:35:17 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Parkes", "Linden", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2006.04765", "submitter": "Konstantinos Michmizos", "authors": "Ioannis Polykretis, Konstantinos P. Michmizos", "title": "An Astrocyte-Modulated Neuromorphic Central Pattern Generator for\n  Hexapod Robot Locomotion on Intel's Loihi", "comments": "8 pages, 7 figures, International Conference on Neuromorphic Systems\n  (ICONS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locomotion is a crucial challenge for legged robots that is addressed\n\"effortlessly\" by biological networks abundant in nature, named central pattern\ngenerators (CPG). The multitude of CPG network models that have so far become\nbiomimetic robotic controllers is not applicable to the emerging neuromorphic\nhardware, depriving mobile robots of a robust walking mechanism that would\nresult in inherently energy-efficient systems. Here, we propose a brain-morphic\nCPG controler based on a comprehensive spiking neural-astrocytic network that\ngenerates two gait patterns for a hexapod robot. Building on the recently\nidentified astrocytic mechanisms for neuromodulation, our proposed CPG\narchitecture is seamlessly integrated into Intel's Loihi neuromorphic chip by\nleveraging a real-time interaction framework between the chip and the robotic\noperating system (ROS) environment, that we also propose. Here, we demonstrate\nthat a Loihi-run CPG can be used to control a walking robot with robustness to\nsensory noise and varying speed profiles. Our results pave the way for scaling\nthis and other approaches towards Loihi-controlled locomotion in autonomous\nmobile robots.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:35:48 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Polykretis", "Ioannis", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2006.05113", "submitter": "Lukas Muttenthaler", "authors": "Lukas Muttenthaler, Nora Hollenstein, Maria Barrett", "title": "Human brain activity for machine attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitively inspired NLP leverages human-derived data to teach machines about\nlanguage processing mechanisms. Recently, neural networks have been augmented\nwith behavioral data to solve a range of NLP tasks spanning syntax and\nsemantics. We are the first to exploit neuroscientific data, namely\nelectroencephalography (EEG), to inform a neural attention model about language\nprocessing of the human brain. The challenge in working with EEG data is that\nfeatures are exceptionally rich and need extensive pre-processing to isolate\nsignals specific to text processing. We devise a method for finding such EEG\nfeatures to supervise machine attention through combining theoretically\nmotivated cropping with random forest tree splits. After this dimensionality\nreduction, the pre-processed EEG features are capable of distinguishing two\nreading tasks retrieved from a publicly available EEG corpus. We apply these\nfeatures to regularise attention on relation classification and show that EEG\nis more informative than strong baselines. This improvement depends on both the\ncognitive load of the task and the EEG frequency domain. Hence, informing\nneural attention models with EEG signals is beneficial but requires further\ninvestigation to understand which dimensions are the most useful across NLP\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:39:07 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 22:06:31 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Muttenthaler", "Lukas", ""], ["Hollenstein", "Nora", ""], ["Barrett", "Maria", ""]]}, {"id": "2006.05176", "submitter": "Francesco Bonchi", "authors": "Tommaso Lanciano, Francesco Bonchi, Aristides Gionis", "title": "Explainable Classification of Brain Networks via Contrast Subgraphs", "comments": "To be published at KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining human-brain networks to discover patterns that can be used to\ndiscriminate between healthy individuals and patients affected by some\nneurological disorder, is a fundamental task in neuroscience. Learning simple\nand interpretable models is as important as mere classification accuracy. In\nthis paper we introduce a novel approach for classifying brain networks based\non extracting contrast subgraphs, i.e., a set of vertices whose induced\nsubgraphs are dense in one class of graphs and sparse in the other. We formally\ndefine the problem and present an algorithmic solution for extracting contrast\nsubgraphs. We then apply our method to a brain-network dataset consisting of\nchildren affected by Autism Spectrum Disorder and children Typically Developed.\nOur analysis confirms the interestingness of the discovered patterns, which\nmatch background knowledge in the neuroscience literature. Further analysis on\nother classification tasks confirm the simplicity, soundness, and high\nexplainability of our proposal, which also exhibits superior classification\naccuracy, to more complex state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:48:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Lanciano", "Tommaso", ""], ["Bonchi", "Francesco", ""], ["Gionis", "Aristides", ""]]}, {"id": "2006.05249", "submitter": "Daniel Harari", "authors": "Hanna Benoni, Daniel Harari and Shimon Ullman", "title": "What takes the brain so long: Object recognition at the level of minimal\n  images develops for up to seconds of presentation time", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich empirical evidence has shown that visual object recognition in the brain\nis fast and effortless, with relevant brain signals reported to start as early\nas 80 ms. Here we study the time trajectory of the recognition process at the\nlevel of minimal recognizable images (termed MIRC). These are images that can\nbe recognized reliably, but in which a minute change of the image (reduction by\neither size or resolution) has a drastic effect on recognition. Subjects were\nassigned to one of nine exposure conditions: 200, 500, 1000, 2000 ms with or\nwithout masking, as well as unlimited time. The subjects were not limited in\ntime to respond after presentation. The results show that in the masked\nconditions, recognition rates develop gradually over an extended period, e.g.\naverage of 18% for 200 ms exposure and 45% for 500 ms, increasing significantly\nwith longer exposure even above 2 secs. When presented for unlimited time\n(until response), MIRC recognition rates were equivalent to the rates of\nfull-object images presented for 50 ms followed by masking. What takes the\nbrain so long to recognize such images? We discuss why processes involving\neye-movements, perceptual decision-making and pattern completion are unlikely\nexplanations. Alternatively, we hypothesize that MIRC recognition requires an\nextended top-down process complementing the feed-forward phase.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:33:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Benoni", "Hanna", ""], ["Harari", "Daniel", ""], ["Ullman", "Shimon", ""]]}, {"id": "2006.05406", "submitter": "Jeremy Miller", "authors": "Jeremy A. Miller, Nathan W. Gouwens, Bosiljka Tasic, Forrest Collman,\n  Cindy T. J. van Velthoven, Trygve E. Bakken, Michael J. Hawrylycz, Hongkui\n  Zeng, Ed S. Lein and Amy Bernard", "title": "Common Cell type Nomenclature for the mammalian brain: A systematic,\n  extensible convention", "comments": "29 pages, 5 figures, 4 tables, 1 supplementary table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advancement of single cell RNA-sequencing technologies has led to an\nexplosion of cell type definitions across multiple organs and organisms. While\nstandards for data and metadata intake are arising, organization of cell types\nhas largely been left to individual investigators, resulting in widely varying\nnomenclature and limited alignment between taxonomies. To facilitate\ncross-dataset comparison, the Allen Institute created the Common Cell type\nNomenclature (CCN) for matching and tracking cell types across studies that is\nqualitatively similar to gene transcript management across different genome\nbuilds. The CCN can be readily applied to new or established taxonomies and was\napplied herein to diverse cell type datasets derived from multiple quantifiable\nmodalities. The CCN facilitates assigning accurate yet flexible cell type names\nin the mammalian cortex as a step towards community-wide efforts to organize\nmulti-source, data-driven information related to cell type taxonomies from any\norganism.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 17:11:20 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 21:18:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Miller", "Jeremy A.", ""], ["Gouwens", "Nathan W.", ""], ["Tasic", "Bosiljka", ""], ["Collman", "Forrest", ""], ["van Velthoven", "Cindy T. J.", ""], ["Bakken", "Trygve E.", ""], ["Hawrylycz", "Michael J.", ""], ["Zeng", "Hongkui", ""], ["Lein", "Ed S.", ""], ["Bernard", "Amy", ""]]}, {"id": "2006.05467", "submitter": "Hidenori Tanaka", "authors": "Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli", "title": "Pruning neural networks without any data by iteratively conserving\n  synaptic flow", "comments": "NeurIPS 2020, 18 pages, 10 figures", "journal-ref": "Advances in Neural Information Processing Systems 2020", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning the parameters of deep neural networks has generated intense interest\ndue to potential savings in time, memory and energy both during training and at\ntest time. Recent works have identified, through an expensive sequence of\ntraining and pruning cycles, the existence of winning lottery tickets or sparse\ntrainable subnetworks at initialization. This raises a foundational question:\ncan we identify highly sparse trainable subnetworks at initialization, without\never training, or indeed without ever looking at the data? We provide an\naffirmative answer to this question through theory driven algorithm design. We\nfirst mathematically formulate and experimentally verify a conservation law\nthat explains why existing gradient-based pruning algorithms at initialization\nsuffer from layer-collapse, the premature pruning of an entire layer rendering\na network untrainable. This theory also elucidates how layer-collapse can be\nentirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow\nPruning (SynFlow). This algorithm can be interpreted as preserving the total\nflow of synaptic strengths through the network at initialization subject to a\nsparsity constraint. Notably, this algorithm makes no reference to the training\ndata and consistently competes with or outperforms existing state-of-the-art\npruning algorithms at initialization over a range of models (VGG and ResNet),\ndatasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to\n99.99 percent). Thus our data-agnostic pruning algorithm challenges the\nexisting paradigm that, at initialization, data must be used to quantify which\nsynapses are important.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:21:57 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:05:52 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 03:54:34 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tanaka", "Hidenori", ""], ["Kunin", "Daniel", ""], ["Yamins", "Daniel L. K.", ""], ["Ganguli", "Surya", ""]]}, {"id": "2006.05572", "submitter": "MohammadReza Ebrahimi", "authors": "MohammadReza Ebrahimi, Navona Calarco, Kieran Campbell, Colin Hawco,\n  Aristotle Voineskos, Ashish Khisti", "title": "Time-Resolved fMRI Shared Response Model using Gaussian Process Factor\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-subject fMRI studies are challenging due to the high variability of\nboth brain anatomy and functional brain topographies across participants. An\neffective way of aggregating multi-subject fMRI data is to extract a shared\nrepresentation that filters out unwanted variability among subjects. Some\nrecent work has implemented probabilistic models to extract a shared\nrepresentation in task fMRI. In the present work, we improve upon these models\nby incorporating temporal information in the common latent structures. We\nintroduce a new model, Shared Gaussian Process Factor Analysis (S-GPFA), that\ndiscovers shared latent trajectories and subject-specific functional\ntopographies, while modelling temporal correlation in fMRI data. We demonstrate\nthe efficacy of our model in revealing ground truth latent structures using\nsimulated data, and replicate experimental performance of time-segment matching\nand inter-subject similarity on the publicly available Raider and Sherlock\ndatasets. We further test the utility of our model by analyzing its learned\nmodel parameters in the large multi-site SPINS dataset, on a social cognition\ntask from participants with and without schizophrenia.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:15:01 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 01:13:56 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ebrahimi", "MohammadReza", ""], ["Calarco", "Navona", ""], ["Campbell", "Kieran", ""], ["Hawco", "Colin", ""], ["Voineskos", "Aristotle", ""], ["Khisti", "Ashish", ""]]}, {"id": "2006.06109", "submitter": "Jona Carmon", "authors": "Jona Carmon, Moritz Bammel, Peter Brugger, Bigna Lenggenhager", "title": "Uncertainty promotes neuroreductionism: A behavioral online study on\n  folk psychological causal inference from neuroimaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction. Increased efforts in neuroscience try to understand mental\ndisorders as brain disorders. In the present study we investigate how common a\nneuroreductionist inclination is among highly educated people. In particular,\nwe shed light on implicit presuppositions of mental disorders little is known\nabout in the public, exemplified here by the case of Body Integrity Dysphoria\n(BID) that is considered a mental disorder for the first time in ICD-11.\n  Methods. Identically graphed, simulated data of mind-brain correlations were\nshown in three contexts with presumably different presumptions about causality.\n738 highly-educated laymen rated plausibility of causality attribution from\nbrain to mind and from mind to brain for correlations between brain structural\nproperties and mental phenomena. We contrasted participants' plausibility\nratings of causality in the contexts of commonly perceived brain-lesion induced\nbehavior (aphasia), behavior-induced training effects (piano playing), and a\nnewly described mental disorder (BID).\n  Results. The findings reveal the expected context-dependent modulation of\ncausality attributions in the contexts of aphasia and piano playing.\nFurthermore, we observed a significant tendency to more readily attribute\ncausal inference from brain to mind than vice versa with respect to BID.\n  Conclusion. In some contexts, exemplified here by aphasia and piano playing,\nunidirectional causality attributions may be justified. However, with respect\nto BID, we critically discuss presumably unjustified neuroreductionist\ninclinations under causal uncertainty. Finally, we emphasize the need for a\npresupposition-free approach in psychiatry.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 23:28:00 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 13:55:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Carmon", "Jona", ""], ["Bammel", "Moritz", ""], ["Brugger", "Peter", ""], ["Lenggenhager", "Bigna", ""]]}, {"id": "2006.06311", "submitter": "Ece Kocagoncu", "authors": "Ece Kocagoncu, Anastasia Klimovich-Gray, Laura E Hughes, James B Rowe", "title": "Evidence and implications of abnormal predictive coding in dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The diversity of cognitive deficits and neuropathological processes\nassociated with dementias has encouraged divergence in pathophysiological\nexplanations of disease. Here, we review an alternative framework that\nemphasises convergent critical features of pathophysiology, rather than the\nloss of memory centres or language centres, or singular neurotransmitter\nsystems. Cognitive deficits are interpreted in the light of advances in\nnormative accounts of brain function, based on predictive coding in\nhierarchical neural networks. The predicting coding rests on Bayesian\nintegration of beliefs and sensory evidence, with hierarchical predictions and\nprediction errors, for memory, perception, speech and behaviour. We describe\nhow analogous impairments in predictive coding in parallel neurocognitive\nsystems can generate diverse clinical phenomena, in neurodegenerative\ndementias. The review presents evidence from behavioural and neurophysiological\nstudies of perception, language, memory and decision-making. The re-formulation\nof cognitive deficits in dementia in terms of predictive coding has several\nadvantages. It brings diverse clinical phenomena into a common framework, such\nas linking cognitive and movement disorders; and it makes specific predictions\non cognitive physiology that support translational and experimental medicine\nstudies. The insights into complex human cognitive disorders from the\npredictive coding model may therefore also inform future therapeutic\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:23:08 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kocagoncu", "Ece", ""], ["Klimovich-Gray", "Anastasia", ""], ["Hughes", "Laura E", ""], ["Rowe", "James B", ""]]}, {"id": "2006.06497", "submitter": "Gabriel Mahuas", "authors": "Gabriel Mahuas, Giulio Isacchini, Olivier Marre, Ulisse Ferrari and\n  Thierry Mora", "title": "A new inference approach for training shallow and deep generalized\n  linear models of noisy interacting neurons", "comments": "Final submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generalized linear models are one of the most efficient paradigms for\npredicting the correlated stochastic activity of neuronal networks in response\nto external stimuli, with applications in many brain areas. However, when\ndealing with complex stimuli, the inferred coupling parameters often do not\ngeneralize across different stimulus statistics, leading to degraded\nperformance and blowup instabilities. Here, we develop a two-step inference\nstrategy that allows us to train robust generalized linear models of\ninteracting neurons, by explicitly separating the effects of correlations in\nthe stimulus from network interactions in each training step. Applying this\napproach to the responses of retinal ganglion cells to complex visual stimuli,\nwe show that, compared to classical methods, the models trained in this way\nexhibit improved performance, are more stable, yield robust interaction\nnetworks, and generalize well across complex visual statistics. The method can\nbe extended to deep convolutional neural networks, leading to models with high\npredictive accuracy for both the neuron firing rates and their correlations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:09:53 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 21:10:19 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 15:01:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Mahuas", "Gabriel", ""], ["Isacchini", "Giulio", ""], ["Marre", "Olivier", ""], ["Ferrari", "Ulisse", ""], ["Mora", "Thierry", ""]]}, {"id": "2006.06580", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi", "title": "Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior", "comments": "To the best of our knowledge, this is the first attempt to explore\n  the full spectrum of reinforcement learning agents (multi-armed bandits,\n  contextual bandits and reinforcement learning) in the sequential social\n  dilemma. This mental variants section supersedes and extends our work\n  arXiv:1706.02897 (MAB), arXiv:2005.04544 (CB) and arXiv:1906.11286 (RL) into\n  the multi-agent setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prisoner's Dilemma mainly treat the choice to cooperate or defect as an\natomic action. We propose to study online learning algorithm behavior in the\nIterated Prisoner's Dilemma (IPD) game, where we explored the full spectrum of\nreinforcement learning agents: multi-armed bandits, contextual bandits and\nreinforcement learning. We have evaluate them based on a tournament of iterated\nprisoner's dilemma where multiple agents can compete in a sequential fashion.\nThis allows us to analyze the dynamics of policies learned by multiple\nself-interested independent reward-driven agents, and also allows us study the\ncapacity of these algorithms to fit the human behaviors. Results suggest that\nconsidering the current situation to make decision is the worst in this kind of\nsocial dilemma game. Multiples discoveries on online learning behaviors and\nclinical validations are stated.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:58:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 14:17:09 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""]]}, {"id": "2006.06829", "submitter": "Amin Dehghani", "authors": "Amin Dehghani, Hamid Soltanian-Zadeh, Gholam-Ali Hossein-Zadeh", "title": "Probing fMRI brain connectivity and activity changes during emotion\n  regulation by EEG neurofeedback", "comments": "40 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurofeedback is a non-invasive brain training with long-term medical and\nnon-medical applications. Despite the existence of several emotion regulation\nstudies using neurofeedback, further investigation is needed to understand\ninteractions of the brain regions involved in the process. We implemented EEG\nneurofeedback with simultaneous fMRI using a modified happiness-inducing task\nthrough autobiographical memories to upregulate positive emotion. The results\nshowed increased activity of prefrontal, occipital, parietal, and limbic\nregions and increased functional connectivity between prefrontal, parietal,\nlimbic system, and insula in the experimental group. New connectivity links\nwere identified by comparing the functional connectivity of different\nexperimental conditions within the experimental group and between the\nexperimental and control groups. The proposed multimodal approach quantified\nthe changes in the brain activity (up to 1.9% increase) and connectivity\n(FDR-corrected for multiple comparison, q = 0.05) during emotion regulation\nin/between prefrontal, parietal, limbic, and insula regions. Psychometric\nassessments confirmed significant changes in positive and negative mood states\nby neurofeedback with a p-value smaller than 0.002 in the experimental group.\nThis study quantifies the effects of EEG neurofeedback in changing functional\nconnectivity of all brain regions involved in emotion regulation. For the brain\nregions involved in emotion regulation, we found significant BOLD and\nfunctional connectivity increases due to neurofeedback in the experimental\ngroup but no learning effect was observed in the control group. The results\nreveal the neurobiological substrate of emotion regulation by the EEG\nneurofeedback and separate the effect of the neurofeedback and the recall of\nthe autobiographical memories.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:16:59 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Dehghani", "Amin", ""], ["Soltanian-Zadeh", "Hamid", ""], ["Hossein-Zadeh", "Gholam-Ali", ""]]}, {"id": "2006.06862", "submitter": "Donghan Liu", "authors": "Donghan Liu, Benjamin C. M. Fung, Tak Pan Wong", "title": "Deep Learning-based Stress Determinator for Mouse Psychiatric Analysis\n  using Hippocampus Activity", "comments": "The paper need re-evaluated and reviewed, may cause some significant\n  changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding neurons to extract information from transmission and employ them\ninto other use is the goal of neuroscientists' study. Due to that the field of\nneuroscience is utilizing the traditional methods presently, we hence combine\nthe state-of-the-art deep learning techniques with the theory of neuron\ndecoding to discuss its potential of accomplishment. Besides, the stress level\nthat is related to neuron activity in hippocampus is statistically examined as\nwell. The experiments suggest that our state-of-the-art deep learning-based\nstress determinator provides good performance with respect to its model\nprediction accuracy and additionally, there is strong evidence against\nequivalence of mouse stress level under diverse environments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 22:32:31 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:31:14 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Donghan", ""], ["Fung", "Benjamin C. M.", ""], ["Wong", "Tak Pan", ""]]}, {"id": "2006.06902", "submitter": "Guruprasad Raghavan", "authors": "Guruprasad Raghavan, Cong Lin, Matt Thomson", "title": "Self-organization of multi-layer spiking neural networks", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Living neural networks in our brains autonomously self-organize into large,\ncomplex architectures during early development to result in an organized and\nfunctional organic computational device. A key mechanism that enables the\nformation of complex architecture in the developing brain is the emergence of\ntraveling spatio-temporal waves of neuronal activity across the growing brain.\nInspired by this strategy, we attempt to efficiently self-organize large neural\nnetworks with an arbitrary number of layers into a wide variety of\narchitectures. To achieve this, we propose a modular tool-kit in the form of a\ndynamical system that can be seamlessly stacked to assemble multi-layer neural\nnetworks. The dynamical system encapsulates the dynamics of spiking units,\ntheir inter/intra layer interactions as well as the plasticity rules that\ncontrol the flow of information between layers. The key features of our\ntool-kit are (1) autonomous spatio-temporal waves across multiple layers\ntriggered by activity in the preceding layer and (2) Spike-timing dependent\nplasticity (STDP) learning rules that update the inter-layer connectivity based\non wave activity in the connecting layers. Our framework leads to the\nself-organization of a wide variety of architectures, ranging from multi-layer\nperceptrons to autoencoders. We also demonstrate that emergent waves can\nself-organize spiking network architecture to perform unsupervised learning,\nand networks can be coupled with a linear classifier to perform classification\non classic image datasets like MNIST. Broadly, our work shows that a dynamical\nsystems framework for learning can be used to self-organize large computational\ndevices.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 01:44:48 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Raghavan", "Guruprasad", ""], ["Lin", "Cong", ""], ["Thomson", "Matt", ""]]}, {"id": "2006.07123", "submitter": "Roman Pogodin", "authors": "Roman Pogodin and Peter E. Latham", "title": "Kernelized information bottleneck leads to biologically plausible\n  3-factor Hebbian learning in deep networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the art machine learning approach to training deep neural\nnetworks, backpropagation, is implausible for real neural networks: neurons\nneed to know their outgoing weights; training alternates between a bottom-up\nforward pass (computation) and a top-down backward pass (learning); and the\nalgorithm often needs precise labels of many data points. Biologically\nplausible approximations to backpropagation, such as feedback alignment, solve\nthe weight transport problem, but not the other two. Thus, fully biologically\nplausible learning rules have so far remained elusive. Here we present a family\nof learning rules that does not suffer from any of these problems. It is\nmotivated by the information bottleneck principle (extended with kernel\nmethods), in which networks learn to compress the input as much as possible\nwithout sacrificing prediction of the output. The resulting rules have a\n3-factor Hebbian structure: they require pre- and post-synaptic firing rates\nand an error signal - the third factor - consisting of a global teaching signal\nand a layer-specific term, both available without a top-down pass. They do not\nrequire precise labels; instead, they rely on the similarity between pairs of\ndesired outputs. Moreover, to obtain good performance on hard problems and\nretain biological plausibility, our rules need divisive normalization - a known\nfeature of biological networks. Finally, simulations show that our rules\nperform nearly as well as backpropagation on image classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 12:30:53 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 17:00:59 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pogodin", "Roman", ""], ["Latham", "Peter E.", ""]]}, {"id": "2006.07239", "submitter": "Sebastian Billaudelle", "authors": "Benjamin Cramer, Sebastian Billaudelle, Simeon Kanya, Aron Leibfried,\n  Andreas Gr\\\"ubl, Vitali Karasenko, Christian Pehle, Korbinian Schreiber,\n  Yannik Stradmann, Johannes Weis, Johannes Schemmel, Friedemann Zenke", "title": "Surrogate gradients for analog neuromorphic computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To rapidly process temporal information at a low metabolic cost, biological\nneurons integrate inputs as an analog sum but communicate with spikes, binary\nevents in time. Analog neuromorphic hardware uses the same principles to\nemulate spiking neural networks with exceptional energy-efficiency. However,\ninstantiating high-performing spiking networks on such hardware remains a\nsignificant challenge due to device mismatch and the lack of efficient training\nalgorithms. Here, we introduce a general in-the-loop learning framework based\non surrogate gradients that resolves these issues. Using the BrainScaleS-2\nneuromorphic system, we show that learning self-corrects for device mismatch\nresulting in competitive spiking network performance on both vision and speech\nbenchmarks. Our networks display sparse spiking activity with, on average, far\nless than one spike per hidden neuron and input, perform inference at rates of\nup to 85 k frames/second, and consume less than 200 mW. In summary, our work\nsets several new benchmarks for low-energy spiking network processing on analog\nneuromorphic hardware and paves the way for future on-chip learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:45:12 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:52:22 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 14:13:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cramer", "Benjamin", ""], ["Billaudelle", "Sebastian", ""], ["Kanya", "Simeon", ""], ["Leibfried", "Aron", ""], ["Gr\u00fcbl", "Andreas", ""], ["Karasenko", "Vitali", ""], ["Pehle", "Christian", ""], ["Schreiber", "Korbinian", ""], ["Stradmann", "Yannik", ""], ["Weis", "Johannes", ""], ["Schemmel", "Johannes", ""], ["Zenke", "Friedemann", ""]]}, {"id": "2006.07352", "submitter": "Eli Shlizerman", "authors": "Jimin Kim, Eli Shlizerman", "title": "Deep Reinforcement Learning for Neural Control", "comments": "Please see the associated Video at: https://youtu.be/ixsUMfb9m_U", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology for control of neural circuits based on deep\nreinforcement learning. Our approach achieves aimed behavior by generating\nexternal continuous stimulation of existing neural circuits (neuromodulation\ncontrol) or modulations of neural circuits architecture (connectome control).\nBoth forms of control are challenging due to nonlinear and recurrent complexity\nof neural activity. To infer candidate control policies, our approach maps\nneural circuits and their connectome into a grid-world like setting and infers\nthe actions needed to achieve aimed behavior. The actions are inferred by\nadaptation of deep Q-learning methods known for their robust performance in\nnavigating grid-worlds. We apply our approach to the model of \\textit{C.\nelegans} which simulates the full somatic nervous system with muscles and body.\nOur framework successfully infers neuropeptidic currents and synaptic\narchitectures for control of chemotaxis. Our findings are consistent with in\nvivo measurements and provide additional insights into neural control of\nchemotaxis. We further demonstrate the generality and scalability of our\nmethods by inferring chemotactic neural circuits from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:41:12 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Kim", "Jimin", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.07412", "submitter": "Eli Shlizerman", "authors": "Yang Zheng, Jinlin Xiang, Kun Su, Eli Shlizerman", "title": "BI-MAML: Balanced Incremental Approach for Meta Learning", "comments": "Please see associated video at: https://youtu.be/4qlb-iG5SFo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Balanced Incremental Model Agnostic Meta Learning system\n(BI-MAML) for learning multiple tasks. Our method implements a meta-update rule\nto incrementally adapt its model to new tasks without forgetting old tasks.\nSuch a capability is not possible in current state-of-the-art MAML approaches.\nThese methods effectively adapt to new tasks, however, suffer from\n'catastrophic forgetting' phenomena, in which new tasks that are streamed into\nthe model degrade the performance of the model on previously learned tasks. Our\nsystem performs the meta-updates with only a few-shots and can successfully\naccomplish them. Our key idea for achieving this is the design of balanced\nlearning strategy for the baseline model. The strategy sets the baseline model\nto perform equally well on various tasks and incorporates time efficiency. The\nbalanced learning strategy enables BI-MAML to both outperform other\nstate-of-the-art models in terms of classification accuracy for existing tasks\nand also accomplish efficient adaption to similar new tasks with less required\nshots. We evaluate BI-MAML by conducting comparisons on two common benchmark\ndatasets with multiple number of image classification tasks. BI-MAML\nperformance demonstrates advantages in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:28:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zheng", "Yang", ""], ["Xiang", "Jinlin", ""], ["Su", "Kun", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.07429", "submitter": "Kristopher Jensen", "authors": "Kristopher T. Jensen, Ta-Chu Kao, Marco Tripodi, and Guillaume\n  Hennequin", "title": "Manifold GPLVMs for discovering non-Euclidean latent structure in neural\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in neuroscience is to elucidate the collective neural\nrepresentations of behaviorally important variables such as head direction,\nspatial location, upcoming movements, or mental spatial transformations. Often,\nthese latent variables are internal constructs not directly accessible to the\nexperimenter. Here, we propose a new probabilistic latent variable model to\nsimultaneously identify the latent state and the way each neuron contributes to\nits representation in an unsupervised way. In contrast to previous models which\nassume Euclidean latent spaces, we embrace the fact that latent states often\nbelong to symmetric manifolds such as spheres, tori, or rotation groups of\nvarious dimensions. We therefore propose the manifold Gaussian process latent\nvariable model (mGPLVM), where neural responses arise from (i) a shared latent\nvariable living on a specific manifold, and (ii) a set of non-parametric tuning\ncurves determining how each neuron contributes to the representation.\nCross-validated comparisons of models with different topologies can be used to\ndistinguish between candidate manifolds, and variational inference enables\nquantification of uncertainty. We demonstrate the validity of the approach on\nseveral synthetic datasets, as well as on calcium recordings from the ellipsoid\nbody of Drosophila melanogaster and extracellular recordings from the mouse\nanterodorsal thalamic nucleus. These circuits are both known to encode head\ndirection, and mGPLVM correctly recovers the ring topology expected from neural\npopulations representing a single angular variable.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:08:54 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 15:06:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jensen", "Kristopher T.", ""], ["Kao", "Ta-Chu", ""], ["Tripodi", "Marco", ""], ["Hennequin", "Guillaume", ""]]}, {"id": "2006.07882", "submitter": "Bastian Rieck", "authors": "Bastian Rieck, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy\n  Wolf, Nicholas Turk-Browne, Smita Krishnaswamy", "title": "Uncovering the Topology of Time-Varying fMRI Data using Cubical\n  Persistence", "comments": "Accepted at the Conference on Neural Information Processing Systems\n  (NeurIPS) 2020; camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.IV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is a crucial technology for\ngaining insights into cognitive processes in humans. Data amassed from fMRI\nmeasurements result in volumetric data sets that vary over time. However,\nanalysing such data presents a challenge due to the large degree of noise and\nperson-to-person variation in how information is represented in the brain. To\naddress this challenge, we present a novel topological approach that encodes\neach time point in an fMRI data set as a persistence diagram of topological\nfeatures, i.e. high-dimensional voids present in the data. This representation\nnaturally does not rely on voxel-by-voxel correspondence and is robust to\nnoise. We show that these time-varying persistence diagrams can be clustered to\nfind meaningful groupings between participants, and that they are also useful\nin studying within-subject brain state trajectories of subjects performing a\nparticular task. Here, we apply both clustering and trajectory analysis\ntechniques to a group of participants watching the movie 'Partly Cloudy'. We\nobserve significant differences in both brain state trajectories and overall\ntopological activity between adults and children watching the same movie.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 12:29:37 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 17:35:21 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Rieck", "Bastian", ""], ["Yates", "Tristan", ""], ["Bock", "Christian", ""], ["Borgwardt", "Karsten", ""], ["Wolf", "Guy", ""], ["Turk-Browne", "Nicholas", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "2006.07991", "submitter": "Arturo Deza", "authors": "Arturo Deza and Talia Konkle", "title": "Emergent Properties of Foveated Perceptual Systems", "comments": "An updated pre-print. Currently under review at NeurIPS 2021. Themes:\n  Foveation, Machine Perception & Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:34:44 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:23:00 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 21:21:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Deza", "Arturo", ""], ["Konkle", "Talia", ""]]}, {"id": "2006.08115", "submitter": "Qianyi Li", "authors": "Qianyi Li, Cengiz Pehlevan", "title": "Minimax Dynamics of Optimally Balanced Spiking Networks of Excitatory\n  and Inhibitory Neurons", "comments": "There was a typo in Eq. 3 for the definition of firing rates, where\n  we had e^{-(t-t')/\\tau_E} in the integrand, which should be e^{-t'/\\tau_E},\n  it is corrected in this version", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Excitation-inhibition (E-I) balance is ubiquitously observed in the cortex.\nRecent studies suggest an intriguing link between balance on fast timescales,\ntight balance, and efficient information coding with spikes. We further this\nconnection by taking a principled approach to optimal balanced networks of\nexcitatory (E) and inhibitory (I) neurons. By deriving E-I spiking neural\nnetworks from greedy spike-based optimizations of constrained minimax\nobjectives, we show that tight balance arises from correcting for deviations\nfrom the minimax optima. We predict specific neuron firing rates in the network\nby solving the minimax problem, going beyond statistical theories of balanced\nnetworks. Finally, we design minimax objectives for reconstruction of an input\nsignal, associative memory, and storage of manifold attractors, and derive from\nthem E-I networks that perform the computation. Overall, we present a novel\nnormative modeling approach for spiking E-I networks, going beyond the\nwidely-used energy minimizing networks that violate Dale's law. Our networks\ncan be used to model cortical circuits and computations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 03:54:32 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 22:17:48 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 19:43:48 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 20:04:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Qianyi", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "2006.08266", "submitter": "Vidya Sagar Sharma", "authors": "Vidya Sagar Sharma, Piyush Srivastava", "title": "The PSPACE-hardness of understanding neural circuits", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, an important aspect of understanding the function of a\nneural circuit is to determine which, if any, of the neurons in the circuit are\nvital for the biological behavior governed by the neural circuit. A similar\nproblem is to determine whether a given small set of neurons may be enough for\nthe behavior to be displayed, even if all other neurons in the circuit are\ndeactivated. Such a subset of neurons forms what is called a degenerate circuit\nfor the behavior being studied.\n  Recent advances in experimental techniques have provided researchers with\ntools to activate and deactivate subsets of neurons with a very high\nresolution, even in living animals. The data collected from such experiments\nmay be of the following form: when a given subset of neurons is deactivated, is\nthe behavior under study observed? This setting leads to the algorithmic\nquestion of determining the minimal vital or degenerate sets of neurons when\none is given as input a description of the neural circuit. The algorithmic\nproblem entails both figuring out which subsets of neurons should be perturbed\n(activated/deactivated), and then using the data from those perturbations to\ndetermine the minimal vital or degenerate sets. Given the large number of\npossible perturbations, and the recurrent nature of neural circuits, the\npossibility of a combinatorial explosion in such an approach has been\nrecognized in the biology and the neuroscience literature.\n  In this paper, we prove that the problems of finding minimal or minimum-size\ndegenerate sets, and of finding the set of vital neurons, of a neural circuit\ngiven as input, are in fact PSPACE-hard. More importantly, we prove our\nhardness results by showing that a simpler problem, that of simulating such\nneural circuits, is itself PSPACE-hard.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 10:05:51 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:14:48 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 01:59:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Sharma", "Vidya Sagar", ""], ["Srivastava", "Piyush", ""]]}, {"id": "2006.08537", "submitter": "Pau Vilimelis Aceituno", "authors": "Pau Vilimelis Aceituno", "title": "Resonances induced by Spiking Time Dependent Plasticity", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural populations exposed to a certain stimulus learn to represent it\nbetter. However, the process that leads local, self-organized rules to do so is\nunclear. We address the question of how can a neural periodic input be learned\nand use the Differential Hebbian Learning framework, coupled with a homeostatic\nmechanism to derive two self-consistency equations that lead to increased\nresponses to the same stimulus. Although all our simulations are done with\nsimple Leaky-Integrate and Fire neurons and standard Spiking Time Dependent\nPlasticity learning rules, our results can be easily interpreted in terms of\nrates and population codes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:41:51 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Aceituno", "Pau Vilimelis", ""]]}, {"id": "2006.08655", "submitter": "Luigi Acerbi", "authors": "Luigi Acerbi", "title": "Variational Bayesian Monte Carlo with Noisy Likelihoods", "comments": "To appear in Advances in Neural Information Processing Systems 33\n  (NeurIPS 2020). 26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian Monte Carlo (VBMC) is a recently introduced framework\nthat uses Gaussian process surrogates to perform approximate Bayesian inference\nin models with black-box, non-cheap likelihoods. In this work, we extend VBMC\nto deal with noisy log-likelihood evaluations, such as those arising from\nsimulation-based models. We introduce new `global' acquisition functions, such\nas expected information gain (EIG) and variational interquantile range (VIQR),\nwhich are robust to noise and can be efficiently evaluated within the VBMC\nsetting. In a novel, challenging, noisy-inference benchmark comprising of a\nvariety of models with real datasets from computational and cognitive\nneuroscience, VBMC+VIQR achieves state-of-the-art performance in recovering the\nground-truth posteriors and model evidence. In particular, our method vastly\noutperforms `local' acquisition functions and other surrogate-based inference\nmethods while keeping a small algorithmic cost. Our benchmark corroborates VBMC\nas a general-purpose technique for sample-efficient black-box Bayesian\ninference also with noisy models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:06:18 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:30:43 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 05:54:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Acerbi", "Luigi", ""]]}, {"id": "2006.08691", "submitter": "Elif K\\\"oksal Ers\\\"oz", "authors": "Elif K\\\"oksal Ers\\\"oz, Julien Modolo, Fabrice Bartolomei, Fabrice\n  Wendling", "title": "Neural mass modeling of slow-fast dynamics of seizure initiation and\n  abortion", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1008430", "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a dynamic and complex neurological disease affecting about 1% of\nthe worldwide population, among which 30% of the patients are drug-resistant.\nEpilepsy is characterized by recurrent episodes of paroxysmal neural discharges\n(the so-called seizures), which manifest themselves through a large-amplitude\nrhythmic activity observed in depth-EEG recordings, in particular in local\nfield potentials (LFPs). The signature characterizing the transition to\nseizures involves complex oscillatory patterns, which could serve as a marker\nto prevent seizure initiation by triggering appropriate therapeutic\nneurostimulation methods. To investigate such protocols, neurophysiological\nlumped-parameter models at the mesoscopic scale, namely neural mass models, are\npowerful tools that not only mimic the LFP signals but also give insights on\nthe neural mechanisms related to different stages of seizures. Here, we analyze\nthe multiple time-scale dynamics of a neural mass model and explain the\nunderlying structure of the complex oscillations observed before seizure\ninitiation. We investigate population-specific effects of the stimulation and\nthe dependence of stimulation parameters on synaptic timescales. In particular,\nwe show that intermediate stimulation frequencies (>20 Hz) can abort seizures\nif the timescale difference is pronounced. Those results have the potential in\nthe design of therapeutic brain stimulation protocols based on the\nneurophysiological properties of tissue.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:00:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ers\u00f6z", "Elif K\u00f6ksal", ""], ["Modolo", "Julien", ""], ["Bartolomei", "Fabrice", ""], ["Wendling", "Fabrice", ""]]}, {"id": "2006.08771", "submitter": "Philipp H\\\"ovel", "authors": "Thomas Maertens, Eckehard Sch\\\"oll, Jorge Ruiz, Philipp H\\\"ovel", "title": "Multilayer network analysis of C. elegans: Looking into the locomotory\n  circuitry", "comments": "32 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how locomotory behavior is generated in the brain focusing on\nthe paradigmatic connectome of nematode Caenorhabditis elegans (C. elegans) and\non neuronal activity patterns that control forward locomotion. We map the\nneuronal network of the worm as a multilayer network that takes into account\nvarious neurotransmitters and neuropeptides. Using logistic regression\nanalysis, we predict the neurons of the locomotory subnetwork. Combining\nHindmarsh-Rose equations for neuronal activity with a leaky integrator model\nfor muscular activity, we study the dynamics within this subnetwork and predict\nthe forward locomotion of the worm using a harmonic wave model. The application\nof time-delayed feedback control reveals synchronization effects that\ncontribute to a coordinated locomotion of C. elegans. Analyzing the\nsynchronicity when the activity of certain neurons is silenced informs us about\ntheir significance for a coordinated locomotory behavior. Since the information\nprocessing is the same in humans and C. elegans, the study of the locomotory\ncircuitry provides new insights for understanding how the brain generates\nmotion behavior.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 21:09:00 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Maertens", "Thomas", ""], ["Sch\u00f6ll", "Eckehard", ""], ["Ruiz", "Jorge", ""], ["H\u00f6vel", "Philipp", ""]]}, {"id": "2006.09454", "submitter": "Wenxing Hu", "authors": "Wenxing Hu, Xianghe Meng, Yuntong Bai, Aiying Zhang, Biao Cai, Gemeng\n  Zhang, Tony W. Wilson, Julia M. Stephen, Vince D. Calhoun, Yu-Ping Wang", "title": "Interpretable multimodal fusion networks reveal mechanisms of brain\n  cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal fusion benefits disease diagnosis by providing a more\ncomprehensive perspective. Developing algorithms is challenging due to data\nheterogeneity and the complex within- and between-modality associations.\nDeep-network-based data-fusion models have been developed to capture the\ncomplex associations and the performance in diagnosis has been improved\naccordingly. Moving beyond diagnosis prediction, evaluation of disease\nmechanisms is critically important for biomedical research. Deep-network-based\ndata-fusion models, however, are difficult to interpret, bringing about\ndifficulties for studying biological mechanisms. In this work, we develop an\ninterpretable multimodal fusion model, namely gCAM-CCL, which can perform\nautomated diagnosis and result interpretation simultaneously. The gCAM-CCL\nmodel can generate interpretable activation maps, which quantify pixel-level\ncontributions of the input features. This is achieved by combining intermediate\nfeature maps using gradient-based weights. Moreover, the estimated activation\nmaps are class-specific, and the captured cross-data associations are\ninterest/label related, which further facilitates class-specific analysis and\nbiological mechanism analysis. We validate the gCAM-CCL model on a brain\nimaging-genetic study, and show gCAM-CCL's performed well for both\nclassification and mechanism analysis. Mechanism analysis suggests that during\ntask-fMRI scans, several object recognition related regions of interests (ROIs)\nare first activated and then several downstream encoding ROIs get involved.\nResults also suggest that the higher cognition performing group may have\nstronger neurotransmission signaling while the lower cognition performing group\nmay have problem in brain/neuron development, resulting from genetic\nvariations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:52:50 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hu", "Wenxing", ""], ["Meng", "Xianghe", ""], ["Bai", "Yuntong", ""], ["Zhang", "Aiying", ""], ["Cai", "Biao", ""], ["Zhang", "Gemeng", ""], ["Wilson", "Tony W.", ""], ["Stephen", "Julia M.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2006.09549", "submitter": "Jack Lindsey", "authors": "Jack Lindsey, Ashok Litwin-Kumar", "title": "Learning to Learn with Feedback and Local Plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in biologically inspired alternatives to backpropagation is driven\nby the desire to both advance connections between deep learning and\nneuroscience and address backpropagation's shortcomings on tasks such as\nonline, continual learning. However, local synaptic learning rules like those\nemployed by the brain have so far failed to match the performance of\nbackpropagation in deep networks. In this study, we employ meta-learning to\ndiscover networks that learn using feedback connections and local, biologically\ninspired learning rules. Importantly, the feedback connections are not tied to\nthe feedforward weights, avoiding biologically implausible weight transport.\nOur experiments show that meta-trained networks effectively use feedback\nconnections to perform online credit assignment in multi-layer architectures.\nSurprisingly, this approach matches or exceeds a state-of-the-art\ngradient-based online meta-learning algorithm on regression and classification\ntasks, excelling in particular at continual learning. Analysis of the weight\nupdates employed by these models reveals that they differ qualitatively from\ngradient descent in a way that reduces interference between updates. Our\nresults suggest the existence of a class of biologically plausible learning\nmechanisms that not only match gradient descent-based learning, but also\novercome its limitations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 22:49:07 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Lindsey", "Jack", ""], ["Litwin-Kumar", "Ashok", ""]]}, {"id": "2006.09885", "submitter": "Diyuan Lu", "authors": "Diyuan Lu, Sebastian Bauer, Valentin Neubert, Lara Sophie Costard,\n  Felix Rosenow, Jochen Triesch", "title": "Staging Epileptogenesis with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a common neurological disorder characterized by recurrent\nseizures accompanied by excessive synchronous brain activity. The process of\nstructural and functional brain alterations leading to increased seizure\nsusceptibility and eventually spontaneous seizures is called epileptogenesis\n(EPG) and can span months or even years. Detecting and monitoring the\nprogression of EPG could allow for targeted early interventions that could slow\ndown disease progression or even halt its development. Here, we propose an\napproach for staging EPG using deep neural networks and identify potential\nelectroencephalography (EEG) biomarkers to distinguish different phases of EPG.\nSpecifically, continuous intracranial EEG recordings were collected from a\nrodent model where epilepsy is induced by electrical perforant pathway\nstimulation (PPS). A deep neural network (DNN) is trained to distinguish EEG\nsignals from before stimulation (baseline), shortly after the PPS and long\nafter the PPS but before the first spontaneous seizure (FSS). Experimental\nresults show that our proposed method can classify EEG signals from the three\nphases with an average area under the curve (AUC) of 0.93, 0.89, and 0.86. To\nthe best of our knowledge, this represents the first successful attempt to\nstage EPG prior to the FSS using DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:08:12 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Lu", "Diyuan", ""], ["Bauer", "Sebastian", ""], ["Neubert", "Valentin", ""], ["Costard", "Lara Sophie", ""], ["Rosenow", "Felix", ""], ["Triesch", "Jochen", ""]]}, {"id": "2006.09928", "submitter": "Biao Cai", "authors": "Biao Cai, Gemeng Zhang, Aiying Zhang, Li Xiao, Wenxing Hu, Julia M.\n  Stephen, Tony W. Wilson, Vince D. Calhoun, Yu-Ping Wang", "title": "Functional connectome fingerprinting: Identifying individuals and\n  predicting cognitive function via deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic characteristics of functional network connectivity have been\nwidely acknowledged and studied. Both shared and unique information has been\nshown to be present in the connectomes. However, very little has been known\nabout whether and how this common pattern can predict the individual\nvariability of the brain, i.e. \"brain fingerprinting\", which attempts to\nreliably identify a particular individual from a pool of subjects. In this\npaper, we propose to enhance the individual uniqueness based on an autoencoder\nnetwork. More specifically, we rely on the hypothesis that the common neural\nactivities shared across individuals may lessen individual discrimination. By\nreducing contributions from shared activities, inter-subject variability can be\nenhanced. Results show that that refined connectomes utilizing an autoencoder\nwith sparse dictionary learning can successfully distinguish one individual\nfrom the remaining participants with reasonably high accuracy (up to 99:5% for\nthe rest-rest pair). Furthermore, high-level cognitive behavior (e.g., fluid\nintelligence, executive function, and language comprehension) can also be\nbetter predicted using refined functional connectivity profiles. As expected,\nthe high-order association cortices contributed more to both individual\ndiscrimination and behavior prediction. The proposed approach provides a\npromising way to enhance and leverage the individualized characteristics of\nbrain networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:15:35 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Cai", "Biao", ""], ["Zhang", "Gemeng", ""], ["Zhang", "Aiying", ""], ["Xiao", "Li", ""], ["Hu", "Wenxing", ""], ["Stephen", "Julia M.", ""], ["Wilson", "Tony W.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2006.10113", "submitter": "Tomas Van Pottelbergh", "authors": "Tomas Van Pottelbergh, Guillaume Drion, Rodolphe Sepulchre", "title": "From biophysical to integrate-and-fire modelling", "comments": "This is the authors' final version. The article has been accepted for\n  publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a methodology to extract a low-dimensional\nintegrate-and-fire model from an arbitrarily detailed single-compartment\nbiophysical model. The method aims at relating the modulation of maximal\nconductance parameters in the biophysical model to the modulation of parameters\nin the proposed integrate-and-fire model. The approach is illustrated on two\nwell-documented examples of cellular neuromodulation: the transition between\nType I and Type II excitability and the transition between spiking and\nbursting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:26:15 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 16:03:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Van Pottelbergh", "Tomas", ""], ["Drion", "Guillaume", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "2006.10122", "submitter": "Carter Johnson", "authors": "Carter L. Johnson, Timothy J. Lewis, Robert D. Guy", "title": "Neuromechanical Mechanisms of Gait Adaptation in C. elegans: Relative\n  Roles of Neural and Mechanical Coupling", "comments": "Pages 25, Figures 14. Submitted to SIAM Journal on Applied Dynamical\n  Systems", "journal-ref": "SIAM Journal on Applied Dynamical Systems, 2021, Vol. 20, No. 2 :\n  pp. 1022-1052", "doi": "10.1137/20M1346122", "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding principles of neurolocomotion requires the synthesis of neural\nactivity, sensory feedback, and biomechanics. The nematode \\textit{C. elegans}\nis an ideal model organism for studying locomotion in an integrated\nneuromechanical setting because its neural circuit has a well-characterized\nmodular structure and its undulatory forward swimming gait adapts to the\nsurrounding fluid with a shorter wavelength in higher viscosity environments.\nThis adaptive behavior emerges from the neural modules interacting through a\ncombination of mechanical forces, neuronal coupling, and sensory feedback\nmechanisms. However, the relative contributions of these coupling modes to gait\nadaptation are not understood. The model consists of repeated neuromechanical\nmodules that are coupled through the mechanics of the body, short-range\nproprioception, and gap-junctions. The model captures the experimentally\nobserved gait adaptation over a wide range of mechanical parameters, provided\nthat the muscle response to input from the nervous system is faster than the\nbody response to changes in internal and external forces. The modularity of the\nmodel allows the use of the theory of weakly coupled oscillators to identify\nthe relative roles of body mechanics, gap-junctional coupling, and\nproprioceptive coupling in coordinating the undulatory gait. The analysis shows\nthat the wavelength of body undulations is set by the relative strengths of\nthese three coupling forms. In a low-viscosity fluid environment, the\ncompetition between gap-junctions and proprioception produces a long wavelength\nundulation, which is only achieved in the model with sufficiently strong\ngap-junctional coupling.The experimentally observed decrease in wavelength in\nresponse to increasing fluid viscosity is the result of an increase in the\nrelative strength of mechanical coupling, which promotes a short wavelength.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:48:39 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 23:51:52 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 20:51:01 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 19:15:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Johnson", "Carter L.", ""], ["Lewis", "Timothy J.", ""], ["Guy", "Robert D.", ""]]}, {"id": "2006.10212", "submitter": "Yu Takagi", "authors": "Yu Takagi, Steven W. Kennerley, Jun-ichiro Hirayama, Laurence T. Hunt", "title": "Demixed shared component analysis of neural population data from\n  multiple brain areas", "comments": "Accepted at the conference on Neural Information Processing Systems\n  (NeurIPS 2020, spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in neuroscience data acquisition allow for the simultaneous\nrecording of large populations of neurons across multiple brain areas while\nsubjects perform complex cognitive tasks. Interpreting these data requires us\nto index how task-relevant information is shared across brain regions, but this\nis often confounded by the mixing of different task parameters at the single\nneuron level. Here, inspired by a method developed for a single brain area, we\nintroduce a new technique for demixing variables across multiple brain areas,\ncalled demixed shared component analysis (dSCA). dSCA decomposes population\nactivity into a few components, such that the shared components capture the\nmaximum amount of shared information across brain regions while also depending\non relevant task parameters. This yields interpretable components that express\nwhich variables are shared between different brain regions and when this\ninformation is shared across time. To illustrate our method, we reanalyze two\ndatasets recorded during decision-making tasks in rodents and macaques. We find\nthat dSCA provides new insights into the shared computation between different\nbrain areas in these datasets, relating to several different aspects of\ndecision formation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:13:12 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 10:24:35 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Takagi", "Yu", ""], ["Kennerley", "Steven W.", ""], ["Hirayama", "Jun-ichiro", ""], ["Hunt", "Laurence T.", ""]]}, {"id": "2006.10259", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Jianwen Xie, Xue-Xin Wei, Song-Chun Zhu, Ying Nian Wu", "title": "On Path Integration of Grid Cells: Group Representation and Isotropic\n  Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how grid cells perform path integration calculations remains a\nfundamental problem. In this paper, we conduct theoretical analysis of a\ngeneral representation model of path integration by grid cells, where the 2D\nself-position is encoded as a higher dimensional vector, and the 2D self-motion\nis represented by a general transformation of the vector. We identify two\nconditions on the transformation. One is a group representation condition that\nis necessary for path integration. The other is an isotropic scaling condition\nthat ensures locally conformal embedding, so that the error in the vector\nrepresentation translates proportionally to the error in the 2D self-position.\nThen we investigate the simplest transformation, i.e., the linear\ntransformation, uncover its explicit algebraic and geometric structure as\nmatrix Lie group of rotation, and establish the connection between the\nisotropic scaling condition and hexagon grid patterns of grid cells under the\nlinear transformation. Finally, with our optimization-based approach, we manage\nto learn hexagon grid patterns that share similar properties of the grid cells\nin the rodent brain. The learned model is capable of accurate long distance\npath integration.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:44:35 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 19:11:46 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 07:52:16 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 22:15:14 GMT"}, {"version": "v5", "created": "Tue, 8 Jun 2021 22:55:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gao", "Ruiqi", ""], ["Xie", "Jianwen", ""], ["Wei", "Xue-Xin", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2006.10357", "submitter": "Miroslav Andjelkovic", "authors": "Miroslav Andjelkovic, Bosiljka Tadic, Roderick Melnik", "title": "The topology of higher-order complexes associated with brain-function\n  hubs in human connectomes", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order connectivity in complex systems described by simplexes of\ndifferent orders provides a geometry for simplex-based dynamical variables and\ninteractions. Simplicial complexes that constitute a functional geometry of the\nhuman connectome can be crucial for the brain complex dynamics. In this\ncontext, the best-connected brain areas, designated as hub nodes, play a\ncentral role in supporting integrated brain function. Here, we study the\nstructure of simplicial complexes attached to eight global hubs in the female\nand male connectomes and identify the core networks among the affected brain\nregions. These eight hubs (Putamen, Caudate, Hippocampus and Thalamus-Proper in\nthe left and right cerebral hemisphere) are the highest-ranking according to\ntheir topological dimension, defined as the number of simplexes of all orders\nin which the node participates. Furthermore, we analyse the weight-dependent\nheterogeneity of simplexes. We demonstrate changes in the structure of\nidentified core networks and topological entropy when the threshold weight is\ngradually increased. These results highlight the role of higher-order\ninteractions in human brain networks and provide additional evidence for\n(dis)similarity between the female and male connectomes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 08:29:35 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Andjelkovic", "Miroslav", ""], ["Tadic", "Bosiljka", ""], ["Melnik", "Roderick", ""]]}, {"id": "2006.10723", "submitter": "Bronte Mckeown", "authors": "Bront\\\"e Mckeown, Will H. Strawson, Hao-Ting Wang, Theodoros\n  Karapanagiotidis, Reinder Vos de Wael, Oualid Benkarim, Adam Turnbull, Daniel\n  Margulies, Elizabeth Jefferies, Cade McCall, Boris Bernhardt and Jonathan\n  Smallwood", "title": "The relationship between individual variation in macroscale functional\n  gradients and distinct aspects of ongoing thought", "comments": "29 pages, 5 in-text figures, 1 in-text table, 3 supplementary\n  figures, 2 supplementary tables, accepted for publication in Neuroimage on\n  17/06/20", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary accounts of ongoing thought recognise it as a heterogeneous and\nmultidimensional construct, varying in both form and content. An emerging body\nof evidence demonstrates that distinct types of experience are associated with\nunique neurocognitive profiles, that can be described at the whole-brain level\nas interactions between multiple large-scale networks. The current study sought\nto explore the possibility that whole-brain functional connectivity patterns at\nrest may be meaningfully related to patterns of ongoing thought that occurred\nover this period. Participants underwent resting-state functional magnetic\nresonance imaging (rs-fMRI) followed by a questionnaire retrospectively\nassessing the content and form of their ongoing thoughts during the scan. A\nnon-linear dimension reduction algorithm was applied to the rs-fMRI data to\nidentify components explaining the greatest variance in whole-brain\nconnectivity patterns, and ongoing thought patterns during the resting-state\nwere measured retrospectively at the end of the scan. Multivariate analyses\nrevealed that individuals for whom the connectivity of the sensorimotor system\nwas maximally distinct from the visual system were most likely to report\nthoughts related to finding solutions to problems or goals and least likely to\nreport thoughts related to the past. These results add to an emerging\nliterature that suggests that unique patterns of experience are associated with\ndistinct distributed neurocognitive profiles and highlight that unimodal\nsystems may play an important role in this process.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:52:29 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mckeown", "Bront\u00eb", ""], ["Strawson", "Will H.", ""], ["Wang", "Hao-Ting", ""], ["Karapanagiotidis", "Theodoros", ""], ["de Wael", "Reinder Vos", ""], ["Benkarim", "Oualid", ""], ["Turnbull", "Adam", ""], ["Margulies", "Daniel", ""], ["Jefferies", "Elizabeth", ""], ["McCall", "Cade", ""], ["Bernhardt", "Boris", ""], ["Smallwood", "Jonathan", ""]]}, {"id": "2006.10811", "submitter": "Ari Benjamin", "authors": "Ari S. Benjamin and Konrad P. Kording", "title": "Learning to infer in recurrent biological networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A popular theory of perceptual processing holds that the brain learns both a\ngenerative model of the world and a paired recognition model using variational\nBayesian inference. Most hypotheses of how the brain might learn these models\nassume that neurons in a population are conditionally independent given their\ncommon inputs. This simplification is likely not compatible with the type of\nlocal recurrence observed in the brain. Seeking an alternative that is\ncompatible with complex inter-dependencies yet consistent with known biology,\nwe argue here that the cortex may learn with an adversarial algorithm. Many\nobservable symptoms of this approach would resemble known neural phenomena,\nincluding wake/sleep cycles and oscillations that vary in magnitude with\nsurprise, and we describe how further predictions could be tested. We\nillustrate the idea on recurrent neural networks trained to model image and\nvideo datasets. This framework for learning brings variational inference closer\nto neuroscience and yields multiple testable hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 19:04:47 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:33:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Benjamin", "Ari S.", ""], ["Kording", "Konrad P.", ""]]}, {"id": "2006.11035", "submitter": "Stefano Melacci", "authors": "Lapo Faggi, Alessandro Betti, Dario Zanca, Stefano Melacci, Marco Gori", "title": "Wave Propagation of Visual Stimuli in Focus of Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast reactions to changes in the surrounding visual environment require\nefficient attention mechanisms to reallocate computational resources to most\nrelevant locations in the visual field. While current computational models keep\nimproving their predictive ability thanks to the increasing availability of\ndata, they still struggle approximating the effectiveness and efficiency\nexhibited by foveated animals. In this paper, we present a\nbiologically-plausible computational model of focus of attention that exhibits\nspatiotemporal locality and that is very well-suited for parallel and\ndistributed implementations. Attention emerges as a wave propagation process\noriginated by visual stimuli corresponding to details and motion information.\nThe resulting field obeys the principle of \"inhibition of return\" so as not to\nget stuck in potential holes. An accurate experimentation of the model shows\nthat it achieves top level performance in scanpath prediction tasks. This can\neasily be understood at the light of a theoretical result that we establish in\nthe paper, where we prove that as the velocity of wave propagation goes to\ninfinity, the proposed model reduces to recently proposed state of the art\ngravitational models of focus of attention.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:33:21 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Faggi", "Lapo", ""], ["Betti", "Alessandro", ""], ["Zanca", "Dario", ""], ["Melacci", "Stefano", ""], ["Gori", "Marco", ""]]}, {"id": "2006.11036", "submitter": "Friedrich Schuessler", "authors": "Friedrich Schuessler, Francesca Mastrogiuseppe, Alexis Dubreuil,\n  Srdjan Ostojic, Omri Barak", "title": "The interplay between randomness and structure during learning in RNNs", "comments": "Presented at Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) trained on low-dimensional tasks have been\nwidely used to model functional biological networks. However, the solutions\nfound by learning and the effect of initial connectivity are not well\nunderstood. Here, we examine RNNs trained using gradient descent on different\ntasks inspired by the neuroscience literature. We find that the changes in\nrecurrent connectivity can be described by low-rank matrices, despite the\nunconstrained nature of the learning algorithm. To identify the origin of the\nlow-rank structure, we turn to an analytically tractable setting: training a\nlinear RNN on a simplified task. We show how the low-dimensional task structure\nleads to low-rank changes to connectivity. This low-rank structure allows us to\nexplain and quantify the phenomenon of accelerated learning in the presence of\nrandom initial connectivity. Altogether, our study opens a new perspective to\nunderstanding trained RNNs in terms of both the learning process and the\nresulting network structure.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:40:19 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 17:57:31 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 13:18:02 GMT"}, {"version": "v4", "created": "Thu, 13 May 2021 19:14:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Schuessler", "Friedrich", ""], ["Mastrogiuseppe", "Francesca", ""], ["Dubreuil", "Alexis", ""], ["Ostojic", "Srdjan", ""], ["Barak", "Omri", ""]]}, {"id": "2006.11099", "submitter": "Michael G. M\\\"uller", "authors": "Agnes Korcsak-Gorzo, Michael G. M\\\"uller, Andreas Baumbach, Luziwei\n  Leng, Oliver Julien Breitwieser, Sacha J. van Albada, Walter Senn, Karlheinz\n  Meier, Robert Legenstein, Mihai A. Petrovici", "title": "Cortical oscillations implement a backbone for sampling-based\n  computation in spiking neural networks", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brains need to deal with an uncertain world. Often, this requires visiting\nmultiple interpretations of the available information or multiple solutions to\nan encountered problem. This gives rise to the so-called mixing problem: since\nall of these \"valid\" states represent powerful attractors, but between\nthemselves can be very dissimilar, switching between such states can be\ndifficult. We propose that cortical oscillations can be effectively used to\novercome this challenge. By acting as an effective temperature, background\nspiking activity modulates exploration. Rhythmic changes induced by cortical\noscillations can then be interpreted as a form of simulated tempering. We\nprovide a rigorous mathematical discussion of this link and study some of its\nphenomenological implications in computer simulations. This identifies a new\ncomputational role of cortical oscillations and connects them to various\nphenomena in the brain, such as sampling-based probabilistic inference, memory\nreplay, multisensory cue combination and place cell flickering.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 12:18:43 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 17:18:34 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 16:26:15 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 20:24:02 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Korcsak-Gorzo", "Agnes", ""], ["M\u00fcller", "Michael G.", ""], ["Baumbach", "Andreas", ""], ["Leng", "Luziwei", ""], ["Breitwieser", "Oliver Julien", ""], ["van Albada", "Sacha J.", ""], ["Senn", "Walter", ""], ["Meier", "Karlheinz", ""], ["Legenstein", "Robert", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "2006.11254", "submitter": "Osame Kinouchi", "authors": "Emilio F. Galera and Osame Kinouchi", "title": "Physics of Psychophysics: two coupled square lattices of spiking neurons\n  have huge dynamic range at criticality", "comments": "22 pages, 7 figures, accepted by Physical Review Research", "journal-ref": "Phys. Rev. Research 2, 033057 (2020)", "doi": "10.1103/PhysRevResearch.2.033057", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Psychophysics try to relate physical input magnitudes to psychological or\nneural correlates. Microscopic models to account for macroscopic psychophysical\nlaws, in the sense of statistical physics, are an almost unexplored area. Here\nwe examine a sensory epithelium composed of two connected square lattices of\nstochastic integrate-and-fire cells. With one square lattice we obtain a\nStevens's law $\\rho \\propto h^m$ with Stevens's exponent $m = 0.254$ and a\nsigmoidal saturation, where $\\rho$ is the neuronal network activity and $h$ is\nthe input intensity (external field). We relate Stevens's power law exponent\nwith the field critical exponent as $m = 1/\\delta_h = \\beta/\\sigma$. We also\nshow that this system pertains to the Directed Percolation (DP) universality\nclass (or perhaps the Compact-DP class). With stacked two layers of square\nlattices, and a fraction of connectivity between the first and second layer, we\nobtain at the output layer $\\rho_ 2 \\propto h^{m_2}$, with $m_2 = 0.08 \\approx\nm^2$, which corresponds to a huge dynamic range. This enhancement of the\ndynamic range only occurs when the layers are close to their critical point.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:24:19 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Galera", "Emilio F.", ""], ["Kinouchi", "Osame", ""]]}, {"id": "2006.11495", "submitter": "Manuel Baltieri Dr", "authors": "Manuel Baltieri, Christopher L. Buckley, Jelle Bruineberg", "title": "Predictions in the eye of the beholder: an active inference account of\n  Watt governors", "comments": "Accepted at ALife 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Active inference introduces a theory describing action-perception loops via\nthe minimisation of variational (and expected) free energy or, under\nsimplifying assumptions, (weighted) prediction error. Recently, active\ninference has been proposed as part of a new and unifying framework in the\ncognitive sciences: predictive processing. Predictive processing is often\nassociated with traditional computational theories of the mind, strongly\nrelying on internal representations presented in the form of generative models\nthought to explain different functions of living and cognitive systems. In this\nwork, we introduce an active inference formulation of the Watt centrifugal\ngovernor, a system often portrayed as the canonical \"anti-representational\"\nmetaphor for cognition. We identify a generative model of a steam engine for\nthe governor, and derive a set of equations describing \"perception\" and\n\"action\" processes as a form of prediction error minimisation. In doing so, we\nfirstly challenge the idea of generative models as explicit internal\nrepresentations for cognitive systems, suggesting that such models serve only\nas implicit descriptions for an observer. Secondly, we consider current\nproposals of predictive processing as a theory of cognition, focusing on some\nof its potential shortcomings and in particular on the idea that virtually any\nsystem admits a description in terms of prediction error minimisation,\nsuggesting that this theory may offer limited explanatory power for cognitive\nsystems. Finally, as a silver lining we emphasise the instrumental role this\nframework can nonetheless play as a mathematical tool for modelling cognitive\narchitectures interpreted in terms of Bayesian (active) inference.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 04:55:39 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 03:02:44 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Baltieri", "Manuel", ""], ["Buckley", "Christopher L.", ""], ["Bruineberg", "Jelle", ""]]}, {"id": "2006.11532", "submitter": "Keita Tokuda", "authors": "Keita Tokuda and Naoya Fujiwara and Akihito Sudo and Yuichi Katori", "title": "Chaos may enhance expressivity in cerebellar granular layer", "comments": null, "journal-ref": "Neural Networks, Volume 136, April 2021, Pages 72-86", "doi": "10.1016/j.neunet.2020.12.020", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent evidence suggests that Golgi cells in the cerebellar granular layer\nare densely connected to each other with massive gap junctions. Here, we\npropose that the massive gap junctions between the Golgi cells contribute to\nthe representational complexity of the granular layer of the cerebellum by\ninducing chaotic dynamics. We construct a model of cerebellar granular layer\nwith diffusion coupling through gap junctions between the Golgi cells, and\nevaluate the representational capability of the network with the reservoir\ncomputing framework. First, we show that the chaotic dynamics induced by\ndiffusion coupling results in complex output patterns containing a wide range\nof frequency components. Second, the long non-recursive time series of the\nreservoir represents the passage of time from an external input. These\nproperties of the reservoir enable mapping different spatial inputs into\ndifferent temporal patterns.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 09:46:28 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tokuda", "Keita", ""], ["Fujiwara", "Naoya", ""], ["Sudo", "Akihito", ""], ["Katori", "Yuichi", ""]]}, {"id": "2006.11569", "submitter": "Haiping Huang", "authors": "Jianwen Zhou, and Haiping Huang", "title": "Weakly-correlated synapses promote dimension reduction in deep neural\n  networks", "comments": "21 pages, 8 figures", "journal-ref": "Phys. Rev. E 103, 012315 (2021)", "doi": "10.1103/PhysRevE.103.012315", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By controlling synaptic and neural correlations, deep learning has achieved\nempirical successes in improving classification performances. How synaptic\ncorrelations affect neural correlations to produce disentangled hidden\nrepresentations remains elusive. Here we propose a simplified model of\ndimension reduction, taking into account pairwise correlations among synapses,\nto reveal the mechanism underlying how the synaptic correlations affect\ndimension reduction. Our theory determines the synaptic-correlation scaling\nform requiring only mathematical self-consistency, for both binary and\ncontinuous synapses. The theory also predicts that weakly-correlated synapses\nencourage dimension reduction compared to their orthogonal counterparts. In\naddition, these synapses slow down the decorrelation process along the network\ndepth. These two computational roles are explained by the proposed mean-field\nequation. The theoretical predictions are in excellent agreement with numerical\nsimulations, and the key features are also captured by a deep learning with\nHebbian rules.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 13:11:37 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhou", "Jianwen", ""], ["Huang", "Haiping", ""]]}, {"id": "2006.11975", "submitter": "Hyunsu Lee", "authors": "Hyunsu Lee", "title": "Toward the biological model of the hippocampus as the successor\n  representation agent", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The hippocampus is an essential brain region for spatial memory and learning.\nRecently, a theoretical model study on the hippocampus applying temporal\ndifference (TD) learning, one of reinforcement learning algorithms, has been\npublished. The theory was inspired by the successor representation (SR)\nlearning algorithms, which decompose value function of TD learning into reward\nand state transition: it argued that the probability of state transition is\nrepresented by the rate of firing of CA1 place cells in the hippocampus. This\ntheory, called predictive map theory, claims that the hippocampus representing\nspace learns the probability of transition from the current state to the future\nstate. The neural correlates of expecting the future state are the firing rates\nof the CA1 place cells. This explanation is plausible for the results recorded\nin behavioral experiments, but it is lacking the neurobiological implications.\n  I tried to add biological implications to the predictive map theory by\nmodifying the SR learning algorithm. Similar with the simultaneous needs of\ninformation of the current and future state in the SR learning algorithm, the\nCA1 pyramidal neurons receive two inputs from CA3 and entorhinal cortex.\nMathematical transformation showed that the SR learning algorithm is equivalent\nto the heterosynaptic plasticity rule. Then I discussed papers that reported\nheterosynaptic plasticity phenomena in CA1; and I compared them with our\ntransformed rule from SR the algorithm. This study provides answers to what is\nthe neurobiological mechanisms of the TD algorithms invented in reinforcement\nlearning, which can be a cornerstone for further researches in neuroscience and\nartificial intelligence fields.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 02:44:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Hyunsu", ""]]}, {"id": "2006.12148", "submitter": "Kuan-Ting Chou", "authors": "Chen-Zhi Su, Kuan-Ting Chou, Hsuan-Pei Huang, Chung-Chuan Lo, and\n  Daw-Wei Wang", "title": "Identification of Neuronal Polarity by Node-Based Machine Learning", "comments": "Manuscript: 18 pages and 9 figures; Appendix: 14 pages, 5 figures,\n  and 2 tables", "journal-ref": null, "doi": "10.1101/2020.06.20.160564", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identify the directions of signal flows in neural networks is one of the most\nimportant stages for understanding the intricate information dynamics of a\nliving brain. Using a dataset of 213 projection neurons distributed in\ndifferent regions of Drosophila brain, we develop a powerful machine learning\nalgorithm: node-based polarity identifier of neurons (NPIN). The proposed model\nis trained by nodal information only and includes both Soma Features (which\ncontain spatial information from a given node to a soma) and Local Features\n(which contain morphological information of a given node). After including the\nspatial correlations between nodal polarities, our NPIN provided extremely high\naccuracy (>96.0%) for the classification of neuronal polarity, even for complex\nneurons with more than two dendrite/axon clusters. Finally, we further apply\nNPIN to classify the neuronal polarity of the blowfly, which has much less\nneuronal data available. Our results demonstrate that NPIN is a powerful tool\nto identify the neuronal polarity of insects and to map out the signal flows in\nthe brain's neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:24:51 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Su", "Chen-Zhi", ""], ["Chou", "Kuan-Ting", ""], ["Huang", "Hsuan-Pei", ""], ["Lo", "Chung-Chuan", ""], ["Wang", "Daw-Wei", ""]]}, {"id": "2006.12253", "submitter": "Victor Geadah", "authors": "Victor Geadah, Giancarlo Kerg, Stefan Horoi, Guy Wolf, Guillaume\n  Lajoie", "title": "Advantages of biologically-inspired adaptive neural activation in RNNs\n  during learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic adaptation in single-neuron response plays a fundamental role in\nneural coding in biological neural networks. Yet, most neural activation\nfunctions used in artificial networks are fixed and mostly considered as an\ninconsequential architecture choice. In this paper, we investigate nonlinear\nactivation function adaptation over the large time scale of learning, and\noutline its impact on sequential processing in recurrent neural networks. We\nintroduce a novel parametric family of nonlinear activation functions, inspired\nby input-frequency response curves of biological neurons, which allows\ninterpolation between well-known activation functions such as ReLU and sigmoid.\nUsing simple numerical experiments and tools from dynamical systems and\ninformation theory, we study the role of neural activation features in learning\ndynamics. We find that activation adaptation provides distinct task-specific\nsolutions and in some cases, improves both learning speed and performance.\nImportantly, we find that optimal activation features emerging from our\nparametric family are considerably different from typical functions used in the\nliterature, suggesting that exploiting the gap between these usual\nconfigurations can help learning. Finally, we outline situations where neural\nactivation adaptation alone may help mitigate changes in input statistics in a\ngiven task, suggesting mechanisms for transfer learning optimization.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:49:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Geadah", "Victor", ""], ["Kerg", "Giancarlo", ""], ["Horoi", "Stefan", ""], ["Wolf", "Guy", ""], ["Lajoie", "Guillaume", ""]]}, {"id": "2006.12323", "submitter": "Xu Ji", "authors": "Xu Ji, Joao Henriques, Tinne Tuytelaars, Andrea Vedaldi", "title": "Automatic Recall Machines: Internal Replay, Continual Learning and the\n  Brain", "comments": "NeurIPS 2020 Workshop on BabyMind", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replay in neural networks involves training on sequential data with memorized\nsamples, which counteracts forgetting of previous behavior caused by\nnon-stationarity. We present a method where these auxiliary samples are\ngenerated on the fly, given only the model that is being trained for the\nassessed objective, without extraneous buffers or generator networks. Instead\nthe implicit memory of learned samples within the assessed model itself is\nexploited. Furthermore, whereas existing work focuses on reinforcing the full\nseen data distribution, we show that optimizing for not forgetting calls for\nthe generation of samples that are specialized to each real training batch,\nwhich is more efficient and scalable. We consider high-level parallels with the\nbrain, notably the use of a single model for inference and recall, the\ndependency of recalled samples on the current environment batch, top-down\nmodulation of activations and learning, abstract recall, and the dependency\nbetween the degree to which a task is learned and the degree to which it is\nrecalled. These characteristics emerge naturally from the method without being\ncontrolled for.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 15:07:06 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:17:33 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 17:58:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ji", "Xu", ""], ["Henriques", "Joao", ""], ["Tuytelaars", "Tinne", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2006.12336", "submitter": "Daniel Van Den Corput", "authors": "Dani\\\"el van den Corput", "title": "Locked in Syndrome Machine Learning Classification using Sentence\n  Comprehension EEG Data", "comments": "Master's Thesis, 16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locked-in Syndrome patients are often misdiagnosed and face pessimistic\nprognosis because of similarities with disorders of consciousness, a lack of\nobjective biomarkers and a difficult-to-recognize pathogenesis. Biomarkers show\npromise in identifying similar conditions, utilizing electroencephalography\n(EEG) data. This data, particularly in the form of event-related potentials\n(ERPs), while successful in varying applications, suffers from methodological\nconstraints and interpretation obstacles. The study documented in this body of\nwork explores a machine learning paradigm with regards to N400 ERP data\nretrieved from a sentence comprehension task to tackle these hindrances and\nproposes a new auxiliary diagnostic tool for LIS and possibly disorders of\nconsciousness. A support vector machine (SVC) and a random forest classifier\n(RF) were able to classify conscious individuals from unconscious ones with\noptimistic performance metrics. Based on these results, the proposed models and\ncontinuations thereof present valuable opportunities for the development of an\nauxiliary diagnostic tool for the classification of LIS patients, aiding\ndiagnosis, improving prognosis, stimulating recovery and reducing mortality\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:09:09 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Corput", "Dani\u00ebl van den", ""]]}, {"id": "2006.12578", "submitter": "Richard Futrell", "authors": "Richard Futrell", "title": "An information-theoretic account of semantic interference in word\n  production", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a computational-level model of semantic interference effects in\nword production. Word production is cast as a rate-distortion problem where an\nagent selects words to minimize a measure of cost while also minimizing the\nresources used to compute the output word based on perceptual input and\nbehavioral goals. I show that similarity-based interference among words arises\nnaturally in this setup, and I present a series of simulations showing that the\nmodel captures some of the key empirical patterns observed in Stroop and\nPicture-Word Interference paradigms. I argue that the rate-distortion account\nof interference provides a high-level formalization of computational principles\nthat are instantiated more mechanistically in existing models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:29:33 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Futrell", "Richard", ""]]}, {"id": "2006.12616", "submitter": "Guido Schillaci", "authors": "Guido Schillaci and Luis Miranda and Uwe Schmidt", "title": "Prediction error-driven memory consolidation for continual learning. On\n  the case of adaptive greenhouse models", "comments": "Revised version. Paper under review, submitted to Springer German\n  Journal on Artificial Intelligence (K\\\"unstliche Intelligenz), Special Issue\n  on Developmental Robotics", "journal-ref": null, "doi": "10.1007/s13218-020-00700-8", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents an adaptive architecture that performs online learning and\nfaces catastrophic forgetting issues by means of episodic memories and\nprediction-error driven memory consolidation. In line with evidences from the\ncognitive science and neuroscience, memories are retained depending on their\ncongruency with the prior knowledge stored in the system. This is estimated in\nterms of prediction error resulting from a generative model. Moreover, this AI\nsystem is transferred onto an innovative application in the horticulture\nindustry: the learning and transfer of greenhouse models. This work presents a\nmodel trained on data recorded from research facilities and transferred to a\nproduction greenhouse.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:22:53 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 11:16:28 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Schillaci", "Guido", ""], ["Miranda", "Luis", ""], ["Schmidt", "Uwe", ""]]}, {"id": "2006.12618", "submitter": "Aiying Zhang", "authors": "Aiying Zhang, Gemeng Zhang, Biao Cai, Tony W. Wilson, Julia M.\n  Stephen, Vince D. Calhoun and Yu-Ping Wang", "title": "A Bayesian incorporated linear non-Gaussian acyclic model for multiple\n  directed graph estimation to study brain emotion circuit development in\n  adolescence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion perception is essential to affective and cognitive development which\ninvolves distributed brain circuits. The ability of emotion identification\nbegins in infancy and continues to develop throughout childhood and\nadolescence. Understanding the development of brain's emotion circuitry may\nhelp us explain the emotional changes observed during adolescence. Our previous\nstudy delineated the trajectory of brain functional connectivity (FC) from late\nchildhood to early adulthood during emotion identification tasks. In this work,\nwe endeavour to deepen our understanding from association to causation. We\nproposed a Bayesian incorporated linear non-Gaussian acyclic model (BiLiNGAM),\nwhich incorporated our previous association model into the prior estimation\npipeline. In particular, it can jointly estimate multiple directed acyclic\ngraphs (DAGs) for multiple age groups at different developmental stages.\nSimulation results indicated more stable and accurate performance over various\nsettings, especially when the sample size was small (high-dimensional cases).\nWe then applied to the analysis of real data from the Philadelphia\nNeurodevelopmental Cohort (PNC). This included 855 individuals aged 8-22 years\nwho were divided into five different adolescent stages. Our network analysis\nrevealed the development of emotion-related intra- and inter- modular\nconnectivity and pinpointed several emotion-related hubs. We further\ncategorized the hubs into two types: in-hubs and out-hubs, as the center of\nreceiving and distributing information. Several unique developmental hub\nstructures and group-specific patterns were also discovered. Our findings help\nprovide a causal understanding of emotion development in the human brain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 21:35:12 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhang", "Aiying", ""], ["Zhang", "Gemeng", ""], ["Cai", "Biao", ""], ["Wilson", "Tony W.", ""], ["Stephen", "Julia M.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2006.12852", "submitter": "Christoph Baur", "authors": "Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab", "title": "Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain pathologies can vary greatly in size and shape, ranging from few pixels\n(i.e. MS lesions) to large, space-occupying tumors. Recently proposed\nAutoencoder-based methods for unsupervised anomaly segmentation in brain MRI\nhave shown promising performance, but face difficulties in modeling\ndistributions with high fidelity, which is crucial for accurate delineation of\nparticularly small lesions. Here, similar to these previous works, we model the\ndistribution of healthy brain MRI to localize pathologies from erroneous\nreconstructions. However, to achieve improved reconstruction fidelity at higher\nresolutions, we learn to compress and reconstruct different frequency bands of\nhealthy brain MRI using the laplacian pyramid. In a range of experiments\ncomparing our method to different State-of-the-Art approaches on three\ndifferent brain MR datasets with MS lesions and tumors, we show improved\nanomaly segmentation performance and the general capability to obtain much more\ncrisp reconstructions of input data at native resolution. The modeling of the\nlaplacian pyramid further enables the delineation and aggregation of lesions at\nmultiple scales, which allows to effectively cope with different pathologies\nand lesion sizes using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:20:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Baur", "Christoph", ""], ["Wiestler", "Benedikt", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "2006.13158", "submitter": "Hideaki Shimazaki", "authors": "Hideaki Shimazaki", "title": "The principles of adaptation in organisms and machines II:\n  Thermodynamics of the Bayesian brain", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews how organisms learn and recognize the world through the\ndynamics of neural networks from the perspective of Bayesian inference, and\nintroduces a view on how such dynamics is described by the laws for the entropy\nof neural activity, a paradigm that we call thermodynamics of the Bayesian\nbrain. The Bayesian brain hypothesis sees the stimulus-evoked activity of\nneurons as an act of constructing the Bayesian posterior distribution based on\nthe generative model of the external world that an organism possesses. A closer\nlook at the stimulus-evoked activity at early sensory cortices reveals that\nfeedforward connections initially mediate the stimulus-response, which is later\nmodulated by input from recurrent connections. Importantly, not the initial\nresponse, but the delayed modulation expresses animals' cognitive states such\nas awareness and attention regarding the stimulus. Using a simple generative\nmodel made of a spiking neural population, we reproduce the stimulus-evoked\ndynamics with the delayed feedback modulation as the process of the Bayesian\ninference that integrates the stimulus evidence and a prior knowledge with\ntime-delay. We then introduce a thermodynamic view on this process based on the\nlaws for the entropy of neural activity. This view elucidates that the process\nof the Bayesian inference works as the recently-proposed information-theoretic\nengine (neural engine, an analogue of a heat engine in thermodynamics), which\nallows us to quantify the perceptual capacity expressed in the delayed\nmodulation in terms of entropy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:57:46 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Shimazaki", "Hideaki", ""]]}, {"id": "2006.13187", "submitter": "Juliana Gonzalez-Astudillo", "authors": "Juliana Gonzalez-Astudillo, Tiziana Cattai, Giulia Bassignana,\n  Marie-Constance Corsi, Fabrizio De Vico Fallani", "title": "Network-based brain computer interfaces: principles and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) make possible to interact with the external\nenvironment by decoding the mental intention of individuals. BCIs can therefore\nbe used to address basic neuroscience questions but also to unlock a variety of\napplications from exoskeleton control to neurofeedback (NFB) rehabilitation. In\ngeneral, BCI usability critically depends on the ability to comprehensively\ncharacterize brain functioning and correctly identify the user s mental state.\nTo this end, much of the efforts have focused on improving the classification\nalgorithms taking into account localized brain activities as input features.\nDespite considerable improvement BCI performance is still unstable and, as a\nmatter of fact, current features represent oversimplified descriptors of brain\nfunctioning. In the last decade, growing evidence has shown that the brain\nworks as a networked system composed of multiple specialized and spatially\ndistributed areas that dynamically integrate information. While more complex,\nlooking at how remote brain regions functionally interact represents a grounded\nalternative to better describe brain functioning. Thanks to recent advances in\nnetwork science, i.e. a modern field that draws on graph theory, statistical\nmechanics, data mining and inferential modelling, scientists have now powerful\nmeans to characterize complex brain networks derived from neuroimaging data.\nNotably, summary features can be extracted from these networks to\nquantitatively measure specific organizational properties across a variety of\ntopological scales. In this topical review, we aim to provide the\nstate-of-the-art supporting the development of a network theoretic approach as\na promising tool for understanding BCIs and improve usability.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:39:50 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:22:53 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 13:18:36 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gonzalez-Astudillo", "Juliana", ""], ["Cattai", "Tiziana", ""], ["Bassignana", "Giulia", ""], ["Corsi", "Marie-Constance", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "2006.13290", "submitter": "Eduardo Gonzalez Moreira", "authors": "Manuel Hinojosa-Rodriguez, Jose Oliver De Leo-Jimenez, Maria Elena\n  Juarez-Colin, Eduardo Gonzalez-Moreira, Carlos Sair Flores-Bautista and\n  Thalia Harmony", "title": "Long-Term therapeutic effects of Katona therapy in moderate-to-severe\n  perinatal brain damage", "comments": "33 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aim: To determine the long-term efficacy of Katona therapy and early\nrehabilitation of infants with moderate-to-severe perinatal brain damage (PBD).\nMethods: Thirty-two participants were recruited (7-to-16 years) and divided\ninto 3 groups: one Healthy group (n = 11), one group with PBD treated with\nKatona methodology from 2 months of corrected age, and with long-term follow-up\n(n = 12), and one group with PBD but without treatment in the first year of\nlife due to late diagnosis of PBD (n = 9). Neuropediatric evaluations, motor\nevoked potentials (MEPs) and magnetic resonance images (MRI) were made. The PBD\ngroups were matched by severity and topography of lesion. Results: The patients\ntreated with Katona had better motor performance when compared to patients\nwithout early treatment (Gross Motor Function Classification System levels; 75%\nof Katona group were classified in levels I and II and 78% of patients without\nearly treatment were classified in levels III and IV). Furthermore, independent\nk-means cluster analyses of MRI, MEPs, and neuropediatric evaluations data were\nperformed. Katona and non-treated early groups were classified in the same MRI\ncluster which is the expected for patient's population with PBD. However, in\nMEPs and neuropediatric evaluations clustering, the 67% of Katona group were\nassigning into Healthy group showing the impact of Katona therapy over the\npatients treated with it. These results highlight the Katona therapy benefits\nin early rehabilitation of infants with moderate-to-severe PBD. Conclusions:\nKatona therapy and early rehabilitation have an important therapeutic effect in\ninfants with moderate-to-severe PBD by decreasing the severity of motor\ndisability in later stages of life.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 19:51:16 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hinojosa-Rodriguez", "Manuel", ""], ["De Leo-Jimenez", "Jose Oliver", ""], ["Juarez-Colin", "Maria Elena", ""], ["Gonzalez-Moreira", "Eduardo", ""], ["Flores-Bautista", "Carlos Sair", ""], ["Harmony", "Thalia", ""]]}, {"id": "2006.13332", "submitter": "Laureline Logiaco", "authors": "Laureline Logiaco, G. Sean Escola", "title": "Thalamocortical motor circuit insights for more robust hierarchical\n  control of complex sequences", "comments": "14 pages, 5 figures. Submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning of recurrent neural networks that produce temporal\nsequences consisting of the concatenation of re-usable \"motifs\". In the context\nof neuroscience or robotics, these motifs would be the motor primitives from\nwhich complex behavior is generated. Given a known set of motifs, can a new\nmotif be learned without affecting the performance of the known set and then\nused in new sequences without first explicitly learning every possible\ntransition? Two requirements enable this: (i) parameter updates while learning\na new motif do not interfere with the parameters used for the previously\nacquired ones; and (ii) a new motif can be robustly generated when starting\nfrom the network state reached at the end of any of the other motifs, even if\nthat state was not present during training. We meet the first requirement by\ninvestigating artificial neural networks (ANNs) with specific architectures,\nand attempt to meet the second by training them to generate motifs from random\ninitial states. We find that learning of single motifs succeeds but that\nsequence generation is not robust: transition failures are observed. We then\ncompare these results with a model whose architecture and\nanalytically-tractable dynamics are inspired by the motor thalamocortical\ncircuit, and that includes a specific module used to implement motif\ntransitions. The synaptic weights of this model can be adjusted without\nrequiring stochastic gradient descent (SGD) on the simulated network outputs,\nand we have asymptotic guarantees that transitions will not fail. Indeed, in\nsimulations, we achieve single-motif accuracy on par with the previously\nstudied ANNs and have improved sequencing robustness with no transition\nfailures. Finally, we show that insights obtained by studying the transition\nsubnetwork of this model can also improve the robustness of transitioning in\nthe traditional ANNs previously studied.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 21:08:38 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Logiaco", "Laureline", ""], ["Escola", "G. Sean", ""]]}, {"id": "2006.13471", "submitter": "Sergio Verduzco-Flores", "authors": "Sergio Verduzco-Flores, William Dorrell, Erik DeSchutter", "title": "An approach to synaptic learning for autonomous motor control", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the realm of motor control, artificial agents cannot match the performance\nof their biological counterparts. We thus explore a neural control architecture\nthat is both biologically plausible, and capable of fully autonomous learning.\nThe architecture consists of feedback controllers that learn to achieve a\ndesired state by selecting the errors that should drive them. This selection\nhappens through a family of differential Hebbian learning rules that, through\ninteraction with the environment, can learn to control systems where the error\nresponds monotonically to the control signal. We next show that in a more\ngeneral case, neural reinforcement learning can be coupled with a feedback\ncontroller to reduce errors that arise non-monotonically from the control\nsignal. The use of feedback control reduces the complexity of the reinforcement\nlearning problem, because only a desired value must be learned, with the\ncontroller handling the details of how it is reached. This makes the function\nto be learned simpler, potentially allowing to learn more complex actions. We\ndiscuss how this approach could be extended to hierarchical architectures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 04:25:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:12:46 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Verduzco-Flores", "Sergio", ""], ["Dorrell", "William", ""], ["DeSchutter", "Erik", ""]]}, {"id": "2006.13915", "submitter": "Arturo Deza", "authors": "Arturo Deza, Qianli Liao, Andrzej Banburski, Tomaso Poggio", "title": "Hierarchically Compositional Tasks and Deep Convolutional Networks", "comments": "A pre-print. Currently Under Review", "journal-ref": null, "doi": null, "report-no": "MIT Center for Brains, Minds and Machines (CBMM) Memo #109", "categories": "cs.LG eess.IV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main success stories of deep learning, starting with ImageNet, depend on\ndeep convolutional networks, which on certain tasks perform significantly\nbetter than traditional shallow classifiers, such as support vector machines,\nand also better than deep fully connected networks; but what is so special\nabout deep convolutional networks? Recent results in approximation theory\nproved an exponential advantage of deep convolutional networks with or without\nshared weights in approximating functions with hierarchical locality in their\ncompositional structure. More recently, the hierarchical structure was proved\nto be hard to learn from data, suggesting that it is a powerful prior embedded\nin the architecture of the network. These mathematical results, however, do not\nsay which real-life tasks correspond to input-output functions with\nhierarchical locality. To evaluate this, we consider a set of visual tasks\nwhere we disrupt the local organization of images via \"deterministic\nscrambling\" to later perform a visual task on these images structurally-altered\nin the same way for training and testing. For object recognition we find, as\nexpected, that scrambling does not affect the performance of shallow or deep\nfully connected networks contrary to the out-performance of convolutional\nnetworks. Not all tasks involving images are however affected. Texture\nperception and global color estimation are much less sensitive to deterministic\nscrambling showing that the underlying functions corresponding to these tasks\nare not hierarchically local; and also counter-intuitively showing that these\ntasks are better approximated by networks that are not deep (texture) nor\nconvolutional (color). Altogether, these results shed light into the importance\nof matching a network architecture with its embedded prior of the task to be\nlearned.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:46:45 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 04:30:26 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 04:03:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Deza", "Arturo", ""], ["Liao", "Qianli", ""], ["Banburski", "Andrzej", ""], ["Poggio", "Tomaso", ""]]}, {"id": "2006.14178", "submitter": "Jonathan Kadmon", "authors": "Jonathan Kadmon, Jonathan Timcheck, and Surya Ganguli", "title": "Predictive coding in balanced neural networks with noise, chaos and\n  delays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neural networks face a formidable task: performing reliable\ncomputations in the face of intrinsic stochasticity in individual neurons,\nimprecisely specified synaptic connectivity, and nonnegligible delays in\nsynaptic transmission. A common approach to combatting such biological\nheterogeneity involves averaging over large redundant networks of $N$ neurons\nresulting in coding errors that decrease classically as $1/\\sqrt{N}$. Recent\nwork demonstrated a novel mechanism whereby recurrent spiking networks could\nefficiently encode dynamic stimuli, achieving a superclassical scaling in which\ncoding errors decrease as $1/N$. This specific mechanism involved two key\nideas: predictive coding, and a tight balance, or cancellation between strong\nfeedforward inputs and strong recurrent feedback. However, the theoretical\nprinciples governing the efficacy of balanced predictive coding and its\nrobustness to noise, synaptic weight heterogeneity and communication delays\nremain poorly understood. To discover such principles, we introduce an\nanalytically tractable model of balanced predictive coding, in which the degree\nof balance and the degree of weight disorder can be dissociated unlike in\nprevious balanced network models, and we develop a mean field theory of coding\naccuracy. Overall, our work provides and solves a general theoretical framework\nfor dissecting the differential contributions neural noise, synaptic disorder,\nchaos, synaptic delays, and balance to the fidelity of predictive neural codes,\nreveals the fundamental role that balance plays in achieving superclassical\nscaling, and unifies previously disparate models in theoretical neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 05:03:27 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kadmon", "Jonathan", ""], ["Timcheck", "Jonathan", ""], ["Ganguli", "Surya", ""]]}, {"id": "2006.14304", "submitter": "Irina Higgins", "authors": "Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis,\n  Christopher Summerfield, Doris Tsao, Matthew Botvinick", "title": "Unsupervised deep learning identifies semantic disentanglement in single\n  inferotemporal neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep supervised neural networks trained to classify objects have emerged as\npopular models of computation in the primate ventral stream. These models\nrepresent information with a high-dimensional distributed population code,\nimplying that inferotemporal (IT) responses are also too complex to interpret\nat the single-neuron level. We challenge this view by modelling neural\nresponses to faces in the macaque IT with a deep unsupervised generative model,\nbeta-VAE. Unlike deep classifiers, beta-VAE \"disentangles\" sensory data into\ninterpretable latent factors, such as gender or hair length. We found a\nremarkable correspondence between the generative factors discovered by the\nmodel and those coded by single IT neurons. Moreover, we were able to\nreconstruct face images using the signals from just a handful of cells. This\nsuggests that the ventral visual stream may be optimising the disentangling\nobjective, producing a neural code that is low-dimensional and semantically\ninterpretable at the single-unit level.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 10:50:51 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Higgins", "Irina", ""], ["Chang", "Le", ""], ["Langston", "Victoria", ""], ["Hassabis", "Demis", ""], ["Summerfield", "Christopher", ""], ["Tsao", "Doris", ""], ["Botvinick", "Matthew", ""]]}, {"id": "2006.14537", "submitter": "Andrea Ferrario", "authors": "Andrea Ferrario, James Rankin", "title": "Auditory streaming emerges from fast excitation and slow delayed\n  inhibition", "comments": "Supplementary Material is at the end of the file", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the auditory streaming paradigm alternating sequences of pure tones can be\nperceived as a single galloping rhythm (integration) or as two sequences with\nseparated low and high tones (segregation). Although studied for decades, the\nneural mechanisms underlining this perceptual grouping of sound remains a\nmystery. With the aim of identifying a plausible minimal neural circuit that\ncaptures this phenomenon, we propose a firing rate model with two periodically\nforced neural populations coupled by fast direct excitation and slow delayed\ninhibition. By analyzing the model in a non-smooth, slow-fast regime we\nanalytically prove the existence of a rich repertoire of dynamical states and\nof their parameter dependent transitions. We impose plausible parameter\nrestrictions and link all states with perceptual interpretations. Regions of\nstimulus parameters occupied by states linked with each percept matches those\nfound in behavioral experiments. Our model suggests that slow inhibition masks\nthe perception of subsequent tones during segregation (forward masking), while\nfast excitation enables integration for large pitch differences between the two\ntones.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:35:58 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 12:06:59 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ferrario", "Andrea", ""], ["Rankin", "James", ""]]}, {"id": "2006.14565", "submitter": "Karol Szymula", "authors": "Karol P. Szymula, Fabio Pasqualetti, Ann M. Graybiel, Theresa M.\n  Desrochers, and Danielle S. Bassett", "title": "Habit learning supported by efficiently controlled network dynamics in\n  naive macaque monkeys", "comments": "Main Text: 17 pages and 6 figures; Supplement Text: 9 pages and 8\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT math.OC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primates display a marked ability to learn habits in uncertain and dynamic\nenvironments. The associated perceptions and actions of such habits engage\ndistributed neural circuits. Yet, precisely how such circuits support the\ncomputations necessary for habit learning remain far from understood. Here we\nconstruct a formal theory of network energetics to account for how changes in\nbrain state produce changes in sequential behavior. We exercise the theory in\nthe context of multi-unit recordings spanning the caudate nucleus, prefrontal\ncortex, and frontal eyefields of female macaque monkeys engaged in 60-180\nsessions of a free scan task that induces motor habits. The theory relies on\nthe determination of effective connectivity between recording channels, and on\nthe stipulation that a brain state is taken to be the trial-specific firing\nrate across those channels. The theory then predicts how much energy will be\nrequired to transition from one state into another, given the constraint that\nactivity can spread solely through effective connections. Consistent with the\ntheory's predictions, we observed smaller energy requirements for transitions\nbetween more similar and more complex trial saccade patterns, and for sessions\ncharacterized by less entropic selection of saccade patterns. Using a virtual\nlesioning approach, we demonstrate the resilience of the observed relationships\nbetween minimum control energy and behavior to significant disruptions in the\ninferred effective connectivity. Our theoretically principled approach to the\nstudy of habit learning paves the way for future efforts examining how behavior\narises from changing patterns of activity in distributed neural circuitry.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:09:07 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Szymula", "Karol P.", ""], ["Pasqualetti", "Fabio", ""], ["Graybiel", "Ann M.", ""], ["Desrochers", "Theresa M.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2006.14684", "submitter": "Lars Gjesteby", "authors": "Adam Michaleas, Lars A. Gjesteby, Michael Snyder, David Chavez, Meagan\n  Ash, Matthew A. Melton, Damon G. Lamb, Sara N. Burke, Kevin J. Otto, Lee\n  Kamentsky, Webster Guan, Kwanghun Chung, Laura J. Brattain", "title": "Active Learning Pipeline for Brain Mapping in a High Performance\n  Computing Environment", "comments": "6 pages, 5 figures, submitted to IEEE HPEC 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a scalable active learning pipeline prototype for\nlarge-scale brain mapping that leverages high performance computing power. It\nenables high-throughput evaluation of algorithm results, which, after human\nreview, are used for iterative machine learning model training. Image\nprocessing and machine learning are performed in a batch layer. Benchmark\ntesting of image processing using pMATLAB shows that a 100$\\times$ increase in\nthroughput (10,000%) can be achieved while total processing time only increases\nby 9% on Xeon-G6 CPUs and by 22% on Xeon-E5 CPUs, indicating robust\nscalability. The images and algorithm results are provided through a serving\nlayer to a browser-based user interface for interactive review. This pipeline\nhas the potential to greatly reduce the manual annotation burden and improve\nthe overall performance of machine learning-based brain mapping.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 20:27:33 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Michaleas", "Adam", ""], ["Gjesteby", "Lars A.", ""], ["Snyder", "Michael", ""], ["Chavez", "David", ""], ["Ash", "Meagan", ""], ["Melton", "Matthew A.", ""], ["Lamb", "Damon G.", ""], ["Burke", "Sara N.", ""], ["Otto", "Kevin J.", ""], ["Kamentsky", "Lee", ""], ["Guan", "Webster", ""], ["Chung", "Kwanghun", ""], ["Brattain", "Laura J.", ""]]}, {"id": "2006.14800", "submitter": "Randall O'Reilly", "authors": "Randall C. O'Reilly, Jacob L. Russin, Maryam Zolfaghar, and John\n  Rohrlich", "title": "Deep Predictive Learning in Neocortex and Pulvinar", "comments": "56 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How do humans learn from raw sensory experience? Throughout life, but most\nobviously in infancy, we learn without explicit instruction. We propose a\ndetailed biological mechanism for the widely-embraced idea that learning is\nbased on the differences between predictions and actual outcomes (i.e.,\npredictive error-driven learning). Specifically, numerous weak projections into\nthe pulvinar nucleus of the thalamus generate top-down predictions, and sparse,\nfocal driver inputs from lower areas supply the actual outcome, originating in\nlayer 5 intrinsic bursting (5IB) neurons. Thus, the outcome is only briefly\nactivated, roughly every 100 msec (i.e., 10 Hz, alpha), resulting in a temporal\ndifference error signal, which drives local synaptic changes throughout the\nneocortex, resulting in a biologically-plausible form of error backpropagation\nlearning. We implemented these mechanisms in a large-scale model of the visual\nsystem, and found that the simulated inferotemporal (IT) pathway learns to\nsystematically categorize 3D objects according to invariant shape properties,\nbased solely on predictive learning from raw visual inputs. These categories\nmatch human judgments on the same stimuli, and are consistent with neural\nrepresentations in IT cortex in primates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 05:02:44 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 10:56:07 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["O'Reilly", "Randall C.", ""], ["Russin", "Jacob L.", ""], ["Zolfaghar", "Maryam", ""], ["Rohrlich", "John", ""]]}, {"id": "2006.14933", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim and Woochang Lim", "title": "Influence of Various Temporal Recoding on Pavlovian Eyeblink\n  Conditioning in The Cerebellum", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.11325", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Pavlovian eyeblink conditioning (EBC) via repeated\npresentation of paired conditioned stimulus (tone) and unconditioned stimulus\n(airpuff). The influence of various temporal recoding of granule cells on the\nEBC is investigated in a cerebellar network where the connection probability\n$p_c$ from Golgi to granule cells is changed. In an optimal case of\n$p_c^*~(=0.029)$, individual granule cells show various well- and ill-matched\nfiring patterns relative to the unconditioned stimulus. Then, these\nvariously-recoded signals are fed into the Purkinje cells (PCs) through\nparallel-fibers (PFs). In the case of well-matched PF-PC synapses, their\nsynaptic weights are strongly depressed through strong long-term depression\n(LTD). On the other hand, practically no LTD occurs for the ill-matched PF-PC\nsynapses. This type of \"effective\" depression at the PF-PC synapses coordinates\nfirings of PCs effectively, which then make effective inhibitory coordination\non cerebellar nucleus neuron [which elicits conditioned response (CR;\neyeblink)]. When the learning trial passes a threshold, acquisition of CR\nbegins. In this case, the timing degree ${\\cal T}_d$ of CR becomes good due to\npresence of the ill-matched firing group which plays a role of protection\nbarrier for the timing. With further increase in the trial, strength $\\cal S$\nof CR (corresponding to the amplitude of eyelid closure) increases due to\nstrong LTD in the well-matched firing group. Thus, with increasing the learning\ntrial, the (overall) learning efficiency degree ${\\cal L}_e$ (taking into\nconsideration both timing and strength of CR) for the CR is increased, and\neventually it becomes saturated. By changing $p_c$ from $p_c^*$, we also\ninvestigate the influence of various temporal recoding on the EBC. It is thus\nfound that, the more various in temporal recoding, the more effective in\nlearning for the Pavlovian EBC.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 02:39:48 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 04:02:15 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "2006.15099", "submitter": "Jakob Jordan", "authors": "Jakob Jordan, Jo\\~ao Sacramento, Mihai A. Petrovici, Walter Senn", "title": "Conductance-based dendrites perform reliability-weighted opinion pooling", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cue integration, the combination of different sources of information to\nreduce uncertainty, is a fundamental computational principle of brain function.\nStarting from a normative model we show that the dynamics of multi-compartment\nneurons with conductance-based dendrites naturally implement the required\nprobabilistic computations. The associated error-driven plasticity rule allows\nneurons to learn the relative reliability of different pathways from data\nsamples, approximating Bayes-optimal observers in multisensory integration\ntasks. Additionally, the model provides a functional interpretation of neural\nrecordings from multisensory integration experiments and makes specific\npredictions for membrane potential and conductance dynamics of individual\nneurons.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:04:28 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Jordan", "Jakob", ""], ["Sacramento", "Jo\u00e3o", ""], ["Petrovici", "Mihai A.", ""], ["Senn", "Walter", ""]]}, {"id": "2006.15128", "submitter": "John Antrobus Ph.D.", "authors": "John S. Antrobus, Yusuke Shono, Wolfgang M. Pauli, and Bala Sundaram", "title": "All Recognition is Accomplished By Interacting Bottom-Up Sensory and\n  Top-Down Context Bias in Occipital to Frontal Cortex Neural Networks", "comments": "39 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recognition of every word is accomplished by close collaboration of bottom-up\nsub-word and word recognition neural networks with top-down cognitive word\ncontext expectations. The utility of this context appropriate collaboration is\nsubstantial savings in recognition time, accuracy and cortical neural\nprocessing resources. Repetition priming, the simplest form of context\nfacilitation, has been studied extensively, but behavioral and cognitive\nneuroscience research has failed to produce a common shared model. Facilitation\nis attributed to temporary lowered word recognition thresholds. Recent fMRI\nevidence identifies frontal, prefrontal, left temporal cortex interactions as\nthe source of this priming bias. Five experiments presented here clearly\ndemonstrate that word recognition facilitation is a bias effect. Context-Biased\nFast Accurate Recognition, a recurrent neural network model, shows how this\nanticipatory bias is accomplished by interactions among top-down conceptual\ncognitive networks and bottom-up lexical word recognition networks. Signal\ndetection theory says that this facilitation bias is offset by the cost of\nmiss-recognizing similar, but different words. However, the prime typically\ncreates a temporary time-space recognition window within which probability of\nprime recurrence is substantially raised paradoxically transforming bias into\nsensitivity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:36:58 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Antrobus", "John S.", ""], ["Shono", "Yusuke", ""], ["Pauli", "Wolfgang M.", ""], ["Sundaram", "Bala", ""]]}, {"id": "2006.15802", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Gia H. Ngo, Keith Jamison, Amy Kuceyeski and Mert R.\n  Sabuncu", "title": "A shared neural encoding model for the prediction of subject-specific\n  fMRI response", "comments": "MICCAI 2020 early accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of naturalistic paradigms in fMRI (such as movie\nwatching) demands novel strategies for multi-subject data analysis, such as use\nof neural encoding models. In the present study, we propose a shared\nconvolutional neural encoding method that accounts for individual-level\ndifferences. Our method leverages multi-subject data to improve the prediction\nof subject-specific responses evoked by visual or auditory stimuli. We showcase\nour approach on high-resolution 7T fMRI data from the Human Connectome Project\nmovie-watching protocol and demonstrate significant improvement over\nsingle-subject encoding models. We further demonstrate the ability of the\nshared encoding model to successfully capture meaningful individual differences\nin response to traditional task-based facial and scenes stimuli. Taken\ntogether, our findings suggest that inter-subject knowledge transfer can be\nbeneficial to subject-specific predictive models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 04:10:14 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 03:10:46 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Ngo", "Gia H.", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2006.15960", "submitter": "Emmanuel Dauc\\'e", "authors": "Emmanuel Dauc\\'e", "title": "End-Effect Exploration Drive for Effective Motor Learning", "comments": "6 pages, 3 figures, submitted to IWAI 2020 (1st International\n  Workshop on Active Inference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stemming on the idea that a key objective in reinforcement learning is to\ninvert a target distribution of effects, end-effect drives are proposed as an\neffective way to implement goal-directed motor learning, in the absence of an\nexplicit forward model. An end-effect model relies on a simple statistical\nrecording of the effect of the current policy, here used as a substitute for\nthe more resource-demanding forward models. When combined with a reward\nstructure, it forms the core of a lightweight variational free energy\nminimization setup. The main difficulty lies in the maintenance of this\nsimplified effect model together with the online update of the policy. When the\nprior target distribution is uniform, it provides a ways to learn an efficient\nexploration policy, consistently with the intrinsic curiosity principles. When\ncombined with an extrinsic reward, our approach is finally shown to provide a\nfaster training than traditional off-policy techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:59:34 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 14:43:05 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Dauc\u00e9", "Emmanuel", ""]]}, {"id": "2006.15969", "submitter": "Ekaterina Kondrateva", "authors": "Maxim Kan, Ruslan Aliev, Anna Rudenko, Nikita Drobyshev, Nikita\n  Petrashen, Ekaterina Kondrateva, Maxim Sharaev, Alexander Bernstein, Evgeny\n  Burnaev", "title": "Interpretation of 3D CNNs for Brain MRI Data Classification", "comments": "12 pages, 3 figures", "journal-ref": "AIST2020", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning shows high potential for many medical image analysis tasks.\nNeural networks can work with full-size data without extensive preprocessing\nand feature generation and, thus, information loss. Recent work has shown that\nthe morphological difference in specific brain regions can be found on MRI with\nthe means of Convolution Neural Networks (CNN). However, interpretation of the\nexisting models is based on a region of interest and can not be extended to\nvoxel-wise image interpretation on a whole image. In the current work, we\nconsider the classification task on a large-scale open-source dataset of young\nhealthy subjects -- an exploration of brain differences between men and women.\nIn this paper, we extend the previous findings in gender differences from\ndiffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\nCNN interpretation comparing the results of three interpretation methods:\nMeaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\nwith the open-source library.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 17:56:46 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 16:14:44 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kan", "Maxim", ""], ["Aliev", "Ruslan", ""], ["Rudenko", "Anna", ""], ["Drobyshev", "Nikita", ""], ["Petrashen", "Nikita", ""], ["Kondrateva", "Ekaterina", ""], ["Sharaev", "Maxim", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2006.15971", "submitter": "Gabriel Silva", "authors": "Joshua M. Roldan, Sebastian Pardo G., Vivek Kurien George, and Gabriel\n  A. Silva", "title": "Construction of edge-ordered multidirected graphlets for comparing\n  dynamics of spatial temporal neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration and transmission of information in the brain are dependent on\nthe interplay between structural and dynamical properties. Implicit in any\npursuit aimed at understanding neural dynamics from appropriate sets of\nmathematically bounded conditions is the notion of an underlying fundamental\nstructure-function constraint imposed by the geometry of the structural\nnetworks and the resultant latencies involved with transfer of information. We\nrecently described the construction and theoretical analysis of a framework\nthat models how local structure-function rules give rise to emergent global\ndynamics on a neural network. An important part of this research program is the\nrequirement for a set of mathematical methods that allow us to catalog,\ntheoretically analyze, and numerically study the rich dynamical patterns that\nresult. One direction we are exploring is an extension of the theory of\ngraphlets. In this paper we introduce an extension of graphlets and associated\nmetric that maps the topological transition of a network from one moment in\ntime to another at the same time that causal relationships are preserved.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:29:57 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Roldan", "Joshua M.", ""], ["G.", "Sebastian Pardo", ""], ["George", "Vivek Kurien", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "2006.15974", "submitter": "Alejandro Rodr\\'iguez-Collado", "authors": "Cristina Rueda, Alejandro Rodr\\'iguez-Collado and Yolanda Larriba", "title": "A novel wave decomposition for oscillatory signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oscillatory systems arise in the different science fields. Complex\nmathematical formulations with differential equations have been proposed to\nmodel the dynamics of these systems. While they have the advantage of having a\ndirect physiological meaning, they are not useful in practice as a result of\nthe parameter adjustment complexity and the presence of noise. In this paper, a\nsignal plus error model is proposed to analyze oscillations, where the signal\nis a multicomponent $FMM$ and the noise is assumed Gaussian. The signal\nformulation is also a novel decomposition approach in AM-FM components,\ncompeting with Fourier and other decompositions. Several interesting\ntheoretical properties are derived including the Ordinary Differential\nEquations describing the signal. Furthermore, the usefulness in real practice\nis demonstrate to analyze signals associated to neuron synapses and by\naddressing other questions in Neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:45:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:24:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Rueda", "Cristina", ""], ["Rodr\u00edguez-Collado", "Alejandro", ""], ["Larriba", "Yolanda", ""]]}, {"id": "2006.16486", "submitter": "Andrey Kuznetsov", "authors": "Ivan A. Kuznetsov, Andrey V. Kuznetsov", "title": "Modeling tau transport in the axon initial segment", "comments": "final accepted version", "journal-ref": "Mathematical Biosciences, vol. 329, article # 108468, 2020", "doi": "10.1016/j.mbs.2020.108468", "report-no": null, "categories": "q-bio.SC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By assuming that tau protein can be in seven kinetic states, we developed a\nmodel of tau protein transport in the axon and in the axon initial segment\n(AIS). Two separate sets of kinetic constants were determined, one in the axon\nand the other in the AIS. This was done by fitting the model predictions in the\naxon with experimental results and by fitting the model predictions in the AIS\nwith the assumed linear increase of the total tau concentration in the AIS. The\ncalibrated model was used to make predictions about tau transport in the axon\nand in the AIS. To the best of our knowledge, this is the first paper that\npresents a mathematical model of tau transport in the AIS. Our modeling results\nsuggest that binding of free tau to MTs creates a negative gradient of free tau\nin the AIS. This leads to diffusion-driven tau transport from the soma into the\nAIS. The model further suggests that slow axonal transport and diffusion-driven\ntransport of tau work together in the AIS, moving tau anterogradely. Our\nnumerical results predict an interplay between these two mechanisms: as the\ndistance from the soma increases, the diffusion-driven transport decreases,\nwhile motor-driven transport becomes larger. Thus, the machinery in the AIS\nworks as a pump, moving tau into the axon.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:36:56 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 15:55:36 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Kuznetsov", "Ivan A.", ""], ["Kuznetsov", "Andrey V.", ""]]}, {"id": "2006.16630", "submitter": "Torbj{\\o}rn V Ness", "authors": "Torbj{\\o}rn V. Ness and Geir Halnes and Solveig N{\\ae}ss and Klas H.\n  Pettersen and Gaute T. Einevoll", "title": "Computing extracellular electric potentials from neuronal simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements of electric potentials from neural activity have played a key\nrole in neuroscience for almost a century, and simulations of neural activity\nis an important tool for understanding such measurements. Volume conductor (VC)\ntheory is used to compute extracellular electric potentials such as\nextracellular spikes, MUA, LFP, ECoG and EEG surrounding neurons, and also\ninversely, to reconstruct neuronal current source distributions from recorded\npotentials through current source density methods. In this book chapter, we\nshow how VC theory can be derived from a detailed electrodiffusive theory for\nion concentration dynamics in the extracellular medium, and show what\nassumptions that must be introduced to get the VC theory on the simplified form\nthat is commonly used by neuroscientists. Furthermore, we provide examples of\nhow the theory is applied to compute spikes, LFP signals and EEG signals\ngenerated by neurons and neuronal populations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:46:57 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 07:11:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ness", "Torbj\u00f8rn V.", ""], ["Halnes", "Geir", ""], ["N\u00e6ss", "Solveig", ""], ["Pettersen", "Klas H.", ""], ["Einevoll", "Gaute T.", ""]]}, {"id": "2006.16676", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki, Ulrich Reitebuch, Eric Zimmermann", "title": "Experimental visually-guided investigation of sub-structures in\n  three-dimensional Turing-like patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-20", "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his 1952 paper \"The chemical basis of morphogenesis\", Alan M. Turing\npresented a model for the formation of skin patterns. While it took several\ndecades, the model has been validated by finding corresponding natural\nphenomena, e.g. in the skin pattern formation of zebrafish. More surprising,\nseemingly unrelated pattern formations can also be studied via the model, like\ne.g. the formation of plant patches around termite hills. In 1984, David A.\nYoung proposed a discretization of Turing's model, reducing it to an\nactivator/inhibitor process on a discrete domain. From this model, the concept\nof three-dimensional Turing-like patterns was derived.\n  In this paper, we consider this generalization to pattern-formation in\nthree-dimensional space. We are particularly interested in classifying the\ndifferent arising sub-structures of the patterns. By providing examples for the\ndifferent structures, we prove a conjecture regarding these structures within\nthe setup of three-dimensional Turing-like pattern. Furthermore, we investigate\n- guided by visual experiments - how these sub-structures are distributed in\nthe parameter space of the discrete model. We found two-fold versions of zero-\nand one-dimensional sub-structures as well as two-dimensional sub-structures\nand use our experimental findings to formulate several conjectures for\nthree-dimensional Turing-like patterns and higher-dimensional cases.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:50:00 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Skrodzki", "Martin", ""], ["Reitebuch", "Ulrich", ""], ["Zimmermann", "Eric", ""]]}, {"id": "2006.16736", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kristof Meding, Felix A. Wichmann", "title": "Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans\n  by measuring error consistency", "comments": "NeurIPS 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in cognitive science and behavioural neuroscience as well\nas in machine learning and artificial intelligence research is to ascertain\nwhether two or more decision makers (be they brains or algorithms) use the same\nstrategy. Accuracy alone cannot distinguish between strategies: two systems may\nachieve similar accuracy with very different strategies. The need to\ndifferentiate beyond accuracy is particularly pressing if two systems are near\nceiling performance, like Convolutional Neural Networks (CNNs) and humans on\nvisual object recognition. Here we introduce trial-by-trial error consistency,\na quantitative analysis for measuring whether two decision making systems\nsystematically make errors on the same inputs. Making consistent errors on a\ntrial-by-trial basis is a necessary condition for similar processing strategies\nbetween decision makers. Our analysis is applicable to compare algorithms with\nalgorithms, humans with humans, and algorithms with humans. When applying error\nconsistency to object recognition we obtain three main findings: (1.)\nIrrespective of architecture, CNNs are remarkably consistent with one another.\n(2.) The consistency between CNNs and human observers, however, is little above\nwhat can be expected by chance alone -- indicating that humans and CNNs are\nlikely implementing very different strategies. (3.) CORnet-S, a recurrent model\ntermed the \"current best model of the primate ventral visual stream\", fails to\ncapture essential characteristics of human behavioural data and behaves\nessentially like a standard purely feedforward ResNet-50 in our analysis. Taken\ntogether, error consistency analysis suggests that the strategies used by human\nand machine vision are still very different -- but we envision our\ngeneral-purpose error consistency analysis to serve as a fruitful tool for\nquantifying future progress.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:47:17 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:43:53 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 15:39:48 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Geirhos", "Robert", ""], ["Meding", "Kristof", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "2006.16976", "submitter": "Nikhil Parthasarathy", "authors": "Nikhil Parthasarathy and Eero P. Simoncelli", "title": "Self-Supervised Learning of a Biologically-Inspired Visual Texture Model", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model for representing visual texture in a low-dimensional\nfeature space, along with a novel self-supervised learning objective that is\nused to train it on an unlabeled database of texture images. Inspired by the\narchitecture of primate visual cortex, the model uses a first stage of oriented\nlinear filters (corresponding to cortical area V1), consisting of both\nrectified units (simple cells) and pooled phase-invariant units (complex\ncells). These responses are processed by a second stage (analogous to cortical\narea V2) consisting of convolutional filters followed by half-wave\nrectification and pooling to generate V2 'complex cell' responses. The second\nstage filters are trained on a set of unlabeled homogeneous texture images,\nusing a novel contrastive objective that maximizes the distance between the\ndistribution of V2 responses to individual images and the distribution of\nresponses across all images. When evaluated on texture classification, the\ntrained model achieves substantially greater data-efficiency than a variety of\ndeep hierarchical model architectures. Moreover, we show that the learned model\nexhibits stronger representational similarity to texture responses of neural\npopulations recorded in primate V2 than pre-trained deep CNNs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:12:09 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Parthasarathy", "Nikhil", ""], ["Simoncelli", "Eero P.", ""]]}]