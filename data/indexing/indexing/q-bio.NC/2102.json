[{"id": "2102.00002", "submitter": "Elvira Di Nardo Prof.", "authors": "A. Buonocore, A. Di Crescenzo, E. Di Nardo", "title": "Input-output behaviour of a model neuron with alternating drift", "comments": null, "journal-ref": "BioSystems (2002) 67, 27-34", "doi": "10.1016/S0303-2647(02)00060-6", "report-no": null, "categories": "q-bio.NC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input-output behaviour of the Wiener neuronal model subject to\nalternating input is studied under the assumption that the effect of such an\ninput is to make the drift itself of an alternating type. Firing densities and\nrelated statistics are obtained via simulations of the sample-paths of the\nprocess in the following three cases: the drift changes occur during random\nperiods characterized by (i) exponential distribution, (ii) Erlang distribution\nwith a preassigned shape parameter, and (iii) deterministic distribution. The\nobtained results are compared with those holding for the Wiener neuronal model\nsubject to sinusoidal input\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 16:30:22 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Buonocore", "A.", ""], ["Di Crescenzo", "A.", ""], ["Di Nardo", "E.", ""]]}, {"id": "2102.00772", "submitter": "Rava A. da Silveira", "authors": "Rava Azeredo da Silveira and Fred Rieke", "title": "The Geometry of Information Coding in Correlated Neural Populations", "comments": "30 pages; 3 figures; review article", "journal-ref": null, "doi": "10.1146/annurev-neuro-120320-082744", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons in the brain represent information in their collective activity. The\nfidelity of this neural population code depends on whether and how variability\nin the response of one neuron is shared with other neurons. Two decades of\nstudies have investigated the influence of these noise correlations on the\nproperties of neural coding. We provide an overview of the theoretical\ndevelopments on the topic. Using simple, qualitative and general arguments, we\ndiscuss, categorize, and relate the various published results. We emphasize the\nrelevance of the fine structure of noise correlation, and we present a new\napproach to the issue. Throughout we emphasize a geometrical picture of how\nnoise correlations impact the neural code.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 11:14:46 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["da Silveira", "Rava Azeredo", ""], ["Rieke", "Fred", ""]]}, {"id": "2102.01303", "submitter": "Tijl Grootswagers", "authors": "Tijl Grootswagers, Amanda K. Robinson, Sophia M. Shatek, Thomas A.\n  Carlson", "title": "The neural dynamics underlying prioritisation of task-relevant\n  information", "comments": "Published in Neurons, Behavior, Data analysis, and Theory (NBDT)", "journal-ref": "Neurons, Behavior, Data Analysis, and Theory (2021), 5(1)", "doi": "10.51628/001c.21174", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human brain prioritises relevant sensory information to perform different\ntasks. Enhancement of task-relevant information requires flexible allocation of\nattentional resources, but it is still a mystery how this is operationalised in\nthe brain. We investigated how attentional mechanisms operate in situations\nwhere multiple stimuli are presented in the same location and at the same time.\nIn two experiments, participants performed a challenging two-back task on\ndifferent types of visual stimuli that were presented simultaneously and\nsuperimposed over each other. Using electroencephalography and multivariate\ndecoding, we analysed the effect of attention on the neural responses to each\nindividual stimulus. Whole brain neural responses contained considerable\ninformation about both the attended and unattended stimuli, even though they\nwere presented simultaneously and represented in overlapping receptive fields.\nAs expected, attention increased the decodability of stimulus-related\ninformation contained in the neural responses, but this effect was evident\nearlier for stimuli that were presented at smaller sizes. Our results show that\nearly neural responses to stimuli in fast-changing displays contain remarkable\ninformation about the sensory environment but are also modulated by attention\nin a manner dependent on perceptual characteristics of the relevant stimuli.\nStimuli, code, and data for this study can be found at https://osf.io/7zhwp/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:24:51 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 05:02:07 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Grootswagers", "Tijl", ""], ["Robinson", "Amanda K.", ""], ["Shatek", "Sophia M.", ""], ["Carlson", "Thomas A.", ""]]}, {"id": "2102.01316", "submitter": "Harikrishnan Nellippallil Balakrishnan", "authors": "Harikrishnan NB and Nithin Nagaraj", "title": "When Noise meets Chaos: Stochastic Resonance in Neurochaos Learning", "comments": "12 pages, 19 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chaos and Noise are ubiquitous in the Brain. Inspired by the chaotic firing\nof neurons and the constructive role of noise in neuronal models, we for the\nfirst time connect chaos, noise and learning. In this paper, we demonstrate\nStochastic Resonance (SR) phenomenon in Neurochaos Learning (NL). SR manifests\nat the level of a single neuron of NL and enables efficient subthreshold signal\ndetection. Furthermore, SR is shown to occur in single and multiple neuronal NL\narchitecture for classification tasks - both on simulated and real-world spoken\ndigit datasets. Intermediate levels of noise in neurochaos learning enables\npeak performance in classification tasks thus highlighting the role of SR in AI\napplications, especially in brain inspired learning architectures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 05:16:55 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 07:01:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["NB", "Harikrishnan", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "2102.01597", "submitter": "Jorge Ram\\'irez-Ruiz", "authors": "Jorge Ram\\'irez-Ruiz and Rub\\'en Moreno-Bote", "title": "Optimal allocation of finite sampling capacity in accumulator models of\n  multi-alternative decision making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When facing many options, we narrow down our focus to very few of them.\nAlthough behaviors like this can be a sign of heuristics, they can actually be\noptimal under limited cognitive resources. Here we study the problem of how to\noptimally allocate limited sampling time to multiple options, modelled as\naccumulators of noisy evidence, to determine the most profitable one. We show\nthat the effective sampling capacity of an agent increases with both available\ntime and the discriminability of the options, and optimal policies undergo a\nsharp transition as a function of it. For small capacity, it is best to\nallocate time evenly to exactly five options and to ignore all the others,\nregardless of the prior distribution of rewards. For large capacities, the\noptimal number of sampled accumulators grows sub-linearly, closely following a\npower law for a wide variety of priors. We find that allocating equal times to\nthe sampled accumulators is better than using uneven time allocations. Our work\nhighlights that multi-alternative decisions are endowed with breadth-depth\ntradeoffs, demonstrates how their optimal solutions depend on the amount of\nlimited resources and the variability of the environment, and shows that\nnarrowing down to a handful of options is always optimal for small capacities.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 16:47:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ram\u00edrez-Ruiz", "Jorge", ""], ["Moreno-Bote", "Rub\u00e9n", ""]]}, {"id": "2102.01803", "submitter": "Ganchao Wei", "authors": "Ganchao Wei, Ian H. Stevenson", "title": "Tracking fast and slow changes in synaptic weights from simultaneously\n  observed pre- and postsynaptic spiking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Synapses change on multiple timescales, ranging from milliseconds to minutes,\ndue to a combination of both short- and long-term plasticity. Here we develop\nan extension of the common Generalized Linear Model to infer both short- and\nlong-term changes in the coupling between a pre- and post-synaptic neuron based\non observed spiking activity. We model short-term synaptic plasticity using\nadditive effects that depend on the presynaptic spike timing, and we model\nlong-term changes in both synaptic weight and baseline firing rate using point\nprocess adaptive smoothing. Using simulations, we first show that this model\ncan accurately recover time-varying synaptic weights 1) for both depressing and\nfacilitating synapses, 2) with a variety of long-term changes (including\nrealistic changes, such as due to STDP), 3) with a range of pre- and\npost-synaptic firing rates, and 4) for both excitatory and inhibitory synapses.\nWe then apply our model to two experimentally recorded putative synaptic\nconnections. We find that simultaneously tracking fast changes in synaptic\nweights, slow changes in synaptic weights, and unexplained variations in\nbaseline firing is essential. Omitting any one of these factors can lead to\nspurious inferences for the others. Altogether, this model provides a flexible\nframework for tracking short- and long-term variation in spike transmission.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 23:54:00 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 02:12:41 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wei", "Ganchao", ""], ["Stevenson", "Ian H.", ""]]}, {"id": "2102.01807", "submitter": "Cole Hurwitz", "authors": "Cole Hurwitz, Nina Kudryashova, Arno Onken, Matthias H. Hennig", "title": "Building population models for large-scale neural recordings:\n  opportunities and pitfalls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 00:06:49 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 04:47:18 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 17:03:02 GMT"}, {"version": "v4", "created": "Sat, 10 Jul 2021 16:05:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hurwitz", "Cole", ""], ["Kudryashova", "Nina", ""], ["Onken", "Arno", ""], ["Hennig", "Matthias H.", ""]]}, {"id": "2102.01852", "submitter": "Hiroki Kojima", "authors": "Hiroki Kojima and Takashi Ikegami", "title": "Organization of a Latent Space structure in VAE/GAN trained by\n  navigation data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel artificial cognitive mapping system using generative deep\nneural networks (VAE/GAN), which can map input images to latent vectors and\ngenerate temporal sequences internally. The results show that the distance of\nthe predicted image is reflected in the distance of the corresponding latent\nvector after training. This indicates that the latent space is constructed to\nreflect the proximity structure of the data set, and may provide a mechanism by\nwhich many aspects of cognition are spatially represented. The present study\nallows the network to internally generate temporal sequences analogous to\nhippocampal replay/pre-play, where VAE produces only near-accurate replays of\npast experiences, but by introducing GANs, latent vectors of temporally close\nimages are closely aligned and sequence acquired some instability. This may be\nthe origin of the generation of the new sequences found in the hippocampus.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 03:13:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kojima", "Hiroki", ""], ["Ikegami", "Takashi", ""]]}, {"id": "2102.01955", "submitter": "Zhaoyang Pang", "authors": "Zhaoyang Pang, Callum Biggs O'May, Bhavin Choksi, Rufin VanRullen", "title": "Predictive coding feedback results in perceived illusory contours in a\n  recurrent neural network", "comments": "Manuscript under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern feedforward convolutional neural networks (CNNs) can now solve some\ncomputer vision tasks at super-human levels. However, these networks only\nroughly mimic human visual perception. One difference from human vision is that\nthey do not appear to perceive illusory contours (e.g. Kanizsa squares) in the\nsame way humans do. Physiological evidence from visual cortex suggests that the\nperception of illusory contours could involve feedback connections. Would\nrecurrent feedback neural networks perceive illusory contours like humans? In\nthis work we equip a deep feedforward convolutional network with brain-inspired\nrecurrent dynamics. The network was first pretrained with an unsupervised\nreconstruction objective on a natural image dataset, to expose it to natural\nobject contour statistics. Then, a classification decision layer was added and\nthe model was finetuned on a form discrimination task: squares vs. randomly\noriented inducer shapes (no illusory contour). Finally, the model was tested\nwith the unfamiliar ''illusory contour'' configuration: inducer shapes oriented\nto form an illusory square. Compared with feedforward baselines, the iterative\n''predictive coding'' feedback resulted in more illusory contours being\nclassified as physical squares. The perception of the illusory contour was\nmeasurable in the luminance profile of the image reconstructions produced by\nthe model, demonstrating that the model really ''sees'' the illusion. Ablation\nstudies revealed that natural image pretraining and feedback error correction\nare both critical to the perception of the illusion. Finally we validated our\nconclusions in a deeper network (VGG): adding the same predictive coding\nfeedback dynamics again leads to the perception of illusory contours.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:07:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:43:15 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pang", "Zhaoyang", ""], ["O'May", "Callum Biggs", ""], ["Choksi", "Bhavin", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2102.02046", "submitter": "Akke Mats Houben", "authors": "Akke Mats Houben", "title": "Matched transient and steady-state approximation of first-passage-time\n  distributions of coloured noise driven leaky neurons", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first-passage-time distribution of a leaky integrate-and-fire neuron\ndriven by a characteristically coloured noise is approximated by matching a\ntransient and a steady-state solution of the membrane voltage distribution.\nThese approximations follow from a simple manipulation, made possible by the\nspecific `eigen' colouring of the noise, which allows to express the membrane\npotential as a Gaussian diffusion process on top of a deterministic exponential\nmovement. Following, the presented method is extended to the case of an\narbitrarily coloured noise driving by factoring out the `eigen' noise and\nreplacing the residue with an equivalent Gaussian process. It is shown that the\nobtained expressions agree well with numerical simulations for different values\nof the neuron parameters and noise colouring.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 13:14:51 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Houben", "Akke Mats", ""]]}, {"id": "2102.02908", "submitter": "Juan Antonio Arias L\\'opez", "authors": "Juan A. Arias, Carmen Cadarso-Su\\'arez, Pablo Aguiar-Fern\\'andez", "title": "Simultaneous Confidence Corridors for neuroimaging data analysis:\n  applications to Alzheimer's Disease diagnosis", "comments": "Working Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Alzheimer's disease (AD) is a chronic neurodegenerative condition responsible\nfor most cases of dementia and considered as one of the greatest challenges for\nneuroscience in this century. Early Ad signs are usually mistaken for normal\nage-related cognitive dysfunctions, thus patients usually start their treatment\nin advanced AD stages, when its benefits are severely limited. AD has no known\ncure, as such, hope lies on early diagnosis which usually depends on\nneuroimaging techniques such as Positron Emission Tomography (PET). PET data is\nthen analyzed with Statistical Parametric Mapping (SPM) software, which uses\nmass univariate statistical analysis, inevitably incurring in errors derived\nfrom this multiple testing approach. Recently, Wang et al. (2020) formulated an\nalternative: applying functional data analysis (FDA), a relatively new branch\nof statistics, to calculate mean function and simultaneous confidence corridors\n(SCCs) for the difference between two groups' PET values. Here we test this\napproach with a practical application for AD diagnosis, estimating mean\nfunctions and SCCs for the difference between AD and control group's PET\nactivity and locating regions where this difference galls outside estimated\nSCCs, indicating differences in brain activity attributable to AD-derived\nneural loss. Our results are consistent with previous literature on AD\npathology and suggest that this FDA approach is more resilient to reductions in\nsample size and less dependent on ad hoc selection of an {\\alpha} level than\nits counterpart, suggesting that this novel technique is a promising venue for\nresearch in the field of medical imaging.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:22:22 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Arias", "Juan A.", ""], ["Cadarso-Su\u00e1rez", "Carmen", ""], ["Aguiar-Fern\u00e1ndez", "Pablo", ""]]}, {"id": "2102.03039", "submitter": "Cian O'Donnell", "authors": "Beatriz E.P. Mizusaki and Cian O'Donnell", "title": "Neural circuit function redundancy in brain disorders", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Redundancy is a ubiquitous property of the nervous system. This means that\nvastly different configurations of cellular and synaptic components can enable\nthe same neural circuit functions. However, until recently very little brain\ndisorder research considered the implications of this characteristic when\ndesigning experiments or interpreting data. Here, we first summarise the\nevidence for redundancy in healthy brains, explaining redundancy and three of\nits sub-concepts: sloppiness, dependencies, and multiple solutions. We then lay\nout key implications for brain disorder research, covering recent examples of\nredundancy effects in experimental studies on psychiatric disorders. Finally,\nwe give predictions for future experiments based on these concepts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 07:53:49 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Mizusaki", "Beatriz E. P.", ""], ["O'Donnell", "Cian", ""]]}, {"id": "2102.03438", "submitter": "Ekkehard Ullner", "authors": "Afifurrahman and Ekkehard Ullner and Antonio Politi", "title": "Collective dynamics in the presence of finite-width pulses", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The idealisation of neuronal pulses as $\\delta$-spikes is a convenient\napproach in neuroscience but can sometimes lead to erroneous conclusions. We\ninvestigate the effect of a finite pulse-width on the dynamics of balanced\nneuronal networks. In particular, we study two populations of identical\nexcitatory and inhibitory neurons in a random network of phase oscillators\ncoupled through exponential pulses with different widths. We consider three\ncoupling functions, inspired by leaky integrate-and-fire neurons with delay and\ntype-I phase-response curves. By exploring the role of the pulse-widths for\ndifferent coupling strengths we find a robust collective irregular dynamics,\nwhich collapses onto a fully synchronous regime if the inhibitory pulses are\nsufficiently wider than the excitatory ones. The transition to synchrony is\naccompanied by hysteretic phenomena (i.e. the co-existence of collective\nirregular and synchronous dynamics). Our numerical results are supported by a\ndetailed scaling and stability analysis of the fully synchronous solution. A\nconjectured first-order phase transition emerging for $\\delta$-spikes is\nsmoothed out for finite-width pulses.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 22:24:43 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:29:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Afifurrahman", "", ""], ["Ullner", "Ekkehard", ""], ["Politi", "Antonio", ""]]}, {"id": "2102.03740", "submitter": "Haiping Huang", "authors": "Wenxuan Zou, Chan Li, and Haiping Huang", "title": "Ensemble perspective for understanding temporal credit assignment", "comments": "17 pages, 18 figures, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are widely used for modeling spatio-temporal\nsequences in both nature language processing and neural population dynamics.\nHowever, understanding the temporal credit assignment is hard. Here, we propose\nthat each individual connection in the recurrent computation is modeled by a\nspike and slab distribution, rather than a precise weight value. We then derive\nthe mean-field algorithm to train the network at the ensemble level. The method\nis then applied to classify handwritten digits when pixels are read in\nsequence, and to the multisensory integration task that is a fundamental\ncognitive function of animals. Our model reveals important connections that\ndetermine the overall performance of the network. The model also shows how\nspatio-temporal information is processed through the hyperparameters of the\ndistribution, and moreover reveals distinct types of emergent neural\nselectivity. It is thus promising to study the temporal credit assignment in\nrecurrent neural networks from the ensemble perspective.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 08:14:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zou", "Wenxuan", ""], ["Li", "Chan", ""], ["Huang", "Haiping", ""]]}, {"id": "2102.03847", "submitter": "Suzette Geriente", "authors": "Diederik Aerts, Jonito Aerts Argu\\\"elles, Lester Beltran, Suzette\n  Geriente (Center Leo Apostel for Interdisciplinary Studies, Free University\n  of Brussels (VUB), Brussels, Belgium), and Sandro Sozzo (School of Business\n  and Centre IQSCS, University of Leicester, Leicester, United Kingdom)", "title": "Entanglement in Cognition violating Bell Inequalities Beyond Cirel'son's\n  Bound", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the results of two tests where a sample of human participants were\nasked to make judgements about the conceptual combinations {\\it The Animal\nActs} and {\\it The Animal eats the Food}. Both tests significantly violate the\nClauser-Horne-Shimony-Holt version of Bell inequalities (`CHSH inequality'),\nthus exhibiting manifestly non-classical behaviour due to the meaning\nconnection between the individual concepts that are combined. We then apply a\nquantum-theoretic framework which we developed for any Bell-type situation and\nrepresent empirical data in complex Hilbert space. We show that the observed\nviolations of the CHSH inequality can be explained as a consequence of a strong\nform of `quantum entanglement' between the component conceptual entities in\nwhich both the state and measurements are entangled. We finally observe that a\nquantum model in Hilbert space can be elaborated in these Bell-type situations\neven when the CHSH violation exceeds the known `Cirel'son bound', in contrast\nto a widespread belief. These findings confirm and strengthen the results we\nrecently obtained in a variety of cognitive tests and document and image\nretrieval operations on the same conceptual combinations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 16:57:59 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Aerts", "Diederik", "", "Center Leo Apostel for Interdisciplinary Studies, Free University\n  of Brussels"], ["Argu\u00eblles", "Jonito Aerts", "", "Center Leo Apostel for Interdisciplinary Studies, Free University\n  of Brussels"], ["Beltran", "Lester", "", "Center Leo Apostel for Interdisciplinary Studies, Free University\n  of Brussels"], ["Geriente", "Suzette", "", "Center Leo Apostel for Interdisciplinary Studies, Free University\n  of Brussels"], ["Sozzo", "Sandro", "", "School of Business\n  and Centre IQSCS, University of Leicester, Leicester, United Kingdom"]]}, {"id": "2102.03849", "submitter": "Igor Ovchinnikov V.", "authors": "Igor V. Ovchinnikov and Skirmantas Janusonis", "title": "Toward an Effective Theory of Neurodynamics: Topological Supersymmetry\n  Breaking, Network Coarse-Graining, and Instanton Interaction", "comments": "revtex 4-1, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math-ph math.MP nlin.AO nlin.PS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experimental research has shown that the brain's fast electrochemical\ndynamics, or neurodynamics (ND), is strongly stochastic, chaotic, and instanton\n(neuroavalanche)-dominated. It is also partly scale-invariant which has been\nloosely associated with critical phenomena. It has been recently demonstrated\nthat the supersymmetric theory of stochastics (STS) offers a theoretical\nframework that can explain all of the above ND features. In the STS, all\nstochastic models possess a topological supersymmetry (TS), and the\n\"criticality\" of ND and similar stochastic processes is associated with\nnoise-induced, spontaneous breakdown of this TS (due to instanton condensation\nnear the border with ordinary chaos in which TS is broken by\nnon-integrability). Here, we propose a new approach that may be useful for the\nconstruction of low-energy effective theories of ND. Its centerpiece is a\ncoarse-graining procedure of neural networks based on simplicial complexes and\nthe concept of the \"enveloping lattice.\" It represents a neural network as a\ncontinuous, high-dimensional base space whose rich topology reflects that of\nthe original network. The reduced one-instanton state space is determined by\nthe de Rham cohomology classes of this base space, and the effective ND\ndynamics can be recognized as interactions of the instantons in the spirit of\nthe Segal-Atiyah formalism.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 17:06:20 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ovchinnikov", "Igor V.", ""], ["Janusonis", "Skirmantas", ""]]}, {"id": "2102.04219", "submitter": "Larissa Albantakis", "authors": "Larissa Albantakis and Giulio Tononi", "title": "What we are is more than what we do", "comments": "4 pages; German version of this article to appear as a contribution\n  to the anthology \"Artificial Intelligence with Consciousness? Statements\n  2021\" edited by the Karlsruhe Institute of Technology (KIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If we take the subjective character of consciousness seriously, consciousness\nbecomes a matter of \"being\" rather than \"doing\". Because \"doing\" can be\ndissociated from \"being\", functional criteria alone are insufficient to decide\nwhether a system possesses the necessary requirements for being a physical\nsubstrate of consciousness. The dissociation between \"being\" and \"doing\" is\nmost salient in artificial general intelligence, which may soon replicate any\nhuman capacity: computers can perform complex functions (in the limit\nresembling human behavior) in the absence of consciousness. Complex behavior\nbecomes meaningless if it is not performed by a conscious being.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 19:26:15 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Albantakis", "Larissa", ""], ["Tononi", "Giulio", ""]]}, {"id": "2102.04312", "submitter": "Henrik Daniel Mettler", "authors": "Henrik D. Mettler, Maximilian Schmidt, Walter Senn, Mihai A.\n  Petrovici, Jakob Jordan", "title": "Evolving Neuronal Plasticity Rules using Cartesian Genetic Programming", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We formulate the search for phenomenological models of synaptic plasticity as\nan optimization problem. We employ Cartesian genetic programming to evolve\nbiologically plausible human-interpretable plasticity rules that allow a given\nnetwork to successfully solve tasks from specific task families. While our\nevolving-to-learn approach can be applied to various learning paradigms, here\nwe illustrate its power by evolving plasticity rules that allow a network to\nefficiently determine the first principal component of its input distribution.\nWe demonstrate that the evolved rules perform competitively with known\nhand-designed solutions. We explore how the statistical properties of the\ndatasets used during the evolutionary search influences the form of the\nplasticity rules and discover new rules which are adapted to the structure of\nthe corresponding datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:17:15 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Mettler", "Henrik D.", ""], ["Schmidt", "Maximilian", ""], ["Senn", "Walter", ""], ["Petrovici", "Mihai A.", ""], ["Jordan", "Jakob", ""]]}, {"id": "2102.04720", "submitter": "Brian Mathias", "authors": "Brian Mathias, Christian Andrae, Anika Schwager, Manuela Macedonia,\n  Katharina von Kriegstein", "title": "Twelve- and fourteen-year-old school children differentially benefit\n  from sensorimotor- and multisensory-enriched vocabulary training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Both children and adults have been shown to benefit from the integration of\nmultisensory and sensorimotor enrichment into pedagogy. For example,\nintegrating pictures or gestures into foreign language (L2) vocabulary learning\ncan improve learning outcomes relative to unisensory learning. However, whereas\nadults seem to benefit to a greater extent from sensorimotor enrichment such as\nthe performance of gestures in contrast to multisensory enrichment with\npictures, this is not the case in elementary school children. Here, we compared\nmultisensory- and sensorimotor-enriched learning in an intermediate age group\nthat falls between the age groups tested in previous studies (elementary school\nchildren and young adults), in an attempt to determine the developmental time\npoint at which children's responses to enrichment mature from a child-like\npattern into an adult-like pattern. Twelve-year-old and fourteen-year-old\nGerman children were trained over 5 consecutive days on auditorily-presented,\nconcrete and abstract, Spanish vocabulary. The vocabulary was learned under\npicture-enriched, gesture-enriched, and non-enriched (auditory-only)\nconditions. The children performed vocabulary recall and translation tests at 3\ndays, 2 months, and 6 months post-learning. Both picture and gesture enrichment\ninterventions were found to benefit children's L2 learning relative to\nnon-enriched learning up to 6 months post-training. Interestingly,\ngesture-enriched learning was even more beneficial than picture-enriched\nlearning for the fourteen-year-olds, while the twelve-year-olds benefitted\nequivalently from learning enriched with pictures and gestures. These findings\nprovide evidence for opting to integrate gestures rather than pictures into L2\npedagogy starting at fourteen years of age.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 09:32:12 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mathias", "Brian", ""], ["Andrae", "Christian", ""], ["Schwager", "Anika", ""], ["Macedonia", "Manuela", ""], ["von Kriegstein", "Katharina", ""]]}, {"id": "2102.04896", "submitter": "Dalton Sakthivadivel", "authors": "Dalton A R Sakthivadivel", "title": "Formalising the Use of the Activation Function in Neural Inference", "comments": "Six pages and one of references, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate how the activation function can be used to describe neural\nfiring in an abstract way, and in turn, why it works well in artificial neural\nnetworks. We discuss how a spike in a biological neurone belongs to a\nparticular universality class of phase transitions in statistical physics. We\nthen show that the artificial neurone is, mathematically, a mean field model of\nbiological neural membrane dynamics, which arises from modelling spiking as a\nphase transition. This allows us to treat selective neural firing in an\nabstract way, and formalise the role of the activation function in perceptron\nlearning. The resultant statistical physical model allows us to recover the\nexpressions for some known activation functions as various special cases. Along\nwith deriving this model and specifying the analogous neural case, we analyse\nthe phase transition to understand the physics of neural network learning.\nTogether, it is shown that there is not only a biological meaning, but a\nphysical justification, for the emergence and performance of typical activation\nfunctions; implications for neural learning and inference are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 19:42:21 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 16:55:31 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sakthivadivel", "Dalton A R", ""]]}, {"id": "2102.05236", "submitter": "Pan Wang", "authors": "Pan Wang, Rui Zhou, Shuo Wang, Ling Li, Wenjia Bai, Jialu Fan, Chunlin\n  Li, Peter Childs, and Yike Guo", "title": "A General Framework for Revealing Human Mind with auto-encoding GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing the question of visualising human mind could help us to find\nregions that are associated with observed cognition and responsible for\nexpressing the elusive mental image, leading to a better understanding of\ncognitive function. The traditional approach treats brain decoding as a\nclassification problem, reading the mind through statistical analysis of brain\nactivity. However, human thought is rich and varied, that it is often\ninfluenced by more of a combination of object features than a specific type of\ncategory. For this reason, we propose an end-to-end brain decoding framework\nwhich translates brain activity into an image by latent space alignment. To\nfind the correspondence from brain signal features to image features, we\nembedded them into two latent spaces with modality-specific encoders and then\naligned the two spaces by minimising the distance between paired latent\nrepresentations. The proposed framework was trained by simultaneous\nelectroencephalogram and functional MRI data, which were recorded when the\nsubjects were viewing or imagining a set of image stimuli. In this paper, we\nfocused on implementing the fMRI experiment. Our experimental results\ndemonstrated the feasibility of translating brain activity to an image. The\nreconstructed image matches image stimuli approximate in both shape and colour.\nOur framework provides a promising direction for building a direct\nvisualisation to reveal human mind.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:18:46 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wang", "Pan", ""], ["Zhou", "Rui", ""], ["Wang", "Shuo", ""], ["Li", "Ling", ""], ["Bai", "Wenjia", ""], ["Fan", "Jialu", ""], ["Li", "Chunlin", ""], ["Childs", "Peter", ""], ["Guo", "Yike", ""]]}, {"id": "2102.05501", "submitter": "Yanis Bahroun", "authors": "Yanis Bahroun and Dmitri B. Chklovskii", "title": "A Neural Network with Local Learning Rules for Minor Subspace Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of neuromorphic hardware and modeling of biological neural\nnetworks requires algorithms with local learning rules. Artificial neural\nnetworks using local learning rules to perform principal subspace analysis\n(PSA) and clustering have recently been derived from principled objective\nfunctions. However, no biologically plausible networks exist for minor subspace\nanalysis (MSA), a fundamental signal processing task. MSA extracts the\nlowest-variance subspace of the input signal covariance matrix. Here, we\nintroduce a novel similarity matching objective for extracting the minor\nsubspace, Minor Subspace Similarity Matching (MSSM). Moreover, we derive an\nadaptive MSSM algorithm that naturally maps onto a novel neural network with\nlocal learning rules and gives numerical results showing that our method\nconverges at a competitive rate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:44:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bahroun", "Yanis", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2102.05503", "submitter": "Yanis Bahroun", "authors": "Yanis Bahroun and Anirvan M. Sengupta and Dmitri B. Chklovskii", "title": "A Similarity-preserving Neural Network Trained on Transformed Images\n  Recapitulates Salient Features of the Fly Motion Detection Circuit", "comments": "Body and supplementary materials of NeurIPS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to detect content-independent transformations from data is one of\nthe central problems in biological and artificial intelligence. An example of\nsuch problem is unsupervised learning of a visual motion detector from pairs of\nconsecutive video frames. Rao and Ruderman formulated this problem in terms of\nlearning infinitesimal transformation operators (Lie group generators) via\nminimizing image reconstruction error. Unfortunately, it is difficult to map\ntheir model onto a biologically plausible neural network (NN) with local\nlearning rules. Here we propose a biologically plausible model of motion\ndetection. We also adopt the transformation-operator approach but, instead of\nreconstruction-error minimization, start with a similarity-preserving objective\nfunction. An online algorithm that optimizes such an objective function\nnaturally maps onto an NN with biologically plausible learning rules. The\ntrained NN recapitulates major features of the well-studied motion detector in\nthe fly. In particular, it is consistent with the experimental observation that\nlocal motion detectors combine information from at least three adjacent pixels,\nsomething that contradicts the celebrated Hassenstein-Reichardt model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:45:40 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bahroun", "Yanis", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2102.05888", "submitter": "Michael Schirner", "authors": "Michael Schirner, Lia Domide, Dionysios Perdikis, Paul Triebkorn, Leon\n  Stefanovski, Roopa Pai, Paula Popa, Bogdan Valean, Jessica Palmer, Chlo\\^e\n  Langford, Andr\\'e Blickensd\\\"orfer, Michiel van der Vlag, Sandra Diaz-Pier,\n  Alexander Peyser, Wouter Klijn, Dirk Pleiter, Anne Nahm, Oliver Schmid,\n  Marmaduke Woodman, Lyuba Zehl, Jan Fousek, Spase Petkoski, Lionel Kusch,\n  Meysam Hashemi, Daniele Marinazzo, Jean-Fran\\c{c}ois Mangin, Agnes Fl\\\"oel,\n  Simisola Akintoye, Bernd Carsten Stahl, Michael Cepic, Emily Johnson, Gustavo\n  Deco, Anthony R. McIntosh, Claus C. Hilgetag, Marc Morgan, Bernd Schuller,\n  Alex Upton, Colin McMurtrie, Timo Dickscheid, Jan G. Bjaalie, Katrin Amunts,\n  Jochen Mersmann, Viktor Jirsa, Petra Ritter", "title": "Brain Modelling as a Service: The Virtual Brain on EBRAINS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CR cs.DC q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Virtual Brain (TVB) is now available as open-source cloud ecosystem on\nEBRAINS, a shared digital research platform for brain science. It offers\nservices for constructing, simulating and analysing brain network models (BNMs)\nincluding the TVB network simulator; magnetic resonance imaging (MRI)\nprocessing pipelines to extract structural and functional connectomes;\nmultiscale co-simulation of spiking and large-scale networks; a domain specific\nlanguage for automatic high-performance code generation from user-specified\nmodels; simulation-ready BNMs of patients and healthy volunteers; Bayesian\ninference of epilepsy spread; data and code for mouse brain simulation; and\nextensive educational material. TVB cloud services facilitate reproducible\nonline collaboration and discovery of data assets, models, and software\nembedded in scalable and secure workflows, a precondition for research on large\ncohort data sets, better generalizability and clinical translation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 08:33:50 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:22:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schirner", "Michael", ""], ["Domide", "Lia", ""], ["Perdikis", "Dionysios", ""], ["Triebkorn", "Paul", ""], ["Stefanovski", "Leon", ""], ["Pai", "Roopa", ""], ["Popa", "Paula", ""], ["Valean", "Bogdan", ""], ["Palmer", "Jessica", ""], ["Langford", "Chlo\u00ea", ""], ["Blickensd\u00f6rfer", "Andr\u00e9", ""], ["van der Vlag", "Michiel", ""], ["Diaz-Pier", "Sandra", ""], ["Peyser", "Alexander", ""], ["Klijn", "Wouter", ""], ["Pleiter", "Dirk", ""], ["Nahm", "Anne", ""], ["Schmid", "Oliver", ""], ["Woodman", "Marmaduke", ""], ["Zehl", "Lyuba", ""], ["Fousek", "Jan", ""], ["Petkoski", "Spase", ""], ["Kusch", "Lionel", ""], ["Hashemi", "Meysam", ""], ["Marinazzo", "Daniele", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Fl\u00f6el", "Agnes", ""], ["Akintoye", "Simisola", ""], ["Stahl", "Bernd Carsten", ""], ["Cepic", "Michael", ""], ["Johnson", "Emily", ""], ["Deco", "Gustavo", ""], ["McIntosh", "Anthony R.", ""], ["Hilgetag", "Claus C.", ""], ["Morgan", "Marc", ""], ["Schuller", "Bernd", ""], ["Upton", "Alex", ""], ["McMurtrie", "Colin", ""], ["Dickscheid", "Timo", ""], ["Bjaalie", "Jan G.", ""], ["Amunts", "Katrin", ""], ["Mersmann", "Jochen", ""], ["Jirsa", "Viktor", ""], ["Ritter", "Petra", ""]]}, {"id": "2102.06273", "submitter": "William Levy Ph. D.", "authors": "William B Levy and Victoria G. Calvert", "title": "Cerebral cortical communication overshadows computational energy-use,\n  but these combine to predict synapse number", "comments": null, "journal-ref": "PNAS, 2021", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Darwinian evolution tends to produce energy-efficient outcomes. On the other\nhand, energy limits computation, be it neural and probabilistic or digital and\nlogical. Taking a particular energy-efficient viewpoint, we define neural\ncomputation and make use of an energy-constrained, computational function. This\nfunction can be optimized over a variable that is proportional to the number of\nsynapses per neuron. This function also implies a specific distinction between\nATP-consuming processes, especially computation \\textit{per se} vs the\ncommunication processes including action potentials and transmitter release.\nThus to apply this mathematical function requires an energy audit with a\npartitioning of energy consumption that differs from earlier work. The audit\npoints out that, rather than the oft-quoted 20 watts of glucose available to\nthe brain \\cite{sokoloff1960metabolism,sawada2013synapse}, the fraction\npartitioned to cortical computation is only 0.1 watts of ATP. On the other hand\nat 3.5 watts, long-distance communication costs are 35-fold greater. Other\nnovel quantifications include (i) a finding that the biological vs ideal values\nof neural computational efficiency differ by a factor of $10^8$ and (ii) two\npredictions of $N$, the number of synaptic transmissions needed to fire a\nneuron (2500 vs 2000).\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 21:10:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Levy", "William B", ""], ["Calvert", "Victoria G.", ""]]}, {"id": "2102.06592", "submitter": "Ben Baker", "authors": "Ben Baker, Benjamin Lansdell, Konrad Kording", "title": "A Philosophical Understanding of Representation for Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neuroscientists often describe neural activity as a representation of\nsomething, or claim to have found evidence for a neural representation. But\nwhat do these statements mean? The reasons to call some neural activity a\nrepresentation and the assumptions that come with this term are not generally\nmade clear from its common uses in neuroscience. Representation is a central\nconcept in philosophy of mind, with a rich history going back to the ancient\nperiod. In order to clarify its usage in neuroscience, here we advance a link\nbetween the connotations of this term across these disciplines. We draw on a\nbroad range of discourse in philosophy to distinguish three key aspects of\nrepresentation: correspondence, functional role, and teleology. We argue that\neach of these aspects are implied by the explanatory role the term plays in\nneuroscience. However, evidence related to all three aspects is rarely\npresented or discussed in the course of individual studies that aim to identify\nrepresentations. Overlooking the significance of all three aspects hinders\ncommunication in neuroscience, as it obscures the limitations of experimental\nparadigms and conceals gaps in our understanding of the phenomena of primary\ninterest. Working from this three-part view, we discuss how to move toward\nclearer communication about representations in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:01:24 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 13:03:55 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Baker", "Ben", ""], ["Lansdell", "Benjamin", ""], ["Kording", "Konrad", ""]]}, {"id": "2102.06847", "submitter": "Lu Zhang", "authors": "Lu Zhang, Li Wang, Dajiang Zhu", "title": "Representing Alzheimer's Disease Progression via Deep Prototype Tree", "comments": "Submitted to Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, a variety of predictive approaches have been proposed and\nevaluated in terms of their predicting capability for Alzheimer's Disease (AD)\nand its precursor - mild cognitive impairment (MCI). Most of them focused on\nprediction or identification of statistical differences among different\nclinical groups or phases (e.g., longitudinal studies). The continuous nature\nof AD development and transition states between successive AD related stages\nhave been overlooked, especially in binary or multi-class classification.\nThough a few progression models of AD have been studied recently, they mainly\ndesigned to determine and compare the order of specific biomarkers. How to\neffectively predict the individual patient's status within a wide spectrum of\nAD progression has been understudied. In this work, we developed a novel\nstructure learning method to computationally model the continuum of AD\nprogression as a tree structure. By conducting a novel prototype learning with\na deep manner, we are able to capture intrinsic relations among different\nclinical groups as prototypes and represent them in a continuous process for AD\ndevelopment. We named this method as Deep Prototype Learning and the learned\ntree structure as Deep Prototype Tree - DPTree. DPTree represents different\nclinical stages as a trajectory reflecting AD progression and predict clinical\nstatus by projecting individuals onto this continuous trajectory. Through this\nway, DPTree can not only perform efficient prediction for patients at any\nstages of AD development (77.8% accuracy for five groups), but also provide\nmore information by examining the projecting locations within the entire AD\nprogression process.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 02:11:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Lu", ""], ["Wang", "Li", ""], ["Zhu", "Dajiang", ""]]}, {"id": "2102.07036", "submitter": "Joaquin Torres", "authors": "Jorge Pretel and Joaquin J. Torres and J. Marro", "title": "EEGs disclose significant brain activity correlated with synaptic\n  fickleness", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here study a network of synaptic relations mingling excitatory and\ninhibitory neuron nodes that displays oscillations quite similar to\nelectroencephalogram (EEG) brain waves, and identify abrupt variations brought\nabout by swift synaptic mediations. We thus conclude that corresponding changes\nin EEG series surely come from the slowdown of the activity in neuron\npopulations due to synaptic restrictions. The latter happens to generate an\nimbalance between excitation and inhibition causing a quick explosive increase\nof excitatory activity, which turns out to be a (first-order) transition among\ndynamic mental phases. Besides, near this phase transition, our model system\nexhibits waves with a strong component in the so-called \\textit{delta-theta\ndomain} that coexist with fast oscillations. These findings provide a simple\nexplanation for the observed \\textit{delta-gamma} and \\textit{theta-gamma\nmodulation} in actual brains, and open a serious and versatile path to\nunderstand deeply large amounts of apparently erratic, easily accessible brain\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 00:10:06 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:22:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pretel", "Jorge", ""], ["Torres", "Joaquin J.", ""], ["Marro", "J.", ""]]}, {"id": "2102.08209", "submitter": "Masoumeh Zareh", "authors": "Masoumeh Zareh, Mohammad Hossein Manshaei, and Sayed Jalal Zahabi", "title": "Modeling the Hallucinating Brain: A Generative Adversarial Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper looks into the modeling of hallucination in the human's brain.\nHallucinations are known to be causally associated with some malfunctions\nwithin the interaction of different areas of the brain involved in perception.\nFocusing on visual hallucination and its underlying causes, we identify an\nadversarial mechanism between different parts of the brain which are\nresponsible in the process of visual perception. We then show how the\ncharacterized adversarial interactions in the brain can be modeled by a\ngenerative adversarial network.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 14:30:14 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zareh", "Masoumeh", ""], ["Manshaei", "Mohammad Hossein", ""], ["Zahabi", "Sayed Jalal", ""]]}, {"id": "2102.08211", "submitter": "Laura Kriener", "authors": "Laura Kriener, Julian G\\\"oltz, Mihai A. Petrovici", "title": "The Yin-Yang dataset", "comments": "3 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yin-Yang dataset was developed for research on biologically plausible\nerror backpropagation and deep learning in spiking neural networks. It serves\nas an alternative to classic deep learning datasets, especially in algorithm-\nand model-prototyping scenarios, by providing several advantages. First, it is\nsmaller and therefore faster to learn, thereby being better suited for the\ndeployment on neuromorphic chips with limited network sizes. Second, it\nexhibits a very clear gap between the accuracies achievable using shallow as\ncompared to deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 15:18:05 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Kriener", "Laura", ""], ["G\u00f6ltz", "Julian", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "2102.08524", "submitter": "Nicolas Roth", "authors": "Teresa Chouzouris, Nicolas Roth, Caglar Cakan, Klaus Obermayer", "title": "Applications of optimal nonlinear control to a whole-brain network of\n  FitzHugh-Nagumo oscillators", "comments": "Main paper: 20 pages, 14 figures; supplemental material: 10 pages, 9\n  figures. For associated video files, see\n  https://github.com/rederoth/nonlinearControlFHN-videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SY eess.SY nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the framework of optimal nonlinear control to steer the dynamics of\na whole-brain network of FitzHugh-Nagumo oscillators. Its nodes correspond to\nthe cortical areas of an atlas-based segmentation of the human cerebral cortex,\nand the inter-node coupling strengths are derived from Diffusion Tensor Imaging\ndata of the connectome of the human brain. Nodes are coupled using an additive\nscheme without delays and are driven by background inputs with fixed mean and\nadditive Gaussian noise. Optimal control inputs to nodes are determined by\nminimizing a cost functional that penalizes the deviations from a desired\nnetwork dynamic, the control energy, and spatially non-sparse control inputs.\nUsing the strength of the background input and the overall coupling strength as\norder parameters, the network's state-space decomposes into regions of low and\nhigh activity fixed points separated by a high amplitude limit cycle all of\nwhich qualitatively correspond to the states of an isolated network node. Along\nthe borders, however, additional limit cycles, asynchronous states and\nmultistability can be observed. Optimal control is applied to several\nstate-switching and network synchronization tasks, and the results are compared\nto controllability measures from linear control theory for the same connectome.\nWe find that intuitions from the latter about the roles of nodes in steering\nthe network dynamics, which are solely based on connectome features, do not\ngenerally carry over to nonlinear systems, as had been previously implied.\nInstead, the role of nodes under optimal nonlinear control critically depends\non the specified task and the system's location in state space. Our results\nshed new light on the controllability of brain network states and may serve as\nan inspiration for the design of new paradigms for non-invasive brain\nstimulation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 01:38:56 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 17:29:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chouzouris", "Teresa", ""], ["Roth", "Nicolas", ""], ["Cakan", "Caglar", ""], ["Obermayer", "Klaus", ""]]}, {"id": "2102.08623", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Alexander Smith, Gary Shiu", "title": "Reviews: Topological Distances and Losses for Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Almost all statistical and machine learning methods in analyzing brain\nnetworks rely on distances and loss functions, which are mostly Euclidean or\nmatrix norms. The Euclidean or matrix distances may fail to capture underlying\nsubtle topological differences in brain networks. Further, Euclidean distances\nare sensitive to outliers. A few extreme edge weights may severely affect the\ndistance. Thus it is necessary to use distances and loss functions that\nrecognize topology of data. In this review paper, we survey various topological\ndistance and loss functions from topological data analysis (TDA) and persistent\nhomology that can be used in brain network analysis more effectively. Although\nthere are many recent brain imaging studies that are based on TDA methods,\npossibly due to the lack of method awareness, TDA has not taken as the\nmainstream tool in brain imaging field yet. The main purpose of this paper is\nprovide the relevant technical survey of these powerful tools that are\nimmediately applicable to brain network data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 08:23:20 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Chung", "Moo K.", ""], ["Smith", "Alexander", ""], ["Shiu", "Gary", ""]]}, {"id": "2102.09061", "submitter": "Eddy Kwessi", "authors": "Eddy Kwessi, Lloyd Edwards", "title": "Analysis of EEG data using complex geometric structurization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electroencephalogram (EEG) is a common tool used to understand brain\nactivities. The data are typically obtained by placing electrodes at the\nsurface of the scalp and recording the oscillations of currents passing through\nthe electrodes. These oscillations can sometimes lead to various\ninterpretations, depending on the subject's health condition, the experiment\ncarried out, the sensitivity of the tools used, human manipulations etc. The\ndata obtained over time can be considered a time series. There is evidence in\nthe literature that epilepsy EEG data may be chaotic. Either way, the embedding\ntheory in dynamical systems suggests that time series from a complex system\ncould be used to reconstruct its phase space under proper conditions. In this\npaper, we propose an analysis of epilepsy electroencephalogram time series data\nbased on a novel approach dubbed complex geometric structurization. Complex\ngeometric structurization stems from the construction of strange attractors\nusing embedding theory from dynamical systems. The complex geometric structures\nare themselves obtained using a geometry tool, namely the $\\alpha$-shapes from\nshape analysis. Initial analyses show a proof of concept in that these complex\nstructures capture the expected changes brain in lobes under consideration.\nFurther, a deeper analysis suggests that these complex structures can be used\nas biomarkers for seizure changes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:49:33 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kwessi", "Eddy", ""], ["Edwards", "Lloyd", ""]]}, {"id": "2102.09124", "submitter": "Dietmar Plenz Dr", "authors": "Dietmar Plenz, Tiago L. Ribeiro, Stephanie R. Miller, Patrick A.\n  Kells, Ali Vakili, Elliott L. Capek (Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA)", "title": "Self-Organized Criticality in the Brain", "comments": "40 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-organized criticality (SOC) refers to the ability of complex systems to\nevolve towards a 2nd-order phase transition at which interactions between\nsystem components lead to scale-invariant events beneficial for system\nperformance. For the last two decades, considerable experimental evidence\naccumulated that the mammalian cortex with its diversity in cell types and\nconnections might exhibit SOC. Here we review experimental findings of\nisolated, layered cortex preparations to self-organize towards four dynamical\nmotifs identified in the cortex in vivo: up-states, oscillations, neuronal\navalanches, and coherence potentials. During up-states, the synchronization\nobserved for nested theta/gamma-oscillations embeds scale-invariant neuronal\navalanches that exhibit robust power law scaling in size with a slope of -3/2\nand a critical branching parameter of 1. This dynamical coordination, tracked\nin the local field potential (nLFP) and pyramidal neuron activity using\n2-photon imaging, emerges autonomously in superficial layers of organotypic\ncortex cultures and acute cortex slices, is homeostatically regulated, displays\nseparation of time scales, and reveals unique size vs. quiet time dependencies.\nA threshold operation identifies coherence potentials; avalanches that in\naddition maintain the precise time course of propagated synchrony. Avalanches\nemerge under conditions of external driving. Control parameters are established\nby the balance of excitation and inhibition (E/I) and the neuromodulator\ndopamine. This rich dynamical repertoire is not observed in dissociated cortex\ncultures, which lack cortical layers and exhibit dynamics similar to a\n1st-order phase transition. The precise interactions between up-states, nested\noscillations, avalanches, and coherence potentials in superficial cortical\nlayers provide compelling evidence for SOC in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 02:53:23 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 13:37:14 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Plenz", "Dietmar", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"], ["Ribeiro", "Tiago L.", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"], ["Miller", "Stephanie R.", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"], ["Kells", "Patrick A.", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"], ["Vakili", "Ali", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"], ["Capek", "Elliott L.", "", "Section on Critical Brain Dynamics,\n  National Institute of Mental Health, National Institutes of Health, USA"]]}, {"id": "2102.09146", "submitter": "Dalton Sakthivadivel", "authors": "Dalton A R Sakthivadivel", "title": "Characterising the Non-Equilibrium Dynamics of a Neural Cell", "comments": "Seven pages and one of references; two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech math.DS nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We examine the dynamical evolution of the state of a neurone, with particular\ncare to the non-equilibrium nature of the forces influencing its movement in\nstate space. We combine non-equilibrium statistical mechanics and dynamical\nsystems theory to characterise the nature of the neural resting state, and its\nrelationship to firing. The stereotypical shape of the action potential arises\nfrom this model, as well as bursting dynamics, and the non-equilibrium phase\ntransition from resting to spiking. Geometric properties of the system are\ndiscussed, such as the birth and shape of the neural limit cycle, which provide\na complementary understanding of these dynamics. This provides a multiscale\nmodel of the neural cell, from molecules to spikes, and explains various\nphenomena in a unified manner. Some more general notions for damped\noscillators, birth-death processes, and stationary non-equilibrium systems are\nincluded.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:54:16 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Sakthivadivel", "Dalton A R", ""]]}, {"id": "2102.09409", "submitter": "Alberto P\\'erez-Cervera", "authors": "Gregory Dumont, Alberto P\\'erez-Cervera, Boris Gutkin", "title": "Adjoint Method for Macroscopic Phase-Resetting Curves of Generic Spiking\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain rhythms emerge as a result of synchronization among interconnected\nspiking neurons. Key properties of such rhythms can be gleaned from the\nphase-resetting curve (PRC). Inferring the macroscopic PRC and developing a\nsystematic phase reduction theory for emerging rhythms remains an outstanding\ntheoretical challenge. Here we present a practical theoretical framework to\ncompute the PRC of generic spiking networks with emergent collective\noscillations. To do so, we adopt a refractory density approach where neurons\nare described by the time since their last action potential. In the\nthermodynamic limit, the network dynamics are captured by a continuity equation\nknown as the refractory density equation. We develop an appropriate adjoint\nmethod for this equation which in turn gives a semi-analytical expression of\nthe infinitesimal PRC. We confirm the validity of our framework for specific\nexamples of neural networks. Our theoretical findings highlight the\nrelationship between key biological properties at the individual neuron scale\nand the macroscopic oscillatory properties assessed by the PRC.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:53:42 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 15:18:46 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Dumont", "Gregory", ""], ["P\u00e9rez-Cervera", "Alberto", ""], ["Gutkin", "Boris", ""]]}, {"id": "2102.09729", "submitter": "Tijl Grootswagers", "authors": "Tijl Grootswagers, Amanda K Robinson", "title": "Overfitting the literature to one set of stimuli and data", "comments": null, "journal-ref": "Front. Hum. Neurosci. 15:682661 (2021)", "doi": "10.3389/fnhum.2021.682661", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fast-growing field of Computational Cognitive Neuroscience is on track to\nmeet its first crisis. A large number of papers in this nascent field are\ndeveloping and testing novel analysis methods using the same stimuli and\nneuroimaging datasets. Publication bias and confirmatory exploration will\nresult in overfitting to the limited available data. The field urgently needs\nto collect more good quality open neuroimaging data using a variety of\nexperimental stimuli, to test the generalisability of current published\nresults, and allow for more robust results in future work.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 04:06:39 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 00:44:36 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Grootswagers", "Tijl", ""], ["Robinson", "Amanda K", ""]]}, {"id": "2102.09882", "submitter": "Fei He", "authors": "Dominik Klepl, Fei He, Min Wu, Matteo De Marco, Daniel J. Blackburn,\n  Ptolemaios Sarrigiannis", "title": "Characterising Alzheimer's Disease with EEG-based Energy Landscape\n  Analysis", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT cs.SY eess.SP eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is one of the most common neurodegenerative\ndiseases, with around 50 million patients worldwide. Accessible and\nnon-invasive methods of diagnosing and characterising AD are therefore urgently\nrequired. Electroencephalography (EEG) fulfils these criteria and is often used\nwhen studying AD. Several features derived from EEG were shown to predict AD\nwith high accuracy, e.g. signal complexity and synchronisation. However, the\ndynamics of how the brain transitions between stable states have not been\nproperly studied in the case of AD and EEG data. Energy landscape analysis is a\nmethod that can be used to quantify these dynamics. This work presents the\nfirst application of this method to both AD and EEG. Energy landscape assigns\nenergy value to each possible state, i.e. pattern of activations across brain\nregions. The energy is inversely proportional to the probability of occurrence.\nBy studying the features of energy landscapes of 20 AD patients and 20 healthy\nage-matched counterparts, significant differences were found. The dynamics of\nAD patients' brain networks were shown to be more constrained - with more local\nminima, less variation in basin size, and smaller basins. We show that energy\nlandscapes can predict AD with high accuracy, performing significantly better\nthan baseline models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:51:08 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:51:47 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Klepl", "Dominik", ""], ["He", "Fei", ""], ["Wu", "Min", ""], ["De Marco", "Matteo", ""], ["Blackburn", "Daniel J.", ""], ["Sarrigiannis", "Ptolemaios", ""]]}, {"id": "2102.10331", "submitter": "Chee-Ming Ting PhD", "authors": "Chee-Ming Ting, Jeremy I. Skipper, Steven L. Small, Hernando Ombao", "title": "Separating Stimulus-Induced and Background Components of Dynamic\n  Functional Connectivity in Naturalistic fMRI", "comments": "Main paper: 10 pages, 8 figures. Supplemental file: 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.SD eess.AS eess.IV eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the challenges in extracting stimulus-related neural dynamics\nfrom other intrinsic processes and noise in naturalistic functional magnetic\nresonance imaging (fMRI). Most studies rely on inter-subject correlations (ISC)\nof low-level regional activity and neglect varying responses in individuals. We\npropose a novel, data-driven approach based on low-rank plus sparse (L+S)\ndecomposition to isolate stimulus-driven dynamic changes in brain functional\nconnectivity (FC) from the background noise, by exploiting shared network\nstructure among subjects receiving the same naturalistic stimuli. The\ntime-resolved multi-subject FC matrices are modeled as a sum of a low-rank\ncomponent of correlated FC patterns across subjects, and a sparse component of\nsubject-specific, idiosyncratic background activities. To recover the shared\nlow-rank subspace, we introduce a fused version of principal component pursuit\n(PCP) by adding a fusion-type penalty on the differences between the rows of\nthe low-rank matrix. The method improves the detection of stimulus-induced\ngroup-level homogeneity in the FC profile while capturing inter-subject\nvariability. We develop an efficient algorithm via a linearized alternating\ndirection method of multipliers to solve the fused-PCP. Simulations show\naccurate recovery by the fused-PCP even when a large fraction of FC edges are\nseverely corrupted. When applied to natural fMRI data, our method reveals FC\nchanges that were time-locked to auditory processing during movie watching,\nwith dynamic engagement of sensorimotor systems for speech-in-noise. It also\nprovides a better mapping to auditory content in the movie than ISC.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:35:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ting", "Chee-Ming", ""], ["Skipper", "Jeremy I.", ""], ["Small", "Steven L.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2102.10400", "submitter": "Alain Moise Dikande Pr.", "authors": "Alain M. Dikande", "title": "On a nonlinear electromechanical model of nerve", "comments": "7 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.PS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generation of action potential brings into play specific mechanosensory\nstimuli manifest in the variation of membrane capacitance, resulting from the\nselective membrane permeability to ions exchanges and testifying to the central\nrole of electromechanical processes in the buildup mechanism of nerve impulse.\nAs well established [See e.g. D. Gross et al, Cellular and Molecular\nNeurobiology vol. 3, p. 89 (1983)], in these electromechanical processes the\nnet instantaneous charge stored in the membrane is regulated by the rate of\nchange of the net fluid density through the membrane, orresponding to the\ndifference in densities of extacellular and intracellular fluids. An\nelectromechanical model is proposed for which mechanical forces are assumed to\nresult from the flow of ionic liquids through the nerve membrane, generating\npressure waves stimulating the membrane and hence controlling the net charge\nstored in the membrane capacitor. The model features coupled nonlinear partial\ndifferential equations: the familiar Hodgkin-Huxley's cable equation for the\ntransmembrane voltage in which the membrane capacitor is now a capacitive\ndiode, and the Heimburg-Jackson's nonlinear hydrodynamic equation for the\npressure wave controlling the total charge in the membrane capacitor. In the\nstationary regime, the Hodgkin-Huxley cable equation with variable capacitance\nreduces to a linear operator problem with zero eigenvalue, the bound states of\nwhich can be obtained exactly for specific values of characteristic parameters\nof the model. In the dynamical regime, numerical simulations of the modified\nHodgkin-Huxley equation lead to a variety of typical figures for the\ntransmembrane voltage, reminiscent of action potentials observed in real\nphysiological contexts.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 17:43:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dikande", "Alain M.", ""]]}, {"id": "2102.10471", "submitter": "Fei He", "authors": "Yuan Yang, Bhavya Vasudeva, Hazem H. Refai, Fei He", "title": "Multi-Phase Locking Value: A Generalized Method for Determining\n  Instantaneous Multi-frequency Phase Coupling", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many physical, biological and neural systems behave as coupled oscillators,\nwith characteristic phase coupling across different frequencies. Methods such\nas $n:m$ phase locking value and bi-phase locking value have previously been\nproposed to quantify phase coupling between two resonant frequencies (e.g. $f$,\n$2f/3$) and across three frequencies (e.g. $f_1$, $f_2$, $f_1+f_2$),\nrespectively. However, the existing phase coupling metrics have their\nlimitations and limited applications. They cannot be used to detect or quantify\nphase coupling across multiple frequencies (e.g. $f_1$, $f_2$, $f_3$, $f_4$,\n$f_1+f_2+f_3-f_4$), or coupling that involves non-integer multiples of the\nfrequencies (e.g. $f_1$, $f_2$, $2f_1/3+f_2/3$). To address the gap, this paper\nproposes a generalized approach, named multi-phase locking value (M-PLV), for\nthe quantification of various types of instantaneous multi-frequency phase\ncoupling. Different from most instantaneous phase coupling metrics that measure\nthe simultaneous phase coupling, the proposed M-PLV method also allows the\ndetection of delayed phase coupling and the associated time lag between coupled\noscillators. The M-PLV has been tested on cases where synthetic coupled signals\nare generated using white Gaussian signals, and a system comprised of multiple\ncoupled R\\\"ossler oscillators. Results indicate that the M-PLV can provide a\nreliable estimation of the time window and frequency combination where the\nphase coupling is significant, as well as a precise determination of time lag\nin the case of delayed coupling. This method has the potential to become a\npowerful new tool for exploring phase coupling in complex nonlinear dynamic\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 23:10:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Yuan", ""], ["Vasudeva", "Bhavya", ""], ["Refai", "Hazem H.", ""], ["He", "Fei", ""]]}, {"id": "2102.10994", "submitter": "Yurui Ming", "authors": "Yurui Ming", "title": "Coherence of Working Memory Study Between Deep Neural Network and\n  Neurophysiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The auto feature extraction capability of deep neural networks (DNN) endows\nthem the potentiality for analysing complicated electroencephalogram (EEG) data\ncaptured from brain functionality research. This work investigates the\npotential coherent correspondence between the region-of-interest (ROI) for DNN\nto explore, and ROI for conventional neurophysiological oriented methods to\nwork with, exemplified in the case of working memory study. The attention\nmechanism induced by global average pooling (GAP) is applied to a public EEG\ndataset of working memory, to unveil these coherent ROIs via a classification\nproblem. The result shows the alignment of ROIs from different research\ndisciplines. This work asserts the confidence and promise of utilizing DNN for\nEEG data analysis, albeit in lack of the interpretation to network operations.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 09:09:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ming", "Yurui", ""]]}, {"id": "2102.11013", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Nicha Dvornek, Sekhar Tatikonda, Xenophon\n  Papademetris, Pamela Ventola, James Duncan", "title": "Multiple-shooting adjoint method for whole-brain dynamic causal modeling", "comments": "27th International Conference on Information Processing in Medical\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic causal modeling (DCM) is a Bayesian framework to infer directed\nconnections between compartments, and has been used to describe the\ninteractions between underlying neural populations based on functional\nneuroimaging data. DCM is typically analyzed with the expectation-maximization\n(EM) algorithm. However, because the inversion of a large-scale continuous\nsystem is difficult when noisy observations are present, DCM by EM is typically\nlimited to a small number of compartments ($<10$). Another drawback with the\ncurrent method is its complexity; when the forward model changes, the posterior\nmean changes, and we need to re-derive the algorithm for optimization. In this\nproject, we propose the Multiple-Shooting Adjoint (MSA) method to address these\nlimitations. MSA uses the multiple-shooting method for parameter estimation in\nordinary differential equations (ODEs) under noisy observations, and is\nsuitable for large-scale systems such as whole-brain analysis in functional MRI\n(fMRI). Furthermore, MSA uses the adjoint method for accurate gradient\nestimation in the ODE; since the adjoint method is generic, MSA is a generic\nmethod for both linear and non-linear systems, and does not require\nre-derivation of the algorithm as in EM. We validate MSA in extensive\nexperiments: 1) in toy examples with both linear and non-linear models, we show\nthat MSA achieves better accuracy in parameter value estimation than EM;\nfurthermore, MSA can be successfully applied to large systems with up to 100\ncompartments; and 2) using real fMRI data, we apply MSA to the estimation of\nthe whole-brain effective connectome and show improved classification of autism\nspectrum disorder (ASD) vs. control compared to using the functional\nconnectome. The package is provided\n\\url{https://jzkay12.github.io/TorchDiffEqPack}\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:00:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhuang", "Juntang", ""], ["Dvornek", "Nicha", ""], ["Tatikonda", "Sekhar", ""], ["Papademetris", "Xenophon", ""], ["Ventola", "Pamela", ""], ["Duncan", "James", ""]]}, {"id": "2102.11437", "submitter": "Carina Curto", "authors": "Daniela Egas Santander, Stefania Ebli, Alice Patania, Nicole\n  Sanderson, Felicia Burtscher, Katherine Morrison, Carina Curto", "title": "Nerve theorems for fixed points of neural networks", "comments": "26 pages, 17 figures, updated author order", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nonlinear network dynamics are notoriously difficult to understand. Here we\nstudy a class of recurrent neural networks called combinatorial\nthreshold-linear networks (CTLNs) whose dynamics are determined by the\nstructure of a directed graph. They are a special case of TLNs, a popular\nframework for modeling neural activity in computational neuroscience. In prior\nwork, CTLNs were found to be surprisingly tractable mathematically. For small\nnetworks, the fixed points of the network dynamics can often be completely\ndetermined via a series of graph rules that can be applied directly to the\nunderlying graph. For larger networks, it remains a challenge to understand how\nthe global structure of the network interacts with local properties. In this\nwork, we propose a method of covering graphs of CTLNs with a set of smaller\ndirectional graphs that reflect the local flow of activity. While directional\ngraphs may or may not have a feedforward architecture, their fixed point\nstructure is indicative of feedforward dynamics. The combinatorial structure of\nthe graph cover is captured by the nerve of the cover. The nerve is a smaller,\nsimpler graph that is more amenable to graphical analysis. We present three\nnerve theorems that provide strong constraints on the fixed points of the\nunderlying network from the structure of the nerve. We then illustrate the\npower of these theorems with some examples. Remarkably, we find that the nerve\nnot only constrains the fixed points of CTLNs, but also gives insight into the\ntransient and asymptotic dynamics. This is because the flow of activity in the\nnetwork tends to follow the edges of the nerve.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 01:00:43 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 17:50:25 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Santander", "Daniela Egas", ""], ["Ebli", "Stefania", ""], ["Patania", "Alice", ""], ["Sanderson", "Nicole", ""], ["Burtscher", "Felicia", ""], ["Morrison", "Katherine", ""], ["Curto", "Carina", ""]]}, {"id": "2102.11812", "submitter": "Adam Tyson", "authors": "Adam L Tyson, Troy W Margrie", "title": "Mesoscale microscopy for micromammals: image analysis tools for\n  understanding the rodent brain", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last ten years, developments in whole-brain microscopy now allow for\nhigh-resolution imaging of intact brains of small rodents such as mice. These\ncomplex images contain a wealth of information, but many neuroscience\nlaboratories do not have all of the computational knowledge and tools needed to\nprocess these data. We review recent open source tools for registration of\nimages to atlases, and the segmentation, visualisation and analysis of brain\nregions and labelled structures such as neurons. Since the field lacks fully\nintegrated analysis pipelines for all types of whole-brain microscopy analysis,\nwe propose a pathway for tool developers to work together to meet this\nchallenge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:22:49 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tyson", "Adam L", ""], ["Margrie", "Troy W", ""]]}, {"id": "2102.11914", "submitter": "Matthew Whelan", "authors": "Matthew T. Whelan, Tony J. Prescott, Eleni Vasilaki", "title": "A Robotic Model of Hippocampal Reverse Replay for Reinforcement Learning", "comments": "39 pages, 6 figures, 2 tables, journal, submitted to Bioinspiration\n  and Biomimetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hippocampal reverse replay is thought to contribute to learning, and\nparticularly reinforcement learning, in animals. We present a computational\nmodel of learning in the hippocampus that builds on a previous model of the\nhippocampal-striatal network viewed as implementing a three-factor\nreinforcement learning rule. To augment this model with hippocampal reverse\nreplay, a novel policy gradient learning rule is derived that associates place\ncell activity with responses in cells representing actions. This new model is\nevaluated using a simulated robot spatial navigation task inspired by the\nMorris water maze. Results show that reverse replay can accelerate learning\nfrom reinforcement, whilst improving stability and robustness over multiple\ntrials. As implied by the neurobiological data, our study implies that reverse\nreplay can make a significant positive contribution to reinforcement learning,\nalthough learning that is less efficient and less stable is possible in its\nabsence. We conclude that reverse replay may enhance reinforcement learning in\nthe mammalian hippocampal-striatal system rather than provide its core\nmechanism.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:47:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Whelan", "Matthew T.", ""], ["Prescott", "Tony J.", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "2102.12523", "submitter": "Chunjong Park", "authors": "Chunjong Park, Morelle Arian, Xin Liu, Leon Sasson, Jeffrey Kahn,\n  Shwetak Patel, Alex Mariakakis, Tim Althoff", "title": "Online Mobile App Usage as an Indicator of Sleep Behavior and Job\n  Performance", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450093", "report-no": null, "categories": "cs.HC cs.CY q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sleep is critical to human function, mediating factors like memory, mood,\nenergy, and alertness; therefore, it is commonly conjectured that a good\nnight's sleep is important for job performance. However, both real-world sleep\nbehavior and job performance are hard to measure at scale. In this work, we\nshow that people's everyday interactions with online mobile apps can reveal\ninsights into their job performance in real-world contexts. We present an\nobservational study in which we objectively tracked the sleep behavior and job\nperformance of salespeople (N = 15) and athletes (N = 19) for 18 months, using\na mattress sensor and online mobile app. We first demonstrate that cumulative\nsleep measures are correlated with job performance metrics, showing that an\nhour of daily sleep loss for a week was associated with a 9.0% and 9.5%\nreduction in performance of salespeople and athletes, respectively. We then\nexamine the utility of online app interaction time as a passively collectible\nand scalable performance indicator. We show that app interaction time is\ncorrelated with the performance of the athletes, but not the salespeople. To\nsupport that our app-based performance indicator captures meaningful variation\nin psychomotor function and is robust against potential confounds, we conducted\na second study to evaluate the relationship between sleep behavior and app\ninteraction time in a cohort of 274 participants. Using a generalized additive\nmodel to control for per-participant random effects, we demonstrate that\nparticipants who lost one hour of daily sleep for a week exhibited 5.0% slower\napp interaction times. We also find that app interaction time exhibits\nmeaningful chronobiologically consistent correlations with sleep history, time\nawake, and circadian rhythms. Our findings reveal an opportunity for online app\ndevelopers to generate new insights regarding cognition and productivity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:30:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Park", "Chunjong", ""], ["Arian", "Morelle", ""], ["Liu", "Xin", ""], ["Sasson", "Leon", ""], ["Kahn", "Jeffrey", ""], ["Patel", "Shwetak", ""], ["Mariakakis", "Alex", ""], ["Althoff", "Tim", ""]]}, {"id": "2102.12582", "submitter": "Zhijian Yang", "authors": "Zhijian Yang, Ilya M. Nasrallah, Haochang Shou, Junhao Wen, Jimit\n  Doshi, Mohamad Habes, Guray Erus, Ahmed Abdulkadir, Susan M. Resnick, David\n  Wolk, Christos Davatzikos", "title": "Disentangling brain heterogeneity via semi-supervised deep-learning and\n  MRI: dimensional representations of Alzheimer's Disease", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity of brain diseases is a challenge for precision\ndiagnosis/prognosis. We describe and validate Smile-GAN (SeMI-supervised\ncLustEring-Generative Adversarial Network), a novel semi-supervised\ndeep-clustering method, which dissects neuroanatomical heterogeneity, enabling\nidentification of disease subtypes via their imaging signatures relative to\ncontrols. When applied to MRIs (2 studies; 2,832 participants; 8,146 scans)\nincluding cognitively normal individuals and those with cognitive impairment\nand dementia, Smile-GAN identified 4 neurodegenerative patterns/axes: P1,\nnormal anatomy and highest cognitive performance; P2, mild/diffuse atrophy and\nmore prominent executive dysfunction; P3, focal medial temporal atrophy and\nrelatively greater memory impairment; P4, advanced neurodegeneration. Further\napplication to longitudinal data revealed two distinct progression pathways:\nP1$\\rightarrow$P2$\\rightarrow$P4 and P1$\\rightarrow$P3$\\rightarrow$P4. Baseline\nexpression of these patterns predicted the pathway and rate of future\nneurodegeneration. Pattern expression offered better yet complementary\nperformance in predicting clinical progression, compared to amyloid/tau. These\ndeep-learning derived biomarkers offer promise for precision diagnostics and\ntargeted clinical trial recruitment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 22:09:16 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yang", "Zhijian", ""], ["Nasrallah", "Ilya M.", ""], ["Shou", "Haochang", ""], ["Wen", "Junhao", ""], ["Doshi", "Jimit", ""], ["Habes", "Mohamad", ""], ["Erus", "Guray", ""], ["Abdulkadir", "Ahmed", ""], ["Resnick", "Susan M.", ""], ["Wolk", "David", ""], ["Davatzikos", "Christos", ""]]}, {"id": "2102.12616", "submitter": "Nicholas Watters", "authors": "Nicholas Watters and Joshua Tenenbaum and Mehrdad Jazayeri", "title": "Modular Object-Oriented Games: A Task Framework for Reinforcement\n  Learning, Psychology, and Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, trends towards studying simulated games have gained momentum\nin the fields of artificial intelligence, cognitive science, psychology, and\nneuroscience. The intersections of these fields have also grown recently, as\nresearchers increasing study such games using both artificial agents and human\nor animal subjects. However, implementing games can be a time-consuming\nendeavor and may require a researcher to grapple with complex codebases that\nare not easily customized. Furthermore, interdisciplinary researchers studying\nsome combination of artificial intelligence, human psychology, and animal\nneurophysiology face additional challenges, because existing platforms are\ndesigned for only one of these domains. Here we introduce Modular\nObject-Oriented Games, a Python task framework that is lightweight, flexible,\ncustomizable, and designed for use by machine learning, psychology, and\nneurophysiology researchers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 01:17:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Watters", "Nicholas", ""], ["Tenenbaum", "Joshua", ""], ["Jazayeri", "Mehrdad", ""]]}, {"id": "2102.13300", "submitter": "Yi-Ling Chen", "authors": "Yi-Ling Chen and Chun-Chung Chen and Yu-Ying Mei and Ning Zhou and\n  Dongchuan Wu and Ting-Kuo Lee", "title": "Ubiquitous proximity to a critical state for collective neural activity\n  in the CA1 region of freely moving mice", "comments": "19 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using miniscope recordings of calcium fluorescence signals in the CA1 region\nof the hippocampus of mice, we monitor the neural activity of hippocampal\nregions while the animals are freely moving in an open chamber. Using a\ndata-driven statistical modeling approach, the statistical properties of the\nrecorded data are mapped to spin-glass models with pairwise interactions.\nConsidering the parameter space of the model, the observed system is generally\nnear a critical state between two distinct phases. The close proximity to the\ncriticality is found to be robust against different ways of sampling and\nsegmentation of the measured data. By independently altering the coupling\ndistribution and the network structure of the statistical model, the network\nstructures are found to be vital to maintain the proximity to the critical\nstate. We further find the observed assignment of the coupling strengths makes\nthe net coupling at each site more balanced with slight variation, which likely\nhelps the maintenance of the critical state. Network analysis on the\nconnectivity obtained by thresholding the coupling strengths find the\nconnectivity of the networks to be well described by a random network model.\nThese results are consistent across different experiments, sampling and\nsegmentation choices in our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 04:37:57 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chen", "Yi-Ling", ""], ["Chen", "Chun-Chung", ""], ["Mei", "Yu-Ying", ""], ["Zhou", "Ning", ""], ["Wu", "Dongchuan", ""], ["Lee", "Ting-Kuo", ""]]}]