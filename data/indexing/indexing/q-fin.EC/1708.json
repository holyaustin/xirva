[{"id": "1708.01085", "submitter": "Alexandre Jacquemain", "authors": "Alexandre Jacquemain", "title": "Lorenz curves interpretations of the Bruss-Duerinckx theorem for\n  resource dependent branching processes", "comments": "10 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bruss and Duerinckx theorem for resource dependent branching processes\nstates that the survival of any society form is nested in an envelope formed by\ntwo extreme policies. The objective of this paper is to give a novel\ninterpretation of this theorem through the use of Lorenz curves. This\nrepresentation helps us visualize how the parameters interplay. Besides, as we\nwill show, it clarifies the impact of inequality in consumption.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 10:06:26 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Jacquemain", "Alexandre", ""]]}, {"id": "1708.01161", "submitter": "Andrea Zaccaria", "authors": "Andrea Gabrielli, Matthieu Cristelli, Dario Mazzilli, Andrea\n  Tacchella, Andrea Zaccaria, Luciano Pietronero", "title": "Why we like the ECI+ algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a measure for Economic Complexity named ECI+ has been proposed by\nAlbeaik et al. We like the ECI+ algorithm because it is mathematically\nidentical to the Fitness algorithm, the measure for Economic Complexity we\nintroduced in 2012. We demonstrate that the mathematical structure of ECI+ is\nstrictly equivalent to that of Fitness (up to normalization and rescaling). We\nthen show how the claims of Albeaik et al. about the ability of Fitness to\ndescribe the Economic Complexity of a country are incorrect. Finally, we\nhypothesize how the wrong results reported by these authors could have been\nobtained by not iterating the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 14:35:16 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Gabrielli", "Andrea", ""], ["Cristelli", "Matthieu", ""], ["Mazzilli", "Dario", ""], ["Tacchella", "Andrea", ""], ["Zaccaria", "Andrea", ""], ["Pietronero", "Luciano", ""]]}, {"id": "1708.01308", "submitter": "Yuchong Zhang", "authors": "Marcel Nutz, Yuchong Zhang", "title": "A Mean Field Competition", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mean field game with rank-based reward: competing agents\noptimize their effort to achieve a goal, are ranked according to their\ncompletion time, and paid a reward based on their relative rank. First, we\npropose a tractable Poissonian model in which we can describe the optimal\neffort for a given reward scheme. Second, we study the principal--agent problem\nof designing an optimal reward scheme. A surprising, explicit design is found\nto minimize the time until a given fraction of the population has reached the\ngoal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 20:58:47 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Nutz", "Marcel", ""], ["Zhang", "Yuchong", ""]]}, {"id": "1708.01890", "submitter": "Shaolin Ji", "authors": "Larry G. Epstein and Shaolin Ji", "title": "Optimal Learning under Robustness and Time-Consistency", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model learning in a continuous-time Brownian setting where there is prior\nambiguity. The associated model of preference values robustness and is\ntime-consistent. It is applied to study optimal learning when the choice\nbetween actions can be postponed, at a per-unit-time cost, in order to observe\na signal that provides information about an unknown parameter. The\ncorresponding optimal stopping problem is solved in closed-form, with a focus\non two specific settings: Ellsberg's two-urn thought experiment expanded to\nallow learning before the choice of bets, and a robust version of the classical\nproblem of sequential testing of two simple hypotheses about the unknown drift\nof a Wiener process. In both cases, the link between robustness and the demand\nfor learning is studied.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 13:44:10 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 01:34:16 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Epstein", "Larry G.", ""], ["Ji", "Shaolin", ""]]}, {"id": "1708.01974", "submitter": "David Frazier", "authors": "David T. Frazier, Christian P. Robert and Judith Rousseau", "title": "Model Misspecification in ABC: Consequences and Diagnostics", "comments": null, "journal-ref": null, "doi": "10.1111/369--7412/20/82421", "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the behavior of approximate Bayesian computation (ABC) when the\nmodel generating the simulated data differs from the actual data generating\nprocess; i.e., when the data simulator in ABC is misspecified. We demonstrate\nboth theoretically and in simple, but practically relevant, examples that when\nthe model is misspecified different versions of ABC can yield substantially\ndifferent results. Our theoretical results demonstrate that even though the\nmodel is misspecified, under regularity conditions, the accept/reject ABC\napproach concentrates posterior mass on an appropriately defined pseudo-true\nparameter value. However, under model misspecification the ABC posterior does\nnot yield credible sets with valid frequentist coverage and has non-standard\nasymptotic behavior. In addition, we examine the theoretical behavior of the\npopular local regression adjustment to ABC under model misspecification and\ndemonstrate that this approach concentrates posterior mass on a completely\ndifferent pseudo-true value than accept/reject ABC. Using our theoretical\nresults, we suggest two approaches to diagnose model misspecification in ABC.\nAll theoretical results and diagnostics are illustrated in a simple running\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 03:10:04 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 05:35:22 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 05:53:48 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 21:58:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Frazier", "David T.", ""], ["Robert", "Christian P.", ""], ["Rousseau", "Judith", ""]]}, {"id": "1708.02193", "submitter": "Adam Krawiec", "authors": "Adam Krawiec, Tomasz Stachowiak, Marek Szydlowski", "title": "The phase space structure of the oligopoly dynamical system by means of\n  Darboux integrability", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.DS nlin.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical complexity of Cournot oligopoly dynamics of\nthree firms by using the qualitative methods of dynamical systems to study the\nphase structure of this model. The phase space is organized with\none-dimensional and two-dimensional invariant submanifolds (for the monopoly\nand duopoly) and unique stable node (global attractor) in the positive quadrant\nof the phase space (Cournot equilibrium). We also study the integrability of\nthe system. We demonstrate the effectiveness of the method of the Darboux\npolynomials in searching for first integrals of the oligopoly. The general\nmethod as well as examples of adopting this method are presented. We study\nDarboux non-integrability of the oligopoly for linear demand functions and find\nfirst integrals of this system for special classes of the system, in\nparticular, rational integrals can be found for a quite general set of model\nparameters. We show how first integral can be useful in lowering the dimension\nof the system using the example of $n$ almost identical firms. This first\nintegral also gives information about the structure of the phase space and the\nbehaviour of trajectories in the neighbourhood of a Nash equilibrium\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:40:34 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Krawiec", "Adam", ""], ["Stachowiak", "Tomasz", ""], ["Szydlowski", "Marek", ""]]}, {"id": "1708.02365", "submitter": "David Frazier", "authors": "David T. Frazier, Tatsushi Oka and Dan Zhu", "title": "Indirect Inference with a Non-Smooth Criterion Function", "comments": "This paper is a revision of arXiv:1708.02365 and supersedes the\n  earlier arXiv paper \"Derivative-Based Optimization with a Non-Smooth\n  Simulated Criterion\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 04:04:02 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:16:17 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 21:57:34 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Frazier", "David T.", ""], ["Oka", "Tatsushi", ""], ["Zhu", "Dan", ""]]}, {"id": "1708.03511", "submitter": "Evangelos Evangelou", "authors": "Lorenzo Napolitano, Evangelos Evangelou, Emanuele Pugliese, Paolo\n  Zeppini, Graham Room", "title": "Technology networks: the autocatalytic origins of innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.SI q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the autocatalytic structure of technological networks and evaluate\nits significance for the dynamics of innovation patenting. To this aim, we\ndefine a directed network of technological fields based on the International\nPatents Classification, in which a source node is connected to a receiver node\nvia a link if patenting activity in the source field anticipates patents in the\nreceiver field in the same region more frequently than we would expect at\nrandom. We show that the evolution of the technology network is compatible with\nthe presence of a growing autocatalytic structure, i.e. a portion of the\nnetwork in which technological fields mutually benefit from being connected to\none another. We further show that technological fields in the core of the\nautocatalytic set display greater fitness, i.e. they tend to appear in a\ngreater number of patents, thus suggesting the presence of positive spillovers\nas well as positive reinforcement. Finally, we observe that core shifts take\nplace whereby different groups of technology fields alternate within the\nautocatalytic structure; this points to the importance of recombinant\ninnovation taking place between close as well as distant fields of the\nhierarchical classification of technological fields.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 12:02:02 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 09:48:07 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 14:40:15 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Napolitano", "Lorenzo", ""], ["Evangelou", "Evangelos", ""], ["Pugliese", "Emanuele", ""], ["Zeppini", "Paolo", ""], ["Room", "Graham", ""]]}, {"id": "1708.04107", "submitter": "Cesar Hidalgo", "authors": "Saleh Albeaik, Mary Kaltenberg, Mansour Alsaleh, C\\'esar A. Hidalgo", "title": "729 new measures of economic complexity (Addendum to Improving the\n  Economic Complexity Index)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we uploaded to the arxiv a paper entitled: Improving the Economic\nComplexity Index. There, we compared three metrics of the knowledge intensity\nof an economy, the original metric we published in 2009 (the Economic\nComplexity Index or ECI), a variation of the metric proposed in 2012, and a\nvariation we called ECI+. It was brought to our attention that the definition\nof ECI+ was equivalent to the variation of the metric proposed in 2012. We have\nverified this claim, and found that while the equations are not exactly the\nsame, they are similar enough to be our own oversight. More importantly, we now\nask: how many variations of the original ECI work? In this paper we provide a\nsimple unifying framework to explore multiple variations of ECI, including both\nthe original 2009 ECI and the 2012 variation. We found that a large fraction of\nvariations have a similar predictive power, indicating that the chance of\nfinding a variation of ECI that works, after the seminal 2009 measure, are\nsurprisingly high. In fact, more than 28 percent of these variations have a\npredictive power that is within 90 percent of the maximum for any variation.\nThese findings show that, once the idea of measuring economic complexity was\nout, creating a variation with a similar predictive power (like the ones\nproposed in 2012) was trivial (a 1 in 3 shot). More importantly, the result\nshow that using exports data to measure the knowledge intensity of an economy\nis a robust phenomenon that works for multiple functional forms. Moreover, the\nfact that multiple variations of the 2009 ECI perform close to the maximum,\ntells us that no variation of ECI will have a performance that is substantially\nbetter. This suggests that research efforts should focus on uncovering the\nmechanisms that contribute to the diffusion and accumulation of productive\nknowledge instead of on exploring small variations to existing measures.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:41:16 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Albeaik", "Saleh", ""], ["Kaltenberg", "Mary", ""], ["Alsaleh", "Mansour", ""], ["Hidalgo", "C\u00e9sar A.", ""]]}, {"id": "1708.04430", "submitter": "Sindhuja Ranganathan", "authors": "Sindhuja Ranganathan, Mikko Kivel\\\"a, Juho Kanniainen", "title": "Dynamics of Investor Spanning Trees Around Dot-Com Bubble", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0198807", "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify temporal investor networks for Nokia stock by constructing\nnetworks from correlations between investor-specific net-volumes and analyze\nchanges in the networks around dot-com bubble. We conduct the analysis\nseparately for households, non-financial institutions, and financial\ninstitutions. Our results indicate that spanning tree measures for households\nreflected the boom and crisis: the maximum spanning tree measures had clear\nupward tendency in the bull markets when the bubble was building up, and, even\nmore importantly, the minimum spanning tree measures pre-reacted the burst of\nbubble. At the same time, we find less clear reactions in minimal and maximal\nspanning trees of non-financial and financial institutions around the bubble,\nwhich suggest that household investors can have a greater herding tendency\naround bubbles.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 08:36:39 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ranganathan", "Sindhuja", ""], ["Kivel\u00e4", "Mikko", ""], ["Kanniainen", "Juho", ""]]}, {"id": "1708.04711", "submitter": "Andrikopoulos Athanasios", "authors": "Athanasios Andrikopoulos", "title": "Generalizations of Szpilrajn's Theorem in economic and game theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Szpilrajn's Lemma entails that each partial order extends to a linear order.\nDushnik and Miller use Szpilrajn's Lemma to show that each partial order has a\nrelizer. Since then, many authors utilize Szpilrajn's Theorem and the\nWell-ordering principle to prove more general existence type theorems on\nextending binary relations. Nevertheless, we are often interested not only in\nthe existence of extensions of a binary relation $R$ satisfying certain axioms\nof orderability, but in something more: (A) The conditions of the sets of\nalternatives and the properties which $R$ satisfies to be inherited when one\npasses to any member of a subfamily of the family of extensions of $R$ and: (B)\nThe size of a family of ordering extensions of $R$, whose intersection is $R$,\nto be the smallest one. The key to addressing these kinds of problems is the\nszpilrajn inherited method. In this paper, we define the notion of\n$\\Lambda(m)$-consistency, where $m$ can reach the first infinite ordinal\n$\\omega$, and we give two general inherited type theorems on extending binary\nrelations, a Szpilrajn type and a Dushnik-Miller type theorem, which generalize\nall the well known existence and inherited type extension theorems in the\nliterature. \\keywords{Consistent binary relations, Extension theorems,\nIntersection of binary relations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 22:48:35 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Andrikopoulos", "Athanasios", ""]]}, {"id": "1708.04952", "submitter": "Swati Singh", "authors": "Swati Singh and Manoj Joshi", "title": "New Market Creation via Innovation: A Study on Tata Nano", "comments": "13 pages", "journal-ref": "aWEshkar Vol. XIX Issue 2, Sept 2015", "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research paper focuses on how innovations support new market creation\nemerging from latent opportunities for low-income group. It also emphasizes on\nnovel strategies that can be implemented for sustaining. The paper concludes\nwith a discussion on the implications of the study and directions to stimulate\nfuture research on the subject.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 06:49:01 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Singh", "Swati", ""], ["Joshi", "Manoj", ""]]}, {"id": "1708.05689", "submitter": "Afshin Montakhab", "authors": "Ali Hussein Samadi, Afshin Montakhab, Hussein Marzban, Sakine Owjimehr", "title": "Quantum Barro--Gordon Game in Monetary Economics", "comments": "15 pages, 2 tables. Accepted for publication in Physica A", "journal-ref": "Physica A 489 (2018) 94 - 101", "doi": "10.1016/j.physa.2017.07.029", "report-no": null, "categories": "q-fin.EC cond-mat.stat-mech quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical game theory addresses decision problems in multi-agent environment\nwhere one rational agent's decision affects other agents' payoffs. Game theory\nhas widespread application in economic, social and biological sciences. In\nrecent years quantum versions of classical games have been proposed and\nstudied. In this paper, we consider a quantum version of the classical\nBarro-Gordon game which captures the problem of time inconsistency in monetary\neconomics. Such time inconsistency refers to the temptation of weak policy\nmaker to implement high inflation when the public expects low inflation. The\ninconsistency arises when the public punishes the weak policy maker in the next\ncycle. We first present a quantum version of the Barro-Gordon game. Next, we\nshow that in a particular case of the quantum game, time-consistent Nash\nequilibrium could be achieved when public expects low inflation, thus resolving\nthe game.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 19:07:20 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Samadi", "Ali Hussein", ""], ["Montakhab", "Afshin", ""], ["Marzban", "Hussein", ""], ["Owjimehr", "Sakine", ""]]}, {"id": "1708.06160", "submitter": "Amir Ahmadi Javid", "authors": "Amir Ahmadi-Javid and Mohsen Ebadi", "title": "Economic Design of Memory-Type Control Charts: The Fallacy of the\n  Formula Proposed by Lorenzen and Vance (1986)", "comments": "Computational Statistics, 2020", "journal-ref": "Ahmadi-Javid, A., & Ebadi, M. (2020). Economic design of\n  memory-type control charts: The fallacy of the formula proposed by Lorenzen\n  and Vance (1986). Computational Statistics, DOI: 10.1007/s00180-020-01019-6", "doi": "10.1007/s00180-020-01019-6", "report-no": null, "categories": "stat.AP cs.CE econ.GN math.OC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory-type control charts, such as EWMA and CUSUM, are powerful tools\nfor detecting small quality changes in univariate and multivariate processes.\nMany papers on economic design of these control charts use the formula proposed\nby Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The\neconomic design of control charts: A unified approach. Technometrics, 28(1),\n3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct\nfor memory-type control charts and its values can significantly deviate from\nthe original values even if the ARL values used in this formula are accurately\ncomputed. Consequently, the use of this formula can result in charts that are\nnot economically optimal. The formula is corrected for memory-type control\ncharts, but unfortunately the modified formula is not a helpful tool from a\ncomputational perspective. We show that simulation-based optimization is a\npossible alternative method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 11:38:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ahmadi-Javid", "Amir", ""], ["Ebadi", "Mohsen", ""]]}, {"id": "1708.06233", "submitter": "Co-Pierre Georg", "authors": "Christoph Aymanns and Jakob Foerster and Co-Pierre Georg", "title": "Fake News in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SI physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the spread of news as a social learning game on a network. Agents\ncan either endorse or oppose a claim made in a piece of news, which itself may\nbe either true or false. Agents base their decision on a private signal and\ntheir neighbors' past actions. Given these inputs, agents follow strategies\nderived via multi-agent deep reinforcement learning and receive utility from\nacting in accordance with the veracity of claims. Our framework yields\nstrategies with agent utility close to a theoretical, Bayes optimal benchmark,\nwhile remaining flexible to model re-specification. Optimized strategies allow\nagents to correctly identify most false claims, when all agents receive\nunbiased private signals. However, an adversary's attempt to spread fake news\nby targeting a subset of agents with a biased private signal can be successful.\nEven more so when the adversary has information about agents' network position\nor private signal. When agents are aware of the presence of an adversary they\nre-optimize their strategies in the training stage and the adversary's attack\nis less effective. Hence, exposing agents to the possibility of fake news can\nbe an effective way to curtail the spread of fake news in social networks. Our\nresults also highlight that information about the users' private beliefs and\ntheir social network structure can be extremely valuable to adversaries and\nshould be well protected.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:09:31 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Aymanns", "Christoph", ""], ["Foerster", "Jakob", ""], ["Georg", "Co-Pierre", ""]]}, {"id": "1708.06704", "submitter": "Ezequiel Alvarez", "authors": "Thomas Pedro Eggarter", "title": "Unemployment: Study of Causes and Possible Solutions", "comments": "T.P.Eggarter (physicist) passed away in August 1997. This work was\n  done during his last months of life and only locally published up to now.\n  Work is in Spanish and could be translated upon request. Please contact E.\n  Alvarez sequi@df.uba.ar", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following measures against unemployment are proposed: In the short term,\nto promote greater income for the poorest sectors. It is shown that this can be\npaid with the resulting increased production, without losing income to the\nother economic agents. In the mid term, the creation of ad-hoc companies for\ninvestment in projects profitable but long lasting. And in the long run, the\nabandonment of the competitive models. As these proposals go against current\nideas (liberalisation, labour market flexibility, free market, etc.), the\nstatements are rigorously demonstrated, even at the risk of making the lecture\nharder.\n  Part 1 explores the problem and uses a simple model and others heuristic\narguments to create familiarity with macroeconomic models. Part 2 is a\nsimplified summary of Macroeconomic Theory textbook. It serves as a review to\nthe reader whose knowledge in economy are out of date, or as a first\napproximation to the topic if he or she does not have them. In the light of the\ntheory, economic policies are evaluated for the Argentine case in the 90's. The\nwork accepts the Keynesian explanation of unemployment (insufficient demand),\nbut we disagree on its solution (public expenditure). Finally, in Part 3 we\nelaborate and justify the proposals.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 03:09:56 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Eggarter", "Thomas Pedro", ""]]}, {"id": "1708.06855", "submitter": "Adam Wu", "authors": "Adam Wu", "title": "Systematic Noise: Micro-movements in Equity Options Markets", "comments": "Undergraduate Research Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equity options are known to be notoriously difficult to price accurately, and\neven with the development of established mathematical models there are many\nassumptions that must be made about the underlying processes driving market\nmovements. As such, the theoretical prices outputted by these models are often\nslightly different from the realized or actual market price. The choice of\nmodel traders use can create many different valuations on the same asset, which\nmay lead to a form of systematic micro-movement or noise. The analysis in this\npaper demonstrates that approximately 1.7%-4.5% of market volume for options\nwritten on the SPY ETF within the last two years could potentially be due to\nsystematic noise.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 00:02:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Wu", "Adam", ""]]}, {"id": "1708.07037", "submitter": "Refk Selmi", "authors": "Jamal Bouoiyour (1), Refk Selmi (1), Amal Miftah (1) ((1) CATT)", "title": "Relationship between Remittances and Macroeconomic Variables in Times of\n  Political and Social Upheaval: Evidence from Tunisia's Arab Spring", "comments": "ERF 23rd Annual Conference , Mar 2017, Amman, Jordan", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If Tunisia was hailed as a success story with its high rankings on economic,\neducational, and other indicators compared to other Arab countries, the 2011\npopular uprisings demonstrate the need for political reforms but also major\neconomic reforms. The Arab spring highlights the fragility of its main economic\npillars including the tourism and the foreign direct investment. In such\nturbulent times, the paper examines the economic impact of migrant'\nremittances, expected to have a countercyclical behavior. Our results reveal\nthat prior to the Arab Spring, the impacts of remittances on growth and\nconsumption seem negative and positive respectively, while they varyingly\ninfluence local investment. These three relationships held in the short-run. By\nconsidering the period surrounding the 2011 uprisings, the investment effect of\nremittances becomes negative and weak in the short-and medium-run, whereas\npositive and strong remittances' impacts on growth and consumption are found in\nthe long term.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:57:26 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bouoiyour", "Jamal", "", "CATT"], ["Selmi", "Refk", "", "CATT"], ["Miftah", "Amal", "", "CATT"]]}, {"id": "1708.07509", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "The Keynesian Model in the General Theory: A Tutorial", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This small overview of the General Theory is the kind of summary I would have\nliked to have read, before embarking in a comprehensive study of the General\nTheory at the time I was a student. As shown here, the main ideas are quite\nsimple and easy to visualize. Unfortunately, numerous introductions to\nKeynesian theory are not actually based on Keynes opus magnum, but in obscure\nneoclassical reinterpretations. This is completely pointless since Keynes' book\nis so readable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 11:58:56 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "1708.07723", "submitter": "Kenan Huremovi\\'c", "authors": "Yann Bramoull\\'e and Kenan Huremovi\\'c", "title": "Promotion through Connections: Favors or Information?", "comments": "35 pages, 2 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connections appear to be helpful in many contexts such as obtaining a job, a\npromotion, a grant, a loan or publishing a paper. This may be due to favoritism\nor to information conveyed by connections. Attempts at identifying both effects\nhave relied on measures of true quality, generally built from data collected\nlong after promotion. This empirical strategy faces important limitations.\nBuilding on earlier work on discrimination, we propose a new method to identify\nfavors and information from classical data collected at time of promotion.\nUnder natural assumptions, we show that promotion decisions look more random\nfor connected candidates, due to the information channel. We obtain new\nidentification results and show how probit models with heteroscedasticity can\nbe used to estimate the strength of the two effects. We apply our method to the\ndata on academic promotions in Spain studied in Zinovyeva & Bagues (2015). We\nfind evidence of both favors and information effects at work. Empirical results\nare consistent with evidence obtained from quality measures collected five\nyears after promotion.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 22:02:17 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 13:16:03 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bramoull\u00e9", "Yann", ""], ["Huremovi\u0107", "Kenan", ""]]}, {"id": "1708.07996", "submitter": "Jean-Bernard Chatelain", "authors": "Jean-Bernard Chatelain, Kirsten Ralf", "title": "A Simple Algorithm for Solving Ramsey Optimal Policy with Exogenous\n  Forcing Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This algorithm extends Ljungqvist and Sargent (2012) algorithm of Stackelberg\ndynamic game to the case of dynamic stochastic general equilibrium models\nincluding exogenous forcing variables. It is based Anderson, Hansen, McGrattan,\nSargent (1996) discounted augmented linear quadratic regulator. It adds an\nintermediate step in solving a Sylvester equation. Forward-looking variables\nare also optimally anchored on forcing variables. This simple algorithm calls\nfor already programmed routines for Ricatti, Sylvester and Inverse matrix in\nMatlab and Scilab. A final step using a change of basis vector computes a\nvector auto regressive representation including Ramsey optimal policy rule\nfunction of lagged observable variables, when the exogenous forcing variables\nare not observable.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 16:55:49 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Chatelain", "Jean-Bernard", ""], ["Ralf", "Kirsten", ""]]}, {"id": "1708.08275", "submitter": "Jacques Tempere", "authors": "Jacques Tempere", "title": "An equilibrium-conserving taxation scheme for income from capital", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": "10.1140/epjb/e2018-80497-x", "report-no": null, "categories": "q-fin.EC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under conditions of market equilibrium, the distribution of capital income\nfollows a Pareto power law, with an exponent that characterizes the given\nequilibrium. Here, a simple taxation scheme is proposed such that the post-tax\ncapital income distribution remains an equilibrium distribution, albeit with a\ndifferent exponent. This taxation scheme is shown to be progressive, and its\nparameters can be simply derived from (i) the total amount of tax that will be\nlevied, (ii) the threshold selected above which capital income will be taxed\nand (iii) the total amount of capital income. The latter can be obtained either\nby using Piketty's estimates of the capital/labor income ratio or by fitting\nthe initial Pareto exponent. Both ways moreover provide a check on the amount\nof declared income from capital.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 11:42:08 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tempere", "Jacques", ""]]}, {"id": "1708.08673", "submitter": "Ron Nielsen", "authors": "Ron W. Nielsen", "title": "Changing the Direction of the Economic and Demographic Research", "comments": "20 pages,10 figures, 11729 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple but useful method of reciprocal values is introduced, explained and\nillustrated. This method simplifies the analysis of hyperbolic distributions,\nwhich are causing serious problems in the demographic and economic research. It\nallows for a unique identification of hyperbolic distributions and for\nunravelling components of more complicated trajectories. This method is\nillustrated by a few examples. They show that fundamental postulates of the\ndemographic and economic research are contradicted by data, even by precisely\nthe same data, which are used in this research. The generally accepted\npostulates are based on the incorrect understanding of hyperbolic\ndistributions, which characterise the historical growth of population and the\nhistorical economic growth. In particular, data used, but never analysed,\nduring the formulation of the Unified Growth Theory show that this theory is\nbased on fundamentally incorrect premises and thus is fundamentally defective.\nApplication of this simple method of analysis points to new directions in the\ndemographic and economic research. It suggests simpler interpretations of the\nmechanism of growth. The concept or the evidence of the past primitive and\ndifficult living conditions, which might be perhaps described as some kind of\nstagnation, is not questioned or disputed. It is only demonstrated that\ntrajectories of the past economic growth and of the growth of population were\nnot reflecting any form of stagnation and thus that they were not shaped by\nthese primitive and difficult living conditions. The concept or evidence of an\nexplosion in technology, medicine, education and in the improved living\nconditions is not questioned or disputed. It is only demonstrated that this\npossible explosion is not reflected in the trajectories of the economic growth\nand of the growth of population.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 09:59:23 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Nielsen", "Ron W.", ""]]}, {"id": "1708.09327", "submitter": "Aleksandra Alori\\'c", "authors": "Aleksandra Alori\\'c, Peter Sollich and Peter McBurney", "title": "Spontaneous Segregation of Agents Across Double Auction Markets", "comments": "12 pages, 7 figures; Artificial Economics 2014 conference; Published\n  online: 17 October 2014", "journal-ref": "Advances in Artificial Economics (2015) pp 79-90. Lecture Notes in\n  Economics and Mathematical Systems, vol 676. Springer, Cham", "doi": "10.1007/978-3-319-09578-3_7", "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the possibility of spontaneous segregation into\ngroups of traders that have to choose among several markets. Even in the\nsimplest case of two markets and Zero Intelligence traders, we are able to\nobserve segregation effects below a critical value Tc of the temperature T; the\nlatter regulates how strongly traders bias their decisions towards choices with\nlarge accumulated scores. It is notable that segregation occurs even though the\ntraders are statistically homogeneous. Traders can in principle change their\nloyalty to a market, but the relevant persistence times become long below Tc.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 15:38:32 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Alori\u0107", "Aleksandra", ""], ["Sollich", "Peter", ""], ["McBurney", "Peter", ""]]}]