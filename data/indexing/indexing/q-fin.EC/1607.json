[{"id": "1607.01999", "submitter": "Somwrita Sarkar", "authors": "Somwrita Sarkar and Sanjay Chawla", "title": "Inferring the contiguity matrix for spatial autoregressive analysis with\n  applications to house price prediction", "comments": "11 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference methods in traditional statistics, machine learning and data mining\nassume that data is generated from an independent and identically distributed\n(iid) process. Spatial data exhibits behavior for which the iid assumption must\nbe relaxed. For example, the standard approach in spatial regression is to\nassume the existence of a contiguity matrix which captures the spatial\nautoregressive properties of the data. However all spatial methods, till now,\nhave assumed that the contiguity matrix is given apriori or can be estimated by\nusing a spatial similarity function. In this paper we propose a convex\noptimization formulation to solve the spatial autoregressive regression (SAR)\nmodel in which both the contiguity matrix and the non-spatial regression\nparameters are unknown and inferred from the data. We solve the problem using\nthe alternating direction method of multipliers (ADMM) which provides a\nsolution which is both robust and efficient. While our approach is general we\nuse data from housing markets of Boston and Sydney to both guide the analysis\nand validate our results. A novel side effect of our approach is the automatic\ndiscovery of spatial clusters which translate to submarkets in the housing data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 13:14:43 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Sarkar", "Somwrita", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1607.02378", "submitter": "Andrey Subochev", "authors": "Fuad Aleskerov, Andrey Subochev", "title": "Matrix-vector representation of various solution concepts", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified matrix-vector representation is developed of such solution concepts\nas the core, the uncovered, the uncaptured, the minimal weakly stable, the\nminimal undominated, the minimal dominant and the untrapped sets. We also\npropose several new versions of solution sets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:31:28 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Aleskerov", "Fuad", ""], ["Subochev", "Andrey", ""]]}, {"id": "1607.02419", "submitter": "Alexander Rubchinsky", "authors": "Alexander Rubchinsky", "title": "Divisive-agglomerative algorithm and complexity of automatic\n  classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm of solution of the Automatic Classification (AC for brevity)\nproblem is set forth in the paper. In the AC problem, it is required to find\none or several artitions, starting with the given pattern matrix or\ndissimilarity, similarity matrix.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:25:02 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Rubchinsky", "Alexander", ""]]}, {"id": "1607.02421", "submitter": "Andrey Subochev", "authors": "Andrey Subochev, Igor Zakhlebin", "title": "Alternative versions of the global competitive industrial performance\n  ranking constructed by methods from social choice theory", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Competitive Industrial Performance index (developed by experts of the\nUNIDO) is designed as a measure of national competitiveness. Index is an\naggregate of eight observable variables, representing different dimensions of\ncompetitive industrial performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:11:33 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Subochev", "Andrey", ""], ["Zakhlebin", "Igor", ""]]}, {"id": "1607.02422", "submitter": "Alexander Karminsky", "authors": "Alexander Karminsky", "title": "Rating models: emerging market distinctions", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Basel II Accords have sparked increased interest in the development of\napproaches based on internal ratings systems and have initiated the elaboration\nof models for remote ratings forecasts based on external ones as part of Risk\nManagement and Early Warning Systems. This article evaluates the peculiarities\nof current ratings systems and addresses specific issues of development of\neconometrical rating models for emerging market companies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:04:19 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Karminsky", "Alexander", ""]]}, {"id": "1607.02423", "submitter": "Alexander Rubchinsky", "authors": "Alexander Rubchinsky", "title": "Fair division with divisible and indivisible items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the work the fair division problem for two participants in presence of\nboth divisible and indivisible items is considered. The set of all divisions is\nformally described; it is demonstrated that fair (in terms of Brams and Taylor)\ndivisions, unlikely the case where all the items are divisible, not always\nexist. The necessary and sufficient conditions of existence of proportional and\nequitable division were found.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:05:36 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Rubchinsky", "Alexander", ""]]}, {"id": "1607.02688", "submitter": "Luis Alcala", "authors": "Luis A. Alcala", "title": "On the time consistency of collective preferences", "comments": "33 pages; changes in notation and major corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic model of collective consumption and saving decisions made by a\nfinite number of agents with constant but different discount rates is\ndeveloped. Collective utility is a weighted sum of individual utilities with\ntime-varying utility weights. Under standard separability assumptions, it is\nshown that collective preferences may be nonstationary but still satisfy time\nconsistency. The assumption of time-varying weights is key to balance the need\nof the group for a changing distribution of consumption among its members over\ntime with their tolerance for consumption fluctuations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 03:55:29 GMT"}, {"version": "v2", "created": "Sun, 31 Jul 2016 03:51:40 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 17:26:46 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Alcala", "Luis A.", ""]]}, {"id": "1607.03161", "submitter": "Romulus Breban", "authors": "Romulus Breban", "title": "A mathematical model for a gaming community", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large community of individuals who mix strongly and meet in\npairs to bet on a coin toss. We investigate the asset distribution of the\nplayers involved in this zero-sum repeated game. Our main result is that the\nasset distribution converges to the exponential distribution, irrespective of\nthe size of the bet, as long as players can never go bankrupt. Analytical\nresults suggests that the exponential distribution is a stable fixed point for\nthis zero-sum repreated game. This is confirmed in numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2016 10:30:24 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Breban", "Romulus", ""]]}, {"id": "1607.04155", "submitter": "Jean-Francois Mercure", "authors": "Jean-Francois Mercure", "title": "Fashion, fads and the popularity of choices: micro-foundations for\n  diffusion consumer theory", "comments": "20 pages including appendix", "journal-ref": "Structural Change and Economic Dynamics, 2018", "doi": "10.1016/j.strueco.2018.06.001", "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge acquisition by consumers is a key process in the diffusion of\ninnovations. However, in standard theories of the representative agent, agents\ndo not learn and innovations are adopted instantaneously. Here, we show that in\na discrete choice model where utility-maximising agents with heterogenous\npreferences learn about products through peers, their stock of knowledge on\nproducts becomes heterogenous, fads and fashions arise, and transitivity in\naggregate preferences is lost. Non-equilibrium path-dependent dynamics emerge,\nthe representative agent exhibits behavioural rules different than individual\nagents, and aggregate utility cannot be optimised. Instead, an evolutionary\ntheory of product innovation and diffusion emerges.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 14:46:04 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 09:16:06 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 10:12:36 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Mercure", "Jean-Francois", ""]]}, {"id": "1607.04484", "submitter": "Bent Flyvbjerg", "authors": "Bent Flyvbjerg, Allison Stewart, Alexander Budzier", "title": "The Oxford Olympics Study 2016: Cost and Cost Overrun at the Games", "comments": "28 pp", "journal-ref": "Said Business School Working Papers (Oxford: University of\n  Oxford), july 2016", "doi": null, "report-no": null, "categories": "q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that Olympic Games held over the past decade each have cost USD 8.9\nbillion on average, the size and financial risks of the Games warrant study.\nThe objectives of the Oxford Olympics study are to (1) establish the actual\noutturn costs of previous Olympic Games in a manner where cost can consistently\nbe compared across Games; (2) establish cost overruns for previous Games, i.e.,\nthe degree to which final outturn costs reflect projected budgets at the bid\nstage, again in a way that allows comparison across Games; (3) test whether the\nOlympic Games Knowledge Management Program has reduced cost risk for the Games,\nand, finally, (4) benchmark cost and cost overrun for the Rio 2016 Olympics\nagainst previous Games. The main contribution of the Oxford study is to\nestablish a phenomenology of cost and cost overrun at the Olympics, which\nallows consistent and systematic comparison across Games. This has not been\ndone before. The study concludes that for a city and nation to decide to stage\nthe Olympic Games is to decide to take on one of the most costly and\nfinancially most risky type of megaproject that exists, something that many\ncities and nations have learned to their peril.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:17:10 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Flyvbjerg", "Bent", ""], ["Stewart", "Allison", ""], ["Budzier", "Alexander", ""]]}, {"id": "1607.06163", "submitter": "David Frazier", "authors": "David T. Frazier and Eric Renault", "title": "Indirect Inference With(Out) Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Inference (I-I) estimation of structural parameters $\\theta$\n{{requires matching observed and simulated statistics, which are most often\ngenerated using an auxiliary model that depends on instrumental parameters\n$\\beta$.}} {The estimators of the instrumental parameters will encapsulate} the\nstatistical information used for inference about the structural parameters. As\nsuch, artificially constraining these parameters may restrict the ability of\nthe auxiliary model to accurately replicate features in the structural data,\nwhich may lead to a range of issues, such as, a loss of identification.\nHowever, in certain situations the parameters $\\beta$ naturally come with a set\nof $q$ restrictions. Examples include settings where $\\beta$ must be estimated\nsubject to $q$ possibly strict inequality constraints $g(\\beta) > 0$, such as,\nwhen I-I is based on GARCH auxiliary models. In these settings we propose a\nnovel I-I approach that uses appropriately modified unconstrained auxiliary\nstatistics, which are simple to compute and always exists. We state the\nrelevant asymptotic theory for this I-I approach without constraints and show\nthat it can be reinterpreted as a standard implementation of I-I through a\nproperly modified binding function. Several examples that have featured in the\nliterature illustrate our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:57:10 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:18:18 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 10:11:32 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Frazier", "David T.", ""], ["Renault", "Eric", ""]]}, {"id": "1607.06247", "submitter": "Richard Tol", "authors": "Monika Novackova and Richard S.J. Tol", "title": "Effects of Sea Level Rise on Economy of the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the first ex post study of the economic impact of sea level rise.\nWe apply two econometric approaches to estimate the past effects of sea level\nrise on the economy of the USA, viz. Barro type growth regressions adjusted for\nspatial patterns and a matching estimator. Unit of analysis is 3063 counties of\nthe USA. We fit growth regressions for 13 time periods and we estimated\nnumerous varieties and robustness tests for both growth regressions and\nmatching estimator. Although there is some evidence that sea level rise has a\npositive effect on economic growth, in most specifications the estimated\neffects are insignificant. We therefore conclude that there is no stable,\nsignificant effect of sea level rise on economic growth. This finding\ncontradicts previous ex ante studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 09:43:56 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Novackova", "Monika", ""], ["Tol", "Richard S. J.", ""]]}, {"id": "1607.07398", "submitter": "Andrea Saltelli", "authors": "Andrea Saltelli, Mario Giampietro", "title": "The fallacy of evidence based policy", "comments": null, "journal-ref": null, "doi": "10.1016/j.futures.2016.11.012", "report-no": null, "categories": "q-fin.EC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of science for policy is at the core of a perfect storm generated by\nthe insurgence of several concurrent crises: of science, of trust, of\nsustainability. The modern positivistic model of science for policy, known as\nevidence based policy, is based on dramatic simplifications and compressions of\navailable perceptions of the state of affairs and possible explanations\n(hypocognition). This model can result in flawed prescriptions. The flaws\nbecome more evident when dealing with complex issues characterized by\nconcomitant uncertainties in the normative, descriptive and ethical domains. In\nthis situation evidence-based policy may concur to the fragility of the social\nsystem. Science plays an important role in reducing the feeling of\nvulnerability of humans by projecting a promise of protection against\nuncertainties. In many applications quantitative science is used to remove\nuncertainty by transforming it into probability, so that mathematical modelling\ncan play the ritual role of haruspices. This epistemic governance arrangement\nis today in crisis. The primacy of science to adjudicate political issues must\npass through an assessment of the level of maturity and effectiveness of the\nvarious disciplines deployed. The solution implies abandoning dreams of\nprediction, control and optimization obtained by relying on a limited set of\nsimplified narratives to define the problem and moving instead to an open\nexploration of a broader set of plausible and relevant stories. Evidence based\npolicy has to be replaced by robust policy, where robustness is tested with\nrespect to feasibility (compatibility with processes outside human control);\nviability (compatibility with processes under human control, in relation to\nboth the economic and technical dimensions), and desirability domain\n(compatibility with a plurality of normative considerations relevant to a\nplurality of actors).\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 07:46:14 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Saltelli", "Andrea", ""], ["Giampietro", "Mario", ""]]}]