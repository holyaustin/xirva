[{"id": "1808.00160", "submitter": "Alejandro Noriega-Campero", "authors": "Alejandro Noriega-Campero, Alex Rutherford, Oren Lederman, Yves A. de\n  Montjoye, and Alex Pentland", "title": "Mapping the Privacy-Utility Tradeoff in Mobile Phone Data for\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's age of data holds high potential to enhance the way we pursue and\nmonitor progress in the fields of development and humanitarian action. We study\nthe relation between data utility and privacy risk in large-scale behavioral\ndata, focusing on mobile phone metadata as paradigmatic domain. To measure\nutility, we survey experts about the value of mobile phone metadata at various\nspatial and temporal granularity levels. To measure privacy, we propose a\nformal and intuitive measure of reidentification risk$\\unicode{x2014}$the\ninformation ratio$\\unicode{x2014}$and compute it at each granularity level. Our\nresults confirm the existence of a stark tradeoff between data utility and\nreidentifiability, where the most valuable datasets are also most prone to\nreidentification. When data is specified at ZIP-code and hourly levels, outside\nknowledge of only 7% of a person's data suffices for reidentification and\nretrieval of the remaining 93%. In contrast, in the least valuable dataset,\nspecified at municipality and daily levels, reidentification requires on\naverage outside knowledge of 51%, or 31 data points, of a person's data to\nretrieve the remaining 49%. Overall, our findings show that coarsening data\ndirectly erodes its value, and highlight the need for using data-coarsening,\nnot as stand-alone mechanism, but in combination with data-sharing models that\nprovide adjustable degrees of accountability and security.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 04:19:50 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Noriega-Campero", "Alejandro", ""], ["Rutherford", "Alex", ""], ["Lederman", "Oren", ""], ["de Montjoye", "Yves A.", ""], ["Pentland", "Alex", ""]]}, {"id": "1808.01205", "submitter": "Ahmed Mushfiq Mobarak", "authors": "Lori Beaman, Ariel BenYishay, Jeremy Magruder, Ahmed Mushfiq Mobarak", "title": "Can Network Theory-based Targeting Increase Technology Adoption?", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": "d2139", "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to induce farmers to adopt a productive new agricultural technology,\nwe apply simple and complex contagion diffusion models on rich social network\ndata from 200 villages in Malawi to identify seed farmers to target and train\non the new technology. A randomized controlled trial compares these\ntheory-driven network targeting approaches to simpler strategies that either\nrely on a government extension worker or an easily measurable proxy for the\nsocial network (geographic distance between households) to identify seed\nfarmers. Our results indicate that technology diffusion is characterized by a\ncomplex contagion learning environment in which most farmers need to learn from\nmultiple people before they adopt themselves. Network theory based targeting\ncan out-perform traditional approaches to extension, and we identify methods to\nrealize these gains at low cost to policymakers.\n  Keywords: Social Learning, Agricultural Technology Adoption, Complex\nContagion, Malawi\n  JEL Classification Codes: O16, O13\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 14:30:04 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Beaman", "Lori", ""], ["BenYishay", "Ariel", ""], ["Magruder", "Jeremy", ""], ["Mobarak", "Ahmed Mushfiq", ""]]}, {"id": "1808.02826", "submitter": "James Unwin", "authors": "Kyle Gatesman and James Unwin", "title": "Lattice Studies of Gerrymandering Strategies", "comments": "32 Pages, 15 Figures", "journal-ref": "Polit. Anal. 29 (2021) 167-192", "doi": "10.1017/pan.2020.22", "report-no": null, "categories": "physics.soc-ph cs.CY econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three novel gerrymandering algorithms which incorporate the\nspatial distribution of voters with the aim of constructing gerrymandered,\nequal-population, connected districts. Moreover, we develop lattice models of\nvoter distributions, based on analogies to electrostatic potentials, in order\nto compare different gerrymandering strategies. Due to the probabilistic\npopulation fluctuations inherent to our voter models, Monte Carlo methods can\nbe applied to the districts constructed via our gerrymandering algorithms.\nThrough Monte Carlo studies we quantify the effectiveness of each of our\ngerrymandering algorithms and we also argue that gerrymandering strategies\nwhich do not include spatial data lead to (legally prohibited) highly\ndisconnected districts. Of the three algorithms we propose, two are based on\ndifferent strategies for packing opposition voters, and the third is a new\napproach to algorithmic gerrymandering based on genetic algorithms, which\nautomatically guarantees that all districts are connected. Furthermore, we use\nour lattice voter model to examine the effectiveness of isoperimetric quotient\ntests and our results provide further quantitative support for implementing\ncompactness tests in real-world political redistricting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 15:33:07 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gatesman", "Kyle", ""], ["Unwin", "James", ""]]}, {"id": "1808.02910", "submitter": "Ray Fair", "authors": "Ray Fair", "title": "Information Content of DSGE Forecasts", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": "CFDP 2140", "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the question whether information is contained in\nforecasts from DSGE models beyond that contained in lagged values, which are\nextensively used in the models. Four sets of forecasts are examined. The\nresults are encouraging for DSGE forecasts of real GDP. The results suggest\nthat there is information in the DSGE forecasts not contained in forecasts\nbased only on lagged values and that there is no information in the\nlagged-value forecasts not contained in the DSGE forecasts. The opposite is\ntrue for forecasts of the GDP deflator.\n  Keywords: DSGE forecasts, Lagged values\n  JEL Classification Codes: E10, E17, C53\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 18:29:54 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Fair", "Ray", ""]]}, {"id": "1808.03070", "submitter": "Yongli Li", "authors": "Yongli Li, Zhi-Ping Fan, and Wei Zhang", "title": "Network-based Referral Mechanism in a Crowdfunding-based Marketing\n  Pattern", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH econ.GN math.OC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdfunding is gradually becoming a modern marketing pattern. By noting that\nthe success of crowdfunding depends on network externalities, our research aims\nto utilize them to provide an applicable referral mechanism in a\ncrowdfunding-based marketing pattern. In the context of network externalities,\nmeasuring the value of leading customers is chosen as the key to coping with\nthe research problem by considering that leading customers take a critical\nstance in forming a referral network. Accordingly, two sequential-move game\nmodels (i.e., basic model and extended model) were established to measure the\nvalue of leading customers, and a skill of matrix transformation was adopted to\nsolve the model by transforming a complicated multi-sequence game into a simple\nsimultaneous-move game. Based on the defined value of leading customers, a\nnetwork-based referral mechanism was proposed by exploring exactly how many\nawards are allocated along the customer sequence to encourage the leading\ncustomers' actions of successful recommendation and by demonstrating two\ngeneral rules of awarding the referrals in our model setting. Moreover, the\nproposed solution approach helps deepen an understanding of the effect of the\nleading position, which is meaningful for designing more numerous referral\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 09:41:12 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Li", "Yongli", ""], ["Fan", "Zhi-Ping", ""], ["Zhang", "Wei", ""]]}, {"id": "1808.03404", "submitter": "Ali Hosseiny", "authors": "Ali Hosseiny, Mohammadreza Absalan, Mohammad Sherafati, Mauro\n  Gallegati", "title": "Hysteresis of economic networks in an XY model", "comments": "To be appeared in Physica A: Statistical Mechanics and its\n  Applications", "journal-ref": null, "doi": "10.1016/j.physa.2018.08.064", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many-body systems can have multiple equilibria. Though the energy of\nequilibria might be the same, still systems may resist to switch from an\nunfavored equilibrium to a favored one. In this paper we investigate occurrence\nof such phenomenon in economic networks. In times of crisis when governments\nintend to stimulate economy, a relevant question is on the proper size of\nstimulus bill. To address the answer, we emphasize the role of hysteresis in\neconomic networks. In times of crises, firms and corporations cut their\nproductions; now since their level of activity is correlated, metastable\nfeatures in the network become prominent. This means that economic networks\nresist against the recovery actions. To measure the size of resistance in the\nnetwork against recovery, we deploy the XY model. Though theoretically the XY\nmodel has no hysteresis, when it comes to the kinetic behavior in the\ndeterministic regimes, we observe a dynamic hysteresis. We find that to\novercome the hysteresis of the network, a minimum size of stimulation is needed\nfor success. Our simulations show that as long as the networks are\nWatts-Strogatz, such minimum is independent of the characteristics of the\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 03:55:20 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hosseiny", "Ali", ""], ["Absalan", "Mohammadreza", ""], ["Sherafati", "Mohammad", ""], ["Gallegati", "Mauro", ""]]}, {"id": "1808.03482", "submitter": "Jaehyung Lee", "authors": "Jaehyung Lee and Minhyung Cho", "title": "Exeum: A Decentralized Financial Platform for Price-Stable\n  Cryptocurrencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Price stability has often been cited as a key reason that cryptocurrencies\nhave not gained widespread adoption as a medium of exchange and continue to\nprove incapable of powering the economy of decentralized applications (DApps)\nefficiently. Exeum proposes a novel method to provide price stable digital\ntokens whose values are pegged to real world assets, serving as a bridge\nbetween the real world and the decentralized economy.\n  Pegged tokens issued by Exeum - for example, USDE refers to a stable token\nissued by the system whose value is pegged to USD - are backed by virtual\nassets in a virtual asset exchange where users can deposit the base token of\nthe system and take long or short positions. Guaranteeing the stability of the\npegged tokens boils down to the problem of maintaining the peg of the virtual\nassets to real world assets, and the main mechanism used by Exeum is\ncontrolling the swap rate of assets. If the swap rate is fully controlled by\nthe system, arbitrageurs can be incentivized enough to restore a broken peg;\nExeum distributes statistical arbitrage trading software to decentralize this\ntype of market making activity. The last major component of the system is a\ncentral bank equivalent that determines the long term interest rate of the base\ntoken, pays interest on the deposit by inflating the supply if necessary, and\nremoves the need for stability fees on pegged tokens, improving their\nusability.\n  To the best of our knowledge, Exeum is the first to propose a truly\ndecentralized method for developing a stablecoin that enables 1:1 value\nconversion between the base token and pegged assets, completely removing the\nmismatch between supply and demand. In this paper, we will also discuss its\napplications, such as improving staking based DApp token models, price stable\ngas fees, pegging to an index of DApp tokens, and performing cross-chain asset\ntransfer of legacy crypto assets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 10:45:58 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Lee", "Jaehyung", ""], ["Cho", "Minhyung", ""]]}, {"id": "1808.03804", "submitter": "Hendrik Scholten", "authors": "Sandra Schneemann, Hendrik Scholten, Christian Deutscher", "title": "The Impact of Age on Nationality Bias: Evidence from Ski Jumping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This empirical research explores the impact of age on nationality bias. World\nCup competition data suggest that judges of professional ski jumping\ncompetitions prefer jumpers of their own nationality and exhibit this\npreference by rewarding them with better marks. Furthermore, the current study\nreveals that this nationality bias is diminished among younger judges, in\naccordance with the reported lower levels of national discrimination among\nyounger generations. Globalisation and its effect in reducing class-based\nthinking may explain this reduced bias in judgment of others.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 13:49:47 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Schneemann", "Sandra", ""], ["Scholten", "Hendrik", ""], ["Deutscher", "Christian", ""]]}, {"id": "1808.04150", "submitter": "Masoud Fekri", "authors": "Sina Aghaei, Amirreza Safari Langroudi, Masoud Fekri", "title": "A Predictive Model for Oil Market under Uncertainty: Data-Driven System\n  Dynamics Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years, there have been a lot of sharp changes in the oil price.\nThese rapid changes cause the traditional models to fail in predicting the\nprice behavior. The main reason for the failure of the traditional models is\nthat they consider the actual value of parameters instead of their\nexpectational ones. In this paper, we propose a system dynamics model that\nincorporates expectational variables in determining the oil price. In our\nmodel, the oil price is determined by the expected demand and supply vs. their\nactual values. Our core model is based upon regression analysis on several\nhistoric time series and adjusted by adding many casual loops in the oil\nmarket. The proposed model in simulated in different scenarios that have\nhappened in the past and our results comply with the trends of the oil price in\neach of the scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:14:44 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Aghaei", "Sina", ""], ["Langroudi", "Amirreza Safari", ""], ["Fekri", "Masoud", ""]]}, {"id": "1808.05142", "submitter": "D\\'ora Gr\\'eta Petr\\'oczy", "authors": "D\\'ora Gr\\'eta Petr\\'oczy, Mark Francis Rogers, L\\'aszl\\'o \\'A.\n  K\\'oczy", "title": "Brexit: The Belated Threat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debates on an EU-leaving referendum arose in several member states after\nBrexit. We want to highlight how the exit of an additional country affects the\npower distribution in the Council of the European Union. We inspect the power\nindices of the member states both with and without the country which might\nleave the union. Our results show a pattern connected to a change in the\nthreshold of the number of member states required for a decision. An exit that\nmodifies this threshold benefits the countries with high population, while an\nexit that does not cause such a change benefits the small member states.\nAccording to our calculations, the threat of Brexit would have worked\ndifferently before the entry of Croatia.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 07:31:15 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Petr\u00f3czy", "D\u00f3ra Gr\u00e9ta", ""], ["Rogers", "Mark Francis", ""], ["K\u00f3czy", "L\u00e1szl\u00f3 \u00c1.", ""]]}, {"id": "1808.07854", "submitter": "Fabi\\'an Riquelme", "authors": "Fabi\\'an Riquelme, Pablo Gonz\\'alez-Cantergiani, Gabriel Godoy", "title": "Voting power of political parties in the Senate of Chile during the\n  whole binomial system period: 1990-2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binomial system is an electoral system unique in the world. It was used\nto elect the senators and deputies of Chile during 27 years, from the return of\ndemocracy in 1990 until 2017. In this paper we study the real voting power of\nthe different political parties in the Senate of Chile during the whole\nbinomial period. We not only consider the different legislative periods, but\nalso any party changes between one period and the next. The real voting power\nis measured by considering power indices from cooperative game theory, which\nare based on the capability of the political parties to form winning\ncoalitions. With this approach, we can do an analysis that goes beyond the\nsimple count of parliamentary seats.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 19:32:11 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:11:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Riquelme", "Fabi\u00e1n", ""], ["Gonz\u00e1lez-Cantergiani", "Pablo", ""], ["Godoy", "Gabriel", ""]]}, {"id": "1808.07909", "submitter": "M. R. Grasselli", "authors": "Matheus R. Grasselli, Alexander Lipton", "title": "On the Normality of Negative Interest Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that a negative interest rate policy (NIRP) can be an effect tool\nfor macroeconomic stabilization. We first discuss how implementing negative\nrates on reserves held at a central bank does not pose any theoretical\ndifficulty, with a reduction in rates operating in exactly the same way when\nrates are positive or negative, and show that this is compatible with an\nendogenous money point of view. We then propose a simplified stock-flow\nconsistent macroeconomic model where rates are allowed to become arbitrarily\nnegative and present simulation evidence for their stabilizing effects. In\npractice, the existence of physical cash imposes a lower bound for interest\nrates, which in our view is the main reason for the lack of effectiveness of\nnegative interest rates in the countries that adopted them as part of their\nmonetary policy. We conclude by discussing alternative ways to overcome this\nlower bound , in particular the use of central bank digital currencies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 19:13:42 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Grasselli", "Matheus R.", ""], ["Lipton", "Alexander", ""]]}, {"id": "1808.07959", "submitter": "Jorge Faleiro", "authors": "Jorge Faleiro, Edward Tsang", "title": "Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual\n  Framework for Large-Scale Collaboration and Transparent Investigation in\n  Financial Markets", "comments": "14th Simulation and Analytics Seminar, Bank of Finland, Helsinki,\n  2016", "journal-ref": null, "doi": null, "report-no": "WP076-15", "categories": "q-fin.CP econ.EM econ.GN q-fin.EC q-fin.TR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern investigation in economics and in other sciences requires the ability\nto store, share, and replicate results and methods of experiments that are\noften multidisciplinary and yield a massive amount of data. Given the\nincreasing complexity and growing interaction across diverse bodies of\nknowledge it is becoming imperative to define a platform to properly support\ncollaborative research and track origin, accuracy and use of data. This paper\nstarts by defining a set of methods leveraging scientific principles and\nadvocating the importance of those methods in multidisciplinary, computer\nintensive fields like computational finance. The next part of this paper\ndefines a class of systems called scientific support systems, vis-a-vis usages\nin other research fields such as bioinformatics, physics and engineering. We\noutline a basic set of fundamental concepts, and list our goals and motivation\nfor leveraging such systems to enable large-scale investigation, \"crowd powered\nscience\", in economics. The core of this paper provides an outline of FRACTI in\nfive steps. First we present definitions related to scientific support systems\nintrinsic to finance and describe common characteristics of financial use\ncases. The second step concentrates on what can be exchanged through the\ndefinition of shareable entities called contributions. The third step is the\ndescription of a classification system for building blocks of the conceptual\nframework, called facets. The fourth step introduces the meta-model that will\nenable provenance tracking and representation of data fragments and simulation.\nFinally we describe intended cases of use to highlight main strengths of\nFRACTI: application of the scientific method for investigation in computational\nfinance, large-scale collaboration and simulation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 21:54:02 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Faleiro", "Jorge", ""], ["Tsang", "Edward", ""]]}, {"id": "1808.08249", "submitter": "Orazio Angelini", "authors": "Orazio Angelini, Tiziana Di Matteo", "title": "Complexity of products: the effect of data regularisation", "comments": null, "journal-ref": null, "doi": "10.3390/e20110814", "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among several developments, the field of Economic Complexity (EC) has notably\nseen the introduction of two new techniques. One is the Bootstrapped Selective\nPredictability Scheme (SPSb), which can provide quantitative forecasts of the\nGross Domestic Product of countries. The other, Hidden Markov Model (HMM)\nregularisation, denoises the datasets typically employed in the literature. We\ncontribute to EC along three different directions. First, we prove the\nconvergence of the SPSb algorithm to a well-known statistical learning\ntechnique known as Nadaraya-Watson Kernel regression. The latter has\nsignificantly lower time complexity, produces deterministic results, and it is\ninterchangeable with SPSb for the purpose of making predictions. Second, we\nstudy the effects of HMM regularization on the Product Complexity and logPRODY\nmetrics, for which a model of time evolution has been recently proposed. We\nfind confirmation for the original interpretation of the logPRODY model as\ndescribing the change in the global market structure of products with new\ninsights allowing a new interpretation of the Complexity measure, for which we\npropose a modification. Third, we explore new effects of regularisation on the\ndata. We find that it reduces noise, and observe for the first time that it\nincreases nestedness in the export network adjacency matrix.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 18:10:21 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 14:20:06 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Angelini", "Orazio", ""], ["Di Matteo", "Tiziana", ""]]}, {"id": "1808.08585", "submitter": "Linjing Li", "authors": "Jiaqi Liang, Linjing Li, Daniel Zeng", "title": "Evolutionary dynamics of cryptocurrency transaction networks: An\n  empirical study", "comments": null, "journal-ref": "PLoS ONE 13(8): e0202202 (2018)", "doi": "10.1371/journal.pone.0202202", "report-no": null, "categories": "q-fin.ST cs.CR cs.DC econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptocurrency is a well-developed blockchain technology application that is\ncurrently a heated topic throughout the world. The public availability of\ntransaction histories offers an opportunity to analyze and compare different\ncryptocurrencies. In this paper, we present a dynamic network analysis of three\nrepresentative blockchain-based cryptocurrencies: Bitcoin, Ethereum, and\nNamecoin. By analyzing the accumulated network growth, we find that, unlike\nmost other networks, these cryptocurrency networks do not always densify over\ntime, and they are changing all the time with relatively low node and edge\nrepetition ratios. Therefore, we then construct separate networks on a monthly\nbasis, trace the changes of typical network characteristics (including degree\ndistribution, degree assortativity, clustering coefficient, and the largest\nconnected component) over time, and compare the three. We find that the degree\ndistribution of these monthly transaction networks cannot be well fitted by the\nfamous power-law distribution, at the same time, different currency still has\ndifferent network properties, e.g., both Bitcoin and Ethereum networks are\nheavy-tailed with disassortative mixing, however, only the former can be\ntreated as a small world. These network properties reflect the evolutionary\ncharacteristics and competitive power of these three cryptocurrencies and\nprovide a foundation for future research.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 16:27:21 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Liang", "Jiaqi", ""], ["Li", "Linjing", ""], ["Zeng", "Daniel", ""]]}, {"id": "1808.08717", "submitter": "Ashwin Seshadri", "authors": "Ashwin K Seshadri", "title": "Economics of carbon-dioxide abatement under an exogenous constraint on\n  cumulative emissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fossil-fuel induced contribution to further warming over the 21st century\nwill be determined largely by integrated CO2 emissions over time rather than\nthe precise timing of the emissions, with a relation of near-proportionality\nbetween global warming and cumulative CO2 emissions. This paper examines\noptimal abatement pathways under an exogenous constraint on cumulative\nemissions. Least cost abatement pathways have carbon tax rising at the\nrisk-free interest rate, but if endogenous learning or climate damage costs are\nincluded in the analysis, the carbon tax grows more slowly. The inclusion of\ndamage costs in the optimization leads to a higher initial carbon tax, whereas\nthe effect of learning depends on whether it appears as an additive or\nmultiplicative contribution to the marginal cost curve. Multiplicative models\nare common in the literature and lead to delayed abatement and a smaller\ninitial tax. The required initial carbon tax increases with the cumulative\nabatement goal and is higher for lower interest rates. Delaying the start of\nabatement is costly owing to the increasing marginal abatement cost. Lower\ninterest rates lead to higher relative costs of delaying abatement because\nthese induce higher abatement rates early on. The fraction of business-as-usual\nemissions (BAU) avoided in optimal pathways increases for low interest rates\nand rapid growth of the abatement cost curve, which allows a lower threshold\nglobal warming goal to become attainable without overshoot in temperature. Each\nyear of delay in starting abatement raises this threshold by an increasing\namount, because the abatement rate increases exponentially with time.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:46:23 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 12:00:37 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Seshadri", "Ashwin K", ""]]}, {"id": "1808.09686", "submitter": "Samuel Cohen", "authors": "Samuel N. Cohen, Timo Henckel, Gordon D. Menzies, Johannes\n  Muhle-Karbe, Daniel J. Zizzo", "title": "Switching Cost Models as Hypothesis Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN math.OC q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We relate models based on costs of switching beliefs (e.g. due to\ninattention) to hypothesis tests. Specifically, for an inference problem with a\npenalty for mistakes and for switching the inferred value, a band of inaction\nis optimal. We show this band is equivalent to a confidence interval, and\ntherefore to a two-sided hypothesis test.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 08:49:50 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Cohen", "Samuel N.", ""], ["Henckel", "Timo", ""], ["Menzies", "Gordon D.", ""], ["Muhle-Karbe", "Johannes", ""], ["Zizzo", "Daniel J.", ""]]}, {"id": "1808.09887", "submitter": "A. Mushfiq Mobarak", "authors": "Andres Gonzalez Lira and Ahmed Mushfiq Mobarak", "title": "Enforcing Regulation Under Illicit Adaptation", "comments": "63 pages; Keywords: Enforcement, Regulation, Law and Economics,\n  Fisheries; JEL Classification Codes: K42, O1, L51", "journal-ref": null, "doi": null, "report-no": "CFDP 2143", "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to curb illegal activity by enforcing regulations gets complicated\nwhen agents react to the new regulatory regime in unanticipated ways to\ncircumvent enforcement. We present a research strategy that uncovers such\nreactions, and permits program evaluation net of such adaptive behaviors. Our\ninterventions were designed to reduce over-fishing of the critically endangered\nPacific hake by either (a) monitoring and penalizing vendors that sell illegal\nfish or (b) discouraging consumers from purchasing using an information\ncampaign. Vendors attempt to circumvent the ban through hidden sales and other\nmeans, which we track using mystery shoppers. Instituting random monitoring\nvisits are much more effective in reducing true hake availability by limiting\nsuch cheating, compared to visits that occur on a predictable schedule.\nMonitoring at higher frequency (designed to limit temporal displacement of\nillegal sales) backfires, because targeted agents learn faster, and cheat more\neffectively. Sophisticated policy design is therefore crucial for determining\nthe sustained, longer-term effects of enforcement. Data collected from\nfishermen, vendors, and consumers allow us to document the upstream,\ndownstream, spillover, and equilibrium effects of enforcement on the entire\nsupply chain. The consumer information campaign generates two-thirds of the\ngains compared to random monitoring, but is simpler for the government to\nimplement and almost as cost-effective.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:38:24 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Lira", "Andres Gonzalez", ""], ["Mobarak", "Ahmed Mushfiq", ""]]}, {"id": "1808.10428", "submitter": "Andrea Zaccaria", "authors": "Angelica Sbardella, Emanuele Pugliese, Andrea Zaccaria, and Pasquale\n  Scaramozzino", "title": "The role of complex analysis in modeling economic growth", "comments": null, "journal-ref": null, "doi": "10.3390/e20110883", "report-no": null, "categories": "econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development and growth are complex and tumultuous processes. Modern economic\ngrowth theories identify some key determinants of economic growth. However, the\nrelative importance of the determinants remains unknown, and additional\nvariables may help clarify the directions and dimensions of the interactions.\nThe novel stream of literature on economic complexity goes beyond aggregate\nmeasures of productive inputs, and considers instead a more granular and\nstructural view of the productive possibilities of countries, i.e. their\ncapabilities. Different endowments of capabilities are crucial ingredients in\nexplaining differences in economic performances. In this paper we employ\neconomic fitness, a measure of productive capabilities obtained through complex\nnetwork techniques. Focusing on the combined roles of fitness and some more\ntraditional drivers of growth, we build a bridge between economic growth\ntheories and the economic complexity literature. Our findings, in agreement\nwith other recent empirical studies, show that fitness plays a crucial role in\nfostering economic growth and, when it is included in the analysis, can be\neither complementary to traditional drivers of growth or can completely\novershadow them.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:47:28 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sbardella", "Angelica", ""], ["Pugliese", "Emanuele", ""], ["Zaccaria", "Andrea", ""], ["Scaramozzino", "Pasquale", ""]]}, {"id": "1808.10651", "submitter": "Jaap Abbring", "authors": "Jaap H. Abbring and {\\O}ystein Daljord", "title": "Identifying the Discount Factor in Dynamic Discrete Choice Models", "comments": "39 pages", "journal-ref": "Quantitative Economics 11(2) 471-501 (May 2020)", "doi": "10.3982/QE1352", "report-no": null, "categories": "econ.EM econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical research often cites observed choice responses to variation that\nshifts expected discounted future utilities, but not current utilities, as an\nintuitive source of information on time preferences. We study the\nidentification of dynamic discrete choice models under such economically\nmotivated exclusion restrictions on primitive utilities. We show that each\nexclusion restriction leads to an easily interpretable moment condition with\nthe discount factor as the only unknown parameter. The identified set of\ndiscount factors that solves this condition is finite, but not necessarily a\nsingleton. Consequently, in contrast to common intuition, an exclusion\nrestriction does not in general give point identification. Finally, we show\nthat exclusion restrictions have nontrivial empirical content: The implied\nmoment conditions impose restrictions on choices that are absent from the\nunconstrained model.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:53:21 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 07:28:37 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 23:30:29 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 15:08:50 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Abbring", "Jaap H.", ""], ["Daljord", "\u00d8ystein", ""]]}]