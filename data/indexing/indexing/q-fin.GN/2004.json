[{"id": "2004.01831", "submitter": "Johannes Stroebel", "authors": "Stefano Giglio, Matteo Maggiori, Johannes Stroebel, Stephen Utkus", "title": "Inside the Mind of a Stock Market Crash", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how investor expectations about economic growth and stock returns\nchanged during the February-March 2020 stock market crash induced by the\nCOVID-19 pandemic, as well as during the subsequent partial stock market\nrecovery. We surveyed retail investors who are clients of Vanguard at three\npoints in time: (i) on February 11-12, around the all-time stock market high,\n(ii) on March 11-12, after the stock market had collapsed by over 20\\%, and\n(iii) on April 16-17, after the market had rallied 25\\% from its lowest point.\nFollowing the crash, the average investor turned more pessimistic about the\nshort-run performance of both the stock market and the real economy. Investors\nalso perceived higher probabilities of both further extreme stock market\ndeclines and large declines in short-run real economic activity. In contrast,\ninvestor expectations about long-run (10-year) economic and stock market\noutcomes remained largely unchanged, and, if anything, improved. Disagreement\namong investors about economic and stock market outcomes also increased\nsubstantially following the stock market crash, with the disagreement\npersisting through the partial market recovery. Those respondents who were the\nmost optimistic in February saw the largest decline in expectations, and sold\nthe most equity. Those respondents who were the most pessimistic in February\nlargely left their portfolios unchanged during and after the crash.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 01:30:10 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 22:51:41 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Giglio", "Stefano", ""], ["Maggiori", "Matteo", ""], ["Stroebel", "Johannes", ""], ["Utkus", "Stephen", ""]]}, {"id": "2004.04605", "submitter": "Yo-Der Song", "authors": "Yo-Der Song and Tomaso Aste (University College London)", "title": "The cost of Bitcoin mining has never really increased", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bitcoin network is burning a large amount of energy for mining. In this\npaper we estimate the lower bound for the global energy cost for a period of\nten years from 2010, taking into account changing oil costs, improvements in\nhashing technologies and hashing activity. Despite a ten-billion-fold increase\nin hashing activity and a ten-million-fold increase in total energy\nconsumption, we find the cost relative to the volume of transactions has not\nincreased nor decreased since 2010. This is consistent with the perspective\nthat, in order to keep a the Blockchain system secure from double spending\nattacks, the proof or work must cost a sizable fraction of the value that can\nbe transferred through the network. We estimate that in the Bitcoin network\nthis fraction is of the order of 1%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:47:11 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 21:50:27 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Song", "Yo-Der", "", "University College London"], ["Aste", "Tomaso", "", "University College London"]]}, {"id": "2004.06626", "submitter": "Jose Luis Subias", "authors": "J. L. Subias", "title": "Potential in the Schrodinger equation: estimation from empirical data", "comments": "17 pages, 24 figures, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.TR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent model for the stock market calculates future price distributions of\na stock as a wave function of a quantum particle confined in an infinite\npotential well. In such a model the question arose as to how to estimate the\nclassical potential needed for solving the Schrodinger equation. In the present\narticle the method used in that work for evaluating the potential is described,\nin the simplest version to implement, and more sophisticated implementations\nare suggested later.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:43:50 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Subias", "J. L.", ""]]}, {"id": "2004.06642", "submitter": "Jim Samuel", "authors": "Jim Samuel", "title": "Information Token Driven Machine Learning for Electronic Markets:\n  Performance Effects in Behavioral Financial Big Data Analytics", "comments": "Post-print, to be cited as (APA): Samuel, J. (2017). Information\n  Token Driven Machine Learning for Electronic Markets: Performance Effects in\n  Behavioral Financial Big Data Analytics. JISTEM-Journal of Information\n  Systems and Technology Management, 14(3), 371-383", "journal-ref": "JISTEM - Journal of Information Systems and Technology Management,\n  2017, vol.14 no.3, On-line version ISSN 1807-1775", "doi": "10.4301/s1807-17752017000300005", "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunct with the universal acceleration in information growth, financial\nservices have been immersed in an evolution of information dynamics. It is not\njust the dramatic increase in volumes of data, but the speed, the complexity\nand the unpredictability of big-data phenomena that have compounded the\nchallenges faced by researchers and practitioners in financial services. Math,\nstatistics and technology have been leveraged creatively to create analytical\nsolutions. Given the many unique characteristics of financial bid data (FBD) it\nis necessary to gain insights into strategies and models that can be used to\ncreate FBD specific solutions. Behavioral finance data, a subset of FBD, is\nseeing exponential growth and this presents an unprecedented opportunity to\nstudy behavioral finance employing big data analytics methodologies. The\npresent study maps machine learning (ML) techniques and behavioral finance\ncategories to explore the potential for using ML techniques to address\nbehavioral aspects in FBD. The ontological feasibility of such an approach is\npresented and the primary purpose of this study is propositioned- ML based\nbehavioral models can effectively estimate performance in FBD. A simple machine\nlearning algorithm is successfully employed to study behavioral performance in\nan artificial stock market to validate the propositions.\n  Keywords: Information; Big Data; Electronic Markets; Analytics; Behavior\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:42:23 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Samuel", "Jim", ""]]}, {"id": "2004.08167", "submitter": "Louis Bertucci", "authors": "Charles Bertucci (1), Louis Bertucci (2 and 3), Jean-Michel Lasry (4),\n  Pierre-Louis Lions (4 and 5) ((1) CMAP, Ecole Polytechnique, Palaiseau,\n  France, (2) Institut Louis Bachelier, Paris, France, (3) Haas School of\n  Business, UC Berkeley, Berkeley, California, (4) Universit\\'e Paris-Dauphine,\n  PSL Research University, CEREMADE, Paris, France, (5) Coll\\`ege de France,\n  Paris, France)", "title": "Mean Field Game Approach to Bitcoin Mining", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH math.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of the Proof-of-Work consensus algorithm, used on the\nBitcoin blockchain, using a Mean Field Game framework. Using a master equation,\nwe provide an equilibrium characterization of the total computational power\ndevoted to mining the blockchain (hashrate). From a simple setting we show how\nthe master equation approach allows us to enrich the model by relaxing most of\nthe simplifying assumptions. The essential structure of the game is preserved\nacross all the enrichments. In deterministic settings, the hashrate ultimately\nreaches a steady state in which it increases at the rate of technological\nprogress. In stochastic settings, there exists a target for the hashrate for\nevery possible random state. As a consequence, we show that in equilibrium the\nsecurity of the underlying blockchain is either $i)$ constant, or $ii)$\nincreases with the demand for the underlying cryptocurrency.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 10:57:33 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bertucci", "Charles", "", "2 and 3"], ["Bertucci", "Louis", "", "2 and 3"], ["Lasry", "Jean-Michel", "", "4 and 5"], ["Lions", "Pierre-Louis", "", "4 and 5"]]}, {"id": "2004.10119", "submitter": "Luigi Bellomarini", "authors": "Luigi Bellomarini, Marco Benedetti, Andrea Gentili, Rosario Laurendi,\n  Davide Magnanimi, Antonio Muci, Emanuel Sallinger", "title": "COVID-19 and Company Knowledge Graphs: Assessing Golden Powers and\n  Economic Impact of Selective Lockdown via AI Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the COVID-19 outbreak, governments have applied progressive restrictions\nto production activities, permitting only those that are considered strategic\nor that provide essential services. This is particularly apparent in countries\nthat have been stricken hard by the virus, with Italy being a major example.\nYet we know that companies are not just isolated entities: They organize\nthemselves into intricate shareholding structures --- forming company networks\n--- distributing decision power and dividends in sophisticated schemes for\nvarious purposes.\n  One tool from the Artificial Intelligence (AI) toolbox that is particularly\neffective to perform reasoning tasks on domains characterized by many entities\nhighly interconnected with one another is Knowledge Graphs (KG). In this work,\nwe present a visionary opinion and report on ongoing work about the application\nof Automated Reasoning and Knowledge Graph technology to address the impact of\nthe COVID-19 outbreak on the network of Italian companies and support the\napplication of legal instruments for the protection of strategic companies from\ntakeovers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:55:47 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Bellomarini", "Luigi", ""], ["Benedetti", "Marco", ""], ["Gentili", "Andrea", ""], ["Laurendi", "Rosario", ""], ["Magnanimi", "Davide", ""], ["Muci", "Antonio", ""], ["Sallinger", "Emanuel", ""]]}, {"id": "2004.10318", "submitter": "Simon Rudkin", "authors": "Wanling Qiu, Simon Rudkin, Pawel Dlotko", "title": "Refining Understanding of Corporate Failure through a Topological Data\n  Analysis Mapping of Altman's Z-Score Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporate failure resonates widely leaving practitioners searching for\nunderstanding of default risk. Managers seek to steer away from trouble, credit\nproviders to avoid risky loans and investors to mitigate losses. Applying\nTopological Data Analysis tools this paper explores whether failing firms from\nthe United States organise neatly along the five predictors of default proposed\nby the Z-score models. Firms are represented as a point cloud in a five\ndimensional space, one axis for each predictor. Visualising that cloud using\nBall Mapper reveals failing firms are not often neighbours. As new modelling\napproaches vie to better predict firm failure, often using black boxes to\ndeliver potentially over-fitting models, a timely reminder is sounded on the\nimportance of evidencing the identification process. Value is added to the\nunderstanding of where in the parameter space failure occurs, and how firms\nmight act to move away from financial distress. Further, lenders may find\nopportunity amongst subsets of firms that are traditionally considered to be in\ndanger of bankruptcy but actually sit in characteristic spaces where failure\nhas not occurred.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:58:26 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Qiu", "Wanling", ""], ["Rudkin", "Simon", ""], ["Dlotko", "Pawel", ""]]}, {"id": "2004.10537", "submitter": "Dominik Martin", "authors": "Dominik Martin, Philipp Spitzer, Niklas K\\\"uhl", "title": "A New Metric for Lumpy and Intermittent Demand Forecasts:\n  Stock-keeping-oriented Prediction Error Costs", "comments": "Proceedings of the 53rd Annual Hawaii International Conference on\n  System Sciences (HICSS-53), Grand Wailea, Maui, HI, January 7-10, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasts of product demand are essential for short- and long-term\noptimization of logistics and production. Thus, the most accurate prediction\npossible is desirable. In order to optimally train predictive models, the\ndeviation of the forecast compared to the actual demand needs to be assessed by\na proper metric. However, if a metric does not represent the actual prediction\nerror, predictive models are insufficiently optimized and, consequently, will\nyield inaccurate predictions. The most common metrics such as MAPE or RMSE,\nhowever, are not suitable for the evaluation of forecasting errors, especially\nfor lumpy and intermittent demand patterns, as they do not sufficiently account\nfor, e.g., temporal shifts (prediction before or after actual demand) or\ncost-related aspects. Therefore, we propose a novel metric that, in addition to\nstatistical considerations, also addresses business aspects. Additionally, we\nevaluate the metric based on simulated and real demand time series from the\nautomotive aftermarket.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:50:24 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Martin", "Dominik", ""], ["Spitzer", "Philipp", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2004.11118", "submitter": "Le Anh Vu", "authors": "Le Anh Vu, Duong Quang Hoa, Nguyen Minh Tri and Ha Van Hieu", "title": "Some Applications of Lie Groups in Theory of Technical Progress", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent decades, we have known some interesting applications of Lie theory\nin the theory of technological progress. Firstly, we will discuss some results\nof R. Saito in \\cite{rS1980} and \\cite{rS1981} about the application modeling\nof Lie groups in the theory of technical progress. Next, we will describe the\nresult on Romanian economy of G. Zaman and Z. Goschin in \\cite{ZG2010}.\nFinally, by using Sato's results and applying the method of G. Zaman and Z.\nGoschin, we give an estimation of the GDP function of Viet Nam for the\n1995-2018 period and give several important observations about the impact of\ntechnical progress on economic growth of Viet Nam.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 08:50:21 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Vu", "Le Anh", ""], ["Hoa", "Duong Quang", ""], ["Tri", "Nguyen Minh", ""], ["Van Hieu", "Ha", ""]]}, {"id": "2004.11121", "submitter": "Takahiro Yabe", "authors": "Takahiro Yabe, Yunchang Zhang, Satish Ukkusuri", "title": "Quantifying the Economic Impact of Extreme Shocks on Businesses using\n  Human Mobility Data: a Bayesian Causal Inference Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, extreme shocks, such as natural disasters, are increasing in\nboth frequency and intensity, causing significant economic loss to many cities\naround the world. Quantifying the economic cost of local businesses after\nextreme shocks is important for post-disaster assessment and pre-disaster\nplanning. Conventionally, surveys have been the primary source of data used to\nquantify damages inflicted on businesses by disasters. However, surveys often\nsuffer from high cost and long time for implementation, spatio-temporal\nsparsity in observations, and limitations in scalability. Recently, large scale\nhuman mobility data (e.g. mobile phone GPS) have been used to observe and\nanalyze human mobility patterns in an unprecedented spatio-temporal granularity\nand scale. In this work, we use location data collected from mobile phones to\nestimate and analyze the causal impact of hurricanes on business performance.\nTo quantify the causal impact of the disaster, we use a Bayesian structural\ntime series model to predict the counterfactual performances of affected\nbusinesses (what if the disaster did not occur?), which may use performances of\nother businesses outside the disaster areas as covariates. The method is tested\nto quantify the resilience of 635 businesses across 9 categories in Puerto Rico\nafter Hurricane Maria. Furthermore, hierarchical Bayesian models are used to\nreveal the effect of business characteristics such as location and category on\nthe long-term resilience of businesses. The study presents a novel and more\nefficient method to quantify business resilience, which could assist policy\nmakers in disaster preparation and relief processes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:44:56 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yabe", "Takahiro", ""], ["Zhang", "Yunchang", ""], ["Ukkusuri", "Satish", ""]]}, {"id": "2004.11122", "submitter": "Ravi Vadlamani", "authors": "Vadlamani Ravi and Vadlamani Madhav", "title": "Optimizing the reliability of a bank with Logistic Regression and\n  Particle Swarm Optimization", "comments": "11 Pages, 2 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is well-known that disciplines such as mechanical engineering, electrical\nengineering, civil engineering, aerospace engineering, chemical engineering and\nsoftware engineering witnessed successful applications of reliability\nengineering concepts. However, the concept of reliability in its strict sense\nis missing in financial services. Therefore, in order to fill this gap, in a\nfirst-of-its-kind-study, we define the reliability of a bank/firm in terms of\nthe financial ratios connoting the financial health of the bank to withstand\nthe likelihood of insolvency or bankruptcy. For the purpose of estimating the\nreliability of a bank, we invoke a statistical and machine learning algorithm\nnamely, logistic regression (LR). Once, the parameters are estimated in the 1st\nstage, we fix them and treat the financial ratios as decision variables. Thus,\nin the 1st stage, we accomplish the hitherto unknown way of estimating the\nreliability of a bank. Subsequently, in the 2nd stage, in order to maximize the\nreliability of the bank, we formulate an unconstrained optimization problem in\na single-objective environment and solve it using the well-known particle swarm\noptimization (PSO) algorithm. Thus, in essence, these two stages correspond to\npredictive and prescriptive analytics respectively. The proposed 2-stage\nstrategy of using them in tandem is beneficial to the decision-makers within a\nbank who can try to achieve the optimal or near-optimal values of the financial\nratios in order to maximize the reliability which is tantamount to safeguarding\ntheir bank against solvency or bankruptcy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:37:43 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ravi", "Vadlamani", ""], ["Madhav", "Vadlamani", ""]]}, {"id": "2004.11148", "submitter": "Min-Young Lee", "authors": "Min-Young Lee, Woo-Sung Jung, Gabjin Oh", "title": "Trading characteristics of member firms on the Korea Exchange", "comments": null, "journal-ref": null, "doi": "10.3938/jkps.76.1144", "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the characteristics of the member firms on the Korea\nExchange. The member firms intermediate between the market participants and the\nexchange, and all the participants should trade stocks through members. To\nidentify the characteristics of member firms, all member firms are categorized\ninto three groups, such as the domestic members similar to individuals (DIMs),\nthe domestic members similar to institutions (DSMs), and the foreign members\n(FRMs), in terms of the type of investor. We examine the dynamics of the member\nfirms. The trading characteristics of members are revealed through the\ndirectionality and trend. While FRMs tend to trade one-way and move with the\nprice change, DIMs are the opposite. In the market, DIMs and DSMs do herd and\nthe herding moves in the opposite direction of the price change. One the other\nhand, FRMs do herd in the direction of the price change. The network analysis\nsupports that the members are clustered into three groups similar to DIMs,\nDSMs, and FRMs. Finally, random matrix theory and a cross-sectional regression\nshow that the inventory variation of members possesses significant information\nabout stock prices and that member herding helps to price the stocks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 13:11:37 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Lee", "Min-Young", ""], ["Jung", "Woo-Sung", ""], ["Oh", "Gabjin", ""]]}, {"id": "2004.11270", "submitter": "Ivan Arraut Dr.", "authors": "Ivan Arraut, Alan Au and Alan Ching-biu Tse", "title": "On the multiplicity of the martingale condition: Spontaneous symmetry\n  breaking in Quantum Finance", "comments": "14 pages, submitted to Physica A", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the martingale condition in the stock market can be\ninterpreted as a vacuum condition when we express the financial equations in\nthe Hamiltonian form. We then show that the symmetry under the changes of the\nprices is spontaneously broken in general and the symmetry under changes in the\nvolatility, for the case of the Merton-Garman (MG) equation, is also\nspontaneously broken. This reproduces a vacuum degeneracy for the system. In\nthis way, we find the conditions under which, the martingale condition can be\nconsidered to be a non-degenerate vacuum. This gives us a surprising connection\nbetween spontaneous symmetry breaking and the flow of information through the\nboundaries for the financial systems. Subsequently, we find an extended\nmartingale condition for the MG equation, depending not only prices but also on\nthe volatility and finally, we show what happens if we include additional\nnon-derivative terms on the Black Scholes and on the MG equations, breaking\nthen some other symmetries of the system spontaneously.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 08:19:39 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Arraut", "Ivan", ""], ["Au", "Alan", ""], ["Tse", "Alan Ching-biu", ""]]}, {"id": "2004.11279", "submitter": "Osman Gulseven", "authors": "Osman Gulseven", "title": "Estimating the Demand Factors and Willingness to Pay for Agricultural\n  Insurance", "comments": "6 pages, 3 graphs, 3 figures", "journal-ref": "Australian Journal of Engineering Research, 1(4), pp 13 to 18\n  (2014)", "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates the effect of prices and socio-demographic\nvariables on the farmers decision to purchase agricultural insurance. A survey\nhas been conducted to 200 farmers most of whom are engaged in diversified\nincome-generating activities. The logistic estimation results suggest that\neducation and household income from farming activities positively affect the\nlikelihood of purchasing insurance. The demand for insurance is negatively\ncorrelated with the premium paid per insured value, suggesting that insurance\nis a normal good. Farmers are willing to pay (WTP) increasingly higher premiums\nfor contracts with a higher coverage ratio. According to the valuation model,\nthe WTP declines sharply for coverage ratios under 70%.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 12:32:57 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gulseven", "Osman", ""]]}, {"id": "2004.13008", "submitter": "Anca Gheorghiu", "authors": "Ion Spanulescu, Anca Gheorghiu", "title": "Econophysics Approach and Model on Mixed Economy", "comments": "15 pages, 4 figures, ENEC 2019, Bucharest, Romania, 2019", "journal-ref": "Hyperion International Journal of Econophysics and New Economy,vol\n  12 (2019),nr.2, pp.19-34", "doi": null, "report-no": null, "categories": "q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the general principles and categories of mixed economy that\ncurrently exist in almost all countries of the world are presented. The paper\nalso presents an Advanced Model of Mixed Economy with Threshold (AMMET), which\nis characterized by a reduced value (approx. 10-15%) of the State and public\nsector participation in the national economy and proposes and analyzes an\neconophysics model for the mixed economy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 08:09:27 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Spanulescu", "Ion", ""], ["Gheorghiu", "Anca", ""]]}, {"id": "2004.13871", "submitter": "Alan Lewis", "authors": "Alan L. Lewis", "title": "US Equity Risk Premiums during the COVID-19 Pandemic", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study equity risk premiums in the United States during the COVID-19\npandemic.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 22:18:04 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lewis", "Alan L.", ""]]}, {"id": "2004.13919", "submitter": "Anuraag Singh", "authors": "Anuraag Singh, Giorgio Triulzi and Christopher L. Magee", "title": "Technological improvement rate estimates for all technologies: Use of\n  patent data and an extended domain description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CY physics.soc-ph q-fin.EC q-fin.GN q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt to provide a comprehensive granular account of the\npace of technological change. More specifically, we survey estimated yearly\nperformance improvement rates for nearly all definable technologies for the\nfirst time. We do this by creating a correspondence of all patents within the\nUS patent system to a set of technology domains. A technology domain is a body\nof patented inventions achieving the same technological function using the same\nknowledge and scientific principles. We obtain a set of 1757 domains using an\nextension of the previously defined classification overlap method (COM). These\ndomains contain 97.14% of all patents within the entire US patent system. From\nthe identified patent sets, we calculated the average centrality of the patents\nin each domain to estimate their improvement rates, following a methodology\ntested in prior work. The estimated improvement rates vary from a low of 1.9%\nper year for the Mechanical Skin treatment - Hair Removal and wrinkles domain\nto a high of 228.8% per year for the Network management - client-server\napplications domain. We developed a one-line descriptor identifying the\ntechnological function achieved and the underlying knowledge base for the\nlargest 50, fastest 20 as well as slowest 20 of these domains, which cover more\nthan forty percent of the patent system. In general, the rates of improvement\nwere not a strong function of the patent set size and the fastest improving\ndomains are predominantly software-based. We make available an online system\nthat allows for automated searching for domains and improvement rates\ncorresponding to any technology of interest to researchers, strategists and\npolicy formulators.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:54:55 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Singh", "Anuraag", ""], ["Triulzi", "Giorgio", ""], ["Magee", "Christopher L.", ""]]}]