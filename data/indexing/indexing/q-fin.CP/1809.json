[{"id": "1809.00082", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios and Cody Hyndman", "title": "NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation", "comments": "28 pages: main body, 24 pages: appendix, 8 Figures, 11 Tables", "journal-ref": "Journal of Machine Learning Research (JMLR), Volume: 22; 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective feature representation is key to the predictive performance of any\nalgorithm. This paper introduces a meta-procedure, called Non-Euclidean\nUpgrading (NEU), which learns feature maps that are expressive enough to embed\nthe universal approximation property (UAP) into most model classes while only\noutputting feature maps that preserve any model class's UAP. We show that NEU\ncan learn any feature map with these two properties if that feature map is\nasymptotically deformable into the identity. We also find that the\nfeature-representations learned by NEU are always submanifolds of the feature\nspace. NEU's properties are derived from a new deep neural model that is\nuniversal amongst all orientation-preserving homeomorphisms on the input space.\nWe derive qualitative and quantitative approximation guarantees for this\narchitecture. We quantify the number of parameters required for this new\narchitecture to memorize any set of input-output pairs while simultaneously\nfixing every point of the input space lying outside some compact set, and we\nquantify the size of this set as a function of our model's depth. Moreover, we\nshow that no deep feed-forward network with commonly used activation function\nhas all these properties. NEU's performance is evaluated against competing\nmachine learning methods on various regression and dimension reduction tasks\nboth with financial and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 23:38:00 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 16:43:31 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 12:23:47 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 09:50:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kratsios", "Anastasis", ""], ["Hyndman", "Cody", ""]]}, {"id": "1809.01332", "submitter": "Michael Harre", "authors": "Michael S. Harr\\'e", "title": "Multi-agent Economics and the Emergence of Critical Markets", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN nlin.AO q-fin.CP q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dual crises of the sub-prime mortgage crisis and the global financial\ncrisis has prompted a call for explanations of non-equilibrium market dynamics.\nRecently a promising approach has been the use of agent based models (ABMs) to\nsimulate aggregate market dynamics. A key aspect of these models is the\nendogenous emergence of critical transitions between equilibria, i.e. market\ncollapses, caused by multiple equilibria and changing market parameters.\nSeveral research themes have developed microeconomic based models that include\nmultiple equilibria: social decision theory (Brock and Durlauf), quantal\nresponse models (McKelvey and Palfrey), and strategic complementarities\n(Goldstein). A gap that needs to be filled in the literature is a unified\nanalysis of the relationship between these models and how aggregate criticality\nemerges from the individual agent level. This article reviews the agent-based\nfoundations of markets starting with the individual agent perspective of\nMcFadden and the aggregate perspective of catastrophe theory emphasising\nconnections between the different approaches. It is shown that changes in the\nuncertainty agents have in the value of their interactions with one another,\neven if these changes are one-sided, plays a central role in systemic market\nrisks such as market instability and the twin crises effect. These interactions\ncan endogenously cause crises that are an emergent phenomena of markets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 05:25:07 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Harr\u00e9", "Michael S.", ""]]}, {"id": "1809.01342", "submitter": "Gianni Arioli", "authors": "Giovanni Paolinelli, Gianni Arioli", "title": "A model for stocks dynamics based on a non-Gaussian path integral", "comments": "26 pages", "journal-ref": null, "doi": "10.1016/j.physa.2018.11.044", "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for the dynamics of stock prices based on a non\nquadratic path integral. The model is a generalization of Ilinski's path\nintegral model, more precisely we choose a different action, which can be tuned\nto different time scales. The result is a model with a very small number of\nparameters that provides very good fits of some stock prices and indices\nfluctuations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:13:44 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 12:31:41 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 13:50:04 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Paolinelli", "Giovanni", ""], ["Arioli", "Gianni", ""]]}, {"id": "1809.01501", "submitter": "Arthur Tarso Rego", "authors": "Arthur T. Rego and Thiago R. dos Santos", "title": "Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler", "comments": "27 pages, 12 figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a model for estimating volatility from financial\ntime series, extending the non-Gaussian family of space-state models with exact\nmarginal likelihood proposed by Gamerman, Santos and Franco (2013). On the\nliterature there are models focused on estimating financial assets risk,\nhowever, most of them rely on MCMC methods based on Metropolis algorithms,\nsince full conditional posterior distributions are not known. We present an\nalternative model capable of estimating the volatility, in an automatic way,\nsince all full conditional posterior distributions are known, and it is\npossible to obtain an exact sample of parameters via Gibbs Sampler. The\nincorporation of jumps in returns allows the model to capture speculative\nmovements of the data, so that their influence does not propagate to\nvolatility. We evaluate the performance of the algorithm using synthetic and\nreal data time series.\n  Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,\nDynamic linear models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:53:28 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:39:23 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Rego", "Arthur T.", ""], ["Santos", "Thiago R. dos", ""]]}, {"id": "1809.02233", "submitter": "Andrew Green", "authors": "Ryan Ferguson and Andrew Green", "title": "Deeply Learning Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses deep learning to value derivatives. The approach is broadly\napplicable, and we use a call option on a basket of stocks as an example. We\nshow that the deep learning model is accurate and very fast, capable of\nproducing valuations a million times faster than traditional models. We develop\na methodology to randomly generate appropriate training data and explore the\nimpact of several parameters including layer width and depth, training data\nquality and quantity on model speed and accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 21:59:46 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 19:12:10 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 19:25:29 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2018 20:25:14 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Ferguson", "Ryan", ""], ["Green", "Andrew", ""]]}, {"id": "1809.04925", "submitter": "Jeonggyu Huh", "authors": "Jeonggyu Huh", "title": "Measuring Systematic Risk with Neural Network Factor Model", "comments": "16 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we measure systematic risk with a new nonparametric factor\nmodel, the neural network factor model. The suitable factors for systematic\nrisk can be naturally found by inserting daily returns on a wide range of\nassets into the bottleneck network. The network-based model does not stick to a\nprobabilistic structure unlike parametric factor models, and it does not need\nfeature engineering because it selects notable features by itself. In addition,\nwe compare performance between our model and the existing models using 20-year\ndata of S&P 100 components. Although the new model can not outperform the best\nones among the parametric factor models due to limitations of the variational\ninference, the estimation method used for this study, it is still noteworthy in\nthat it achieves the performance as best the comparable models could without\nany prior knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 12:57:10 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Huh", "Jeonggyu", ""]]}, {"id": "1809.05328", "submitter": "Ludovic Goudenege", "authors": "Ludovic Gouden\\`ege (FR3487), Andrea Molent, Antonino Zanette\n  (MATHRISK)", "title": "Computing Credit Valuation Adjustment solving coupled PIDEs in the Bates\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit value adjustment (CVA) is the charge applied by financial institutions\nto the counterparty to cover the risk of losses on a counterpart default event.\nIn this paper we estimate such a premium under the Bates stochastic model\n(Bates [4]), which considers an underlying affected by both stochastic\nvolatility and random jumps. We propose an efficient method which improves the\nfinite-difference Monte Carlo (FDMC) approach introduced by de Graaf et al.\n[11]. In particular, the method we propose consists in replacing the Monte\nCarlo step of the FDMC approach with a finite difference step and the whole\nmethod relies on the efficient solution of two coupled partial\nintegro-differential equations (PIDE) which is done by employing the Hybrid\nTree-Finite Difference method developed by Briani et al. [6, 7, 8]. Moreover,\nthe direct application of the hybrid techniques in the original FDMC approach\nis also considered for comparison purposes. Several numerical tests prove the\neffectiveness and the reliability of the proposed approach when both European\nand American options are considered.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 09:45:38 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Gouden\u00e8ge", "Ludovic", "", "FR3487"], ["Molent", "Andrea", "", "MATHRISK"], ["Zanette", "Antonino", "", "MATHRISK"]]}, {"id": "1809.05643", "submitter": "Yumiharu Nakano", "authors": "Yuki Kinoshita and Yumiharu Nakano", "title": "Kernel-based collocation methods for Heath-Jarrow-Morton models with\n  Musiela parametrization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose kernel-based collocation methods for numerical solutions to\nHeath-Jarrow-Morton models with Musiela parametrization. The methods can be\nseen as the Euler-Maruyama approximation of some finite dimensional stochastic\ndifferential equations, and allow us to compute the derivative prices by the\nusual Monte Carlo methods. We derive a bound on the rate of convergence under\nsome decay condition on the inverse of the interpolation matrix and some\nregularity conditions on the volatility functionals.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 03:45:01 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 14:47:31 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 01:21:35 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 06:58:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kinoshita", "Yuki", ""], ["Nakano", "Yumiharu", ""]]}, {"id": "1809.06471", "submitter": "Jorge Faleiro Jr", "authors": "Jorge Faleiro", "title": "A Language for Large-Scale Collaboration in Economics: A Streamlined\n  Computational Representation of Financial Models", "comments": "20 pages, 12 equations, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces Sigma, a domain-specific computational representation\nfor collaboration in large-scale for the field of economics. A computational\nrepresentation is not a programming language or a software platform. A\ncomputational representation is a domain-specific representation system based\non three specific elements: facets, contributions, and constraints of data.\nFacets are definable aspects that make up a subject or an object. Contributions\nare shareable and formal evidence, carrying specific properties, and produced\nas a result of a crowd-based scientific investigation. Constraints of data are\nrestrictions defining domain-specific rules of association between entities and\nrelationships. A computational representation serves as a layer of abstraction\nthat is required in order to define domain-specific concepts in computers, in a\nway these concepts can be shared in a crowd for the purposes of a controlled\nscientific investigation in large-scale by crowds. Facets, contributions, and\nconstraints of data are defined for any domain of knowledge by the application\nof a generic set of inputs, procedural steps, and products called a\nrepresentational process. The application of this generic process to our domain\nof knowledge, the field of economics, produces Sigma. Sigma is described in\nthis paper in terms of its three elements: facets (streaming, reactives,\ndistribution, and simulation), contributions (financial models, processors, and\nendpoints), and constraints of data (configuration, execution, and simulation\nmeta-model). Each element of the generic representational process and the Sigma\ncomputational representation is described and formalized in details.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:05:15 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Faleiro", "Jorge", ""]]}, {"id": "1809.07195", "submitter": "Jorge Faleiro Jr", "authors": "Jorge Faleiro", "title": "Enabling Scientific Crowds: The Theory of Enablers for Crowd-Based\n  Scientific Investigation", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1809.02671", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evidence shows that in a significant number of cases the current methods of\nresearch do not allow for reproducible and falsifiable procedures of scientific\ninvestigation. As a consequence, the majority of critical decisions at all\nlevels, from personal investment choices to overreaching global policies, rely\non some variation of try-and-error and are mostly non-scientific by definition.\nWe lack transparency for procedures and evidence, proper explanation of market\nevents, predictability on effects, or identification of causes. There is no\nclear demarcation of what is inherently scientific, and as a consequence, the\nline between fake and genuine is blurred. This paper presents highlights of the\nTheory of Enablers for Crowd-Based Scientific Investigation, or Theory of\nEnablers for short. The Theory of Enablers assumes the use of a next-generation\ninvestigative approach leveraging forces of human diversity, micro-specialized\ncrowds, and proper computer-assisted control methods associated with\naccessibility, reproducibility, communication, and collaboration. This paper\ndefines the set of very specific cognitive and non-cognitive enablers for\ncrowd-based scientific investigation: methods of proof, large-scale\ncollaboration, and a domain-specific computational representation. These\nenablers allow the application of procedures of structured scientific\ninvestigation powered by crowds, a collective brain in which neurons are human\ncollaborators\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 17:08:01 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Faleiro", "Jorge", ""]]}, {"id": "1809.07300", "submitter": "Soeren Wolfers", "authors": "Christian Bayer, Ra\\'ul Tempone, S\\\"oren Wolfers", "title": "Pricing American Options by Exercise Rate Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for the numerical pricing of American options based\non Monte Carlo simulation and the optimization of exercise strategies. Previous\nsolutions to this problem either explicitly or implicitly determine so-called\noptimal exercise regions, which consist of points in time and space at which a\ngiven option is exercised. In contrast, our method determines the exercise\nrates of randomized exercise strategies. We show that the supremum of the\ncorresponding stochastic optimization problem provides the correct option\nprice. By integrating analytically over the random exercise decision, we obtain\nan objective function that is differentiable with respect to perturbations of\nthe exercise rate even for finitely many sample paths. The global optimum of\nthis function can be approached gradually when starting from a constant\nexercise rate.\n  Numerical experiments on vanilla put options in the multivariate\nBlack-Scholes model and a preliminary theoretical analysis underline the\nefficiency of our method, both with respect to the number of\ntime-discretization steps and the required number of degrees of freedom in the\nparametrization of the exercise rates. Finally, we demonstrate the flexibility\nof our method through numerical experiments on max call options in the\nclassical Black-Scholes model, and vanilla put options in both the Heston model\nand the non-Markovian rough Bergomi model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 17:10:07 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 16:17:30 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Bayer", "Christian", ""], ["Tempone", "Ra\u00fal", ""], ["Wolfers", "S\u00f6ren", ""]]}, {"id": "1809.07727", "submitter": "Andrey Itkin", "authors": "Peter Carr, Andrey Itkin", "title": "Geometric Local Variance Gamma model", "comments": "36 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1802.09611", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes another extension of the Local Variance Gamma model\noriginally proposed by P. Carr in 2008, and then further elaborated on by Carr\nand Nadtochiy, 2017 (CN2017), and Carr and Itkin, 2018 (CI2018). As compared\nwith the latest version of the model developed in CI2018 and called the ELVG\n(the Expanded Local Variance Gamma model), here we provide two innovations.\nFirst, in all previous papers the model was constructed based on a Gamma\ntime-changed {\\it arithmetic} Brownian motion: with no drift in CI2017, and\nwith drift in CI2018, and the local variance to be a function of the spot level\nonly. In contrast, here we develop a {\\it geometric} version of this model with\ndrift. Second, in CN2017 the model was calibrated to option smiles assuming the\nlocal variance is a piecewise constant function of strike, while in CI2018 the\nlocal variance is a piecewise linear} function of strike. In this paper we\nconsider 3 piecewise linear models: the local variance as a function of strike,\nthe local variance as function of log-strike, and the local volatility as a\nfunction of strike (so, the local variance is a piecewise quadratic function of\nstrike). We show that for all these new constructions it is still possible to\nderive an ordinary differential equation for the option price, which plays a\nrole of Dupire's equation for the standard local volatility model, and,\nmoreover, it can be solved in closed form. Finally, similar to CI2018, we show\nthat given multiple smiles the whole local variance/volatility surface can be\nrecovered which does not require solving any optimization problem. Instead, it\ncan be done term-by-term by solving a system of non-linear algebraic equations\nfor each maturity which is fast.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:05:22 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 18:23:40 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Carr", "Peter", ""], ["Itkin", "Andrey", ""]]}, {"id": "1809.08390", "submitter": "Jiahao Li", "authors": "Junfeng Jiang, Jiahao Li", "title": "Constructing Financial Sentimental Factors in Chinese Market Using\n  Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design an integrated algorithm to evaluate the sentiment of\nChinese market. Firstly, with the help of the web browser automation, we crawl\na lot of news and comments from several influential financial websites\nautomatically. Secondly, we use techniques of Natural Language Processing(NLP)\nunder Chinese context, including tokenization, Word2vec word embedding and\nsemantic database WordNet, to compute Senti-scores of these news and comments,\nand then construct the sentimental factor. Here, we build a finance-specific\nsentimental lexicon so that the sentimental factor can reflect the sentiment of\nfinancial market but not the general sentiments as happiness, sadness, etc.\nThirdly, we also implement an adjustment of the standard sentimental factor.\nOur experimental performance shows that there is a significant correlation\nbetween our standard sentimental factor and the Chinese market, and the\nadjusted factor is even more informative, having a stronger correlation with\nthe Chinese market. Therefore, our sentimental factors can be important\nreferences when making investment decisions. Especially during the Chinese\nmarket crash in 2015, the Pearson correlation coefficient of adjusted\nsentimental factor with SSE is 0.5844, which suggests that our model can\nprovide a solid guidance, especially in the special period when the market is\ninfluenced greatly by public sentiment.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 06:35:07 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Jiang", "Junfeng", ""], ["Li", "Jiahao", ""]]}, {"id": "1809.08635", "submitter": "Alan Lewis", "authors": "Alan L. Lewis", "title": "Exact Solutions for a GBM-type Stochastic Volatility Model having a\n  Stationary Distribution", "comments": "42 pages, 5 figures, one typo (eqn (66)) corrected", "journal-ref": "Wilmott mag. 101 (2019) 20-41", "doi": "10.1002/wilm.10761", "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find various exact solutions for a new stochastic volatility (SV) model:\nthe transition probability density, European-style option values, and (when it\nexists) the martingale defect. This may represent the first example of an SV\nmodel combining exact solutions, GBM-type volatility noise, and a stationary\nvolatility density.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 17:11:25 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 18:25:22 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lewis", "Alan L.", ""]]}, {"id": "1809.09069", "submitter": "Victor Gaton", "authors": "Javier de Frutos, Victor Gaton", "title": "An extension of Heston's SV model to Stochastic Interest Rates", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics Volume 354, July\n  2019, Pages 174-182", "doi": "10.1016/j.cam.2018.09.010", "report-no": null, "categories": "q-fin.CP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 'A Closed-Form Solution for Options with Stochastic Volatility with\nApplications to Bond and Currency Options', Heston proposes a Stochastic\nVolatility (SV) model with constant interest rate and derives a semi-explicit\nvaluation formula. Heston also describes, in general terms, how the model could\nbe extended to incorporate Stochastic Interest Rates (SIR). This paper is\ndevoted to the construction of an extension of Heston's SV model with a\nparticular stochastic bond model which, just increasing in one the number of\nparameters, allows to incorporate SIR and to derive a semi-explicit formula for\noption pricing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:24:26 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["de Frutos", "Javier", ""], ["Gaton", "Victor", ""]]}, {"id": "1809.09466", "submitter": "Imanol Perez Arribas", "authors": "Imanol Perez Arribas", "title": "Derivatives pricing using signature payoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce signature payoffs, a family of path-dependent derivatives that\nare given in terms of the signature of the price path of the underlying asset.\nWe show that these derivatives are dense in the space of continuous payoffs, a\nresult that is exploited to quickly price arbitrary continuous payoffs. This\napproach to pricing derivatives is then tested with European options, American\noptions, Asian options, lookback options and variance swaps. As we show,\nsignature payoffs can be used to price these derivatives with very high\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 13:30:12 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Arribas", "Imanol Perez", ""]]}]