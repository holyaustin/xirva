[{"id": "2007.01623", "submitter": "Oleg Szehr", "authors": "Loris Cannelli, Giuseppe Nuti, Marzio Sala, Oleg Szehr", "title": "Hedging using reinforcement learning: Contextual $k$-Armed Bandit versus\n  $Q$-learning", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of replication strategies for contingent claims in the\npresence of risk and market friction is a key problem of financial engineering.\nIn real markets, continuous replication, such as in the model of Black, Scholes\nand Merton, is not only unrealistic but it is also undesirable due to high\ntransaction costs. Over the last decades stochastic optimal-control methods\nhave been developed to balance between effective replication and losses. More\nrecently, with the rise of artificial intelligence, temporal-difference\nReinforcement Learning, in particular variations of $Q$-learning in conjunction\nwith Deep Neural Networks, have attracted significant interest. From a\npractical point of view, however, such methods are often relatively sample\ninefficient, hard to train and lack performance guarantees. This motivates the\ninvestigation of a stable benchmark algorithm for hedging. In this article, the\nhedging problem is viewed as an instance of a risk-averse contextual $k$-armed\nbandit problem, for which a large body of theoretical results and well-studied\nalgorithms are available. We find that the $k$-armed bandit model naturally\nfits to the $P\\&L$ formulation of hedging, providing for a more accurate and\nsample efficient approach than $Q$-learning and reducing to the Black-Scholes\nmodel in the absence of transaction costs and risks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:34:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cannelli", "Loris", ""], ["Nuti", "Giuseppe", ""], ["Sala", "Marzio", ""], ["Szehr", "Oleg", ""]]}, {"id": "2007.02076", "submitter": "Zbigniew Palmowski", "authors": "Zbigniew Palmowski and Tomasz Serafin", "title": "Note on simulation pricing of $\\pi$-options", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we adapt a Monte Carlo algorithm introduced by Broadie and\nGlasserman (1997) to price a $\\pi$-option. This method is based on the\nsimulated price tree that comes from discretization and replication of possible\ntrajectories of the underlying asset's price. As a result this algorithm\nproduces the lower and the upper bounds that converge to the true price with\nthe increasing depth of the tree. Under specific parametrization, this\n$\\pi$-option is related to relative maximum drawdown and can be used in the\nreal-market environment to protect a portfolio against volatile and unexpected\nprice drops. We also provide some numerical analysis.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 11:38:56 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:16:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Palmowski", "Zbigniew", ""], ["Serafin", "Tomasz", ""]]}, {"id": "2007.02692", "submitter": "Benjamin Virrion", "authors": "Benjamin Virrion (CEREMADE)", "title": "Deep Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic path-dependent importance sampling algorithm where the\nGirsanov induced change of probability on the path space is represented by a\nsequence of neural networks taking the past of the trajectory as an input. At\neach learning step, the neural networks' parameters are trained so as to reduce\nthe variance of the Monte Carlo estimator induced by this change of measure.\nThis allows for a generic path dependent change of measure which can be used to\nreduce the variance of any path-dependent financial payoff. We show in our\nnumerical experiments that for payoffs consisting of either a call, an\nasymmetric combination of calls and puts, a symmetric combination of calls and\nputs, a multi coupon autocall or a single coupon autocall, we are able to\nreduce the variance of the Monte Carlo estimators by factors between 2 and 9.\nThe numerical experiments also show that the method is very robust to changes\nin the parameter values, which means that in practice, the training can be done\noffline and only updated on a weekly basis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:36:30 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 07:44:21 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Virrion", "Benjamin", "", "CEREMADE"]]}, {"id": "2007.03494", "submitter": "Dirk Roeder", "authors": "Dirk Roeder, Georgi Dimitroff", "title": "Volatility model calibration with neural networks a comparison between\n  direct and indirect methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper \"Deep Learning Volatility\" a fast 2-step deep calibration\nalgorithm for rough volatility models was proposed: in the first step the time\nconsuming mapping from the model parameter to the implied volatilities is\nlearned by a neural network and in the second step standard solver techniques\nare used to find the best model parameter.\n  In our paper we compare these results with an alternative direct approach\nwhere the the mapping from market implied volatilities to model parameters is\napproximated by the neural network, without the need for an extra solver step.\nUsing a whitening procedure and a projection of the target parameter to [0,1],\nin order to be able to use a sigmoid type output function we found that the\ndirect approach outperforms the two-step one for the data sets and methods\npublished in \"Deep Learning Volatility\".\n  For our implementation we use the open source tensorflow 2 library. The paper\nshould be understood as a technical comparison of neural network techniques and\nnot as an methodically new Ansatz.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:29:14 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Roeder", "Dirk", ""], ["Dimitroff", "Georgi", ""]]}, {"id": "2007.04203", "submitter": "Thomas Spooner", "authors": "Thomas Spooner and Rahul Savani", "title": "A Natural Actor-Critic Algorithm with Downside Risk Constraints", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-fin.CP q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on risk-sensitive reinforcement learning - both for symmetric\nand downside risk measures - has typically used direct Monte-Carlo estimation\nof policy gradients. While this approach yields unbiased gradient estimates, it\nalso suffers from high variance and decreased sample efficiency compared to\ntemporal-difference methods. In this paper, we study prediction and control\nwith aversion to downside risk which we gauge by the lower partial moment of\nthe return. We introduce a new Bellman equation that upper bounds the lower\npartial moment, circumventing its non-linearity. We prove that this proxy for\nthe lower partial moment is a contraction, and provide intuition into the\nstability of the algorithm by variance decomposition. This allows\nsample-efficient, on-line estimation of partial moments. For risk-sensitive\ncontrol, we instantiate Reward Constrained Policy Optimization, a recent\nactor-critic method for finding constrained policies, with our proxy for the\nlower partial moment. We extend the method to use natural policy gradients and\ndemonstrate the effectiveness of our approach on three benchmark problems for\nrisk-sensitive reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:44:33 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Spooner", "Thomas", ""], ["Savani", "Rahul", ""]]}, {"id": "2007.04408", "submitter": "Chinonso Nwankwo", "authors": "Chinonso Nwankwo and Weizhong Dai", "title": "An Adaptive and Explicit Fourth Order Runge-Kutta-Fehlberg Method\n  Coupled with Compact Finite Differencing for Pricing American Put Options", "comments": "This is a preprint of an article published in Japan Journal of\n  Industrial and Applied Mathematics (JJIAM). The final authenticated version\n  is available online at: https://doi.org/10.1007/s13160-021-00470-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive and explicit fourth-order Runge-Kutta-Fehlberg method\ncoupled with a fourth-order compact scheme to solve the American put options\nproblem. First, the free boundary problem is converted into a system of partial\ndifferential equations with a fixed domain by using logarithm transformation\nand taking additional derivatives. With the addition of an intermediate\nfunction with a fixed free boundary, a quadratic formula is derived to compute\nthe velocity of the optimal exercise boundary analytically. Furthermore, we\nimplement an extrapolation method to ensure that at least, a third-order\naccuracy in space is maintained at the boundary point when computing the\noptimal exercise boundary from its derivative. As such, it enables us to employ\nfourth-order spatial and temporal discretization with Dirichlet boundary\nconditions for obtaining the numerical solution of the asset option, option\nGreeks, and the optimal exercise boundary. The advantage of the\nRunge-Kutta-Fehlberg method is based on error control and the adjustment of the\ntime step to maintain the error at a certain threshold. By comparing with some\nexisting methods in the numerical experiment, it shows that the present method\nhas a better performance in terms of computational speed and provides a more\naccurate solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 20:20:35 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 12:07:46 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 12:56:53 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2020 12:59:53 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 22:04:11 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Nwankwo", "Chinonso", ""], ["Dai", "Weizhong", ""]]}, {"id": "2007.06262", "submitter": "Filippo Petroni", "authors": "Guglielmo D'Amico and Filippo Petroni", "title": "A micro-to-macro approach to returns, volumes and waiting times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fundamental variables in financial market are not only price and return but a\nvery important role is also played by trading volumes. Here we propose a new\nmultivariate model that takes into account price returns, logarithmic variation\nof trading volumes and also waiting times, the latter to be intended as the\ntime interval between changes in trades, price, and volume of stocks. Our\napproach is based on a generalization of semi-Markov chains where an endogenous\nindex process is introduced. We also take into account the dependence structure\nbetween the above mentioned variables by means of copulae. The proposed model\nis motivated by empirical evidences which are known in financial literature and\nthat are also confirmed in this work by analysing real data from Italian stock\nmarket in the period August 2015 - August 2017. By using Monte Carlo\nsimulations, we show that the model reproduces all these empirical evidences.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:31:49 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["D'Amico", "Guglielmo", ""], ["Petroni", "Filippo", ""]]}, {"id": "2007.07319", "submitter": "Antonio Briola", "authors": "Antonio Briola, Jeremy Turiel, Tomaso Aste", "title": "Deep Learning modeling of Limit Order Book: a comparative perspective", "comments": "16 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG q-fin.CP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present work addresses theoretical and practical questions in the domain\nof Deep Learning for High Frequency Trading. State-of-the-art models such as\nRandom models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention\nmask, CNN-LSTMs and MLPs are reviewed and compared on the same tasks, feature\nspace and dataset, and then clustered according to pairwise similarity and\nperformance metrics. The underlying dimensions of the modeling techniques are\nhence investigated to understand whether these are intrinsic to the Limit Order\nBook's dynamics. We observe that the Multilayer Perceptron performs comparably\nto or better than state-of-the-art CNN-LSTM architectures indicating that\ndynamic spatial and temporal dimensions are a good approximation of the LOB's\ndynamics, but not necessarily the true underlying dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:06:30 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 14:27:15 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 15:44:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Briola", "Antonio", ""], ["Turiel", "Jeremy", ""], ["Aste", "Tomaso", ""]]}, {"id": "2007.08115", "submitter": "Vassilis Polimenis", "authors": "Vassilis Polimenis", "title": "Uncovering a factor-based expected return conditioning structure with\n  Regression Trees jointly for many stocks", "comments": "11 pages, 5 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the success and almost universal acceptance of the simple linear\nregression three-factor model, it is interesting to analyze the informational\ncontent of the three factors in explaining stock returns when the analysis is\nallowed to consider non-linear dependencies between factors and stock returns.\nIn order to better understand factor-based conditioning information with\nrespect to expected stock returns within a regression tree setting, the\nanalysis of stock returns is demonstrated using daily stock return data for 5\nmajor US corporations. The first finding is that in all cases (solo and joint)\nthe most informative factor is always the market excess return factor. Further,\nthree major issues are discussed: a) the balance of a depth=1 tree as it\nrelates to properties of the stock return distribution, b) the mechanism behind\ndepth=1 tree balance in a joint regression tree and c) the dominant stock in a\njoint regression tree. It is shown that high skew values alone cannot explain\nthe imbalance of the resulting tree split as stocks with pronounced skew may\nproduce balanced tree splits.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 05:01:39 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Polimenis", "Vassilis", ""]]}, {"id": "2007.09201", "submitter": "Matthew Lorig", "authors": "Matthew Lorig", "title": "Bond indifference prices and indifference yield curves", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a market with stochastic interest rates, we consider an investor who can\neither (i) invest all if his money in a savings account or (ii) purchase\nzero-coupon bonds and invest the remainder of his wealth in a savings account.\nThe indifference price of the bond is the price for which the investor could\nachieve the same expected utility under both scenarios. In an affine term\nstructure setting, under the assumption that an investor has a utility function\nin either exponential or power form, we show that the indifference price of a\nzero-coupon bond is the root of an integral expression. As an example, we\ncompute bond indifference prices and the corresponding indifference yield\ncurves in the Vasicek setting and interpret the results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:32:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lorig", "Matthew", ""]]}, {"id": "2007.10462", "submitter": "Matthew Dixon", "authors": "Marc Chataigner, St\\'ephane Cr\\'epey and Matthew Dixon", "title": "Deep Local Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for option pricing has emerged as a novel methodology for fast\ncomputations with applications in calibration and computation of Greeks.\nHowever, many of these approaches do not enforce any no-arbitrage conditions,\nand the subsequent local volatility surface is never considered. In this\narticle, we develop a deep learning approach for interpolation of European\nvanilla option prices which jointly yields the full surface of local\nvolatilities. We demonstrate the modification of the loss function or the feed\nforward network architecture to enforce (hard constraints approach) or favor\n(soft constraints approach) the no-arbitrage conditions and we specify the\nexperimental design parameters that are needed for adequate performance. A\nnovel component is the use of the Dupire formula to enforce bounds on the local\nvolatility associated with option prices, during the network fitting. Our\nmethodology is benchmarked numerically on real datasets of DAX vanilla options.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:45:00 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chataigner", "Marc", ""], ["Cr\u00e9pey", "St\u00e9phane", ""], ["Dixon", "Matthew", ""]]}, {"id": "2007.11201", "submitter": "Vishal Keswani", "authors": "Vishal Keswani, Sakshi Singh, Ashutosh Modi", "title": "IITK at the FinSim Task: Hypernym Detection in Financial Domain via\n  Context-Free and Contextualized Word Embeddings", "comments": "6 pages, 1 figure, 4 tables. Accepted at the Second Workshop on\n  Financial Technology and Natural Language Processing (FinNLP-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our approaches for the FinSim 2020 shared task on\n\"Learning Semantic Representations for the Financial Domain\". The goal of this\ntask is to classify financial terms into the most relevant hypernym (or\ntop-level) concept in an external ontology. We leverage both context-dependent\nand context-independent word embeddings in our analysis. Our systems deploy\nWord2vec embeddings trained from scratch on the corpus (Financial Prospectus in\nEnglish) along with pre-trained BERT embeddings. We divide the test dataset\ninto two subsets based on a domain rule. For one subset, we use unsupervised\ndistance measures to classify the term. For the second subset, we use simple\nsupervised classifiers like Naive Bayes, on top of the embeddings, to arrive at\na final prediction. Finally, we combine both the results. Our system ranks 1st\nbased on both the metrics, i.e., mean rank and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 04:56:23 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Keswani", "Vishal", ""], ["Singh", "Sakshi", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2007.14447", "submitter": "Gholamreza Jafari", "authors": "Ali Namaki, Jamshid Ardalankia, Reza Raei, Leila Hedayatifar, Ali\n  Hosseiny, Emmanuel Haven, G.Reza Jafari", "title": "Analysis of the Global Banking Network by Random Matrix Theory", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since 2008, the network analysis of financial systems is one of the most\nimportant subjects in economics. In this paper, we have used the complexity\napproach and Random Matrix Theory (RMT) for analyzing the global banking\nnetwork. By applying this method on a cross border lending network, it is shown\nthat the network has been denser and the connectivity between peripheral nodes\nand the central section has risen. Also, by considering the collective behavior\nof the system and comparing it with the shuffled one, we can see that this\nnetwork obtains a specific structure. By using the inverse participation ratio\nconcept, we can see that after 2000, the participation of different modes to\nthe network has increased and tends to the market mode of the system. Although\nno important change in the total market share of trading occurs, through the\npassage of time, the contribution of some countries in the network structure\nhas increased. The technique proposed in the paper can be useful for analyzing\ndifferent types of interaction networks between countries.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:39:11 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Namaki", "Ali", ""], ["Ardalankia", "Jamshid", ""], ["Raei", "Reza", ""], ["Hedayatifar", "Leila", ""], ["Hosseiny", "Ali", ""], ["Haven", "Emmanuel", ""], ["Jafari", "G. Reza", ""]]}, {"id": "2007.14702", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Wolfgang Karl H\\\"ardle, Campbell R. Harvey, Raphael C. G. Reule", "title": "Editorial: Understanding Cryptocurrencies", "comments": null, "journal-ref": null, "doi": "10.2139/ssrn.3360304", "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrency refers to a type of digital asset that uses distributed\nledger, or blockchain, technology to enable a secure transaction. Although the\ntechnology is widely misunderstood, many central banks are considering\nlaunching their own national cryptocurrency. In contrast to most data in\nfinancial economics, detailed data on the history of every transaction in the\ncryptocurrency complex are freely available. Furthermore, empirically-oriented\nresearch is only now beginning, presenting an extraordinary research\nopportunity for academia. We provide some insights into the mechanics of\ncryptocurrencies, describing summary statistics and focusing on potential\nfuture research avenues in financial economics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:30:57 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["H\u00e4rdle", "Wolfgang Karl", ""], ["Harvey", "Campbell R.", ""], ["Reule", "Raphael C. G.", ""]]}, {"id": "2007.15128", "submitter": "Alexandre Carbonneau", "authors": "Alexandre Carbonneau", "title": "Deep Hedging of Long-Term Financial Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a deep reinforcement learning approach for global hedging\nof long-term financial derivatives. A similar setup as in Coleman et al. (2007)\nis considered with the risk management of lookback options embedded in\nguarantees of variable annuities with ratchet features. The deep hedging\nalgorithm of Buehler et al. (2019a) is applied to optimize neural networks\nrepresenting global hedging policies with both quadratic and non-quadratic\npenalties. To the best of the author's knowledge, this is the first paper that\npresents an extensive benchmarking of global policies for long-term contingent\nclaims with the use of various hedging instruments (e.g. underlying and\nstandard options) and with the presence of jump risk for equity. Monte Carlo\nexperiments demonstrate the vast superiority of non-quadratic global hedging as\nit results simultaneously in downside risk metrics two to three times smaller\nthan best benchmarks and in significant hedging gains. Analyses show that the\nneural networks are able to effectively adapt their hedging decisions to\ndifferent penalties and stylized facts of risky asset dynamics only by\nexperiencing simulations of the financial market exhibiting these features.\nNumerical results also indicate that non-quadratic global policies are\nsignificantly more geared towards being long equity risk which entails earning\nthe equity risk premium.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:57:29 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Carbonneau", "Alexandre", ""]]}]