[{"id": "0904.0624", "submitter": "Josef Teichmann", "authors": "Juan-Pablo Ortega and Rainer Pullirsch and Josef Teichmann and Julian\n  Wergieluk", "title": "A new approach for scenario generation in Risk management", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new dynamic approach to scenario generation for the purposes of\nrisk management in the banking industry. We connect ideas from conventional\ntechniques -- like historical and Monte Carlo simulation -- and we come up with\na hybrid method that shares the advantages of standard procedures but\neliminates several of their drawbacks. Instead of considering the static\nproblem of constructing one or ten day ahead distributions for vectors of risk\nfactors, we embed the problem into a dynamic framework, where any time horizon\ncan be consistently simulated. Additionally, we use standard models from\nmathematical finance for each risk factor, whence bridging the worlds of\ntrading and risk management.\n  Our approach is based on stochastic differential equations (SDEs), like the\nHJM-equation or the Black-Scholes equation, governing the time evolution of\nrisk factors, on an empirical calibration method to the market for the chosen\nSDEs, and on an Euler scheme (or high-order schemes) for the numerical\nevaluation of the respective SDEs. The empirical calibration procedure\npresented in this paper can be seen as the SDE-counterpart of the so called\nFiltered Historical Simulation method; the behavior of volatility stems in our\ncase out of the assumptions on the underlying SDEs. Furthermore, we are able to\neasily incorporate \"middle-size\" and \"large-size\" events within our framework\nalways making a precise distinction between the information obtained from the\nmarket and the one coming from the necessary a-priori intuition of the risk\nmanager.\n  Results of one concrete implementation are provided.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2009 17:22:04 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2009 16:33:11 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Ortega", "Juan-Pablo", ""], ["Pullirsch", "Rainer", ""], ["Teichmann", "Josef", ""], ["Wergieluk", "Julian", ""]]}, {"id": "0904.0830", "submitter": "Xiaolin Luo Dr", "authors": "Xiaolin Luo, Pavel V. Shevchenko", "title": "Computing Tails of Compound Distributions Using Direct Numerical\n  Integration", "comments": null, "journal-ref": "Journal of Computational Finance. 13(2), 73-111. (2009)", "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient adaptive direct numerical integration (DNI) algorithm is\ndeveloped for computing high quantiles and conditional Value at Risk (CVaR) of\ncompound distributions using characteristic functions. A key innovation of the\nnumerical scheme is an effective tail integration approximation that reduces\nthe truncation errors significantly with little extra effort. High precision\nresults of the 0.999 quantile and CVaR were obtained for compound losses with\nheavy tails and a very wide range of loss frequencies using the DNI, Fast\nFourier Transform (FFT) and Monte Carlo (MC) methods. These results,\nparticularly relevant to operational risk modelling, can serve as benchmarks\nfor comparing different numerical methods. We found that the adaptive DNI can\nachieve high accuracy with relatively coarse grids. It is much faster than MC\nand competitive with FFT in computing high quantiles and CVaR of compound\ndistributions in the case of moderate to high frequencies and heavy tails.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 01:43:48 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 04:51:38 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2010 07:04:58 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Luo", "Xiaolin", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "0904.1074", "submitter": "Gregory Rayee RG", "authors": "Fr\\'ed\\'eric Bossens, Gr\\'egory Ray\\'ee, Nikos S. Skantzos and\n  Griselda Deelstra", "title": "Vanna-Volga methods applied to FX derivatives : from theory to market\n  practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Vanna-Volga methods which are used to price first generation exotic\noptions in the Foreign Exchange market. They are based on a rescaling of the\ncorrection to the Black-Scholes price through the so-called `probability of\nsurvival' and the `expected first exit time'. Since the methods rely heavily on\nthe appropriate treatment of market data we also provide a summary of the\nrelevant conventions. We offer a justification of the core technique for the\ncase of vanilla options and show how to adapt it to the pricing of exotic\noptions. Our results are compared to a large collection of indicative market\nprices and to more sophisticated models. Finally we propose a simple\ncalibration method based on one-touch prices that allows the Vanna-Volga\nresults to be in line with our pool of market data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 10:22:29 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2009 08:32:58 GMT"}, {"version": "v3", "created": "Mon, 3 May 2010 14:02:38 GMT"}], "update_date": "2010-05-04", "authors_parsed": [["Bossens", "Fr\u00e9d\u00e9ric", ""], ["Ray\u00e9e", "Gr\u00e9gory", ""], ["Skantzos", "Nikos S.", ""], ["Deelstra", "Griselda", ""]]}, {"id": "0904.1078", "submitter": "Juan-Pablo Ortega", "authors": "Juan-Pablo Ortega", "title": "GARCH options via local risk minimization", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a quadratic hedging scheme developed by Foellmer, Schweizer, and\nSondermann to European contingent products whose underlying asset is modeled\nusing a GARCH process and show that local risk-minimizing strategies with\nrespect to the physical measure do exist, even though an associated minimal\nmartingale measure is only available in the presence of bounded innovations.\nMore importantly, since those local risk-minimizing strategies are in general\nconvoluted and difficult to evaluate, we introduce Girsanov-like risk-neutral\nmeasures for the log-prices that yield more tractable and useful results.\nRegarding this subject, we focus on GARCH time series models with Gaussian\ninnovations and we provide specific sufficient conditions that have to do with\nthe finiteness of the kurtosis, under which those martingale measures are\nappropriate in the context of quadratic hedging. When this equivalent\nmartingale measure is adapted to the price representation we are able to\nrecover out of it the classical pricing formulas of Duan and Heston-Nandi, as\nwell as hedging schemes that improve the performance of those proposed in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 08:19:34 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2010 10:57:11 GMT"}], "update_date": "2010-01-29", "authors_parsed": [["Ortega", "Juan-Pablo", ""]]}, {"id": "0904.1131", "submitter": "Sovan Mitra", "authors": "Sovan Mitra", "title": "Optimisation of Stochastic Programming by Hidden Markov Modelling based\n  Scenario Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formed part of a preliminary research report for a risk\nconsultancy and academic research. Stochastic Programming models provide a\npowerful paradigm for decision making under uncertainty. In these models the\nuncertainties are represented by a discrete scenario tree and the quality of\nthe solutions obtained is governed by the quality of the scenarios generated.\nWe propose a new technique to generate scenarios based on Gaussian Mixture\nHidden Markov Modelling. We show that our approach explicitly captures\nimportant time varying dynamics of stochastic processes (such as autoregression\nand jumps) as well as non-Gaussian distribution characteristics (such as\nskewness and kurtosis). Our scenario generation method enables richer\nrobustness and scenario analysis through exploiting the tractable properties of\nMarkov models and Gaussian mixture distributions. We demonstrate the benefits\nof our scenario generation method by conducting numerical experiments on\nFTSE-100 data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 13:36:30 GMT"}], "update_date": "2009-04-08", "authors_parsed": [["Mitra", "Sovan", ""]]}, {"id": "0904.1157", "submitter": "Pavel Shevchenko V", "authors": "P. V. Shevchenko", "title": "Addressing the bias in Monte Carlo pricing of multi-asset options with\n  multiple barriers through discrete sampling", "comments": null, "journal-ref": "The Journal of Computational Finance 6(3), pp.1-20, 2003.\n  www.journalofcomputationalfinance.com", "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient conditioning technique, the so-called Brownian Bridge\nsimulation, has previously been applied to eliminate pricing bias that arises\nin applications of the standard discrete-time Monte Carlo method to evaluate\noptions written on the continuous-time extrema of an underlying asset. It is\nbased on the simple and easy to implement analytic formulas for the\ndistribution of one-dimensional Brownian Bridge extremes. This paper extends\nthe technique to the valuation of multi-asset options with knock-out barriers\nimposed for all or some of the underlying assets. We derive formula for the\nunbiased option price estimator based on the joint distribution of the\nmulti-dimensional Brownian Bridge dependent extrema. As analytic formulas are\nnot available for the joint distribution in general, we develop upper and lower\nbiased option price estimators based on the distribution of independent extrema\nand the Fr\\'echet lower and upper bounds for the unknown distribution. All\nestimators are simple and easy to implement. They can always be used to bind\nthe true value by a confidence interval. Numerical tests indicate that our\nbiased estimators converge rapidly to the true option value as the number of\ntime steps for the asset path simulation increases in comparison to the\nestimator based on the standard discrete-time method. The convergence rate\ndepends on the correlation and barrier structures of the underlying assets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2009 15:03:19 GMT"}], "update_date": "2009-04-08", "authors_parsed": [["Shevchenko", "P. V.", ""]]}, {"id": "0904.1483", "submitter": "Pavel Shevchenko V", "authors": "Gareth W. Peters, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Model uncertainty in claims reserving within Tweedie's compound Poisson\n  models", "comments": null, "journal-ref": "ASTIN Bulletin 39(1), pp.1-33, 2009", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the claims reserving problem using Tweedie's\ncompound Poisson model. We develop the maximum likelihood and Bayesian Markov\nchain Monte Carlo simulation approaches to fit the model and then compare the\nestimated models under different scenarios. The key point we demonstrate\nrelates to the comparison of reserving quantities with and without model\nuncertainty incorporated into the prediction. We consider both the model\nselection problem and the model averaging solutions for the predicted reserves.\nAs a part of this process we also consider the sub problem of variable\nselection to obtain a parsimonious representation of the model being fitted.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2009 09:23:37 GMT"}], "update_date": "2009-04-10", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.1500", "submitter": "Sovan Mitra", "authors": "Sovan Mitra", "title": "Regime Switching Volatility Calibration by the Baum-Welch Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regime switching volatility models provide a tractable method of modelling\nstochastic volatility. Currently the most popular method of regime switching\ncalibration is the Hamilton filter. We propose using the Baum-Welch algorithm,\nan established technique from Engineering, to calibrate regime switching models\ninstead. We demonstrate the Baum-Welch algorithm and discuss the significant\nadvantages that it provides compared to the Hamilton filter. We provide\ncomputational results of calibrating the Baum-Welch filter to S&P 500 data and\nvalidate its performance in and out of sample.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2009 10:56:27 GMT"}], "update_date": "2009-04-10", "authors_parsed": [["Mitra", "Sovan", ""]]}, {"id": "0904.1798", "submitter": "Constantinos Kardaras", "authors": "Constantinos Kardaras", "title": "Market viability via absence of arbitrage of the first kind", "comments": "15 pages. Updated, more self-contained version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a semimartingale financial market model, it is shown that there is\nequivalence between absence of arbitrage of the first kind (a weak viability\ncondition) and the existence of a strictly positive process that acts as a\nlocal martingale deflator on nonnegative wealth processes.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2009 12:40:44 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2009 12:37:18 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2010 16:12:45 GMT"}, {"version": "v4", "created": "Sun, 25 Jul 2010 21:34:56 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Kardaras", "Constantinos", ""]]}, {"id": "0904.2910", "submitter": "Pavel Shevchenko V", "authors": "Xiaolin Luo, Pavel V. Shevchenko and John B. Donnelly", "title": "Addressing the Impact of Data Truncation and Parameter Uncertainty on\n  Operational Risk Estimates", "comments": null, "journal-ref": "The Journal of Operational Risk 2(4), 3-26, 2007\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, operational risk losses are reported above some threshold. This\npaper studies the impact of ignoring data truncation on the 0.999 quantile of\nthe annual loss distribution for operational risk for a broad range of\ndistribution parameters and truncation levels. Loss frequency and severity are\nmodelled by the Poisson and Lognormal distributions respectively. Two cases of\nignoring data truncation are studied: the \"naive model\" - fitting a Lognormal\ndistribution with support on a positive semi-infinite interval, and \"shifted\nmodel\" - fitting a Lognormal distribution shifted to the truncation level. For\nall practical cases, the \"naive model\" leads to underestimation (that can be\nsevere) of the 0.999 quantile. The \"shifted model\" overestimates the 0.999\nquantile except some cases of small underestimation for large truncation\nlevels. Conservative estimation of capital charge is usually acceptable and the\nuse of the \"shifted model\" can be justified while the \"naive model\" should not\nbe allowed. However, if parameter uncertainty is taken into account (in\npractice it is often ignored), the \"shifted model\" can lead to considerable\nunderestimation of capital charge. This is demonstrated with a practical\nexample.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2009 12:56:50 GMT"}], "update_date": "2009-04-21", "authors_parsed": [["Luo", "Xiaolin", ""], ["Shevchenko", "Pavel V.", ""], ["Donnelly", "John B.", ""]]}, {"id": "0904.4074", "submitter": "Pavel Shevchenko V", "authors": "Gareth W. Peters, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Dynamic operational risk: modeling dependence and combining different\n  sources of information", "comments": null, "journal-ref": "The Journal of Operational Risk 4(2), pp. 69-104, 2009\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model dependence between operational risks by allowing risk\nprofiles to evolve stochastically in time and to be dependent. This allows for\na flexible correlation structure where the dependence between frequencies of\ndifferent risk categories and between severities of different risk categories\nas well as within risk categories can be modeled. The model is estimated using\nBayesian inference methodology, allowing for combination of internal data,\nexternal data and expert opinion in the estimation procedure. We use a\nspecialized Markov chain Monte Carlo simulation methodology known as Slice\nsampling to obtain samples from the resulting posterior distribution and\nestimate the model parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 02:12:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2009 07:02:31 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "0904.4075", "submitter": "Pavel Shevchenko V", "authors": "Pavel V. Shevchenko and Grigory Temnov", "title": "Modeling operational risk data reported above a time-varying threshold", "comments": null, "journal-ref": "The Journal of Operational Risk 4(2), pp. 19-42, 2009\n  www.journalofoperationalrisk.com", "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, operational risk losses are reported above a threshold. Fitting\ndata reported above a constant threshold is a well known and studied problem.\nHowever, in practice, the losses are scaled for business and other factors\nbefore the fitting and thus the threshold is varying across the scaled data\nsample. A reporting level may also change when a bank changes its reporting\npolicy. We present both the maximum likelihood and Bayesian Markov chain Monte\nCarlo approaches to fitting the frequency and severity loss distributions using\ndata in the case of a time varying threshold. Estimation of the annual loss\ndistribution accounting for parameter uncertainty is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 02:27:43 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2009 01:29:58 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Shevchenko", "Pavel V.", ""], ["Temnov", "Grigory", ""]]}]