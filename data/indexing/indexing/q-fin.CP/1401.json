[{"id": "1401.1457", "submitter": "Tomaso Aste", "authors": "Anna Zaremba and Tomaso Aste", "title": "Measures of Causality in Complex Datasets with application to financial\n  data", "comments": "40 pages; 13 figures", "journal-ref": "Entropy 16 (2014) 2309-2349", "doi": "10.3390/e16042309", "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates the causality structure of financial time series.\nWe concentrate on three main approaches to measuring causality: linear Granger\ncausality, kernel generalisations of Granger causality (based on ridge\nregression and the Hilbert--Schmidt norm of the cross-covariance operator) and\ntransfer entropy, examining each method and comparing their theoretical\nproperties, with special attention given to the ability to capture nonlinear\ncausality. We also present the theoretical benefits of applying non-symmetrical\nmeasures rather than symmetrical measures of dependence. We apply the measures\nto a range of simulated and real data. The simulated data sets were generated\nwith linear and several types of nonlinear dependence, using bivariate, as well\nas multivariate settings. An application to real-world financial data\nhighlights the practical difficulties, as well as the potential of the methods.\nWe use two real data sets: (1) U.S. inflation and one-month Libor; (2) S$\\&$P\ndata and exchange rates for the following currencies: AUDJPY, CADJPY, NZDJPY,\nAUDCHF, CADCHF, NZDCHF. Overall, we reach the conclusion that no single method\ncan be recognised as the best in all circumstances, and each of the methods has\nits domain of best applicability. We also highlight areas for improvement and\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 17:44:09 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 11:00:43 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Zaremba", "Anna", ""], ["Aste", "Tomaso", ""]]}, {"id": "1401.1610", "submitter": "Luxi Chen", "authors": "Luxi Chen", "title": "Computation of the \"Enrichment\" of a Value Functions of an Optimization\n  Problem on Cumulated Transaction-Costs through a Generalized Lax-Hopf Formula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lax-Hopf formula simplifies the value function of an intertemporal\noptimization (infinite dimensional) problem associated with a convex\ntransaction-cost function which depends only on the transactions (velocities)\nof a commodity evolution: it states that the value function is equal to the\nmarginal fonction of a finite dimensional problem with respect to durations and\naverage ransactions, much simpler to solve. The average velocity of the value\nfunction on a investment temporal window is regarded as an enrichment,\nproportional to the profit and inversely proportional to the investment\nduration. At optimum, the Lax-Hopf formula implies that the enrichment is equal\nto the cost of the average transaction on the investment temporal window. In\nthis study, we generalize the Lax-Hopf formula when the transaction-cost\nfunction depends also on time and commodity, for reducing the infinite\ndimensional problem to a finite dimensional problem. For that purpose, we\nintroduce the moderated ansaction-cost function which depends only on the\nduration and on a commodity. Here again, the generalized Lax-Hopf formula\nreduces the computation of the value function to the marginal fonction of an\noptimization problem on durations and commodities involving the moderated\ntransaction cost function. At optimum, the enrichment of the value function is\nstill equal to the moderated transition cost-function of average transaction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 08:52:25 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Chen", "Luxi", ""]]}, {"id": "1401.1757", "submitter": "Mark Bull", "authors": "Mark Tucker and J. Mark Bull", "title": "An efficient algorithm for the calculation of reserves for non-unit\n  linked life policies", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underlying stochastic nature of the requirements for the Solvency II\nregulations has introduced significant challenges if the required calculations\nare to be performed correctly, without resorting to excessive approximations,\nwithin practical timescales. It is generally acknowledged by practising\nactuaries within UK life offices that it is currently impossible to correctly\nfulfil the requirements imposed by Solvency II using existing computational\ntechniques based on commercially available valuation packages. Our work has\nalready shown that it is possible to perform profitability calculations at a\nfar higher rate than is achievable using commercial packages. One of the key\nfactors in achieving these gains is to calculate reserves using recurrence\nrelations that scale linearly with the number of time steps. Here, we present a\ngeneral vector recurrence relation which can be used for a wide range of\nnon-unit linked policies that are covered by Solvency II; such contracts\ninclude annuities, term assurances, and endowments. Our results suggest that by\nusing an optimised parallel implementation of this algorithm, on an affordable\nhardware platform, it is possible to perform the `brute force' approach to\ndemonstrating solvency in a realistic timescale (of the order of a few hours).\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 17:16:29 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 15:23:30 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Tucker", "Mark", ""], ["Bull", "J. Mark", ""]]}, {"id": "1401.1856", "submitter": "Alexander Kushpel", "authors": "Alexander Kushpel", "title": "Pricing of basket options I", "comments": "arXiv admin note: substantial text overlap with arXiv:1309.4546", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pricing of high-dimensional options is a deep problem of the Theoretical\nFinancial Mathematics. In this article we present a new class of L\\'{e}vy\ndriven models of stock markets. In our opinion, any market model should be\nbased on a transparent and intuitively easily acceptable concept. In our case\nthis is a linear system of stochastic equations. Our market model is based on\nthe principle of inheritance, i.e. for the particular choice of parameters it\ncoincides with known models. Also, the model proposed is effectively\nnumerically realizable. For the class of models under cosideration, we give an\nexplicit representations of characteristic functions. This allows us us to\nconstruct a sequence of approximation formulas to price basket options. We show\nthat our approximation formulas have almost optimal rate of convergence in the\nsense of respective n-widths.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 23:09:49 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Kushpel", "Alexander", ""]]}, {"id": "1401.2314", "submitter": "Masaaki Fujii", "authors": "Masaaki Fujii, Akihiko Takahashi", "title": "Optimal Hedging for Fund & Insurance Managers with Partially Observable\n  Investment Flows", "comments": "Revised version. Forthcoming in Quantitative Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All the financial practitioners are working in incomplete markets full of\nunhedgeable risk-factors. Making the situation worse, they are only equipped\nwith the imperfect information on the relevant processes. In addition to the\nmarket risk, fund and insurance managers have to be prepared for sudden and\npossibly contagious changes in the investment flows from their clients so that\nthey can avoid the over- as well as under-hedging. In this work, the prices of\nsecurities, the occurrences of insured events and (possibly a network of) the\ninvestment flows are used to infer their drifts and intensities by a stochastic\nfiltering technique. We utilize the inferred information to provide the optimal\nhedging strategy based on the mean-variance (or quadratic) risk criterion. A\nBSDE approach allows a systematic derivation of the optimal strategy, which is\nshown to be implementable by a set of simple ODEs and the standard Monte Carlo\nsimulation. The presented framework may also be useful for manufactures and\nenergy firms to install an efficient overlay of dynamic hedging by financial\nderivatives to minimize the costs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:47:30 GMT"}, {"version": "v2", "created": "Sat, 26 Jul 2014 05:38:22 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Fujii", "Masaaki", ""], ["Takahashi", "Akihiko", ""]]}, {"id": "1401.2900", "submitter": "Elisa Appolloni", "authors": "Elisa Appolloni and Andrea Ligori", "title": "Efficient tree methods for pricing digital barrier options", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient lattice procedure which permits to obtain European\nand American option prices under the Black and Scholes model for digital\noptions with barrier features. Numerical results show the accuracy of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 16:23:54 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 17:58:33 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Appolloni", "Elisa", ""], ["Ligori", "Andrea", ""]]}, {"id": "1401.3994", "submitter": "Andrea Pallavicini Mr", "authors": "Damiano Brigo and Andrea Pallavicini", "title": "CCP Cleared or Bilateral CSA Trades with Initial/Variation Margins under\n  credit, funding and wrong-way risks: A Unified Valuation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of CCPs in most derivative transactions will dramatically\nchange the landscape of derivatives pricing, hedging and risk management, and,\naccording to the TABB group, will lead to an overall liquidity impact about 2\nUSD trillions. In this article we develop for the first time a comprehensive\napproach for pricing under CCP clearing, including variation and initial\nmargins, gap credit risk and collateralization, showing concrete examples for\ninterest rate swaps. Mathematically, the inclusion of asymmetric borrowing and\nlending rates in the hedge of a claim lead to nonlinearities showing up in\nclaim dependent pricing measures, aggregation dependent prices, nonlinear PDEs\nand BSDEs. This still holds in presence of CCPs and CSA. We introduce a\nmodeling approach that allows us to enforce rigorous separation of the\ninterconnected nonlinear risks into different valuation adjustments where the\nkey pricing nonlinearities are confined to a funding costs component that is\nanalyzed through numerical schemes for BSDEs. We present a numerical case study\nfor Interest Rate Swaps that highlights the relative size of the different\nvaluation adjustments and the quantitative role of initial and variation\nmargins, of liquidity bases, of credit risk, of the margin period of risk and\nof wrong way risk correlations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 11:33:04 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Brigo", "Damiano", ""], ["Pallavicini", "Andrea", ""]]}, {"id": "1401.5666", "submitter": "Leonard Rogers", "authors": "M. Duembgen, L. C. G. Rogers", "title": "Estimate nothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the econometrics of financial time series, it is customary to take some\nparametric model for the data, and then estimate the parameters from historical\ndata. This approach suffers from several problems. Firstly, how is estimation\nerror to be quantified, and then taken into account when making statements\nabout the future behaviour of the observed time series? Secondly, decisions may\nbe taken today committing to future actions over some quite long horizon, as in\nthe trading of derivatives; if the model is re-estimated at some intermediate\ntime, our earlier decisions would need to be revised - but the derivative has\nalready been traded at the earlier price. Thirdly, the exact form of the\nparametric model to be used is generally taken as given at the outset; other\ncompetitor models might possibly work better in some circumstances, but the\nmethodology does not allow them to be factored into the inference. What we\npropose here is a very simple (Bayesian) alternative approach to inference and\naction in financial econometrics which deals decisively with all these issues.\nThe key feature is that nothing is being estimated.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 13:44:31 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Duembgen", "M.", ""], ["Rogers", "L. C. G.", ""]]}]