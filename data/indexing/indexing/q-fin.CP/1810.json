[{"id": "1810.01116", "submitter": "Jaehyuk Choi", "authors": "Jaehyuk Choi, Yeda Du, Qingshuo Song", "title": "Inverse Gaussian quadrature and finite normal-mixture approximation of\n  the generalized hyperbolic distribution", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, 388:113302, 2021", "doi": "10.1016/j.cam.2020.113302", "report-no": null, "categories": "stat.CO q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a numerical quadrature for the generalized inverse Gaussian\ndistribution is derived from the Gauss-Hermite quadrature by exploiting its\nrelationship with the normal distribution. The proposed quadrature is not\nGaussian, but it exactly integrates the polynomials of both positive and\nnegative orders. Using the quadrature, the generalized hyperbolic distribution\nis efficiently approximated as a finite normal variance-mean mixture.\nTherefore, the expectations under the distribution, such as cumulative\ndistribution function and European option price, are accurately computed as\nweighted sums of those under normal distributions. The generalized hyperbolic\nrandom variates are also sampled in a straightforward manner. The accuracy of\nthe methods is illustrated with numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:39:05 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 07:41:45 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 17:09:04 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Choi", "Jaehyuk", ""], ["Du", "Yeda", ""], ["Song", "Qingshuo", ""]]}, {"id": "1810.01165", "submitter": "Tao Li", "authors": "Tao Li, Xudong Liu, Shihan Su", "title": "Semi-supervised Text Regression with Conditional Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2018.8622140", "report-no": null, "categories": "cs.CL cs.AI q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enormous online textual information provides intriguing opportunities for\nunderstandings of social and economic semantics. In this paper, we propose a\nnovel text regression model based on a conditional generative adversarial\nnetwork (GAN), with an attempt to associate textual data and social outcomes in\na semi-supervised manner. Besides promising potential of predicting\ncapabilities, our superiorities are twofold: (i) the model works with\nunbalanced datasets of limited labelled data, which align with real-world\nscenarios; and (ii) predictions are obtained by an end-to-end framework,\nwithout explicitly selecting high-level representations. Finally we point out\nrelated datasets for experiments and future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 10:35:13 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 05:37:37 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Li", "Tao", ""], ["Liu", "Xudong", ""], ["Su", "Shihan", ""]]}, {"id": "1810.02071", "submitter": "Jaehyuk Choi", "authors": "Jeechul Woo, Chenru Liu, Jaehyuk Choi", "title": "Leave-One-Out Least Square Monte Carlo Algorithm for Pricing American\n  Options", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least square Monte Carlo (LSM) algorithm proposed by Longstaff and\nSchwartz (2001) is widely used for pricing American options. The LSM estimator\ncontains undesirable look-ahead bias, and the conventional technique of\nremoving it necessitates doubling simulations. We present the leave-one-out LSM\n(LOOLSM) algorithm for efficiently eliminating look-ahead bias. We also show\nthat look-ahead bias is asymptotically proportional to the\nregressors-to-simulation paths ratio. Our findings are demonstrated with\nseveral option examples, including the multi-asset cases that the LSM algorithm\nsignificantly overvalues. The LOOLSM method can be extended to other\nregression-based algorithms improving the LSM method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 06:49:50 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 14:09:06 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 03:46:44 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Woo", "Jeechul", ""], ["Liu", "Chenru", ""], ["Choi", "Jaehyuk", ""]]}, {"id": "1810.02390", "submitter": "Vadim Kaushansky", "authors": "Alexander Lipton and Vadim Kaushansky", "title": "On the First Hitting Time Density of an Ornstein-Uhlenbeck Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the classical problem of the first passage hitting\ndensity of an Ornstein--Uhlenbeck process. We give two complementary (forward\nand backward) formulations of this problem and provide semi-analytical\nsolutions for both. The corresponding problems are comparable in complexity. By\nusing the method of heat potentials, we show how to reduce these problems to\nlinear Volterra integral equations of the second kind. For small values of $t$,\nwe solve these equations analytically by using Abel equation approximation; for\nlarger $t$ we solve them numerically. We also provide a comparison with other\nknown methods for finding the hitting density of interest, and argue that our\nmethod has considerable advantages and provides additional valuable insights.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 18:42:05 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 07:00:21 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Lipton", "Alexander", ""], ["Kaushansky", "Vadim", ""]]}, {"id": "1810.02447", "submitter": "Alex Garivaltis", "authors": "Alex Garivaltis", "title": "Multilinear Superhedging of Lookback Options", "comments": "41 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR econ.TH q-fin.CP q-fin.GN q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a pathbreaking paper, Cover and Ordentlich (1998) solved a max-min\nportfolio game between a trader (who picks an entire trading algorithm,\n$\\theta(\\cdot)$) and \"nature,\" who picks the matrix $X$ of gross-returns of all\nstocks in all periods. Their (zero-sum) game has the payoff kernel\n$W_\\theta(X)/D(X)$, where $W_\\theta(X)$ is the trader's final wealth and $D(X)$\nis the final wealth that would have accrued to a $\\$1$ deposit into the best\nconstant-rebalanced portfolio (or fixed-fraction betting scheme) determined in\nhindsight. The resulting \"universal portfolio\" compounds its money at the same\nasymptotic rate as the best rebalancing rule in hindsight, thereby beating the\nmarket asymptotically under extremely general conditions. Smitten with this\n(1998) result, the present paper solves the most general tractable version of\nCover and Ordentlich's (1998) max-min game. This obtains for performance\nbenchmarks (read: derivatives) that are separately convex and homogeneous in\neach period's gross-return vector. For completely arbitrary (even\nnon-measurable) performance benchmarks, we show how the axiom of choice can be\nused to \"find\" an exact maximin strategy for the trader.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 22:50:42 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Garivaltis", "Alex", ""]]}, {"id": "1810.02529", "submitter": "Lionel Yelibi", "authors": "Lionel Yelibi, Tim Gebbie", "title": "Fast Super-Paramagnetic Clustering", "comments": "25 pages, 41 Figures and code at\n  https://github.com/tehraio/potts-model-clustering", "journal-ref": "Physica A, Volume 551, 1 August 2020, 124049", "doi": "10.1016/j.physa.2019.124049", "report-no": null, "categories": "q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We map stock market interactions to spin models to recover their hierarchical\nstructure using a simulated annealing based Super-Paramagnetic Clustering (SPC)\nalgorithm. This is directly compared to a modified implementation of a maximum\nlikelihood approach we call Fast Super-Paramagnetic Clustering (f-SPC). The\nmethods are first applied standard toy test-case problems, and then to a\ndata-set of 447 stocks traded on the New York Stock Exchange (NYSE) over 1249\ndays. The signal to noise ratio of stock market correlation matrices is briefly\nconsidered. Our result recover approximately clusters representative of\nstandard economic sectors and mixed ones whose dynamics shine light on the\nadaptive nature of financial markets and raise concerns relating to the\neffectiveness of industry based static financial market classification in the\nworld of real-time data analytics. A key result is that we show that f-SPC\nmaximum likelihood solutions converge to ones found within the\nSuper-Paramagnetic Phase where the entropy is maximum, and those solutions are\nqualitatively better for high dimensionality data-sets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 06:06:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 21:43:08 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yelibi", "Lionel", ""], ["Gebbie", "Tim", ""]]}, {"id": "1810.04868", "submitter": "Eduardo Abi Jaber", "authors": "Eduardo Abi Jaber (CEREMADE)", "title": "Lifting the Heston model", "comments": "Quantitative Finance, Taylor & Francis (Routledge), In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to reconcile the classical Heston model with its rough counterpart? We\nintroduce a lifted version of the Heston model with n multi-factors, sharing\nthe same Brownian motion but mean reverting at different speeds. Our model\nnests as extreme cases the classical Heston model (when n = 1), and the rough\nHeston model (when n goes to infinity). We show that the lifted model enjoys\nthe best of both worlds: Markovianity and satisfactory fits of implied\nvolatility smiles for short maturities with very few parameters. Further, our\napproach speeds up the calibration time and opens the door to time-efficient\nsimulation schemes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 07:21:25 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:12:28 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Jaber", "Eduardo Abi", "", "CEREMADE"]]}, {"id": "1810.05094", "submitter": "David \\v{S}i\\v{s}ka", "authors": "Marc Sabate Vidales and David Siska and Lukasz Szpruch", "title": "Unbiased deep solvers for parametric PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several deep learning algorithms for approximating families of\nparametric PDE solutions. The proposed algorithms approximate solutions\ntogether with their gradients, which in the context of mathematical finance\nmeans that the derivative prices and hedging strategies are computed\nsimulatenously. Having approximated the gradient of the solution one can\ncombine it with a Monte-Carlo simulation to remove the bias in the deep network\napproximation of the PDE solution (derivative price). This is achieved by\nleveraging the Martingale Representation Theorem and combining the Monte Carlo\nsimulation with the neural network. The resulting algorithm is robust with\nrespect to quality of the neural network approximation and consequently can be\nused as a black-box in case only limited a priori information about the\nunderlying problem is available. We believe this is important as neural network\nbased algorithms often require fair amount of tuning to produce satisfactory\nresults. The methods are empirically shown to work for high-dimensional\nproblems (e.g. 100 dimensions). We provide diagnostics that shed light on\nappropriate network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 15:53:38 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 19:53:02 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 11:17:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Vidales", "Marc Sabate", ""], ["Siska", "David", ""], ["Szpruch", "Lukasz", ""]]}, {"id": "1810.06335", "submitter": "Rodwell Kufakunesu", "authors": "Rodwell Kufakunesu and Farai Mhlanga", "title": "On the sensitivity analysis of energy quanto options", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an advent of quanto options in energy markets.\nThe structure of the payoff is rather a different type from other markets since\nit is written as a product of an underlying energy index and a measure of\ntemperature. In the HJM framework, by adopting the futures energy dynamics, we\nuse the Malliavin calculus to derive the delta and the cross-gamma expectation\nformulas. This work can be viewed as an extension of the work done, for example\nby Benth et al. [1].\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 06:33:53 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kufakunesu", "Rodwell", ""], ["Mhlanga", "Farai", ""]]}, {"id": "1810.06696", "submitter": "Zvezdin Besarabov", "authors": "Zvezdin Besarabov, Todor Kolev", "title": "Predicting digital asset market based on blockchain activity data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology shows significant results and huge potential for\nserving as an interweaving fabric that goes through every industry and market,\nallowing decentralized and secure value exchange, thus connecting our\ncivilization like never before. The standard approach for asset value\npredictions is based on market analysis with an LSTM neural network. Blockchain\ntechnologies, however, give us access to vast amounts of public data, such as\nthe executed transactions and the account balance distribution. We explore\nwhether analyzing this data with modern Deep Leaning techniques results in\nhigher accuracies than the standard approach. During a series of experiments on\nthe Ethereum blockchain, we achieved $4$ times error reduction with blockchain\ndata than an LSTM approach with trade volume data. By utilizing blockchain\naccount distribution histograms, spatial dataset modeling, and a Convolutional\narchitecture, the error was reduced further by 26\\%. The proposed methodologies\nare implemented in an open source cryptocurrency prediction framework, allowing\nthem to be used in other analysis contexts.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 21:09:05 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Besarabov", "Zvezdin", ""], ["Kolev", "Todor", ""]]}, {"id": "1810.08584", "submitter": "Davide Venturelli", "authors": "Davide Venturelli and Alexei Kondratyev", "title": "Reverse Quantum Annealing Approach to Portfolio Optimization Problems", "comments": "19 pages, 6 figures (incl. appendix)", "journal-ref": "Quantum Mach. Intell. (2019)", "doi": "10.1007/s42484-019-00001-w", "report-no": null, "categories": "quant-ph q-fin.CP q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a hybrid quantum-classical solution method to the\nmean-variance portfolio optimization problems. Starting from real financial\ndata statistics and following the principles of the Modern Portfolio Theory, we\ngenerate parametrized samples of portfolio optimization problems that can be\nrelated to quadratic binary optimization forms programmable in the analog\nD-Wave Quantum Annealer 2000Q. The instances are also solvable by an\nindustry-established Genetic Algorithm approach, which we use as a classical\nbenchmark. We investigate several options to run the quantum computation\noptimally, ultimately discovering that the best results in terms of expected\ntime-to-solution as a function of number of variables for the hardest instances\nset are obtained by seeding the quantum annealer with a solution candidate\nfound by a greedy local search and then performing a reverse annealing\nprotocol. The optimized reverse annealing protocol is found to be more than 100\ntimes faster than the corresponding forward quantum annealing on average.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 16:58:33 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 20:41:41 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Venturelli", "Davide", ""], ["Kondratyev", "Alexei", ""]]}, {"id": "1810.08923", "submitter": "Ehsan Hoseinzade", "authors": "Ehsan Hoseinzade, Saman Haratizadeh", "title": "CNNPred: CNN-based stock market prediction using several data sources", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction from financial data is one of the most important problems\nin market prediction domain for which many approaches have been suggested.\nAmong other modern tools, convolutional neural networks (CNN) have recently\nbeen applied for automatic feature selection and market prediction. However, in\nexperiments reported so far, less attention has been paid to the correlation\namong different markets as a possible source of information for extracting\nfeatures. In this paper, we suggest a CNN-based framework with specially\ndesigned CNNs, that can be applied on a collection of data from a variety of\nsources, including different markets, in order to extract features for\npredicting the future of those markets. The suggested framework has been\napplied for predicting the next day's direction of movement for the indices of\nS&P 500, NASDAQ, DJI, NYSE, and RUSSELL markets based on various sets of\ninitial features. The evaluations show a significant improvement in\nprediction's performance compared to the state of the art baseline algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 10:34:56 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Hoseinzade", "Ehsan", ""], ["Haratizadeh", "Saman", ""]]}, {"id": "1810.09112", "submitter": "Erik Schlogl", "authors": "Yu Feng, Ralph Rudd, Christopher Baker, Qaphela Mashalaba, Melusi\n  Mavuso, Erik Schl\\\"ogl", "title": "Quantifying the Model Risk Inherent in the Calibration and Recalibration\n  of Option Pricing Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two particular aspects of model risk: the inability of a chosen\nmodel to fit observed market prices at a given point in time (calibration\nerror) and the model risk due to recalibration of model parameters (in\ncontradiction to the model assumptions). In this context, we follow the\napproach of Glasserman and Xu (2014) and use relative entropy as a pre-metric\nin order to quantify these two sources of model risk in a common framework, and\nconsider the trade-offs between them when choosing a model and the frequency\nwith which to recalibrate to the market. We illustrate this approach applied to\nthe models of Black and Scholes (1973) and Heston (1993), using option data for\nApple (AAPL) and Google (GOOG). We find that recalibrating a model more\nfrequently simply shifts model risk from one type to another, without any\nsubstantial reduction of aggregate model risk. Furthermore, moving to a more\ncomplicated stochastic model is seen to be counterproductive if one requires a\nhigh degree of robustness, for example as quantified by a 99 percent quantile\nof aggregate model risk.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:02:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Feng", "Yu", ""], ["Rudd", "Ralph", ""], ["Baker", "Christopher", ""], ["Mashalaba", "Qaphela", ""], ["Mavuso", "Melusi", ""], ["Schl\u00f6gl", "Erik", ""]]}, {"id": "1810.10563", "submitter": "Aleksandr Aravkin", "authors": "Jize Zhang, Tim Leung, and Aleksandr Aravkin", "title": "A Relaxed Optimization Approach for Cardinality-Constrained Portfolio\n  Optimization", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cardinality-constrained portfolio caps the number of stocks to be traded\nacross and within groups or sectors. These limitations arise from real-world\nscenarios faced by fund managers, who are constrained by transaction costs and\nclient preferences as they seek to maximize return and limit risk.\n  We develop a new approach to solve cardinality-constrained portfolio\noptimization problems, extending both Markowitz and conditional value at risk\n(CVaR) optimization models with cardinality constraints. We derive a continuous\nrelaxation method for the NP-hard objective, which allows for very efficient\nalgorithms with standard convergence guarantees for nonconvex problems. For\nsmaller cases, where brute force search is feasible to compute the globally\noptimal cardinality- constrained portfolio, the new approach finds the best\nportfolio for the cardinality-constrained Markowitz model and a very good local\nminimum for the cardinality-constrained CVaR model. For higher dimensions,\nwhere brute-force search is prohibitively expensive, we find feasible\nportfolios that are nearly as efficient as their non-cardinality constrained\ncounterparts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:12:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zhang", "Jize", ""], ["Leung", "Tim", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1810.10726", "submitter": "Vic Norton", "authors": "Vic Norton", "title": "How Not To Do Mean-Variance Analysis", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the 2014 market history of two high-returning biotechnology\nexchange-traded funds to illustrate how ex post mean-variance analysis should\nnot be done. Unfortunately, the way it should not be done is the way it\ngenerally is done -- to our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 05:35:45 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Norton", "Vic", ""]]}, {"id": "1810.11039", "submitter": "Jorge Ignacio Gonz\\'alez C\\'azares", "authors": "Jorge Ignacio Gonz\\'alez C\\'azares, Aleksandar Mijatovi\\'c, Ger\\'onimo\n  Uribe Bravo", "title": "Geometrically Convergent Simulation of the Extrema of L\\'{e}vy Processes", "comments": "Minor revision: reintroduction of the result on the scaling limits.\n  37 pages and 5 figures. Short presentation on: https://youtu.be/P3vHmJUCFbU", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approximate simulation algorithm for the joint law of the\nposition, the running supremum and the time of the supremum of a general L\\'evy\nprocess at an arbitrary finite time. We identify the law of the error in simple\nterms. We prove that the error decays geometrically in $L^p$ (for any $p\\geq\n1$) as a function of the computational cost, in contrast with the polynomial\ndecay for the approximations available in the literature. We establish a\ncentral limit theorem and construct non-asymptotic and asymptotic confidence\nintervals for the corresponding Monte Carlo estimator. We prove that the\nmultilevel Monte Carlo estimator has optimal computational complexity (i.e. of\norder $\\epsilon^{-2}$ if the mean squared error is at most $\\epsilon^2$) for\nlocally Lipschitz and barrier-type functionals of the triplet and develop an\nunbiased version of the estimator. We illustrate the performance of the\nalgorithm with numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:01:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 07:29:29 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 21:22:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["C\u00e1zares", "Jorge Ignacio Gonz\u00e1lez", ""], ["Mijatovi\u0107", "Aleksandar", ""], ["Bravo", "Ger\u00f3nimo Uribe", ""]]}, {"id": "1810.13248", "submitter": "Bertram D\\\"uring", "authors": "Bertram D\\\"uring, Alexander Pitkin", "title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility with contemporaneous jump models", "comments": "6 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1704.05308, arXiv:1710.05542", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the scheme developed in B. D\\\"uring, A. Pitkin, \"High-order compact\nfinite difference scheme for option pricing in stochastic volatility jump\nmodels\", 2019, to the so-called stochastic volatility with contemporaneous\njumps (SVCJ) model, derived by Duffie, Pan and Singleton. The performance of\nthe scheme is assessed through a number of numerical experiments, using\ncomparisons against a standard second-order central difference scheme. We\nobserve that the new high-order compact scheme achieves fourth order\nconvergence and discuss the effects on efficiency and computation time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 09:48:02 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 17:42:04 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["D\u00fcring", "Bertram", ""], ["Pitkin", "Alexander", ""]]}]