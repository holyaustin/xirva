[{"id": "1804.00003", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Alexander Sidorenko, David J. Thomson", "title": "Spectral Estimation of Plasma Fluctuations I: Comparison of Methods", "comments": "Missing Figures", "journal-ref": "Physics of Plasmas, Volume 1, Issue 3, March 1994, pp.485-500", "doi": "10.1063/1.870794", "report-no": null, "categories": "stat.AP eess.AS eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative root mean squared errors (RMSE) of nonparametric methods for\nspectral estimation is compared for microwave scattering data of plasma\nfluctuations. These methods reduce the variance of the periodogram estimate by\naveraging the spectrum over a frequency bandwidth. As the bandwidth increases,\nthe variance decreases, but the bias error increases. The plasma spectra vary\nby over four orders of magnitude, and therefore, using a spectral window is\nnecessary. We compare the smoothed tapered periodogram with the adaptive\nmultiple taper methods and hybrid methods. We find that a hybrid method, which\nuses four orthogonal tapers and then applies a kernel smoother, performs best.\nFor 300 point data segments, even an optimized smoothed tapered periodogram has\na 24 \\% larger relative RMSE than the hybrid method. We present two new\nadaptive multi-taper weightings which outperform Thomson's original adaptive\nweighting.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 19:54:45 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "Alexander", ""], ["Thomson", "David J.", ""]]}, {"id": "1804.00034", "submitter": "Fang Han", "authors": "Fang Han and Tianchen Qian", "title": "On inference validity of weighted U-statistics under data heterogeneity", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by challenges on studying a new correlation measurement being\npopularized in evaluating online ranking algorithms' performance, this\nmanuscript explores the validity of uncertainty assessment for weighted\nU-statistics. Without any commonly adopted assumption, we verify Efron's\nbootstrap and a new resampling procedure's inference validity. Specifically, in\nits full generality, our theory allows both kernels and weights asymmetric and\ndata points not identically distributed, which are all new issues that\nhistorically have not been addressed. For achieving strict generalization, for\nexample, we have to carefully control the order of the \"degenerate\" term in\nU-statistics which are no longer degenerate under the empirical measure for\nnon-i.i.d. data. Our result applies to the motivating task, giving the region\nat which solid statistical inference can be made.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 19:08:18 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Han", "Fang", ""], ["Qian", "Tianchen", ""]]}, {"id": "1804.00102", "submitter": "Cheng Ju", "authors": "Cheng Ju and Antoine Chambaz and Mark J. van der Laan", "title": "Collaborative targeted inference from continuously indexed nuisance\n  parameter estimators", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to infer the value of a parameter at a law from which we sample\nindependent observations. The parameter is smooth and we can define two\nvariation-independent features of the law, its $Q$- and $G$-components, such\nthat estimating them consistently at a fast enough product of rates allows to\nbuild a confidence interval (CI) with a given asymptotic level from a plain\ntargeted minimum loss estimator (TMLE). Say that the above product is not fast\nenough and the algorithm for the $G$-component is fine-tuned by a real-valued\n$h$. A plain TMLE with an $h$ chosen by cross-validation would typically not\nyield a CI. We construct a collaborative TMLE (C-TMLE) and show under mild\nconditions that, if there exists an oracle $h$ that makes a bulky remainder\nterm asymptotically Gaussian, then the C-TMLE yields a CI. We illustrate our\nfindings with the inference of the average treatment effect. We conduct a\nsimulation study where the $G$-component is estimated by the LASSO and $h$ is\nthe bound on the coefficients' norms. It sheds light on small sample\nproperties, in the face of low- to high-dimensional baseline covariates, and\npossibly positivity violation.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 01:30:36 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 18:29:43 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Ju", "Cheng", ""], ["Chambaz", "Antoine", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1804.00108", "submitter": "Navid Ghadermarzy", "authors": "Navid Ghadermarzy and Yaniv Plan and Ozgur Yilmaz", "title": "Learning tensors from partial binary measurements", "comments": "26 pages", "journal-ref": null, "doi": "10.1109/TSP.2018.2879031", "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the 1-bit matrix completion problem to higher\norder tensors. We prove that when $r=O(1)$ a bounded rank-$r$, order-$d$ tensor\n$T$ in $\\mathbb{R}^{N} \\times \\mathbb{R}^{N} \\times \\cdots \\times\n\\mathbb{R}^{N}$ can be estimated efficiently by only $m=O(Nd)$ binary\nmeasurements by regularizing its max-qnorm and M-norm as surrogates for its\nrank. We prove that similar to the matrix case, i.e., when $d=2$, the sample\ncomplexity of recovering a low-rank tensor from 1-bit measurements of a subset\nof its entries is the same as recovering it from unquantized measurements.\nMoreover, we show the advantage of using 1-bit tensor completion over\nmatricization both theoretically and numerically. Specifically, we show how the\n1-bit measurement model can be used for context-aware recommender systems.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 02:31:15 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Ghadermarzy", "Navid", ""], ["Plan", "Yaniv", ""], ["Yilmaz", "Ozgur", ""]]}, {"id": "1804.00232", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Continuous Record Laplace-based Inference about the Break Date in\n  Structural Change Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building upon the continuous record asymptotic framework recently introduced\nby Casini and Perron (2018a) for inference in structural change models, we\npropose a Laplace-based (Quasi-Bayes) procedure for the construction of the\nestimate and confidence set for the date of a structural change. It is defined\nby an integration rather than an optimization-based method. A transformation of\nthe least-squares criterion function is evaluated in order to derive a proper\ndistribution, referred to as the Quasi-posterior. For a given choice of a loss\nfunction, the Laplace-type estimator is the minimizer of the expected risk with\nthe expectation taken under the Quasi-posterior. Besides providing an\nalternative estimate that is more precise|lower mean absolute error (MAE) and\nlower root-mean squared error (RMSE)|than the usual least-squares one, the\nQuasi-posterior distribution can be used to construct asymptotically valid\ninference using the concept of Highest Density Region. The resulting\nLaplace-based inferential procedure is shown to have lower MAE and RMSE, and\nthe confidence sets strike the best balance between empirical coverage rates\nand average lengths of the confidence sets relative to traditional long-span\nmethods, whether the break size is small or large.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:04:25 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 22:09:51 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 22:15:44 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "1804.00355", "submitter": "Arkadi Nemirovski", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "Near-Optimal Recovery of Linear and N-Convex Functions on Unions of\n  Convex Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we build provably near-optimal, in the minimax sense, estimates\nof linear forms and, more generally, \"$N$-convex functionals\" (the simplest\nexample being the maximum of several fractional-linear functions) of unknown\n\"signal\" known to belong to the union of finitely many convex compact sets from\nindirect noisy observations of the signal. Our main assumption is that the\nobservation scheme in question is good in the sense of A. Goldenshluger, A.\nJuditsky, A. Nemirovski, Electr. J. Stat. 9(2) (2015), arXiv:1311.6765, the\nsimplest example being the Gaussian scheme where the observation is the sum of\nlinear image of the signal and the standard Gaussian noise. The proposed\nestimates, same as upper bounds on their worst-case risks, stem from solutions\nto explicit convex optimization problems, making the estimates\n\"computation-friendly.\"\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 22:46:34 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 17:09:50 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 12:21:24 GMT"}, {"version": "v4", "created": "Sat, 9 Jun 2018 13:09:41 GMT"}, {"version": "v5", "created": "Fri, 29 Mar 2019 17:03:57 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1804.00567", "submitter": "Zongming Ma", "authors": "Debapratim Banerjee, Zongming Ma", "title": "Asymptotic normality and analysis of variance of log-likelihood ratios\n  in spiked random matrix models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present manuscript studies signal detection by likelihood ratio tests in\na number of spiked random matrix models, including but not limited to Gaussian\nmixtures and spiked Wishart covariance matrices. We work directly with\nmulti-spiked cases in these models and with flexible priors on the signal\ncomponent that allow dependence across spikes. We derive asymptotic normality\nfor the log-likelihood ratios when the signal-to- noise ratios are below\ncertain thresholds. In addition, we show that the variances of the\nlog-likelihood ratios can be asymptotically decomposed as the sums of those of\na collection of statistics which we call bipartite signed cycles.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 14:44:30 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Banerjee", "Debapratim", ""], ["Ma", "Zongming", ""]]}, {"id": "1804.00778", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Santiago Segarra, Caroline Uhler", "title": "High-Dimensional Joint Estimation of Multiple Directed Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly estimating multiple related directed\nacyclic graph (DAG) models based on high-dimensional data from each graph. This\nproblem is motivated by the task of learning gene regulatory networks based on\ngene expression data from different tissues, developmental stages or disease\nstates. We prove that under certain regularity conditions, the proposed\n$\\ell_0$-penalized maximum likelihood estimator converges in Frobenius norm to\nthe adjacency matrices consistent with the data-generating distributions and\nhas the correct sparsity. In particular, we show that this joint estimation\nprocedure leads to a faster convergence rate than estimating each DAG model\nseparately. As a corollary, we also obtain high-dimensional consistency results\nfor causal inference from a mix of observational and interventional data. For\npractical purposes, we propose \\emph{jointGES} consisting of Greedy Equivalence\nSearch (GES) to estimate the union of all DAG models followed by variable\nselection using lasso to obtain the different DAGs, and we analyze its\nconsistency guarantees. The proposed method is illustrated through an analysis\nof simulated data as well as epithelial ovarian cancer gene expression data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 01:29:23 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 11:57:46 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 13:18:26 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Yuhao", ""], ["Segarra", "Santiago", ""], ["Uhler", "Caroline", ""]]}, {"id": "1804.00793", "submitter": "Fei Jiang", "authors": "Fei Jiang, Yanyuan Ma, Raymond J. Carroll", "title": "A spline-assisted semiparametric approach to non-parametric measurement\n  error models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the minimax rates of convergence of nonparametric\ndensity and regression function estimation of a random variable measured with\nerror is much slower than the rate in the error free case. Surprisingly, we\nshow that if one is willing to impose a relatively mild assumption in requiring\nthat the error-prone variable has a compact support, then the results can be\ngreatly improved. We describe new and constructive methods to take full\nadvantage of the compact support assumption via spline-assisted semiparametric\nmethods. We further prove that the new estimator achieves the usual\nnonparametric rate in estimating both the density and regression functions as\nif there were no measurement error. The proof involves linear and bilinear\noperator theories, semiparametric theory, asymptotic analysis regarding\nBsplines, as well as integral equation treatments. The performance of the new\nmethods is demonstrated through several simulations and a data example.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 02:26:04 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 18:24:10 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Jiang", "Fei", ""], ["Ma", "Yanyuan", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1804.00849", "submitter": "Vicky Fasen-Hartmann", "authors": "Vicky Fasen-Hartmann and Sebastian Kimmig", "title": "Robust estimation of stationary continuous-time ARMA models via indirect\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a robust estimator for the parameters of a\ncontinuous-time ARMA(p,q) (CARMA(p,q)) process sampled equidistantly which is\nnot necessarily Gaussian. Therefore, an indirect estimation procedure is used.\nIt is an indirect estimation because we first estimate the parameters of the\nauxiliary AR(r) representation ($r\\geq 2p-1$) of the sampled CARMA process\nusing a generalized M- (GM-)estimator. Since the map which maps the parameters\nof the auxiliary AR(r) representation to the parameters of the CARMA process is\nnot given explicitly, a separate simulation part is necessary where the\nparameters of the AR(r) representation are estimated from simulated CARMA\nprocesses. Then, the parameter which takes the minimum distance between the\nestimated AR parameters and the simulated AR parameters gives an estimator for\nthe CARMA parameters. First, we show that under some standard assumptions the\nGM-estimator for the AR(r) parameters is consistent and asymptotically normally\ndistributed. Next, we prove that the indirect estimator is consistent and\nasymptotically normally distributed as well using in the simulation part the\nasymptotically normally distributed LS-estimator. The indirect estimator\nsatisfies several important robustness properties such as weak resistance,\n$\\pi_{d_n}$-robustness and it has a bounded influence functional. The practical\napplicability of our method is demonstrated through a simulation study with\nreplacement outliers and compared to the non-robust quasi-maximum-likelihood\nestimation method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 07:23:15 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:39:18 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fasen-Hartmann", "Vicky", ""], ["Kimmig", "Sebastian", ""]]}, {"id": "1804.00864", "submitter": "Botond Szabo", "authors": "Botond Szabo and Harry van Zanten", "title": "Adaptive distributed methods under communication constraints", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed estimation methods under communication constraints in a\ndistributed version of the nonparametric random design regression model. We\nderive minimax lower bounds and exhibit methods that attain those bounds.\nMoreover, we show that adaptive estimation is possible in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 08:18:12 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 23:45:56 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Szabo", "Botond", ""], ["van Zanten", "Harry", ""]]}, {"id": "1804.00989", "submitter": "Sara van de Geer", "authors": "Sara van de Geer", "title": "On tight bounds for the Lasso", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present upper and lower bounds for the prediction error of the Lasso. For\nthe case of random Gaussian design, we show that under mild conditions the\nprediction error of the Lasso is up to smaller order terms dominated by the\nprediction error of its noiseless counterpart. We then provide exact\nexpressions for the prediction error of the latter, in terms of compatibility\nconstants. Here, we assume the active components of the underlying regression\nfunction satisfy some \"betamin\" condition. For the case of fixed design, we\nprovide upper and lower bounds, again in terms of compatibility constants. As\nan example, we give an up to a logarithmic term tight bound for the least\nsquares estimator with total variation penalty.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 14:26:18 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["van de Geer", "Sara", ""]]}, {"id": "1804.01071", "submitter": "Stephane Chretien", "authors": "Stephane Chretien, Christophe Guyeux and Zhen-Wai Olivier HO", "title": "Average performance analysis of the stochastic gradient method for\n  online PCA", "comments": "11 pages, 1 figure, Submitted to LOD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the complexity of the stochastic gradient algorithm for\nPCA when the data are observed in a streaming setting. We also propose an\nonline approach for selecting the learning rate. Simulation experiments confirm\nthe practical relevance of the plain stochastic gradient approach and that\ndrastic improvements can be achieved by learning the learning rate.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 17:31:32 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Chretien", "Stephane", ""], ["Guyeux", "Christophe", ""], ["HO", "Zhen-Wai Olivier", ""]]}, {"id": "1804.01187", "submitter": "Amir Nikooienejad", "authors": "Amir Nikooienejad and Valen E. Johnson", "title": "On the Existence of Uniformly Most Powerful Bayesian Tests With\n  Application to Non-Central Chi-Squared Tests", "comments": "16 pages, 3 figures", "journal-ref": "Bayesian Analysis (2020)", "doi": "10.1214/19-ba1194", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniformly most powerful Bayesian tests (UMPBT's) are an objective class of\nBayesian hypothesis tests that can be considered the Bayesian counterpart of\nclassical uniformly most powerful tests. Because the rejection regions of\nUMPBT's can be matched to the rejection regions of classical uniformly most\npowerful tests (UMPTs), UMPBT's provide a mechanism for calibrating Bayesian\nevidence thresholds, Bayes factors, classical significance levels and p-values.\nThe purpose of this article is to expand the application of UMPBT's outside the\nclass of exponential family models. Specifically, we introduce sufficient\nconditions for the existence of UMPBT's and propose a unified approach for\ntheir derivation. An important application of our methodology is the extension\nof UMPBT's to testing whether the non-centrality parameter of a chi-squared\ndistribution is zero. The resulting tests have broad applicability, providing\ndefault alternative hypotheses to compute Bayes factors in, for example,\nPearson's chi-squared test for goodness-of-fit, tests of independence in\ncontingency tables, and likelihood ratio, score and Wald tests.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:39:57 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 02:54:08 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Nikooienejad", "Amir", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1804.01208", "submitter": "Jonathan Roth", "authors": "Jonathan Roth", "title": "Should We Adjust for the Test for Pre-trends in Difference-in-Difference\n  Designs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common practice in difference-in-difference (DiD) designs is to check for\nparallel trends prior to treatment assignment, yet typical estimation and\ninference does not account for the fact that this test has occurred. I analyze\nthe properties of the traditional DiD estimator conditional on having passed\n(i.e. not rejected) the test for parallel pre-trends. When the DiD design is\nvalid and the test for pre-trends confirms it, the typical DiD estimator is\nunbiased, but traditional standard errors are overly conservative.\nAdditionally, there exists an alternative unbiased estimator that is more\nefficient than the traditional DiD estimator under parallel trends. However,\nwhen in population there is a non-zero pre-trend but we fail to reject the\nhypothesis of parallel pre-trends, the DiD estimator is generally biased\nrelative to the population DiD coefficient. Moreover, if the trend is monotone,\nthen under reasonable assumptions the bias from conditioning exacerbates the\nbias relative to the true treatment effect. I propose new estimation and\ninference procedures that account for the test for parallel trends, and compare\ntheir performance to that of the traditional estimator in a Monte Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:54:37 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 00:01:57 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Roth", "Jonathan", ""]]}, {"id": "1804.01230", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec", "title": "The noise barrier and the large signal bias of the Lasso and other\n  convex estimators", "comments": "This paper supersedes the previous article arXiv:1703.01332", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex estimators such as the Lasso, the matrix Lasso and the group Lasso\nhave been studied extensively in the last two decades, demonstrating great\nsuccess in both theory and practice. Two quantities are introduced, the noise\nbarrier and the large scale bias, that provides insights on the performance of\nthese convex regularized estimators. It is now well understood that the Lasso\nachieves fast prediction rates, provided that the correlations of the design\nsatisfy some Restricted Eigenvalue or Compatibility condition, and provided\nthat the tuning parameter is large enough. Using the two quantities introduced\nin the paper, we show that the compatibility condition on the design matrix is\nactually unavoidable to achieve fast prediction rates with the Lasso. The Lasso\nmust incur a loss due to the correlations of the design matrix, measured in\nterms of the compatibility constant. This results holds for any design matrix,\nany active subset of covariates, and any tuning parameter. It is now well known\nthat the Lasso enjoys a dimension reduction property: the prediction error is\nof order $\\lambda\\sqrt k$ where $k$ is the sparsity; even if the ambient\ndimension $p$ is much larger than $k$. Such results require that the tuning\nparameters is greater than some universal threshold. We characterize sharp\nphase transitions for the tuning parameter of the Lasso around a critical\nthreshold dependent on $k$. If $\\lambda$ is equal or larger than this critical\nthreshold, the Lasso is minimax over $k$-sparse target vectors. If $\\lambda$ is\nequal or smaller than critical threshold, the Lasso incurs a loss of order\n$\\sigma\\sqrt k$ --which corresponds to a model of size $k$-- even if the target\nvector has fewer than $k$ nonzero coefficients. Remarkably, the lower bounds\nobtained in the paper also apply to random, data-driven tuning parameters. The\nresults extend to convex penalties beyond the Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 03:56:59 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 16:37:39 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 01:44:40 GMT"}, {"version": "v4", "created": "Sun, 28 Oct 2018 03:57:45 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Bellec", "Pierre C", ""]]}, {"id": "1804.01528", "submitter": "Mikael Escobar-Bach", "authors": "Mikael Escobar-Bach and Ingrid Van Keilegom", "title": "Non-parametric cure rate estimation under insufficient follow-up using\n  extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important research topic in survival analysis is related to the modeling\nand estimation of the cure rate, i.e. the proportion of subjects that will\nnever experience the event of interest. However, most estimation methods\nproposed so far in the literature do not handle the case of insufficient\nfollow-up, that is when the right end point of the support of the censoring\ntime is strictly less than that of the survival time of the susceptible\nsubjects, and consequently these estimators overestimate the cure rate in that\ncase. We fill this gap by proposing a new estimator of the cure rate that makes\nuse of extrapolation techniques from the area of extreme value theory. We\nestablish the asymptotic normality of the proposed estimator, and show how the\nestimator works for small samples by means of a simulation study. We also\nillustrate its practical applicability through the analysis of data on the\nsurvival of breast cancer patients.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:39:29 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Escobar-Bach", "Mikael", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1804.01619", "submitter": "Yuansi Chen", "authors": "Yuansi Chen, Chi Jin and Bin Yu", "title": "Stability and Convergence Trade-off of Iterative Optimization Algorithms", "comments": "45 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overall performance or expected excess risk of an iterative machine\nlearning algorithm can be decomposed into training error and generalization\nerror. While the former is controlled by its convergence analysis, the latter\ncan be tightly handled by algorithmic stability. The machine learning community\nhas a rich history investigating convergence and stability separately. However,\nthe question about the trade-off between these two quantities remains open. In\nthis paper, we show that for any iterative algorithm at any iteration, the\noverall performance is lower bounded by the minimax statistical error over an\nappropriately chosen loss function class. This implies an important trade-off\nbetween convergence and stability of the algorithm -- a faster converging\nalgorithm has to be less stable, and vice versa. As a direct consequence of\nthis fundamental tradeoff, new convergence lower bounds can be derived for\nclasses of algorithms constrained with different stability bounds. In\nparticular, when the loss function is convex (or strongly convex) and smooth,\nwe discuss the stability upper bounds of gradient descent (GD) and stochastic\ngradient descent and their variants with decreasing step sizes. For Nesterov's\naccelerated gradient descent (NAG) and heavy ball method (HB), we provide\nstability upper bounds for the quadratic loss function. Applying existing\nstability upper bounds for the gradient methods in our trade-off framework, we\nobtain lower bounds matching the well-established convergence upper bounds up\nto constants for these algorithms and conjecture similar lower bounds for NAG\nand HB. Finally, we numerically demonstrate the tightness of our stability\nbounds in terms of exponents in the rate and also illustrate via a simulated\nlogistic regression problem that our stability bounds reflect the\ngeneralization errors better than the simple uniform convergence bounds for GD\nand NAG.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:23:40 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Chen", "Yuansi", ""], ["Jin", "Chi", ""], ["Yu", "Bin", ""]]}, {"id": "1804.01631", "submitter": "Sami Stouli", "authors": "Richard Spady, Sami Stouli", "title": "Simultaneous Mean-Variance Regression", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose simultaneous mean-variance regression for the linear estimation\nand approximation of conditional mean functions. In the presence of\nheteroskedasticity of unknown form, our method accounts for varying dispersion\nin the regression outcome across the support of conditioning variables by using\nweights that are jointly determined with the mean regression parameters.\nSimultaneity generates outcome predictions that are guaranteed to improve over\nordinary least-squares prediction error, with corresponding parameter standard\nerrors that are automatically valid. Under shape misspecification of the\nconditional mean and variance functions, we establish existence and uniqueness\nof the resulting approximations and characterize their formal interpretation\nand robustness properties. In particular, we show that the corresponding\nmean-variance regression location-scale model weakly dominates the ordinary\nleast-squares location model under a Kullback-Leibler measure of divergence,\nwith strict improvement in the presence of heteroskedasticity. The simultaneous\nmean-variance regression loss function is globally convex and the corresponding\nestimator is easy to implement. We establish its consistency and asymptotic\nnormality under misspecification, provide robust inference methods, and present\nnumerical simulations that show large improvements over ordinary and weighted\nleast-squares in terms of estimation and inference in finite samples. We\nfurther illustrate our method with two empirical applications to the estimation\nof the relationship between economic prosperity in 1500 and today, and demand\nfor gasoline in the United States.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 00:23:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 23:36:59 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Spady", "Richard", ""], ["Stouli", "Sami", ""]]}, {"id": "1804.01647", "submitter": "Sharif Rahman", "authors": "Sharif Rahman", "title": "Mathematical Properties of Polynomial Dimensional Decomposition", "comments": "28 pages, two figures, one table; accepted by SIAM/ASA Journal on\n  Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high-dimensional uncertainty quantification problems are solved by\npolynomial dimensional decomposition (PDD), which represents Fourier-like\nseries expansion in terms of random orthonormal polynomials with increasing\ndimensions. This study constructs dimension-wise and orthogonal splitting of\npolynomial spaces, proves completeness of polynomial orthogonal basis for\nprescribed assumptions, and demonstrates mean-square convergence to the correct\nlimit -- all associated with PDD. A second-moment error analysis reveals that\nPDD cannot commit larger error than polynomial chaos expansion (PCE) for the\nappropriately chosen truncation parameters. From the comparison of\ncomputational efforts, required to estimate with the same precision the\nvariance of an output function involving exponentially attenuating expansion\ncoefficients, the PDD approximation can be markedly more efficient than the PCE\napproximation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 01:41:26 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Rahman", "Sharif", ""]]}, {"id": "1804.01811", "submitter": "Jere Koskela", "authors": "Jere Koskela, Paul A. Jenkins, Adam M. Johansen, Dario Spano", "title": "Asymptotic genealogies of interacting particle systems with an\n  application to sequential Monte Carlo", "comments": "28 pages, 1 figure. An earlier version of this manuscript contained\n  an error, which we have been able to correct and in so doing give a stronger\n  result under cleaner conditions. v7: Added several technical lemmas which\n  make the overall argument more explicit", "journal-ref": "Annals of Statistics 48(1):560-583, 2020", "doi": "10.1214/19-AOS1823", "report-no": null, "categories": "math.ST q-bio.PE stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study weighted particle systems in which new generations are resampled\nfrom current particles with probabilities proportional to their weights. This\ncovers a broad class of sequential Monte Carlo (SMC) methods, widely-used in\napplied statistics and cognate disciplines. We consider the genealogical tree\nembedded into such particle systems, and identify conditions, as well as an\nappropriate time-scaling, under which they converge to the Kingman n-coalescent\nin the infinite system size limit in the sense of finite-dimensional\ndistributions. Thus, the tractable n-coalescent can be used to predict the\nshape and size of SMC genealogies, as we illustrate by characterising the\nlimiting mean and variance of the tree height. SMC genealogies are known to be\nconnected to algorithm performance, so that our results are likely to have\napplications in the design of new methods as well. Our conditions for\nconvergence are strong, but we show by simulation that they do not appear to be\nnecessary.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 12:43:21 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 13:29:16 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 16:41:04 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 19:48:44 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 09:42:48 GMT"}, {"version": "v6", "created": "Mon, 12 Aug 2019 10:39:56 GMT"}, {"version": "v7", "created": "Fri, 16 Jul 2021 19:44:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Koskela", "Jere", ""], ["Jenkins", "Paul A.", ""], ["Johansen", "Adam M.", ""], ["Spano", "Dario", ""]]}, {"id": "1804.01858", "submitter": "Alejandro  Cholaquidis", "authors": "Catherine Aaron, Alejandro Cholaquidis, Ricardo Fraiman, Badih Ghattas", "title": "Robust Fusion Methods for Structured Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address one of the important problems in Big Data, namely how to combine\nestimators from different subsamples by robust fusion procedures, when we are\nunable to deal with the whole sample. We propose a general framework based on\nthe classic idea of `divide and conquer'. In particular we address in some\ndetail the case of a multivariate location and scatter matrix, the covariance\noperator for functional data, and clustering problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 14:02:54 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Aaron", "Catherine", ""], ["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""]]}, {"id": "1804.01864", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Adaptive test for ergodic diffusions plus noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some parametric tests for ergodic diffusion-plus-noise model,\nwhich is a version of state-space modelling in statistics for stochastic\ndiffusion equations. The test statistics are classified into three types:\nlikelihood-ratio-type test statistic; Wald-type one; and Rao-type one. All the\ntest statistics are constructed with quasi-likelihood-functions for local mean\nsequence of noised observation. We also simulate the behaviour of them for\nseveral practical hypothesis tests and check the convergence in law of test\nstatistics under null hypotheses and consistency of the test under alternative\nones. We apply the method for real data analysis of wind data, and examine some\nsets of the hypotheses mainly with respect to the structure of diffusion\ncoefficient.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 14:10:03 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1804.02218", "submitter": "Matthias Neumann", "authors": "Matthias Neumann, Christian Hirsch, Jakub Stan\\v{e}k, Viktor\n  Bene\\v{s}, Volker Schmidt", "title": "Estimation of geodesic tortuosity and constrictivity in stationary\n  random closed sets", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating geodesic tortuosity and\nconstrictivity as two structural characteristics of stationary random closed\nsets. They are of central importance for the analysis of effective transport\nproperties in porous or composite materials. Loosely speaking, geodesic\ntortuosity measures the windedness of paths whereas the notion of\nconstrictivity captures the appearance of bottlenecks resulting from narrow\npassages within a given materials phase. We first provide mathematically\nprecise definitions of these quantities and introduce appropriate estimators.\nThen, we show strong consistency of these estimators for unboundedly growing\nsampling windows. In order to apply our estimators to real datasets, the extent\nof edge effects needs to be controlled. This is illustrated using a model for a\nmulti-phase material that is incorporated in solid oxid fuel cells.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 12:11:48 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 08:34:58 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 13:01:00 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 10:27:58 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Neumann", "Matthias", ""], ["Hirsch", "Christian", ""], ["Stan\u011bk", "Jakub", ""], ["Bene\u0161", "Viktor", ""], ["Schmidt", "Volker", ""]]}, {"id": "1804.02254", "submitter": "Dirk-Philip Brandes", "authors": "Dirk-Philip Brandes and Imma Valentina Curato", "title": "On the sample autocovariance of a L\\'evy driven moving average process\n  when sampled at a renewal sequence", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a L\\'evy driven continuous time moving average process $X$\nsampled at random times which follow a renewal structure independent of $X$.\nAsymptotic normality of the sample mean, the sample autocovariance, and the\nsample autocorrelation is established under certain conditions on the kernel\nand the random times. We compare our results to a classical non-random\nequidistant sampling method and give an application to parameter estimation of\nthe L\\'evy driven Ornstein-Uhlenbeck process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:28:57 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Brandes", "Dirk-Philip", ""], ["Curato", "Imma Valentina", ""]]}, {"id": "1804.02482", "submitter": "Chenglong Ye", "authors": "Chenglong Ye and Yuhong Yang", "title": "High-dimensional Adaptive Minimax Sparse Estimation with Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear regression with interaction effects is broadly\napplied in research fields such as bioinformatics and social science. In this\npaper, we first investigate the minimax rate of convergence for regression\nestimation in high-dimensional sparse linear models with two-way interactions.\nWe derive matching upper and lower bounds under three types of heredity\nconditions: strong heredity, weak heredity and no heredity. From the results:\n(i) A stronger heredity condition may or may not drastically improve the\nminimax rate of convergence. In fact, in some situations, the minimax rates of\nconvergence are the same under all three heredity conditions; (ii) The minimax\nrate of convergence is determined by the maximum of the total price of\nestimating the main effects and that of estimating the interaction effects,\nwhich goes beyond purely comparing the order of the number of non-zero main\neffects $r_1$ and non-zero interaction effects $r_2$; (iii) Under any of the\nthree heredity conditions, the estimation of the interaction terms may be the\ndominant part in determining the rate of convergence for two different reasons:\n1) there exist more interaction terms than main effect terms or 2) a large\nambient dimension makes it more challenging to estimate even a small number of\ninteraction terms. Second, we construct an adaptive estimator that achieves the\nminimax rate of convergence regardless of the true heredity condition and the\nsparsity indices $r_1, r_2$.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 23:14:14 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ye", "Chenglong", ""], ["Yang", "Yuhong", ""]]}, {"id": "1804.02605", "submitter": "Abhishek Chakrabortty", "authors": "Arun Kumar Kuchibhotla and Abhishek Chakrabortty", "title": "Moving Beyond Sub-Gaussianity in High-Dimensional Statistics:\n  Applications in Covariance Estimation and Linear Regression", "comments": "64 pages; Revised version (discussions added and some results\n  modified in Section 4, minor changes made throughout)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concentration inequalities form an essential toolkit in the study of high\ndimensional (HD) statistical methods. Most of the relevant statistics\nliterature in this regard is based on sub-Gaussian or sub-exponential tail\nassumptions. In this paper, we first bring together various probabilistic\ninequalities for sums of independent random variables under much weaker\nexponential type (namely sub-Weibull) tail assumptions. These results extract a\npart sub-Gaussian tail behavior in finite samples, matching the asymptotics\ngoverned by the central limit theorem, and are compactly represented in terms\nof a new Orlicz quasi-norm - the Generalized Bernstein-Orlicz norm - that\ntypifies such tail behaviors.\n  We illustrate the usefulness of these inequalities through the analysis of\nfour fundamental problems in HD statistics. In the first two problems, we study\nthe rate of convergence of the sample covariance matrix in terms of the maximum\nelementwise norm and the maximum k-sub-matrix operator norm which are key\nquantities of interest in bootstrap, HD covariance matrix estimation and HD\ninference. The third example concerns the restricted eigenvalue condition,\nrequired in HD linear regression, which we verify for all sub-Weibull random\nvectors through a unified analysis, and also prove a more general result\nrelated to restricted strong convexity in the process. In the final example, we\nconsider the Lasso estimator for linear regression and establish its rate of\nconvergence under much weaker than usual tail assumptions (on the errors as\nwell as the covariates), while also allowing for misspecified models and both\nfixed and random design. To our knowledge, these are the first such results for\nLasso obtained in this generality. The common feature in all our results over\nall the examples is that the convergence rates under most exponential tails\nmatch the usual ones under sub-Gaussian assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 00:27:45 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 01:40:10 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 20:56:42 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Chakrabortty", "Abhishek", ""]]}, {"id": "1804.02686", "submitter": "Valentina Ros", "authors": "Valentina Ros, Gerard Ben Arous, Giulio Biroli, Chiara Cammarota", "title": "Complex energy landscapes in spiked-tensor and simple glassy models:\n  ruggedness, arrangements of local minima and phase transitions", "comments": "v2 with references added, typos corrected", "journal-ref": "Phys. Rev. X 9, 011003 (2019)", "doi": "10.1103/PhysRevX.9.011003", "report-no": null, "categories": "cond-mat.dis-nn math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study rough high-dimensional landscapes in which an increasingly stronger\npreference for a given configuration emerges. Such energy landscapes arise in\nglass physics and inference. In particular we focus on random Gaussian\nfunctions, and on the spiked-tensor model and generalizations. We thoroughly\nanalyze the statistical properties of the corresponding landscapes and\ncharacterize the associated geometrical phase transitions. In order to perform\nour study, we develop a framework based on the Kac-Rice method that allows to\ncompute the complexity of the landscape, i.e. the logarithm of the typical\nnumber of stationary points and their Hessian. This approach generalizes the\none used to compute rigorously the annealed complexity of mean-field glass\nmodels. We discuss its advantages with respect to previous frameworks, in\nparticular the thermodynamical replica method which is shown to lead to\npartially incorrect predictions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 13:03:54 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 13:45:03 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ros", "Valentina", ""], ["Arous", "Gerard Ben", ""], ["Biroli", "Giulio", ""], ["Cammarota", "Chiara", ""]]}, {"id": "1804.02689", "submitter": "Alexander Jurisch", "authors": "Alexander Jurisch", "title": "An extremal fractional Gaussian with a possible application to\n  option-pricing with skew and smile", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an extremal fractional Gaussian by employing the L\\'evy-Khintchine\ntheorem and L\\'evian noise. With the fractional Gaussian we then generalize the\nBlack-Scholes-Merton option-pricing formula. We obtain an easily applicable and\nexponentially convergent option-pricing formula for fractional markets. We also\ncarry out an analysis of the structure of the implied volatility in this\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 13:30:52 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 10:22:43 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jurisch", "Alexander", ""]]}, {"id": "1804.02757", "submitter": "Mikhail Zhitlukhin", "authors": "Alexey Muravlev, Mikhail Zhitlukhin", "title": "A Bayesian sequential test for the drift of a fractional Brownian motion", "comments": "13 pages", "journal-ref": "Adv. Appl. Probab. 52 (2020) 1308-1324", "doi": "10.1017/apr.2020.43", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fractional Brownian motion with unknown linear drift such that\nthe drift coefficient has a prior normal distribution and construct a\nsequential test for the hypothesis that the drift is positive versus the\nalternative that it is negative. We show that the problem of constructing the\ntest reduces to an optimal stopping problem for a standard Brownian motion,\nobtained by a transformation of the fractional one. The solution is described\nas the first exit time from some set, whose boundaries are shown to satisfy a\ncertain integral equation, which is solved numerically.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 21:08:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Muravlev", "Alexey", ""], ["Zhitlukhin", "Mikhail", ""]]}, {"id": "1804.02811", "submitter": "Nan Wu", "authors": "John Malik, Chao Shen, Hau-Tieng Wu, Nan Wu", "title": "Connecting Dots -- from Local Covariance to Empirical Intrinsic Geometry\n  and Locally Linear Embedding", "comments": "25pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local covariance structure under the manifold setup has been widely applied\nin the machine learning society. Based on the established theoretical results,\nwe provide an extensive study of two relevant manifold learning algorithms,\nempirical intrinsic geometry (EIG) and the locally linear embedding (LLE) under\nthe manifold setup. Particularly, we show that without an accurate dimension\nestimation, the geodesic distance estimation by EIG might be corrupted.\nFurthermore, we show that by taking the local covariance matrix into account,\nwe can more accurately estimate the local geodesic distance. When understanding\nLLE based on the local covariance structure, its intimate relationship with the\ncurvature suggests a variation of LLE depending on the \"truncation scheme\". We\nprovide a theoretical analysis of the variation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:26:44 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 20:20:27 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Malik", "John", ""], ["Shen", "Chao", ""], ["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "1804.03029", "submitter": "Hisayuki Tsukuma", "authors": "Hisayuki Tsukuma", "title": "Estimation in a simple linear regression model with measurement error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of estimating a slope parameter in a simple\nlinear regression model, where independent variables have functional\nmeasurement errors. Measurement errors in independent variables, as is well\nknown, cause biasedness of the ordinary least squares estimator. A general\nprocedure for the bias reduction is presented in a finite sample situation, and\nsome exact bias-reduced estimators are proposed. Also, it is shown that certain\ntruncation procedures improve the mean square errors of the ordinary least\nsquares and the bias-reduced estimators.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:44:48 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tsukuma", "Hisayuki", ""]]}, {"id": "1804.03062", "submitter": "Marco Doretti", "authors": "Elena Stanghellini, Marco Doretti", "title": "On marginal and conditional parameters in logistic regression models", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": "10.1093/biomet/asz019", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental research question is how much a variation in a covariate\ninfluences a binary response variable in a logistic regression model, both\ndirectly or through mediators. We derive the exact formula linking the\nparameters of marginal and conditional regression models with binary mediators\nwhen no conditional independence assumptions can be made. The formula has the\nappealing property of being the sum of terms that vanish whenever parameters of\nthe conditional models vanish, thereby recovering well-known results as\nparticular cases. It also permits to quantify the distortion induced by\nomission of some relevant covariates, opening the way to sensitivity analysis.\nAlso in this case, as the parameters of the conditional models are multiplied\nby terms that are always positive or bounded, the formula may be used to\nconstruct reasonable bounds on the parameters of interest. We assume that,\nconditionally on a set of covariates, the data-generating process can be\nrepresented by a Directed Acyclic Graph. We also show how the results here\npresented lead to the extension of path analysis to a system of binary random\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 15:43:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Stanghellini", "Elena", ""], ["Doretti", "Marco", ""]]}, {"id": "1804.03105", "submitter": "Alex Chin", "authors": "Alex Chin", "title": "Central limit theorems via Stein's method for randomized experiments\n  under interference", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study conditions under which treatment effect estimators constructed under\nthe no-interference assumption in randomized experiments are asymptotically\nnormal in the presence of interference. We prove that the standard\nHorvitz-Thompson estimator is asymptotically normal under a restricted\ninterference condition characterized by limiting the degree of the dependency\ngraph. The amount of interference is allowed to grow with the population size.\nWe then provide a central limit theorem for the difference-in-means estimator\nthat can handle interference that exists between all pairs of units, provided\nmost of the interference is captured by a restricted-degree dependency graph.\nThe asymptotic variance admits a decomposition into two terms: (a) the variance\nthat is expected under no-interference and (b) the additional variance\ncontributed by interference. We propose a conservative variance estimator based\non this variance decomposition. The results arise as an application of Stein's\nmethod. For practitioners, our results show that standard estimators continue\nto exhibit normality in large sample sizes and that inference can be made\nrobust to mild forms of interference.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:11:37 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 23:40:51 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Chin", "Alex", ""]]}, {"id": "1804.03206", "submitter": "Dominik Janzing", "authors": "Dominik Janzing", "title": "Merging joint distributions via causal model classes with low VC\n  dimension", "comments": "21 pages, two errors in V1 corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If $X,Y,Z$ denote sets of random variables, two different data sources may\ncontain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that\ncausal inference can help inferring properties of the 'unobserved joint\ndistributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional\nindependences or also quantitative statements about dependences.\n  More generally, we define a learning scenario where the input is a subset of\nvariables and the label is some statistical property of that subset. Sets of\njointly observed variables define the training points, while unobserved sets\nare possible test points. To solve this learning task, we infer, as an\nintermediate step, a causal model from the observations that then entails\nproperties of unobserved sets. Accordingly, we can define the VC dimension of a\nclass of causal models and derive generalization bounds for the predictions.\n  Here, causal inference becomes more modest and better accessible to empirical\ntests than usual: rather than trying to find a causal hypothesis that is 'true'\n(which is a problematic term when it is unclear how to define interventions) a\ncausal hypothesis is useful whenever it correctly predicts statistical\nproperties of unobserved joint distributions.\n  Within such a 'pragmatic' application of causal inference, some popular\nheuristic approaches become justified in retrospect. It is, for instance,\nallowed to infer DAGs from partial correlations instead of conditional\nindependences if the DAGs are only used to predict partial correlations.\n  I hypothesize that our pragmatic view on causality may even cover the usual\nmeaning in terms of interventions and sketch why predicting the impact of\ninterventions can sometimes also be phrased as a task of the above type.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:54:48 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 08:41:14 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Janzing", "Dominik", ""]]}, {"id": "1804.03366", "submitter": "Efstathios Paparoditis", "authors": "Anne Leucht, Efstathios Paparoditis and Theofanis Sapatinas", "title": "Testing equality of spectral density operators for functional linear\n  processes", "comments": "superseded by arXiv:2004.03412", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing equality of the entire second order structure of two\nindependent functional linear processes is considered. A fully functional\n$L^2$-type test is developed which evaluates, over all frequencies, the\nHilbert-Schmidt distance between the estimated spectral density operators of\nthe two processes. The asymptotic behavior of the test statistic is\ninvestigated and its limiting distribution under the null hypothesis is\nderived. Furthermore, a novel frequency domain bootstrap method is developed\nwhich approximates more accurately the distribution of the test statistic under\nthe null than the large sample Gaussian approximation obtained. Asymptotic\nvalidity of the bootstrap procedure is established and consistency of the\nbootstrap-based test under the alternative is proved. Numerical simulations\nshow that, even for small samples, the bootstrap-based test has very good size\nand power behavior. An application to meteorological functional time series is\nalso presented.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 06:56:03 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 07:04:06 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Leucht", "Anne", ""], ["Paparoditis", "Efstathios", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "1804.03378", "submitter": "Andr\\'es Felipe L\\'opez-Lopera", "authors": "Fran\\c{c}ois Bachoc, Agn\\`es Lagnoux, and Andr\\'es F. L\\'opez-Lopera", "title": "Maximum likelihood estimation for Gaussian processes under inequality\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider covariance parameter estimation for a Gaussian process under\ninequality constraints (boundedness, monotonicity or convexity) in fixed-domain\nasymptotics. We address the estimation of the variance parameter and the\nestimation of the microergodic parameter of the Mat\\'ern and Wendland\ncovariance functions. First, we show that the (unconstrained) maximum\nlikelihood estimator has the same asymptotic distribution, unconditionally and\nconditionally to the fact that the Gaussian process satisfies the inequality\nconstraints. Then, we study the recently suggested constrained maximum\nlikelihood estimator. We show that it has the same asymptotic distribution as\nthe (unconstrained) maximum likelihood estimator. In addition, we show in\nsimulations that the constrained maximum likelihood estimator is generally more\naccurate on finite samples. Finally, we provide extensions to prediction and to\nnoisy observations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 07:15:32 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 09:27:00 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Lagnoux", "Agn\u00e8s", ""], ["L\u00f3pez-Lopera", "Andr\u00e9s F.", ""]]}, {"id": "1804.03452", "submitter": "Enrico Gavagnin", "authors": "Enrico Gavagnin, Jennifer P. Owen and Christian A. Yates", "title": "Pair correlation functions for identifying spatial correlation in\n  discrete domains", "comments": null, "journal-ref": "Phys. Rev. E vol. 97, Issue 6 , pages 062104 2018", "doi": "10.1103/PhysRevE.97.062104", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and quantifying spatial correlation are important aspects of\nstudying the collective behaviour of multi-agent systems. Pair correlation\nfunctions (PCFs) are powerful statistical tools which can provide qualitative\nand quantitative information about correlation between pairs of agents. Despite\nthe numerous PCFs defined for off-lattice domains, only a few recent studies\nhave considered a PCF for discrete domains. Our work extends the study of\nspatial correlation in discrete domains by defining a new set of PCFs using two\nnatural and intuitive definitions of distance for a square lattice: the taxicab\nand uniform metric. We show how these PCFs improve upon previous attempts and\ncompare between the quantitative data acquired. We also extend our definitions\nof the PCF to other types of regular tessellation which have not been studied\nbefore, including hexagonal, triangular and cuboidal. Finally, we provide a\ncomprehensive PCF for any tessellation and metric allowing investigation of\nspatial correlation in irregular lattices for which recognising correlation is\nless intuitive.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 11:10:23 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 10:39:36 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gavagnin", "Enrico", ""], ["Owen", "Jennifer P.", ""], ["Yates", "Christian A.", ""]]}, {"id": "1804.03509", "submitter": "Florencia Leonardi", "authors": "Andressa Cerqueira and Florencia Leonardi", "title": "Strong consistency of Krichevsky-Trofimov estimator for the number of\n  communities in the Stochastic Block Model", "comments": null, "journal-ref": "IEEE Transactions on Information Theory, 66(10), 6403-6412 (2020)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an estimator for the number of communities in the\nStochastic Block Model (SBM), based on the maximization of a penalized version\nof the so-called Krichevsky-Trofimov mixture distribution. We prove its\neventual almost sure convergence to the underlying number of communities,\nwithout assuming a known upper bound on that quantity. Our results apply to\nboth the dense and the sparse regimes. To our knowledge this is the first\nconsistency result for the estimation of the number of communities in the SBM\nin the unbounded case, that is when the number of communities is allowed to\ngrow with the same size.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 13:23:44 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 19:35:02 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cerqueira", "Andressa", ""], ["Leonardi", "Florencia", ""]]}, {"id": "1804.03601", "submitter": "Wanli Qiao", "authors": "Wanli Qiao", "title": "Nonparametric Estimation of Surface Integrals on Level Sets", "comments": "50 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface integrals on density level sets often appear in asymptotic results in\nnonparametric level set estimation (such as for confidence regions and\nbandwidth selection). Also surface integrals can be used to describe the shape\nof level sets (using such as Willmore energy and Minkowski functionals), and\nlink geometry (curvature) and topology (Euler characteristic) of level sets\nthrough the Gauss-Bonnet theorem. We consider three estimators of surface\nintegrals on density level sets, one as a direct plug-in estimator, and the\nother two based on different neighborhoods of level sets. We allow the\nintegrands of the surface integrals to be known or unknown. For both of these\nscenarios, we derive the rates of the convergence and asymptotic normality of\nthe three estimators.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:50:36 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 19:45:48 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Qiao", "Wanli", ""]]}, {"id": "1804.03636", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and John Peebles", "title": "Testing Identity of Multidimensional Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of identity testing for multidimensional histogram\ndistributions. A distribution $p: D \\rightarrow \\mathbb{R}_+$, where $D\n\\subseteq \\mathbb{R}^d$, is called a $k$-histogram if there exists a partition\nof the domain into $k$ axis-aligned rectangles such that $p$ is constant within\neach such rectangle. Histograms are one of the most fundamental nonparametric\nfamilies of distributions and have been extensively studied in computer science\nand statistics. We give the first identity tester for this problem with {\\em\nsub-learning} sample complexity in any fixed dimension and a nearly-matching\nsample complexity lower bound.\n  In more detail, let $q$ be an unknown $d$-dimensional $k$-histogram\ndistribution in fixed dimension $d$, and $p$ be an explicitly given\n$d$-dimensional $k$-histogram. We want to correctly distinguish, with\nprobability at least $2/3$, between the case that $p = q$ versus $\\|p-q\\|_1\n\\geq \\epsilon$. We design an algorithm for this hypothesis testing problem with\nsample complexity $O((\\sqrt{k}/\\epsilon^2) 2^{d/2} \\log^{2.5 d}(k/\\epsilon))$\nthat runs in sample-polynomial time. Our algorithm is robust to model\nmisspecification, i.e., succeeds even if $q$ is only promised to be {\\em close}\nto a $k$-histogram. Moreover, for $k = 2^{\\Omega(d)}$, we show a sample\ncomplexity lower bound of $(\\sqrt{k}/\\epsilon^2) \\cdot \\Omega(\\log(k)/d)^{d-1}$\nwhen $d\\geq 2$. That is, for any fixed dimension $d$, our upper and lower\nbounds are nearly matching. Prior to our work, the sample complexity of the\n$d=1$ case was well-understood, but no algorithm with sub-learning sample\ncomplexity was known, even for $d=2$. Our new upper and lower bounds have\ninteresting conceptual implications regarding the relation between learning and\ntesting in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 17:28:47 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 02:42:51 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Peebles", "John", ""]]}, {"id": "1804.03911", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Paul Rubenstein, Bernhard Sch\\\"olkopf", "title": "Structural causal models for macro-variables in time-series", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a bivariate time series $(X_t,Y_t)$ that is given by a simple\nlinear autoregressive model. Assuming that the equations describing each\nvariable as a linear combination of past values are considered structural\nequations, there is a clear meaning of how intervening on one particular $X_t$\ninfluences $Y_{t'}$ at later times $t'>t$. In the present work, we describe\nconditions under which one can define a causal model between variables that are\ncoarse-grained in time, thus admitting statements like `setting $X$ to $x$\nchanges $Y$ in a certain way' without referring to specific time instances. We\nshow that particularly simple statements follow in the frequency domain, thus\nproviding meaning to interventions on frequencies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 10:21:22 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Janzing", "Dominik", ""], ["Rubenstein", "Paul", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1804.03926", "submitter": "Adrien Saumard", "authors": "Adrien Saumard", "title": "Weighted Poincar\\'e inequalities, concentration inequalities and tail\n  bounds related to the Stein kernels in dimension one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate links between the so-called Stein's density approach in\ndimension one and some functional and concentration inequalities. We show that\nmeasures having a finite first moment and a density with connected support\nsatisfy a weighted Poincar\\'e inequality with the weight being the Stein\nkernel, that indeed exists and is unique in this case. Furthermore, we prove\nweighted log-Sobolev and asymmetric Brascamp-Lieb type inequalities related to\nStein kernels. We also show that existence of a uniformly bounded Stein kernel\nis sufficient to ensure a positive Cheeger isoperimetric constant. Then we\nderive new concentration inequalities. In particular, we prove generalized\nMills' type inequalities when a Stein kernel is uniformly bounded and sub-gamma\nconcentration for Lipschitz functions of a variable with a sub-linear Stein\nkernel. When some exponential moments are finite, a general concentration\ninequality is then expressed in terms of Legendre-Fenchel transform of the\nLaplace transform of the Stein kernel. Along the way, we prove a general lemma\nfor bounding the Laplace transform of a random variable, that should be useful\nin many other contexts when deriving concentration inequalities. Finally, we\nprovide density and tail formulas as well as tail bounds, generalizing previous\nresults that where obtained in the context of Malliavin calculus.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:11:30 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 13:49:29 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 09:21:52 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Saumard", "Adrien", ""]]}, {"id": "1804.03989", "submitter": "Kenric Nelson Ph.D.", "authors": "Kenric P. Nelson, Mark A. Kon and Sabir R. Umarov", "title": "Use of the geometric mean as a statistic for the scale of the coupled\n  Gaussian distributions", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.physa.2018.09.049", "report-no": null, "categories": "stat.ME cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric mean is shown to be an appropriate statistic for the scale of a\nheavy-tailed coupled Gaussian distribution or equivalently the Student's t\ndistribution. The coupled Gaussian is a member of a family of distributions\nparameterized by the nonlinear statistical coupling which is the reciprocal of\nthe degree of freedom and is proportional to fluctuations in the inverse scale\nof the Gaussian. Existing estimators of the scale of the coupled Gaussian have\nrelied on estimates of the full distribution, and they suffer from problems\nrelated to outliers in heavy-tailed distributions. In this paper, the scale of\na coupled Gaussian is proven to be equal to the product of the generalized mean\nand the square root of the coupling. From our numerical computations of the\nscales of coupled Gaussians using the generalized mean of random samples, it is\nindicated that only samples from a Cauchy distribution (with coupling parameter\none) form an unbiased estimate with diminishing variance for large samples.\nNevertheless, we also prove that the scale is a function of the geometric mean,\nthe coupling term and a harmonic number. Numerical experiments show that this\nestimator is unbiased with diminishing variance for large samples for a broad\nrange of coupling values.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 13:31:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 13:06:34 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Nelson", "Kenric P.", ""], ["Kon", "Mark A.", ""], ["Umarov", "Sabir R.", ""]]}, {"id": "1804.04034", "submitter": "Daniel Rudolf", "authors": "Manuel Diehn, Axel Munk, Daniel Rudolf", "title": "Maximum likelihood estimation in hidden Markov models with inhomogeneous\n  noise", "comments": "31 pages, 6 figures, Accepted for publication in ESAIM Probab. Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation in finite hidden state space Markov models\nwith time-dependent inhomogeneous noise, where the inhomogeneity vanishes\nsufficiently fast. Based on the concept of asymptotic mean stationary processes\nwe prove that the maximum likelihood and a quasi-maximum likelihood estimator\n(QMLE) are strongly consistent. The computation of the QMLE ignores the\ninhomogeneity, hence, is much simpler and robust. The theory is motivated by an\nexample from biophysics and applied to a Poisson- and linear Gaussian model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:03:03 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 20:13:00 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Diehn", "Manuel", ""], ["Munk", "Axel", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1804.04108", "submitter": "Kohei Chiba", "authors": "Kohei Chiba", "title": "LAN property for stochastic differential equations driven by fractional\n  Brownian motion of Hurst parameter $H\\in(1/4,1/2)$", "comments": "v1: 48 pages, comments are welcome. v2: 48 pages, (1) some proofs are\n  simplified, (2) some mistakes are corrected, and (3) a sketch of the proof\n  and the calculation of the Fisher information in fOU case are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the drift parameter of\nsolution to the stochastic differential equation driven by a fractional\nBrownian motion with Hurst parameter less than $1/2$ under complete\nobservation. We derive a formula for the likelihood ratio and prove local\nasymptotic normality when $H \\in (1/4,1/2)$. Our result shows that the\nconvergence rate is $T^{-1/2}$ for the parameters satisfying a certain equation\nand $T^{-(1-H)}$ for the others.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:28:25 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 19:02:41 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Chiba", "Kohei", ""]]}, {"id": "1804.04210", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente, Daniela Rodriguez and Mariela Sued", "title": "The spatial sign covariance operator: Asymptotic results and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing recording capability, functional data analysis has\nbecome an important research topic. For functional data the study of outlier\ndetection and/or the development of robust statistical procedures has started\nrecently. One robust alternative to the sample covariance operator is the\nsample spatial sign covariance operator. In this paper, we study the asymptotic\nbehaviour of the sample spatial sign covariance operator when location is\nunknown. Among other possible applications of the obtained results, we derive\nthe asymptotic distribution of the principal directions obtained from the\nsample spatial sign covariance operator and we develop test to detect\ndifferences between the scatter operators of two populations. In particular,\nthe test performance is illustrated through a Monte Carlo study for small\nsample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:35:05 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Boente", "Graciela", ""], ["Rodriguez", "Daniela", ""], ["Sued", "Mariela", ""]]}, {"id": "1804.04699", "submitter": "Max Fathi", "authors": "Max Fathi", "title": "Stein kernels and moment maps", "comments": "15 pages, comments are welcome. v2: improved dependence on the\n  dimension in the quantitative CLT", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AP math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a construction of Stein kernels using moment maps, which are\nsolutions to a variant of the Monge-Amp\\`ere equation. As a consequence, we\nshow how regularity bounds on these maps control the rate of convergence in the\nclassical central limit theorem, and derive new rates in\nKantorovitch-Wasserstein distance in the log-concave situation, with explicit\npolynomial dependence on the dimension.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 19:24:30 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 15:59:24 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Fathi", "Max", ""]]}, {"id": "1804.04846", "submitter": "Martin Genzel", "authors": "Martin Genzel and Alexander Stollenwerk", "title": "Robust 1-Bit Compressed Sensing via Hinge Loss Minimization", "comments": null, "journal-ref": "Inf. Inference 9.2 (2020), 361-422", "doi": "10.1093/imaiai/iaz010", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work theoretically studies the problem of estimating a structured\nhigh-dimensional signal $x_0 \\in \\mathbb{R}^n$ from noisy $1$-bit Gaussian\nmeasurements. Our recovery approach is based on a simple convex program which\nuses the hinge loss function as data fidelity term. While such a risk\nminimization strategy is very natural to learn binary output models, such as in\nclassification, its capacity to estimate a specific signal vector is largely\nunexplored. A major difficulty is that the hinge loss is just piecewise linear,\nso that its \"curvature energy\" is concentrated in a single point. This is\nsubstantially different from other popular loss functions considered in signal\nestimation, e.g., the square or logistic loss, which are at least locally\nstrongly convex. It is therefore somewhat unexpected that we can still prove\nvery similar types of recovery guarantees for the hinge loss estimator, even in\nthe presence of strong noise. More specifically, our non-asymptotic error\nbounds show that stable and robust reconstruction of $x_0$ can be achieved with\nthe optimal oversampling rate $O(m^{-1/2})$ in terms of the number of\nmeasurements $m$. Moreover, we permit a wide class of structural assumptions on\nthe ground truth signal, in the sense that $x_0$ can belong to an arbitrary\nbounded convex set $K \\subset \\mathbb{R}^n$. The proofs of our main results\nrely on some recent advances in statistical learning theory due to Mendelson.\nIn particular, we invoke an adapted version of Mendelson's small ball method\nthat allows us to establish a quadratic lower bound on the error of the first\norder Taylor approximation of the empirical hinge loss function.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:06:26 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 10:07:01 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 14:28:32 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Genzel", "Martin", ""], ["Stollenwerk", "Alexander", ""]]}, {"id": "1804.04851", "submitter": "Antoine Chevreuil", "authors": "Antoine Chevreuil and Philippe Loubaton", "title": "On the detection of low rank matrices in the high-dimensional regime", "comments": "7 pages, 2 figures, submitted to EUSIPCO2018", "journal-ref": "EUSIPCO 2018", "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the detection of a low rank $n\\times n$deterministic matrix\n$\\mathbf{X}_{0}$ from the noisy observation ${\\bf X}_{0}+{\\bf Z}$ when\n$n\\to\\infty$, where ${\\bf Z}$ is a complex Gaussian random matrix with\nindependent identically distributed $\\mathcal{N}_{c}(0,\\frac{1}{n})$ entries.\nThanks to large random matrix theory results, it is now well-known that if the\nlargest singular value $\\lambda_{1}$ of ${\\bf X}_{0}$ verifies $\\lambda_{1}>1$,\nthen it is possible to exhibit consistent tests. In this contribution, we prove\na contrario that under the condition $\\lambda_{1}<1$, there are no consistent\ntests. Our proof is rather simple, inspired by previous works devoted to the\ncase of rank 1 matrices ${\\bf X}_{0}$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:23:22 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 13:24:22 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Chevreuil", "Antoine", ""], ["Loubaton", "Philippe", ""]]}, {"id": "1804.04916", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Max H. Farrell, Yingjie Feng", "title": "Large Sample Properties of Partitioning-Based Series Estimators", "comments": null, "journal-ref": "Annals of Statistics 2020, Vol. 48, No. 3, 1718-1741", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present large sample results for partitioning-based least squares\nnonparametric regression, a popular method for approximating conditional\nexpectation functions in statistics, econometrics, and machine learning. First,\nwe obtain a general characterization of their leading asymptotic bias. Second,\nwe establish integrated mean squared error approximations for the point\nestimator and propose feasible tuning parameter selection. Third, we develop\npointwise inference methods based on undersmoothing and robust bias correction.\nFourth, employing different coupling approaches, we develop uniform\ndistributional approximations for the undersmoothed and robust bias-corrected\nt-statistic processes and construct valid confidence bands. In the univariate\ncase, our uniform distributional approximations require seemingly minimal rate\nrestrictions and improve on approximation rates known in the literature.\nFinally, we apply our general results to three partitioning-based estimators:\nsplines, wavelets, and piecewise polynomials. The supplemental appendix\nincludes several other general and example-specific technical and\nmethodological results. A companion R package is provided.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 12:33:48 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 13:42:18 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 10:46:41 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""], ["Feng", "Yingjie", ""]]}, {"id": "1804.05012", "submitter": "Phil Long", "authors": "Peter L. Bartlett, Steven N. Evans and Philip M. Long", "title": "Representing smooth functions as compositions of near-identity functions\n  with implications for deep network optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any smooth bi-Lipschitz $h$ can be represented exactly as a\ncomposition $h_m \\circ ... \\circ h_1$ of functions $h_1,...,h_m$ that are close\nto the identity in the sense that each $\\left(h_i-\\mathrm{Id}\\right)$ is\nLipschitz, and the Lipschitz constant decreases inversely with the number $m$\nof functions composed. This implies that $h$ can be represented to any accuracy\nby a deep residual network whose nonlinear layers compute functions with a\nsmall Lipschitz constant. Next, we consider nonlinear regression with a\ncomposition of near-identity nonlinear maps. We show that, regarding Fr\\'echet\nderivatives with respect to the $h_1,...,h_m$, any critical point of a\nquadratic criterion in this near-identity region must be a global minimizer. In\ncontrast, if we consider derivatives with respect to parameters of a fixed-size\nresidual network with sigmoid activation functions, we show that there are\nnear-identity critical points that are suboptimal, even in the realizable case.\nInformally, this means that functional gradient methods for residual networks\ncannot get stuck at suboptimal critical points corresponding to near-identity\nlayers, whereas parametric gradient methods for sigmoidal residual networks\nsuffer from suboptimal critical points in the near-identity region.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:24:17 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 17:07:53 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Evans", "Steven N.", ""], ["Long", "Philip M.", ""]]}, {"id": "1804.05454", "submitter": "Tony Jebara", "authors": "Tony Jebara", "title": "A refinement of Bennett's inequality with applications to portfolio\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG q-fin.PM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A refinement of Bennett's inequality is introduced which is strictly tighter\nthan the classical bound. The new bound establishes the convergence of the\naverage of independent random variables to its expected value. It also\ncarefully exploits information about the potentially heterogeneous mean,\nvariance, and ceiling of each random variable. The bound is strictly sharper in\nthe homogeneous setting and very often significantly sharper in the\nheterogeneous setting. The improved convergence rates are obtained by\nleveraging Lambert's W function. We apply the new bound in a portfolio\noptimization setting to allocate a budget across investments with heterogeneous\nreturns.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 00:11:14 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Jebara", "Tony", ""]]}, {"id": "1804.05632", "submitter": "Michael Fauss", "authors": "Michael Fauss and Abdelhak M. Zoubir and H. Vincent Poor", "title": "On the Equivalence of f-Divergence Balls and Density Bands in Robust\n  Detection", "comments": "5 pages, 1 figure, accepted for publication in the Proceedings of the\n  IEEE International Conference on Acoustics, Speech, and Signal Processing\n  (ICASSP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with minimax optimal statistical tests for two composite\nhypotheses, where each hypothesis is defined by a non-parametric uncertainty\nset of feasible distributions. It is shown that for every pair of uncertainty\nsets of the f-divergence ball type, a pair of uncertainty sets of the density\nband type can be constructed, which is equivalent in the sense that it admits\nthe same pair of least favorable distributions. This result implies that robust\ntests under $f$-divergence ball uncertainty, which are typically only minimax\noptimal for the single sample case, are also fixed sample size minimax optimal\nwith respect to the equivalent density band uncertainty sets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:12:32 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Fauss", "Michael", ""], ["Zoubir", "Abdelhak M.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1804.05676", "submitter": "Sharif Rahman", "authors": "Sharif Rahman", "title": "A Polynomial Chaos Expansion in Dependent Random Variables", "comments": "26 pages, three figures, four tables; accepted by Journal of\n  Mathematical Analysis and Applications. arXiv admin note: substantial text\n  overlap with arXiv:1704.07912; text overlap with arXiv:1804.01647", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new generalized polynomial chaos expansion (PCE)\ncomprising measure-consistent multivariate orthonormal polynomials in dependent\nrandom variables. Unlike existing PCEs, whether classical or generalized, no\ntensor-product structure is assumed or required. Important mathematical\nproperties of the generalized PCE are studied by constructing orthogonal\ndecomposition of polynomial spaces, explaining completeness of orthogonal\npolynomials for prescribed assumptions, exploiting whitening transformation for\ngenerating orthonormal polynomial bases, and demonstrating mean-square\nconvergence to the correct limit. Analytical formulae are proposed to calculate\nthe mean and variance of a truncated generalized PCE for a general output\nvariable in terms of the expansion coefficients. An example derived from a\nstochastic boundary-value problem illustrates the generalized PCE approximation\nin estimating the statistical properties of an output variable for 12 distinct\nnon-product-type probability measures of input variables.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:39:48 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Rahman", "Sharif", ""]]}, {"id": "1804.05770", "submitter": "Jochen Br\\\"ocker", "authors": "Jochen Br\\\"ocker", "title": "Assessing the reliability of ensemble forecasting systems under serial\n  dependence", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": "10.1002/qj.3379", "report-no": null, "categories": "physics.ao-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing the reliability of ensemble forecasting systems is\nrevisited. A popular tool to assess the reliability of ensemble forecasting\nsystems (for scalar verifications) is the rank histogram, this histogram is\nexpected to be more or less flat, since for a reliable ensemble, the ranks are\nuniformly distributed among their possible outcomes. Quantitative tests for\nflatness (e.g.\\ Pearson's goodness--of--fit test) have been suggested, without\nexception though, these tests assume the ranks to be a sequence of independent\nrandom variables, which is not the case in general as can be demonstrated with\nsimple toy examples. In this paper, tests are developed that take the temporal\ncorrelations between the ranks into account. A refined analysis shows that\nexploiting the reliability property, the ranks still exhibit strong decay of\ncorrelations. This property is key to the analysis, and the proposed tests are\nvalid for general ensemble forecasting systems with minimal extraneous\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:17:37 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Br\u00f6cker", "Jochen", ""]]}, {"id": "1804.05783", "submitter": "Charles Tillier", "authors": "Natalie Neumeyer, Leonie Selk and Charles Tillier", "title": "Semi-parametric transformation boundary regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of nonparametric regression models with one-sided errors, we\nconsider parametric transformations of the response variable in order to obtain\nindependence between the errors and the covariates. We focus in this paper on\nstritcly increasing and continuous transformations. In view of estimating the\ntranformation parameter, we use a minimum distance approach and show the\nuniform consistency of the estimator under mild conditions. The boundary curve,\ni.e. the regression function, is estimated applying a smoothed version of a\nlocal constant approximation for which we also prove the uniform consistency.\nWe deal with both cases of random covariates and deterministic (fixed) design\npoints. To highlight the applicability of the procedures and to demonstrate\ntheir performance, the small sample behavior is investigated in a simulation\nstudy using the so-called Yeo-Johnson transformations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:54:01 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 13:14:40 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Selk", "Leonie", ""], ["Tillier", "Charles", ""]]}, {"id": "1804.05915", "submitter": "Liyuan Zhang", "authors": "Liyuan Zhang and Kshitij Khare", "title": "Trace class Markov chains for the Normal-Gamma Bayesian shrinkage model", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data, where the number of variables exceeds or is comparable\nto the sample size, is now pervasive in many scientific applications. In recent\nyears, Bayesian shrinkage models have been developed as effective and\ncomputationally feasible tools to analyze such data, especially in the context\nof linear regression. In this paper, we focus on the Normal-Gamma shrinkage\nmodel developed by Griffin and Brown. This model subsumes the popular Bayesian\nlasso model, and a three-block Gibbs sampling algorithm to sample from the\nresulting intractable posterior distribution has been developed by Griffin and\nBrown. We consider an alternative two-block Gibbs sampling algorithm and\nrigorously demonstrate its advantage over the three-block sampler by comparing\nspecific spectral properties. In particular, we show that the Markov operator\ncorresponding to the two-block sampler is trace class (and hence\nHilbert-Schmidt), whereas the operator corresponding to the three-block sampler\nis not even Hilbert-Schmidt. The trace class property for the two-block sampler\nimplies geometric convergence for the associated Markov chain, which justifies\nthe use of Markov chain CLT's to obtain practical error bounds for MCMC based\nestimates. Additionally, it facilitates theoretical comparisons of the\ntwo-block sampler with sandwich algorithms which aim to improve performance by\ninserting inexpensive extra steps in between the two conditional draws of the\ntwo-block sampler.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 19:55:21 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Zhang", "Liyuan", ""], ["Khare", "Kshitij", ""]]}, {"id": "1804.05959", "submitter": "Xiaohan Wei", "authors": "Xiaohan Wei", "title": "Structured Recovery with Heavy-tailed Measurements: A Thresholding\n  Procedure and Optimal Rates", "comments": "43 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general regularized thresholded least-square\nprocedure estimating a structured signal $\\theta_*\\in\\mathbb{R}^d$ from the\nfollowing observations: $y_i = f(\\langle\\mathbf{x}_i, \\theta_*\\rangle,\n\\xi_i),~i\\in\\{1,2,\\cdots,N\\}$, with i.i.d. heavy-tailed measurements\n$\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^N$. A general framework analyzing the\nthresholding procedure is proposed, which boils down to computing three\ncritical radiuses of the bounding balls of the estimator. Then, we demonstrate\nthese critical radiuses can be tightly bounded in the following two scenarios:\n(1) The link function $f(\\cdot)$ is linear, i.e. $y =\n\\langle\\mathbf{x},\\theta_*\\rangle + \\xi$, with $\\theta_*$ being a sparse vector\nand $\\{\\mathbf{x}_i\\}_{i=1}^N$ being general heavy-tailed random measurements\nwith bounded $(20+\\epsilon)$-moments. (2) The function $f(\\cdot)$ is arbitrary\nunknown (possibly discontinuous) and $\\{\\mathbf{x}_i\\}_{i=1}^N$ are\nheavy-tailed elliptical random vectors with bounded $(4+\\epsilon)$-moments. In\nboth scenarios, we show under these rather minimal bounded moment assumptions,\nsuch a procedure and corresponding analysis lead to optimal sample and error\nbounds with high probability in terms of the structural properties of\n$\\theta_*$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 21:58:33 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Wei", "Xiaohan", ""]]}, {"id": "1804.05975", "submitter": "James M. Flegal", "authors": "Ying Liu and Dootika Vats and James M. Flegal", "title": "Batch size selection for variance estimators in MCMC", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider batch size selection for a general class of multivariate batch\nmeans variance estimators, which are computationally viable for\nhigh-dimensional Markov chain Monte Carlo simulations. We derive the asymptotic\nmean squared error for this class of estimators. Further, we propose a\nparametric technique for estimating optimal batch sizes and discuss practical\nissues regarding the estimating process. Vector auto-regressive, Bayesian\nlogistic regression, and Bayesian dynamic space-time examples illustrate the\nquality of the estimation procedure where the proposed optimal batch sizes\noutperform current batch size selection methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 23:16:29 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 15:45:28 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 15:43:39 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Liu", "Ying", ""], ["Vats", "Dootika", ""], ["Flegal", "James M.", ""]]}, {"id": "1804.06186", "submitter": "Stephane Blondeau da Silva", "authors": "St\\'ephane Blondeau da Silva (XLIM-MATHIS)", "title": "Benford or not Benford: a systematic but not always well-founded use of\n  an elegant law in experimental fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will see that the proportion of d as leading digit, d $\\in$\n1, 9, in data (obtained thanks to the hereunder developed model) is more likely\nto follow a law whose probability distribution is determined by a specific\nupper bound, rather than Benford's Law. These probability distributions\nfluctuate around Benford's value as can often be observed in the literature in\nmany naturally occurring collections of data (where the physical , biological\nor economical quantities considered are upper bounded). Knowing beforehand the\nvalue of the upper bound can be a way to find a better adjusted law than\nBenford's one.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:05:33 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 13:37:53 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["da Silva", "St\u00e9phane Blondeau", "", "XLIM-MATHIS"]]}, {"id": "1804.06467", "submitter": "Sharif Rahman", "authors": "Sharif Rahman", "title": "A Galerkin Isogeometric Method for Karhunen-Loeve Approximation of\n  Random Fields", "comments": "31 pages, 12 figures, five tables; accepted by Computer Methods in\n  Applied Mechanics and Engineering", "journal-ref": null, "doi": "10.1016/j.cma.2018.04.026", "report-no": null, "categories": "math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper marks the debut of a Galerkin isogeometric method for solving a\nFredholm integral eigenvalue problem, enabling random field discretization by\nmeans of the Karhunen-Loeve expansion. The method involves a Galerkin\nprojection onto a finite-dimensional subspace of a Hilbert space, basis splines\n(B-splines) and non-uniform rational B-splines (NURBS) spanning the subspace,\nand standard methods of eigensolutions. Compared with the existing Galerkin\nmethods, such as the finite-element and mesh-free methods, the NURBS-based\nisogeometric method upholds exact geometrical representation of the physical or\ncomputational domain and exploits regularity of basis functions delivering\nglobally smooth eigensolutions. Therefore, the introduction of the isogeometric\nmethod for random field discretization is not only new; it also offers a few\ncomputational advantages over existing methods. In the big picture, the use of\nNURBS for random field discretization enriches the isogeometric paradigm. As a\nresult, an uncertainty quantification pipeline of the future can be envisioned\nwhere geometric modeling, stress analysis, and stochastic simulation are all\nintegrated using the same building blocks of NURBS. Three numerical examples,\nincluding a three-dimensional random field discretization problem, illustrate\nthe accuracy and convergence properties of the isogeometric method for\nobtaining eigensolutions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 21:09:47 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Rahman", "Sharif", ""]]}, {"id": "1804.06494", "submitter": "Olivier Collier", "authors": "Alexandra Carpentier, Olivier Collier, La\\\"etitia Comminges, Alexandre\n  B. Tsybakov, Yuhao Wang", "title": "Minimax rate of testing in sparse linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing the hypothesis that the parameter of\nlinear regression model is 0 against an s-sparse alternative separated from 0\nin the l2-distance. We show that, in Gaussian linear regression model with p <\nn, where p is the dimension of the parameter and n is the sample size, the\nnon-asymptotic minimax rate of testing has the form sqrt((s/n) log(1 +\nsqrt(p)/s )). We also show that this is the minimax rate of estimation of the\nl2-norm of the regression parameter.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 22:47:26 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 13:40:29 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 20:13:54 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Collier", "Olivier", ""], ["Comminges", "La\u00ebtitia", ""], ["Tsybakov", "Alexandre B.", ""], ["Wang", "Yuhao", ""]]}, {"id": "1804.06561", "submitter": "Song Mei", "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen", "title": "A Mean Field View of the Landscape of Two-Layers Neural Networks", "comments": "103 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-layer neural networks are among the most powerful models in machine\nlearning, yet the fundamental reasons for this success defy mathematical\nunderstanding. Learning a neural network requires to optimize a non-convex\nhigh-dimensional objective (risk function), a problem which is usually attacked\nusing stochastic gradient descent (SGD). Does SGD converge to a global optimum\nof the risk or only to a local optimum? In the first case, does this happen\nbecause local minima are absent, or because SGD somehow avoids them? In the\nsecond, why do local minima reached by SGD have good generalization properties?\n  In this paper we consider a simple case, namely two-layers neural networks,\nand prove that -in a suitable scaling limit- SGD dynamics is captured by a\ncertain non-linear partial differential equation (PDE) that we call\ndistributional dynamics (DD). We then consider several specific examples, and\nshow how DD can be used to prove convergence of SGD to networks with nearly\nideal generalization error. This description allows to 'average-out' some of\nthe complexities of the landscape of neural networks, and can be used to prove\na general convergence result for noisy SGD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 05:31:45 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 06:21:23 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Mei", "Song", ""], ["Montanari", "Andrea", ""], ["Nguyen", "Phan-Minh", ""]]}, {"id": "1804.06583", "submitter": "Julien Worms", "authors": "Jan Beirlant, Julien Worms (LMV), Rym Worms (LAMA)", "title": "Estimation of the extreme value index in a censorship framework:\n  asymptotic and finite sample behaviour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the estimation of the extreme value index for randomly censored\ndata from a heavy tailed distribution. We introduce a new class of estimators\nwhich encompasses earlier proposals given in Worms and Worms (2014) and\nBeirlant et al. (2018), which were shown to have good bias properties compared\nwith the pseudo maximum likelihood estimator proposed in Beirlant et al. (2007)\nand Einmahl et al. (2008). However the asymptotic normality of the type of\nestimators first proposed in Worms and Worms (2014) was still lacking, in the\nrandom threshold case. We derive an asymptotic representation and the\nasymptotic normality of the larger class of estimators and consider their\nfinite sample behaviour. Special attention is paid to the case of heavy\ncensoring, i.e. where the amount of censoring in the tail is at least 50\\%. We\nobtain the asymptotic normality with a classical $\\sqrt{k}$ rate where $k$\ndenotes the number of top data used in the estimation, depending on the degree\nof censoring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 07:26:28 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Beirlant", "Jan", "", "LMV"], ["Worms", "Julien", "", "LMV"], ["Worms", "Rym", "", "LAMA"]]}, {"id": "1804.06663", "submitter": "Samuel Rosa", "authors": "Samuel Rosa", "title": "E- and R-optimality of block designs for treatment-control comparisons", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal block designs for comparing a set of test treatments with a\ncontrol treatment. We provide the class of all E-optimal approximate block\ndesigns characterized by simple linear constraints. Employing this\ncharacterization, we obtain a class of E-optimal exact designs for\ntreatment-control comparisons for unequal block sizes. In the studied model, we\njustify the use of E-optimality by providing a statistical interpretation for\nall E-optimal approximate designs and for the known classes of E-optimal exact\ndesigns. Moreover, we consider the R-optimality criterion, which minimizes the\nvolume of the rectangular confidence region based on the Bonferroni confidence\nintervals. We show that all approximate A-optimal designs and a large class of\nA-optimal exact designs for treatment-control comparisons are also R-optimal.\nThis further reinforces the observation that A-optimal designs perform well\neven for rectangular confidence regions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 11:44:38 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Rosa", "Samuel", ""]]}, {"id": "1804.06906", "submitter": "Michael Evans", "authors": "Berthold-Georg Englert, Michael Evans, Gun Ho Jang, Hui Khoon Ng,\n  David Nott and Yi-Lin Seah", "title": "Checking the Model and the Prior for the Constrained Multinomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multinomial model is one of the simplest statistical models. When\nconstraints are placed on the possible values for the probabilities, however,\nit becomes much more difficult to deal with. Model checking and checking for\nprior-data conflict is considered here for such models. A theorem is proved\nthat establishes the consistency of the check on the prior. Applications are\npresented to models that arise in quantum state estimation as well as the\nBayesian analysis of models for ordered probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 20:25:13 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Englert", "Berthold-Georg", ""], ["Evans", "Michael", ""], ["Jang", "Gun Ho", ""], ["Ng", "Hui Khoon", ""], ["Nott", "David", ""], ["Seah", "Yi-Lin", ""]]}, {"id": "1804.06952", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya and Cl\\'ement L. Canonne and Himanshu Tyagi", "title": "Distributed Simulation and Distributed Inference", "comments": "This work is superseded by the more recent \"Inference under\n  Information Constraints II: Communication Constraints and Shared Randomness\"\n  (arXiv:1905.08302), by the same authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent samples from an unknown probability distribution $\\bf p$ on a\ndomain of size $k$ are distributed across $n$ players, with each player holding\none sample. Each player can communicate $\\ell$ bits to a central referee in a\nsimultaneous message passing model of communication to help the referee infer a\nproperty of the unknown $\\bf p$. What is the least number of players for\ninference required in the communication-starved setting of $\\ell<\\log k$? We\nbegin by exploring a general \"simulate-and-infer\" strategy for such inference\nproblems where the center simulates the desired number of samples from the\nunknown distribution and applies standard inference algorithms for the\ncollocated setting. Our first result shows that for $\\ell<\\log k$ perfect\nsimulation of even a single sample is not possible. Nonetheless, we present a\nLas Vegas algorithm that simulates a single sample from the unknown\ndistribution using $O(k/2^\\ell)$ samples in expectation. As an immediate\ncorollary, we get that simulate-and-infer attains the optimal sample complexity\nof $\\Theta(k^2/2^\\ell\\epsilon^2)$ for learning the unknown distribution to\ntotal variation distance $\\epsilon$. For the prototypical testing problem of\nidentity testing, simulate-and-infer works with $O(k^{3/2}/2^\\ell\\epsilon^2)$\nsamples, a requirement that seems to be inherent for all communication\nprotocols not using any additional resources. Interestingly, we can break this\nbarrier using public coins. Specifically, we exhibit a public-coin\ncommunication protocol that performs identity testing using\n$O(k/\\sqrt{2^\\ell}\\epsilon^2)$ samples. Furthermore, we show that this is\noptimal up to constant factors. Our theoretically sample-optimal protocol is\neasy to implement in practice. Our proof of lower bound entails showing a\ncontraction in $\\chi^2$ distance of product distributions due to communication\nconstraints and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 00:34:15 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 23:17:06 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 05:27:10 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1804.07065", "submitter": "Paul Jenkins", "authors": "Stefano Favaro, Shui Feng and Paul A. Jenkins", "title": "Bayesian nonparametric analysis of Kingman's coalescent", "comments": "37 pages, 2 figures. To appear in Annales de l'Institut Henri\n  Poincar\\'e - Probabilit\\'es et Statistiques", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kingman's coalescent is one of the most popular models in population\ngenetics. It describes the genealogy of a population whose genetic composition\nevolves in time according to the Wright-Fisher model, or suitable\napproximations of it belonging to the broad class of Fleming-Viot processes.\nAncestral inference under Kingman's coalescent has had much attention in the\nliterature, both in practical data analysis, and from a theoretical and\nmethodological point of view. Given a sample of individuals taken from the\npopulation at time $t>0$, most contributions have aimed at making frequentist\nor Bayesian parametric inference on quantities related to the genealogy of the\nsample. In this paper we propose a Bayesian nonparametric predictive approach\nto ancestral inference. That is, under the prior assumption that the\ncomposition of the population evolves in time according to a neutral\nFleming-Viot process, and given the information contained in an initial sample\nof $m$ individuals taken from the population at time $t>0$, we estimate\nquantities related to the genealogy of an additional unobservable sample of\nsize $m^{\\prime}\\geq1$. As a by-product of our analysis we introduce a class of\nBayesian nonparametric estimators (predictors) which can be thought of as\nGood-Turing type estimators for ancestral inference. The proposed approach is\nillustrated through an application to genetic data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:12:35 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Favaro", "Stefano", ""], ["Feng", "Shui", ""], ["Jenkins", "Paul A.", ""]]}, {"id": "1804.07203", "submitter": "Rajen Shah", "authors": "Rajen D. Shah and Jonas Peters", "title": "The Hardness of Conditional Independence Testing and the Generalised\n  Covariance Measure", "comments": "Small typos corrected", "journal-ref": "Ann. Statist. 48(3): 1514-1538, 2020", "doi": "10.1214/19-AOS1857", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a common saying that testing for conditional independence, i.e.,\ntesting whether whether two random vectors $X$ and $Y$ are independent, given\n$Z$, is a hard statistical problem if $Z$ is a continuous random variable (or\nvector). In this paper, we prove that conditional independence is indeed a\nparticularly difficult hypothesis to test for. Valid statistical tests are\nrequired to have a size that is smaller than a predefined significance level,\nand different tests usually have power against a different class of\nalternatives. We prove that a valid test for conditional independence does not\nhave power against any alternative.\n  Given the non-existence of a uniformly valid conditional independence test,\nwe argue that tests must be designed so their suitability for a particular\nproblem may be judged easily. To address this need, we propose in the case\nwhere $X$ and $Y$ are univariate to nonlinearly regress $X$ on $Z$, and $Y$ on\n$Z$ and then compute a test statistic based on the sample covariance between\nthe residuals, which we call the generalised covariance measure (GCM). We prove\nthat validity of this form of test relies almost entirely on the weak\nrequirement that the regression procedures are able to estimate the conditional\nmeans $X$ given $Z$, and $Y$ given $Z$, at a slow rate. We extend the\nmethodology to handle settings where $X$ and $Y$ may be multivariate or even\nhigh-dimensional. While our general procedure can be tailored to the setting at\nhand by combining it with any regression technique, we develop the theoretical\nguarantees for kernel ridge regression. A simulation study shows that the test\nbased on GCM is competitive with state of the art conditional independence\ntests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:55:34 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 18:53:25 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 14:01:46 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 16:15:26 GMT"}, {"version": "v5", "created": "Tue, 23 Mar 2021 09:56:50 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Shah", "Rajen D.", ""], ["Peters", "Jonas", ""]]}, {"id": "1804.07416", "submitter": "Xiongzhi Chen", "authors": "X. Jessie Jeng and Xiongzhi Chen", "title": "Variable Selection via Adaptive False Negative Control in Linear\n  Regression", "comments": "27 pages and 1 figure; to appear in Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variable selection methods have been developed in linear regression to\nprovide sparse solutions. Recent studies have focused on further\ninterpretations on the sparse solutions in terms of false positive control. In\nthis paper, we consider false negative control for variable selection with the\ngoal to efficiently select a high proportion of relevant predictors. Different\nfrom existing studies in power analysis and sure screening, we propose to\ndirectly estimate the false negative proportion (FNP) of a decision rule and\nselect the smallest subset of predictors that has the estimated FNP less than a\nuser-specified control level. The proposed method is adaptive to the\nuser-specified control level on FNP by selecting less candidates if a higher\nlevel is implemented. On the other hand, when data has stronger effect size or\nlarger sample size, the proposed method controls FNP more efficiently with less\nfalse positives. New analytic techniques are developed to cope with the major\nchallenge of FNP control when relevant predictors cannot be consistently\nseparated from irrelevant ones. Our numerical results are in line with the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 01:10:42 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 18:28:30 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Chen", "Xiongzhi", ""]]}, {"id": "1804.07566", "submitter": "Pierre Neuvial", "authors": "Fran\\c{c}ois Bachoc (IMT), Gilles Blanchard, Pierre Neuvial (IMT)", "title": "On the Post Selection Inference constant under Restricted Isometry\n  Properties", "comments": "Electronic journal of statistics, Shaker Heights, OH : Institute of\n  Mathematical Statistics, 2018", "journal-ref": null, "doi": "10.1214/18-ejs1490", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniformly valid confidence intervals post model selection in regression can\nbe constructed based on Post-Selection Inference (PoSI) constants. PoSI\nconstants are minimal for orthogonal design matrices, and can be upper bounded\nin function of the sparsity of the set of models under consideration, for\ngeneric design matrices. In order to improve on these generic sparse upper\nbounds, we consider design matrices satisfying a Restricted Isometry Property\n(RIP) condition. We provide a new upper bound on the PoSI constant in this\nsetting. This upper bound is an explicit function of the RIP constant of the\ndesign matrix, thereby giving an interpolation between the orthogonal setting\nand the generic sparse setting. We show that this upper bound is asymptotically\noptimal in many settings by constructing a matching lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 12:00:05 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 10:08:56 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Blanchard", "Gilles", "", "IMT"], ["Neuvial", "Pierre", "", "IMT"]]}, {"id": "1804.07600", "submitter": "Yesim Guney", "authors": "Yesim Guney, Yetkin Tuac, Senay Ozdemir, and Olcay Arslan", "title": "Conditional Maximum Lq-Likelihood Estimation for Regression Model with\n  Autoregressive Error Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the parameter estimation of regression model\nwith pth order autoregressive (AR(p)) error term. We use the Maximum\nLq-likelihood (MLq) estimation method that is proposed by Ferrari and Yang\n(2010a), as a robust alternative to the classical maximum likelihood (ML)\nestimation method to handle the outliers in the data. After exploring the MLq\nestimators for the parameters of interest, we provide some asymptotic\nproperties of the resulting MLq estimators. We give a simulation study and a\nreal data example to illustrate the performance of the new estimators over the\nML estimators and observe that the MLq estimators have superiority over the ML\nestimators when outliers are present in the data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 13:22:46 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Guney", "Yesim", ""], ["Tuac", "Yetkin", ""], ["Ozdemir", "Senay", ""], ["Arslan", "Olcay", ""]]}, {"id": "1804.07742", "submitter": "Rafael Frongillo", "authors": "Krisztina Dearborn and Rafael Frongillo", "title": "On the Indirect Elicitability of the Mode and Modal Interval", "comments": "14 pages, 3 figures, 1 table; substantially revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring functions are commonly used to evaluate a point forecast of a\nparticular statistical functional. This scoring function should be consistent,\nmeaning the correct value of the functional is the Bayes act, in which case we\nsay the scoring function elicits the functional. Recent results show that the\nmode functional is not elicitable. In this work, we ask whether it is at least\npossible to indirectly elicit the mode, wherein one elicits a low-dimensional\nfunctional from which the mode can be computed. We show that this cannot be\ndone: neither the mode nor a modal interval are indirectly elicitable with\nrespect to the class of identifiable functionals.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:35:29 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 22:34:23 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 18:06:27 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Dearborn", "Krisztina", ""], ["Frongillo", "Rafael", ""]]}, {"id": "1804.07833", "submitter": "Bamdad Hosseini Dr.", "authors": "Bamdad Hosseini", "title": "Two Metropolis-Hastings algorithms for posterior measures with\n  non-Gaussian priors in infinite dimensions", "comments": "Minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two classes of Metropolis-Hastings algorithms for sampling\ntarget measures that are absolutely continuous with respect to non-Gaussian\nprior measures on infinite-dimensional Hilbert spaces. In particular, we focus\non certain classes of prior measures for which prior-reversible proposal\nkernels of the autoregressive type can be designed. We then use these proposal\nkernels to design algorithms that satisfy detailed balance with respect to the\ntarget measures. Afterwards, we introduce a new class of prior measures, called\nthe Bessel-K priors, as a generalization of the gamma distribution to measures\nin infinite dimensions. The Bessel-K priors interpolate between well-known\npriors such as the gamma distribution and Besov priors and can model sparse or\ncompressible parameters. We present concrete instances of our algorithms for\nthe Bessel-K priors in the context of numerical examples in density estimation,\nfinite-dimensional denoising and deconvolution on the circle.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:36:08 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 16:24:16 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 23:00:00 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 22:50:13 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hosseini", "Bamdad", ""]]}, {"id": "1804.07919", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "Probabilistic Analysis of Balancing Scores for Causal Inference", "comments": "11 pages", "journal-ref": "Journal of Mathematics Research (2015); Vol. 7, No. 2; pp. 90-100", "doi": "10.5539/jmr.v7n2p90", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity scores are often used for stratification of treatment and control\ngroups of subjects in observational data to remove confounding bias when\nestimating of causal effect of the treatment on an outcome in so-called\npotential outcome causal modeling framework. In this article, we try to get\nsome insights into basic behavior of the propensity scores in a probabilistic\nsense. We do a simple analysis of their usage confining to the case of discrete\nconfounding covariates and outcomes. While making clear about behavior of the\npropensity score our analysis shows how the so-called prognostic score can be\nderived simultaneously. However the prognostic score is derived in a limited\nsense in the current literature whereas our derivation is more general and\nshows all possibilities of having the score. And we call it outcome score. We\nargue that application of both the propensity score and the outcome score is\nthe most efficient way for reduction of dimension in the confounding covariates\nas opposed to current belief that the propensity score alone is the most\nefficient way.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 08:55:41 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}, {"id": "1804.07937", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "A geometric view on Pearson's correlation coefficient and a\n  generalization of it to non-linear dependencies", "comments": "19 pages", "journal-ref": "Ratio Mathematica, Issue N. 30 (2016) Pp. 3-21", "doi": "10.23755/rm.v30i1.5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring strength or degree of statistical dependence between two random\nvariables is a common problem in many domains. Pearson's correlation\ncoefficient $\\rho$ is an accurate measure of linear dependence. We show that\n$\\rho$ is a normalized, Euclidean type distance between joint probability\ndistribution of the two random variables and that when their independence is\nassumed while keeping their marginal distributions. And the normalizing\nconstant is the geometric mean of two maximal distances, each between the joint\nprobability distribution when the full linear dependence is assumed while\npreserving respective marginal distribution and that when the independence is\nassumed. Usage of it is restricted to linear dependence because it is based on\nEuclidean type distances that are generally not metrics and considered full\ndependence is linear. Therefore, we argue that if a suitable distance metric is\nused while considering all possible maximal dependences then it can measure any\nnon-linear dependence. But then, one must define all the full dependences.\nHellinger distance that is a metric can be used as the distance measure between\nprobability distributions and obtain a generalization of $\\rho$ for the\ndiscrete case.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 10:28:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}, {"id": "1804.08116", "submitter": "John Duchi", "authors": "John C. Duchi and Feng Ruan", "title": "A constrained risk inequality for general losses", "comments": "10 pages. This version (v2) adds applications to efficient\n  nonparametric estimation and some pedagogical comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general constrained risk inequality that applies to arbitrary\nnon-decreasing losses, extending a result of Brown and Low [Ann. Stat. 1996].\nGiven two distributions $P_0$ and $P_1$, we find a lower bound for the risk of\nestimating a parameter $\\theta(P_1)$ under $P_1$ given an upper bound on the\nrisk of estimating the parameter $\\theta(P_0)$ under $P_0$. The inequality is a\nuseful pedagogical tool, as its proof relies only on the Cauchy-Schwartz\ninequality, it applies to general losses, and it transparently gives risk lower\nbounds on super-efficient and adaptive estimators.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 14:18:52 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 04:55:14 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 04:58:11 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Duchi", "John C.", ""], ["Ruan", "Feng", ""]]}, {"id": "1804.08143", "submitter": "W.J. Handley", "authors": "Will Handley and Marius Millea", "title": "Maximum-Entropy Priors with Derived Parameters in a Specified\n  Distribution", "comments": "7 pages, 2 figures, Published in Entropy", "journal-ref": null, "doi": "10.3390/e21030272", "report-no": null, "categories": "math.ST astro-ph.CO astro-ph.IM hep-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for transforming probability distributions so that\nparameters of interest are forced into a specified distribution. We prove that\nthis approach is the maximum entropy choice, and provide a motivating example\napplicable to neutrino hierarchy inference.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 17:39:04 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 14:04:27 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 07:55:33 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Handley", "Will", ""], ["Millea", "Marius", ""]]}, {"id": "1804.08222", "submitter": "Kun He", "authors": "Kun He, Mengjie Li, Yan Fu, Fuzhou Gong and Xiaoming Sun", "title": "Null-free False Discovery Rate Control Using Decoy Permutations", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional approaches to false discovery rate (FDR) control in multiple\nhypothesis testing are usually based on the null distribution of a test\nstatistic. However, all types of null distributions, including the theoretical,\npermutation-based and empirical ones, have some inherent drawbacks. For\nexample, the theoretical null might fail because of improper assumptions on the\nsample distribution. Here, we propose a null distribution-free approach to FDR\ncontrol for multiple hypothesis testing. This approach, named target-decoy\nprocedure, simply builds on the ordering of tests by some statistic or score,\nthe null distribution of which is not required to be known. Competitive decoy\ntests are constructed from permutations of original samples and are used to\nestimate the false target discoveries. We prove that this approach controls the\nFDR when the statistics are independent between different tests. Simulation\ndemonstrates that it is more stable and powerful than two existing popular\napproaches. Evaluation is also made on a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 02:05:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 04:42:24 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:57:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["He", "Kun", ""], ["Li", "Mengjie", ""], ["Fu", "Yan", ""], ["Gong", "Fuzhou", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1804.08408", "submitter": "Mikhail Moklyachuk", "authors": "Oleksandr Masyutka, Mikhail Moklyachuk, Maria Sidei", "title": "Filtering of Multidimensional Stationary Sequences with Missing\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of mean-square optimal linear estimation of linear functionals\nwhich depend on the unknown values of a multidimensional stationary stochastic\nsequence from observations of the sequence with a noise and missing\nobservations is considered. Formulas for calculating the mean-square errors and\nthe spectral characteristics of the optimal linear estimates of the functionals\nare proposed under the condition of spectral certainty, where spectral\ndensities of the sequences are exactly known. The minimax (robust) method of\nestimation is applied in the case where spectral densities are not known\nexactly while some sets of admissible spectral densities are given. Formulas\nthat determine the least favorable spectral densities and minimax spectral\ncharacteristics are proposed for some special sets of admissible densities.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 13:20:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Masyutka", "Oleksandr", ""], ["Moklyachuk", "Mikhail", ""], ["Sidei", "Maria", ""]]}, {"id": "1804.08553", "submitter": "Samuel W.K. Wong", "authors": "Saptarshi Chakraborty and Samuel W.K. Wong", "title": "On the circular correlation coefficients for bivariate von Mises\n  distributions on a torus", "comments": "29 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies circular correlations for the bivariate von Mises sine and\ncosine distributions. These are two simple and appealing models for bivariate\nangular data with five parameters each that have interpretations comparable to\nthose in the ordinary bivariate normal model. However, the variability and\nassociation of the angle pairs cannot be easily deduced from the model\nparameters unlike the bivariate normal. Thus to compute such summary measures,\ntools from circular statistics are needed. We derive analytic expressions and\nstudy the properties of the Jammalamadaka-Sarma and Fisher-Lee circular\ncorrelation coefficients for the von Mises sine and cosine models.\nLikelihood-based inference of these coefficients from sample data is then\npresented. The correlation coefficients are illustrated with numerical and\nvisual examples, and the maximum likelihood estimators are assessed on\nsimulated and real data, with comparisons to their non-parametric counterparts.\nImplementations of these computations for practical use are provided in our R\npackage BAMBI.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:41:03 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:12:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1804.08650", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Lizhen Lin", "title": "Bayesian Bandwidth Test and Selection for High-dimensional Banded\n  Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming a banded structure is one of the common practice in the estimation\nof high-dimensional precision matrix. In this case, estimating the bandwidth of\nthe precision matrix is a crucial initial step for subsequent analysis.\nAlthough there exist some consistent frequentist tests for the bandwidth\nparameter, bandwidth selection consistency for precision matrices has not been\nestablished in a Bayesian framework. In this paper, we propose a prior\ndistribution tailored to the bandwidth estimation of high-dimensional precision\nmatrices. The banded structure is imposed via the Cholesky factor from the\nmodified Cholesky decomposition. We establish the strong model selection\nconsistency for the bandwidth as well as the consistency of the Bayes factor.\nThe convergence rates for Bayes factors under both the null and alternative\nhypotheses are derived which yield similar order of rates. As a by-product, we\nalso proposed an estimation procedure for the Cholesky factors yielding an\nalmost optimal order of convergence rates. Two-sample bandwidth test is also\nconsidered, and it turns out that our method is able to consistently detect the\nequality of bandwidths between two precision matrices. The simulation study\nconfirms that our method in general outperforms the existing frequentist and\nBayesian methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:21:40 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 00:19:14 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lin", "Lizhen", ""]]}, {"id": "1804.08741", "submitter": "Alexander Bulinski", "authors": "Alexander Bulinski and Alexey Kozhevin", "title": "Statistical Estimation of Conditional Shannon Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new estimates of the conditional Shannon entropy are introduced in the\nframework of the model describing a discrete response variable depending on a\nvector of d factors having a density w.r.t. the Lebesgue measure in R^d.\nNamely, the mixed-pair model (X,Y) is considered where X and Y take values in\nR^d and an arbitrary finite set, respectively. Such models include, for\ninstance, the famous logistic regression. In contrast to the well-known\nKozachenko -- Leonenko estimates of unconditional entropy the proposed\nestimates are constructed by means of the certain spacial order statistics (or\nk-nearest neighbor statistics where k=k_n depends on amount of observations n)\nand a random number of i.i.d. observations contained in the balls of specified\nrandom radii. The asymptotic unbiasedness and L^2-consistency of the new\nestimates are established under simple conditions. The obtained results can be\napplied to the feature selection problem which is important, e.g., for medical\nand biological investigations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 21:16:15 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Bulinski", "Alexander", ""], ["Kozhevin", "Alexey", ""]]}, {"id": "1804.08841", "submitter": "Haoyang Liu", "authors": "Haoyang Liu and Rina Foygel Barber", "title": "Between hard and soft thresholding: optimal iterative thresholding\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative thresholding algorithms seek to optimize a differentiable objective\nfunction over a sparsity or rank constraint by alternating between gradient\nsteps that reduce the objective, and thresholding steps that enforce the\nconstraint. This work examines the choice of the thresholding operator, and\nasks whether it is possible to achieve stronger guarantees than what is\npossible with hard thresholding. We develop the notion of relative concavity of\na thresholding operator, a quantity that characterizes the worst-case\nconvergence performance of any thresholding operator on the target optimization\nproblem. Surprisingly, we find that commonly used thresholding operators, such\nas hard thresholding and soft thresholding, are suboptimal in terms of\nworst-case convergence guarantees. Instead, a general class of thresholding\noperators, lying between hard thresholding and soft thresholding, is shown to\nbe optimal with the strongest possible convergence guarantee among all\nthresholding operators. Examples of this general class includes $\\ell_q$\nthresholding with appropriate choices of $q$, and a newly defined {\\em\nreciprocal thresholding} operator. We also investigate the implications of the\nimproved optimization guarantee in the statistical setting of sparse linear\nregression, and show that this new class of thresholding operators attain the\noptimal rate for computationally efficient estimators, matching the Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 05:19:44 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:04:50 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 01:06:22 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 19:54:59 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Liu", "Haoyang", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1804.09014", "submitter": "Yuri Golubev", "authors": "Yuri Golubev and Mher Safarian", "title": "On robust stopping times for detecting changes in distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,X_2,\\ldots $ be independent random variables observed sequentially\nand such that $X_1,\\ldots,X_{\\theta-1}$ have a common probability density\n$p_0$, while $X_\\theta,X_{\\theta+1},\\ldots $ are all distributed according to\n$p_1\\neq p_0$. It is assumed that $p_0$ and $p_1$ are known, but the time\nchange $\\theta\\in \\mathbb{Z}^+$ is unknown and the goal is to construct a\nstopping time $\\tau$ that detects the change-point $\\theta$ as soon as\npossible. The existing approaches to this problem rely essentially on some a\npriori information about $\\theta$. For instance, in Bayes approaches, it is\nassumed that $\\theta$ is a random variable with a known probability\ndistribution. In methods related to hypothesis testing, this a priori\ninformation is hidden in the so-called average run length. The main goal in\nthis paper is to construct stopping times which do not make use of a priori\ninformation about $\\theta$, but have nearly Bayesian detection delays. More\nprecisely, we propose stopping times solving approximately the following\nproblem: \\begin{equation*} \\begin{split} &\\quad\n\\Delta(\\theta;\\tau^\\alpha)\\rightarrow\\min_{\\tau^\\alpha}\\quad \\textbf{subject\nto}\\quad \\alpha(\\theta;\\tau^\\alpha)\\le \\alpha \\ \\textbf{ for any}\\ \\theta\\ge1,\n\\end{split} \\end{equation*} where\n$\\alpha(\\theta;\\tau)=\\mathbf{P}_\\theta\\bigl\\{\\tau<\\theta \\bigr\\}$ is\n\\textit{the false alarm probability} and\n$\\Delta(\\theta;\\tau)=\\mathbf{E}_\\theta(\\tau-\\theta)_+$ is \\textit{the average\ndetection delay}, %In this paper, we construct $\\widetilde{\\tau}^\\alpha$ such\nthat %\\[ % \\max_{\\theta\\ge 1}\\alpha(\\theta;\\widetilde{\\tau}^\\alpha)\\le \\alpha\\\n\\text{and}\\ %\\Delta(\\theta;\\widetilde{\\tau}^\\alpha)\\le\n(1+o(1))\\log(\\theta/\\alpha), \\ \\text{as} \\ \\theta/\\alpha%\\rightarrow\\infty, %\\]\nand explain why such stopping times are robust w.r.t. a priori information\nabout $\\theta$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:27:44 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Golubev", "Yuri", ""], ["Safarian", "Mher", ""]]}, {"id": "1804.09402", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "On nonparametric inference for spatial regression models under domain\n  expanding and infill asymptotics", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop nonparametric inference on spatial regression\nmodels as an extension of Lu and Tj\\ostheim(2014), which develops nonparametric\ninference on density functions of stationary spatial processes under domain\nexpanding and infill (DEI) asymptotics. In particular, we derive multivariate\ncentral limit theorems of mean and variance functions of nonparametric spatial\nregression models. Built upon those results, we propose a method to construct\nconfidence bands for mean and variance functions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:41:08 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 06:53:19 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 08:59:28 GMT"}, {"version": "v4", "created": "Thu, 11 Jul 2019 11:42:36 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "1804.09472", "submitter": "Ionel Popescu", "authors": "Saba Amsalu, Juntao Duan, Heinrich Matzinger, Ionel Popescu", "title": "Recovery of spectrum from estimated covariance matrices and statistical\n  kernels for machine learning and big data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose two schemes for the recovery of the spectrum of a\ncovariance matrix from the empirical covariance matrix, in the case where the\ndimension of the matrix is a subunitary multiple of the number of observations.\nWe test, compare and analyze these on simulated data and also on some data\ncoming from the stock market.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 10:38:00 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Amsalu", "Saba", ""], ["Duan", "Juntao", ""], ["Matzinger", "Heinrich", ""], ["Popescu", "Ionel", ""]]}, {"id": "1804.09804", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Multivariate subjective fiducial inference", "comments": "Final version with corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to firmly establish subjective fiducial inference as\na rival to the more conventional schools of statistical inference, and to show\nthat Fisher's intuition concerning the importance of the fiducial argument was\ncorrect. In this regard, methodology outlined in an earlier paper is modified,\nenhanced and extended to deal with general inferential problems in which\nvarious parameters are unknown. As a key part of what is put forward, the joint\nfiducial distribution of all the parameters of a given model is determined on\nthe basis of the full conditional fiducial distributions of these parameters by\nusing an analytical approach or a Gibbs sampling method, the latter of which\ndoes not require these conditional distributions to be compatible. Although the\nresulting theory is classified as being \"subjective\", this is essentially due\nto the argument that all probability statements made about fixed but unknown\nparameters must be inherently subjective. In particular, it is systematically\nargued that, in general, there is no need to place a great emphasis on the\ndifference between the fiducial probabilities derived by using this theory of\ninference and objective probabilities. Some important examples of the\napplication of this theory are presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:25:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 16:38:29 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 16:50:28 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2020 16:27:30 GMT"}, {"version": "v5", "created": "Wed, 7 Apr 2021 16:35:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1804.09879", "submitter": "Jason Klusowski M", "authors": "Victor-Emmanuel Brunel, Jason M. Klusowski, Dana Yang", "title": "Estimation of convex supports from noisy measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular class of problem in statistics deals with estimating the support of\na density from $n$ observations drawn at random from a $d$-dimensional\ndistribution. The one-dimensional case reduces to estimating the end points of\na univariate density. In practice, an experimenter may only have access to a\nnoisy version of the original data. Therefore, a more realistic model allows\nfor the observations to be contaminated with additive noise.\n  In this paper, we consider estimation of convex bodies when the additive\nnoise is distributed according to a multivariate Gaussian distribution, even\nthough our techniques could easily be adapted to other noise distributions.\nUnlike standard methods in deconvolution that are implemented by thresholding a\nkernel density estimate, our method avoids tuning parameters and Fourier\ntransforms altogether. We show that our estimator, computable in $(O(\\ln\nn))^{(d-1)/2}$ time, converges at a rate of $ O_d(\\log\\log n/\\sqrt{\\log n}) $\nin Hausdorff distance, in accordance with the polylogarithmic rates encountered\nin Gaussian deconvolution problems. Part of our analysis also involves the\noptimality of the proposed estimator. We provide a lower bound for the minimax\nrate of estimation in Hausdorff distance that is $\\Omega_d(1/\\log^2 n)$.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 03:46:11 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""], ["Klusowski", "Jason M.", ""], ["Yang", "Dana", ""]]}, {"id": "1804.09887", "submitter": "Shujun Bi", "authors": "Shujun Bi and Shaohua Pan", "title": "GEP-MSCRA for computing the group zero-norm regularized least squares\n  estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns with the group zero-norm regularized least squares\nestimator which, in terms of the variational characterization of the zero-norm,\ncan be obtained from a mathematical program with equilibrium constraints\n(MPEC). By developing the global exact penalty for the MPEC, this estimator is\nshown to arise from an exact penalization problem that not only has a favorable\nbilinear structure but also implies a recipe to deliver equivalent DC\nestimators such as the SCAD and MCP estimators. We propose a multi-stage convex\nrelaxation approach (GEP-MSCRA) for computing this estimator, and under a\nrestricted strong convexity assumption on the design matrix, establish its\ntheoretical guarantees which include the decreasing of the error bounds for the\niterates to the true coefficient vector and the coincidence of the iterates\nafter finite steps with the oracle estimator. Finally, we implement the\nGEP-MSCRA with the subproblems solved by a semismooth Newton augmented\nLagrangian method (ALM) and compare its performance with that of SLEP and\nMALSAR, the solvers for the weighted $\\ell_{2,1}$-norm regularized estimator,\non synthetic group sparse regression problems and real multi-task learning\nproblems. Numerical comparison indicates that the GEP-MSCRA has significant\nadvantage in reducing error and achieving better sparsity than the SLEP and the\nMALSAR do.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 04:54:57 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Bi", "Shujun", ""], ["Pan", "Shaohua", ""]]}, {"id": "1804.09912", "submitter": "Nicolas Auguin", "authors": "Nicolas Auguin, David Morales-Jimenez, Matthew R. McKay, Romain\n  Couillet", "title": "Large-dimensional behavior of regularized Maronna's M-estimators of\n  covariance matrices", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2018.2831629", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimators of large covariance matrices are considered, comprising\nregularized (linear shrinkage) modifications of Maronna's classical\nM-estimators. These estimators provide robustness to outliers, while\nsimultaneously being well-defined when the number of samples does not exceed\nthe number of variables. By applying tools from random matrix theory, we\ncharacterize the asymptotic performance of such estimators when the numbers of\nsamples and variables grow large together. In particular, our results show\nthat, when outliers are absent, many estimators of the regularized-Maronna type\nshare the same asymptotic performance, and for these estimators we present a\ndata-driven method for choosing the asymptotically optimal regularization\nparameter with respect to a quadratic loss. Robustness in the presence of\noutliers is then studied: in the non-regularized case, a large-dimensional\nrobustness metric is proposed, and explicitly computed for two particular types\nof estimators, exhibiting interesting differences depending on the underlying\ncontamination model. The impact of outliers in regularized estimators is then\nstudied, with interesting differences with respect to the non-regularized case,\nleading to new practical insights on the choice of particular estimators.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 06:51:20 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Auguin", "Nicolas", ""], ["Morales-Jimenez", "David", ""], ["McKay", "Matthew R.", ""], ["Couillet", "Romain", ""]]}, {"id": "1804.09940", "submitter": "Tsubasa Ito", "authors": "Tsubasa Ito and Tatsuya Kubokawa", "title": "Empirical Best Linear Unbiased Predictors in Multivariate Nested-Error\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For analyzing unit-level multivariate data in small area estimation, we\nconsider the multivariate nested error regression model (MNER) and provide the\nempirical best linear unbiased predictor (EBLUP) of a small area characteristic\nbased on second-order unbiased and consistent estimators of the `within' and\n`between' multivariate components of variance. The second-order approximation\nof the mean squared error (MSE) matrix of the EBLUP and its unbiased estimator\nare derived in closed forms. The confidence interval with second-order accuracy\nis also provided analytically.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:44:45 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Ito", "Tsubasa", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1804.09941", "submitter": "Tsubasa Ito", "authors": "Tsubasa Ito and Tatsuya Kubokawa", "title": "On Measuring the Variability of Small Area Estimators in a Multivariate\n  Fay-Herriot Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the small area estimation in the multivariate\nFay-Herriot model where covariance matrix of random effects are fully unknown.\nThe covariance matrix is estimated by a Prasad-Rao type consistent estimator,\nand the empirical best linear un- biased predictor (EBLUP) of a vector of small\narea characteristics is provided. When the EBLUP is measured in terms of a mean\nsquared error matrix (MSEM), a second-order approximation of MSEM of the EBLUP\nand a second-order unbiased estimator of the MSEM is derived analytically in\nclosed forms. The performance is investigated through numerical and empirical\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:48:14 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Ito", "Tsubasa", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1804.10032", "submitter": "Tsubasa Ito", "authors": "Tsubasa Ito and Tatsuya Kubokawa", "title": "Corrected Empirical Bayes Confidence Region in a Multivariate\n  Fay-Herriot Model", "comments": "arXiv admin note: text overlap with arXiv:1804.09941", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the small area estimation, the empirical best linear unbiased predictor\n(EBLUP) in the linear mixed model is useful because it gives a stable estimate\nfor a mean of a smallarea. For measuring uncertainty of EBLUP, much of research\nis focused on second-orderunbiased estimation of mean squared prediction errors\nin the univariate case. In this paper, we consider the multivariate Fay-Herriot\nmodel where the covariance matrix of random effects is fully unknown, and\nobtain a confidence reagion of the small area mean that is based on the\nMahalanobis distance centered around EBLUP and is second order correct. A\npositive-definite, consistent and second-order unbiased estimator of the\ncovariance matrix of the random effects is also suggested. The performance is\ninvestigated through simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 13:00:46 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ito", "Tsubasa", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1804.10151", "submitter": "Michael Fauss", "authors": "Michael Fauss and Alex Dytso and Abdelhak M. Zoubir and H. Vincent\n  Poor", "title": "Tight MMSE Bounds for the AGN Channel Under KL Divergence Constraints on\n  the Input Distribution", "comments": "5 pages, 3 figures, accepted for publication in the proceedings of\n  the IEEE Statistical Signal Processing Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tight bounds on the minimum mean square error for the additive Gaussian noise\nchannel are derived, when the input distribution is constrained to be\nepsilon-close to a Gaussian reference distribution in terms of the\nKullback--Leibler divergence. The distributions that attain the bounds are\nshown be Gaussian whose means are identical to that of the reference\ndistribution and whose covariance matrices are defined implicitly via systems\nof matrix equations. The estimator that attains the upper bound is identified\nas a minimax optimal estimator that is robust against deviations from the\nassumed prior. The lower bound is shown to provide a potentially tighter\nalternative to the Cramer--Rao bound. Both properties are illustrated with\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:26:29 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Fauss", "Michael", ""], ["Dytso", "Alex", ""], ["Zoubir", "Abdelhak M.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1804.10556", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Convergence and Concentration of Empirical Measures under Wasserstein\n  Distance in Unbounded Functional Spaces", "comments": "35 pages", "journal-ref": "Bernoulli, Volume 26, Number 1 (2020), 767-798", "doi": "10.3150/19-BEJ1151", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide upper bounds of the expected Wasserstein distance between a\nprobability measure and its empirical version, generalizing recent results for\nfinite dimensional Euclidean spaces and bounded functional spaces. Such a\ngeneralization can cover Euclidean spaces with large dimensionality, with the\noptimal dependence on the dimensionality. Our method also covers the important\ncase of Gaussian processes in separable Hilbert spaces, with rate-optimal upper\nbounds for functional data distributions whose coordinates decay geometrically\nor polynomially. Moreover, our bounds of the expected value can be combined\nwith mean-concentration results to yield improved exponential tail probability\nbounds for the Wasserstein error of empirical measures under Bernstein-type or\nlog Sobolev-type conditions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 15:21:25 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 11:05:08 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1804.10611", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Antoine Channarond and Bruno Pelletier and\n  Nicolas Verzelen", "title": "On the Estimation of Latent Distances Using Graph Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.MG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given the adjacency matrix of a geometric graph and the task of\nrecovering the latent positions. We study one of the most popular approaches\nwhich consists in using the graph distances and derive error bounds under\nvarious assumptions on the link function. In the simplest case where the link\nfunction is proportional to an indicator function, the bound matches an\ninformation lower bound that we derive.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 17:49:46 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 22:48:12 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Channarond", "Antoine", ""], ["Pelletier", "Bruno", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1804.10821", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "On Convergence of Moments for Approximating Processes and Applications\n  to Surrogate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study critera for a pair $ (\\{ X_n \\} $, $ \\{ Y_n \\}) $ of approximating\nprocesses which guarantee closeness of moments by generalizing known results\nfor the special case that $ Y_n = Y $ for all $n$ and $ X_n $ converges to $Y$\nin probability. This problem especially arises when working with surrogate\nmodels, e.g. to enrich observed data by simulated data, where the surrogates\n$Y_n$'s are constructed to justify that they approximate the $ X_n $'s.\n  The results of this paper deal with sequences of random variables. Since this\nframework does not cover many applications where surrogate models such as deep\nneural networks are used to approximate more general stochastic processes, we\nextend the results to the more general framework of random fields of stochastic\nprocesses. This framework especially covers image data and sequences of images.\nWe show that uniform integrability is sufficient, and this holds even for the\ncase of processes provided they satisfy a weak stationarity condition.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 15:36:56 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1804.10887", "submitter": "Yuchao Liu", "authors": "Yuchao Liu, Jiaqi Guo", "title": "Distribution-Free, Size Adaptive Submatrix Detection with Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large matrix containing independent data entries, we consider the\nproblem of detecting a submatrix inside the data matrix that contains\nlarger-than-usual values. Different from previous literature, we do not have\nexact information about the dimension of the potential elevated submatrix. We\npropose a Bonferroni type testing procedure based on permutation tests, and\nshow that our proposed test loses no first-order asymptotic power compared to\ntests with full knowledge of potential elevated submatrix. In order to speed up\nthe calculation during the test, an approximation net is constructed and we\nshow that Bonferroni type permutation test on the approximation net loses no\npower on the first order asymptotically.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 08:00:53 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 07:16:54 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Yuchao", ""], ["Guo", "Jiaqi", ""]]}, {"id": "1804.10889", "submitter": "Chengcheng Huang", "authors": "Chengcheng Huang, Housen Li, Lizhi Cheng and Wei Peng", "title": "A linear time algorithm for multiscale quantile simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point problems have appeared in a great many applications for example\ncancer genetics, econometrics and climate change. Modern multiscale type\nsegmentation methods are considered to be a statistically efficient approach\nfor multiple change-point detection, which minimize the number of change-points\nunder a multiscale side-constraint. The constraint threshold plays a critical\nrole in balancing the data-fit and model complexity. However, the computation\ntime of such a threshold is quadratic in terms of sample size $n$, making it\nimpractical for large scale problems. In this paper we proposed an\n$\\mathcal{O}(n)$ algorithm by utilizing the hidden quasiconvexity structure of\nthe problem. It applies to all regression models in exponential family with\narbitrary convex scale penalties. Simulations verify its computational\nefficiency and accuracy. An implementation is provided in R-package \"linearQ\"\non CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 08:15:36 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 09:09:07 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Huang", "Chengcheng", ""], ["Li", "Housen", ""], ["Cheng", "Lizhi", ""], ["Peng", "Wei", ""]]}, {"id": "1804.10928", "submitter": "Tanujit Chakraborty", "authors": "Tanujit Chakraborty and Ashis Kumar Chakraborty and C.A. Murthy", "title": "A Nonparametric Ensemble Binary Classifier and its Statistical\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an ensemble of classification trees (CT) and\nartificial neural networks (ANN). Several statistical properties including\nuniversal consistency and upper bound of an important parameter of the proposed\nclassifier are shown. Numerical evidence is also provided using various real\nlife data sets to assess the performance of the model. Our proposed\nnonparametric ensemble classifier doesn't suffer from the `curse of\ndimensionality' and can be used in a wide variety of feature selection cum\nclassification problems. Performance of the proposed model is quite better when\ncompared to many other state-of-the-art models used for similar situations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 13:41:58 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:01:02 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Chakraborty", "Ashis Kumar", ""], ["Murthy", "C. A.", ""]]}, {"id": "1804.10948", "submitter": "Philippe  Soulier", "authors": "Clemonell Bilayi-Biakana, Rafal Kulik, Philippe Soulier", "title": "Statistical inference for heavy tailed series with extremal independence", "comments": null, "journal-ref": null, "doi": "10.1007/s10687-019-00365-z", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stationary time series $\\{X_j, j \\in Z\\} whose finite dimensional\ndistributions are regularly varying with extremal independence. We assume that\nfor each $h \\geq 1$, conditionally on $X_0$ to exceed a threshold tending to\ninfinity, the conditional distribution of $X_h$ suitably normalized converges\nweakly to a non degenerate distribution. We consider in this paper the\nestimation of the normalization and of the limiting distribution.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:55:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bilayi-Biakana", "Clemonell", ""], ["Kulik", "Rafal", ""], ["Soulier", "Philippe", ""]]}, {"id": "1804.10950", "submitter": "Abhijit Mandal", "authors": "Ayanendranath Basu, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "A Robust Wald-type Test for Testing the Equality of Two Means from\n  Log-Normal Samples", "comments": null, "journal-ref": null, "doi": "10.1007/s11009-018-9639-y", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-normal distribution is one of the most common distributions used for\nmodeling skewed and positive data. It frequently arises in many disciplines of\nscience, specially in the biological and medical sciences. The statistical\nanalysis for comparing the means of two independent log-normal distributions is\nan issue of significant interest. In this paper we present a robust test for\nthis problem. The unknown parameters of the model are estimated by minimum\ndensity power divergence estimators (Basu et al 1998, Biometrika, 85(3),\n549-559). The robustness as well as the asymptotic properties of the proposed\ntest statistics are rigorously established. The performance of the test is\nexplored through simulations and real data analysis. The test is compared with\nsome existing methods, and it is demonstrated that the proposed test\noutperforms the others in the presence of outliers.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:59:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1804.11249", "submitter": "Julia Shore", "authors": "Julia A. Shore, Jeremy G. Sumner and Barbara R. Holland", "title": "The impracticalities of multiplicatively-closed codon models: a retreat\n  to linear alternatives", "comments": null, "journal-ref": "Journal of Mathematical Biology, 1-25 (2020)", "doi": null, "report-no": null, "categories": "q-bio.PE math.RA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matrix Lie algebra is a linear space of matrices closed under the operation\n$ [A, B] = AB-BA $. The \"Lie closure\" of a set of matrices is the smallest\nmatrix Lie algebra which contains the set. In the context of Markov chain\ntheory, if a set of rate matrices form a Lie algebra, their corresponding\nMarkov matrices are closed under matrix multiplication; this has been found to\nbe a useful property in phylogenetics. Inspired by previous research involving\nLie closures of DNA models, it was hypothesised that finding the Lie closure of\na codon model could help to solve the problem of mis-estimation of the\nnon-synonymous/synonymous rate ratio, $ \\omega $. We propose two different\nmethods of finding a linear space from a model: the first is the \\emph{linear\nclosure} which is the smallest linear space which contains the model, and the\nsecond is the \\emph{linear version} which changes multiplicative constraints in\nthe model to additive ones. For each of these linear spaces we then find the\nLie closures of them. Under both methods, it was found that closed codon models\nwould require thousands of parameters, and that any partial solution to this\nproblem that was of a reasonable size violated stochasticity. Investigation of\ntoy models indicated that finding the Lie closure of matrix linear spaces which\ndeviated only slightly from a simple model resulted in a Lie closure that was\nclose to having the maximum number of parameters possible. Given that Lie\nclosures are not practical, we propose further consideration of the two\nvariants of linearly closed models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 00:43:18 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 05:52:32 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 00:54:05 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Shore", "Julia A.", ""], ["Sumner", "Jeremy G.", ""], ["Holland", "Barbara R.", ""]]}, {"id": "1804.11267", "submitter": "Moritz Schauer", "authors": "Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian inference for Gamma-type L\\'evy subordinators", "comments": null, "journal-ref": "Communications in Mathematical Sciences, Volume 17, Number 3, 2019", "doi": "10.4310/CMS.2019.v17.n3.a8", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given discrete time observations over a growing time interval, we consider a\nnonparametric Bayesian approach to estimation of the L\\'evy density of a L\\'evy\nprocess belonging to a flexible class of infinite activity subordinators.\nPosterior inference is performed via MCMC, and we circumvent the problem of the\nintractable likelihood via the data augmentation device, that in our case\nrelies on bridge process sampling via Gamma process bridges. Our approach also\nrequires the use of a new infinite-dimensional form of a reversible jump MCMC\nalgorithm. We show that our method leads to good practical results in\nchallenging simulation examples. On the theoretical side, we establish that our\nnonparametric Bayesian procedure is consistent: in the low frequency data\nsetting, with equispaced in time observations and intervals between successive\nobservations remaining fixed, the posterior asymptotically, as the sample size\n$n\\rightarrow\\infty$, concentrates around the L\\'evy density under which the\ndata have been generated. Finally, we test our method on a classical insurance\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:12:23 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 17:59:48 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Belomestny", "Denis", ""], ["Gugushvili", "Shota", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}]