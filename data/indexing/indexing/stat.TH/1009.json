[{"id": "1009.0101", "submitter": "Bent J{\\o}rgensen", "authors": "Bent J{\\o}rgensen, J. Ra\\'ul Mart\\'inez and Clarice G. B. Dem\\'etrio", "title": "Self-Similarity and Lamperti Convergence for Families of Stochastic\n  Processes", "comments": "23 pages. IMADA preprint 2010-09-01", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new type of self-similarity for one-parameter families of\nstochastic processes, which applies to a number of important families of\nprocesses that are not self-similar in the conventional sense. This includes a\nnew class of fractional Hougaard motions defined as moving averages of Hougaard\nL\\'evy process, as well as some well-known families of Hougaard L\\'evy\nprocesses such as the Poisson processes, Brownian motions with drift, and the\ninverse Gaussian processes. Such families have many properties in common with\nordinary self-similar processes, including the form of their covariance\nfunctions, and the fact that they appear as limits in a Lamperti-type limit\ntheorem for families of stochastic processes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 07:17:46 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["J\u00f8rgensen", "Bent", ""], ["Mart\u00ednez", "J. Ra\u00fal", ""], ["Dem\u00e9trio", "Clarice G. B.", ""]]}, {"id": "1009.0473", "submitter": "Eberhard Mayerhofer", "authors": "Eberhard Mayerhofer", "title": "On the existence of non-central Wishart distributions", "comments": "This version contains an Appendix which explains the relation of my\n  definition of non-central Wishart distributions to alternative ones from the\n  standard literature", "journal-ref": "Journal of Multivariate Analysis 114 (2013) pages 448-456", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the existence issue of non-central Wishart\ndistributions which is a research topic initiated by Wishart (1928), and with\nimportant contributions by e.g., L\\'evy (1937), Gindikin (1975), Shanbhag\n(1988), Peddada and Richards (1991). We present a new method involving the\ntheory of affine Markov processes, which reveals joint necessary conditions on\nshape and non-centrality parameter. While Eaton's conjecture concerning the\nnecessary range of the shape parameter is confirmed, we also observe that it is\nnot sufficient anymore that it only belongs to the Gindikin ensemble, as is in\nthe central case.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 16:55:16 GMT"}, {"version": "v2", "created": "Tue, 14 Sep 2010 21:07:15 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2011 14:30:59 GMT"}, {"version": "v4", "created": "Wed, 5 Oct 2011 08:06:14 GMT"}, {"version": "v5", "created": "Thu, 5 Jul 2012 09:52:51 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mayerhofer", "Eberhard", ""]]}, {"id": "1009.0530", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, Philipp Rutimann, Min Xu, and Peter Buhlmann", "title": "High-dimensional covariance estimation based on Gaussian graphical\n  models", "comments": "50 Pages, 6 figures. Major revision", "journal-ref": "Journal of Machine Learning Research, Volume 12, pp 2975-3026,\n  2011", "doi": null, "report-no": "University of Michigan, Department of Statistics Technical Report\n  512", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphs are often used to describe high dimensional distributions.\nUnder sparsity conditions, the graph can be estimated using\n$\\ell_1$-penalization methods. We propose and study the following method. We\ncombine a multiple regression approach with ideas of thresholding and\nrefitting: first we infer a sparse undirected graphical model structure via\nthresholding of each among many $\\ell_1$-norm penalized regression functions;\nwe then estimate the covariance matrix and its inverse using the maximum\nlikelihood estimator. We show that under suitable conditions, this approach\nyields consistent estimation in terms of graphical structure and fast\nconvergence rates with respect to the operator and Frobenius norm for the\ncovariance matrix and its inverse. We also derive an explicit bound for the\nKullback Leibler divergence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 20:06:40 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2011 22:49:08 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Zhou", "Shuheng", ""], ["Rutimann", "Philipp", ""], ["Xu", "Min", ""], ["Buhlmann", "Peter", ""]]}, {"id": "1009.0562", "submitter": "Andrew Nobel", "authors": "Xing Sun and Andrew B. Nobel", "title": "On the maximal size of Large-Average and ANOVA-fit Submatrices in a\n  Gaussian Random Matrix", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the maximal size of distinguished submatrices of a Gaussian\nrandom matrix. Of interest are submatrices whose entries have average greater\nthan or equal to a positive constant, and submatrices whose entries are\nwell-fit by a two-way ANOVA model. We identify size thresholds and associated\n(asymptotic) probability bounds for both large-average and ANOVA-fit\nsubmatrices. Results are obtained when the matrix and submatrices of interest\nare square, and in rectangular cases when the matrix submatrices of interest\nhave fixed aspect ratios. In addition, we obtain a strong, interval\nconcentration result for the size of large average submatrices in the square\ncase. A simulation study shows good agreement between the observed and\npredicted sizes of large average submatrices in matrices of moderate size.\n", "versions": [{"version": "v1", "created": "Fri, 3 Sep 2010 00:58:42 GMT"}], "update_date": "2010-09-06", "authors_parsed": [["Sun", "Xing", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1009.0679", "submitter": "Houman Owhadi", "authors": "Houman Owhadi, Clint Scovel, Timothy John Sullivan, Mike McKerns and\n  Michael Ortiz", "title": "Optimal Uncertainty Quantification", "comments": "90 pages. Accepted for publication in SIAM Review (Expository\n  Research Papers). See SIAM Review for higher quality figures", "journal-ref": "SIAM Rev. 55(2):271--345, 2013", "doi": "10.1137/10080782X", "report-no": null, "categories": "math.PR cs.IT math.IT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rigorous framework for Uncertainty Quantification (UQ) in which\nthe UQ objectives and the assumptions/information set are brought to the\nforefront. This framework, which we call \\emph{Optimal Uncertainty\nQuantification} (OUQ), is based on the observation that, given a set of\nassumptions and information about the problem, there exist optimal bounds on\nuncertainties: these are obtained as values of well-defined optimization\nproblems corresponding to extremizing probabilities of failure, or of\ndeviations, subject to the constraints imposed by the scenarios compatible with\nthe assumptions and information. In particular, this framework does not\nimplicitly impose inappropriate assumptions, nor does it repudiate relevant\ninformation. Although OUQ optimization problems are extremely large, we show\nthat under general conditions they have finite-dimensional reductions. As an\napplication, we develop \\emph{Optimal Concentration Inequalities} (OCI) of\nHoeffding and McDiarmid type. Surprisingly, these results show that\nuncertainties in input parameters, which propagate to output uncertainties in\nthe classical sensitivity analysis paradigm, may fail to do so if the transfer\nfunctions (or probability distributions) are imperfectly known. We show how,\nfor hierarchical structures, this phenomenon may lead to the non-propagation of\nuncertainties or information across scales. In addition, a general algorithmic\nframework is developed for OUQ and is tested on the Caltech surrogate model for\nhypervelocity impact and on the seismic safety assessment of truss structures,\nsuggesting the feasibility of the framework for important complex systems. The\nintroduction of this paper provides both an overview of the paper and a\nself-contained mini-tutorial about basic concepts and issues of UQ.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 14:37:45 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2011 17:36:37 GMT"}, {"version": "v3", "created": "Wed, 23 May 2012 20:30:15 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Owhadi", "Houman", ""], ["Scovel", "Clint", ""], ["Sullivan", "Timothy John", ""], ["McKerns", "Mike", ""], ["Ortiz", "Michael", ""]]}, {"id": "1009.0796", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui and Rolando J. Biscay-Lirio", "title": "Dynamic interactions in terms of senders, hubs, and receivers (SHR)\n  using the singular value decomposition of time series: Theory and brain\n  connectivity applications", "comments": "Technical report 2010-September-04, The KEY Institute for Brain-Mind\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.bio-ph q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Understanding of normal and pathological brain function requires the\nidentification and localization of functional connections between specialized\nregions. The availability of high time resolution signals of electric neuronal\nactivity at several regions offers information for quantifying the connections\nin terms of information flow. When the signals cover the whole cortex, the\nnumber of connections is very large, making visualization and interpretation\nvery difficult. We introduce here the singular value decomposition of\ntime-lagged multiple signals, which localizes the senders, hubs, and receivers\n(SHR) of information transmission. Unlike methods that operate on large\nconnectivity matrices, such as correlation thresholding and graph-theoretic\nanalyses, this method operates on the multiple time series directly, providing\n3D brain images that assign a score to each location in terms of its sending,\nrelaying, and receiving capacity. The scope of the method is general and\nencompasses other applications outside the field of brain connectivity.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 01:26:31 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 00:36:11 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay-Lirio", "Rolando J.", ""]]}, {"id": "1009.0802", "submitter": "Richard D. Gill", "authors": "Richard D. Gill, Piet Groeneboom, Peter de Jong", "title": "Elementary Statistics on Trial (the case of Lucia de Berk)", "comments": "10 pages, 3 figures and 2 tables", "journal-ref": "CHANCE, 31:4, 9-15 (2018)", "doi": "10.1080/09332480.2018.1549809", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the conviction of Lucia de Berk an important role was played by a simple\nhypergeometric model, used by the expert consulted by the court, which produced\nvery small probabilities of occurrences of certain numbers of incidents. We\nwant to draw attention to the fact that, if we take into account the variation\namong nurses in incidents they experience during their shifts, these\nprobabilities can become considerably larger. This points to the danger of\nusing an oversimplified discrete probability model in these circumstances.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 03:41:12 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 11:52:25 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 15:43:12 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gill", "Richard D.", ""], ["Groeneboom", "Piet", ""], ["de Jong", "Peter", ""]]}, {"id": "1009.0805", "submitter": "Christian Robert Y", "authors": "Paul Doukhan, Silika Prohl, Christian Y. Robert", "title": "Subsampling weakly dependent times series and application to extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides extensions of the work on subsampling by Bertail et al.\n(2004) for strongly mixing case to weakly dependent case by application of the\nresults of Doukhan and Louhichi (1999). We investigate properties of smooth and\nrough subsampling estimators for distributions of converging and extreme\nstatistics when the underlying time series is {\\eta} or {\\lambda}-weakly\ndependent.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 05:16:40 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Doukhan", "Paul", ""], ["Prohl", "Silika", ""], ["Robert", "Christian Y.", ""]]}, {"id": "1009.0891", "submitter": "Song Cai", "authors": "Song Cai, James V. Zidek, Nathaniel Newlands", "title": "Predicting Sequences of Progressive Events Times with Time-dependent\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to modeling progressive event-history data\nwhen the overall objective is prediction based on time-dependent covariates.\nThis approach does not model the hazard function directly. Instead, it models\nthe process of the state indicators of the event history so that the\ntime-dependent covariates can be incorporated and predictors of the future\nevents easily formulated. Our model can be applied to a range of real-world\nproblems in medical and agricultural science.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 04:59:15 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Cai", "Song", ""], ["Zidek", "James V.", ""], ["Newlands", "Nathaniel", ""]]}, {"id": "1009.0906", "submitter": "Zvika Ben-Haim", "authors": "Zvika Ben-Haim and Yonina C. Eldar", "title": "Near-Oracle Performance of Greedy Block-Sparse Estimation Techniques\n  from Noisy Measurements", "comments": "15 pages, 2 figures. Submitted to IEEE J. Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2011.2160250", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the ability of greedy algorithms to estimate a block\nsparse parameter vector from noisy measurements. In particular, block sparse\nversions of the orthogonal matching pursuit and thresholding algorithms are\nanalyzed under both adversarial and Gaussian noise models. In the adversarial\nsetting, it is shown that estimation accuracy comes within a constant factor of\nthe noise power. Under Gaussian noise, the Cramer-Rao bound is derived, and it\nis shown that the greedy techniques come close to this bound at high SNR. The\nguarantees are numerically compared with the actual performance of block and\nnon-block algorithms, highlighting the advantages of block sparse techniques.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 10:51:34 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ben-Haim", "Zvika", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1009.1016", "submitter": "Alexander Goldenshluger", "authors": "Alexander Goldenshluger, Oleg Lepski", "title": "Bandwidth selection in kernel density estimation: Oracle inequalities\n  and adaptive minimax optimality", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS883 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 3, 1608-1632", "doi": "10.1214/11-AOS883", "report-no": "IMS-AOS-AOS883", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of density estimation with $\\mathbb{L}_s$-loss by\nselection of kernel estimators. We develop a selection procedure and derive\ncorresponding $\\mathbb{L}_s$-risk oracle inequalities. It is shown that the\nproposed selection rule leads to the estimator being minimax adaptive over a\nscale of the anisotropic Nikol'skii classes. The main technical tools used in\nour derivations are uniform bounds on the $\\mathbb{L}_s$-norms of empirical\nprocesses developed recently by Goldenshluger and Lepski [Ann. Probab. (2011),\nto appear].\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 10:42:50 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2012 08:57:43 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Goldenshluger", "Alexander", ""], ["Lepski", "Oleg", ""]]}, {"id": "1009.1052", "submitter": "Zhiyi Chi", "authors": "Zhiyi Chi", "title": "A local stochastic Lipschitz condition with application to Lasso for\n  high dimensional generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For regularized estimation, the upper tail behavior of the random Lipschitz\ncoefficient associated with empirical loss functions is known to play an\nimportant role in the error bound of Lasso for high dimensional generalized\nlinear models. The upper tail behavior is known for linear models but much less\nso for nonlinear models. We establish exponential type inequalities for the\nupper tail of the coefficient and illustrate an application of the results to\nLasso likelihood estimation for high dimensional generalized linear models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 13:21:30 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Chi", "Zhiyi", ""]]}, {"id": "1009.1312", "submitter": "Nickos Papadatos", "authors": "Nickos Papadatos", "title": "Linear Estimation of Location and Scale Parameters Using Partial Maxima", "comments": "This article is devoted to the memory of my six-years-old, little\n  daughter, Dionyssia, who leaved us on August 25, 2010, at Cephalonia isl. (26\n  pages, to appear in Metrika)", "journal-ref": "Metrika (2012), vol. 75, pp. 243-270", "doi": "10.1007/s00184-010-0325-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an i.i.d. sample X^*_1,X^*_2,...,X^*_n from a location-scale family,\nand assume that the only available observations consist of the partial maxima\n(or minima)sequence, X^*_{1:1},X^*_{2:2},...,X^*_{n:n}, where\nX^*_{j:j}=max{X^*_1,...,X^*_j}. This kind of truncation appears in several\ncircumstances, including best performances in athletics events. In the case of\npartial maxima, the form of the BLUEs (best linear unbiased estimators) is\nquite similar to the form of the well-known Lloyd's (1952, Least-squares\nestimation of location and scale parameters using order statistics, Biometrika,\nvol. 39, pp. 88-95) BLUEs, based on (the sufficient sample of) order\nstatistics, but, in contrast to the classical case, their consistency is no\nlonger obvious. The present paper is mainly concerned with the scale parameter,\nshowing that the variance of the partial maxima BLUE is at most of order\nO(1/log n), for a wide class of distributions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 14:41:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Papadatos", "Nickos", ""]]}, {"id": "1009.1370", "submitter": "Dominique Bontemps", "authors": "Dominique Bontemps (LM-Orsay)", "title": "Bernstein von Mises Theorems for Gaussian Regression with increasing\n  number of regressors", "comments": null, "journal-ref": "Annals of Statistics 39, 5 (2011) 2557-2584", "doi": "10.1214/11-AOS912", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper brings a contribution to the Bayesian theory of nonparametric and\nsemiparametric estimation. We are interested in the asymptotic normality of the\nposterior distribution in Gaussian linear regression models when the number of\nregressors increases with the sample size. Two kinds of Bernstein-von Mises\nTheorems are obtained in this framework: nonparametric theorems for the\nparameter itself, and semiparametric theorems for functionals of the parameter.\nWe apply them to the Gaussian sequence model and to the regression of functions\nin Sobolev and $C^{\\alpha}$ classes, in which we get the minimax convergence\nrates. Adaptivity is reached for the Bayesian estimators of functionals in our\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 19:11:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2011 13:51:45 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2012 07:43:17 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Bontemps", "Dominique", "", "LM-Orsay"]]}, {"id": "1009.1706", "submitter": "Nicolas Verzelen", "authors": "Yuri I. Ingster (LETI), Alexandre B. Tsybakov (CREST, PMA), Nicolas\n  Verzelen (MISTEA)", "title": "Detection boundary in sparse regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detection of a p-dimensional sparse vector of\nparameters in the linear regression model with Gaussian noise. We establish the\ndetection boundary, i.e., the necessary and sufficient conditions for the\npossibility of successful detection as both the sample size n and the dimension\np tend to the infinity. Testing procedures that achieve this boundary are also\nexhibited. Our results encompass the high-dimensional setting (p>> n). The main\nmessage is that, under some conditions, the detection boundary phenomenon that\nhas been proved for the Gaussian sequence model, extends to high-dimensional\nlinear regression. Finally, we establish the detection boundaries when the\nvariance of the noise is unknown. Interestingly, the detection boundaries\nsometimes depend on the knowledge of the variance in a high-dimensional\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 08:06:30 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Ingster", "Yuri I.", "", "LETI"], ["Tsybakov", "Alexandre B.", "", "CREST, PMA"], ["Verzelen", "Nicolas", "", "MISTEA"]]}, {"id": "1009.1751", "submitter": "Jan Vyb\\'iral", "authors": "Jan Vyb\\'iral", "title": "Average best $m$-term approximation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of average best $m$-term approximation widths with\nrespect to a probability measure on the unit ball of $\\ell_p^n$. We estimate\nthese quantities for the embedding $id:\\ell_p^n\\to\\ell_q^n$ with $0<p\\le q\\le\n\\infty$ for the normalized cone and surface measure. Furthermore, we consider\ncertain tensor product weights and show that a typical vector with respect to\nsuch a measure exhibits a strong compressible (i.e. nearly sparse) structure.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 12:29:07 GMT"}, {"version": "v2", "created": "Tue, 2 Nov 2010 09:13:17 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2012 12:33:57 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Vyb\u00edral", "Jan", ""]]}, {"id": "1009.1926", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama and William E. Strawderman", "title": "Robust Bayesian variable selection with sub-harmonic priors", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Bayesian variable selection in linear models with general\nspherically symmetric error distributions. We propose sub-harmonic priors which\narise as a class of mixtures of Zellner's g-priors for which the Bayes factors\nare independent of the underlying error distribution, as long as it is in the\nspherically symmetric class. Because of this invariance to spherically\nsymmetric error distribution, we refer to our method as a robust Bayesian\nvariable selection method. We demonstrate that our Bayes factors have model\nselection consistency and are coherent. We also develop Laplace approximations\nto Bayes factors for a number of recently studied mixtures of g-priors that\nhave recently appeared in the literature (including our own) for Gaussian\nerrors. These approximations, in each case, are given by the Gaussian Bayes\nfactor based on BIC times a simple rational function of the prior's\nhyper-parameters and the R^2's for the respective models. We also extend model\nselection consistency for several g-prior based Bayes factor methods for\nGaussian errors to the entire class of spherically symmetric error\ndistributions. Additionally we demonstrate that our class of sub-harmonic\npriors are the only ones within a large class of mixtures of g-priors studied\nin the literature which are robust in our sense. A simulation study and an\nanalysis of two real data sets indicates good performance of our robust Bayes\nfactors relative to BIC and to other mixture of g-prior based methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 03:59:24 GMT"}, {"version": "v2", "created": "Mon, 20 Sep 2010 02:04:50 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2011 02:01:40 GMT"}, {"version": "v4", "created": "Mon, 11 Mar 2013 02:28:43 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "1009.2022", "submitter": "Luis Mendo", "authors": "Luis Mendo", "title": "Estimation of a probability in inverse binomial sampling under\n  normalized linear-linear and inverse-linear loss", "comments": "4 figures", "journal-ref": "Test (2012), 21, pp. 656--675", "doi": "10.1007/s11749-011-0267-x", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential estimation of the success probability $p$ in inverse binomial\nsampling is considered in this paper. For any estimator $\\hat p$, its quality\nis measured by the risk associated with normalized loss functions of\nlinear-linear or inverse-linear form. These functions are possibly asymmetric,\nwith arbitrary slope parameters $a$ and $b$ for $\\hat p<p$ and $\\hat p>p$\nrespectively. Interest in these functions is motivated by their significance\nand potential uses, which are briefly discussed. Estimators are given for which\nthe risk has an asymptotic value as $p$ tends to $0$, and which guarantee that,\nfor any $p$ in $(0,1)$, the risk is lower than its asymptotic value. This\nallows selecting the required number of successes, $r$, to meet a prescribed\nquality irrespective of the unknown $p$. In addition, the proposed estimators\nare shown to be approximately minimax when $a/b$ does not deviate too much from\n$1$, and asymptotically minimax as $r$ tends to infinity when $a=b$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 14:35:47 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 12:20:19 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2013 10:03:38 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Mendo", "Luis", ""]]}, {"id": "1009.2048", "submitter": "Olivier Catoni", "authors": "Olivier Catoni", "title": "Challenging the empirical mean and empirical variance: a deviation study", "comments": "Second version presents an improved variance estimate in section 4.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new M-estimators of the mean and variance of real valued random\nvariables, based on PAC-Bayes bounds. We analyze the non-asymptotic minimax\nproperties of the deviations of those estimators for sample distributions\nhaving either a bounded variance or a bounded variance and a bounded kurtosis.\nUnder those weak hypotheses, allowing for heavy-tailed distributions, we show\nthat the worst case deviations of the empirical mean are suboptimal. We prove\nindeed that for any confidence level, there is some M-estimator whose\ndeviations are of the same order as the deviations of the empirical mean of a\nGaussian statistical sample, even when the statistical sample is instead\nheavy-tailed. Experiments reveal that these new estimators perform even better\nthan predicted by our bounds, showing deviation quantile functions uniformly\nlower at all probability levels than the empirical mean for non Gaussian sample\ndistributions as simple as the mixture of two Gaussian measures.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 16:38:58 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2011 15:48:26 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Catoni", "Olivier", ""]]}, {"id": "1009.2065", "submitter": "Stephen Becker", "authors": "Stephen R. Becker, Emmanuel J. Cand\\`es, Michael Grant", "title": "Templates for Convex Cone Problems with Applications to Sparse Signal\n  Recovery", "comments": "The TFOCS software is available at http://tfocs.stanford.edu This\n  version has updated references", "journal-ref": "Mathematical Programming Computation, Volume 3, Number 3, 165-218,\n  2011", "doi": "10.1007/s12532-011-0029-5", "report-no": null, "categories": "math.OC math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for solving a variety of convex cone\nproblems that frequently arise in signal processing, machine learning,\nstatistics, and other fields. The approach works as follows: first, determine a\nconic formulation of the problem; second, determine its dual; third, apply\nsmoothing; and fourth, solve using an optimal first-order method. A merit of\nthis approach is its flexibility: for example, all compressed sensing problems\ncan be solved via this approach. These include models with objective\nfunctionals such as the total-variation norm, ||Wx||_1 where W is arbitrary, or\na combination thereof. In addition, the paper also introduces a number of\ntechnical contributions such as a novel continuation scheme, a novel approach\nfor controlling the step size, and some new results showing that the smooth and\nunsmoothed problems are sometimes formally equivalent. Combined with our\nframework, these lead to novel, stable and computationally efficient\nalgorithms. For instance, our general implementation is competitive with\nstate-of-the-art methods for solving intensively studied problems such as the\nLASSO. Further, numerical experiments show that one can solve the Dantzig\nselector problem, for which no efficient large-scale solvers exist, in a few\nhundred iterations. Finally, the paper is accompanied with a software release.\nThis software is not a single, monolithic solver; rather, it is a suite of\nprograms and routines designed to serve as building blocks for constructing\ncomplete algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 17:47:58 GMT"}, {"version": "v2", "created": "Tue, 5 Oct 2010 23:54:02 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2011 15:42:52 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Becker", "Stephen R.", ""], ["Cand\u00e8s", "Emmanuel J.", ""], ["Grant", "Michael", ""]]}, {"id": "1009.2111", "submitter": "Guang Cheng", "authors": "Guang Cheng", "title": "How Many Iterations are Sufficient for Semiparametric Estimation?", "comments": "42 pages, submitted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common practice in obtaining a semiparametric efficient estimate is through\niteratively maximizing the (penalized) log-likelihood w.r.t. its Euclidean\nparameter and functional nuisance parameter via Newton-Raphson algorithm. The\npurpose of this paper is to provide a formula in calculating the minimal number\nof iterations $k^\\ast$ needed to produce an efficient estimate\n$\\hat\\theta_n^{(k^\\ast)}$ from a theoretical point of view. We discover that\n(a) $k^\\ast$ depends on the convergence rates of the initial estimate and\nnuisance estimate; (b) more than $k^\\ast$ iterations, i.e., $k$, will only\nimprove the higher order asymptotic efficiency of $\\hat\\theta_n^{(k)}$; (c)\n$k^\\ast$ iterations are also sufficient for recovering the estimation sparsity\nin high dimensional data. These general conclusions hold, in particular, when\nthe nuisance parameter is not estimable at root-n rate, and apply to\nsemiparametric models estimated under various regularizations, e.g., kernel or\npenalized estimation. This paper provides a first general theoretical\njustification for the \"one-/two-step iteration\" phenomena observed in the\nliterature, and may be useful in reducing the bootstrap computational cost for\nthe semiparametric models.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 22:04:20 GMT"}, {"version": "v2", "created": "Wed, 22 Sep 2010 03:46:42 GMT"}], "update_date": "2010-09-23", "authors_parsed": [["Cheng", "Guang", ""]]}, {"id": "1009.2118", "submitter": "Sahand Negahban", "authors": "Sahand Negahban and Martin J. Wainwright", "title": "Restricted strong convexity and weighted matrix completion: Optimal\n  bounds with noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the matrix completion problem under a form of row/column weighted\nentrywise sampling, including the case of uniform entrywise sampling as a\nspecial case. We analyze the associated random observation operator, and prove\nthat with high probability, it satisfies a form of restricted strong convexity\nwith respect to weighted Frobenius norm. Using this property, we obtain as\ncorollaries a number of error bounds on matrix completion in the weighted\nFrobenius norm under noisy sampling and for both exact and near low-rank\nmatrices. Our results are based on measures of the \"spikiness\" and\n\"low-rankness\" of matrices that are less restrictive than the incoherence\nconditions imposed in previous work. Our technique involves an $M$-estimator\nthat includes controls on both the rank and spikiness of the solution, and we\nestablish non-asymptotic error bounds in weighted Frobenius norm for recovering\nmatrices lying with $\\ell_q$-\"balls\" of bounded spikiness. Using\ninformation-theoretic methods, we show that no algorithm can achieve better\nestimates (up to a logarithmic factor) over these same sets, showing that our\nconditions on matrices and associated rates are essentially optimal.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 23:08:58 GMT"}, {"version": "v2", "created": "Sun, 15 May 2011 17:30:12 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Negahban", "Sahand", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1009.2164", "submitter": "Takanori Sugiyama", "authors": "Takanori Sugiyama, Peter S. Turner, Mio Murao", "title": "Error probability analysis in quantum tomography: a tool for evaluating\n  experiments", "comments": "14pages, 2 figures (an analysis of an example is added, and the proof\n  of Lemma 2 is corrected)", "journal-ref": "Phys. Rev. A 83, 012105 (2011)", "doi": "10.1103/PhysRevA.83.012105", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand the scope of the statistical notion of error probability, i.e., how\noften large deviations are observed in an experiment, in order to make it\ndirectly applicable to quantum tomography. We verify that the error probability\ncan decrease at most exponentially in the number of trials, derive the explicit\nrate that bounds this decrease, and show that a maximum likelihood estimator\nachieves this bound. We also show that the statistical notion of\nidentifiability coincides with the tomographic notion of informational\ncompleteness. Our result implies that two quantum tomographic apparatuses that\nhave the same risk function, (e.g. variance), can have different error\nprobability, and we give an example in one qubit state tomography. Thus by\ncombining these two approaches we can evaluate, in a reconstruction independent\nway, the performance of such experiments more discerningly.\n", "versions": [{"version": "v1", "created": "Sat, 11 Sep 2010 13:17:55 GMT"}, {"version": "v2", "created": "Fri, 3 Dec 2010 21:20:34 GMT"}], "update_date": "2011-01-24", "authors_parsed": [["Sugiyama", "Takanori", ""], ["Turner", "Peter S.", ""], ["Murao", "Mio", ""]]}, {"id": "1009.2439", "submitter": "Vladimir Koltchinskii", "authors": "Vladimir Koltchinskii", "title": "Von Neumann Entropy Penalization and Low Rank Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem of statistical estimation of a Hermitian nonnegatively definite\nmatrix of unit trace (for instance, a density matrix in quantum state\ntomography) is studied. The approach is based on penalized least squares method\nwith a complexity penalty defined in terms of von Neumann entropy. A number of\noracle inequalities have been proved showing how the error of the estimator\ndepends on the rank and other characteristics of the oracles. The methods of\nproofs are based on empirical processes theory and probabilistic inequalities\nfor random matrices, in particular, noncommutative versions of Bernstein\ninequality.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 16:14:30 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Koltchinskii", "Vladimir", ""]]}, {"id": "1009.2651", "submitter": "Qiyu Sun", "authors": "Qiyu Sun and Michael Unser", "title": "Left-Inverses of Fractional Laplacian and Sparse Stochastic Processes", "comments": "Advances in Computational Mathematics, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fractional Laplacian $(-\\triangle)^{\\gamma/2}$ commutes with the primary\ncoordination transformations in the Euclidean space $\\RR^d$: dilation,\ntranslation and rotation, and has tight link to splines, fractals and stable\nLevy processes. For $0<\\gamma<d$, its inverse is the classical Riesz potential\n$I_\\gamma$ which is dilation-invariant and translation-invariant. In this work,\nwe investigate the functional properties (continuity, decay and invertibility)\nof an extended class of differential operators that share those invariance\nproperties. In particular, we extend the definition of the classical Riesz\npotential $I_\\gamma$ to any non-integer number $\\gamma$ larger than $d$ and\nshow that it is the unique left-inverse of the fractional Laplacian\n$(-\\triangle)^{\\gamma/2}$ which is dilation-invariant and\ntranslation-invariant. We observe that, for any $1\\le p\\le \\infty$ and\n$\\gamma\\ge d(1-1/p)$, there exists a Schwartz function $f$ such that $I_\\gamma\nf$ is not $p$-integrable. We then introduce the new unique left-inverse\n$I_{\\gamma, p}$ of the fractional Laplacian $(-\\triangle)^{\\gamma/2}$ with the\nproperty that $I_{\\gamma, p}$ is dilation-invariant (but not\ntranslation-invariant) and that $I_{\\gamma, p}f$ is $p$-integrable for any\nSchwartz function $f$. We finally apply that linear operator $I_{\\gamma, p}$\nwith $p=1$ to solve the stochastic partial differential equation\n$(-\\triangle)^{\\gamma/2} \\Phi=w$ with white Poisson noise as its driving term\n$w$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 13:17:51 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Sun", "Qiyu", ""], ["Unser", "Michael", ""]]}, {"id": "1009.2698", "submitter": "Michael Amrein", "authors": "Michael Amrein and Hans R. K\\\"unsch", "title": "Approximate variances for tapered spectral estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approximation of the asymptotic variance that removes a certain\ndiscontinuity in the usual formula for the raw and the smoothed periodogram in\ncase a data taper is used. It is based on an approximation of the covariance of\nthe (tapered) periodogram at two arbitrary frequencies. Exact computations of\nthe variances for a Gaussian white noise and an AR(4) process show that the\napproximation is more accurate than the usual formula.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 15:37:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 17:26:33 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Amrein", "Michael", ""], ["K\u00fcnsch", "Hans R.", ""]]}, {"id": "1009.2707", "submitter": "Karim Lounici", "authors": "Pierre Alquier, Karim Lounici", "title": "Pac-bayesian bounds for sparse regression estimation with exponential\n  weights", "comments": "19 pages", "journal-ref": "Electronic Journal of Statistics, Vol 5(2011), 127-145", "doi": "10.1214/11-EJS601", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sparse regression model where the number of parameters $p$ is\nlarger than the sample size $n$. The difficulty when considering\nhigh-dimensional problems is to propose estimators achieving a good compromise\nbetween statistical and computational performances. The BIC estimator for\ninstance performs well from the statistical point of view \\cite{BTW07} but can\nonly be computed for values of $p$ of at most a few tens. The Lasso estimator\nis solution of a convex minimization problem, hence computable for large value\nof $p$. However stringent conditions on the design are required to establish\nfast rates of convergence for this estimator. Dalalyan and Tsybakov\n\\cite{arnak} propose a method achieving a good compromise between the\nstatistical and computational aspects of the problem. Their estimator can be\ncomputed for reasonably large $p$ and satisfies nice statistical properties\nunder weak assumptions on the design. However, \\cite{arnak} proposes sparsity\noracle inequalities in expectation for the empirical excess risk only. In this\npaper, we propose an aggregation procedure similar to that of \\cite{arnak} but\nwith improved statistical performances. Our main theoretical result is a\nsparsity oracle inequality in probability for the true excess risk for a\nversion of exponential weight estimator. We also propose a MCMC method to\ncompute our estimator for reasonably large values of $p$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 16:17:29 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 14:53:23 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Alquier", "Pierre", ""], ["Lounici", "Karim", ""]]}, {"id": "1009.2898", "submitter": "Tamas Szantai", "authors": "Edith Kovacs and Tamas Szantai", "title": "Multivariate Copula Expressed by Lower Dimensional Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of high order multivariate probability distribution is a difficult\nproblem which occurs in many fields. Copula approach is a good choice for this\npurpose, but the curse of dimensionality still remains a problem. In this paper\nwe give a theorem which expresses a multivariate copula by using only some\nlower dimensional ones based on the conditional independences between the\nvariables. In general the construction of a multivariate copula using this\ntheorem is quite difficult, due the consistency properties which have to be\nfulfilled. For this purpose we introduce the sample derivated copula, and prove\nthat the dependence between the random variables involved depends just on this\ncopula and on the partition. By using the sample derivated copula the theorem\ncan be successfully applied, in order to to construct a multivariate discrete\ncopula by using some of its marginals.\n", "versions": [{"version": "v1", "created": "Wed, 15 Sep 2010 11:22:55 GMT"}], "update_date": "2010-09-16", "authors_parsed": [["Kovacs", "Edith", ""], ["Szantai", "Tamas", ""]]}, {"id": "1009.2943", "submitter": "Grigorios Pavliotis", "authors": "J. Nolen, G.A. Pavliotis, A.M. Stuart", "title": "Multiscale Modelling and Inverse Problems", "comments": "Submitted to the proceedings of the summer school \"Numerical Analysis\n  of Multiscale Problems\", edited by I.G. Graham, T.Y. Hou, O. Lakkis and R.\n  Scheichl. It will be published in the \"Lecture Notes in Computational Science\n  and Engineering\" series by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to blend observational data and mathematical models arises in many\napplications and leads naturally to inverse problems. Parameters appearing in\nthe model, such as constitutive tensors, initial conditions, boundary\nconditions, and forcing can be estimated on the basis of observed data. The\nresulting inverse problems are often ill-posed and some form of regularization\nis required. These notes discuss parameter estimation in situations where the\nunknown parameters vary across multiple scales. We illustrate the main ideas\nusing a simple model for groundwater flow.\n  We will highlight various approaches to regularization for inverse problems,\nincluding Tikhonov and Bayesian methods. We illustrate three ideas that arise\nwhen considering inverse problems in the multiscale context. The first idea is\nthat the choice of space or set in which to seek the solution to the inverse\nproblem is intimately related to whether a homogenized or full multiscale\nsolution is required. This is a choice of regularization. The second idea is\nthat, if a homogenized solution to the inverse problem is what is desired, then\nthis can be recovered from carefully designed observations of the full\nmultiscale system. The third idea is that the theory of homogenization can be\nused to improve the estimation of homogenized coefficients from multiscale\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 15 Sep 2010 14:37:29 GMT"}], "update_date": "2010-09-16", "authors_parsed": [["Nolen", "J.", ""], ["Pavliotis", "G. A.", ""], ["Stuart", "A. M.", ""]]}, {"id": "1009.3168", "submitter": "Jose A. Diaz-Garcia", "authors": "Jos\\'e A. D\\'iaz-Garc\\'ia and Francisco J. Caro-Lopera", "title": "Generalised shape theory via pseudo-Wishart distribution", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non isotropic noncentral elliptical shape distributions via\npseudo-Wishart distribution are founded. This way, the classical shape theory\nis extended to non isotropic case and the normality assumption is replaced by\nassuming a elliptical distribution. In several cases, the new shape\ndistributions are easily computable and then the inference procedure can be\nstudied under exact densities. An application in Biology is studied under the\nclassical gaussian approach and two non gaussian models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 13:03:45 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["D\u00edaz-Garc\u00eda", "Jos\u00e9 A.", ""], ["Caro-Lopera", "Francisco J.", ""]]}, {"id": "1009.3353", "submitter": "Alexander Jung", "authors": "Sebastian Schmutzhard, Alexander Jung, Franz Hlawatsch, Zvika\n  Ben-Haim, Yonina C. Eldar", "title": "A Lower Bound on the Estimator Variance for the Sparse Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of estimators of a sparse nonrandom vector based on\nan observation which is linearly transformed and corrupted by additive white\nGaussian noise. Using the reproducing kernel Hilbert space framework, we derive\na new lower bound on the estimator variance for a given differentiable bias\nfunction (including the unbiased case) and an almost arbitrary transformation\nmatrix (including the underdetermined case considered in compressed sensing\ntheory). For the special case of a sparse vector corrupted by white Gaussian\nnoise-i.e., without a linear transformation-and unbiased estimation, our lower\nbound improves on previously proposed bounds.\n", "versions": [{"version": "v1", "created": "Fri, 17 Sep 2010 07:38:37 GMT"}], "update_date": "2010-09-20", "authors_parsed": [["Schmutzhard", "Sebastian", ""], ["Jung", "Alexander", ""], ["Hlawatsch", "Franz", ""], ["Ben-Haim", "Zvika", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1009.3601", "submitter": "David Hardoon", "authors": "David R. Hardoon and Kristiaan Pelcksman", "title": "Pair-Wise Cluster Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning clusters which are consistently\npresent in different (continuously valued) representations of observed data.\nOur setup differs slightly from the standard approach of (co-) clustering as we\nuse the fact that some form of `labeling' becomes available in this setup: a\ncluster is only interesting if it has a counterpart in the alternative\nrepresentation. The contribution of this paper is twofold: (i) the problem\nsetting is explored and an analysis in terms of the PAC-Bayesian theorem is\npresented, (ii) a practical kernel-based algorithm is derived exploiting the\ninherent relation to Canonical Correlation Analysis (CCA), as well as its\nextension to multiple views. A content based information retrieval (CBIR) case\nstudy is presented on the multi-lingual aligned Europal document dataset which\nsupports the above findings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 02:28:35 GMT"}], "update_date": "2010-09-21", "authors_parsed": [["Hardoon", "David R.", ""], ["Pelcksman", "Kristiaan", ""]]}, {"id": "1009.4025", "submitter": "Shankar Bhamidi", "authors": "Shankar Bhamidi, Remco van der Hofstad, Gerard Hooghiemstra", "title": "Weak disorder in the stochastic mean-field model of distance II", "comments": "Published in at http://dx.doi.org/10.3150/11-BEJ402 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 2, 363-386", "doi": "10.3150/11-BEJ402", "report-no": "IMS-BEJ-BEJ402", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the complete graph $K_n$ with n vertices, where we\nattach an independent and identically distributed (i.i.d.) weight to each of\nthe n(n-1)/2 edges. We focus on the weight $W_n$ and the number of edges $H_n$\nof the minimal weight path between vertex 1 and vertex n. It is shown in (Ann.\nAppl. Probab. 22 (2012) 29-69) that when the weights on the edges are i.i.d.\nwith distribution equal to that of $E^s$, where $s>0$ is some parameter, and E\nhas an exponential distribution with mean 1, then $H_n$ is asymptotically\nnormal with asymptotic mean $s\\log n$ and asymptotic variance $s^2\\log n$. In\nthis paper, we analyze the situation when the weights have distribution\n$E^{-s},s>0$, in which case the behavior of $H_n$ is markedly different as\n$H_n$ is a tight sequence of random variables. More precisely, we use the\nmethod of Stein-Chen for Poisson approximations to show that, for almost all\n$s>0$, the hopcount $H_n$ converges in probability to the nearest integer of\ns+1 greater than or equal to 2, and identify the limiting distribution of the\nrecentered and rescaled minimal weight. For a countable set of special s values\ndenoted by $\\mathcal{S}=\\{s_j\\}_{j\\geq2}$, the hopcount $H_n$ takes on the\nvalues j and j+1 each with positive probability.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 09:21:12 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 06:37:49 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Bhamidi", "Shankar", ""], ["van der Hofstad", "Remco", ""], ["Hooghiemstra", "Gerard", ""]]}, {"id": "1009.4070", "submitter": "Shuyan Liu", "authors": "Shuyan Liu", "title": "Estimation of the spectral measure of multivariate regularly varying\n  distributions", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, the estimator for the spectral measure of multivariate stable\ndistributions introduced by Davydov and co-workers are extended to the\nregularly varying distributions. The sampling method is modified to optimize\nthe rate of convergence of estimator. An estimator of the total mass of\nspectral measure is proposed. The consistency and the asymptotic normality of\nestimators are proved.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 12:09:55 GMT"}], "update_date": "2010-09-22", "authors_parsed": [["Liu", "Shuyan", ""]]}, {"id": "1009.4217", "submitter": "Victoria Zinde-Walsh", "authors": "Victoria Zinde-Walsh", "title": "Measurement error and deconvolution in spaces of generalized functions", "comments": "This is a revised version of the paper \"Measurement Error and\n  Deconvolution in Generalized Functions Spaces\". The revision maintains the\n  focus on the two types of equations, the convolution equation and the system\n  of equations; this version does not include Theorems 6 and 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers convolution equations that arise from problems such as\nmeasurement error and non-parametric regression with errors in variables with\nindependence conditions. The equations are examined in spaces of generalized\nfunctions to account for possible singularities; this makes it possible to\nconsider densities for arbitrary and not only absolutely continuous\ndistributions, and to operate with Fourier transforms for polynomially growing\nregression functions. Results are derived for identification and well-posedness\nin the topology of generalized functions for the deconvolution problem and for\nsome regression models. Conditions for consistency of plug-in estimation for\nthese models are derived.\n", "versions": [{"version": "v1", "created": "Tue, 21 Sep 2010 21:07:03 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 18:40:48 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Zinde-Walsh", "Victoria", ""]]}, {"id": "1009.4279", "submitter": "Elena Stanghellini", "authors": "Elena Stanghellini, Barbara Vantaggi", "title": "Identification of discrete concentration graph models with one hidden\n  binary variable", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ435 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5A, 1920-1937", "doi": "10.3150/12-BEJ435", "report-no": "IMS-BEJ-BEJ435", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditions are presented for different types of identifiability of discrete\nvariable models generated over an undirected graph in which one node represents\na binary hidden variable. These models can be seen as extensions of the latent\nclass model to allow for conditional associations between the observable random\nvariables. Since local identification corresponds to full rank of the\nparametrization map, we establish a necessary and sufficient condition for the\nrank to be full everywhere in the parameter space. The condition is based on\nthe topology of the undirected graph associated to the model. For non-full rank\nmodels, the obtained characterization allows us to find the subset of the\nparameter space where the identifiability breaks down.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 07:10:42 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 07:29:24 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Stanghellini", "Elena", ""], ["Vantaggi", "Barbara", ""]]}, {"id": "1009.4281", "submitter": "Tomonari Sei", "authors": "Tomonari SEI", "title": "Efron's curvature of the structural gradient model", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structural gradient model is a multivariate statistical model in order to\nextract various interactions of given data set. In this note, we show that\nEfron's statistical curvature of the structural gradient model is less than\nthat of a competitive mixture model under a null hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 07:43:53 GMT"}], "update_date": "2010-09-23", "authors_parsed": [["SEI", "Tomonari", ""]]}, {"id": "1009.4286", "submitter": "Arijit Chakrabarty", "authors": "Arijit Chakrabarty", "title": "Asymptotic normality of Hill Estimator for truncated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the tail index from truncated data is addressed in\nChakrabarty and Samorodnitsky (2009). In that paper, a sample based (and hence\nrandom) choice of k is suggested, and it is shown that the choice leads to a\nconsistent estimator of the inverse of the tail index. In this paper, the\nsecond order behavior of the Hill estimator with that choice of k is studied,\nunder some additional assumptions. In the untruncated situation, it is well\nknown that asymptotic normality of the Hill estimator follows from the\nassumption of second order regular variation of the underlying distribution.\nMotivated by this, we show the same in the truncated case in light of the\nsecond order regular variation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 08:12:13 GMT"}], "update_date": "2010-09-23", "authors_parsed": [["Chakrabarty", "Arijit", ""]]}, {"id": "1009.4345", "submitter": "Domenico Marinucci", "authors": "Claudio Durastanti, Daryl Geller, Domenico Marinucci", "title": "Adaptive Nonparametric Regression on Spin Fiber Bundles", "comments": "40 pages", "journal-ref": "Journal of Multivariate Analysis, 104, pp. 16-38, (2012)", "doi": null, "report-no": null, "categories": "math.ST astro-ph.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of adaptive nonparametric procedures by means of wavelet\nthresholding techniques is now a classical topic in modern mathematical\nstatistics. In this paper, we extend this framework to the analysis of\nnonparametric regression on sections of spin fiber bundles defined on the\nsphere. This can be viewed as a regression problem where the function to be\nestimated takes as its values algebraic curves (for instance, ellipses) rather\nthan scalars, as usual. The problem is motivated by many important\nastrophysical applications, concerning for instance the analysis of the weak\ngravitational lensing effect, i.e. the distortion effect of gravity on the\nimages of distant galaxies. We propose a thresholding procedure based upon the\n(mixed) spin needlets construction recently advocated by Geller and Marinucci\n(2008,2010) and Geller et al. (2008,2009), and we investigate their rates of\nconvergence and their adaptive properties over spin Besov balls.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 12:51:30 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Durastanti", "Claudio", ""], ["Geller", "Daryl", ""], ["Marinucci", "Domenico", ""]]}, {"id": "1009.4434", "submitter": "Ramon Van Handel", "authors": "Ramon van Handel", "title": "The universal Glivenko-Cantelli property", "comments": "26 pages", "journal-ref": "Probab. Th. Rel. Fields 155, 911-934 (2013)", "doi": "10.1007/s00440-012-0416-5", "report-no": null, "categories": "math.PR math.FA math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F be a separable uniformly bounded family of measurable functions on a\nstandard measurable space, and let N_{[]}(F,\\epsilon,\\mu) be the smallest\nnumber of \\epsilon-brackets in L^1(\\mu) needed to cover F. The following are\nequivalent:\n  1. F is a universal Glivenko-Cantelli class.\n  2. N_{[]}(F,\\epsilon,\\mu)<\\infty for every \\epsilon>0 and every probability\nmeasure \\mu.\n  3. F is totally bounded in L^1(\\mu) for every probability measure \\mu.\n  4. F does not contain a Boolean \\sigma-independent sequence.\n  It follows that universal Glivenko-Cantelli classes are uniformity classes\nfor general sequences of almost surely convergent random measures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 17:40:15 GMT"}, {"version": "v2", "created": "Tue, 2 Nov 2010 17:32:51 GMT"}, {"version": "v3", "created": "Tue, 15 Mar 2011 19:09:22 GMT"}, {"version": "v4", "created": "Tue, 24 Jan 2012 16:26:34 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["van Handel", "Ramon", ""]]}, {"id": "1009.4926", "submitter": "Nizar Demni", "authors": "Nizar Demni (PMA)", "title": "On classical and free stable laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the representative Bernstein measure of the density of\n$(X_{\\alpha})^{-\\alpha/(1-\\alpha)}, 0 < \\alpha < 1$, where $X_{\\alpha}$ is a\npositive stable random variable, as a Fox-H function. When $1-\\alpha = 1/j$ for\nsome integer $j \\geq 2$, the Fox H-function reduces to a Meijer G-function so\nthat the Kanter's random variable (see below) is closely related to a product\nof $(j-1)$ independent Beta random variables. When $\\alpha$ tends to 0, the\nBernstein measure becomes degenerate thereby agrees with Cressie's result for\nthe asymptotic behaviour of stable distributions for small values of $\\alpha$.\nComing to free probability, our result makes more explicit that of Biane on the\ndensity of its free analog. The paper is closed with analytic arguments\nexplaining the occurence of the Kanter's random variable in both the classical\nand the free settings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Sep 2010 19:56:16 GMT"}, {"version": "v2", "created": "Tue, 11 Jan 2011 14:17:01 GMT"}], "update_date": "2011-01-13", "authors_parsed": [["Demni", "Nizar", "", "PMA"]]}, {"id": "1009.5072", "submitter": "Fumiyasu Komaki", "authors": "Fumiyasu Komaki", "title": "Bayesian Predictive Densities Based on Latent Information Priors", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference, Volume 141, Issue\n  12, 2011, Pages 3705-3715", "doi": "10.1016/j.jspi.2011.06.009", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction methods for prior densities are investigated from a predictive\nviewpoint. Predictive densities for future observables are constructed by using\nobserved data. The simultaneous distribution of future observables and observed\ndata is assumed to belong to a parametric submodel of a multinomial model.\nFuture observables and data are possibly dependent. The discrepancy of a\npredictive density to the true conditional density of future observables given\nobserved data is evaluated by the Kullback-Leibler divergence. It is proved\nthat limits of Bayesian predictive densities form an essentially complete\nclass. Latent information priors are defined as priors maximizing the\nconditional mutual information between the parameter and the future observables\ngiven the observed data. Minimax predictive densities are constructed as limits\nof Bayesian predictive densities based on prior sequences converging to the\nlatent information priors.\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 08:26:15 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Komaki", "Fumiyasu", ""]]}, {"id": "1009.5165", "submitter": "Christophe Giraud", "authors": "Christophe Giraud (CMAP)", "title": "Low rank Multivariate regression", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the multivariate regression problem, when the\ntarget regression matrix $A$ is close to a low rank matrix. Our primary\ninterest in on the practical case where the variance of the noise is unknown.\nOur main contribution is to propose in this setting a criterion to select among\na family of low rank estimators and prove a non-asymptotic oracle inequality\nfor the resulting estimator. We also investigate the easier case where the\nvariance of the noise is known and outline that the penalties appearing in our\ncriterions are minimal (in some sense). These penalties involve the expected\nvalue of the Ky-Fan quasi-norm of some random matrices. These quantities can be\nevaluated easily in practice and upper-bounds can be derived from recent\nresults in random matrix theory.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 06:19:14 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2011 06:57:35 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Giraud", "Christophe", "", "CMAP"]]}, {"id": "1009.5337", "submitter": "Martin Wendler", "authors": "Martin Wendler", "title": "U-Processes, U-Quantile Processes and Generalized Linear Statistics of\n  Dependent Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear statistics are an unifying class that contains\nU-statistics, U-quantiles, L-statistics as well as trimmed and winsorized\nU-statistics. For example, many commonly used estimators of scale fall into\nthis class. GL-statistics only have been studied under independence; in this\npaper, we develop an asymptotic theory for GL-statistics of sequences which are\nstrongly mixing or L^1 near epoch dependent on an absolutely regular process.\nFor this purpose, we prove an almost sure approximation of the empirical\nU-process by a Gaussian process. With the help of a generalized Bahadur\nrepresentation, it follows that such a strong invariance principle also holds\nfor the empirical U-quantile process and consequently for GL-statistics. We\nobtain central limit theorems and laws of the iterated logarithm for\nU-processes, U-quantile processes and GL-statistics as straightforward\ncorollaries.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 17:28:37 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2011 19:59:08 GMT"}, {"version": "v3", "created": "Fri, 1 Apr 2011 08:06:43 GMT"}, {"version": "v4", "created": "Thu, 18 Aug 2011 11:35:47 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Wendler", "Martin", ""]]}, {"id": "1009.5397", "submitter": "Tugkan Batu", "authors": "Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and\n  Patrick White", "title": "Testing Closeness of Discrete Distributions", "comments": "26 pages, A preliminary version of this paper appeared in the 41st\n  Symposium on Foundations of Computer Science, 2000, Redondo Beach, CA, A\n  comment from W.D. Smith has been added on the title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from two distributions over an $n$-element set, we wish to test\nwhether these distributions are statistically close. We present an algorithm\nwhich uses sublinear in $n$, specifically, $O(n^{2/3}\\epsilon^{-8/3}\\log n)$,\nindependent samples from each distribution, runs in time linear in the sample\nsize, makes no assumptions about the structure of the distributions, and\ndistinguishes the cases when the distance between the distributions is small\n(less than $\\max\\{\\epsilon^{4/3}n^{-1/3}/32, \\epsilon n^{-1/2}/4\\}$) or large\n(more than $\\epsilon$) in $\\ell_1$ distance. This result can be compared to the\nlower bound of $\\Omega(n^{2/3}\\epsilon^{-2/3})$ for this problem given by\nValiant.\n  Our algorithm has applications to the problem of testing whether a given\nMarkov process is rapidly mixing. We present sublinear for several variants of\nthis problem as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 20:57:00 GMT"}, {"version": "v2", "created": "Thu, 4 Nov 2010 12:27:08 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Batu", "Tugkan", ""], ["Fortnow", "Lance", ""], ["Rubinfeld", "Ronitt", ""], ["Smith", "Warren D.", ""], ["White", "Patrick", ""]]}, {"id": "1009.5614", "submitter": "Ian Manchester", "authors": "Ian R. Manchester", "title": "Input Design for System Identification via Convex Relaxation", "comments": "Preprint submitted for journal publication, extended version of a\n  paper at 2010 IEEE Conference on Decision and Control", "journal-ref": null, "doi": "10.1109/CDC.2010.5717097", "report-no": null, "categories": "math.OC cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for the optimization of excitation inputs\nfor system identification. The optimization problem considered is to maximize a\nreduced Fisher information matrix in any of the classical D-, E-, or A-optimal\nsenses. In contrast to the majority of published work on this topic, we\nconsider the problem in the time domain and subject to constraints on the\namplitude of the input signal. This optimization problem is nonconvex. The main\nresult of the paper is a convex relaxation that gives an upper bound accurate\nto within $2/\\pi$ of the true maximum. A randomized algorithm is presented for\nfinding a feasible solution which, in a certain sense is expected to be at\nleast $2/\\pi$ as informative as the globally optimal input signal. In the case\nof a single constraint on input power, the proposed approach recovers the true\nglobal optimum exactly. Extensions to situations with both power and amplitude\nconstraints on both inputs and outputs are given. A simple simulation example\nillustrates the technique.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 16:07:15 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Manchester", "Ian R.", ""]]}, {"id": "1009.5689", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov, Lie Wang", "title": "Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic\n  Programming", "comments": null, "journal-ref": "Biometrika (2011) 98(4): 791-806", "doi": "10.1093/biomet/asr043", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pivotal method for estimating high-dimensional sparse linear\nregression models, where the overall number of regressors $p$ is large,\npossibly much larger than $n$, but only $s$ regressors are significant. The\nmethod is a modification of the lasso, called the square-root lasso. The method\nis pivotal in that it neither relies on the knowledge of the standard deviation\n$\\sigma$ or nor does it need to pre-estimate $\\sigma$. Moreover, the method\ndoes not rely on normality or sub-Gaussianity of noise. It achieves near-oracle\nperformance, attaining the convergence rate $\\sigma \\{(s/n)\\log p\\}^{1/2}$ in\nthe prediction norm, and thus matching the performance of the lasso with known\n$\\sigma$. These performance results are valid for both Gaussian and\nnon-Gaussian errors, under some mild moment restrictions. We formulate the\nsquare-root lasso as a solution to a convex conic programming problem, which\nallows us to implement the estimator using efficient algorithmic methods, such\nas interior-point and first-order methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 20:39:14 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2011 14:59:58 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 02:34:49 GMT"}, {"version": "v4", "created": "Sun, 11 Dec 2011 17:19:18 GMT"}, {"version": "v5", "created": "Sun, 18 Dec 2011 22:23:18 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Wang", "Lie", ""]]}, {"id": "1009.5839", "submitter": "Nicole Kraemer", "authors": "Gilles Blanchard, Nicole Kraemer", "title": "Optimal learning rates for Kernel Conjugate Gradient regression", "comments": "to appear in Neural Information Processing Systems 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove rates of convergence in the statistical sense for kernel-based least\nsquares regression using a conjugate gradient algorithm, where regularization\nagainst overfitting is obtained by early stopping. This method is directly\nrelated to Kernel Partial Least Squares, a regression method that combines\nsupervised dimensionality reduction with least squares projection. The rates\ndepend on two key quantities: first, on the regularity of the target regression\nfunction and second, on the intrinsic dimensionality of the data mapped into\nthe kernel space. Lower bounds on attainable rates depending on these two\nquantities were established in earlier literature, and we obtain upper bounds\nfor the considered method that match these lower bounds (up to a log factor) if\nthe true regression function belongs to the reproducing kernel Hilbert space.\nIf this assumption is not fulfilled, we obtain similar convergence rates\nprovided additional unlabeled data are available. The order of the learning\nrates match state-of-the-art results that were recently obtained for least\nsquares support vector machines and for linear regularization operators.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 11:05:55 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kraemer", "Nicole", ""]]}, {"id": "1009.5981", "submitter": "David R. Bickel", "authors": "Marta Padilla and David R. Bickel", "title": "Empirical Bayes methods corrected for small numbers of tests", "comments": "This version adds new methods and a simulation study", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 11 (5),\n  art. 4 (2012)", "doi": "10.1515/1544-6115.1807", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram-based empirical Bayes methods developed for analyzing data for\nlarge numbers of genes, SNPs, or other biological features tend to have large\nbiases when applied to data with a smaller number of features such as genes\nwith expression measured conventionally, proteins, and metabolites. To analyze\nsuch small-scale and medium-scale data in an empirical Bayes framework, we\nintroduce corrections of maximum likelihood estimators (MLE) of the local false\ndiscovery rate (LFDR). In this context, the MLE estimates the LFDR, which is a\nposterior probability of null hypothesis truth, by estimating the prior\ndistribution. The corrections lie in excluding each feature when estimating one\nor more parameters on which the prior depends. An application of the new\nestimators and previous estimators to protein abundance data illustrates how\ndifferent estimators lead to very different conclusions about which proteins\nare affected by cancer.\n  The estimators are compared using simulated data of two different numbers of\nfeatures, two different detectability levels, and all possible numbers of\naffected features. The simulations show that some of the corrected MLEs\nsubstantially reduce a negative bias of the MLE. (The best-performing corrected\nMLE was derived from the minimum description length principle.) However, even\nthe corrected MLEs have strong negative biases when the proportion of features\nthat are unaffected is greater than 90%. Therefore, since the number of\naffected features is unknown in the case of real data, we recommend an\noptimally weighted combination of the best of the corrected MLEs with a\nconservative estimator that has weaker parametric assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 19:30:39 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 23:38:33 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Padilla", "Marta", ""], ["Bickel", "David R.", ""]]}]