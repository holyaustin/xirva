[{"id": "1709.00081", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Jingshu Wang, Jack Bowden and Dylan S. Small", "title": "Two-sample instrumental variable analyses using heterogeneous samples", "comments": "21 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable analysis is a widely used method to estimate causal\neffects in the presence of unmeasured confounding. When the instruments,\nexposure and outcome are not measured in the same sample, Angrist and Krueger\n(1992) suggested to use two-sample instrumental variable (TSIV) estimators that\nuse sample moments from an instrument-exposure sample and an instrument-outcome\nsample. However, this method is biased if the two samples are from\nheterogeneous populations so that the distributions of the instruments are\ndifferent. In linear structural equation models, we derive a new class of TSIV\nestimators that are robust to heterogeneous samples under the key assumption\nthat the structural relations in the two samples are the same. The widely used\ntwo-sample two-stage least squares estimator belongs to this class. It is\ngenerally not asymptotically efficient, although we find that it performs\nsimilarly to the optimal TSIV estimator in most practical situations. We then\nattempt to relax the linearity assumption. We find that, unlike one-sample\nanalyses, the TSIV estimator is not robust to misspecified exposure model.\nAdditionally, to nonparametrically identify the magnitude of the causal effect,\nthe noise in the exposure must have the same distributions in the two samples.\nHowever, this assumption is in general untestable because the exposure is not\nobserved in one sample. Nonetheless, we may still identify the sign of the\ncausal effect in the absence of homogeneity of the noise.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 20:58:25 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 18:35:26 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Wang", "Jingshu", ""], ["Bowden", "Jack", ""], ["Small", "Dylan S.", ""]]}, {"id": "1709.00092", "submitter": "Emre Demirkaya", "authors": "Yingying Fan, Emre Demirkaya, Gaorong Li and Jinchi Lv", "title": "RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs", "comments": "37 pages, 6 tables, 9 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power and reproducibility are key to enabling refined scientific discoveries\nin contemporary big data applications with general high-dimensional nonlinear\nmodels. In this paper, we provide theoretical foundations on the power and\nrobustness for the model-free knockoffs procedure introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2016) in high-dimensional setting when the\ncovariate distribution is characterized by Gaussian graphical model. We\nestablish that under mild regularity conditions, the power of the oracle\nknockoffs procedure with known covariate distribution in high-dimensional\nlinear models is asymptotically one as sample size goes to infinity. When\nmoving away from the ideal case, we suggest the modified model-free knockoffs\nmethod called graphical nonlinear knockoffs (RANK) to accommodate the unknown\ncovariate distribution. We provide theoretical justifications on the robustness\nof our modified procedure by showing that the false discovery rate (FDR) is\nasymptotically controlled at the target level and the power is asymptotically\none with the estimated covariate distribution. To the best of our knowledge,\nthis is the first formal theoretical result on the power for the knockoffs\nprocedure. Simulation results demonstrate that compared to existing approaches,\nour method performs competitively in both FDR control and power. A real data\nset is analyzed to further assess the performance of the suggested knockoffs\nprocedure.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 21:50:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Fan", "Yingying", ""], ["Demirkaya", "Emre", ""], ["Li", "Gaorong", ""], ["Lv", "Jinchi", ""]]}, {"id": "1709.00232", "submitter": "Nina Munkholt Jakobsen", "authors": "Nina Munkholt Jakobsen and Michael S{\\o}rensen", "title": "Estimating functions for jump-diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic theory for approximate martingale estimating functions is\ngeneralised to diffusions with finite-activity jumps, when the sampling\nfrequency and terminal sampling time go to infinity. Rate optimality and\nefficiency are of particular concern. Under mild assumptions, it is shown that\nestimators of drift, diffusion, and jump parameters are consistent and\nasymptotically normal, as well as rate-optimal for the drift and jump\nparameters. Additional conditions are derived, which ensure rate-optimality for\nthe diffusion parameter as well as efficiency for all parameters. The findings\nindicate a potentially fruitful direction for the further development of\nestimation for jump-diffusions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 10:18:17 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 20:09:20 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jakobsen", "Nina Munkholt", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1709.00291", "submitter": "Vladislav Tadi\\'c B", "authors": "Vladislav B. Tadic, Arnaud Doucet", "title": "Asymptotic Bias of Stochastic Gradient Search", "comments": "arXiv admin note: text overlap with arXiv:0907.1020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behavior of the stochastic gradient algorithm with a biased\ngradient estimator is analyzed. Relying on arguments based on the dynamic\nsystem theory (chain-recurrence) and the differential geometry (Yomdin theorem\nand Lojasiewicz inequality), tight bounds on the asymptotic bias of the\niterates generated by such an algorithm are derived. The obtained results hold\nunder mild conditions and cover a broad class of high-dimensional nonlinear\nalgorithms. Using these results, the asymptotic properties of the\npolicy-gradient (reinforcement) learning and adaptive population Monte Carlo\nsampling are studied. Relying on the same results, the asymptotic behavior of\nthe recursive maximum split-likelihood estimation in hidden Markov models is\nanalyzed, too.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 20:07:51 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tadic", "Vladislav B.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1709.00353", "submitter": "Yuta Koike", "authors": "Yuta Koike", "title": "Gaussian approximation of maxima of Wiener functionals and its\n  application to high-frequency data", "comments": "48 pages. There were a few errors in Corollary 3.1 and Remark 3.7 of\n  the previous version, and they have been corrected. To appear in Annals of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes an upper bound for the Kolmogorov distance between the\nmaximum of a high-dimensional vector of smooth Wiener functionals and the\nmaximum of a Gaussian random vector. As a special case, we show that the\nmaximum of multiple Wiener-It\\^o integrals with common orders is\nwell-approximated by its Gaussian analog in terms of the Kolmogorov distance if\ntheir covariance matrices are close to each other and the maximum of the fourth\ncumulants of the multiple Wiener-It\\^o integrals is close to zero. This may be\nviewed as a new kind of fourth moment phenomenon, which has attracted\nconsiderable attention in the recent studies of probability. This type of\nGaussian approximation result has many potential applications to statistics. To\nillustrate this point, we present two statistical applications in\nhigh-frequency financial econometrics: One is the hypothesis testing problem\nfor the absence of lead-lag effects and the other is the construction of\nuniform confidence bands for spot volatility.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 14:56:21 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 09:16:16 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 16:57:13 GMT"}, {"version": "v4", "created": "Wed, 13 Jun 2018 11:40:42 GMT"}, {"version": "v5", "created": "Wed, 6 Feb 2019 02:26:30 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Koike", "Yuta", ""]]}, {"id": "1709.00407", "submitter": "Xueyu Mao", "authors": "Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti", "title": "Estimating Mixed Memberships with Sharp Eigenvector Deviations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating community memberships of nodes in a\nnetwork, where every node is associated with a vector determining its degree of\nmembership in each community. Existing provably consistent algorithms often\nrequire strong assumptions about the population, are computationally expensive,\nand only provide an overall error bound for the whole community membership\nmatrix. This paper provides uniform rates of convergence for the inferred\ncommunity membership vector of each node in a network generated from the Mixed\nMembership Stochastic Blockmodel (MMSB); to our knowledge, this is the first\nwork to establish per-node rates for overlapping community detection in\nnetworks. We achieve this by establishing sharp row-wise eigenvector deviation\nbounds for MMSB. Based on the simplex structure inherent in the\neigen-decomposition of the population matrix, we build on established\ncorner-finding algorithms from the optimization community to infer the\ncommunity membership vectors. Our results hold over a broad parameter regime\nwhere the average degree only grows poly-logarithmically with the number of\nnodes. Using experiments with simulated and real datasets, we show that our\nmethod achieves better error with lower variability over competing methods, and\nprocesses real world networks of up to 100,000 nodes within tens of seconds.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 18:25:02 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 10:05:34 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 00:30:00 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Mao", "Xueyu", ""], ["Sarkar", "Purnamrita", ""], ["Chakrabarti", "Deepayan", ""]]}, {"id": "1709.00710", "submitter": "Kou Fujimori", "authors": "Kou Fujimori", "title": "The Dantzig selector for a linear model of diffusion processes", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a linear model of diffusion processes with unknown drift and\ndiagonal diffusion matrices is discussed. We will consider the estimation\nproblems for unknown parameters based on the discrete time observation in\nhigh-dimensional and sparse settings. To estimate drift matrices, the Dantzig\nselector which was proposed by Cand\\'es and Tao in 2007 will be applied. Then,\nwe will prove two types of consistency of the estimator of drift matrix; one is\nthe consistency in the sense of $l_q$ norm for every $q \\in [1,\\infty]$ and the\nother is the variable selection consistency. Moreover, we will construct an\nasymptotically normal estimator of the drift matrix by using the variable\nselection consistency of the Dantzig selector.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 13:00:05 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Fujimori", "Kou", ""]]}, {"id": "1709.00747", "submitter": "Abdelhakim Necir", "authors": "Abdelhakim Necir", "title": "Koml\\'os-Major-Tusn\\'ady approximations to increments of uniform\n  empirical processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The well-known Koml\\'os-Major-Tusn\\'ady inequalities [Z. Wahrsch. Verw.\nGebiete 32 (1975) 111-131; Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provide\nsharp inequalities to partial sums of iid standard exponential random variables\nby a sequence of standard Brownian motions. In this paper, we employ these\nresults to establish Gaussian approximations to weighted increments of uniform\nempirical and quantile processes. This approach provides rates to the\napproximations which, among others, have direct applications to statistics of\nextreme values for randomly censored data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 17:14:31 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 16:10:48 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 14:56:08 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 18:38:19 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Necir", "Abdelhakim", ""]]}, {"id": "1709.00801", "submitter": "Mohammad Arashi", "authors": "M. Arashi", "title": "Some theoretical results on tensor elliptical distribution", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilinear normal distribution is a widely used tool in tensor analysis\nof magnetic resonance imaging (MRI). Diffusion tensor MRI provides a\nstatistical estimate of a symmetric 2nd-order diffusion tensor, for each voxel\nwithin an imaging volume. In this article, tensor elliptical (TE) distribution\nis introduced as an extension to the multilinear normal (MLN) distribution.\nSome properties including the characteristic function and distribution of\naffine transformations are given. An integral representation connecting\ndensities of TE and MLN distributions is exhibited that is used in deriving the\nexpectation of any measurable function of a TE variate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 03:30:00 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Arashi", "M.", ""]]}, {"id": "1709.00869", "submitter": "Anna Ben-Hamou", "authors": "Anna Ben-Hamou, Roberto I. Oliveira and Yuval Peres", "title": "Estimating graph parameters with random walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm observes the trajectories of random walks over an unknown graph\n$G$, starting from the same vertex $x$, as well as the degrees along the\ntrajectories. For all finite connected graphs, one can estimate the number of\nedges $m$ up to a bounded factor in\n$O\\left(t_{\\mathrm{rel}}^{3/4}\\sqrt{m/d}\\right)$ steps, where\n$t_{\\mathrm{rel}}$ is the relaxation time of the lazy random walk on $G$ and\n$d$ is the minimum degree in $G$. Alternatively, $m$ can be estimated in\n$O\\left(t_{\\mathrm{unif}} +t_{\\mathrm{rel}}^{5/6}\\sqrt{n}\\right)$, where $n$ is\nthe number of vertices and $t_{\\mathrm{unif}}$ is the uniform mixing time on\n$G$. The number of vertices $n$ can then be estimated up to a bounded factor in\nan additional $O\\left(t_{\\mathrm{unif}}\\frac{m}{n}\\right)$ steps. Our\nalgorithms are based on counting the number of intersections of random walk\npaths $X,Y$, i.e. the number of pairs $(t,s)$ such that $X_t=Y_s$. This\nimproves on previous estimates which only consider collisions (i.e., times $t$\nwith $X_t=Y_t$). We also show that the complexity of our algorithms is optimal,\neven when restricting to graphs with a prescribed relaxation time. Finally, we\nshow that, given either $m$ or the mixing time of $G$, we can compute the\n\"other parameter\" with a self-stopping algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 08:58:43 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 16:23:33 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Ben-Hamou", "Anna", ""], ["Oliveira", "Roberto I.", ""], ["Peres", "Yuval", ""]]}, {"id": "1709.01131", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Yasin Asar and S. Ejaz Ahmed", "title": "Liu-type Shrinkage Estimations in Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present the preliminary test, Stein-type and positive part\nLiu estimators in the linear models when the parameter vector\n$\\boldsymbol{\\beta}$ is partitioned into two parts, namely, the main effects\n$\\boldsymbol{\\beta}_1$ and the nuisance effects $\\boldsymbol{\\beta}_2$ such\nthat $\\boldsymbol{\\beta}=\\left(\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2\n\\right)$. We consider the case that a priori known or suspected set of the\nexplanatory variables do not contribute to predict the response so that a\nsub-model may be enough for this purpose. Thus, the main interest is to\nestimate $\\boldsymbol{\\beta}_1$ when $\\boldsymbol{\\beta}_2$ is close to zero.\nTherefore, we conduct a Monte Carlo simulation study to evaluate the relative\nefficiency of the suggested estimators, where we demonstrate the superiority of\nthe proposed estimators.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 19:42:17 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Asar", "Yasin", ""], ["Ahmed", "S. Ejaz", ""]]}, {"id": "1709.01430", "submitter": "Giovanni Pistone", "authors": "Luigi Montrucchio, Giovanni Pistone", "title": "A class of non-parametric deformed exponential statistical models", "comments": "Revised submission to proceedings GSI 2017 Conference, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the class on non-parametric deformed statistical models where the\ndeformed exponential has linear growth at infinity and is sub-exponential at\nzero. This class generalizes the class introduced by N.J.~Newton. We discuss\nthe convexity and regularity of the normalization operator, the form of the\ndeformed statistical divergences and their convex duality, the properties of\nthe escort densities, and the affine manifold structure of the statistical\nbundle.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 12:37:47 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 15:26:00 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 09:32:02 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Montrucchio", "Luigi", ""], ["Pistone", "Giovanni", ""]]}, {"id": "1709.02062", "submitter": "Xu He", "authors": "Xu He", "title": "Lattice-based designs possessing quasi-optimal separation distance on\n  all projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental designs that spread out points apart from each other on\nprojections are important for computer experiments when not necessarily all\nfactors have substantial influence on the response. We provide a theoretical\nframework to generate designs that possess quasi-optimal separation distance on\nall of the projections and quasi-optimal fill distance on univariate margins.\nThe key is to use special techniques to rotate certain lattices. One such type\nof design is densest packing-based maximum projection designs, which outperform\nexisting types of space-filling designs in many scenarios. Computer code to\ngenerate these designs is provided in R package LatticeDesign.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 04:10:38 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 12:13:08 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 15:07:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["He", "Xu", ""]]}, {"id": "1709.02069", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Linglong Kong and Ivan Mizera", "title": "An Alternative Approach to Functional Linear Partial Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have previously proposed the partial quantile regression (PQR) prediction\nprocedure for functional linear model by using partial quantile covariance\ntechniques and developed the simple partial quantile regression (SIMPQR)\nalgorithm to efficiently extract PQR basis for estimating functional\ncoefficients. However, although the PQR approach is considered as an attractive\nalternative to projections onto the principal component basis, there are\ncertain limitations to uncovering the corresponding asymptotic properties\nmainly because of its iterative nature and the non-differentiability of the\nquantile loss function. In this article, we propose and implement an\nalternative formulation of partial quantile regression (APQR) for functional\nlinear model by using block relaxation method and finite smoothing techniques.\nThe proposed reformulation leads to insightful results and motivates new\ntheory, demonstrating consistency and establishing convergence rates by\napplying advanced techniques from empirical process theory. Two simulations and\ntwo real data from ADHD-200 sample and ADNI are investigated to show the\nsuperiority of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:03:24 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Yu", "Dengdeng", ""], ["Kong", "Linglong", ""], ["Mizera", "Ivan", ""]]}, {"id": "1709.02087", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Sharp Bounds for Generalized Uniformity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of generalized uniformity testing \\cite{BC17} of a\ndiscrete probability distribution: Given samples from a probability\ndistribution $p$ over an {\\em unknown} discrete domain $\\mathbf{\\Omega}$, we\nwant to distinguish, with probability at least $2/3$, between the case that $p$\nis uniform on some {\\em subset} of $\\mathbf{\\Omega}$ versus $\\epsilon$-far, in\ntotal variation distance, from any such uniform distribution.\n  We establish tight bounds on the sample complexity of generalized uniformity\ntesting. In more detail, we present a computationally efficient tester whose\nsample complexity is optimal, up to constant factors, and a matching\ninformation-theoretic lower bound. Specifically, we show that the sample\ncomplexity of generalized uniformity testing is\n$\\Theta\\left(1/(\\epsilon^{4/3}\\|p\\|_3) + 1/(\\epsilon^{2} \\|p\\|_2) \\right)$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 06:16:08 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1709.02223", "submitter": "Konstantinos Spiliopoulos", "authors": "Siragan Gailus and Konstantinos Spiliopoulos", "title": "Discrete-Time Statistical Inference for Multiscale Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical inference for small-noise-perturbed multiscale dynamical\nsystems under the assumption that we observe a single time series from the slow\nprocess only. We construct estimators for both averaging and homogenization\nregimes, based on an appropriate misspecified model motivated by a second-order\nstochastic Taylor expansion of the slow process with respect to a function of\nthe time-scale separation parameter. In the case of a fixed number of\nobservations, we establish consistency, asymptotic normality, and asymptotic\nstatistical efficiency of a minimum contrast estimator (MCE), the limiting\nvariance having been identified explicitly; we furthermore establish\nconsistency and asymptotic normality of a simplified minimum constrast\nestimator (SMCE), which is however not in general efficient. These results are\nthen extended to the case of high-frequency observations under a condition\nrestricting the rate at which the number of observations may grow vis-\\`a-vis\nthe separation of scales. Numerical simulations illustrate the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:12:20 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 01:18:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Gailus", "Siragan", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1709.02244", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Yasin Asar, Ahmet Demiralp and\n  M.\\c{S}amil \\c{S}{\\i}k", "title": "Improved Quantile Regression Estimators when the Errors are\n  Independently and Non-identically Distributed", "comments": "arXiv admin note: text overlap with arXiv:1707.03820", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a classical regression model, it is usually assumed that the explanatory\nvariables are independent of each other and error terms are normally\ndistributed. But when these assumptions are not met, situations like the error\nterms are not independent or they are not identically distributed or both of\nthese, LSE will not be robust. Hence, quantile regression has been used to\ncomplement this deficiency of classical regression analysis and to improve the\nleast square estimation (LSE). In this study, we consider preliminary test and\nshrinkage estimation strategies for quantile regression models with\nindependently and non-identically distributed (i.ni.d.) errors. A Monte Carlo\nsimulation study is conducted to assess the relative performance of the\nestimators. Also, we numerically compare their performance with Ridge, Lasso,\nElastic Net penalty estimation strategies. A real data example is presented to\nillustrate the usefulness of the suggested methods. Finally, we obtain the\nasymptotic results of suggested estimators\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 12:32:04 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Asar", "Yasin", ""], ["Demiralp", "Ahmet", ""], ["\u015e\u0131k", "M. \u015eamil", ""]]}, {"id": "1709.02294", "submitter": "Erwan Le", "authors": "Esther Derman (CMAP), Erwan Le Pennec (CMAP, XPOP)", "title": "Clustering and Model Selection via Penalized Likelihood for\n  Different-sized Categorical Data Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider unsupervised clustering of categorical vectors\nthat can be of different size using mixture. We use likelihood maximization to\nestimate the parameters of the underlying mixture model and a penalization\ntechnique to select the number of mixture components. Regardless of the true\ndistribution that generated the data, we show that an explicit penalty, known\nup to a multiplicative constant, leads to a non-asymptotic oracle inequality\nwith the Kullback-Leibler divergence on the two sides of the inequality. This\ntheoretical result is illustrated by a document clustering application. To this\naim a novel robust expectation-maximization algorithm is proposed to estimate\nthe mixture parameters that best represent the different topics. Slope\nheuristics are used to calibrate the penalty and to select a number of\nclusters.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:59:50 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Derman", "Esther", "", "CMAP"], ["Pennec", "Erwan Le", "", "CMAP, XPOP"]]}, {"id": "1709.02532", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence", "comments": "34 pages, 10 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:36:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 05:59:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:56:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:21:46 GMT"}, {"version": "v5", "created": "Sun, 25 Feb 2018 22:58:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1709.02637", "submitter": "Viktor Skorniakov", "authors": "Viktor Skorniakov", "title": "On asymptotic normality of certain linear rank statistics", "comments": null, "journal-ref": null, "doi": "10.1016/j.spl.2018.08.016", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider asymptotic normality of linear rank statistics under various\nrandomization rules met in clinical trials and designed for patients'\nallocation into treatment and placebo arms. Exposition relies on some general\nlimit theorem due to McLeish (1974) which appears to be well suited for the\nproblem considered and may be employed for other similar rules undis- cussed in\nthe paper. Examples of applications include well known results as well as\nseveral new ones.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 10:37:40 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Skorniakov", "Viktor", ""]]}, {"id": "1709.02647", "submitter": "Anthea Monod", "authors": "Anthea Monod, Sara Kali\\v{s}nik, Juan \\'Angel Pati\\~no-Galindo, Lorin\n  Crawford", "title": "Tropical Sufficient Statistics for Persistent Homology", "comments": "31 pages, 5 figures", "journal-ref": "SIAM Journal on Applied Algebra and Geometry 3 (2), 337-371 (2019)", "doi": "10.1137/17M1148037", "report-no": "MPI MIS 66/2017", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that an embedding in Euclidean space based on tropical geometry\ngenerates stable sufficient statistics for barcodes. In topological data\nanalysis, barcodes are multiscale summaries of algebraic topological\ncharacteristics that capture the `shape' of data; however, in practice, they\nhave complex structures that make them difficult to use in statistical\nsettings. The sufficiency result presented in this work allows for classical\nprobability distributions to be assumed on the tropical geometric\nrepresentation of barcodes. This makes a variety of parametric statistical\ninference methods amenable to barcodes, all while maintaining their initial\ninterpretations. More specifically, we show that exponential family\ndistributions may be assumed, and that likelihood functions for persistent\nhomology may be constructed. We conceptually demonstrate sufficiency and\nillustrate its utility in persistent homology dimensions 0 and 1 with concrete\nparametric applications to human immunodeficiency virus and avian influenza\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 11:06:16 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 07:09:13 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 00:58:52 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 11:14:19 GMT"}, {"version": "v5", "created": "Thu, 20 Dec 2018 17:27:51 GMT"}, {"version": "v6", "created": "Sun, 30 Jun 2019 11:59:32 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Monod", "Anthea", ""], ["Kali\u0161nik", "Sara", ""], ["Pati\u00f1o-Galindo", "Juan \u00c1ngel", ""], ["Crawford", "Lorin", ""]]}, {"id": "1709.02695", "submitter": "Ryan Martin", "authors": "Minwoo Chae, Ryan Martin, Stephen G. Walker", "title": "Applications of an algorithm for solving Fredholm equations of the first\n  kind", "comments": "22 pages, 7 figures", "journal-ref": "Statistics and Computing, 2019, volume 29, number 4, pages\n  645--654", "doi": "10.1007/s11222-018-9829-z", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use an iterative algorithm for solving Fredholm equations of\nthe first kind. The basic algorithm is known and is based on an EM algorithm\nwhen involved functions are non-negative and integrable. With this algorithm we\ndemonstrate two examples involving the estimation of a mixing density and a\nfirst passage time density function involving Brownian motion. We also develop\nthe basic algorithm to include functions which are not necessarily non-negative\nand again present illustrations under this scenario. A self contained proof of\nconvergence of all the algorithms employed is presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 13:28:17 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chae", "Minwoo", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1709.02793", "submitter": "Steven Rosenberg", "authors": "Eric Kolaczyk, Lizhen Lin, Steven Rosenberg, Jie Xu, Jackson Walters", "title": "Averages of Unlabeled Networks: Geometric Characterization and\n  Asymptotic Behavior", "comments": "This version contains two extensions of the main result on the\n  uniqueness of the Fr\\'echet mean, and has added references to related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly common to see large collections of network data\nobjects -- that is, data sets in which a network is viewed as a fundamental\nunit of observation. As a result, there is a pressing need to develop\nnetwork-based analogues of even many of the most basic tools already standard\nfor scalar and vector data. In this paper, our focus is on averages of\nunlabeled, undirected networks with edge weights. Specifically, we (i)\ncharacterize a certain notion of the space of all such networks, (ii) describe\nkey topological and geometric properties of this space relevant to doing\nprobability and statistics thereupon, and (iii) use these properties to\nestablish the asymptotic behavior of a generalized notion of an empirical mean\nunder sampling from a distribution supported on this space. Our results rely on\na combination of tools from geometry, probability theory, and statistical shape\nanalysis. In particular, the lack of vertex labeling necessitates working with\na quotient space modding out permutations of labels. This results in a\nnontrivial geometry for the space of unlabeled networks, which in turn is found\nto have important implications on the types of probabilistic and statistical\nresults that may be obtained and the techniques needed to obtain them.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 17:50:51 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 19:00:28 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 17:41:15 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 13:56:48 GMT"}, {"version": "v5", "created": "Thu, 7 Feb 2019 15:33:24 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Kolaczyk", "Eric", ""], ["Lin", "Lizhen", ""], ["Rosenberg", "Steven", ""], ["Xu", "Jie", ""], ["Walters", "Jackson", ""]]}, {"id": "1709.02932", "submitter": "Daniel Burgarth", "authors": "Daniel Klaus Burgarth", "title": "Identifying combinatorially symmetric Hidden Markov Models", "comments": "Although the result is very simple, I could not find much (closely)\n  related work. If I missed out something I'd be grateful if you could let me\n  know via email to dkb3@aber.ac.uk", "journal-ref": "Electronic Journal of Linear Algebra, Volume 34, pp. 393-398\n  (2018)", "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a sufficient criterion for the unique parameter identification of\ncombinatorially symmetric Hidden Markov Models based on the structure of their\ntransition matrix. If the observed states of the chain form a zero forcing set\nof the graph of the Markov model then it is uniquely identifiable and an\nexplicit reconstruction method is given.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 09:13:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Burgarth", "Daniel Klaus", ""]]}, {"id": "1709.03137", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel", "title": "Methods for Estimation of Convex Sets", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of shape constrained estimation, we review methods and works\ndone in convex set estimation. These methods mostly build on stochastic and\nconvex geometry, empirical process theory, functional analysis, linear\nprogramming, extreme value theory, etc. The statistical problems that we review\ninclude density support estimation, estimation of the level sets of densities\nor depth functions, nonparametric regression, etc. We focus on the estimation\nof convex sets under the Nikodym and Hausdorff metrics, which require different\ntechniques and, quite surprisingly, lead to very different results, in\nparticular in density support estimation. Finally, we discuss computational\nissues in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 17:03:36 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 13:20:39 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1709.03154", "submitter": "Richard Samworth", "authors": "Richard J. Samworth", "title": "Recent progress in log-concave density estimation", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, log-concave density estimation via maximum likelihood\nestimation has emerged as a fascinating alternative to traditional\nnonparametric smoothing techniques, such as kernel density estimation, which\nrequire the choice of one or more bandwidths. The purpose of this article is to\ndescribe some of the properties of the class of log-concave densities on\n$\\mathbb{R}^d$ which make it so attractive from a statistical perspective, and\nto outline the latest methodological, theoretical and computational advances in\nthe area.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 18:59:00 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Samworth", "Richard J.", ""]]}, {"id": "1709.03183", "submitter": "Jiaming Xu", "authors": "Jiaming Xu", "title": "Rates of Convergence of Spectral Methods for Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating the grahpon model - the\nunderlying generating mechanism of a network. Graphon estimation arises in many\napplications such as predicting missing links in networks and learning user\npreferences in recommender systems. The graphon model deals with a random graph\nof $n$ vertices such that each pair of two vertices $i$ and $j$ are connected\nindependently with probability $\\rho \\times f(x_i,x_j)$, where $x_i$ is the\nunknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric\nfunction, and $\\rho$ is a scaling parameter characterizing the graph sparsity.\nRecent studies have identified the minimax error rate of estimating the graphon\nfrom a single realization of the random graph. However, there exists a wide gap\nbetween the known error rates of computationally efficient estimation\nprocedures and the minimax optimal error rate.\n  Here we analyze a spectral method, namely universal singular value\nthresholding (USVT) algorithm, in the relatively sparse regime with the average\nvertex degree $n\\rho=\\Omega(\\log n)$. When $f$ belongs to H\\\"{o}lder or Sobolev\nspace with smoothness index $\\alpha$, we show the error rate of USVT is at most\n$(n\\rho)^{ -2 \\alpha / (2\\alpha+d)}$, approaching the minimax optimal error\nrate $\\log (n\\rho)/(n\\rho)$ for $d=1$ as $\\alpha$ increases. Furthermore, when\n$f$ is analytic, we show the error rate of USVT is at most $\\log^d\n(n\\rho)/(n\\rho)$. In the special case of stochastic block model with $k$\nblocks, the error rate of USVT is at most $k/(n\\rho)$, which is larger than the\nminimax optimal error rate by at most a multiplicative factor $k/\\log k$. This\ncoincides with the computational gap observed for community detection. A key\nstep of our analysis is to derive the eigenvalue decaying rate of the edge\nprobability matrix using piecewise polynomial approximations of the graphon\nfunction $f$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 21:45:48 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Xu", "Jiaming", ""]]}, {"id": "1709.03234", "submitter": "Zhendong Huang", "authors": "Zhendong Huang and Davide Ferrari", "title": "Fast construction of efficient composite likelihood equations", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth in both size and complexity of modern data challenges the\napplicability of traditional likelihood-based inference. Composite likelihood\n(CL) methods address the difficulties related to model selection and\ncomputational intractability of the full likelihood by combining a number of\nlow-dimensional likelihood objects into a single objective function used for\ninference. This paper introduces a procedure to combine partial likelihood\nobjects from a large set of feasible candidates and simultaneously carry out\nparameter estimation. The new method constructs estimating equations balancing\nstatistical efficiency and computing cost by minimizing an approximate distance\nfrom the full likelihood score subject to a L1-norm penalty representing the\navailable computing resources. This results in truncated CL equations\ncontaining only the most informative partial likelihood score terms. An\nasymptotic theory within a framework where both sample size and data dimension\ngrow is developed and finite-sample properties are illustrated through\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 04:07:41 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Huang", "Zhendong", ""], ["Ferrari", "Davide", ""]]}, {"id": "1709.03342", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat, Fabien Panloup", "title": "Optimal non-asymptotic bound of the Ruppert-Polyak averaging without\n  strong convexity", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the non-asymptotic control of the mean-squared error\nfor the Ruppert-Polyak stochastic averaged gradient descent introduced in the\nseminal contributions of [Rup88] and [PJ92]. In our main results, we establish\nnon-asymptotic tight bounds (optimal with respect to the Cramer-Rao lower\nbound) in a very general framework that includes the uniformly strongly convex\ncase as well as the one where the function f to be minimized satisfies a weaker\nKurdyka-Lojiasewicz-type condition [Loj63, Kur98]. In particular, it makes it\npossible to recover some pathological examples such as on-line learning for\nlogistic regression (see [Bac14]) and recursive quan- tile estimation (an even\nnon-convex situation).\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 11:49:19 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Panloup", "Fabien", ""]]}, {"id": "1709.03393", "submitter": "William Leeb", "authors": "Edgar Dobriban, William Leeb, Amit Singer", "title": "Optimal prediction in the linearly transformed spiked model", "comments": "This paper replaces the preprint \"PCA from noisy, linearly reduced\n  data: the diagonal case\" by Edgar Dobriban, William Leeb, and Amit Singer\n  (arXiv:1611.10333)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linearly transformed spiked model, where observations $Y_i$\nare noisy linear transforms of unobserved signals of interest $X_i$:\n\\begin{align*}\n  Y_i = A_i X_i + \\varepsilon_i, \\end{align*} for $i=1,\\ldots,n$. The transform\nmatrices $A_i$ are also observed. We model $X_i$ as random vectors lying on an\nunknown low-dimensional space. How should we predict the unobserved signals\n(regression coefficients) $X_i$?\n  The naive approach of performing regression for each observation separately\nis inaccurate due to the large noise. Instead, we develop optimal linear\nempirical Bayes methods for predicting $X_i$ by \"borrowing strength\" across the\ndifferent samples. Our methods are applicable to large datasets and rely on\nweak moment assumptions. The analysis is based on random matrix theory.\n  We discuss applications to signal processing, deconvolution, cryo-electron\nmicroscopy, and missing data in the high-noise regime. For missing data, we\nshow in simulations that our methods are faster, more robust to noise and to\nunequal sampling than well-known matrix completion methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 22:14:03 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 22:54:13 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Dobriban", "Edgar", ""], ["Leeb", "William", ""], ["Singer", "Amit", ""]]}, {"id": "1709.03473", "submitter": "Andrii Babii", "authors": "Andrii Babii and Jean-Pierre Florens", "title": "Is completeness necessary? Estimation in nonidentified linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper documents the consequences of the identification failures for a\nclass of linear ill-posed inverse models. The Tikhonov-regularized estimator\nconverges to a well-defined limit equal to the best approximation of the\nstructural parameter in the orthogonal complement to the null space of the\noperator. We illustrate that in many cases the best approximation may coincide\nwith the structural parameter or at least may reasonably approximate it. We\ncharacterize the nonasymptotic Hilbert space norm and the uniform norm\nconvergence rates for the best approximation. Nonidentification has important\nimplications for the large sample distribution of the Tikhonov-regularized\nestimator, and we document the transition between the Gaussian and the weighted\nchi-squared limits. The theoretical results are illustrated for the\nnonparametric IV and the functional linear IV regressions and are further\nsupported by the Monte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 16:49:19 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 04:06:04 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 20:54:50 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Babii", "Andrii", ""], ["Florens", "Jean-Pierre", ""]]}, {"id": "1709.03570", "submitter": "Ervin T\\'anczos", "authors": "Bob Mankoff, Robert Nowak, Ervin Tanczos", "title": "A KL-LUCB Bandit Algorithm for Large-Scale Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on best-arm identification in multi-armed bandits with\nbounded rewards. We develop an algorithm that is a fusion of lil-UCB and\nKL-LUCB, offering the best qualities of the two algorithms in one method. This\nis achieved by proving a novel anytime confidence bound for the mean of bounded\ndistributions, which is the analogue of the LIL-type bounds recently developed\nfor sub-Gaussian distributions. We corroborate our theoretical results with\nnumerical experiments based on the New Yorker Cartoon Caption Contest.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 20:14:59 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Mankoff", "Bob", ""], ["Nowak", "Robert", ""], ["Tanczos", "Ervin", ""]]}, {"id": "1709.03615", "submitter": "Kitty Mohammed", "authors": "Kitty Mohammed and Hariharan Narayanan", "title": "Manifold Learning Using Kernel Density Estimation and Local Principal\n  Components Analysis", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of recovering a $d-$dimensional manifold $\\mathcal{M}\n\\subset \\mathbb{R}^n$ when provided with noiseless samples from $\\mathcal{M}$.\nThere are many algorithms (e.g., Isomap) that are used in practice to fit\nmanifolds and thus reduce the dimensionality of a given data set. Ideally, the\nestimate $\\mathcal{M}_\\mathrm{put}$ of $\\mathcal{M}$ should be an actual\nmanifold of a certain smoothness; furthermore, $\\mathcal{M}_\\mathrm{put}$\nshould be arbitrarily close to $\\mathcal{M}$ in Hausdorff distance given a\nlarge enough sample. Generally speaking, existing manifold learning algorithms\ndo not meet these criteria. Fefferman, Mitter, and Narayanan (2016) have\ndeveloped an algorithm whose output is provably a manifold. The key idea is to\ndefine an approximate squared-distance function (asdf) to $\\mathcal{M}$. Then,\n$\\mathcal{M}_\\mathrm{put}$ is given by the set of points where the gradient of\nthe asdf is orthogonal to the subspace spanned by the largest $n - d$\neigenvectors of the Hessian of the asdf. As long as the asdf meets certain\nregularity conditions, $\\mathcal{M}_\\mathrm{put}$ is a manifold that is\narbitrarily close in Hausdorff distance to $\\mathcal{M}$. In this paper, we\ndefine two asdfs that can be calculated from the data and show that they meet\nthe required regularity conditions. The first asdf is based on kernel density\nestimation, and the second is based on estimation of tangent spaces using local\nprincipal components analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 22:45:10 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Mohammed", "Kitty", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "1709.03702", "submitter": "Guillaume Maillard", "authors": "Guillaume Maillard (LMO, SELECT, LM-Orsay), Sylvain Arlot (LMO,\n  SELECT, LM-Orsay), Matthieu Lerasle (LMO, SELECT, LM-Orsay)", "title": "Cross-validation improved by aggregation: Agghoo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is widely used for selecting among a family of learning\nrules. This paper studies a related method, called aggregated hold-out\n(Agghoo), which mixes cross-validation with aggregation; Agghoo can also be\nrelated to bagging. According to numerical experiments, Agghoo can improve\nsignificantly cross-validation's prediction error, at the same computational\ncost; this makes it very promising as a general-purpose tool for prediction. We\nprovide the first theoretical guarantees on Agghoo, in the supervised\nclassification setting, ensuring that one can use it safely: at worse, Agghoo\nperforms like the hold-out, up to a constant factor. We also prove a\nnon-asymptotic oracle inequality, in binary classification under the margin\ncondition, which is sharp enough to get (fast) minimax rates.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 06:29:12 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Maillard", "Guillaume", "", "LMO, SELECT, LM-Orsay"], ["Arlot", "Sylvain", "", "LMO,\n  SELECT, LM-Orsay"], ["Lerasle", "Matthieu", "", "LMO, SELECT, LM-Orsay"]]}, {"id": "1709.03807", "submitter": "Vladimir Pastukhov", "authors": "Dragi Anevski and Vladimir Pastukhov", "title": "The asymptotic distribution of the isotonic regression estimator over a\n  general countable pre-ordered set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the isotonic regression estimator over a general countable\npre-ordered set. We obtain the limiting distribution of the estimator and study\nits properties. It is proved that, under some general assumptions, the limiting\ndistribution of the isotonized estimator is given by the concatenation of the\nseparate isotonic regressions of the certain subvectors of an unrestrecred\nestimator's asymptotic distribution. Also, we show that the isotonization\npreserves the rate of convergence of the underlying estimator. We apply these\nresults to the problems of estimation of a bimonotone regression function and\nestimation of a bimonotone probability mass function.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 12:36:39 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 16:54:37 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Anevski", "Dragi", ""], ["Pastukhov", "Vladimir", ""]]}, {"id": "1709.03885", "submitter": "Alessandro Rinaldo", "authors": "Steffen L. Lauritzen, Alessandro Rinaldo, Kayvan Sadeghi", "title": "On Exchangeability in Network Models", "comments": "Dedicated to the memory of Steve Fienberg", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive representation theorems for exchangeable distributions on finite\nand infinite graphs using elementary arguments based on geometric and\ngraph-theoretic concepts. Our results elucidate some of the key differences,\nand their implications, between statistical network models that are finitely\nexchangeable and models that define a consistent sequence of probability\ndistributions on graphs of increasing size.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:52:46 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 20:55:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lauritzen", "Steffen L.", ""], ["Rinaldo", "Alessandro", ""], ["Sadeghi", "Kayvan", ""]]}, {"id": "1709.03907", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang, Alexander Rakhlin", "title": "Weighted Message Passing and Minimum Energy Flow for Heterogeneous\n  Stochastic Block Models with Side Information", "comments": "31 pages, 1 figures", "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-34", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the misclassification error for community detection in general\nheterogeneous stochastic block models (SBM) with noisy or partial label\ninformation. We establish a connection between the misclassification rate and\nthe notion of minimum energy on the local neighborhood of the SBM. We develop\nan optimally weighted message passing algorithm to reconstruct labels for SBM\nbased on the minimum energy flow and the eigenvectors of a certain Markov\ntransition matrix. The general SBM considered in this paper allows for\nunequal-size communities, degree heterogeneity, and different connection\nprobabilities among blocks. We focus on how to optimally weigh the message\npassing to improve misclassification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:22:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1709.04078", "submitter": "Emanuel Knill", "authors": "Peter Wills, Emanuel Knill, Kevin Coakley, Yanbao Zhang", "title": "Performance of Test Supermartingale Confidence Intervals for the Success\n  Probability of Bernoulli Trials", "comments": null, "journal-ref": "Journal of Research NIST 125 (2020) 003", "doi": "10.6028/jres.125.003", "report-no": null, "categories": "math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a composite null hypothesis H, test supermartingales are non-negative\nsupermartingales with respect to H with initial value 1. Large values of test\nsupermartingales provide evidence against H. As a result, test supermartingales\nare an effective tool for rejecting H, particularly when the p-values obtained\nare very small and serve as certificates against the null hypothesis. Examples\ninclude the rejection of local realism as an explanation of Bell test\nexperiments in the foundations of physics and the certification of entanglement\nin quantum information science. Test supermartingales have the advantage of\nbeing adaptable during an experiment and allowing for arbitrary stopping rules.\nBy inversion of acceptance regions, they can also be used to determine\nconfidence sets. We use an example to compare the performance of test\nsupermartingales for computing p-values and confidence intervals to\nChernoff-Hoeffding bounds and the \"exact\" p-value. The example is the problem\nof inferring the probability of success in a sequence of Bernoulli trials.\nThere is a cost in using a technique that has no restriction on stopping rules,\nand for a particular test supermartingale, our study quantifies this cost.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 22:49:21 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wills", "Peter", ""], ["Knill", "Emanuel", ""], ["Coakley", "Kevin", ""], ["Zhang", "Yanbao", ""]]}, {"id": "1709.04159", "submitter": "Huiming Zhang", "authors": "Huiming Zhang, Xiaoxu Wu", "title": "Compound Poisson Point Processes, Concentration and Oracle Inequalities", "comments": "25 pages, enlarge the orginal version", "journal-ref": null, "doi": "10.1186/s13660-019-2263-8", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note aims at presenting several new theoretical results for the compound\nPoisson point process, which follows the work of Zhang \\emph{et al.}\n[Insurance~Math.~Econom.~59(2014), 325-336]. The first part provides a new\ncharacterization for a discrete compound Poisson point process (proposed by\n{Acz{\\'e}l} [Acta~Math.~Hungar.~3(3)(1952), 219-224]), it extends the\ncharacterization of the Poisson point process given by Copeland and Regan\n[Ann.~Math.~(1936): 357-362]. Next, we derive some concentration inequalities\nfor discrete compound Poisson point process (negative binomial random variable\nwith unknown dispersion is a significant example). These concentration\ninequalities are potentially useful in count data regressions. We give an\napplication in the weighted Lasso penalized negative binomial regression whose\nKKT conditions of penalized likelihood hold with high probability and then we\nderive non-asymptotic oracle inequalities for a weighted Lasso estimator.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 07:09:52 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 17:21:43 GMT"}, {"version": "v3", "created": "Sun, 8 Dec 2019 15:31:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhang", "Huiming", ""], ["Wu", "Xiaoxu", ""]]}, {"id": "1709.04212", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi and Sumio Watanabe", "title": "Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation\n  and Stochastic Matrix Factorization", "comments": "Containing 36 pages, 2 figures, and 1 table. To appear in SN Computer\n  Science", "journal-ref": "SN Computer Science volume 1, Article number: 69 (2020)", "doi": "10.1007/s42979-020-0071-3", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is useful in document analysis, image\nprocessing, and many information systems; however, its generalization\nperformance has been left unknown because it is a singular learning machine to\nwhich regular statistical theory can not be applied.\n  Stochastic matrix factorization (SMF) is a restricted matrix factorization in\nwhich matrix factors are stochastic; the column of the matrix is in a simplex.\nSMF is being applied to image recognition and text mining. We can understand\nSMF as a statistical model by which a stochastic matrix of given data is\nrepresented by a product of two stochastic matrices, whose generalization\nperformance has also been left unknown because of non-regularity.\n  In this paper, by using an algebraic and geometric method, we show the\nanalytic equivalence of LDA and SMF, both of which have the same real log\ncanonical threshold (RLCT), resulting in that they asymptotically have the same\nBayesian generalization error and the same log marginal likelihood. Moreover,\nwe derive the upper bound of the RLCT and prove that it is smaller than the\ndimension of the parameter divided by two, hence the Bayesian generalization\nerrors of them are smaller than those of regular statistical models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 09:37:03 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 10:45:36 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 08:22:09 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2018 13:47:32 GMT"}, {"version": "v5", "created": "Sat, 23 Jun 2018 00:26:47 GMT"}, {"version": "v6", "created": "Mon, 14 Jan 2019 16:20:44 GMT"}, {"version": "v7", "created": "Fri, 22 Mar 2019 14:56:22 GMT"}, {"version": "v8", "created": "Thu, 30 Jan 2020 15:39:39 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Hayashi", "Naoki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1709.04285", "submitter": "Eni Musta", "authors": "Juan-Juan Cai and Eni Musta", "title": "Estimation of the marginal expected shortfall under asymptotic\n  independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic behavior of the marginal expected shortfall when the\ntwo random variables are asymptotic independent but positive associated, which\nis modeled by the so-called tail dependent coefficient. We construct an\nestimator of the marginal expected shortfall which is shown to be\nasymptotically normal. The finite sample performance of the estimator is\ninvestigated in a small simulation study. The method is also applied to\nestimate the expected amount of rainfall at a weather station given that there\nis a once every 100 years rainfall at another weather station nearby.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:33:04 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Cai", "Juan-Juan", ""], ["Musta", "Eni", ""]]}, {"id": "1709.04342", "submitter": "Chao Zheng", "authors": "Chao Zheng, Davide Ferrari and Yuhong Yang", "title": "Model Selection Confidence Sets by Likelihood Ratio Testing", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional activity of model selection aims at discovering a single\nmodel superior to other candidate models. In the presence of pronounced noise,\nhowever, multiple models are often found to explain the same data equally well.\nTo resolve this model selection ambiguity, we introduce the general approach of\nmodel selection confidence sets (MSCSs) based on likelihood ratio testing. A\nMSCS is defined as a list of models statistically indistinguishable from the\ntrue model at a user-specified level of confidence, which extends the familiar\nnotion of confidence intervals to the model-selection framework. Our approach\nguarantees asymptotically correct coverage probability of the true model when\nboth sample size and model dimension increase. We derive conditions under which\nthe MSCS contains all the relevant information about the true model structure.\nIn addition, we propose natural statistics based on the MSCS to measure\nimportance of variables in a principled way that accounts for the overall model\nuncertainty. When the space of feasible models is large, MSCS is implemented by\nan adaptive stochastic search algorithm which samples MSCS models with high\nprobability. The MSCS methodology is illustrated through numerical experiments\non synthetic data and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:11:41 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zheng", "Chao", ""], ["Ferrari", "Davide", ""], ["Yang", "Yuhong", ""]]}, {"id": "1709.04418", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and David Preinerstorfer", "title": "Power in High-Dimensional Testing Problems", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fan et al. (2015) recently introduced a remarkable method for increasing\nasymptotic power of tests in high-dimensional testing problems. If applicable\nto a given test, their power enhancement principle leads to an improved test\nthat has the same asymptotic size, uniformly non-inferior asymptotic power, and\nis consistent against a strictly broader range of alternatives than the\ninitially given test. We study under which conditions this method can be\napplied and show the following: In asymptotic regimes where the dimensionality\nof the parameter space is fixed as sample size increases, there often exist\ntests that can not be further improved with the power enhancement principle.\nHowever, when the dimensionality of the parameter space increases sufficiently\nslowly with sample size and a marginal local asymptotic normality (LAN)\ncondition is satisfied, every test with asymptotic size smaller than one can be\nimproved with the power enhancement principle. While the marginal LAN condition\nalone does not allow one to extend the latter statement to all rates at which\nthe dimensionality increases with sample size, we give sufficient conditions\nunder which this is the case.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:02:16 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 14:08:22 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 22:08:04 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 18:35:55 GMT"}, {"version": "v5", "created": "Mon, 28 Jan 2019 18:39:18 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Preinerstorfer", "David", ""]]}, {"id": "1709.04548", "submitter": "Jeremy Sumner", "authors": "Jonathan D. Mitchell, Jeremy G. Sumner, and Barbara R. Holland", "title": "Distinguishing between convergent evolution and violation of the\n  molecular clock", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a non-technical introduction to convergence-divergence models, a new\nmodeling approach for phylogenetic data that allows for the usual divergence of\nspecies post speciation but also allows for species to converge, i.e. become\nmore similar over time. By examining the $3$-taxon case in some detail we\nillustrate that phylogeneticists have been \"spoiled\" in the sense of not having\nto think about the structural parameters in their models by virtue of the\nstrong assumption that evolution is treelike. We show that there are not always\ngood statistical reasons to prefer the usual class of treelike models over more\ngeneral convergence-divergence models. Specifically we show many $3$-taxon\ndatasets can be equally well explained by supposing violation of the molecular\nclock due to change in the rate of evolution along different edges, or by\nkeeping the assumption of a constant rate of evolution but instead assuming\nthat evolution is not a purely divergent process. Given the abundance of\nevidence that evolution is not strictly treelike, our discussion is an\nillustration that as phylogeneticists we often need to think clearly about the\nstructural form of the models we use.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 21:45:46 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Mitchell", "Jonathan D.", ""], ["Sumner", "Jeremy G.", ""], ["Holland", "Barbara R.", ""]]}, {"id": "1709.04606", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "Goodness-of-Fit Tests for Random Partitions via Symmetric Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider goodness-of-fit tests with i.i.d. samples generated from a\ncategorical distribution $(p_1,...,p_k)$. For a given $(q_1,...,q_k)$, we test\nthe null hypothesis whether $p_j=q_{\\pi(j)}$ for some label permutation $\\pi$.\nThe uncertainty of label permutation implies that the null hypothesis is\ncomposite instead of being singular. In this paper, we construct a testing\nprocedure using statistics that are defined as indefinite integrals of some\nsymmetric polynomials. This method is aimed directly at the invariance of the\nproblem, and avoids the need of matching the unknown labels. The asymptotic\ndistribution of the testing statistic is shown to be chi-squared, and its power\nis proved to be nearly optimal under a local alternative hypothesis. Various\ndegenerate structures of the null hypothesis are carefully analyzed in the\npaper. A two-sample version of the test is also studied.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 03:50:26 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 02:38:39 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "1709.04840", "submitter": "Fei Xue", "authors": "Fei Xue and Annie Qu", "title": "Variable Selection for Highly Correlated Predictors", "comments": "44 pages (including 14 pages of supplementary materials), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalty-based variable selection methods are powerful in selecting relevant\ncovariates and estimating coefficients simultaneously. However, variable\nselection could fail to be consistent when covariates are highly correlated.\nThe partial correlation approach has been adopted to solve the problem with\ncorrelated covariates. Nevertheless, the restrictive range of partial\ncorrelation is not effective for capturing signal strength for relevant\ncovariates. In this paper, we propose a new Semi-standard PArtial Covariance\n(SPAC) which is able to reduce correlation effects from other predictors while\nincorporating the magnitude of coefficients. The proposed SPAC variable\nselection facilitates choosing covariates which have direct association with\nthe response variable, via utilizing dependency among covariates. We show that\nthe proposed method with the Lasso penalty (SPAC-Lasso) enjoys strong sign\nconsistency in both finite-dimensional and high-dimensional settings under\nregularity conditions. Simulation studies and the `HapMap' gene data\napplication show that the proposed method outperforms the traditional Lasso,\nadaptive Lasso, SCAD, and Peter-Clark-simple (PC-simple) methods for highly\ncorrelated predictors.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:28:04 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Xue", "Fei", ""], ["Qu", "Annie", ""]]}, {"id": "1709.04938", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "M. D. Ruiz-Medina and J. \\'Alvarez-Li\\'ebana", "title": "A note on strong-consistency of componentwise ARH(1) predictors", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new result on strong-consistency, in the trace norm, of\na diagonal componentwise parameter estimator of the autocorrelation operator of\nan autoregressive process of order one (ARH(1) process), allowing\nstrong-consistency of the associated plug-in predictor. These results are\nderived, when the eigenvectors of the autocovariance operator are unknown, and\nthe autocorrelation operator does not admit a diagonal spectral representation\nwith respect to the eigenvectors of the autocovariance operator.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 18:19:33 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ruiz-Medina", "M. D.", ""], ["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1709.04952", "submitter": "Holger Dette", "authors": "Kirsten Schorning, Holger Dette, Katrin Kettelhake, Tilman M\\\"oller", "title": "Optimal designs for enzyme inhibition kinetic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for determining optimal designs for\nenzyme inhibition kinetic models, which are used to model the influence of the\nconcentration of a substrate and an inhibition on the velocity of a reaction.\nThe approach uses a nonlinear transformation of the vector of predictors such\nthat the model in the new coordinates is given by an incomplete response\nsurface model. Although there exist no explicit solutions of the optimal design\nproblem for incomplete response surface models so far, the corresponding design\nproblem in the new coordinates is substantially more transparent, such that\nexplicit or numerical solutions can be determined more easily. The designs for\nthe original problem can finally be found by an inverse transformation of the\noptimal designs determined for the response surface model. We illustrate the\nmethod determining explicit solutions for the $D$-optimal design and for the\noptimal design problem for estimating the individual coefficients in a\nnon-competitive enzyme inhibition kinetic model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 19:22:37 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Schorning", "Kirsten", ""], ["Dette", "Holger", ""], ["Kettelhake", "Katrin", ""], ["M\u00f6ller", "Tilman", ""]]}, {"id": "1709.05153", "submitter": "Asbj{\\o}rn Nilsen Riseth", "authors": "Asbj{\\o}rn N. Riseth and Jake P. Taylor-King", "title": "Operator Fitting for Parameter Estimation of Stochastic Differential\n  Equations", "comments": "21 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of parameters is a crucial part of model development. When models\nare deterministic, one can minimise the fitting error; for stochastic systems\none must be more careful. Broadly parameterisation methods for stochastic\ndynamical systems fit into maximum likelihood estimation- and method of\nmoment-inspired techniques. We propose a method where one matches a finite\ndimensional approximation of the Koopman operator with the implied Koopman\noperator as generated by an extended dynamic mode decomposition approximation.\nOne advantage of this approach is that the objective evaluation cost can be\nindependent the number of samples for some dynamical systems. We test our\napproach on two simple systems in the form of stochastic differential\nequations, compare to benchmark techniques, and consider limited\neigen-expansions of the operators being approximated. Other small variations on\nthe technique are also considered, and we discuss the advantages to our\nformulation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 11:14:02 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 08:27:08 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Riseth", "Asbj\u00f8rn N.", ""], ["Taylor-King", "Jake P.", ""]]}, {"id": "1709.05269", "submitter": "Ye Liang", "authors": "Ye Liang, Joshua D. Habiger and Xiaoyi Min", "title": "The Inuence of Misspecified Covariance on False Discovery Control when\n  Using Posterior Probabilities", "comments": "22 pages, 5 figures", "journal-ref": "Statistical Theory and Related Fields, Vol 1 (2017) 205-215", "doi": "10.1080/24754269.2017.1387445", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the influence of a misspecified covariance structure on\nfalse discovery rate for the large scale multiple testing problem.\nSpecifically, we evaluate the influence on the marginal distribution of local\nfdr statistics, which are used in many multiple testing procedures and related\nto Bayesian posterior probabilities. Explicit forms of the marginal\ndistributions under both correctly specified and incorrectly specified models\nare derived. The Kullback-Leibler divergence is used to quantify the influence\ncaused by a misspecification. Several numerical examples are provided to\nillustrate the influence. A real spatio-temporal data on soil humidity is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:31:21 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liang", "Ye", ""], ["Habiger", "Joshua D.", ""], ["Min", "Xiaoyi", ""]]}, {"id": "1709.05441", "submitter": "Yongcheng Qi", "authors": "Wenhao Gui and Yongcheng Qi", "title": "Spectral Radii of Truncated Circular Unitary Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a truncated circular unitary matrix which is a $p_n$ by $p_n$\nsubmatrix of an $n$ by $n$ circular unitary matrix by deleting the last $n-p_n$\ncolumns and rows. Jiang and Qi (2017) proved that the maximum absolute value of\nthe eigenvalues (known as spectral radius) of the truncated matrix, after\nproperly normalized, converges in distribution to the Gumbel distribution if\n$p_n/n$ is bounded away from $0$ and $1$. In this paper we investigate the\nlimiting distribution of the spectral radius under one of the following four\nconditions: (1). $p_n\\to\\infty$ and $p_n/n\\to 0$ as $n\\to\\infty$; (2).\n$(n-p_n)/n\\to 0$ and $(n-p_n)/(\\log n)^3\\to\\infty$ as $n\\to\\infty$; (3).\n$n-p_n\\to\\infty$ and $(n-p_n)/\\log n\\to 0$ as $n\\to\\infty$ and (4). $n-p_n=k\\ge\n1$ is a fixed integer. We prove that the spectral radius converges in\ndistribution to the Gumbel distribution under the first three conditions and to\na reversed Weibull distribution under the fourth condition.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:56:31 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Gui", "Wenhao", ""], ["Qi", "Yongcheng", ""]]}, {"id": "1709.05454", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Donniell E. Fishkind, Keith Levin, Vince Lyzinski,\n  Youngser Park, Yichen Qin, Daniel L. Sussman, Minh Tang, Joshua T.\n  Vogelstein, and Carey E. Priebe", "title": "Statistical inference on random dot product graphs: a survey", "comments": "An expository survey paper on a comprehensive paradigm for inference\n  for random dot product graphs, centered on graph adjacency and Laplacian\n  spectral embeddings. Paper outlines requisite background; summarizes theory,\n  methodology, and applications from previous and ongoing work; and closes with\n  a discussion of several open problems", "journal-ref": "Journal of Machine Learning Research, 2018", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random dot product graph (RDPG) is an independent-edge random graph that\nis analytically tractable and, simultaneously, either encompasses or can\nsuccessfully approximate a wide range of random graphs, from relatively simple\nstochastic block models to complex latent position graphs. In this survey\npaper, we describe a comprehensive paradigm for statistical inference on random\ndot product graphs, a paradigm centered on spectral embeddings of adjacency and\nLaplacian matrices. We examine the analogues, in graph inference, of several\ncanonical tenets of classical Euclidean inference: in particular, we summarize\na body of existing results on the consistency and asymptotic normality of the\nadjacency and Laplacian spectral embeddings, and the role these spectral\nembeddings can play in the construction of single- and multi-sample hypothesis\ntests for graph data. We investigate several real-world applications, including\ncommunity detection and classification in large social networks and the\ndetermination of functional and biologically relevant network properties from\nan exploratory data analysis of the Drosophila connectome. We outline requisite\nbackground and current open problems in spectral graph inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 04:22:57 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Athreya", "Avanti", ""], ["Fishkind", "Donniell E.", ""], ["Levin", "Keith", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Qin", "Yichen", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1709.05534", "submitter": "Camilo Jose Torres-Jimenez", "authors": "Camilo Jose Torres-Jimenez and Alvaro Mauricio Montenegro-Diaz", "title": "An alternative to continuous univariate distributions supported on a\n  bounded interval: The BMT distribution", "comments": "30 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the BMT distribution as an unimodal alternative\nto continuous univariate distributions supported on a bounded interval. The\nideas behind the mathematical formulation of this new distribution come from\ncomputer aid geometric design, specifically from Bezier curves. First, we\nreview general properties of a distribution given by parametric equations and\nextend the definition of a Bezier distribution. Then, after proposing the BMT\ncumulative distribution function, we derive its probability density function\nand a closed-form expression for quantile function, median, interquartile\nrange, mode, and moments. The domain change from [0,1] to [c,d] is mentioned.\nEstimation of parameters is approached by the methods of maximum likelihood and\nmaximum product of spacing. We test the numerical estimation procedures using\nsome simulated data. Usefulness and flexibility of the new distribution are\nillustrated in three real data sets. The BMT distribution has a significant\npotential to estimate domain parameters and to model data outside the scope of\nthe beta or similar distributions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 15:48:19 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Torres-Jimenez", "Camilo Jose", ""], ["Montenegro-Diaz", "Alvaro Mauricio", ""]]}, {"id": "1709.05585", "submitter": "Xin Tong Thomson", "authors": "Nan Chen and Andrew J. Majda and Xin T. Tong", "title": "Rigorous Analysis for Efficient Statistically Accurate Algorithms for\n  Solving Fokker-Planck Equations in Large Dimensions", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a rigorous analysis for efficient statistically\naccurate algorithms for solving the Fokker-Planck equations associated with\nhigh-dimensional nonlinear turbulent dynamical systems with conditional\nGaussian structures. Despite the conditional Gaussianity, these nonlinear\nsystems contain many strong non-Gaussian features such as intermittency and\nfat-tailed probability density functions (PDFs). The algorithms involve a\nhybrid strategy that requires only a small number of samples $L$ to capture\nboth the transient and the equilibrium non-Gaussian PDFs with high accuracy.\nHere, a conditional Gaussian mixture in a high-dimensional subspace via an\nextremely efficient parametric method is combined with a judicious Gaussian\nkernel density estimation in the remaining low-dimensional subspace. Rigorous\nanalysis shows that the mean integrated squared error in the recovered PDFs in\nthe high-dimensional subspace is bounded by the inverse square root of the\ndeterminant of the conditional covariance, where the conditional covariance is\ncompletely determined by the underlying dynamics and is independent of $L$.\nThis is fundamentally different from a direct application of kernel methods to\nsolve the full PDF, where $L$ needs to increase exponentially with the\ndimension of the system and the bandwidth shrinks. A detailed comparison\nbetween different methods justifies that the efficient statistically accurate\nalgorithms are able to overcome the curse of dimensionality. It is also shown\nwith mathematical rigour that these algorithms are robust in long time provided\nthat the system is controllable and stochastically stable. Particularly,\ndynamical systems with energy-conserving quadratic nonlinearity as in many\ngeophysical and engineering turbulence are proved to have these properties.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 00:24:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Chen", "Nan", ""], ["Majda", "Andrew J.", ""], ["Tong", "Xin T.", ""]]}, {"id": "1709.05603", "submitter": "Zheng Tracy Ke", "authors": "Jiashun Jin, Zheng Tracy Ke", "title": "A Sharp Lower Bound for Mixed-membership Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an undirected network with $n$ nodes and $K$ perceivable\ncommunities, where some nodes may have mixed memberships. We assume that for\neach node $1 \\leq i \\leq n$, there is a probability mass function $\\pi_i$\ndefined over $\\{1, 2, \\ldots, K\\}$ such that \\[ \\pi_i(k) = \\mbox{the weight of\nnode $i$ on community $k$}, \\qquad 1 \\leq k \\leq K. \\] The goal is to estimate\n$\\{\\pi_i, 1 \\leq i \\leq n\\}$ (i.e., membership estimation).\n  We model the network with the {\\it degree-corrected mixed membership (DCMM)}\nmodel \\cite{Mixed-SCORE}. Since for many natural networks, the degrees have an\napproximate power-law tail, we allow {\\it severe degree heterogeneity} in our\nmodel.\n  For any membership estimation $\\{\\hat{\\pi}_i, 1 \\leq i \\leq n\\}$, since each\n$\\pi_i$ is a probability mass function, it is natural to measure the errors by\nthe average $\\ell^1$-norm \\[ \\frac{1}{n} \\sum_{i = 1}^n \\| \\hat{\\pi}_i -\n\\pi_i\\|_1. \\] We also consider a variant of the $\\ell^1$-loss, where each\n$\\|\\hat{\\pi}_i - \\pi_i\\|_1$ is re-weighted by the degree parameter $\\theta_i$\nin DCMM (to be introduced).\n  We present a sharp lower bound. We also show that such a lower bound is\nachievable under a broad situation. More discussion in this vein is continued\nin our forthcoming manuscript.\n  The results are very different from those on community detection. For\ncommunity detection, the focus is on the special case where all $\\pi_i$ are\ndegenerate; the goal is clustering, so Hamming distance is the natural choice\nof loss function, and the rate can be exponentially fast. The setting here is\nbroader and more difficult: it is more natural to use the $\\ell^1$-loss, and\nthe rate is only polynomially fast.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 03:23:43 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""]]}, {"id": "1709.05613", "submitter": "Subrata Chakraborty", "authors": "S. Chakraborty, S. H. Ong and C. M. Ng", "title": "A new probability model with support on unit interval: Structural\n  properties, regression of bounded response and applications", "comments": "37 pages; 2 Figures, 3 Table, Pre-print Version-4.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new distribution on (0, 1), generalized Log-Lindley distribution, is\nproposed by extending the Log-Lindley distribution. This new distribution is\nshown to be a weighted Log-Lindley distribution. Important probabilistic and\nstatistical properties have been derived. An interesting characterization of\nthe weighted distribution in terms of Kullback-Liebler distance and weighted\nentropy has also been obtained. A useful result in insurance for the distorted\npremium principal is presented and verified with numerical calculations. New\nregression models for bounded responses based on this distribution and their\napplication is illustrated by considering modeling a real life data on risk\nmanagement and another data set on outpatient health expenditure in comparison\nwith beta regression and Log-Lindley regression models. Much better fits for\nboth data sets justify the relevance of the new distribution in statistical\nmodeling and analysis. Furthermore this generalization, apart from adding\nflexibility for modelling, retains the compactness and tractability of\nstatistical quantities required for statistical analysis, which is a feature of\nthe Log-Lindley distribution. Thus, the generalized Log-Lindley distribution\nshould be a useful addition to statistical models for practitioners.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 06:51:38 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 05:52:26 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 18:35:56 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 05:28:42 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Chakraborty", "S.", ""], ["Ong", "S. H.", ""], ["Ng", "C. M.", ""]]}, {"id": "1709.05673", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Mariela Sued", "title": "Semi-supervised learning", "comments": "The paper as it is now, contains some mistakes in the proofs.\n  Hopefully soon I will submit a new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning deals with the problem of how, if possible, to take\nadvantage of a huge amount of not classified data, to perform classification,\nin situations when, typically, the labelled data are few. Even though this is\nnot always possible (it depends on how useful is to know the distribution of\nthe unlabelled data in the inference of the labels), several algorithm have\nbeen proposed recently. A new algorithm is proposed, that under almost\nneccesary conditions, attains asymptotically the performance of the best\ntheoretical rule, when the size of unlabeled data tends to infinity. The set of\nnecessary assumptions, although reasonables, show that semi-parametric\nclassification only works for very well conditioned problems.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:45:42 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 13:02:03 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Sued", "Mariela", ""]]}, {"id": "1709.05707", "submitter": "Bodhisattva Sen", "authors": "Adityanand Guntuboyina and Bodhisattva Sen", "title": "Nonparametric Shape-restricted Regression", "comments": "This is a survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of nonparametric regression under shape constraints.\nThe main examples include isotonic regression (with respect to any partial\norder), unimodal/convex regression, additive shape-restricted regression, and\nconstrained single index model. We review some of the theoretical properties of\nthe least squares estimator (LSE) in these problems, emphasizing on the\nadaptive nature of the LSE. In particular, we study the behavior of the risk of\nthe LSE, and its pointwise limiting distribution theory, with special emphasis\nto isotonic regression. We survey various methods for constructing pointwise\nconfidence intervals around these shape-restricted functions. We also briefly\ndiscuss the computation of the LSE and indicate some open research problems and\nfuture directions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 19:13:59 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 06:44:04 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1709.06183", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao and Yanjun Han", "title": "Bias Correction with Jackknife, Bootstrap, and Taylor Series", "comments": "to appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze bias correction methods using jackknife, bootstrap, and Taylor\nseries. We focus on the binomial model, and consider the problem of bias\ncorrection for estimating $f(p)$, where $f \\in C[0,1]$ is arbitrary. We\ncharacterize the supremum norm of the bias of general jackknife and bootstrap\nestimators for any continuous functions, and demonstrate the in delete-$d$\njackknife, different values of $d$ may lead to drastically different behaviors\nin jackknife. We show that in the binomial model, iterating the bootstrap bias\ncorrection infinitely many times may lead to divergence of bias and variance,\nand demonstrate that the bias properties of the bootstrap bias corrected\nestimator after $r-1$ rounds are of the same order as that of the $r$-jackknife\nestimator if a bounded coefficients condition is satisfied.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:04:10 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 18:38:39 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 19:59:28 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 00:32:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Jiao", "Jiantao", ""], ["Han", "Yanjun", ""]]}, {"id": "1709.06230", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian", "title": "Truncated Cram\\'er-von Mises test of normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new test of normality based on a standardised empirical process is\nintroduced in this article.\n  The first step is to introduce a Cram\\'er-von Mises type statistic with\nweights equal to the inverse of the standard normal density function supported\non a symmetric interval $[-a_n,a_n]$ depending on the sample size $n.$ The\nsequence of end points $a_n$ tends to infinity, and is chosen so that the\nstatistic goes to infinity at the speed of $\\ln \\ln n.$ After substracting the\nmean, a suitable test statistic is obtained, with the same asymptotic law as\nthe well-known Shapiro-Wilk statistic. The performance of the new test is\ndescribed and compared with three other well-known tests of normality, namely,\nShapiro-Wilk, Anderson-Darling and that of del Barrio-Matr\\'an, Cuesta\nAlbertos, and Rodr\\'{\\i}guez Rodr\\'{\\i}guez, by means of power calculations\nunder many alternative hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 02:48:33 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 01:23:33 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Kalemkerian", "Juan", ""]]}, {"id": "1709.06256", "submitter": "Aur\\'elien Bibaut", "authors": "Mark J. van der Laan, Aur\\'elien F. Bibaut", "title": "Uniform Consistency of the Highly Adaptive Lasso Estimator of Infinite\n  Dimensional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the case that we observe $n$ independent and identically distributed\ncopies of a random variable with a probability distribution known to be an\nelement of a specified statistical model. We are interested in estimating an\ninfinite dimensional target parameter that minimizes the expectation of a\nspecified loss function. In \\cite{generally_efficient_TMLE} we defined an\nestimator that minimizes the empirical risk over all multivariate real valued\ncadlag functions with variation norm bounded by some constant $M$ in the\nparameter space, and selects $M$ with cross-validation. We referred to this\nestimator as the Highly-Adaptive-Lasso estimator due to the fact that the\nconstrained can be formulated as a bound $M$ on the sum of the coefficients a\nlinear combination of a very large number of basis functions. Specifically, in\nthe case that the target parameter is a conditional mean, then it can be\nimplemented with the standard LASSO regression estimator. In\n\\cite{generally_efficient_TMLE} we proved that the HAL-estimator is consistent\nw.r.t. the (quadratic) loss-based dissimilarity at a rate faster than\n$n^{-1/2}$ (i.e., faster than $n^{-1/4}$ w.r.t. a norm), even when the\nparameter space is completely nonparametric. The only assumption required for\nthis rate is that the true parameter function has a finite variation norm. The\nloss-based dissimilarity is often equivalent with the square of an\n$L^2(P_0)$-type norm. In this article, we establish that under some weak\ncontinuity condition, the HAL-estimator is also uniformly consistent.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 05:00:18 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Bibaut", "Aur\u00e9lien F.", ""]]}, {"id": "1709.06272", "submitter": "Udaysinh T. Bhosale", "authors": "Udaysinh T. Bhosale", "title": "Entanglement transitions induced by large deviations", "comments": "12 pages, 4 figures. Comments are welcome", "journal-ref": "Phys. Rev. E 96, 062149 (2017)", "doi": "10.1103/PhysRevE.96.062149", "report-no": null, "categories": "quant-ph math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability of large deviations of the smallest Schmidt eigenvalue for\nrandom pure states of bipartite systems, denoted as $A$ and $B$, is computed\nanalytically using a Coulomb gas method. It is shown that this probability, for\nlarge $N$, goes as $\\exp[-\\beta N^2\\Phi(\\zeta)]$, where the parameter $\\beta$\nis the Dyson index of the ensemble, $\\zeta$ is the large deviation parameter\nwhile the rate function $\\Phi(\\zeta)$ is calculated exactly. Corresponding\nequilibrium Coulomb charge density is derived for its large deviations. Effects\nof the large deviations of the extreme (largest and smallest) Schmidt\neigenvalues on the bipartite entanglement are studied using the von Neumann\nentropy. Effect of these deviations is also studied on the entanglement between\nsubsystems $1$ and $2$, obtained by further partitioning the subsystem $A$,\nusing the properties of the density matrix's partial transpose\n$\\rho_{12}^\\Gamma$. The density of states of $\\rho_{12}^\\Gamma$ is found to be\nclose to the Wigner's semicircle law with these large deviations. The\nentanglement properties are captured very well by a simple random matrix model\nfor the partial transpose. The model predicts the entanglement transition\nacross a critical large deviation parameter $\\zeta$. Log negativity is used to\nquantify the entanglement between subsystems $1$ and $2$. Analytical formulas\nfor it are derived using the simple model. Numerical simulations are in\nexcellent agreement with the analytical results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 07:04:59 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 11:40:51 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 06:15:19 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bhosale", "Udaysinh T.", ""]]}, {"id": "1709.06360", "submitter": "Alisa Kirichenko", "authors": "Alisa Kirichenko and Harry van Zanten", "title": "Minimax lower bounds for function estimation on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax lower bounds for function estimation problems on large graph\nwhen the target function is smoothly varying over the graph. We derive minimax\nrates in the context of regression and classification problems on graphs that\nsatisfy an asymptotic shape assumption and with a smoothness condition on the\ntarget function, both formulated in terms of the graph Laplacian.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 11:43:44 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 12:53:00 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kirichenko", "Alisa", ""], ["van Zanten", "Harry", ""]]}, {"id": "1709.06588", "submitter": "Ye Liang", "authors": "Shangyuan Ye, Ye Liang and Ibrahim A. Ahmad", "title": "Orthogonal Series Density Estimation for Complex Surveys", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": "10.1080/10485252.2019.1585539", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an orthogonal series density estimator for complex surveys, where\nsamples are neither independent nor identically distributed. The proposed\nestimator is proved to be design-unbiased and asymptotically design-consistent.\nThe asymptotic normality is proved under both design and combined spaces. Two\ndata driven estimators are proposed based on the proposed oracle estimator. We\nshow the efficiency of the proposed estimators in simulation studies. A real\nsurvey data example is provided for an illustration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:10:18 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 16:38:37 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 03:55:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ye", "Shangyuan", ""], ["Liang", "Ye", ""], ["Ahmad", "Ibrahim A.", ""]]}, {"id": "1709.06606", "submitter": "Lo\\\"ic Giraldi", "authors": "Lo\\\"ic Giraldi, Olivier P. Le Ma\\^itre, Ibrahim Hoteit and Omar M.\n  Knio", "title": "Optimal projection of observations in a Bayesian setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal dimensionality reduction methods are proposed for the Bayesian\ninference of a Gaussian linear model with additive noise in presence of\noverabundant data. Three different optimal projections of the observations are\nproposed based on information theory: the projection that minimizes the\nKullback-Leibler divergence between the posterior distributions of the original\nand the projected models, the one that minimizes the expected Kullback-Leibler\ndivergence between the same distributions, and the one that maximizes the\nmutual information between the parameter of interest and the projected\nobservations. The first two optimization problems are formulated as the\ndetermination of an optimal subspace and therefore the solution is computed\nusing Riemannian optimization algorithms on the Grassmann manifold. Regarding\nthe maximization of the mutual information, it is shown that there exists an\noptimal subspace that minimizes the entropy of the posterior distribution of\nthe reduced model; a basis of the subspace can be computed as the solution to a\ngeneralized eigenvalue problem; an a priori error estimate on the mutual\ninformation is available for this particular solution; and that the\ndimensionality of the subspace to exactly conserve the mutual information\nbetween the input and the output of the models is less than the number of\nparameters to be inferred. Numerical applications to linear and nonlinear\nmodels are used to assess the efficiency of the proposed approaches, and to\nhighlight their advantages compared to standard approaches based on the\nprincipal component analysis of the observations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:54:42 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 08:04:17 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 15:22:08 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Giraldi", "Lo\u00efc", ""], ["Ma\u00eetre", "Olivier P. Le", ""], ["Hoteit", "Ibrahim", ""], ["Knio", "Omar M.", ""]]}, {"id": "1709.06607", "submitter": "Xuan Cao", "authors": "Xuan Cao, Kshitij Khare, Malay Ghosh", "title": "High-dimensional posterior consistency for hierarchical non-local priors\n  in regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of tuning parameters in Bayesian variable selection is a critical\nproblem in modern statistics. In particular, for Bayesian linear regression\nwith non-local priors, the scale parameter in the non-local prior density is an\nimportant tuning parameter which reflects the dispersion of the non-local prior\ndensity around zero, and implicitly determines the size of the regression\ncoefficients that will be shrunk to zero. Current approaches treat the scale\nparameter as given, and suggest choices based on prior coverage/asymptotic\nconsiderations. In this paper, we consider the fully Bayesian approach\nintroduced in (Wu, 2016) with the pMOM non-local prior and an appropriate\nInverse-Gamma prior on the tuning parameter to analyze the underlying\ntheoretical property. Under standard regularity assumptions, we establish\nstrong model selection consistency in a high-dimensional setting, where $p$ is\nallowed to increase at a polynomial rate with n$or even at a sub-exponential\nrate with n. Through simulation studies, we demonstrate that our model\nselection procedure can outperform other Bayesian methods which treat the scale\nparameter as given, and commonly used penalized likelihood methods, in a range\nof simulation settings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:58:11 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 00:34:48 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 01:23:20 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Cao", "Xuan", ""], ["Khare", "Kshitij", ""], ["Ghosh", "Malay", ""]]}, {"id": "1709.06653", "submitter": "James P. Crutchfield", "authors": "Ryan G. James, Jeffrey Emenheiser, and James P. Crutchfield", "title": "Unique Information via Dependency Constraints", "comments": "15 pages, 7 figures, 2 tables, 3 appendices;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/idep.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial information decomposition (PID) is perhaps the leading proposal\nfor resolving information shared between a set of sources and a target into\nredundant, synergistic, and unique constituents. Unfortunately, the PID\nframework has been hindered by a lack of a generally agreed-upon, multivariate\nmethod of quantifying the constituents. Here, we take a step toward rectifying\nthis by developing a decomposition based on a new method that quantifies unique\ninformation. We first develop a broadly applicable method---the dependency\ndecomposition---that delineates how statistical dependencies influence the\nstructure of a joint distribution. The dependency decomposition then allows us\nto define a measure of the information about a target that can be uniquely\nattributed to a particular source as the least amount which the source-target\nstatistical dependency can influence the information shared between the sources\nand the target. The result is the first measure that satisfies the core axioms\nof the PID framework while not satisfying the Blackwell relation, which depends\non a particular interpretation of how the variables are related. This makes a\nkey step forward to a practical PID.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 21:36:28 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 20:22:28 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 17:48:17 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["James", "Ryan G.", ""], ["Emenheiser", "Jeffrey", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1709.06688", "submitter": "Matey Neykov", "authors": "Matey Neykov and Han Liu", "title": "Property Testing in High Dimensional Ising models", "comments": "72 pages, 10 figures; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the information-theoretic limitations of graph property\ntesting in zero-field Ising models. Instead of learning the entire graph\nstructure, sometimes testing a basic graph property such as connectivity, cycle\npresence or maximum clique size is a more relevant and attainable objective.\nSince property testing is more fundamental than graph recovery, any necessary\nconditions for property testing imply corresponding conditions for graph\nrecovery, while custom property tests can be statistically and/or\ncomputationally more efficient than graph recovery based algorithms.\nUnderstanding the statistical complexity of property testing requires the\ndistinction of ferromagnetic (i.e., positive interactions only) and general\nIsing models. Using combinatorial constructs such as graph packing and strong\nmonotonicity, we characterize how target properties affect the corresponding\nminimax upper and lower bounds within the realm of ferromagnets. On the other\nhand, by studying the detection of an antiferromagnetic (i.e., negative\ninteractions only) Curie-Weiss model buried in Rademacher noise, we show that\nproperty testing is strictly more challenging over general Ising models. In\nterms of methodological development, we propose two types of correlation based\ntests: computationally efficient screening for ferromagnets, and score type\ntests for general models, including a fast cycle presence test. Our correlation\nscreening tests match the information-theoretic bounds for property testing in\nferromagnets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 00:48:43 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 14:57:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1709.06855", "submitter": "Natalie Neumeyer", "authors": "Nick Kloodt and Natalie Neumeyer", "title": "Specification tests in semiparametric transformation models - a\n  multiplier bootstrap approach", "comments": "Comparison to the first version: new title; new content: multiplier\n  bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider semiparametric transformation models, where after pre-estimation\nof a parametric transformation of the response the data are modeled by means of\nnonparametric regression. We suggest subsequent procedures for testing\nlack-of-fit of the regression function and for significance of covariables,\nwhich - in contrast to procedures from the literature - are asymptotically not\ninfluenced by the pre-estimation of the transformation. The test statistics are\nasymptotically pivotal and have the same asymptotic distribution as in\nregression models without transformation. We show validity of a multiplier\nbootstrap procedure which is easier to implement and much less computationally\ndemanding than bootstrap procedures based on the transformation model. In a\nsimulation study we demonstrate the superior performance of the procedure in\ncomparison with the competitors from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 13:43:02 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 10:11:41 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Kloodt", "Nick", ""], ["Neumeyer", "Natalie", ""]]}, {"id": "1709.07022", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou", "title": "Anisotropic functional Fourier deconvolution with long-memory dependent\n  errors: a minimax study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate minimax results for the anisotropic functional deconvolution\nmodel when observations are affected by the presence of long-memory. Under\nspecific conditions about the covariance matrices of the errors, we follow a\nstandard procedure to construct an adaptive wavelet-based estimator that\nattains asymptotically near-optimal convergence rates. These rates depend on\nthe parameter associated with the weakest long-range dependence, and\ndeteriorate as the intensity of long-memory increases. This behavior suggests\nthat the estimator adjusts to the best case scenario and that the weakest LM\ndominates.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:17:31 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 01:31:35 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Benhaddou", "Rida", ""]]}, {"id": "1709.07031", "submitter": "Holger Drees", "authors": "Holger Drees, Laurens de Haan, Feridun Turkman", "title": "Extreme Value Estimation for Discretely Sampled Continuous Processes", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In environmental applications of extreme value statistics, the underlying\nstochastic process is often modeled either as a max-stable process in\ncontinuous time/space or as a process in the domain of attraction of such a\nmax-stable process. In practice, however, the processes are typically only\nobserved at discrete points and one has to resort to interpolation to fill in\nthe gaps. We discuss the influence of such an interpolation on estimators of\nmarginal parameters as well as estimators of the exponent measure. In\nparticular, natural conditions on the fineness of the observational scheme are\ndeveloped which ensure that asymptotically the interpolated estimators behave\nin the same way as the estimators which use fully observed continuous\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:44:42 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 19:50:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Drees", "Holger", ""], ["de Haan", "Laurens", ""], ["Turkman", "Feridun", ""]]}, {"id": "1709.07036", "submitter": "Junwei Lu", "authors": "Cong Ma, Junwei Lu and Han Liu", "title": "Inter-Subject Analysis: Inferring Sparse Interactions with Dense\n  Intra-Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new modeling framework for Inter-Subject Analysis (ISA). The\ngoal of ISA is to explore the dependency structure between different subjects\nwith the intra-subject dependency as nuisance. It has important applications in\nneuroscience to explore the functional connectivity between brain regions under\nnatural stimuli. Our framework is based on the Gaussian graphical models, under\nwhich ISA can be converted to the problem of estimation and inference of the\ninter-subject precision matrix. The main statistical challenge is that we do\nnot impose sparsity constraint on the whole precision matrix and we only assume\nthe inter-subject part is sparse. For estimation, we propose to estimate an\nalternative parameter to get around the non-sparse issue and it can achieve\nasymptotic consistency even if the intra-subject dependency is dense. For\ninference, we propose an \"untangle and chord\" procedure to de-bias our\nestimator. It is valid without the sparsity assumption on the inverse Hessian\nof the log-likelihood function. This inferential method is general and can be\napplied to many other statistical problems, thus it is of independent\ntheoretical interest. Numerical experiments on both simulated and brain imaging\ndata validate our methods and theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 18:57:19 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Ma", "Cong", ""], ["Lu", "Junwei", ""], ["Liu", "Han", ""]]}, {"id": "1709.07097", "submitter": "Tullia Padellini", "authors": "Tullia Padellini and Pierpaolo Brutti", "title": "Persistence Flamelets: multiscale Persistent Homology for kernel density\n  exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been noticeable interest in the study of the \"shape\nof data\". Among the many ways a \"shape\" could be defined, topology is the most\ngeneral one, as it describes an object in terms of its connectivity structure:\nconnected components (topological features of dimension 0), cycles (features of\ndimension 1) and so on. There is a growing number of techniques, generally\ndenoted as Topological Data Analysis, aimed at estimating topological\ninvariants of a fixed object; when we allow this object to change, however,\nlittle has been done to investigate the evolution in its topology. In this work\nwe define the Persistence Flamelets, a multiscale version of one of the most\npopular tool in TDA, the Persistence Landscape. We examine its theoretical\nproperties and we show how it could be used to gain insights on KDEs bandwidth\nparameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 22:45:27 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Padellini", "Tullia", ""], ["Brutti", "Pierpaolo", ""]]}, {"id": "1709.07100", "submitter": "Tullia Padellini", "authors": "Tullia Padellini and Pierpaolo Brutti", "title": "Supervised Learning with Indefinite Topological Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is a recent and growing branch of statistics\ndevoted to the study of the shape of the data. In this work we investigate the\npredictive power of TDA in the context of supervised learning. Since\ntopological summaries, most noticeably the Persistence Diagram, are typically\ndefined in complex spaces, we adopt a kernel approach to translate them into\nmore familiar vector spaces. We define a topological exponential kernel, we\ncharacterize it, and we show that, despite not being positive semi-definite, it\ncan be successfully used in regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 23:05:00 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Padellini", "Tullia", ""], ["Brutti", "Pierpaolo", ""]]}, {"id": "1709.07143", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian", "title": "Fractional iterated Ornstein-Uhlenbeck Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a Gaussian process that arise from the iteration of p\nfractional Ornstein-Uhlenbeck processes generated by the same fractional\nBrownian motion. This iteration results, when the values of lambdas are\npairwise differents, in a particular linear combination of those processes.\nAlthough for $H>1/2$ each term of the linear combination is a long memory\nprocesses, we prove that it results in a short memory processes. We include\napplications to real data that show improvement in predictive performance\ncompared with different ARMA models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 03:15:55 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Kalemkerian", "Juan", ""]]}, {"id": "1709.07155", "submitter": "Marco Gaboardi", "authors": "Marco Gaboardi and Ryan Rogers", "title": "Local Private Hypothesis Testing: Chi-Square Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local model for differential privacy is emerging as the reference model\nfor practical applications collecting and sharing sensitive information while\nsatisfying strong privacy guarantees. In the local model, there is no trusted\nentity which is allowed to have each individual's raw data as is assumed in the\ntraditional curator model for differential privacy. So, individuals' data are\nusually perturbed before sharing them.\n  We explore the design of private hypothesis tests in the local model, where\neach data entry is perturbed to ensure the privacy of each participant.\nSpecifically, we analyze locally private chi-square tests for goodness of fit\nand independence testing, which have been studied in the traditional, curator\nmodel for differential privacy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 04:48:53 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 00:43:53 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Gaboardi", "Marco", ""], ["Rogers", "Ryan", ""]]}, {"id": "1709.07232", "submitter": "Moritz von Rohrscheidt", "authors": "Cornelia Wichelhaus and Moritz von Rohrscheidt", "title": "Bayesian nonparametric inference for the M/G/1 queueing systems based on\n  the marked departure process", "comments": "Supported by the Deutsche Forschungsgemeinschaft (German Research\n  Foundation), grant GRK 1953", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work we study Bayesian nonparametric inference for the\ncontinuous-time M/G/1 queueing system. In the focus of the study is the\nunobservable service time distribution. We assume that the only available data\nof the system are the marked departure process of customers with the marks\nbeing the queue lengths just after departure instants. These marks constitute\nan embedded Markov chain whose distribution may be parametrized by stochastic\nmatrices of a special delta form. We develop the theory in order to obtain\nintegral mixtures of Markov measures with respect to suitable prior\ndistributions. We have found a sufficient statistic with a distribution of a\nso-called S-structure sheding some new light on the inner statistical structure\nof the M/G/1 queue. Moreover, it allows to update suitable prior distributions\nto the posterior. Our inference methods are validated by large sample results\nas posterior consistency and posterior normality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 09:35:12 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Wichelhaus", "Cornelia", ""], ["von Rohrscheidt", "Moritz", ""]]}, {"id": "1709.07264", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and Arnold Janssen", "title": "Detectability of nonparametric signals: higher criticism versus\n  likelihood ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the signal detection problem in high dimensional noise data\n(possibly) containing rare and weak signals. Log-likelihood ratio (LLR) tests\ndepend on unknown parameters, but they are needed to judge the quality of\ndetection tests since they determine the detection regions. The popular Tukey's\nhigher criticism (HC) test was shown to achieve the same completely detectable\nregion as the LLR test does for different (mainly) parametric models. We\npresent a novel technique to prove this result for very general signal models,\nincluding even nonparametric $p$-value models. Moreover, we address the\nfollowing questions which are still pending since the initial paper of Donoho\nand Jin: What happens on the border of the completely detectable region, the\nso-called detection boundary? Does HC keep its optimality there? In particular,\nwe give a complete answer for the heteroscedastic normal mixture model. As a\nbyproduct, we give some new insights about the LLR test's behavior on the\ndetection boundary by discussing, among others, Pitmans's asymptotic efficiency\nas an application of Le Cam's theory.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 11:21:16 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 14:08:54 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Janssen", "Arnold", ""]]}, {"id": "1709.07345", "submitter": "Bercu Bernard", "authors": "Bernard Bercu, Lucile Laulin", "title": "On the multi-dimensional elephant random walk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to investigate the asymptotic behavior of the\nmulti-dimensional elephant random walk (MERW). It is a non-Markovian random\nwalk which has a complete memory of its entire history. A wide range of\nliterature is available on the one-dimensional ERW. Surprisingly, no references\nare available on the MERW. The goal of this paper is to fill the gap by\nextending the results on the one-dimensional ERW to the MERW. In the diffusive\nand critical regimes, we establish the almost sure convergence, the law of\niterated logarithm and the quadratic strong law for the MERW. The asymptotic\nnormality of the MERW, properly normalized, is also provided. In the\nsuperdiffusive regime, we prove the almost sure convergence as well as the mean\nsquare convergence of the MERW. All our analysis relies on asymptotic results\nfor multi-dimensional martingales.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:32:07 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Bercu", "Bernard", ""], ["Laulin", "Lucile", ""]]}, {"id": "1709.07446", "submitter": "Daniel Naiman", "authors": "Daniel Q. Naiman, Edward R. Scheinerman", "title": "Arbitrage and Geometry", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the notion of arbitrage for a situation involving a\ncollection of investments and a payoff matrix describing the return to an\ninvestor of each investment under each of a set of possible scenarios. We\nexplain the Arbitrage Theorem, discuss its geometric meaning, and show its\nequivalence to Farkas' Lemma. We then ask a seemingly innocent question: given\na random payoff matrix, what is the probability of an arbitrage opportunity?\nThis question leads to some interesting geometry involving hyperplane\narrangements and related topics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 13:14:07 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Naiman", "Daniel Q.", ""], ["Scheinerman", "Edward R.", ""]]}, {"id": "1709.07593", "submitter": "Pedro Ramos", "authors": "Pedro Luiz Ramos, Diego Nascimento, Francisco Louzada", "title": "The Long Term Fr\\'echet distribution: Estimation, Properties and its\n  Application", "comments": "13 pages, 2 figures, 7 tables", "journal-ref": "Biom Biostat Int J 6(3): 00170", "doi": "10.15406/bbij.2017.06.00170", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new long-term survival distribution is proposed. The so\ncalled long term Fr\\'echet distribution allows us to fit data where a part of\nthe population is not susceptible to the event of interest. This model may be\nused, for example, in clinical studies where a portion of the population can be\ncured during a treatment. It is shown an account of mathematical properties of\nthe new distribution such as its moments and survival properties. As well is\npresented the maximum likelihood estimators (MLEs) for the parameters. A\nnumerical simulation is carried out in order to verify the performance of the\nMLEs. Finally, an important application related to the leukemia free-survival\ntimes for transplant patients are discussed to illustrates our proposed\ndistribution\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:05:22 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Ramos", "Pedro Luiz", ""], ["Nascimento", "Diego", ""], ["Louzada", "Francisco", ""]]}, {"id": "1709.07601", "submitter": "Yasushi Kawase", "authors": "Yasushi Kawase", "title": "Stochastic Input Models in Online Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study twelve stochastic input models for online problems\nand reveal the relationships among the competitive ratios for the models. The\ncompetitive ratio is defined as the worst ratio between the expected optimal\nvalue and the expected profit of the solution obtained by the online algorithm\nwhere the input distribution is restricted according to the model. To handle a\nbroad class of online problems, we use a framework called request-answer games\nthat is introduced by Ben-David et al. The stochastic input models consist of\ntwo types: known distribution and unknown distribution. For each type, we\nconsider six classes of distributions: dependent distributions, deterministic\ninput, independent distributions, identical independent distribution, random\norder of a deterministic input, and random order of independent distributions.\nAs an application of the models, we consider two basic online problems, which\nare variants of the secretary problem and the prophet inequality problem, under\nthe twelve stochastic input models. We see the difference of the competitive\nratios through these problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:30:53 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Kawase", "Yasushi", ""]]}, {"id": "1709.07752", "submitter": "Jakob S\\\"ohl", "authors": "Richard Nickl and Jakob S\\\"ohl", "title": "Bernstein -- von Mises theorems for statistical inverse problems II:\n  Compound Poisson processes", "comments": "51 pages, to appear in Electronic Journal of Statistics", "journal-ref": "Electron. J. Statist. 13(2) (2019) 3513-3571", "doi": "10.1214/19-EJS1609", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric Bayesian statistical inference for the parameters\ngoverning a pure jump process of the form $$Y_t = \\sum_{k=1}^{N(t)} Z_k,~~~ t\n\\ge 0,$$ where $N(t)$ is a standard Poisson process of intensity $\\lambda$, and\n$Z_k$ are drawn i.i.d.~from jump measure $\\mu$. A high-dimensional wavelet\nseries prior for the L\\'evy measure $\\nu = \\lambda \\mu$ is devised and the\nposterior distribution arises from observing discrete samples $Y_\\Delta,\nY_{2\\Delta}, \\dots, Y_{n\\Delta}$ at fixed observation distance $\\Delta$, giving\nrise to a nonlinear inverse inference problem. We derive contraction rates in\nuniform norm for the posterior distribution around the true L\\'evy density that\nare optimal up to logarithmic factors over H\\\"older classes, as sample size $n$\nincreases. We prove a functional Bernstein-von Mises theorem for the\ndistribution functions of both $\\mu$ and $\\nu$, as well as for the intensity\n$\\lambda$, establishing the fact that the posterior distribution is\napproximated by an infinite-dimensional Gaussian measure whose covariance\nstructure is shown to attain the information lower bound for this inverse\nproblem. As a consequence posterior based inferences, such as nonparametric\ncredible sets, are asymptotically valid and optimal from a frequentist point of\nview.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 13:53:09 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 13:31:21 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 06:20:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Nickl", "Richard", ""], ["S\u00f6hl", "Jakob", ""]]}, {"id": "1709.07778", "submitter": "Abdolnasser Sadeghkhani", "authors": "\\'Eric Marchand, Abdolnasser Sadeghkhani", "title": "On predictive density estimation with additional information", "comments": "30 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on independently distributed $X_1 \\sim N_p(\\theta_1, \\sigma^2_1 I_p)$\nand $X_2 \\sim N_p(\\theta_2, \\sigma^2_2 I_p)$, we consider the efficiency of\nvarious predictive density estimators for $Y_1 \\sim N_p(\\theta_1, \\sigma^2_Y\nI_p)$, with the additional information $\\theta_1 - \\theta_2 \\in A$ and known\n$\\sigma^2_1, \\sigma^2_2, \\sigma^2_Y$. We provide improvements on benchmark\npredictive densities such as plug-in, the maximum likelihood, and the minimum\nrisk equivariant predictive densities. Dominance results are obtained for\n$\\alpha-$divergence losses and include Bayesian improvements for reverse\nKullback-Leibler loss, and Kullback-Leibler (KL) loss in the univariate case\n($p=1$). An ensemble of techniques are exploited, including variance expansion\n(for KL loss), point estimation duality, and concave inequalities.\nRepresentations for Bayesian predictive densities, and in particular for\n$\\hat{q}_{\\pi_{U,A}}$ associated with a uniform prior for $\\theta=(\\theta_1,\n\\theta_2)$ truncated to $\\{\\theta \\in \\mathbb{R}^{2p}: \\theta_1 - \\theta_2 \\in\nA \\}$, are established and are used for the Bayesian dominance findings.\nFinally and interestingly, these Bayesian predictive densities also relate to\nskew-normal distributions, as well as new forms of such distributions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:32:51 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Marchand", "\u00c9ric", ""], ["Sadeghkhani", "Abdolnasser", ""]]}, {"id": "1709.08094", "submitter": "Nhat Ho", "authors": "Nhat Ho, XuanLong Nguyen, Ya'acov Ritov", "title": "Robust estimation of mixing measures in finite mixture models", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finite mixture models, apart from underlying mixing measure, true kernel\ndensity function of each subpopulation in the data is, in many scenarios,\nunknown. Perhaps the most popular approach is to choose some kernel functions\nthat we empirically believe our data are generated from and use these kernels\nto fit our models. Nevertheless, as long as the chosen kernel and the true\nkernel are different, statistical inference of mixing measure under this\nsetting will be highly unstable. To overcome this challenge, we propose\nflexible and efficient robust estimators of the mixing measure in these models,\nwhich are inspired by the idea of minimum Hellinger distance estimator, model\nselection criteria, and superefficiency phenomenon. We demonstrate that our\nestimators consistently recover the true number of components and achieve the\noptimal convergence rates of parameter estimation under both the well- and\nmis-specified kernel settings for any fixed bandwidth. These desirable\nasymptotic properties are illustrated via careful simulation studies with both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 18:37:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1709.08104", "submitter": "Martin Slawski", "authors": "Martin Slawski", "title": "On Principal Components Regression, Random Projections, and Column\n  Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Components Regression (PCR) is a traditional tool for dimension\nreduction in linear regression that has been both criticized and defended. One\nconcern about PCR is that obtaining the leading principal components tends to\nbe computationally demanding for large data sets. While random projections do\nnot possess the optimality properties of the leading principal subspace, they\nare computationally appealing and hence have become increasingly popular in\nrecent years. In this paper, we present an analysis showing that for random\nprojections satisfying a Johnson-Lindenstrauss embedding property, the\nprediction error in subsequent regression is close to that of PCR, at the\nexpense of requiring a slightly large number of random projections than\nprincipal components. Column sub-sampling constitutes an even cheaper way of\nrandomized dimension reduction outside the class of Johnson-Lindenstrauss\ntransforms. We provide numerical results based on synthetic and real data as\nwell as basic theory revealing differences and commonalities in terms of\nstatistical performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 19:12:41 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 03:04:27 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Slawski", "Martin", ""]]}, {"id": "1709.08148", "submitter": "Tong Li", "authors": "Krishnakumar Balasubramanian, Tong Li, Ming Yuan", "title": "On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproducing kernel Hilbert space (RKHS) embedding of distributions offers\na general and flexible framework for testing problems in arbitrary domains and\nhas attracted considerable amount of attention in recent years. To gain\ninsights into their operating characteristics, we study here the statistical\nperformance of such approaches within a minimax framework. Focusing on the case\nof goodness-of-fit tests, our analyses show that a vanilla version of the\nkernel-embedding based test could be suboptimal, and suggest a simple remedy by\nmoderating the embedding. We prove that the moderated approach provides optimal\ntests for a wide range of deviations from the null and can also be made\nadaptive over a large collection of interpolation spaces. Numerical experiments\nare presented to further demonstrate the merits of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 05:28:21 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Li", "Tong", ""], ["Yuan", "Ming", ""]]}, {"id": "1709.08535", "submitter": "Tom Michoel", "authors": "Tom Michoel", "title": "Analytic solution and stationary phase approximation for the Bayesian\n  lasso and elastic net", "comments": "Switched to new NeurIPS style file; 11 pages, 3 figures + appendices\n  29 pages, 3 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso and elastic net linear regression models impose a\ndouble-exponential prior distribution on the model parameters to achieve\nregression shrinkage and variable selection, allowing the inference of robust\nmodels from large data sets. However, there has been limited success in\nderiving estimates for the full posterior distribution of regression\ncoefficients in these models, due to a need to evaluate analytically\nintractable partition function integrals. Here, the Fourier transform is used\nto express these integrals as complex-valued oscillatory integrals over\n\"regression frequencies\". This results in an analytic expansion and stationary\nphase approximation for the partition functions of the Bayesian lasso and\nelastic net, where the non-differentiability of the double-exponential prior\nhas so far eluded such an approach. Use of this approximation leads to highly\naccurate numerical estimates for the expectation values and marginal posterior\ndistributions of the regression coefficients, and allows for Bayesian inference\nof much higher dimensional models than previously possible.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:05:29 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 18:29:17 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 08:09:17 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Michoel", "Tom", ""]]}, {"id": "1709.08795", "submitter": "Krishnakumar Balasubramanian", "authors": "Zhuoran Yang, Krishnakumar Balasubramanian, Han Liu", "title": "On Stein's Identity and Near-Optimal Estimation in High-dimensional\n  Index Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the parametric components of semi-parametric multiple\nindex models in a high-dimensional and non-Gaussian setting. Such models form a\nrich class of non-linear models with applications to signal processing, machine\nlearning and statistics. Our estimators leverage the score function based first\nand second-order Stein's identities and do not require the covariates to\nsatisfy Gaussian or elliptical symmetry assumptions common in the literature.\nMoreover, to handle score functions and responses that are heavy-tailed, our\nestimators are constructed via carefully thresholding their empirical\ncounterparts. We show that our estimator achieves near-optimal statistical rate\nof convergence in several settings. We supplement our theoretical results via\nsimulation experiments that confirm the theory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 03:04:11 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 22:48:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Yang", "Zhuoran", ""], ["Balasubramanian", "Krishnakumar", ""], ["Liu", "Han", ""]]}, {"id": "1709.09009", "submitter": "Simon Vandekar", "authors": "Simon N. Vandekar, Philip T. Reiss, and Russell T. Shinohara", "title": "Interpretable High-Dimensional Inference Via Score Projection with an\n  Application in Neuroimaging", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1448826", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fields of neuroimaging and genetics, a key goal is testing the\nassociation of a single outcome with a very high-dimensional imaging or genetic\nvariable. Often, summary measures of the high-dimensional variable are created\nto sequentially test and localize the association with the outcome. In some\ncases, the results for summary measures are significant, but subsequent tests\nused to localize differences are underpowered and do not identify regions\nassociated with the outcome. Here, we propose a generalization of Rao's score\ntest based on projecting the score statistic onto a linear subspace of a\nhigh-dimensional parameter space. In addition, we provide methods to localize\nsignal in the high-dimensional space by projecting the scores to the subspace\nwhere the score test was performed. This allows for inference in the\nhigh-dimensional space to be performed on the same degrees of freedom as the\nscore test, effectively reducing the number of comparisons. Simulation results\ndemonstrate the test has competitive power relative to others commonly used. We\nillustrate the method by analyzing a subset of the Alzheimer's Disease\nNeuroimaging Initiative dataset. Results suggest cortical thinning of the\nfrontal and temporal lobes may be a useful biological marker of Alzheimer's\nrisk.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 13:36:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vandekar", "Simon N.", ""], ["Reiss", "Philip T.", ""], ["Shinohara", "Russell T.", ""]]}, {"id": "1709.09012", "submitter": "Bin Zhu", "authors": "Bin Zhu and Giacomo Baggio", "title": "On the existence of a solution to a spectral estimation problem\n  \\emph{\\`a la} Byrnes-Georgiou-Lindquist", "comments": "6 pages of two-column draft, accepted for publication in IEEE-TAC", "journal-ref": "IEEE Transactions on Automatic Control ( Volume: 64 , Issue: 2 ,\n  Feb. 2019 ), Page(s): 820 - 825", "doi": "10.1109/TAC.2018.2836984", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parametric spectral estimation problem in the style of Byrnes, Georgiou,\nand Lindquist was posed in \\cite{FPZ-10}, but the existence of a solution was\nonly proved in a special case. Based on their results, we show that a solution\nindeed exists given an arbitrary matrix-valued prior density. The main tool in\nour proof is the topological degree theory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 13:41:44 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 14:25:29 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Zhu", "Bin", ""], ["Baggio", "Giacomo", ""]]}, {"id": "1709.09372", "submitter": "Lionel Truquet", "authors": "Konstantinos Fokianos and Lionel Truquet", "title": "On Categorical Time Series Models With Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of stationarity and ergodicity for autoregressive\nmultinomial logistic time series models which possibly include a latent process\nand are defined by a GARCH-type recursive equation. We improve considerably\nupon the existing results related to stationarity and ergodicity conditions of\nsuch models. Proofs are based on theory developed for chains with complete\nconnections. This approach is based on a useful coupling technique which is\nutilized for studying ergodicity of more general finite-state stochastic\nprocesses. Such processes generalize finite-state Markov chains by assuming\ninfinite order models of past values. For finite order Markov chains, we also\ndiscuss ergodicity properties when some strongly exogenous covariates are\nconsidered in the dynamics of the process.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 07:47:13 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 12:25:12 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 14:48:29 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Fokianos", "Konstantinos", ""], ["Truquet", "Lionel", ""]]}, {"id": "1709.09512", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "Simultaneous-equation Estimation without Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a single equation in a system of linear equations, estimation by\ninstrumental variables is the standard approach. In practice, however, it is\noften difficult to find valid instruments. This paper proposes a maximum\nlikelihood method that does not require instrumental variables; it is\nillustrated by simulation and with a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:45:22 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "1709.09520", "submitter": "Yo Sheena", "authors": "Yo Sheena", "title": "Estimation of a Continuous Distribution on a Real Line by Discretization\n  Methods -- Complete Version--", "comments": "31pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an unknown continuous distribution on a real line, we consider the\napproximate estimation by the discretization. There are two methods for the\ndiscretization. First method is to divide the real line into several intervals\nbefore taking samples (\"fixed interval method\") . Second method is dividing the\nreal line using the estimated percentiles after taking samples (\"moving\ninterval method\"). In either way, we settle down to the estimation problem of a\nmultinomial distribution. We use (symmetrized) $f$-divergence in order to\nmeasure the discrepancy of the true distribution and the estimated one. Our\nmain result is the asymptotic expansion of the risk (i.e. expected divergence)\nup to the second-order term in the sample size. We prove theoretically that the\nmoving interval method is asymptotically superior to the fixed interval method.\nWe also observe how the presupposed intervals (fixed interval method) or\npercentiles (moving interval method) affect the asymptotic risk.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:51:27 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 08:21:46 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Sheena", "Yo", ""]]}, {"id": "1709.09565", "submitter": "Yiqiao Zhong", "authors": "Emmanuel Abbe, Jianqing Fan, Kaizheng Wang and Yiqiao Zhong", "title": "Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank", "comments": "58 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering low-rank structures via eigenvector perturbation analysis is a\ncommon problem in statistical machine learning, such as in factor analysis,\ncommunity detection, ranking, matrix completion, among others. While a large\nvariety of bounds are available for average errors between empirical and\npopulation statistics of eigenvectors, few results are tight for entrywise\nanalyses, which are critical for a number of problems such as community\ndetection.\n  This paper investigates entrywise behaviors of eigenvectors for a large class\nof random matrices whose expectations are low-rank, which helps settle the\nconjecture in Abbe et al. (2014b) that the spectral algorithm achieves exact\nrecovery in the stochastic block model without any trimming or cleaning steps.\nThe key is a first-order approximation of eigenvectors under the $\\ell_\\infty$\nnorm: $$u_k \\approx \\frac{A u_k^*}{\\lambda_k^*},$$ where $\\{u_k\\}$ and\n$\\{u_k^*\\}$ are eigenvectors of a random matrix $A$ and its expectation\n$\\mathbb{E} A$, respectively. The fact that the approximation is both tight and\nlinear in $A$ facilitates sharp comparisons between $u_k$ and $u_k^*$. In\nparticular, it allows for comparing the signs of $u_k$ and $u_k^*$ even if $\\|\nu_k - u_k^*\\|_{\\infty}$ is large. The results are further extended to\nperturbations of eigenspaces, yielding new $\\ell_\\infty$-type bounds for\nsynchronization ($\\mathbb{Z}_2$-spiked Wigner model) and noisy matrix\ncompletion.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 14:55:50 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 19:03:35 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Fan", "Jianqing", ""], ["Wang", "Kaizheng", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1709.09702", "submitter": "Neil A. Spencer", "authors": "Neil A. Spencer and Cosma Rohilla Shalizi", "title": "Projective, Sparse, and Learnable Latent Position Network Models", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling network data using a latent position model, it is typical to\nassume that the nodes' positions are independently and identically distributed.\nHowever, this assumption implies the average node degree grows linearly with\nthe number of nodes, which is inappropriate when the graph is thought to be\nsparse. We propose an alternative assumption---that the latent positions are\ngenerated according to a Poisson point process---and show that it is compatible\nwith various levels of sparsity. Unlike other notions of sparse latent position\nmodels in the literature, our framework also defines a projective sequence of\nprobability models, thus ensuring consistency of statistical inference across\nnetworks of different sizes. We establish conditions for consistent estimation\nof the latent positions, and compare our results to existing frameworks for\nmodeling sparse networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 19:02:54 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 17:19:47 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 21:57:27 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Spencer", "Neil A.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1709.09782", "submitter": "Robert Durrant", "authors": "Ata Kaban and Robert J. Durrant", "title": "Structure-aware error bounds for linear classification with the zero-one\n  loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove risk bounds for binary classification in high-dimensional settings\nwhen the sample size is allowed to be smaller than the dimensionality of the\ntraining set observations. In particular, we prove upper bounds for both\n'compressive learning' by empirical risk minimization (ERM) (that is when the\nERM classifier is learned from data that have been projected from\nhigh-dimensions onto a randomly selected low-dimensional subspace) as well as\nuniform upper bounds in the full high-dimensional space. A novel tool we employ\nin both settings is the 'flipping probability' of Durrant and Kaban (ICML 2013)\nwhich we use to capture benign geometric structures that make a classification\nproblem 'easy' in the sense of demanding a relatively low sample size for\nguarantees of good generalization. Furthermore our bounds also enable us to\nexplain or draw connections between several existing successful classification\nalgorithms. Finally we show empirically that our bounds are informative enough\nin practice to serve as the objective function for learning a classifier (by\nusing them to do so).\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 02:07:02 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Kaban", "Ata", ""], ["Durrant", "Robert J.", ""]]}, {"id": "1709.10250", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Jianbo Chen, Martin J. Wainwright, Michael I. Jordan", "title": "DAGGER: A sequential algorithm for FDR control on DAGs", "comments": "29 pages, 10 figures, accepted for publication by Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a linear-time, single-pass, top-down algorithm for multiple\ntesting on directed acyclic graphs (DAGs), where nodes represent hypotheses and\nedges specify a partial ordering in which hypotheses must be tested. The\nprocedure is guaranteed to reject a sub-DAG with bounded false discovery rate\n(FDR) while satisfying the logical constraint that a rejected node's parents\nmust also be rejected. It is designed for sequential testing settings, when the\nDAG structure is known a priori, but the $p$-values are obtained selectively\n(such as in a sequence of experiments), but the algorithm is also applicable in\nnon-sequential settings when all $p$-values can be calculated in advance (such\nas variable/model selection). Our DAGGER algorithm, shorthand for Greedily\nEvolving Rejections on DAGs, provably controls the false discovery rate under\nindependence, positive dependence or arbitrary dependence of the $p$-values.\nThe DAGGER procedure specializes to known algorithms in the special cases of\ntrees and line graphs, and simplifies to the classical Benjamini-Hochberg\nprocedure when the DAG has no edges. We explore the empirical performance of\nDAGGER using simulations, as well as a real dataset corresponding to a gene\nontology, showing favorable performance in terms of time and power.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:38:11 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 01:21:47 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 20:06:02 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Chen", "Jianbo", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1709.10280", "submitter": "Shanyun Liu", "authors": "Shanyun Liu, Rui She, Pingyi Fan and Khaled B. Letaief", "title": "Non-parametric Message Important Measure: Storage Code Design and\n  Transmission Planning for Big Data", "comments": "30 pages one-colunm, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage and transmission in big data are discussed in this paper, where\nmessage importance is taken into account. Similar to Shannon Entropy and Renyi\nEntropy, we define non-parametric message important measure (NMIM) as a measure\nfor the message importance in the scenario of big data, which can characterize\nthe uncertainty of random events. It is proved that the proposed NMIM can\nsufficiently describe two key characters of big data: rare events finding and\nlarge diversities of events. Based on NMIM, we first propose an effective\ncompressed encoding mode for data storage, and then discuss the channel\ntransmission over some typical channel models. Numerical simulation results\nshow that using our proposed strategy occupies less storage space without\nlosing too much message importance, and there are growth region and saturation\nregion for the maximum transmission, which contributes to designing of better\npractical communication system.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:22:31 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Liu", "Shanyun", ""], ["She", "Rui", ""], ["Fan", "Pingyi", ""], ["Letaief", "Khaled B.", ""]]}, {"id": "1709.10281", "submitter": "Uwe Saint-Mont", "authors": "Uwe Saint-Mont", "title": "Beyond the law of large numbers: Introducing progressive sampling,\n  weaving, the geometric triangle, and corresponding distributions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probability theory and statistics, the IID model represents a single\npopulation, and a large, potentially infinite sample from this population. Main\ntheorems, in particular the central limit theorem and laws of large number\n(LLN) assure convergence, making asymptotic statistics possible.\n  To avoid convergence, it is thus straightforward to consider two populations\nand a sample that ceaselessly fluctuates between them. It is the aim of this\ncontribution to study the effects that thus occur. To this end, we introduce\n\"progressive sampling,\" leading to a straightforward model that is analytically\ntractable. With a minimum of technical overhead, a number of interesting\nresults thus ensue:\n  In particular, one encounters a multiplicate structure (similar to Pascal's\ntriangle) that is associated with a new class of distributions (related to the\nbinomial). Although the argument is completely probabilistic, it entails a\nwell-known fractal structure. It also turns out that the new (global) operation\nof \"weaving\" is equivalent to a certain (local) cascade process.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:22:31 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Saint-Mont", "Uwe", ""]]}, {"id": "1709.10314", "submitter": "Annika Lang", "authors": "Peter E. Creasey and Annika Lang", "title": "Fast generation of isotropic Gaussian random fields on the sphere", "comments": "Corrected link to software in arXiv's online abstract, added journal\n  reference", "journal-ref": "Monte Carlo Meth. Appl., Vol. 24, No. 1, 1-11, March 2018", "doi": "10.1515/mcma-2018-0001", "report-no": null, "categories": "math.NA astro-ph.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient simulation of isotropic Gaussian random fields on the unit\nsphere is a task encountered frequently in numerical applications. A fast\nalgorithm based on Markov properties and fast Fourier Transforms in 1d is\npresented that generates samples on an n x n grid in O(n^2 log n). Furthermore,\nan efficient method to set up the necessary conditional covariance matrices is\nderived and simulations demonstrate the performance of the algorithm. An open\nsource implementation of the code has been made available at\nhttps://github.com/pec27/smerfs .\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 10:12:11 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 13:21:13 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 12:19:42 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Creasey", "Peter E.", ""], ["Lang", "Annika", ""]]}, {"id": "1709.10424", "submitter": "Tulasi Ram  Reddy Annapareddy", "authors": "Tulasi Ram Reddy, Sreekar Vadlamani, D. Yogeshwaran", "title": "Central limit theorem for exponentially quasi-local statistics of spin\n  models on Cayley graphs", "comments": "Minor changes incorporated based on suggestions by referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central limit theorems for linear statistics of lattice random fields\n(including spin models) are usually proven under suitable mixing conditions or\nquasi-associativity. Many interesting examples of spin models do not satisfy\nmixing conditions, and on the other hand, it does not seem easy to show central\nlimit theorem for local statistics via quasi-associativity. In this work, we\nprove general central limit theorems for local statistics and exponentially\nquasi-local statistics of spin models on discrete Cayley graphs with polynomial\ngrowth. Further, we supplement these results by proving similar central limit\ntheorems for random fields on discrete Cayley graphs and taking values in a\ncountable space but under the stronger assumptions of {\\alpha}-mixing (for\nlocal statistics) and exponential {\\alpha}-mixing (for exponentially\nquasi-local statistics). All our central limit theorems assume a suitable\nvariance lower bound like many others in the literature. We illustrate our\ngeneral central limit theorem with specific examples of lattice spin models and\nstatistics arising in computational topology, statistical physics and random\nnetworks. Examples of clustering spin models include quasi-associated spin\nmodels with fast decaying covariances like the off-critical Ising model, level\nsets of Gaussian random fields with fast decaying covariances like the massive\nGaussian free field and determinantal point processes with fast decaying\nkernels. Examples of local statistics include intrinsic volumes, face counts,\ncomponent counts of random cubical complexes while exponentially quasi-local\nstatistics include nearest neighbour distances in spin models and Betti numbers\nof sub-critical random cubical complexes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:22:00 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 06:02:41 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Reddy", "Tulasi Ram", ""], ["Vadlamani", "Sreekar", ""], ["Yogeshwaran", "D.", ""]]}]