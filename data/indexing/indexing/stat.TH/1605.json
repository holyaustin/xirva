[{"id": "1605.00155", "submitter": "Chad Hazlett", "authors": "Chad Hazlett", "title": "Kernel Balancing: A flexible non-parametric weighting procedure for\n  estimating causal effects", "comments": "Work originally included in PhD Thesis, May 2014, MIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the absence of unobserved confounders, matching and weighting methods are\nwidely used to estimate causal quantities including the Average Treatment\nEffect on the Treated (ATT). Unfortunately, these methods do not necessarily\nachieve their goal of making the multivariate distribution of covariates for\nthe control group identical to that of the treated, leaving some (potentially\nmultivariate) functions of the covariates with different means between the two\ngroups. When these \"imbalanced\" functions influence the non-treatment potential\noutcome, the conditioning on observed covariates fails, and ATT estimates may\nbe biased. Kernel balancing, introduced here, targets a weaker requirement for\nunbiased ATT estimation, specifically, that the expected non-treatment\npotential outcome for the treatment and control groups are equal. The\nconditional expectation of the non-treatment potential outcome is assumed to\nfall in the space of functions associated with a choice of kernel, implying a\nset of basis functions in which this regression surface is linear. Weights are\nthen chosen on the control units such that the treated and control group have\nequal means on these basis functions. As a result, the expectation of the\nnon-treatment potential outcome must also be equal for the treated and control\ngroups after weighting, allowing unbiased ATT estimation by subsequent\ndifference in means or an outcome model using these weights. Moreover, the\nweights produced are (1) precisely those that equalize a particular\nkernel-based approximation of the multivariate distribution of covariates for\nthe treated and control, and (2) equivalent to a form of stabilized inverse\npropensity score weighting, though it does not require assuming any model of\nthe treatment assignment mechanism. An R package, KBAL, is provided to\nimplement this approach.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 19:49:20 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Hazlett", "Chad", ""]]}, {"id": "1605.00157", "submitter": "Yingxi Liu", "authors": "Yingxi Liu and Ahmed Tewfik", "title": "Empirical Likelihood Ratio Test with Distribution Function Constraints", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 61, no. 18, pp.\n  4463-4472, Sept.15, 2013", "doi": "10.1109/TSP.2013.2271484", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study non-parametric hypothesis testing problem with\ndistribution function constraints. The empirical likelihood ratio test has been\nwidely used in testing problems with moment (in)equality constraints. However,\nsome detection problems cannot be described using moment (in)equalities. We\npropose a distribution function constraint along with an empirical likelihood\nratio test. This detector is applicable to a wide variety of robust\nparametric/non-parametric detection problems. Since the distribution function\nconstraints provide a more exact description of the null hypothesis, the test\noutperforms the empirical likelihood ratio test with moment constraints as well\nas many popular goodness-of-fit tests, such as the robust Kolmogorov-Smirnov\ntest and the Cram\\'er-von Mises test. Examples from communication systems with\nreal-world noise samples are provided to show their performance. Specifically,\nthe proposed test significantly outperforms the robust Kolmogorov-Smirnov test\nand the Cram\\'er-von Mises test when the null hypothesis is nested in the\nalternative hypothesis. The same example is repeated when we assume no noise\nuncertainty. By doing so, we are able to claim that in our case, it is\nnecessary to include uncertainty in noise distribution. Additionally, the\nasymptotic optimality of the proposed test is provided.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 19:58:57 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Liu", "Yingxi", ""], ["Tewfik", "Ahmed", ""]]}, {"id": "1605.00189", "submitter": "Robert Staudte", "authors": "Robert G. Staudte", "title": "The Shapes of Things to Come: Probability Density Quantiles", "comments": "21 pages, 8 figures, 8 tables", "journal-ref": null, "doi": "10.1080/02331888.2016.1277225", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every discrete or continuous location-scale family having a\nsquare-integrable density, there is a unique continuous probability\ndistribution on the unit interval that is determined by the density-quantile\ncomposition introduced by Parzen in 1979. These probability density quantiles\n(pdQs) only differ in shape, and can be usefully compared with the Hellinger\ndistance or Kullback-Leibler divergences. Convergent empirical estimates of\nthese pdQs are provided, which leads to a robust global fitting procedure of\nshape families to data. Asymmetry can be measured in terms of distance or\ndivergence of pdQs from the symmetric class. Further, a precise classification\nof shapes by tail behavior can be defined simply in terms of pdQ boundary\nderivatives.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 00:37:02 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 01:18:41 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Staudte", "Robert G.", ""]]}, {"id": "1605.00265", "submitter": "Thomas Kahle", "authors": "Thomas Kahle", "title": "On the feasibility of semi-algebraic sets in Poisson regression", "comments": "Extended abstract for ICMS 2016 in Berlin. 6 pages", "journal-ref": "Proc. ICMS 2016, pp 142-147", "doi": "10.1007/978-3-319-42432-3_18", "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing experiments for generalized linear models is difficult because\noptimal designs depend on unknown parameters. The local optimality approach is\nto study the regions in parameter space where a given design is optimal. In\nmany situations these regions are semi-algebraic. We investigate regions of\noptimality using computer tools such as yalmip, qepcad, and Mathematica.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 15:31:55 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Kahle", "Thomas", ""]]}, {"id": "1605.00353", "submitter": "Anru R. Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Rate-Optimal Perturbation Bounds for Singular Subspaces with\n  Applications to High-Dimensional Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Perturbation bounds for singular spaces, in particular Wedin's $\\sin \\Theta$\ntheorem, are a fundamental tool in many fields including high-dimensional\nstatistics, machine learning, and applied mathematics. In this paper, we\nestablish separate perturbation bounds, measured in both spectral and Frobenius\n$\\sin \\Theta$ distances, for the left and right singular subspaces. Lower\nbounds, which show that the individual perturbation bounds are rate-optimal,\nare also given.\n  The new perturbation bounds are applicable to a wide range of problems. In\nthis paper, we consider in detail applications to low-rank matrix denoising and\nsingular space estimation, high-dimensional clustering, and canonical\ncorrelation analysis (CCA). In particular, separate matching upper and lower\nbounds are obtained for estimating the left and right singular spaces. To the\nbest of our knowledge, this is the first result that gives different optimal\nrates for the left and right singular spaces under the same perturbation. In\naddition to these problems, applications to other high-dimensional problems\nsuch as community detection in bipartite networks, multidimensional scaling,\nand cross-covariance matrix estimation are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 04:47:43 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 13:42:47 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 18:11:06 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 05:42:08 GMT"}, {"version": "v5", "created": "Fri, 5 Jun 2020 05:15:03 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1605.00414", "submitter": "Nikolai Dokuchaev", "authors": "Nikolai Dokuchaev", "title": "On sampling theorem with sparse decimated samples: exploring branching\n  spectrum degeneracy", "comments": "arXiv admin note: text overlap with arXiv:1603.04174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates possibility of recovery of sequences from their\ndecimated subsequences. It is shown that this recoverability is associated with\ncertain spectrum degeneracy of a new kind, and that a sequences of a general\nkind can be approximated by sequences featuring this degeneracy. This is\napplied to sparse sampling of continuous time band-limited functions. The paper\nshows that these functions allow an arbitrarily close approximation by\nfunctions that can be recovered from sparse equidistant samples with sampling\ndistance larger than the distance defined by the critical Nyquist rate for the\nunderlying function. This allows to bypass, in a certain sense, the restriction\non the sampling rate defined by the Nyquist rate.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 10:03:00 GMT"}, {"version": "v10", "created": "Mon, 6 Feb 2017 11:21:55 GMT"}, {"version": "v11", "created": "Wed, 8 Feb 2017 16:00:51 GMT"}, {"version": "v12", "created": "Tue, 28 Feb 2017 15:01:32 GMT"}, {"version": "v13", "created": "Mon, 30 Oct 2017 06:56:22 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 07:33:23 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 09:15:19 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 13:39:59 GMT"}, {"version": "v5", "created": "Thu, 9 Jun 2016 07:05:21 GMT"}, {"version": "v6", "created": "Mon, 13 Jun 2016 08:10:21 GMT"}, {"version": "v7", "created": "Sun, 20 Nov 2016 12:34:58 GMT"}, {"version": "v8", "created": "Mon, 2 Jan 2017 13:53:17 GMT"}, {"version": "v9", "created": "Tue, 3 Jan 2017 14:24:56 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Dokuchaev", "Nikolai", ""]]}, {"id": "1605.00499", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen, Timothy Christensen and Elie Tamer", "title": "Monte Carlo Confidence Sets for Identified Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complicated/nonlinear parametric models, it is generally hard to know\nwhether the model parameters are point identified. We provide computationally\nattractive procedures to construct confidence sets (CSs) for identified sets of\nfull parameters and of subvectors in models defined through a likelihood or a\nvector of moment equalities or inequalities. These CSs are based on level sets\nof optimal sample criterion functions (such as likelihood or optimally-weighted\nor continuously-updated GMM criterions). The level sets are constructed using\ncutoffs that are computed via Monte Carlo (MC) simulations directly from the\nquasi-posterior distributions of the criterions. We establish new Bernstein-von\nMises (or Bayesian Wilks) type theorems for the quasi-posterior distributions\nof the quasi-likelihood ratio (QLR) and profile QLR in partially-identified\nregular models and some non-regular models. These results imply that our MC CSs\nhave exact asymptotic frequentist coverage for identified sets of full\nparameters and of subvectors in partially-identified regular models, and have\nvalid but potentially conservative coverage in models with reduced-form\nparameters on the boundary. Our MC CSs for identified sets of subvectors are\nshown to have exact asymptotic coverage in models with singularities. We also\nprovide results on uniform validity of our CSs over classes of DGPs that\ninclude point and partially identified models. We demonstrate good\nfinite-sample coverage properties of our procedures in two simulation\nexperiments. Finally, our procedures are applied to two non-trivial empirical\nexamples: an airline entry game and a model of trade flows.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 14:18:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 23:39:16 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:14:19 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""], ["Tamer", "Elie", ""]]}, {"id": "1605.00533", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Real time change-point detection in a nonlinear quantile model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies in real time change-point detection either focus on the linear\nmodel or use the CUSUM method under classical assumptions on model errors. This\npaper considers the sequential change-point detection in a nonlinear quantile\nmodel. A test statistic based on the CUSUM of the quantile process subgradient\nis proposed and studied. Under null hypothesis that the model does not change,\nthe asymptotic distribution of the test statistic is determined. Under\nalternative hypothesis that at some unknown observation there is a change in\nmodel, the proposed test statistic converges in probability to $\\infty$. These\nresults allow to build the critical regions on open-end and on closed-end\nprocedures. Simulation results, using Monte Carlo technique, investigate the\nperformance of the test statistic, specially for heavy-tailed error\ndistributions. We also compare it with the classical CUSUM test statistic.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 15:40:25 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1605.00868", "submitter": "Mikkel Bennedsen", "authors": "Mikkel Bennedsen and Ulrich Hounyo and Asger Lunde and Mikko S.\n  Pakkanen", "title": "The Local Fractional Bootstrap", "comments": null, "journal-ref": "Scandinavian Journal of Statistics 2018, Vol. 46, No. 1, 329-359", "doi": "10.1111/sjos.12355", "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a bootstrap procedure for high-frequency statistics of Brownian\nsemistationary processes. More specifically, we focus on a hypothesis test on\nthe roughness of sample paths of Brownian semistationary processes, which uses\nan estimator based on a ratio of realized power variations. Our new resampling\nmethod, the local fractional bootstrap, relies on simulating an auxiliary\nfractional Brownian motion that mimics the fine properties of high frequency\ndifferences of the Brownian semistationary process under the null hypothesis.\nWe prove the first order validity of the bootstrap method and in simulations we\nobserve that the bootstrap-based hypothesis test provides considerable\nfinite-sample improvements over an existing test that is based on a central\nlimit theorem. This is important when studying the roughness properties of time\nseries data; we illustrate this by applying the bootstrap method to two\nempirical data sets: we assess the roughness of a time series of high-frequency\nasset prices and we test the validity of Kolmogorov's scaling law in\natmospheric turbulence data.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 12:36:17 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 12:42:07 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bennedsen", "Mikkel", ""], ["Hounyo", "Ulrich", ""], ["Lunde", "Asger", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "1605.01011", "submitter": "Jisu Kim", "authors": "Jisu Kim, Alessandro Rinaldo, Larry Wasserman", "title": "Minimax Rates for Estimating the Dimension of a Manifold", "comments": "54 pages, 11 figures, to be published in Journal of Computational\n  Geometry, Volume 10, Number 1", "journal-ref": null, "doi": "10.20382/jocg.v10i1a3", "report-no": null, "categories": "math.ST cs.CG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms in machine learning and computational geometry require, as\ninput, the intrinsic dimension of the manifold that supports the probability\ndistribution of the data. This parameter is rarely known and therefore has to\nbe estimated. We characterize the statistical difficulty of this problem by\nderiving upper and lower bounds on the minimax rate for estimating the\ndimension. First, we consider the problem of testing the hypothesis that the\nsupport of the data-generating probability distribution is a well-behaved\nmanifold of intrinsic dimension $d_1$ versus the alternative that it is of\ndimension $d_2$, with $d_{1}<d_{2}$. With an i.i.d. sample of size $n$, we\nprovide an upper bound on the probability of choosing the wrong dimension of\n$O\\left( n^{-\\left(d_{2}/d_{1}-1-\\epsilon\\right)n} \\right)$, where $\\epsilon$\nis an arbitrarily small positive number. The proof is based on bounding the\nlength of the traveling salesman path through the data points. We also\ndemonstrate a lower bound of $\\Omega \\left( n^{-(2d_{2}-2d_{1}+\\epsilon)n}\n\\right)$, by applying Le Cam's lemma with a specific set of $d_{1}$-dimensional\nprobability distributions. We then extend these results to get minimax rates\nfor estimating the dimension of well-behaved manifolds. We obtain an upper\nbound of order $O \\left( n^{-(\\frac{1}{m-1}-\\epsilon)n} \\right)$ and a lower\nbound of order $\\Omega \\left( n^{-(2+\\epsilon)n} \\right)$, where $m$ is the\nembedding dimension.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:21:37 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 16:16:14 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 01:17:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kim", "Jisu", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1605.01333", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Beatriz Pateiro-L\\'opez and Alberto\n  Rodr\\'iguez-Casal", "title": "Minimax Estimation of the Volume of a Set with Smooth Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the volume of a compact domain in a\nEuclidean space based on a uniform sample from the domain. We assume the domain\nhas a boundary with positive reach. We propose a data splitting approach to\ncorrect the bias of the plug-in estimator based on the sample alpha-convex\nhull. We show that this simple estimator achieves a minimax lower bound that we\nderive. Some numerical experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 16:21:08 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pateiro-L\u00f3pez", "Beatriz", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1605.01340", "submitter": "Yang Kang", "authors": "Jose Blanchet and Yang Kang", "title": "Sample Out-Of-Sample Inference Based on Wasserstein Distance", "comments": null, "journal-ref": null, "doi": "10.1287/opre.2020.2028", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel inference approach that we call Sample Out-of-Sample (or\nSOS) inference. The approach can be used widely, ranging from semi-supervised\nlearning to stress testing, and it is fundamental in the application of\ndata-driven Distributionally Robust Optimization (DRO). Our method enables\nmeasuring the impact of plausible out-of-sample scenarios in a given\nperformance measure of interest, such as a financial loss. The methodology is\ninspired by Empirical Likelihood (EL), but we optimize the empirical\nWasserstein distance (instead of the empirical likelihood) induced by\nobservations. From a methodological standpoint, our analysis of the asymptotic\nbehavior of the induced Wasserstein-distance profile function shows dramatic\nqualitative differences relative to EL. For instance, in contrast to EL, which\ntypically yields chi-squared weak convergence limits, our asymptotic\ndistributions are often not chi-squared. Also, the rates of convergence that we\nobtain have some dependence on the dimension in a non-trivial way but remain\ncontrolled as the dimension increases.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 16:37:05 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 02:36:03 GMT"}, {"version": "v3", "created": "Wed, 24 Aug 2016 23:47:05 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2020 01:04:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Blanchet", "Jose", ""], ["Kang", "Yang", ""]]}, {"id": "1605.01440", "submitter": "Debraj Das", "authors": "Debraj Das and Soumendra Nath Lahiri", "title": "Second Order Correctness of Perturbation Bootstrap M-Estimator of\n  Multiple Linear Regression Parameter", "comments": "key words: M-Estimation, S.O.C., Perturbation Bootstrap, Edgeworth\n  Expansion, Studentization, Residual Bootstrap, Generalized Bootstrap, Wild\n  Bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the multiple linear regression model $y_{i} = \\boldsymbol{x}'_{i}\n\\boldsymbol{\\beta} + \\epsilon_{i}$, where $\\epsilon_i$'s are independent and\nidentically distributed random variables, $\\mathbf{x}_i$'s are known design\nvectors and $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of parameters. An\neffective way of approximating the distribution of the M-estimator\n$\\boldsymbol{\\bar{\\beta}}_n$, after proper centering and scaling, is the\nPerturbation Bootstrap Method. In this current work, second order results of\nthis non-naive bootstrap method have been investigated. Second order\ncorrectness is important for reducing the approximation error uniformly to\n$o(n^{-1/2})$ to get better inferences. We show that the classical studentized\nversion of the bootstrapped estimator fails to be second order correct. We\nintroduce an innovative modification in the studentized version of the\nbootstrapped statistic and show that the modified bootstrapped pivot is second\norder correct (S.O.C.) for approximating the distribution of the studentized\nM-estimator. Additionally, we show that the Perturbation Bootstrap continues to\nbe S.O.C. when the errors $\\epsilon_i$'s are independent, but may not be\nidentically distributed. These findings establish perturbation Bootstrap\napproximation as a significant improvement over asymptotic normality in the\nregression M-estimation.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 21:22:50 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 23:49:14 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Das", "Debraj", ""], ["Lahiri", "Soumendra Nath", ""]]}, {"id": "1605.01485", "submitter": "Shanshan Ding", "authors": "Shanshan Ding and R. Dennis Cook", "title": "Matrix-Variate Regressions and Envelope Models", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern technology often generates data with complex structures in which both\nresponse and explanatory variables are matrix-valued. Existing methods in the\nliterature are able to tackle matrix-valued predictors but are rather limited\nfor matrix-valued responses. In this article, we study matrix-variate\nregressions for such data, where the response Y on each experimental unit is a\nrandom matrix and the predictor X can be either a scalar, a vector, or a\nmatrix, treated as non-stochastic in terms of the conditional distribution Y|X.\nWe propose models for matrix-variate regressions and then develop envelope\nextensions of these models. Under the envelope framework, redundant variation\ncan be eliminated in estimation and the number of parameters can be notably\nreduced when the matrix-variate dimension is large, possibly resulting in\nsignificant gains in efficiency. The proposed methods are applicable to high\ndimensional settings.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 04:08:30 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 05:46:10 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ding", "Shanshan", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1605.01559", "submitter": "Alain Durmus", "authors": "Alain Durmus and Eric Moulines", "title": "High-dimensional Bayesian inference via the Unadjusted Langevin\n  Algorithm", "comments": "Supplementary material available at\n  https://hal.inria.fr/hal-01176084/. arXiv admin note: substantial text\n  overlap with arXiv:1507.05021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the problem of sampling a high-dimensional\nprobability distribution $\\pi$ having a density with respect to the Lebesgue\nmeasure on $\\mathbb{R}^d$, known up to a normalization constant $x \\mapsto\n\\pi(x)= \\mathrm{e}^{-U(x)}/\\int_{\\mathbb{R}^d} \\mathrm{e}^{-U(y)} \\mathrm{d}\ny$. Such problem naturally occurs for example in Bayesian inference and machine\nlearning. Under the assumption that $U$ is continuously differentiable, $\\nabla\nU$ is globally Lipschitz and $U$ is strongly convex, we obtain non-asymptotic\nbounds for the convergence to stationarity in Wasserstein distance of order $2$\nand total variation distance of the sampling method based on the Euler\ndiscretization of the Langevin stochastic differential equation, for both\nconstant and decreasing step sizes. The dependence on the dimension of the\nstate space of these bounds is explicit. The convergence of an appropriately\nweighted empirical measure is also investigated and bounds for the mean square\nerror and exponential deviation inequality are reported for functions which are\nmeasurable and bounded. An illustration to Bayesian inference for binary\nregression is presented to support our claims.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 11:42:35 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 02:19:57 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 08:55:10 GMT"}, {"version": "v4", "created": "Sun, 15 Jul 2018 09:47:23 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "1605.01678", "submitter": "Thomas Kahle", "authors": "Thomas Kahle and Kaie Kubjas and Mario Kummer and Zvi Rosen", "title": "The geometry of rank-one tensor completion", "comments": "24 pages, 3 figures; v2: Final version, accepted in SIAM Journal of\n  Applied Algebra and Geometry (SIAGA)", "journal-ref": "SIAM J. Appl. Algebra Geometry, 1(1) (2017), 200-221", "doi": "10.1137/16M1074102", "report-no": null, "categories": "math.AG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometry of the set of restrictions of rank-one tensors to some of their\ncoordinates is studied. This gives insight into the problem of rank-one\ncompletion of partial tensors. Particular emphasis is put on the semialgebraic\nnature of the problem, which arises for real tensors with constraints on the\nparameters. The algebraic boundary of the completable region is described for\ntensors parametrized by probability distributions and where the number of\nobserved entries equals the number of parameters. If the observations are on\nthe diagonal of a tensor of format $d\\times\\dots\\times d$, the complete\nsemialgebraic description of the completable region is found.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 18:09:09 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:55:03 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Kahle", "Thomas", ""], ["Kubjas", "Kaie", ""], ["Kummer", "Mario", ""], ["Rosen", "Zvi", ""]]}, {"id": "1605.01936", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "Functional Choice and Non-significance Regions in Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data $y$ and $k$ covariates $x$ the problem is to decide which\ncovariates to include when approximating $y$ by a linear function of the\ncovariates. The decision is based on replacing subsets of the covariates by\ni.i.d. normal random variables and comparing the error with that obtained by\nretaining the subsets. If the two errors are not significantly different for a\nparticular subset it is concluded that the covariates in this subset are no\nbetter than random noise and they are not included in the linear approximation\nto $y$.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 13:44:06 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "1605.02077", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Aaditya Ramdas, Michael I. Jordan, and Martin J.\n  Wainwright", "title": "Function-Specific Mixing Times and Concentration Away from Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow mixing is the central hurdle when working with Markov chains, especially\nthose used for Monte Carlo approximations (MCMC). In many applications, it is\nonly of interest to estimate the stationary expectations of a small set of\nfunctions, and so the usual definition of mixing based on total variation\nconvergence may be too conservative. Accordingly, we introduce\nfunction-specific analogs of mixing times and spectral gaps, and use them to\nprove Hoeffding-like function-specific concentration inequalities. These\nresults show that it is possible for empirical expectations of functions to\nconcentrate long before the underlying chain has mixed in the classical sense,\nand we show that the concentration rates we achieve are optimal up to\nconstants. We use our techniques to derive confidence intervals that are\nsharper than those implied by both classical Markov chain Hoeffding bounds and\nBerry-Esseen-corrected CLT bounds. For applications that require testing,\nrather than point estimation, we show similar improvements over recent\nsequential testing results for MCMC. We conclude by applying our framework to\nreal data examples of MCMC, providing evidence that our theory is both accurate\nand relevant to practice.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 20:00:06 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2016 20:53:08 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1605.02205", "submitter": "Sophon Tunyavetchakit", "authors": "Rainer Dahlhaus and Sophon Tunyavetchakit", "title": "Volatility Decomposition and Estimation in Time-Changed Price Models", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of a spot volatility estimate based on a volatility decomposition\nin a time-changed price-model according to the trading times is investigated.\nIn this model clock-time volatility splits up into the product of tick-time\nvolatility and trading intensity, which both can be estimated from data and\ncontain valuable information. By inspecting these two curves individually we\ngain more insight into the cause and structure of volatility. Several examples\nare provided where the tick-time volatility curve is much smoother than the\nclock-time volatility curve meaning that the major part of fluctuations in\nclock-time volatility is due to fluctuations of the trading intensity. Since\nmicrostructure noise only influences the estimation of the (smooth) tick-time\nvolatility curve, the findings lead to an improved pre-averaging estimator of\nspot volatility. This is reflected by a better rate of convergence of the\nestimator. The asymptotic properties of the estimators are derived by an infill\nasymptotic approach.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 15:23:36 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Tunyavetchakit", "Sophon", ""]]}, {"id": "1605.02214", "submitter": "Denis Chetverikov", "authors": "Denis Chetverikov, Zhipeng Liao, Victor Chernozhukov", "title": "On cross-validated Lasso in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive non-asymptotic error bounds for the Lasso estimator\nwhen the penalty parameter for the estimator is chosen using $K$-fold\ncross-validation. Our bounds imply that the cross-validated Lasso estimator has\nnearly optimal rates of convergence in the prediction, $L^2$, and $L^1$ norms.\nFor example, we show that in the model with the Gaussian noise and under fairly\ngeneral assumptions on the candidate set of values of the penalty parameter,\nthe estimation error of the cross-validated Lasso estimator converges to zero\nin the prediction norm with the $\\sqrt{s\\log p / n}\\times \\sqrt{\\log(p n)}$\nrate, where $n$ is the sample size of available data, $p$ is the number of\ncovariates, and $s$ is the number of non-zero coefficients in the model. Thus,\nthe cross-validated Lasso estimator achieves the fastest possible rate of\nconvergence in the prediction norm up to a small logarithmic factor\n$\\sqrt{\\log(p n)}$, and similar conclusions apply for the convergence rate both\nin $L^2$ and in $L^1$ norms. Importantly, our results cover the case when $p$\nis (potentially much) larger than $n$ and also allow for the case of\nnon-Gaussian noise. Our paper therefore serves as a justification for the\nwidely spread practice of using cross-validation as a method to choose the\npenalty parameter for the Lasso estimator.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 16:52:32 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:15:48 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 10:30:30 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 17:25:24 GMT"}, {"version": "v5", "created": "Tue, 13 Aug 2019 07:29:48 GMT"}, {"version": "v6", "created": "Thu, 6 Feb 2020 18:17:55 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Chetverikov", "Denis", ""], ["Liao", "Zhipeng", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1605.02326", "submitter": "Marzia Marcheselli", "authors": "Lucio Barabesi, Carolina Becatti and Marzia Marcheselli", "title": "The tempered discrete Linnik distribution", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tempered version of the discrete Linnik distribution is introduced in order\nto obtain integer-valued distribution families connected to stable laws. The\nproposal constitutes a generalization of the well-known Poisson-Tweedie law,\nwhich is actually a tempered discrete stable law. The features of the new\ntempered discrete Linnik distribution are explored by providing a series of\nidentities in law - which describe its genesis in terms of mixture and compound\nPoisson law, as well as in terms of mixture discrete stable law. A manageable\nexpression of the corresponding probability function is also provided and\nseveral special cases are analysed.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 15:37:41 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Barabesi", "Lucio", ""], ["Becatti", "Carolina", ""], ["Marcheselli", "Marzia", ""]]}, {"id": "1605.02621", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "Power variations and testing for co-jumps: the small noise approach", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the effects of noise on the bipower variation (BPV),\nrealized volatility (RV) and testing for co-jumps in high-frequency data under\nthe small noise framework. We first establish asymptotic properties of the BPV\nin this framework. In the presence of the small noise, the RV is asymptotically\nbiased and the additional asymptotic conditional variance term appears in its\nlimit distribution. We also give feasible estimation methods of the asymptotic\nconditional variances of the RV. Second, we derive the asymptotic distribution\nof the test statistic proposed in Jacod and Todorov(2009) under the presence of\nsmall noise for testing the presence of co-jumps in two dimensional It\\^o\nsemimartingale. In contrast to the setting in Jacod and Todorov(2009), we show\nthat the additional conditional asymptotic variance terms appear, and give\nconsistent estimation procedures for the asymptotic conditional variances in\norder to make the test feasible. Simulation experiments show that our\nasymptotic results give reasonable approximations in the finite sample cases.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 15:22:34 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 00:32:11 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 06:42:11 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "1605.02693", "submitter": "Eric Hall", "authors": "Eric C. Hall and Garvesh Raskutti and Rebecca Willett", "title": "Inference of High-dimensional Autoregressive Generalized Linear Models", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregressive models characterize a variety of time series in which\nlinear combinations of current and past observations can be used to accurately\npredict future observations. For instance, each element of an observation\nvector could correspond to a different node in a network, and the parameters of\nan autoregressive model would correspond to the impact of the network structure\non the time series evolution. Often these models are used successfully in\npractice to learn the structure of social, epidemiological, financial, or\nbiological neural networks. However, little is known about statistical\nguarantees on estimates of such models in non-Gaussian settings. This paper\naddresses the inference of the autoregressive parameters and associated network\nstructure within a generalized linear model framework that includes Poisson and\nBernoulli autoregressive processes. At the heart of this analysis is a\nsparsity-regularized maximum likelihood estimator. While\nsparsity-regularization is well-studied in the statistics and machine learning\ncommunities, those analysis methods cannot be applied to autoregressive\ngeneralized linear models because of the correlations and potential\nheteroscedasticity inherent in the observations. Sample complexity bounds are\nderived using a combination of martingale concentration inequalities and modern\nempirical process techniques for dependent random variables. These bounds,\nwhich are supported by several simulation studies, characterize the impact of\nvarious network parameters on estimator performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:54:17 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 18:20:18 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Hall", "Eric C.", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""]]}, {"id": "1605.02880", "submitter": "Christophe Ley", "authors": "Holger Dette, Christophe Ley and Francisco Javier Rubio", "title": "Natural (non-)informative priors for skew-symmetric distributions", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an innovative method for constructing proper priors\nfor the skewness (shape) parameter in the skew-symmetric family of\ndistributions. The proposed method is based on assigning a prior distribution\non the perturbation effect of the shape parameter, which is quantified in terms\nof the Total Variation distance. We discuss strategies to translate prior\nbeliefs about the asymmetry of the data into an informative prior distribution\nof this class. We show via a Monte Carlo simulation study that our\nnoninformative priors induce posterior distributions with good frequentist\nproperties, similar to those of the Jeffreys prior. Our informative priors\nyield better results than their competitors from the literature. We also\npropose a scale- and location-invariant prior structure for models with unknown\nlocation and scale parameters and provide sufficient conditions for the\npropriety of the corresponding posterior distribution. Illustrative examples\nare presented using simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 09:53:49 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 11:17:09 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Dette", "Holger", ""], ["Ley", "Christophe", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1605.03017", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Giorgio M. Vitetta, Emilio Sirignano, Francesco Montorsi and Matteo\n  Sola", "title": "Marginalized Particle Filtering and Related Filtering Techniques as\n  Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript a factor graph approach is employed to investigate the\nrecursive filtering problem for a mixed linear/nonlinear state-space model,\ni.e. for a model whose state vector can be partitioned in a linear state\nvariable (characterized by conditionally linear dynamics) and a non linear\nstate variable. Our approach allows us to show that: a) the factor graph\ncharacterizing the considered filtering problem is not cycle free; b) in the\ncase of conditionally linear Gaussian systems, applying the sum-product rule,\ntogether with different scheduling procedures for message passing, to this\ngraph results in both known and novel filtering techniques. In particular, it\nis proved that, on the one hand, adopting a specific message scheduling for\nforward only message passing leads to marginalized particle filtering in a\nnatural fashion; on the other hand, if iterative strategies for message passing\nare employed, novel filtering methods, dubbed turbo filters for their\nconceptual resemblance to the turbo decoding methods devised for concatenated\nchannel codes, can be developed.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 13:51:31 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 09:53:14 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 21:39:23 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Vitetta", "Giorgio M.", ""], ["Sirignano", "Emilio", ""], ["Montorsi", "Francesco", ""], ["Sola", "Matteo", ""]]}, {"id": "1605.03280", "submitter": "Neelesh Upadhye Dr", "authors": "Rakshith Jagannath and Neelesh S Upadhye", "title": "The LASSO Estimator: Distributional Properties", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least absolute shrinkage and selection operator (LASSO) is a popular\ntechnique for simultaneous estimation and model selection. There have been a\nlot of studies on the large sample asymptotic distributional properties of the\nLASSO estimator, but it is also well-known that the asymptotic results can give\na wrong picture of the LASSO estimator's actual finite-sample behavior. The\nfinite sample distribution of the LASSO estimator has been previously studied\nfor the special case of orthogonal models. The aim in this work is to\ngeneralize the finite sample distribution properties of LASSO estimator for a\nreal and linear measurement model in Gaussian noise.\n  In this work, we derive an expression for the finite sample characteristic\nfunction of the LASSO estimator, we then use the Fourier slice theorem to\nobtain an approximate expression for the marginal probability density functions\nof the one-dimensional components of a linear transformation of the LASSO\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 04:47:59 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 04:43:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Jagannath", "Rakshith", ""], ["Upadhye", "Neelesh S", ""]]}, {"id": "1605.03301", "submitter": "Ester Mariucci", "authors": "Ester Mariucci", "title": "Le cam theory on the comparison of statistical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recall the main concepts of the Le Cam theory of statistical experiments ,\nespecially the notion of Le Cam distance and its properties. We also review\nclassical tools for bounding such a distance before presenting some examples. A\nproof of the classical equivalence result between density estimation problems\nand Gaussian white noise models will be analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 06:42:12 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Mariucci", "Ester", ""]]}, {"id": "1605.03310", "submitter": "Jinchi Lv", "authors": "Yingying Fan and Jinchi Lv", "title": "Asymptotic equivalence of regularization methods in thresholded\n  parameter space", "comments": "39 pages, 3 figures", "journal-ref": "Journal of the American Statistical Association 108, 1044-1061", "doi": "10.1080/01621459.2013.803972", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data analysis has motivated a spectrum of regularization\nmethods for variable selection and sparse modeling, with two popular classes of\nconvex ones and concave ones. A long debate has been on whether one class\ndominates the other, an important question both in theory and to practitioners.\nIn this paper, we characterize the asymptotic equivalence of regularization\nmethods, with general penalty functions, in a thresholded parameter space under\nthe generalized linear model setting, where the dimensionality can grow up to\nexponentially with the sample size. To assess their performance, we establish\nthe oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the global\nminimizer for these methods under various prediction and variable selection\nlosses. These results reveal an interesting phase transition phenomenon. For\npolynomially growing dimensionality, the $L_1$-regularization method of Lasso\nand concave methods are asymptotically equivalent, having the same convergence\nrates in the oracle inequalities. For exponentially growing dimensionality,\nconcave methods are asymptotically equivalent but have faster convergence rates\nthan the Lasso. We also establish a stronger property of the oracle risk\ninequalities of the regularization methods, as well as the sampling properties\nof computable solutions. Our new theoretical results are illustrated and\njustified by simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 07:39:16 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "1605.03330", "submitter": "Trisha Maitra Mrs", "authors": "Trisha Maitra and Sourabh Bhattacharya", "title": "On Asymptotic Inference in Stochastic Differential Equations with\n  Time-Varying Covariates", "comments": "Updated version that includes simulation study and real data analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a system of stochastic differential equations\n(SDEs) consisting of time-dependent covariates and consider both fixed and\nrandom effects set-ups. We also allow the functional part associated with the\ndrift function to depend upon unknown parameters. In this general set-up of SDE\nsystem we establish consistency and asymptotic normality of the M LE through\nverification of the regularity conditions required by existing relevant\ntheorems. Besides, we consider the Bayesian approach to learning about the\npopulation parameters, and prove consistency and asymptotic normality of the\ncorresponding posterior distribution. We supplement our theoretical\ninvestigation with simulated and real data analyses, obtaining encouraging\nresults in each case.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:39:32 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 11:10:36 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1605.03333", "submitter": "Trisha Maitra Mrs", "authors": "Trisha Maitra and Sourabh Bhattacharya", "title": "On Classical and Bayesian Asymptotics in Stochastic Differential\n  Equations with Random Effects having Mixture Normal Distributions", "comments": "A significantly updated version. Has appeared in Journal of\n  Statistical Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delattre et al. (2013) considered a system of stochastic differential\nequations (SDEs) in a random effects setup. Under the independent and identical\n(iid) situation, and assuming normal distribution of the random effects, they\nestablished weak consistency of the maximum likelihood estimators (M LEs) of\nthe population parameters of the random effects.\n  In this article, respecting the increasing importance and versatility of\nnormal mixtures and their ability to approximate any standard distribution, we\nconsider the random effects having mixture of normal distributions and prove\nasymptotic results associated with the MLEs in both independent and identical\n(iid) and independent but not identical (non-iid) situations. Besides, we\nconsider iid and non-iid setups under the Bayesian paradigm and establish\nposterior consistency and asymptotic normality of the posterior distribution of\nthe population parameters, even when the number of mixture components is\nunknown and treated as a random variable.\n  Although ours is an independent work, we later noted that Delattre et al.\n(2016) also assumed the SDE setup with normal mixture distribution of the\nrandom effect parameters but considered only the iid case and proved only weak\nconsistency of the M LE under an extra, strong assumption as opposed to strong\nconsistency that we are able to prove without the extra assumption.\nFurthermore, they did not deal with asymptotic normality of M LE or the\nBayesian asymptotics counterpart which we investigate in details.\n  Ample simulation experiments and application to a real, stock market data set\nreveal the importance and usefulness of our methods even for small samples.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:46:31 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 15:03:55 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 07:59:21 GMT"}, {"version": "v4", "created": "Fri, 1 May 2020 11:54:58 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1605.03352", "submitter": "Yan Liu", "authors": "Yan Liu", "title": "Quantile tests in frequency domain for sinusoid models", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For second order stationary processes, the spectral distribution function is\nuniquely deter- mined by the autocovariance functions of the processes. We\ndefine the quantiles of the spectral distribution function and propose two\nestimators for the quantiles. Asymptotic properties of both estimators are\nelucidated and the difference from the quantile estimators in time do- main is\nalso indicated. We construct a testing procedure of quantile tests from the\nasymptotic distribution of the estimators and strong statistical power is shown\nin our numerical studies.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 09:42:18 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Liu", "Yan", ""]]}, {"id": "1605.03433", "submitter": "Adrien Saumard", "authors": "Adrien Saumard", "title": "On optimality of empirical risk minimization in linear aggregation", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of this paper, we show that the small-ball condition,\nrecently introduced by Mendelson (2015), may behave poorly for important\nclasses of localized functions such as wavelets, piecewise polynomials or\ntrigonometric polynomials, in particular leading to suboptimal estimates of the\nrate of convergence of ERM for the linear aggregation problem. In a second\npart, we recover optimal rates of covergence for the excess risk of ERM when\nthe dictionary is made of trigonometric functions. Considering the bounded\ncase, we derive the concentration of the excess risk around a single point,\nwhich is an information far more precise than the rate of convergence. In the\ngeneral setting of a L2 noise, we finally refine the small ball argument by\nrightly selecting the directions we are looking at, in such a way that we\nobtain optimal rates of aggregation for the Fourier dictionary.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 13:25:39 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 11:37:26 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 09:35:34 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Saumard", "Adrien", ""]]}, {"id": "1605.03662", "submitter": "Zhuang Ma", "authors": "Zhuang Ma, Xiaodong Li", "title": "Subspace Perspective on Canonical Correlation Analysis: Dimension\n  Reduction and Minimax Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a fundamental statistical tool for\nexploring the correlation structure between two sets of random variables. In\nthis paper, motivated by recent success of applying CCA to learn low\ndimensional representations of high dimensional objects, we propose to quantify\nthe estimation loss of CCA by the excess prediction loss defined through a\nprediction-after-dimension-reduction framework. Such framework suggests viewing\nCCA estimation as estimating the subspaces spanned by the canonical variates.\nInterestedly, the proposed error metrics derived from the excess prediction\nloss turn out to be closely related to the principal angles between the\nsubspaces spanned by the population and sample canonical variates respectively.\n  We characterize the non-asymptotic minimax rates under the proposed metrics,\nespecially the dependency of the minimax rates on the key quantities including\nthe dimensions, the condition number of the covariance matrices, the canonical\ncorrelations and the eigen-gap, with minimal assumptions on the joint\ncovariance matrix. To the best of our knowledge, this is the first finite\nsample result that captures the effect of the canonical correlations on the\nminimax rates.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 03:09:28 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 03:53:44 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ma", "Zhuang", ""], ["Li", "Xiaodong", ""]]}, {"id": "1605.03707", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai, Hans-Georg M\\\"uller, Fang Yao", "title": "Optimal Bayes Classifiers for Functional Data and Density Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes classifiers for functional data pose a challenge. This is because\nprobability density functions do not exist for functional data. As a\nconsequence, the classical Bayes classifier using density quotients needs to be\nmodified. We propose to use density ratios of projections on a sequence of\neigenfunctions that are common to the groups to be classified. The density\nratios can then be factored into density ratios of individual functional\nprincipal components whence the classification problem is reduced to a sequence\nof nonparametric one-dimensional density estimates. This is an extension to\nfunctional data of some of the very earliest nonparametric Bayes classifiers\nthat were based on simple density ratios in the one-dimensional case. By means\nof the factorization of the density quotients the curse of dimensionality that\nwould otherwise severely affect Bayes classifiers for functional data can be\navoided. We demonstrate that in the case of Gaussian functional data, the\nproposed functional Bayes classifier reduces to a functional version of the\nclassical quadratic discriminant. A study of the asymptotic behavior of the\nproposed classifiers in the large sample limit shows that under certain\nconditions the misclassification rate converges to zero, a phenomenon that has\nbeen referred to as \"perfect classification\". The proposed classifiers also\nperform favorably in finite sample applications, as we demonstrate in\ncomparisons with other functional classifiers in simulations and various data\napplications, including wine spectral data, functional magnetic resonance\nimaging (fMRI) data for attention deficit hyperactivity disorder (ADHD)\npatients, and yeast gene expression data.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:40:07 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Dai", "Xiongtao", ""], ["M\u00fcller", "Hans-Georg", ""], ["Yao", "Fang", ""]]}, {"id": "1605.03751", "submitter": "Vincent Brault", "authors": "Vincent Brault, Sarah Ouadah, Laure Sansonnet, C\\'eline L\\'evy-Leduc", "title": "Nonparametric homogeneity tests and multiple change-point estimation for\n  analyzing large Hi-C data matrices", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel nonparametric approach for estimating the location of\nblock boundaries (change-points) of non-overlapping blocks in a random\nsymmetric matrix which consists of random variables having their distribution\nchanging from one block to the other. Our method is based on a nonparametric\ntwo-sample homogeneity test for matrices that we extend to the more general\ncase of several groups. We first provide some theoretical results for the two\nassociated test statistics and we explain how to derive change-point location\nestimators. Then, some numerical experiments are given in order to support our\nclaims. Finally, our approach is applied to Hi-C data which are used in\nmolecular biology for better understanding the influence of the chromosomal\nconformation on the cells functioning.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 10:43:58 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Brault", "Vincent", ""], ["Ouadah", "Sarah", ""], ["Sansonnet", "Laure", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""]]}, {"id": "1605.03868", "submitter": "Bhaswar Bhattacharya", "authors": "Kwonsang Lee, Bhaswar B. Bhattacharya, Jing Qin, and Dylan S. Small", "title": "A Nonparametric Likelihood Approach for Inference in Instrumental\n  Variable Models", "comments": "Major changes. Updated BL method. New theorems and data analysis\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods allow for inference about the treatment effect\nby controlling for unmeasured confounding in randomized experiments with\nnoncompliance. However, many studies do not consider the observed compliance\nbehavior in the testing procedure, which can lead to a loss of power. In this\npaper, we propose a novel nonparametric likelihood approach, referred to as the\nbinomial likelihood (BL) method, that incorporates information on compliance\nbehavior while overcoming several limitations of previous techniques and\nutilizing the advantages of likelihood methods. Our proposed method produces\nproper estimates of the counterfactual distribution functions by maximizing the\nbinomial likelihood over the space of distribution functions. Using this we\npropose two versions of a binomial likelihood ratio test for the null\nhypothesis of no treatment effect. We show that both versions are more powerful\nto detect any distributional change than existing methods in finite sample\ncases, and are asymptotically equivalent to the two-sample Anderson-Darling\ntest. We also develop an efficient algorithm for computing our estimates, and\napply the binomial likelihood method to a study of the effect of Medicaid\ncoverage on mental health using the Oregon Health Insurance Experiment.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:55:51 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 18:30:18 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 00:19:26 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Lee", "Kwonsang", ""], ["Bhattacharya", "Bhaswar B.", ""], ["Qin", "Jing", ""], ["Small", "Dylan S.", ""]]}, {"id": "1605.03896", "submitter": "Bartosz Ko{\\l}odziejek", "authors": "Hideyuki Ishi and Bartosz Ko{\\l}odziejek", "title": "Characterization of the Riesz Exponential Family on homogeneous cones", "comments": "10 pages", "journal-ref": "Colloq. Math. 158 (2019), no. 1, 45--57", "doi": "10.4064/cm7548-9-2018", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we present a characterization theorem of the Riesz measure and a\nWishart exponential family on homogeneous cones through the invariance property\nof a natural exponential family under the action of the triangular group.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 17:13:17 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 13:52:53 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 14:47:55 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 08:59:41 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Ishi", "Hideyuki", ""], ["Ko\u0142odziejek", "Bartosz", ""]]}, {"id": "1605.04029", "submitter": "Cheng Li", "authors": "Cheng Li, Sanvesh Srivastava, and David B. Dunson", "title": "Simple, Scalable and Accurate Posterior Interval Estimation", "comments": "50 pages, 6 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lack of simple and scalable algorithms for uncertainty\nquantification. Bayesian methods quantify uncertainty through posterior and\npredictive distributions, but it is difficult to rapidly estimate summaries of\nthese distributions, such as quantiles and intervals. Variational Bayes\napproximations are widely used, but may badly underestimate posterior\ncovariance. Typically, the focus of Bayesian inference is on point and interval\nestimates for one-dimensional functionals of interest. In small scale problems,\nMarkov chain Monte Carlo algorithms remain the gold standard, but such\nalgorithms face major problems in scaling up to big data. Various modifications\nhave been proposed based on parallelization and approximations based on\nsubsamples, but such approaches are either highly complex or lack theoretical\nsupport and/or good performance outside of narrow settings. We propose a very\nsimple and general posterior interval estimation algorithm, which is based on\nrunning Markov chain Monte Carlo in parallel for subsets of the data and\naveraging quantiles estimated from each subset. We provide strong theoretical\nguarantees and illustrate performance in several applications.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 02:01:08 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 02:24:41 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Li", "Cheng", ""], ["Srivastava", "Sanvesh", ""], ["Dunson", "David B.", ""]]}, {"id": "1605.04055", "submitter": "Florian Heinrichs", "authors": "Maria Konstantinou and Holger Dette", "title": "Bayesian $D$-optimal designs for error-in-variables models", "comments": "Keywords: error-in-variables models, classical errors, Bayesian\n  optimal designs, D-optimality AMS Subject Classification: 62K05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimality criteria provide a robust design strategy to parameter\nmisspecification. We develop an approximate design theory for Bayesian\n$D$-optimality for non-linear regression models with covariates subject to\nmeasurement errors. Both maximum likelihood and least squares estimation are\nstudied and explicit characterisations of the Bayesian $D$-optimal saturated\ndesigns for the Michaelis-Menten, Emax and exponential regression models are\nprovided. Several data examples are considered for the case of no preference\nfor specific parameter values, where Bayesian $D$-optimal saturated designs are\ncalculated using the uniform prior and compared to several other designs,\nincluding the corresponding locally $D$-optimal designs, which are often used\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 06:16:33 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Konstantinou", "Maria", ""], ["Dette", "Holger", ""]]}, {"id": "1605.04059", "submitter": "Kou Fujimori", "authors": "Kou Fujimori and Yoichi Nishiyama", "title": "The $l_q$ consistency of the Dantzig Selector for Cox's Proportional\n  Hazards Model", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dantzig selector for the proportional hazards model proposed by D.R. Cox\nis studied in a high-dimensional and sparse setting. We prove the $l_q$\nconsistency for all $q \\geq 1$ of some estimators based on the compatibility\nfactor, the weak cone invertibility factor, and the restricted eigenvalue for\ncertain deterministic matrix which approximates the Hessian matrix of log\npartial likelihood. Our matrix conditions for these three factors are weaker\nthan those of previous researches.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 06:31:51 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Fujimori", "Kou", ""], ["Nishiyama", "Yoichi", ""]]}, {"id": "1605.04148", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat, Ioana Gavra, Laurent Risser", "title": "How to compute the barycenter of a weighted graph", "comments": "5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete structures like graphs make it possible to naturally and flexibly\nmodel complex phenomena. Since graphs that represent various types of\ninformation are increasingly available today, their analysis has become a\npopular subject of research. The graphs studied in the field of data science at\nthis time generally have a large number of nodes that are not fairly weighted\nand connected to each other, translating a structural specification of the\ndata. Yet, even an algorithm for locating the average position in graphs is\nlacking although this knowledge would be of primary interest for statistical or\nrepresentation problems. In this work, we develop a stochastic algorithm for\nfinding the Frechet mean of weighted undirected metric graphs. This method\nrelies on a noisy simulated annealing algorithm dealt with using\nhomogenization. We then illustrate our algorithm with two examples (subgraphs\nof a social network and of a collaboration and citation network).\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 12:17:18 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Gavra", "Ioana", ""], ["Risser", "Laurent", ""]]}, {"id": "1605.04323", "submitter": "Michael Burr", "authors": "Michael Burr and Robert Fabrizio", "title": "Error Probabilities for Halfspace Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth functions are a generalization of one-dimensional order statistics\nand medians to real spaces of dimension greater than one; in particular, a data\ndepth function quantifies the centrality of a point with respect to a data set\nor a probability distribution. One of the most commonly studied data depth\nfunctions is halfspace depth. It is of interest to computational geometers\nbecause it is highly geometric, and it is of interest to statisticians because\nit shares many desirable theoretical properties with the one-dimensional\nmedian. As the sample size increases, the halfspace depth for a sample\nconverges to the halfspace depth for the underlying distribution, almost\nsurely. In this paper, we use the geometry of halfspace depth to improve the\nexplicit bounds on the rate of convergence.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 20:23:30 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Burr", "Michael", ""], ["Fabrizio", "Robert", ""]]}, {"id": "1605.04358", "submitter": "Anru Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Minimax Rate-optimal Estimation of High-dimensional Covariance Matrices\n  with Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Missing data occur frequently in a wide range of applications. In this paper,\nwe consider estimation of high-dimensional covariance matrices in the presence\nof missing observations under a general missing completely at random model in\nthe sense that the missingness is not dependent on the values of the data.\nBased on incomplete data, estimators for bandable and sparse covariance\nmatrices are proposed and their theoretical and numerical properties are\ninvestigated.\n  Minimax rates of convergence are established under the spectral norm loss and\nthe proposed estimators are shown to be rate-optimal under mild regularity\nconditions. Simulation studies demonstrate that the estimators perform well\nnumerically. The methods are also illustrated through an application to data\nfrom four ovarian cancer studies. The key technical tools developed in this\npaper are of independent interest and potentially useful for a range of related\nproblems in high-dimensional statistical inference with missing data.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 00:48:38 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1605.04446", "submitter": "Bodhisattva Sen", "authors": "Moulinath Banerjee and Cecile Durot and Bodhisattva Sen", "title": "Divide and Conquer in Non-standard Problems and the Super-efficiency\n  Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the divide and conquer principle --- partition the available\ndata into subsamples, compute an estimate from each subsample and combine these\nappropriately to form the final estimator --- works in non-standard problems\nwhere rates of convergence are typically slower than $\\sqrt{n}$ and limit\ndistributions are non-Gaussian, with a special emphasis on the least squares\nestimator (and its inverse) of a monotone regression function. We find that the\npooled estimator, obtained by averaging non-standard estimates across the\nmutually exclusive subsamples, outperforms the non-standard estimator based on\nthe entire sample in the sense of pointwise inference. We also show that, under\nappropriate conditions, if the number of subsamples is allowed to increase at\nappropriate rates, the pooled estimator is asymptotically normally distributed\nwith a variance that is empirically estimable from the subsample-level\nestimates. Further, in the context of monotone function estimation we show that\nthis gain in pointwise efficiency comes at a price --- the pooled estimator's\nperformance, in a uniform sense (maximal risk) over a class of models worsens\nas the number of subsamples increases, leading to a version of the\nsuper-efficiency phenomenon. In the process, we develop analytical results for\nthe order of the bias in isotonic regression, which are of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 18:09:10 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 09:01:47 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 07:58:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Banerjee", "Moulinath", ""], ["Durot", "Cecile", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1605.04542", "submitter": "Patrick Laurie Davies Mr", "authors": "Patrick Laurie Davies", "title": "On stepwise regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data $y$ and $k$ covariates $x$ one problem in linear regression is to\ndecide which in any of the covariates to include when regressing $y$ on the\n$x$. If $k$ is small it is possible to evaluate each subset of the $x$. If\nhowever $k$ is large then some other procedure must be use. Stepwise regression\nand the lasso are two such procedures but they both assume a linear model with\nerror term. A different approach is taken here which does not assume a model. A\ncovariate is included if it is better than random noise. This defines a\nprocedure which is simple both conceptually and algorithmically\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 13:19:26 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Davies", "Patrick Laurie", ""]]}, {"id": "1605.04565", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Alessandro Rinaldo", "title": "Hierarchical Models for Independence Structures of Networks", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of network models, called hierarchical network\nmodels, that allow us to represent in an explicit manner the stochastic\ndependence among the dyads (random ties) of the network. In particular, each\nmember of this family can be associated with a graphical model defining\nconditional independence clauses among the dyads of the network, called the\ndependency graph. Every network model with dyadic independence assumption can\nbe generalized to construct members of this new family. Using this new\nframework, we generalize the Erd\\\"os-R\\'enyi and beta-models to create\nhierarchical Erd\\\"os-R\\'enyi and beta-models. We describe various methods for\nparameter estimation as well as simulation studies for models with sparse\ndependency graphs.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 15:29:40 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 01:46:24 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1605.04729", "submitter": "Dennis Dobler", "authors": "Dennis Dobler and Markus Pauly", "title": "Inference for the Mann-Whitney Effect for Right-Censored and Tied Data", "comments": "24 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mann-Whitney effect is an intuitive measure for discriminating two\nsurvival distributions. Here we analyze various inference techniques for this\nparameter in a two-sample survival setting with independent right-censoring,\nwhere the survival times are even allowed to be discretely distributed. This\nallows for ties in the data and requires the introduction of normalized\nversions of Kaplan-Meier estimators from which adequate point estimates are\ndeduced. From an asymptotic analysis of the latter, asymptotically exact\ninference procedures based on standard normal, bootstrap- and\npermutation-quantiles are developed and compared in simulations. Here, the\nasymptotically robust and, in case of equal survival and censoring\ndistributions, even finitely exact permutation procedure turned out to be the\nbest. Finally, all procedures are illustrated using a real data set.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 11:04:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 15:36:43 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 10:44:24 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Dobler", "Dennis", ""], ["Pauly", "Markus", ""]]}, {"id": "1605.04796", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Jyotishka Datta, Yunfan Li, Nicholas G. Polson and\n  Brandon Willard", "title": "Prediction risk for the horseshoe regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that prediction performance for global-local shrinkage regression can\novercome two major difficulties of global shrinkage regression: (i) the amount\nof relative shrinkage is monotone in the singular values of the design matrix\nand (ii) the shrinkage is determined by a single tuning parameter.\nSpecifically, we show that the horseshoe regression, with heavy-tailed\ncomponent-specific local shrinkage parameters, in conjunction with a global\nparameter providing shrinkage towards zero, alleviates both these difficulties\nand consequently, results in an improved risk for prediction. Numerical\ndemonstrations of improved prediction over competing approaches in simulations\nand in a pharmacogenomics data set confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 15:09:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 14:12:52 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 20:14:30 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Li", "Yunfan", ""], ["Polson", "Nicholas G.", ""], ["Willard", "Brandon", ""]]}, {"id": "1605.04838", "submitter": "Haihan Tang", "authors": "Christian M. Hafner, Oliver B. Linton, Haihan Tang", "title": "Estimation of a Multiplicative Correlation Structure in the Large\n  Dimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Kronecker product model for correlation or covariance matrices\nin the large dimensional case. The number of parameters of the model increases\nlogarithmically with the dimension of the matrix. We propose a minimum distance\n(MD) estimator based on a log-linear property of the model, as well as a\none-step estimator, which is a one-step approximation to the quasi-maximum\nlikelihood estimator (QMLE). We establish rates of convergence and central\nlimit theorems (CLT) for our estimators in the large dimensional case. A\nspecification test and tools for Kronecker product model selection and\ninference are provided. In a Monte Carlo study where a Kronecker product model\nis correctly specified, our estimators exhibit superior performance. In an\nempirical application to portfolio choice for SP500 daily returns, we\ndemonstrate that certain Kronecker product models are good approximations to\nthe general covariance matrix.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 16:53:10 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 23:17:53 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 23:39:15 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2018 03:07:24 GMT"}, {"version": "v5", "created": "Fri, 17 May 2019 13:38:45 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Hafner", "Christian M.", ""], ["Linton", "Oliver B.", ""], ["Tang", "Haihan", ""]]}, {"id": "1605.05042", "submitter": "Monica Pragliola", "authors": "Daniela Calvetti, Salvatore Cuomo, Monica Pragliola, Erkki Somersalo,\n  Gerardo Toraldo", "title": "Computational issues and numerical experiments for Linear Multistep\n  Method Particle Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linear Multistep Method Particle Filter (LMM PF) is a method for\npredicting the evolution in time of a evolutionary system governed by a system\nof differential equations. If some of the parameters of the governing equations\nare unknowns, it is possible to organize the calculations so as to estimate\nthem while following the evolution of the system in time. The underlying\nassumption in the approach that we present is that all unknowns are modelled as\nrandom variables, where the randomness is an indication of the uncertainty of\ntheir values rather than an intrinsic property of the quantities. Consequently,\nthe states of the system and the parameters are described in probabilistic\nterms by their density, often in the form of representative samples. This\napproach is particularly attractive in the context of parameter estimation\ninverse problems, because the statistical formulation naturally provides a\nmeans of assessing the uncertainty in the solution via the spread of the\ndistribution. The computational efficiency of the underlying sampling technique\nis crucial for the success of the method, because the accuracy of the solution\ndepends on the ability to produce representative samples from the distribution\nof the unknown parameters. In this paper LMM PF is tested on a skeletal muscle\nmetabolism problem, which was previously treated within the Ensemble Kalman\nfiltering framework. Here numerical evidences are used to highlight the\ncorrelation between the main sources of errors and the influence of the linera\nmultistep method adopted. Finally, we analyzed the effect of replacing LMM with\nRunge-Kutta class integration methods for supporting the PF technique.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 07:45:34 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Calvetti", "Daniela", ""], ["Cuomo", "Salvatore", ""], ["Pragliola", "Monica", ""], ["Somersalo", "Erkki", ""], ["Toraldo", "Gerardo", ""]]}, {"id": "1605.05051", "submitter": "Lucien Birg\\'e", "authors": "Yannick Baraud (1), Lucien Birg\\'e (2) ((1) JAD, (2) LPMA)", "title": "Rho-estimators revisited: General theory and applications", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following Baraud, Birg\\'e and Sart (2017), we pursue our attempt to design a\nrobust universal estimator of the joint ditribution of $n$ independent (but not\nnecessarily i.i.d.) observations for an Hellinger-type loss. Given such\nobservations with an unknown joint distribution $\\mathbf{P}$ and a dominated\nmodel $\\mathscr{Q}$ for $\\mathbf{P}$, we build an estimator\n$\\widehat{\\mathbf{P}}$ based on $\\mathscr{Q}$ and measure its risk by an\nHellinger-type distance. When $\\mathbf{P}$ does belong to the model, this risk\nis bounded by some quantity which relies on the local complexity of the model\nin a vicinity of $\\mathbf{P}$. In most situations this bound corresponds to the\nminimax risk over the model (up to a possible logarithmic factor). When\n$\\mathbf{P}$ does not belong to the model, its risk involves an additional bias\nterm proportional to the distance between $\\mathbf{P}$ and $\\mathscr{Q}$,\nwhatever the true distribution $\\mathbf{P}$. From this point of view, this new\nversion of $\\rho$-estimators improves upon the previous one described in\nBaraud, Birg\\'e and Sart (2017) which required that $\\mathbf{P}$ be absolutely\ncontinuous with respect to some known reference measure. Further additional\nimprovements have been brought as compared to the former construction. In\nparticular, it provides a very general treatment of the regression framework\nwith random design as well as a computationally tractable procedure for\naggregating estimators. We also give some conditions for the Maximum Likelihood\nEstimator to be a $\\rho$-estimator. Finally, we consider the situation where\nthe Statistician has at disposal many different models and we build a penalized\nversion of the $\\rho$-estimator for model selection and adaptation purposes. In\nthe regression setting, this penalized estimator not only allows to estimate\nthe regression function but also the distribution of the errors.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 08:16:48 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 15:36:17 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2016 09:34:15 GMT"}, {"version": "v4", "created": "Sun, 2 Jul 2017 10:46:08 GMT"}, {"version": "v5", "created": "Wed, 29 Nov 2017 17:56:14 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Baraud", "Yannick", "", "JAD"], ["Birg\u00e9", "Lucien", "", "LPMA"]]}, {"id": "1605.05055", "submitter": "Jerome Dedecker", "authors": "J\\'er\\^ome Dedecker (MAP5), Florence Merlev\\`ede (LAMA)", "title": "Density estimation for $\\beta$-dependent sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": "MAP5 2016-13", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Lp-integrated risk of some classical estimators of the density,\nwhen the observations are drawn from a strictly stationary sequence. The\nresults apply to a large class of sequences, which can be non-mixing in the\nsense of Rosenblatt and long-range dependent. The main probabilistic tool is a\nnew Rosenthal-type inequality for partial sums of BV functions of the\nvariables. As an application, we give the rates of convergence of regular\nHistograms, when estimating the invariant density of a class of expanding maps\nof the unit interval with a neutral fixed point at zero. These Histograms are\nplotted in the section devoted to the simulations.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 08:21:18 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Dedecker", "J\u00e9r\u00f4me", "", "MAP5"], ["Merlev\u00e8de", "Florence", "", "LAMA"]]}, {"id": "1605.05069", "submitter": "Sergei Kucherenko", "authors": "S. Kucherenko, O.V. Klymenko, N. Shah", "title": "Sobol' indices for problems defined in non-rectangular domains", "comments": "24 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel theoretical and numerical framework for the estimation of Sobol\nsensitivity indices for models in which inputs are confined to a\nnon-rectangular domain (e.g., in presence of inequality constraints) is\ndeveloped. Two numerical methods, namely the quadrature integration method\nwhich may be very efficient for problems of low and medium dimensionality and\nthe MC/QMC estimators based on the acceptance-rejection sampling method are\nproposed for the numerical estimation of Sobol sensitivity indices. Several\nmodel test functions with constraints are considered for which analytical\nsolutions for Sobol sensitivity indices were found. These solutions were used\nas benchmarks for verifying numerical estimates. The method is shown to be\ngeneral and efficient.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 09:20:49 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Kucherenko", "S.", ""], ["Klymenko", "O. V.", ""], ["Shah", "N.", ""]]}, {"id": "1605.05382", "submitter": "Michelle Anzarut Ms", "authors": "Michelle Anzarut and Ramses H. Mena", "title": "A Harris process to model stochastic volatility", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tractable non-independent increment process which provides a\nhigh modeling flexibility. The process lies on an extension of the so-called\nHarris chains to continuous time being stationary and Feller. We exhibit\nconstructions, properties, and inference methods for the process. Afterwards,\nwe use the process to propose a stochastic volatility model with an arbitrary\nbut fixed invariant distribution, which can be tailored to fit different\napplied scenarios. We study the model performance through simulation while\nillustrating its use in practice with empirical work. The model proves to be an\ninteresting competitor to a number of short-range stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 22:15:25 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Anzarut", "Michelle", ""], ["Mena", "Ramses H.", ""]]}, {"id": "1605.05524", "submitter": "Tatiana Labopin-Richard", "authors": "T Labopin-Richard (IMT), V Picheny (UBIA, INRA Toulouse)", "title": "Sequential design of experiments for estimating percentiles of black-box\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating percentiles of black-box deterministic functions with random\ninputs is a challenging task when the number of function evaluations is\nseverely restricted, which is typical for computer experiments. This article\nproposes two new sequential Bayesian methods for percentile estimation based on\nthe Gaussian Process metamodel. Both rely on the Stepwise Uncertainty Reduction\nparadigm, hence aim at providing a sequence of function evaluations that\nreduces an uncertainty measure associated with the percentile estimator. The\nproposed strategies are tested on several numerical examples, showing that\naccurate estimators can be obtained using only a small number of functions\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 11:28:52 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 11:12:23 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Labopin-Richard", "T", "", "IMT"], ["Picheny", "V", "", "UBIA, INRA Toulouse"]]}, {"id": "1605.05615", "submitter": "Dennis Dobler", "authors": "Dennis Dobler", "title": "Bootstrapping the Kaplan-Meier Estimator on the Whole Line", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with proving the consistency of Efron's (1981)\nbootstrap for the Kaplan-Meier estimator on the whole support of a survival\nfunction. While other works address the asymptotic Gaussianity of the estimator\nitself without restricting time (e.g. Gill, 1983, and Ying, 1989), we enable\nthe construction of bootstrap-based time-simultaneous confidence bands for the\nwhole survival function. Other practical applications include bootstrap-based\nconfidence bands for the mean residual life-time function or the Lorenz curve\nas well as confidence intervals for the Gini index.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 15:29:13 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Dobler", "Dennis", ""]]}, {"id": "1605.05671", "submitter": "Natesh Pillai", "authors": "Anirban Bhattacharya, David B. Dunson, Debdeep Pati, Natesh S. Pillai", "title": "Sub-optimality of some continuous shrinkage priors", "comments": "Some of the results were announced in this earlier paper\n  arXiv:1212.6088. To appear in Stochastic Processes and Applications, special\n  issue in memoriam Prof. Evarist Gine", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-component mixture priors provide a traditional way to induce sparsity in\nhigh-dimensional Bayes models. However, several aspects of such a prior,\nincluding computational complexities in high-dimensions, interpretation of\nexact zeros and non-sparse posterior summaries under standard loss functions,\nhas motivated an amazing variety of continuous shrinkage priors, which can be\nexpressed as global-local scale mixtures of Gaussians. Interestingly, we\ndemonstrate that many commonly used shrinkage priors, including the Bayesian\nLasso, do not have adequate posterior concentration in high-dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 17:54:39 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Bhattacharya", "Anirban", ""], ["Dunson", "David B.", ""], ["Pati", "Debdeep", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1605.05785", "submitter": "Shashank Singh", "authors": "Shashank Singh, Simon S. Du, Barnab\\'as P\\'oczos", "title": "Efficient Nonparametric Smoothness Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sobolev quantities (norms, inner products, and distances) of probability\ndensity functions are important in the theory of nonparametric statistics, but\nhave rarely been used in practice, partly due to a lack of practical\nestimators. They also include, as special cases, $L^2$ quantities which are\nused in many applications. We propose and analyze a family of estimators for\nSobolev quantities of unknown probability density functions. We bound the bias\nand variance of our estimators over finite samples, finding that they are\ngenerally minimax rate-optimal. Our estimators are significantly more\ncomputationally tractable than previous estimators, and exhibit a\nstatistical/computational trade-off allowing them to adapt to computational\nconstraints. We also draw theoretical connections to recent work on fast\ntwo-sample testing. Finally, we empirically validate our estimators on\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 00:29:38 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 02:47:02 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Singh", "Shashank", ""], ["Du", "Simon S.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1605.05798", "submitter": "James Johndrow", "authors": "James E. Johndrow, Aaron Smith, Natesh Pillai, David B. Dunson", "title": "MCMC for Imbalanced Categorical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications collect highly imbalanced categorical data, with\nsome categories relatively rare. Bayesian hierarchical models combat data\nsparsity by borrowing information, while also quantifying uncertainty. However,\nposterior computation presents a fundamental barrier to routine use; a single\nclass of algorithms does not work well in all settings and practitioners waste\ntime trying different types of MCMC approaches. This article was motivated by\nan application to quantitative advertising in which we encountered extremely\npoor computational performance for common data augmentation MCMC algorithms but\nobtained excellent performance for adaptive Metropolis. To obtain a deeper\nunderstanding of this behavior, we give strong theory results on computational\ncomplexity in an infinitely imbalanced asymptotic regime. Our results show\ncomputational complexity of Metropolis is logarithmic in sample size, while\ndata augmentation is polynomial in sample size. The root cause of poor\nperformance of data augmentation is a discrepancy between the rates at which\nthe target density and MCMC step sizes concentrate. In general, MCMC algorithms\nthat have a similar discrepancy will fail in large samples - a result with\nsubstantial practical impact.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 02:45:46 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 15:06:27 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Johndrow", "James E.", ""], ["Smith", "Aaron", ""], ["Pillai", "Natesh", ""], ["Dunson", "David B.", ""]]}, {"id": "1605.05898", "submitter": "Tim Sullivan", "authors": "T. J. Sullivan", "title": "Well-posed Bayesian inverse problems and heavy-tailed stable\n  quasi-Banach space priors", "comments": "To appear in Inverse Problems and Imaging. This preprint differs from\n  the final published version in layout and typographical details", "journal-ref": null, "doi": "10.3934/ipi.2017040", "report-no": null, "categories": "math.PR math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article extends the framework of Bayesian inverse problems in\ninfinite-dimensional parameter spaces, as advocated by Stuart (Acta Numer.\n19:451--559, 2010) and others, to the case of a heavy-tailed prior measure in\nthe family of stable distributions, such as an infinite-dimensional Cauchy\ndistribution, for which polynomial moments are infinite or undefined. It is\nshown that analogues of the Karhunen--Lo\\`eve expansion for square-integrable\nrandom variables can be used to sample such measures on quasi-Banach spaces.\nFurthermore, under weaker regularity assumptions than those used to date, the\nBayesian posterior measure is shown to depend Lipschitz continuously in the\nHellinger metric upon perturbations of the misfit function and observed data.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 11:44:51 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 17:53:09 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 09:58:14 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 15:34:41 GMT"}, {"version": "v5", "created": "Fri, 30 Jun 2017 13:38:06 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sullivan", "T. J.", ""]]}, {"id": "1605.05899", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama and Toshio Ohnishi", "title": "Harmonic Bayesian prediction under alpha-divergence", "comments": "28 pages, major revision, 1: The minimaxity proof is added. 2: The\n  heat equation is no longer necessary", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Bayesian shrinkage methods for constructing predictive\ndistributions. We consider the multivariate Normal model with a known\ncovariance matrix and show that the Bayesian predictive density with respect to\nStein's harmonic prior dominates the best invariant Bayesian predictive\ndensity, when the dimension is greater than three. Alpha-divergence from the\ntrue distribution to a predictive distribution is adopted as a loss function.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 11:46:02 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 08:22:40 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 08:48:53 GMT"}, {"version": "v4", "created": "Fri, 28 Jul 2017 09:22:19 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Ohnishi", "Toshio", ""]]}, {"id": "1605.05933", "submitter": "T. Tien Mai", "authors": "The Tien Mai, Pierre Alquier", "title": "Pseudo-Bayesian Quantum Tomography with Rank-adaptation", "comments": null, "journal-ref": null, "doi": "10.1016/j.jspi.2016.11.003", "report-no": null, "categories": "math.ST math-ph math.MP quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum state tomography, an important task in quantum information\nprocessing, aims at reconstructing a state from prepared measurement data.\nBayesian methods are recognized to be one of the good and reliable choice in\nestimating quantum states~\\cite{blume2010optimal}. Several numerical works\nshowed that Bayesian estimations are comparable to, and even better than other\nmethods in the problem of $1$-qubit state recovery. However, the problem of\nchoosing prior distribution in the general case of $n$ qubits is not\nstraightforward. More importantly, the statistical performance of Bayesian type\nestimators have not been studied from a theoretical perspective yet. In this\npaper, we propose a novel prior for quantum states (density matrices), and we\ndefine pseudo-Bayesian estimators of the density matrix. Then, using\nPAC-Bayesian theorems, we derive rates of convergence for the posterior mean.\nThe numerical performance of these estimators are tested on simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 13:12:15 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 06:08:03 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Mai", "The Tien", ""], ["Alquier", "Pierre", ""]]}, {"id": "1605.06278", "submitter": "Ritabrata Sengupta", "authors": "K. R. Parthasarathy and Ritabrata Sengupta", "title": "On the Kolmogorov--Wiener--Masani spectrum of a multi-mode weakly\n  stationary quantum process", "comments": "17 pages, added Theorem 4.2 and some remarks. Comments welcome.\n  Keywords: Weakly stationary quantum process, Kolmogorov-Wiener-Masani\n  spectrum, autocovariance map, spectral representation, uncertainty relations", "journal-ref": "Communications on Stochastic Analysis, Vol. 10, No. 4 (2016),\n  433-449", "doi": "10.31390/cosa.10.4.04", "report-no": null, "categories": "quant-ph math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of a $k$-mode weakly stationary quantum process\n$\\varrho$ based on the canonical Schr\\\"odinger pairs of position and momentum\nobservables in copies of $L^2(\\mathbb{R}^k)$, indexed by an additive abelian\ngroup $D$ of countable cardinality. Such observables admit an autocovariance\nmap $\\widetilde{K}$ from $D$ into the space of real $2k \\times 2k$ matrices.\nThe map $\\widetilde{K}$ on the discrete group $D$ admits a spectral\nrepresentation as the Fourier transform of a $2k \\times 2k$ complex Hermitain\nmatrix-valued totally finite measure $\\Phi$ on the compact character group\n$\\widehat{D}$, called the Kolmogorov-Wiener-Masani (KWM) spectrum of the\nprocess $\\varrho$. Necessary and sufficient conditions on a $2k \\times 2k$\ncomplex Hermitian matrix-valued measure $\\Phi$ on $\\widehat{D}$ to be the KWM\nspectrum of a process $\\varrho$ are obtained. This enables the construction of\nexamples. Our theorem reveals the dramatic influence of the uncertainty\nrelations among the position and momentum observables on the KWM spectrum of\nthe process $\\varrho$. In particular, KWM spectrum cannot admit a gap of\npositive Haar measure in $\\widehat{D}$.\n  The relationship between the number of photons in a particular mode at any\nsite of the process and its KWM spectrum needs further investigation.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 10:40:18 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 06:55:04 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Parthasarathy", "K. R.", ""], ["Sengupta", "Ritabrata", ""]]}, {"id": "1605.06362", "submitter": "Julia Schulte", "authors": "Astrid Kousholt and Julia Schulte", "title": "Reconstruction of convex bodies from moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how much information about a convex body can be retrieved from\na finite number of its geometric moments. We give a sufficient condition for a\nconvex body to be uniquely determined by a finite number of its geometric\nmoments, and we show that among all convex bodies, those which are uniquely\ndetermined by a finite number of moments form a dense set. Further, we derive a\nstability result for convex bodies based on geometric moments. It turns out\nthat the stability result is improved considerably by using another set of\nmoments, namely Legendre moments. We present a reconstruction algorithm that\napproximates a convex body using a finite number of its Legendre moments. The\nconsistency of the algorithm is established using the stability result for\nLegendre moments. When only noisy measurements of Legendre moments are\navailable, the consistency of the algorithm is established under certain\nassumptions on the variance of the noise variables.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 14:05:40 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 13:54:24 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:18:20 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kousholt", "Astrid", ""], ["Schulte", "Julia", ""]]}, {"id": "1605.06416", "submitter": "Jisu Kim", "authors": "Jisu Kim, Yen-Chi Chen, Sivaraman Balakrishnan, Alessandro Rinaldo,\n  Larry Wasserman", "title": "Statistical Inference for Cluster Trees", "comments": "20 pages, 6 figures, accepted in Neural Information Processing\n  Systems (NIPS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cluster tree provides a highly-interpretable summary of a density function\nby representing the hierarchy of its high-density clusters. It is estimated\nusing the empirical tree, which is the cluster tree constructed from a density\nestimator. This paper addresses the basic question of quantifying our\nuncertainty by assessing the statistical significance of topological features\nof an empirical cluster tree. We first study a variety of metrics that can be\nused to compare different trees, analyze their properties and assess their\nsuitability for inference. We then propose methods to construct and summarize\nconfidence sets for the unknown true cluster tree. We introduce a partial\nordering on cluster trees which we use to prune some of the statistically\ninsignificant features of the empirical tree, yielding interpretable and\nparsimonious cluster trees. Finally, we illustrate the proposed methods on a\nvariety of synthetic examples and furthermore demonstrate their utility in the\nanalysis of a Graft-versus-Host Disease (GvHD) data set.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:04:01 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 03:00:06 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 17:12:00 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kim", "Jisu", ""], ["Chen", "Yen-Chi", ""], ["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1605.06420", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, James Zou", "title": "Quantifying the accuracy of approximate diffusions and Markov chains", "comments": "In Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chains and diffusion processes are indispensable tools in machine\nlearning and statistics that are used for inference, sampling, and modeling.\nWith the growth of large-scale datasets, the computational cost associated with\nsimulating these stochastic processes can be considerable, and many algorithms\nhave been proposed to approximate the underlying Markov chain or diffusion. A\nfundamental question is how the computational savings trade off against the\nstatistical error incurred due to approximations. This paper develops general\nresults that address this question. We bound the Wasserstein distance between\nthe equilibrium distributions of two diffusions as a function of their mixing\nrates and the deviation in their drifts. We show that this error bound is tight\nin simple Gaussian settings. Our general result on continuous diffusions can be\ndiscretized to provide insights into the computational-statistical trade-off of\nMarkov chains. As an illustration, we apply our framework to derive\nfinite-sample error bounds of approximate unadjusted Langevin dynamics. We\ncharacterize computation-constrained settings where, by using fast-to-compute\napproximate gradients in the Langevin dynamics, we obtain more accurate samples\ncompared to using the exact gradients. Finally, as an additional application of\nour approach, we quantify the accuracy of approximate zig-zag sampling. Our\ntheoretical analyses are supported by simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:17:22 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 17:30:56 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 15:06:21 GMT"}, {"version": "v4", "created": "Wed, 30 Aug 2017 14:50:01 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Zou", "James", ""]]}, {"id": "1605.06422", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala, Marc Lelarge and Lenka Zdeborov\\'a", "title": "Fast Randomized Semi-Supervised Clustering", "comments": null, "journal-ref": "Journal of Physics: Conf. Series 1036 (2018) 012015", "doi": "10.1088/1742-6596/1036/1/012015", "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering partially labeled data from a minimal\nnumber of randomly chosen pairwise comparisons between the items. We introduce\nan efficient local algorithm based on a power iteration of the non-backtracking\noperator and study its performance on a simple model. For the case of two\nclusters, we give bounds on the classification error and show that a small\nerror can be achieved from $O(n)$ randomly chosen measurements, where $n$ is\nthe number of items in the dataset. Our algorithm is therefore efficient both\nin terms of time and space complexities. We also investigate numerically the\nperformance of the algorithm on synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:21:13 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 16:26:16 GMT"}, {"version": "v3", "created": "Sun, 9 Oct 2016 07:45:16 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Lelarge", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1605.06566", "submitter": "Peng Ding", "authors": "Peng Ding, Avi Feller, Luke Miratrix", "title": "Decomposing Treatment Effect Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and characterizing treatment effect variation in randomized\nexperiments has become essential for going beyond the \"black box\" of the\naverage treatment effect. Nonetheless, traditional statistical approaches often\nignore or assume away such variation. In the context of randomized experiments,\nthis paper proposes a framework for decomposing overall treatment effect\nvariation into a systematic component explained by observed covariates and a\nremaining idiosyncratic component. Our framework is fully randomization-based,\nwith estimates of treatment effect variation that are entirely justified by the\nrandomization itself. Our framework can also account for noncompliance, which\nis an important practical complication. We make several contributions. First,\nwe show that randomization-based estimates of systematic variation are very\nsimilar in form to estimates from fully-interacted linear regression and two\nstage least squares. Second, we use these estimators to develop an omnibus test\nfor systematic treatment effect variation, both with and without noncompliance.\nThird, we propose an $R^2$-like measure of treatment effect variation explained\nby covariates and, when applicable, noncompliance. Finally, we assess these\nmethods via simulation studies and apply them to the Head Start Impact Study, a\nlarge-scale randomized experiment.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 00:48:22 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 17:28:06 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke", ""]]}, {"id": "1605.06703", "submitter": "Gaspar Massiot", "authors": "Nicolas Klutchnikoff and Gaspar Massiot", "title": "Kernel estimation of the intensity of Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting processes often written $N=(N_t)_{t\\in\\mathbb{R}^+}$ are used in\nseveral applications of biostatistics, notably for the study of chronic\ndiseases. In the case of respiratory illness it is natural to suppose that the\ncount of the visits of a patient can be described by such a process which\nintensity depends on environmental covariates. Cox processes (also called\ndoubly stochastic Poisson processes) allows to model such situations. The\nrandom intensity then writes $\\lambda(t)=\\theta(t,Z_t)$ where $\\theta$ is a\nnon-random function, $t\\in\\mathbb{R}^+$ is the time variable and\n$(Z_t)_{t\\in\\mathbb{R}^+}$ is the $d$-dimensional covariates process. For a\nlongitudinal study over $n$ patients, we observe\n$(N_t^k,Z_t^k)_{t\\in\\mathbb{R}^+}$ for $k=1,\\ldots,n$. The intention is to\nestimate the intensity of the process using these observations and to study the\nproperties of this estimator.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 21:36:12 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Klutchnikoff", "Nicolas", ""], ["Massiot", "Gaspar", ""]]}, {"id": "1605.06718", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Arthur P. Guillaumin, Jonathan M.\n  Lilly, Jeffrey J. Early", "title": "The De-Biased Whittle Likelihood", "comments": "To appear shortly in Biometrika. Full published version includes\n  extensions of theory to non-Gaussian processes, and new simulation examples\n  with an AR(4) and non-Gaussian process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Whittle likelihood is a widely used and computationally efficient\npseudo-likelihood. However, it is known to produce biased parameter estimates\nfor large classes of models. We propose a method for de-biasing Whittle\nestimates for second-order stationary stochastic processes. The de-biased\nWhittle likelihood can be computed in the same $\\mathcal{O}(n\\log n)$\noperations as the standard approach. We demonstrate the superior performance of\nthe method in simulation studies and in application to a large-scale\noceanographic dataset, where in both cases the de-biased approach reduces bias\nby up to two orders of magnitude, achieving estimates that are close to exact\nmaximum likelihood, at a fraction of the computational cost. We prove that the\nmethod yields estimates that are consistent at an optimal convergence rate of\n$n^{-1/2}$, under weaker assumptions than standard theory, where we do not\nrequire that the power spectral density is continuous in frequency. We describe\nhow the method can be easily combined with standard methods of bias reduction,\nsuch as tapering and differencing, to further reduce bias in parameter\nestimates.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 00:47:52 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 23:38:15 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 15:39:43 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Guillaumin", "Arthur P.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1605.06759", "submitter": "Michael Eichler", "authors": "Michael Eichler and Rainer Dahlhaus and Johannes Dueck", "title": "Graphical Modeling for Multivariate Hawkes Processes with Nonparametric\n  Link Functions", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hawkes (1971) introduced a powerful multivariate point process model of\nmutually exciting processes to explain causal structure in data. In this paper\nit is shown that the Granger causality structure of such processes is fully\nencoded in the corresponding link functions of the model. A new nonparametric\nestimator of the link functions based on a time-discretized version of the\npoint process is introduced by using an infinite order autoregression.\nConsistency of the new estimator is derived. The estimator is applied to\nsimulated data and to neural spike train data from the spinal dorsal horn of a\nrat.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 08:54:13 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Eichler", "Michael", ""], ["Dahlhaus", "Rainer", ""], ["Dueck", "Johannes", ""]]}, {"id": "1605.06792", "submitter": "Sivan Sabato", "authors": "Aryeh Kontorovich, Sivan Sabato, Ruth Urner", "title": "Active Nearest-Neighbor Learning in Metric Spaces", "comments": null, "journal-ref": "A. Kontorovich, S. Sabato, R. Urner, \"Active Nearest-Neighbor\n  Learning in Metric Spaces\", Journal of Machine Learning Research,\n  18(195):1--38, 2018", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pool-based non-parametric active learning algorithm for general\nmetric spaces, called MArgin Regularized Metric Active Nearest Neighbor\n(MARMANN), which outputs a nearest-neighbor classifier. We give prediction\nerror guarantees that depend on the noisy-margin properties of the input\nsample, and are competitive with those obtained by previously proposed passive\nlearners. We prove that the label complexity of MARMANN is significantly lower\nthan that of any passive learner with similar error guarantees. MARMANN is\nbased on a generalized sample compression scheme, and a new label-efficient\nactive model-selection procedure.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 14:00:27 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 08:23:18 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 13:42:26 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Sabato", "Sivan", ""], ["Urner", "Ruth", ""]]}, {"id": "1605.06866", "submitter": "Akram Hourani", "authors": "Akram Al-Hourani, Bill Moran, Sithamparanathan Kandeepan", "title": "Regularizing Random Points: Complementary Mat\\'ern Hard-Core Point\n  Process", "comments": "This paper has been withdrawn by the author due to a possible error\n  in equation (5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a tractable approach for regularizing randomly\nplaced points, by splitting them into two subsets: the first is generated by\nmeans of the Mat\\'ern hard-core point process, while the remaining points\nconstitute the complementary Mat\\'ern hard-core point process. We study the\ncharacteristics of these processes, deriving its pair-correlation functions,\nand the distribution of the distance to the nearest neighbour. The results have\nseveral applications in wireless communications, including the modeling of\nwireless sensor networks, where we investigate an example of regularizing such\nnetworks and illustrate its advantage in reducing the energy consumption of\nwireless nodes.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 00:24:24 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:00:56 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Al-Hourani", "Akram", ""], ["Moran", "Bill", ""], ["Kandeepan", "Sithamparanathan", ""]]}, {"id": "1605.07056", "submitter": "Mathias Vetter", "authors": "Mathias Vetter, Tobias Zwingmann", "title": "A note on central limit theorems for quadratic variation in case of\n  endogenous observation times", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a central limit theorem for quadratic variation\nwhen observations come as exit times from a regular grid. We discuss the\nspecial case of a semimartingale with deterministic characteristics and finite\nactivity jumps in detail and illustrate technical issues in more general\nsituations.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 15:16:27 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Vetter", "Mathias", ""], ["Zwingmann", "Tobias", ""]]}, {"id": "1605.07129", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker", "title": "Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed\n  entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix has attracted a lot of attention of the\nstatistical research community over the years, partially due to important\napplications such as Principal Component Analysis. However, frequently used\nempirical covariance estimator (and its modifications) is very sensitive to\noutliers in the data. As P. J. Huber wrote in 1964, \"...This raises a question\nwhich could have been asked already by Gauss, but which was, as far as I know,\nonly raised a few years ago (notably by Tukey): what happens if the true\ndistribution deviates slightly from the assumed normal one? As is now well\nknown, the sample mean then may have a catastrophically bad performance...\"\nMotivated by this question, we develop a new estimator of the (element-wise)\nmean of a random matrix, which includes covariance estimation problem as a\nspecial case. Assuming that the entries of a matrix possess only finite second\nmoment, this new estimator admits sub-Gaussian or sub-exponential concentration\naround the unknown mean in the operator norm. We will explain the key ideas\nbehind our construction, as well as applications to covariance estimation and\nmatrix completion problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:36:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:52:56 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 16:53:04 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 21:32:07 GMT"}, {"version": "v5", "created": "Sun, 17 Jun 2018 22:26:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Minsker", "Stanislav", ""]]}, {"id": "1605.07244", "submitter": "Zijian Guo", "authors": "Zijian Guo, Wanjie Wang, T. Tony Cai and Hongzhe Li", "title": "Optimal Estimation of Co-heritability in High-dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-heritability is an important concept that characterizes the genetic\nassociations within pairs of quantitative traits. There has been significant\nrecent interest in estimating the co-heritability based on data from the\ngenome-wide association studies (GWAS). This paper introduces two measures of\nco-heritability in the high-dimensional linear model framework, including the\ninner product of the two regression vectors and a normalized inner product by\ntheir lengths. Functional de-biased estimators (FDEs) are developed to estimate\nthese two co-heritability measures. In addition, estimators of quadratic\nfunctionals of the regression vectors are proposed. Both theoretical and\nnumerical properties of the estimators are investigated. In particular, minimax\nrates of convergence are established and the proposed estimators of the inner\nproduct, the quadratic functionals and the normalized inner product are shown\nto be rate-optimal. Simulation results show that the FDEs significantly\noutperform the naive plug-in estimates. The FDEs are also applied to analyze a\nyeast segregant data set with multiple traits to estimate heritability and\nco-heritability among the traits.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 00:35:55 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Guo", "Zijian", ""], ["Wang", "Wanjie", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1605.07249", "submitter": "Wenbin Lu", "authors": "Chengchun Shi, Wenbin Lu and Rui Song", "title": "A Massive Data Framework for M-Estimators with Cubic-Rate", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The divide and conquer method is a common strategy for handling massive data.\nIn this article, we study the divide and conquer method for cubic-rate\nestimators under the massive data framework. We develop a general theory for\nestablishing the asymptotic distribution of the aggregated M-estimators using a\nsimple average. Under certain condition on the growing rate of the number of\nsubgroups, the resulting aggregated estimators are shown to have faster\nconvergence rate and asymptotic normal distribution, which are more tractable\nin both computation and inference than the original M-estimators based on\npooled data. Our theory applies to a wide class of M-estimators with cube root\nconvergence rate, including the location estimator, maximum score estimator and\nvalue search estimator. Empirical performance via simulations also validate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 00:57:37 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 18:33:35 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Shi", "Chengchun", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "1605.07252", "submitter": "Marc Vuffray", "authors": "Marc Vuffray, Sidhant Misra, Andrey Y. Lokhov and Michael Chertkov", "title": "Interaction Screening: Efficient and Sample-Optimal Learning of Ising\n  Models", "comments": "To be published in Advances in Neural Information Processing Systems\n  30", "journal-ref": "Advances in Neural Information Processing Systems, 2595--2603,\n  2016", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the underlying graph of an unknown Ising\nmodel on p spins from a collection of i.i.d. samples generated from the model.\nWe suggest a new estimator that is computationally efficient and requires a\nnumber of samples that is near-optimal with respect to previously established\ninformation-theoretic lower-bound. Our statistical estimator has a physical\ninterpretation in terms of \"interaction screening\". The estimator is consistent\nand is efficiently implemented using convex optimization. We prove that with\nappropriate regularization, the estimator recovers the underlying graph using a\nnumber of samples that is logarithmic in the system size p and exponential in\nthe maximum coupling-intensity and maximum node-degree.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 01:36:48 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 03:00:29 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 13:32:25 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""], ["Lokhov", "Andrey Y.", ""], ["Chertkov", "Michael", ""]]}, {"id": "1605.07385", "submitter": "Yakov Nikitin", "authors": "A.Durio and Ya. Yu. Nikitin", "title": "Local efficiency of integrated goodness-of-fit tests under skew\n  alternatives", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of distribution-free {\\it integrated} goodness-of-fit tests\nwas studied by Henze and Nikitin (2000, 2002) under location alternatives. We\ncalculate local Bahadur efficiencies of these tests under more realistic\ngeneralized skew alternatives. They turn out to be unexpectedly high.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 11:33:08 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Durio", "A.", ""], ["Nikitin", "Ya. Yu.", ""]]}, {"id": "1605.07412", "submitter": "Jeremie Bigot", "authors": "J\\'er\\'emie Bigot, Charles Deledalle and Delphine F\\'eral", "title": "Generalized SURE for optimal shrinkage of singular values in low-rank\n  matrix denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a low-rank signal matrix from noisy\nmeasurements under the assumption that the distribution of the data matrix\nbelongs to an exponential family. In this setting, we derive generalized\nStein's unbiased risk estimation (SURE) formulas that hold for any spectral\nestimators which shrink or threshold the singular values of the data matrix.\nThis leads to new data-driven spectral estimators, whose optimality is\ndiscussed using tools from random matrix theory and through numerical\nexperiments. Under the spiked population model and in the asymptotic setting\nwhere the dimensions of the data matrix are let going to infinity, some\ntheoretical properties of our approach are compared to recent results on\nasymptotically optimal shrinking rules for Gaussian noise. It also leads to new\nprocedures for singular values shrinkage in finite-dimensional matrix denoising\nfor Gamma-distributed and Poisson-distributed measurements.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:33:45 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 20:02:59 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 11:08:57 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Deledalle", "Charles", ""], ["F\u00e9ral", "Delphine", ""]]}, {"id": "1605.07416", "submitter": "Sebastien Gerchinovitz", "authors": "S\\'ebastien Gerchinovitz (IMT, AOC), Tor Lattimore", "title": "Refined Lower Bounds for Adversarial Bandits", "comments": null, "journal-ref": "D. D. Lee; M. Sugiyama; U. V. Luxburg; I. Guyon; R. Garnett. NIPS\n  2016, Dec 2016, Barcelona, Spain. Curran Associates, Inc., pp.1198--1206,\n  Advances in Neural Information Processing Systems 29", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new lower bounds on the regret that must be suffered by\nadversarial bandit algorithms. The new results show that recent upper bounds\nthat either (a) hold with high-probability or (b) depend on the total lossof\nthe best arm or (c) depend on the quadratic variation of the losses, are close\nto tight. Besides this we prove two impossibility results. First, the existence\nof a single arm that is optimal in every round cannot improve the regret in the\nworst case. Second, the regret cannot scale with the effective range of the\nlosses. In contrast, both results are possible in the full-information setting.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:36:47 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 13:48:10 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gerchinovitz", "S\u00e9bastien", "", "IMT, AOC"], ["Lattimore", "Tor", ""]]}, {"id": "1605.07520", "submitter": "Ana Cristina Rosa", "authors": "A. C. Rosa and M. E. Nogueira", "title": "Nonparametric estimation of a regression function using the gamma kernel\n  method in ergodic processes", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the nonparametric estimation of density and\nregression functions with non-negative support using a gamma kernel procedure\nintroduced by Chen (2000). Strong uniform consistency and asymptotic normality\nof the corresponding estimators are established under a general ergodic\nassumption on the data generation process. Our results generalize those of Shi\nand Song (2016), obtained in the classic i.i.d. framework, and the works of\nBouezmarni and Rombouts (2008, 2010b) and Gospodinov and Hirukawa (2007) for\nmixing time series.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 16:03:46 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 11:17:46 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Rosa", "A. C.", ""], ["Nogueira", "M. E.", ""]]}, {"id": "1605.07679", "submitter": "Jiangfan Zhang", "authors": "Jiangfan Zhang, Rick S. Blum, Lance Kaplan, and Xuanxuan Lu", "title": "A Fundamental Limitation on Maximum Parameter Dimension for Accurate\n  Estimation with Quantized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is revealed that there is a link between the quantization approach\nemployed and the dimension of the vector parameter which can be accurately\nestimated by a quantized estimation system. A critical quantity called\ninestimable dimension for quantized data (IDQD) is introduced, which doesn't\ndepend on the quantization regions and the statistical models of the\nobservations but instead depends only on the number of sensors and on the\nprecision of the vector quantizers employed by the system. It is shown that the\nIDQD describes a quantization induced fundamental limitation on the estimation\ncapabilities of the system. To be specific, if the dimension of the desired\nvector parameter is larger than the IDQD of the quantized estimation system,\nthen the Fisher information matrix for estimating the desired vector parameter\nis singular, and moreover, there exist infinitely many nonidentifiable vector\nparameter points in the vector parameter space. Furthermore, it is shown that\nunder some common assumptions on the statistical models of the observations and\nthe quantization system, a smaller IDQD can be obtained, which can specify an\neven more limiting quantization induced fundamental limitation on the\nestimation capabilities of the system.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 22:48:58 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Zhang", "Jiangfan", ""], ["Blum", "Rick S.", ""], ["Kaplan", "Lance", ""], ["Lu", "Xuanxuan", ""]]}, {"id": "1605.07680", "submitter": "Hugo Cruz-Sanchez", "authors": "Hugo Cruz-Sanchez", "title": "Generalized Subjective Lexicographic Expected Utility Representation", "comments": "Originally available on November 18, 2014, in my website\n  (https://sites.google.com/site/hugocruzsanchez/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide foundations for decisions in face of unlikely events by extending\nthe standard framework of Savage to include preferences indexed by a family of\nevents. We derive a subjective lexicographic expected utility representation\nwhich allows for infinitely many lexicographically ordered levels of events and\nfor event-dependent attitudes toward risk. Our model thus provides foundations\nfor models in finance that rely on different attitudes toward risk (e.g.\nSkiadas [9]) and for off-equilibrium reasonings in infinite dynamic games, thus\nextending and generalizing the analysis in Blume, Brandenburger and Dekel [3].\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 22:52:00 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Cruz-Sanchez", "Hugo", ""]]}, {"id": "1605.07696", "submitter": "Yu Lu", "authors": "Chao Gao, Yu Lu, Dengyong Zhou", "title": "Exact Exponent in Optimal Rates for Crowdsourcing", "comments": "To appear in the Proceedings of the 33rd International Conference on\n  Machine Learning, New York, NY, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, crowdsourcing has become the primary\nmeans for label collection. In this paper, we study the optimal error rate for\naggregating labels provided by a set of non-expert workers. Under the classic\nDawid-Skene model, we establish matching upper and lower bounds with an exact\nexponent $mI(\\pi)$ in which $m$ is the number of workers and $I(\\pi)$ the\naverage Chernoff information that characterizes the workers' collective\nability. Such an exact characterization of the error exponent allows us to\nstate a precise sample size requirement\n$m>\\frac{1}{I(\\pi)}\\log\\frac{1}{\\epsilon}$ in order to achieve an $\\epsilon$\nmisclassification error. In addition, our results imply the optimality of\nvarious EM algorithms for crowdsourcing initialized by consistent estimators.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 01:16:06 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:43:49 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Gao", "Chao", ""], ["Lu", "Yu", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1605.07784", "submitter": "Xinyang Yi", "authors": "Xinyang Yi, Dohyung Park, Yudong Chen, Constantine Caramanis", "title": "Fast Algorithms for Robust PCA via Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Robust PCA in the fully and partially observed\nsettings. Without corruptions, this is the well-known matrix completion\nproblem. From a statistical standpoint this problem has been recently\nwell-studied, and conditions on when recovery is possible (how many\nobservations do we need, how many corruptions can we tolerate) via\npolynomial-time algorithms is by now understood. This paper presents and\nanalyzes a non-convex optimization approach that greatly reduces the\ncomputational complexity of the above problems, compared to the best available\nalgorithms. In particular, in the fully observed case, with $r$ denoting rank\nand $d$ dimension, we reduce the complexity from\n$\\mathcal{O}(r^2d^2\\log(1/\\varepsilon))$ to\n$\\mathcal{O}(rd^2\\log(1/\\varepsilon))$ -- a big savings when the rank is big.\nFor the partially observed case, we show the complexity of our algorithm is no\nmore than $\\mathcal{O}(r^4d \\log d \\log(1/\\varepsilon))$. Not only is this the\nbest-known run-time for a provable algorithm under partial observation, but in\nthe setting where $r$ is small compared to $d$, it also allows for\nnear-linear-in-$d$ run-time that can be exploited in the fully-observed case as\nwell, by simply running our algorithm on a subset of the observations.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 09:10:07 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 17:28:25 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Yi", "Xinyang", ""], ["Park", "Dohyung", ""], ["Chen", "Yudong", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1605.07811", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Probabilistic Numerical Methods for Partial Differential Equations and\n  Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a probabilistic numerical method for solution of partial\ndifferential equations (PDEs) and studies application of that method to\nPDE-constrained inverse problems. This approach enables the solution of\nchallenging inverse problems whilst accounting, in a statistically principled\nway, for the impact of discretisation error due to numerical solution of the\nPDE. In particular, the approach confers robustness to failure of the numerical\nPDE solver, with statistical inferences driven to be more conservative in the\npresence of substantial discretisation error. Going further, the problem of\nchoosing a PDE solver is cast as a problem in the Bayesian design of\nexperiments, where the aim is to minimise the impact of solver error on\nstatistical inferences; here the challenge of non-linear PDEs is also\nconsidered. The method is applied to parameter inference problems in which\ndiscretisation error in non-negligible and must be accounted for in order to\nreach conclusions that are statistically valid.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 10:22:19 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 08:55:10 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 15:22:34 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1605.07830", "submitter": "Sergei Kucherenko", "authors": "S. Kucherenko, S. Song", "title": "Derivative-based global sensitivity measures and their link with Sobol\n  sensitivity indices", "comments": "Monte Carlo and Quasi-Monte Carlo Methods 2014, conference, Monte\n  Carlo and Quasi-Monte Carlo Methods, R. Cools and D. Nuyens (eds.), Springer\n  Proceedings in Mathematics & Statistics 163, Springer International\n  Publishing Switzerland 2016", "journal-ref": null, "doi": "10.1007/978-3-319-33507-0_23", "report-no": null, "categories": "math.ST math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variance-based method of Sobol sensitivity indices is very popular among\npractitioners due to its efficiency and easiness of interpretation. However,\nfor high-dimensional models the direct application of this method can be very\ntime consuming and prohibitively expensive to use. One of the alternative\nglobal sensitivity analysis methods known as the method of derivative based\nglobal sensitivity measures (DGSM) has recently become popular among\npractitioners. It has a link with the Morris screening method and Sobol\nsensitivity indices. DGSM are very easy to implement and evaluate numerically.\nThe computational time required for numerical evaluation of DGSM is generally\nmuch lower than that for estimation of Sobol sensitivity indices. We present a\nsurvey of recent advances in DGSM and new results concerning new lower and\nupper bounds on the values of Sobol total sensitivity indices. Using these\nbounds it is possible in most cases to get a good practical estimation of the\nvalues of Sobol total sensitivity indices. Several examples are used to\nillustrate an application of DGSM.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 11:17:22 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kucherenko", "S.", ""], ["Song", "S.", ""]]}, {"id": "1605.07854", "submitter": "Lukas Martig", "authors": "Lukas Martig, J\\\"urg H\\\"usler", "title": "Asymptotic normality of the likelihood moment estimators for a\n  stationary linear process with heavy-tailed innovations", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of estimators for the parameters of the Generalized Pareto\ndistribution, the approximating distribution for excesses over a high\nthreshold, have been proposed, always assuming the underlying data to be\nindependent. We recently proved that the likelihood moment estimators are\nconsistent estimators for the parameters of the Generalized Pareto distribution\nfor the case where the underlying data arises from a (stationary) linear\nprocess with heavy-tailed innovations. In this paper we derive the bivariate\nasymptotic normality under some additional assumptions and give an explicit\nexample on how to check these conditions by using asymptotic expansions.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:31:31 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Martig", "Lukas", ""], ["H\u00fcsler", "J\u00fcrg", ""]]}, {"id": "1605.07913", "submitter": "Marianna Pensky", "authors": "Pawan Gupta and Marianna Pensky", "title": "Solution of linear ill-posed problems using random dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. In the context of\nregression problems, there has been enormous amount of effort to recover an\nunknown function using such dictionaries. One of the most popular methods,\nlasso and its versions, is based on minimizing empirical likelihood and\nunfortunately, requires stringent assumptions on the dictionary, the, so\ncalled, compatibility conditions. Though compatibility conditions are hard to\nsatisfy, it is well known that this can be accomplished by using random\ndictionaries. In the present paper, we show how one can apply random\ndictionaries to solution of ill-posed linear inverse problems. We put a\ntheoretical foundation under the suggested methodology and study its\nperformance via simulations.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:51:50 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 19:42:16 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:35:05 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Gupta", "Pawan", ""], ["Pensky", "Marianna", ""]]}, {"id": "1605.08010", "submitter": "Igor Cialenco", "authors": "Tomasz R. Bielecki, Tao Chen and Igor Cialenco", "title": "Recursive Construction of Confidence Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming that one-step transition kernel of a discrete time, time-homogenous\nMarkov chain model is parameterized by a parameter $\\theta\\in \\boldsymbol\n\\Theta$, we derive a recursive (in time) construction of confidence regions for\nthe unknown parameter of interest, say $\\theta^*\\in \\boldsymbol \\Theta$. It is\nsupposed that the observed data used in construction of the confidence regions\nis generated by a Markov chain whose transition kernel corresponds to\n$\\theta^*$ . The key step in our construction is derivation of a recursive\nscheme for an appropriate point estimator of $\\theta^*$. To achieve this, we\nstart by what we call the base recursive point estimator, using which we design\na quasi-asymptotically linear recursive point estimator (a concept introduced\nin this paper). For the latter estimator we prove its weak consistency and\nasymptotic normality. The recursive construction of confidence regions is\nneeded not only for the purpose of speeding up the computation of the\nsuccessive confidence regions, but, primarily, for the ability to apply the\ndynamic programming principle in the context of robust adaptive stochastic\ncontrol methodology.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 19:16:20 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 16:30:21 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Bielecki", "Tomasz R.", ""], ["Chen", "Tao", ""], ["Cialenco", "Igor", ""]]}, {"id": "1605.08152", "submitter": "Abdelfattah  Mustafa AM", "authors": "Beih S. El-Desouky, Abdelfattah Mustafa and Shamsan AL-Garash", "title": "The Exponential Flexible Weibull Extension Distribution", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper is devoted to study a new three- parameters model called the\nExponential Flexible Weibull extension (EFWE) distribution which exhibits\nbathtub-shaped hazard rate. Some of it's statistical properties are obtained\nincluding ordinary and incomplete moments, quantile and generating functions,\nreliability and order statistics. The method of maximum likelihood is used for\nestimating the model parameters and the observed Fisher's information matrix is\nderived. We illustrate the usefulness of the proposed model by applications to\nreal data.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 05:52:05 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["El-Desouky", "Beih S.", ""], ["Mustafa", "Abdelfattah", ""], ["AL-Garash", "Shamsan", ""]]}, {"id": "1605.08188", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Learning Multivariate Log-concave Distributions", "comments": "To appear in COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating multivariate log-concave probability\ndensity functions. We prove the first sample complexity upper bound for\nlearning log-concave densities on $\\mathbb{R}^d$, for all $d \\geq 1$. Prior to\nour work, no upper bound on the sample complexity of this learning problem was\nknown for the case of $d>3$. In more detail, we give an estimator that, for any\n$d \\ge 1$ and $\\epsilon>0$, draws $\\tilde{O}_d \\left( (1/\\epsilon)^{(d+5)/2}\n\\right)$ samples from an unknown target log-concave density on $\\mathbb{R}^d$,\nand outputs a hypothesis that (with high probability) is $\\epsilon$-close to\nthe target, in total variation distance. Our upper bound on the sample\ncomplexity comes close to the known lower bound of $\\Omega_d \\left(\n(1/\\epsilon)^{(d+1)/2} \\right)$ for this problem.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 08:31:18 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:06:28 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1605.08259", "submitter": "Matteo Smerlak", "authors": "Matteo Smerlak", "title": "Minimum relative entropy distributions with a large mean are Gaussian", "comments": "5 pages, 3 figures", "journal-ref": "Phys. Rev. E 94, 062107 (2016)", "doi": "10.1103/PhysRevE.94.062107", "report-no": null, "categories": "cond-mat.stat-mech math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following frustrated optimization problem: given a prior\nprobability distribution $q$, find the distribution $p$ minimizing the relative\nentropy with respect to $q$ such that $\\textrm{mean}(p)$ is fixed and large. We\nshow that solutions to this problem are asymptotically Gaussian. As an\napplication we derive an $H$-type theorem for evolutionary dynamics: the\nentropy of the (standardized) distribution of fitness of a population evolving\nunder natural selection is eventually increasing.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 13:11:40 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Smerlak", "Matteo", ""]]}, {"id": "1605.08386", "submitter": "Caprice Stanley", "authors": "Caprice Stanley and Tobias Windisch", "title": "Heat-bath random walks with Markov bases", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs on lattice points are studied whose edges come from a finite set of\nallowed moves of arbitrary length. We show that the diameter of these graphs on\nfibers of a fixed integer matrix can be bounded from above by a constant. We\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\nalso state explicit conditions on the set of moves so that the heat-bath random\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\ndimension.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 17:59:46 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Stanley", "Caprice", ""], ["Windisch", "Tobias", ""]]}, {"id": "1605.08400", "submitter": "Veeranjaneyulu Sadhanala", "authors": "Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Ryan Tibshirani", "title": "Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of\n  Linear Smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a function defined over $n$ locations\non a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$). When\nthe function is constrained to have discrete total variation bounded by $C_n$,\nwe derive the minimax optimal (squared) $\\ell_2$ estimation error rate,\nparametrized by $n$ and $C_n$. Total variation denoising, also known as the\nfused lasso, is seen to be rate optimal. Several simpler estimators exist, such\nas Laplacian smoothing and Laplacian eigenmaps. A natural question is: can\nthese simpler estimators perform just as well? We prove that these estimators,\nand more broadly all estimators given by linear transformations of the input\ndata, are suboptimal over the class of functions with bounded variation. This\nextends fundamental findings of Donoho and Johnstone [1998] on 1-dimensional\ntotal variation spaces to higher dimensions. The implication is that the\ncomputationally simpler methods cannot be used for such sophisticated denoising\ntasks, without sacrificing statistical accuracy. We also derive minimax rates\nfor discrete Sobolev spaces over $d$-dimensional grids, which are, in some\nsense, smaller than the total variation function spaces. Indeed, these are\nsmall enough spaces that linear estimators can be optimal---and a few\nwell-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we\nshow. Lastly, we investigate the problem of adaptivity of the total variation\ndenoiser to these smaller Sobolev function spaces.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 18:38:38 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Sadhanala", "Veeranjaneyulu", ""], ["Wang", "Yu-Xiang", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1605.08466", "submitter": "Gourab Mukherjee", "authors": "Lawrence D. Brown, Gourab Mukherjee, Asaf Weinstein", "title": "Empirical Bayes Estimates for a 2-Way Cross-Classified Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical Bayes procedure for estimating the cell means in an\nunbalanced, two-way additive model with fixed effects. We employ a hierarchical\nmodel, which reflects exchangeability of the effects within treatment and\nwithin block but not necessarily between them, as suggested before by Lindley\nand Smith (1972). The hyperparameters of this hierarchical model, instead of\nconsidered fixed, are to be substituted with data-dependent values in such a\nway that the point risk of the empirical Bayes estimator is small. Our method\nchooses the hyperparameters by minimizing an unbiased risk estimate and is\nshown to be asymptotically optimal for the estimation problem defined above.\nThe usual empirical Best Linear Unbiased Predictor (BLUP) is shown to be\nsubstantially different from the proposed method in the unbalanced case and\ntherefore performs sub-optimally. Our estimator is implemented through a\ncomputationally tractable algorithm that is scalable to work under large\ndesigns. The case of missing cell observations is treated as well. We\ndemonstrate the advantages of our method over the BLUP estimator through\nsimulations and in a real data example, where we estimate average nitrate\nlevels in water sources based on their locations and the time of the day.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 22:39:25 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Brown", "Lawrence D.", ""], ["Mukherjee", "Gourab", ""], ["Weinstein", "Asaf", ""]]}, {"id": "1605.08467", "submitter": "Natalia Bochkina", "authors": "Natalia Bochkina and Judith Rousseau", "title": "Adaptive density estimation based on a mixture of Gammas", "comments": null, "journal-ref": "Electron. J. Statist. Volume 11, Number 1 (2017), 916-962", "doi": "10.1214/17-EJS1247", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian density estimation on the positive\nsemiline for possibly unbounded densities. We propose a hierarchical Bayesian\nestimator based on the gamma mixture prior which can be viewed as a location\nmixture. We study convergence rates of Bayesian density estimators based on\nsuch mixtures. We construct approximations of the local H\\\"older densities, and\nof their extension to unbounded densities, to be continuous mixtures of gamma\ndistributions, leading to approximations of such densities by finite mixtures.\nThese results are then used to derive posterior concentration rates, with\npriors based on these mixture models. The rates are minimax (up to a log n\nterm) and since the priors are independent of the smoothness the rates are\nadaptive to the smoothness.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 22:43:05 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 09:53:26 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Bochkina", "Natalia", ""], ["Rousseau", "Judith", ""]]}, {"id": "1605.08534", "submitter": "Sylvain Le Corff", "authors": "Thi Ngoc Minh Nguyen (LTCI), Sylvain Le Corff (LM-Orsay), Eric\n  Moulines (CMAP)", "title": "On the two-filter approximations of marginal smoothing distributions in\n  general state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prevalent problem in general state space models is the approximation of the\nsmoothing distribution of a state conditional on the observations from the\npast, the present, and the future. The aim of this paper is to provide a\nrigorous analysis of such approximations of smoothed distributions provided by\nthe two-filter algorithms. We extend the results available for the\napproximation of smoothing distributions to these two-filter approaches which\ncombine a forward filter approximating the filtering distributions with a\nbackward information filter approximating a quantity proportional to the\nposterior distribution of the state given future observations.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 08:22:12 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Nguyen", "Thi Ngoc Minh", "", "LTCI"], ["Corff", "Sylvain Le", "", "LM-Orsay"], ["Moulines", "Eric", "", "CMAP"]]}, {"id": "1605.08651", "submitter": "Pierre C. Bellec", "authors": "Pierre C. Bellec, Guillaume Lecu\\'e, Alexandre B. Tsybakov", "title": "Slope meets Lasso: improved oracle bounds and optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that two polynomial time methods, a Lasso estimator with adaptively\nchosen tuning parameter and a Slope estimator, adaptively achieve the exact\nminimax prediction and $\\ell_2$ estimation rate $(s/n)\\log (p/s)$ in\nhigh-dimensional linear regression on the class of $s$-sparse target vectors in\n$\\mathbb R^p$. This is done under the Restricted Eigenvalue (RE) condition for\nthe Lasso and under a slightly more constraining assumption on the design for\nthe Slope. The main results have the form of sharp oracle inequalities\naccounting for the model misspecification error. The minimax optimal bounds are\nalso obtained for the $\\ell_q$ estimation errors with $1\\le q\\le 2$ when the\nmodel is well-specified. The results are non-asymptotic, and hold both in\nprobability and in expectation. The assumptions that we impose on the design\nare satisfied with high probability for a large class of random matrices with\nindependent and possibly anisotropically distributed rows. We give a\ncomparative analysis of conditions, under which oracle bounds for the Lasso and\nSlope estimators can be obtained. In particular, we show that several known\nconditions, such as the RE condition and the sparse eigenvalue condition are\nequivalent if the $\\ell_2$-norms of regressors are uniformly bounded.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 14:10:44 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 15:53:46 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 22:50:13 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Bellec", "Pierre C.", ""], ["Lecu\u00e9", "Guillaume", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1605.08667", "submitter": "Daniel Kucharczyk", "authors": "Kucharczyk Daniel. Wy{\\l}oma\\'nska Agnieszka, Zimroz Rados{\\l}aw", "title": "Structural break detection method based on the Adaptive Regression\n  Splines technique", "comments": "13 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1203.1144 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many real data, long term observation consists of different processes\nthat coexist or occur one after the other. Those processes very often exhibit\ndifferent statistical properties and thus before the further analysis the\nobserved data should be segmented. This problem one can find in different\napplications and therefore new segmentation techniques have been appeared in\nthe literature during last years. In this paper we propose a new method of time\nseries segmentation, i.e. extraction from the analysed vector of observations\nhomogeneous parts with similar behaviour. This method is based on the absolute\ndeviation about the median of the signal and is an extension of the previously\nproposed techniques also based on the simple statistics. In this paper we\nintroduce the method of structural break point detection which is based on the\nAdaptive Regression Splines technique, one of the form of regression analysis.\nMoreover we propose also the statistical test which allows testing hypothesis\nof behaviour related to different regimes. First, the methodology we apply to\nthe simulated signals with different distributions in order to show the\neffectiveness of the new technique. Next, in the application part we analyse\nthe real data set that represents the vibration signal from a heavy duty\ncrusher used in a mineral processing plant.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 14:33:25 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Agnieszka", "Kucharczyk Daniel. Wy\u0142oma\u0144ska", ""], ["Rados\u0142aw", "Zimroz", ""]]}, {"id": "1605.08737", "submitter": "Guannan Wang", "authors": "Li Wang, Guannan Wang, Min-Jun Lai and Lei Gao", "title": "Efficient Estimation of Partially Linear Models for Spatial Data over\n  Complex Domain", "comments": null, "journal-ref": "Statistica Sinica, 2020", "doi": "10.5705/ss.202017.024", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation of partially linear models for spatial\ndata distributed over complex domains. We use bivariate splines over\ntriangulations to represent the nonparametric component on an irregular\ntwo-dimensional domain. The proposed method is formulated as a constrained\nminimization problem which does not require constructing finite elements or\nlocally supported basis functions. Thus, it allows an easier implementation of\npiecewise polynomial representations of various degrees and various smoothness\nover an arbitrary triangulation. Moreover, the constrained minimization problem\nis converted into an unconstrained minimization via a QR decomposition of the\nsmoothness constraints, which allows for the development of a fast and\nefficient penalized least squares algorithm to fit the model. The estimators of\nthe parameters are proved to be asymptotically normal under some regularity\nconditions. The estimator of the bivariate function is consistent, and its rate\nof convergence is also established. The proposed method enables us to construct\nconfidence intervals and permits inference for the parameters. The performance\nof the estimators is evaluated by two simulation examples and by a real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:15:28 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 20:07:20 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 02:59:40 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Li", ""], ["Wang", "Guannan", ""], ["Lai", "Min-Jun", ""], ["Gao", "Lei", ""]]}, {"id": "1605.08839", "submitter": "Daniel Hsu", "authors": "Lee H. Dicker, Dean P. Foster, Daniel Hsu", "title": "Kernel ridge vs. principal component regression: minimax bounds and\n  adaptability of regularization operators", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is an essential element of virtually all kernel methods for\nnonparametric regression problems. A critical factor in the effectiveness of a\ngiven kernel method is the type of regularization that is employed. This\narticle compares and contrasts members from a general class of regularization\ntechniques, which notably includes ridge regression and principal component\nregression. We derive an explicit finite-sample risk bound for\nregularization-based estimators that simultaneously accounts for (i) the\nstructure of the ambient function space, (ii) the regularity of the true\nregression function, and (iii) the adaptability (or qualification) of the\nregularization. A simple consequence of this upper bound is that the risk of\nthe regularization-based estimators matches the minimax rate in a variety of\nsettings. The general bound also illustrates how some regularization techniques\nare more adaptable than others to favorable regularity properties that the true\nregression function may possess. This, in particular, demonstrates a striking\ndifference between kernel ridge regression and kernel principal component\nregression. Our theoretical results are supported by numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 03:24:00 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Dicker", "Lee H.", ""], ["Foster", "Dean P.", ""], ["Hsu", "Daniel", ""]]}, {"id": "1605.08875", "submitter": "Elias David Nino Ruiz", "authors": "Elias D. Nino, Adrian Sandu, Xinwei Deng", "title": "An Ensemble Kalman Filter Implementation Based on Modified Cholesky\n  Decomposition for Inverse Covariance Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "CSTR-2/2016", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an efficient implementation of the ensemble Kalman filter\nbased on a modified Cholesky decomposition for inverse covariance matrix\nestimation. This implementation is named EnKF-MC. Background errors\ncorresponding to distant model components with respect to some radius of\ninfluence are assumed to be conditionally independent. This allows to obtain\nsparse estimators of the inverse background error covariance matrix. The\ncomputational effort of the proposed method is discussed and different\nformulations based on various matrix identities are provided. Furthermore, an\nasymptotic proof of convergence with regard to the ensemble size is presented.\nIn order to assess the performance and the accuracy of the proposed method,\nexperiments are performed making use of the Atmospheric General Circulation\nModel SPEEDY. The results are compared against those obtained using the local\nensemble transform Kalman filter (LETKF). Tests are performed for dense\nobservations ($100\\%$ and $50\\%$ of the model components are observed) as well\nas for sparse observations (only $12\\%$, $6\\%$, and $4\\%$ of model components\nare observed). The results reveal that the use of modified Cholesky for inverse\ncovariance matrix estimation can reduce the impact of spurious correlations\nduring the assimilation cycle, i.e., the results of the proposed method are of\nbetter quality than those obtained via the LETKF in terms of root mean square\nerror.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 10:48:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Nino", "Elias D.", ""], ["Sandu", "Adrian", ""], ["Deng", "Xinwei", ""]]}, {"id": "1605.08880", "submitter": "Alexei Onatski", "authors": "Alexei Onatski and Chen Wang", "title": "Alternative asymptotics for cointegration tests in large VARs", "comments": "55 pages, 12 figures, Supplementary Appendix file is available from\n  Alexei Onatski's web site at the University of Cambridge", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Johansen's (1988, 1991) likelihood ratio test for cointegration rank of a\nGaussian VAR depends only on the squared sample canonical correlations between\ncurrent changes and past levels of a simple transformation of the data. We\nstudy the asymptotic behavior of the empirical distribution of those squared\ncanonical correlations when the number of observations and the dimensionality\nof the VAR diverge to infinity simultaneously and proportionally. We find that\nthe distribution almost surely weakly converges to the so-called Wachter\ndistribution. This finding provides a theoretical explanation for the observed\ntendency of Johansen's test to find \"spurious cointegration\". It also sheds\nlight on the workings and limitations of the Bartlett correction approach to\nthe over-rejection problem. We propose a simple graphical device, similar to\nthe scree plot, for a preliminary assessment of cointegration in\nhigh-dimensional VARs.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 12:07:09 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Onatski", "Alexei", ""], ["Wang", "Chen", ""]]}, {"id": "1605.08912", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Vinay Venkataraman, Karthikeyan Natesan Ramamurthy,\n  Pavan Turaga", "title": "A Riemannian Framework for Statistical Analysis of Topological\n  Persistence Diagrams", "comments": "Accepted at DiffCVML 2016 (CVPR 2016 Workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis is becoming a popular way to study high dimensional\nfeature spaces without any contextual clues or assumptions. This paper concerns\nitself with one popular topological feature, which is the number of\n$d-$dimensional holes in the dataset, also known as the Betti$-d$ number. The\npersistence of the Betti numbers over various scales is encoded into a\npersistence diagram (PD), which indicates the birth and death times of these\nholes as scale varies. A common way to compare PDs is by a point-to-point\nmatching, which is given by the $n$-Wasserstein metric. However, a big drawback\nof this approach is the need to solve correspondence between points before\ncomputing the distance; for $n$ points, the complexity grows according to\n$\\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new framework\nbuilt on Riemannian geometry, that models PDs as 2D probability density\nfunctions that are represented in the square-root framework on a Hilbert\nSphere. The resulting space is much more intuitive with closed form expressions\nfor common operations. The distance metric is 1) correspondence-free and also\n2) independent of the number of points in the dataset. The complexity of\ncomputing distance between PDs now grows according to $\\mathcal{O}(K^2)$, for a\n$K \\times K$ discretization of $[0,1]^2$. This also enables the use of existing\nmachinery in differential geometry towards statistical analysis of PDs such as\ncomputing the mean, geodesics, classification etc. We report competitive\nresults with the Wasserstein metric, at a much lower computational load,\nindicating the favorable properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 16:55:40 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Anirudh", "Rushil", ""], ["Venkataraman", "Vinay", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Turaga", "Pavan", ""]]}, {"id": "1605.08988", "submitter": "Emilie Kaufmann", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (SEQUEL, CRIStAL, CNRS),\n  Tor Lattimore", "title": "On Explore-Then-Commit Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimising regret in two-armed bandit problems with\nGaussian rewards. Our objective is to use this simple setting to illustrate\nthat strategies based on an exploration phase (up to a stopping time) followed\nby exploitation are necessarily suboptimal. The results hold regardless of\nwhether or not the difference in means between the two arms is known. Besides\nthe main message, we also refine existing deviation inequalities, which allow\nus to design fully sequential strategies with finite-time regret guarantees\nthat are (a) asymptotically optimal as the horizon grows and (b) order-optimal\nin the minimax sense. Furthermore we provide empirical evidence that the theory\nalso holds in practice and discuss extensions to non-gaussian and\nmultiple-armed case.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 10:35:33 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 12:40:20 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "SEQUEL, CRIStAL, CNRS"], ["Lattimore", "Tor", ""]]}, {"id": "1605.09124", "submitter": "Yanjun Han", "authors": "Yanjun Han, Jiantao Jiao and Tsachy Weissman", "title": "Minimax Rate-Optimal Estimation of Divergences between Discrete\n  Distributions", "comments": "This (v5) is a significantly revised version of (v2), and fixed some\n  typos in (v4)", "journal-ref": "Published in IEEE Journal on Selected Areas in Information Theory,\n  vol. 1, no. 3, pp. 814-823, Nov. 2020", "doi": "10.1109/JSAIT.2020.3041036", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimax estimation of $\\alpha$-divergences between discrete\ndistributions for integer $\\alpha\\ge 1$, which include the Kullback--Leibler\ndivergence and the $\\chi^2$-divergences as special examples. Dropping the usual\ntheoretical tricks to acquire independence, we construct the first minimax\nrate-optimal estimator which does not require any Poissonization, sample\nsplitting, or explicit construction of approximating polynomials. The estimator\nuses a hybrid approach which solves a problem-independent linear program based\non moment matching in the non-smooth regime, and applies a problem-dependent\nbias-corrected plug-in estimator in the smooth regime, with a soft decision\nboundary between these regimes.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 07:24:03 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 04:17:05 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 11:02:28 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 16:49:30 GMT"}, {"version": "v5", "created": "Wed, 3 Mar 2021 06:36:52 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Han", "Yanjun", ""], ["Jiao", "Jiantao", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1605.09288", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Mijke Rhemtulla, Denny Borsboom", "title": "Generalized Network Psychometrics: Combining Network and Latent Variable\n  Models", "comments": "Published in Psychometrika", "journal-ref": null, "doi": "10.1007/s11336-017-9557-x", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the network model as a formal psychometric model,\nconceptualizing the covariance between psychometric indicators as resulting\nfrom pairwise interactions between observable variables in a network structure.\nThis contrasts with standard psychometric models, in which the covariance\nbetween test items arises from the influence of one or more common latent\nvariables. Here, we present two generalizations of the network model that\nencompass latent variable structures, establishing network modeling as parts of\nthe more general framework of Structural Equation Modeling (SEM). In the first\ngeneralization, we model the covariance structure of latent variables as a\nnetwork. We term this framework Latent Network Modeling (LNM) and show that,\nwith LNM, a unique structure of conditional independence relationships between\nlatent variables can be obtained in an explorative manner. In the second\ngeneralization, the residual variance-covariance structure of indicators is\nmodeled as a network. We term this generalization Residual Network Modeling\n(RNM) and show that, within this framework, identifiable models can be obtained\nin which local independence is structurally violated. These generalizations\nallow for a general modeling framework that can be used to fit, and compare,\nSEM models, network models, and the RNM and LNM generalizations. This\nmethodology has been implemented in the free-to-use software package lvnet,\nwhich contains confirmatory model testing as well as two exploratory search\nalgorithms: stepwise search algorithms for low-dimensional datasets and\npenalized maximum likelihood estimation for larger datasets. We show in\nsimulation studies that these search algorithms performs adequately in\nidentifying the structure of the relevant residual or latent networks. We\nfurther demonstrate the utility of these generalizations in an empirical\nexample on a personality inventory dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 15:50:43 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 20:46:17 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 23:25:00 GMT"}, {"version": "v4", "created": "Mon, 11 Sep 2017 20:47:58 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Epskamp", "Sacha", ""], ["Rhemtulla", "Mijke", ""], ["Borsboom", "Denny", ""]]}, {"id": "1605.09456", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel", "title": "Concentration of the empirical level sets of Tukey's halfspace depth", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tukey depth, aka halfspace depth, has attracted much interest in data\nanalysis, because it is a natural way of measuring the notion of depth relative\nto a cloud of points or, more generally, to a probability measure. Given an\ni.i.d. sample, we investigate the concentration of upper level sets of the\nTukey depth relative to that sample around their population version. We also\nstudy the concentration of the upper level sets of a discretized version of\nTukey depth.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 00:45:01 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 00:30:25 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 04:19:20 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1605.09457", "submitter": "Luis Angel Rodr\\'iguez", "authors": "Luis-Angel Rodr\\'iguez", "title": "Asymptotic properties of the maximum likelihood estimator for nonlinear\n  AR processes with markov-switching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we propose a new approach for the proof of the consistency and\nnormality of the maximum likelihood estimator for nonlinear AR processes with\nmarkov-switching under the assumptions of uniform exponential forgetting of the\nprediction filter and $\\alpha$-mixing property. We show that in the linear and\nGaussian case our assumptions are fully satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 00:56:45 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Rodr\u00edguez", "Luis-Angel", ""]]}, {"id": "1605.09646", "submitter": "Quentin Berthet", "authors": "Tengyao Wang, Quentin Berthet, Yaniv Plan", "title": "Average-case Hardness of RIP Certification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted isometry property (RIP) for design matrices gives guarantees\nfor optimal recovery in sparse linear models. It is of high interest in\ncompressed sensing and statistical learning. This property is particularly\nimportant for computationally efficient recovery methods. As a consequence,\neven though it is in general NP-hard to check that RIP holds, there have been\nsubstantial efforts to find tractable proxies for it. These would allow the\nconstruction of RIP matrices and the polynomial-time verification of RIP given\nan arbitrary matrix. We consider the framework of average-case certifiers, that\nnever wrongly declare that a matrix is RIP, while being often correct for\nrandom instances. While there are such functions which are tractable in a\nsuboptimal parameter regime, we show that this is a computationally hard task\nin any better regime. Our results are based on a new, weaker assumption on the\nproblem of detecting dense subgraphs.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 14:38:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Wang", "Tengyao", ""], ["Berthet", "Quentin", ""], ["Plan", "Yaniv", ""]]}]