[{"id": "2012.00145", "submitter": "Yuhan Jiang", "authors": "Kathlen Kohn, Rosa Winter, Yuhan Jiang", "title": "Linear Spaces of Symmetric Matrices with Non-Maximal Maximum Likelihood\n  Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood degree of linear concentration models in\nalgebraic statistics. We relate the geometry of the reciprocal variety to that\nof semidefinite programming. We show that the Zariski closure in the\nGrassmanian of the set of linear spaces that do not attain their maximal\npossible maximum likelihood degree coincides with the Zariski closure of the\nset of linear spaces defining a projection with non-closed image of the\npositive semidefinite cone. In particular, this shows that this closure is a\nunion of coisotropic hypersurfaces.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 22:53:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kohn", "Kathlen", ""], ["Winter", "Rosa", ""], ["Jiang", "Yuhan", ""]]}, {"id": "2012.00180", "submitter": "John R.J. Thompson", "authors": "John R.J. Thompson, W. John Braun", "title": "Anisotropic local constant smoothing for change-point regression\n  function estimation", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding forest fire spread in any region of Canada is critical to\npromoting forest health, and protecting human life and infrastructure.\nQuantifying fire spread from noisy images, where regions of a fire are\nseparated by change-point boundaries, is critical to faithfully estimating fire\nspread rates. In this research, we develop a statistically consistent smooth\nestimator that allows us to denoise fire spread imagery from micro-fire\nexperiments. We develop an anisotropic smoothing method for change-point data\nthat uses estimates of the underlying data generating process to inform\nsmoothing. We show that the anisotropic local constant regression estimator is\nconsistent with convergence rate $O\\left(n^{-1/{(q+2)}}\\right)$. We demonstrate\nits effectiveness on simulated one- and two-dimensional change-point data and\nfire spread imagery from micro-fire experiments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:11:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Thompson", "John R. J.", ""], ["Braun", "W. John", ""]]}, {"id": "2012.00198", "submitter": "Lukas Gustafsson", "authors": "Carlos Am\\'endola, Lukas Gustafsson, Kathl\\'en Kohn, Orlando\n  Marigliano, Anna Seigal", "title": "The Maximum Likelihood Degree of Linear Spaces of Symmetric Matrices", "comments": "21 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multivariate Gaussian models that are described by linear conditions\non the concentration matrix. We compute the maximum likelihood (ML) degrees of\nthese models. That is, we count the critical points of the likelihood function\nover a linear space of symmetric matrices. We obtain new formulae for the ML\ndegree, one via Schubert calculus, and another using Segre classes from\nintersection theory. We settle the case of codimension one models, and\ncharacterize the degenerate case when the ML degree is zero.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 01:04:18 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 15:23:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Gustafsson", "Lukas", ""], ["Kohn", "Kathl\u00e9n", ""], ["Marigliano", "Orlando", ""], ["Seigal", "Anna", ""]]}, {"id": "2012.00369", "submitter": "Nikolay Nikolaev Mr", "authors": "Romeo Ortega, Alexey Bobtsov and Nikolay Nikolaev", "title": "Parameter Identification with Finite-Convergence Time Alertness\n  Preservation", "comments": "arXiv admin note: text overlap with arXiv:1908.05125", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief note we present two new parameter identifiers whose estimates\nconverge in finite time under weak interval excitation assumptions. The main\nnovelty is that, in contrast with other finite-convergence time (FCT)\nestimators, our schemes preserve the FCT property when the parameters change.\nThe previous versions of our FCT estimators can track the parameter variations\nonly asymptotically. Continuous-time and discrete-time versions of the new\nestimators are presented\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:52:05 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ortega", "Romeo", ""], ["Bobtsov", "Alexey", ""], ["Nikolaev", "Nikolay", ""]]}, {"id": "2012.00444", "submitter": "Arlene K. H. Kim", "authors": "Arlene K. H. Kim, Adityanand Guntuboyina", "title": "Minimax bounds for estimating multivariate Gaussian location mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove minimax bounds for estimating Gaussian location mixtures on\n$\\mathbb{R}^d$ under the squared $L^2$ and the squared Hellinger loss\nfunctions. Under the squared $L^2$ loss, we prove that the minimax rate is\nupper and lower bounded by a constant multiple of $n^{-1}(\\log n)^{d/2}$. Under\nthe squared Hellinger loss, we consider two subclasses based on the behavior of\nthe tails of the mixing measure. When the mixing measure has a sub-Gaussian\ntail, the minimax rate under the squared Hellinger loss is bounded from below\nby $(\\log n)^{d}/n$. On the other hand, when the mixing measure is only assumed\nto have a bounded $p^{\\text{th}}$ moment for a fixed $p > 0$, the minimax rate\nunder the squared Hellinger loss is bounded from below by $n^{-p/(p+d)}(\\log\nn)^{-3d/2}$. These rates are minimax optimal up to logarithmic factors.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 12:35:45 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 04:06:01 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kim", "Arlene K. H.", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "2012.00460", "submitter": "Yi Yu", "authors": "Daren Wang, Zifeng Zhao, Yi Yu and Rebecca Willett", "title": "Functional Linear Regression with Mixed Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a functional linear regression model that deals with functional\nresponses and allows for both functional covariates and high-dimensional vector\ncovariates. The proposed model is flexible and nests several functional\nregression models in the literature as special cases. Based on the theory of\nreproducing kernel Hilbert spaces (RKHS), we propose a penalized least squares\nestimator that can accommodate functional variables observed on discrete grids.\nBesides the conventional smoothness penalties, a group Lasso-type penalty is\nfurther imposed to induce sparsity in the high-dimensional vector predictors.\nWe derive finite sample theoretical guarantees and show that the excess\nprediction risk of our estimator is minimax optimal. Furthermore, our analysis\nreveals an interesting phase transition phenomenon that the optimal excess risk\nis determined jointly by the smoothness and the sparsity of the functional\nregression coefficients. A novel efficient optimization algorithm based on\niterative coordinate descent is devised to handle the smoothness and sparsity\npenalties simultaneously. Simulation studies and real data applications\nillustrate the promising performance of the proposed approach compared to the\nstate-of-the-art methods in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:10:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 09:34:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wang", "Daren", ""], ["Zhao", "Zifeng", ""], ["Yu", "Yi", ""], ["Willett", "Rebecca", ""]]}, {"id": "2012.00663", "submitter": "Pankaj Mehta", "authors": "Wenping Cui, Jason W. Rocks, Pankaj Mehta", "title": "The Perturbative Resolvent Method: spectral densities of random matrix\n  ensembles via perturbation theory", "comments": "15 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple, perturbative approach for calculating spectral densities\nfor random matrix ensembles in the thermodynamic limit we call the Perturbative\nResolvent Method (PRM). The PRM is based on constructing a linear system of\nequations and calculating how the solutions to these equation change in\nresponse to a small perturbation using the zero-temperature cavity method. We\nillustrate the power of the method by providing simple analytic derivations of\nthe Wigner Semi-circle Law for symmetric matrices, the Marchenko-Pastur Law for\nWishart matrices, the spectral density for a product Wishart matrix composed of\ntwo square matrices, and the Circle and elliptic laws for real random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:35:28 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cui", "Wenping", ""], ["Rocks", "Jason W.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "2012.00725", "submitter": "Tam\\'as Szabados", "authors": "Tam\\'as Szabados", "title": "Regular multidimensional stationary time series", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to give a relatively simple, usable sufficient and\nnecessary condition to the regularity of generic time series. Also, this\ncondition is used to show how a regular process can be approximated by a lower\nrank \\emph{regular} process.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:37:56 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Szabados", "Tam\u00e1s", ""]]}, {"id": "2012.00732", "submitter": "Vasilis Kontonis", "authors": "Vasilis Kontonis, Sihan Liu, Christos Tzamos", "title": "Convergence and Sample Complexity of SGD in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical convergence guarantees on training Generative\nAdversarial Networks (GANs) via SGD. We consider learning a target distribution\nmodeled by a 1-layer Generator network with a non-linear activation function\n$\\phi(\\cdot)$ parametrized by a $d \\times d$ weight matrix $\\mathbf W_*$, i.e.,\n$f_*(\\mathbf x) = \\phi(\\mathbf W_* \\mathbf x)$.\n  Our main result is that by training the Generator together with a\nDiscriminator according to the Stochastic Gradient Descent-Ascent iteration\nproposed by Goodfellow et al. yields a Generator distribution that approaches\nthe target distribution of $f_*$. Specifically, we can learn the target\ndistribution within total-variation distance $\\epsilon$ using $\\tilde\nO(d^2/\\epsilon^2)$ samples which is (near-)information theoretically optimal.\n  Our results apply to a broad class of non-linear activation functions $\\phi$,\nincluding ReLUs and is enabled by a connection with truncated statistics and an\nappropriate design of the Discriminator network. Our approach relies on a\nbilevel optimization framework to show that vanilla SGDA works.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:50:38 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kontonis", "Vasilis", ""], ["Liu", "Sihan", ""], ["Tzamos", "Christos", ""]]}, {"id": "2012.00742", "submitter": "Chaim Even-Zohar", "authors": "Chaim Even-Zohar and Tsviqa Lakrec and Ran J. Tessler", "title": "Spectral Analysis of Word Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a random text over a finite alphabet, we study the frequencies at which\nfixed-length words occur as subsequences. As the data size grows, the joint\ndistribution of word counts exhibits a rich asymptotic structure. We\ninvestigate all linear combinations of subword statistics, and fully\ncharacterize their different orders of magnitude using diverse algebraic tools.\n  Moreover, we establish the spectral decomposition of the space of word\nstatistics of each order. We provide explicit formulas for the eigenvectors and\neigenvalues of the covariance matrix of the multivariate distribution of these\nstatistics. Our techniques include and elaborate on a set of algebraic word\noperators, recently studied and employed by Dieker and Saliola (Adv Math,\n2018).\n  Subword counts find applications in Combinatorics, Statistics, and Computer\nScience. We revisit special cases from the combinatorial literature, such as\nintransitive dice, random core partitions, and questions on random walk. Our\nstructural approach describes in a unified framework several classical\nstatistical tests. We propose further potential applications to data analysis\nand machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:59:40 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 16:29:12 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Even-Zohar", "Chaim", ""], ["Lakrec", "Tsviqa", ""], ["Tessler", "Ran J.", ""]]}, {"id": "2012.00748", "submitter": "Christoph Fuhrmann", "authors": "Christoph Fuhrmann, Hanns Ludwig Harney, Klaus Harney and Andreas\n  M\\\"uller", "title": "On The Gaussian Approximation To Bayesian Posterior Distributions", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The present article derives the minimal number $N$ of observations needed to\nconsider a Bayesian posterior distribution as Gaussian. Two examples are\npresented. Within one of them, a chi-squared distribution, the observable $x$\nas well as the parameter $\\xi$ are defined all over the real axis, in the other\none, the binomial distribution, the observable $x$ is an entire number while\nthe parameter $\\xi$ is defined on a finite interval of the real axis. The\nrequired minimal $N$ is high in the first case and low for the binomial model.\nIn both cases the precise definition of the measure $\\mu$ on the scale of $\\xi$\nis crucial.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:10:10 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Fuhrmann", "Christoph", ""], ["Harney", "Hanns Ludwig", ""], ["Harney", "Klaus", ""], ["M\u00fcller", "Andreas", ""]]}, {"id": "2012.00807", "submitter": "Matthias L\\\"offler", "authors": "Geoffrey Chinot, Matthias L\\\"offler and Sara van de Geer", "title": "On the robustness of minimum-norm interpolators", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.NA math.IT math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article develops a general theory for minimum-norm interpolated\nestimators in linear models in the presence of additive, potentially\nadversarial, errors. In particular, no conditions on the errors are imposed. A\nquantitative bound for the prediction error is given, relating it to the\nRademacher complexity of the covariates, the norm of the minimum norm\ninterpolator of the errors and the shape of the subdifferential around the true\nparameter. The general theory is illustrated with several examples: the sparse\nlinear model with minimum $\\ell_1$-norm or group Lasso penalty interpolation,\nthe low rank trace regression model with nuclear norm minimization, and minimum\nEuclidean norm interpolation in the linear model. In case of sparsity or\nlow-rank inducing norms, minimum norm interpolation yields a prediction error\nof the order of the average noise level, provided that the overparameterization\nis at least a logarithmic factor larger than the number of samples. Lower\nbounds that show near optimality of the results complement the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 20:03:20 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 14:37:55 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Chinot", "Geoffrey", ""], ["L\u00f6ffler", "Matthias", ""], ["van de Geer", "Sara", ""]]}, {"id": "2012.00933", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Sifan Liu, Zongming Ma", "title": "Global and Individualized Community Detection in Inhomogeneous\n  Multilayer Networks", "comments": "Corrected a few typos. 96 pages (main manuscript: 27 pages,\n  appendices: 69 pages), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network applications, it has become increasingly common to obtain datasets\nin the form of multiple networks observed on the same set of subjects, where\neach network is obtained in a related but different experiment condition or\napplication scenario. Such datasets can be modeled by multilayer networks where\neach layer is a separate network itself while different layers are associated\nand share some common information. The present paper studies community\ndetection in a stylized yet informative inhomogeneous multilayer network model.\nIn our model, layers are generated by different stochastic block models, the\ncommunity structures of which are (random) perturbations of a common global\nstructure while the connecting probabilities in different layers are not\nrelated. Focusing on the symmetric two block case, we establish minimax rates\nfor both \\emph{global estimation} of the common structure and\n\\emph{individualized estimation} of layer-wise community structures. Both\nminimax rates have sharp exponents. In addition, we provide an efficient\nalgorithm that is simultaneously asymptotic minimax optimal for both estimation\ntasks under mild conditions. The optimal rates depend on the \\emph{parity} of\nthe number of most informative layers, a phenomenon that is caused by\ninhomogeneity across layers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 02:42:52 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 14:43:42 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Shuxiao", ""], ["Liu", "Sifan", ""], ["Ma", "Zongming", ""]]}, {"id": "2012.00960", "submitter": "Gwo Dong Lin", "authors": "Gwo Dong Lin and Xiaoling Dou", "title": "An Identity for Two Integral Transforms Applied to the Uniqueness of a\n  Distribution via its Laplace-Stieltjes Transform", "comments": "22 pages", "journal-ref": "Statistics 2021", "doi": "10.1080/02331888.2021.1893728", "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the Laplace-Stieltjes transform of a nonnegative random\nvariable (or random vector) uniquely determines its distribution function. We\nextend this uniqueness theorem by using the Muntz-Szasz Theorem and the\nidentity for the Laplace-Stieltjes and Laplace-Carson transforms of a\ndistribution function. The latter appears for the first time to the best of our\nknowledge. In particular, if X and Y are two nonnegative random variables with\njoint distribution H, then H can be characterized by a suitable set of\ncountably many values of its bivariate Laplace-Stieltjes transform. The general\nhigh-dimensional case is also investigated. Besides, Lerch's uniqueness theorem\nfor conventional Laplace transforms is extended as well. The identity can be\nused to simplify the calculation of Laplace-Stieltjes transforms when the\nunderlying distributions have singular parts. Finally, some examples are given\nto illustrate the characterization results via the uniqueness theorem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 04:20:34 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 04:31:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lin", "Gwo Dong", ""], ["Dou", "Xiaoling", ""]]}, {"id": "2012.00980", "submitter": "Natsumi Makigusa", "authors": "Natsumi Makigusa", "title": "Two-sample test based on maximum variance discrepancy", "comments": "66 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a novel discrepancy called the maximum variance\ndiscrepancy for the purpose of measuring the difference between two\ndistributions in Hilbert spaces that cannot be found via the maximum mean\ndiscrepancy. We also propose a two-sample goodness of fit test based on this\ndiscrepancy. We obtain the asymptotic null distribution of this two-sample\ntest, which provides an efficient approximation method for the null\ndistribution of the test.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 06:11:49 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 03:27:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Makigusa", "Natsumi", ""]]}, {"id": "2012.00990", "submitter": "Natalia Nolde", "authors": "Natalia Nolde and Jennifer L. Wadsworth", "title": "Connections between representations for multivariate extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study of multivariate extremes is dominated by multivariate regular\nvariation, although it is well known that this approach does not provide\nadequate distinction between random vectors whose components are not always\nsimultaneously large. Various alternative dependence measures and\nrepresentations have been proposed, with the most well-known being hidden\nregular variation and the conditional extreme value model. These varying\ndepictions of extremal dependence arise through consideration of different\nparts of the multivariate domain, and particularly exploring what happens when\nextremes of one variable may grow at different rates to other variables. Thus\nfar, these alternative representations have come from distinct sources and\nlinks between them are limited. In this work we elucidate many of the relevant\nconnections through a geometrical approach. In particular, the shape of the\nlimit set of scaled sample clouds in light-tailed margins is shown to provide a\ndescription of several different extremal dependence representations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 06:49:38 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nolde", "Natalia", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2012.01182", "submitter": "Olivier Besson", "authors": "Olivier Besson", "title": "Impact of covariance mismatched training samples on constant false alarm\n  rate detectors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2021.3050567", "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of this paper is that of adaptive detection in Gaussian noise\nwith unknown covariance matrix when the training samples do not share the same\ncovariance matrix as the vector under test. We consider a class of constant\nfalse alarm rate detectors which depend on two statistics $(\\beta,\\ttilde)$\nwhose distribution is parameter-free in the case of no mismatch and we analyze\nthe impact of covariance mismatched training samples. More precisely, we\nprovide a statistical representation of these two variables for an arbitrary\nmismatch. We show that covariance mismatch induces significant variations of\nthe probability of false alarm and we investigate a way to mitigate this\neffect.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:15:04 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Besson", "Olivier", ""]]}, {"id": "2012.01238", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Roberto Vila and Mehmet Niyazi \\c{C}ankaya", "title": "A Bimodal Weibull Distribution: Properties and Inference", "comments": "36 pages; 9 figures;5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling is a challenging topic and using parametric models is an important\nstage to reach flexible function for modeling. Weibull distribution has two\nparameters which are shape $\\alpha$ and scale $\\beta$. In this study,\nbimodality parameter is added and so bimodal Weibull distribution is proposed\nby using a quadratic transformation technique used to generate bimodal\nfunctions produced due to using the quadratic expression. The analytical\nsimplicity of Weibull and quadratic form give an advantage to derive a bimodal\nWeibull via constructing normalizing constant. The characteristics and\nproperties of the proposed distribution are examined to show its usability in\nmodeling. After examination as first stage in modeling issue, it is appropriate\nto use bimodal Weibull for modeling data sets. Two estimation methods which are\nmaximum $\\log_q$ likelihood and its special form including objective functions\n$\\log_q(f)$ and $\\log(f)$ are used to estimate the parameters of shape, scale\nand bimodality parameters of the function. The second stage in modeling is\novercome by using heuristic algorithm for optimization of function according to\nparameters due to fact that converging to global point of objective function is\nperformed by heuristic algorithm based on the stochastic optimization. Real\ndata sets are provided to show the modeling competence of the proposed\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:26:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Vila", "Roberto", ""], ["\u00c7ankaya", "Mehmet Niyazi", ""]]}, {"id": "2012.01697", "submitter": "Yanbo Tang", "authors": "Yanbo Tang, Radu Craiu and Lei Sun", "title": "General Behaviour of P-Values Under the Null and Alternative", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hypothesis testing results often rely on simple, yet important assumptions\nabout the behaviour of the distribution of p-values under the null and the\nalternative. We examine tests for one dimensional parameters of interest that\nconverge to a normal distribution, possibly in the presence of nuisance\nparameters, and characterize the distribution of the p-values using techniques\nfrom the higher order asymptotics literature. We show that commonly held\nbeliefs regarding the distribution of p-values are misleading when the variance\nand location of the test statistic are not well-calibrated or when the higher\norder cumulants of the test statistic are not negligible. Corrected tests are\nproposed and are shown to perform better than their first order counterparts in\ncertain settings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:36:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Tang", "Yanbo", ""], ["Craiu", "Radu", ""], ["Sun", "Lei", ""]]}, {"id": "2012.01758", "submitter": "Steven Siwei Ye", "authors": "Steven Siwei Ye and Oscar Hernan Madrid Padilla", "title": "Non-parametric Quantile Regression via the K-NN Fused Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quantile regression is a statistical method for estimating conditional\nquantiles of a response variable. In addition, for mean estimation, it is well\nknown that quantile regression is more robust to outliers than $l_2$-based\nmethods. By using the fused lasso penalty over a $K$-nearest neighbors graph,\nwe propose an adaptive quantile estimator in a non-parametric setup. We show\nthat the estimator attains optimal rate of $n^{-1/d}$ up to a logarithmic\nfactor, under mild assumptions on the data generation mechanism of the\n$d$-dimensional data. We develop algorithms to compute the estimator and\ndiscuss methodology for model selection. Numerical experiments on simulated and\nreal data demonstrate clear advantages of the proposed estimator over state of\nthe art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:43:20 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:03:31 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 10:27:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Steven Siwei", ""], ["Padilla", "Oscar Hernan Madrid", ""]]}, {"id": "2012.01905", "submitter": "Isobel Davies", "authors": "Isobel Davies and Orlando Marigliano", "title": "Coloured Graphical Models and their Symmetries", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coloured graphical models are Gaussian statistical models determined by an\nundirected coloured graph. These models can be described by linear spaces of\nsymmetric matrices. We outline a relationship between the symmetries of the\ngraph and the linear forms that vanish on the reciprocal variety of the model.\nIn particular, we give four families for which such linear forms are completely\ndescribed by symmetries.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:37:24 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 15:39:20 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 08:38:00 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Davies", "Isobel", ""], ["Marigliano", "Orlando", ""]]}, {"id": "2012.01929", "submitter": "Gersende Fort", "authors": "Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP), Hoi-To Wai", "title": "A Stochastic Path-Integrated Differential EstimatoR Expectation\n  Maximization Algorithm", "comments": null, "journal-ref": "Proceedings of the Conference on Neural Information Processing\n  Systems (NeurIPS 2020), 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation Maximization (EM) algorithm is of key importance for\ninference in latent variable models including mixture of regressors and\nexperts, missing observations. This paper introduces a novel EM algorithm,\ncalled \\texttt{SPIDER-EM}, for inference from a training set of size $n$, $n\n\\gg 1$. At the core of our algorithm is an estimator of the full conditional\nexpectation in the {\\sf E}-step, adapted from the stochastic path-integrated\ndifferential estimator ({\\tt SPIDER}) technique. We derive finite-time\ncomplexity bounds for smooth non-convex likelihood: we show that for\nconvergence to an $\\epsilon$-approximate stationary point, the complexity\nscales as $K_{\\operatorname{Opt}} (n,\\epsilon )={\\cal O}(\\epsilon^{-1})$ and\n$K_{\\operatorname{CE}}( n,\\epsilon ) = n+ \\sqrt{n} {\\cal O}(\\epsilon^{-1} )$,\nwhere $K_{\\operatorname{Opt}}( n,\\epsilon )$ and $K_{\\operatorname{CE}}(n,\n\\epsilon )$ are respectively the number of {\\sf M}-steps and the number of\nper-sample conditional expectations evaluations. This improves over the\nstate-of-the-art algorithms. Numerical results support our findings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:49:31 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fort", "Gersende", "", "IMT"], ["Moulines", "Eric", "", "X-DEP-MATHAPP"], ["Wai", "Hoi-To", ""]]}, {"id": "2012.02119", "submitter": "Ainesh Bakshi", "authors": "Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M. Kane, Pravesh K.\n  Kothari and Santosh S. Vempala", "title": "Robustly Learning Mixtures of $k$ Arbitrary Gaussians", "comments": "This version extends the previous one to yield 1) robust proper\n  learning algorithm with poly(eps) error and 2) an information theoretic\n  argument proving that the same algorithms in fact also yield parameter\n  recovery guarantees. The updates are included in Sections 7,8, and 9 and the\n  main result from the previous version (Thm 1.4) is presented and proved in\n  Section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a polynomial-time algorithm for the problem of robustly estimating a\nmixture of $k$ arbitrary Gaussians in $\\mathbb{R}^d$, for any fixed $k$, in the\npresence of a constant fraction of arbitrary corruptions. This resolves the\nmain open problem in several previous works on algorithmic robust statistics,\nwhich addressed the special cases of robustly estimating (a) a single Gaussian,\n(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of\ntwo Gaussians. Our main tools are an efficient \\emph{partial clustering}\nalgorithm that relies on the sum-of-squares method, and a novel \\emph{tensor\ndecomposition} algorithm that allows errors in both Frobenius norm and low-rank\nterms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:54:03 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 17:24:52 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 16:26:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bakshi", "Ainesh", ""], ["Diakonikolas", "Ilias", ""], ["Jia", "He", ""], ["Kane", "Daniel M.", ""], ["Kothari", "Pravesh K.", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "2012.02385", "submitter": "TrungTin Nguyen", "authors": "Hien Duy Nguyen and TrungTin Nguyen and Faicel Chamroukhi and Geoffrey\n  McLachlan", "title": "Approximations of conditional probability density functions in Lebesgue\n  spaces via mixture of experts models", "comments": "Corrected typos. Added new Section 6. Summary and conclusions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) models are widely applied for conditional\nprobability density estimation problems. We demonstrate the richness of the\nclass of MoE models by proving denseness results in Lebesgue spaces, when\ninputs and outputs variables are both compactly supported. We further prove an\nalmost uniform convergence result when the input is univariate. Auxiliary\nlemmas are proved regarding the richness of the soft-max gating function class,\nand their relationships to the class of Gaussian gating functions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:33:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 18:01:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Nguyen", "Hien Duy", ""], ["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["McLachlan", "Geoffrey", ""]]}, {"id": "2012.02445", "submitter": "Annika Betken", "authors": "Annika Betken, Herold Dehling, Ines M\\\"unker, Alexander Schnurr", "title": "Ordinal pattern dependence as a multivariate dependence measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we show that the recently introduced ordinal pattern\ndependence fits into the axiomatic framework of general multivariate dependence\nmeasures. Furthermore, we consider multivariate generalizations of established\nunivariate dependence measures like Kendall's $\\tau$, Spearman's $\\rho$ and\nPearson's correlation coefficient. Among these, only multivariate Kendall's\n$\\tau$ proves to take the dynamical dependence of random vectors stemming from\nmultidimensional time series into account. Consequently, the article focuses on\na comparison of ordinal pattern dependence and multivariate Kendall's $\\tau$.\nTo this end, limit theorems for multivariate Kendall's $\\tau$ are established\nunder the assumption of near epoch dependent, data-generating time series. We\nanalyze how ordinal pattern dependence compares to multivariate Kendall's\n$\\tau$ and Pearson's correlation coefficient on theoretical grounds.\n  Additionally, a simulation study illustrates differences in the kind of\ndependencies that are revealed by multivariate Kendall's $\\tau$ and ordinal\npattern dependence.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 07:53:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Betken", "Annika", ""], ["Dehling", "Herold", ""], ["M\u00fcnker", "Ines", ""], ["Schnurr", "Alexander", ""]]}, {"id": "2012.02468", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Esra Pamuk\\c{c}u, Mehmet Niyazi \\c{C}ankaya", "title": "Information Complexity Criterion for Model Selection in Robust\n  Regression Using A New Robust Penalty Term", "comments": "3 figures; 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model selection is basically a process of finding the best model from the\nsubset of models in which the explanatory variables are effective on the\nresponse variable. The log likelihood function for the lack of fit term and a\nspecified penalty term are used as two parts in a model selection criteria. In\nthis paper, we derive a new tool for the model selection in robust regression.\nWe introduce a new definition of relative entropy based on objective functions.\nDue to the analytical simplicity, we use Huber's objective function $\\rho_H$\nand propose our specified penalty term $C_0^{\\rho_H}$ to derive new Information\nComplexity Criterion ($RICOMP_{C_0^{\\rho_H}}$) as a robust model selection\ntool. Additionally, by using the properties of $C_0^{\\rho_H}$, we propose a new\nvalue of tuning parameter called $k_{C_0}$ for the Huber's $\\rho_H$. If a\ncontamination to normal distribution exists, $RICOMP_{C_0^{\\rho_H}}$ chooses\nthe true model better than the rival ones. Monte Carlo Simulation studies are\ncarried out to show the utility both of $k_{C_0}$ and $RICOMP_{C_0^{\\rho_H}}$.\nA real data example is also given.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:40:12 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Pamuk\u00e7u", "Esra", ""], ["\u00c7ankaya", "Mehmet Niyazi", ""]]}, {"id": "2012.02635", "submitter": "Fatemeh Asgari", "authors": "Fatemeh Asgari and Mohammad Hossein Alamatsaz and Valeria Vitelli and\n  Saeed Hayati", "title": "Latent function-on-scalar regression models for observed sequences of\n  binary data: a restricted likelihood approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a functional regression setting where the random\nresponse curve is unobserved, and only its dichotomized version observed at a\nsequence of correlated binary data is available. We propose a practical\ncomputational framework for maximum likelihood analysis via the parameter\nexpansion technique. Compared to existing methods, our proposal relies on the\nuse of a complete data likelihood, with the advantage of being able to handle\nnon-equally spaced and missing observations effectively. The proposed method is\nused in the Function-on-Scalar regression setting, with the latent response\nvariable being a Gaussian random element taking values in a separable Hilbert\nspace. Smooth estimations of functional regression coefficients and principal\ncomponents are provided by introducing an adaptive MCEM algorithm that\ncircumvents selecting the smoothing parameters. Finally, the performance of our\nnovel method is demonstrated by various simulation studies and on a real case\nstudy. The proposed method is implemented in the R package dfrr.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:52:26 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Asgari", "Fatemeh", ""], ["Alamatsaz", "Mohammad Hossein", ""], ["Vitelli", "Valeria", ""], ["Hayati", "Saeed", ""]]}, {"id": "2012.02816", "submitter": "Tong Liu", "authors": "Yu Hang Jiang, Tong Liu, Zhiya Lou, Jeffrey S. Rosenthal, Shanshan\n  Shangguan, Fei Wang, and Zixuan Wu", "title": "MCMC Confidence Intervals and Biases", "comments": "20 pages (not including references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent paper \"Simple confidence intervals for MCMC without CLTs\" by J.S.\nRosenthal, showed the derivation of a simple MCMC confidence interval using\nonly Chebyshev's inequality, not CLT. That result required certain assumptions\nabout how the estimator bias and variance grow with the number of iterations\n$n$. In particular, the bias is $o(1/\\sqrt{n})$. This assumption seemed mild.\nIt is generally believed that the estimator bias will be $O(1/n)$ and hence\n$o(1/\\sqrt{n})$. However, questions were raised by researchers about how to\nverify this assumption. Indeed, we show that this assumption might not always\nhold. In this paper, we seek to simplify and weaken the assumptions in the\npreviously mentioned paper, to make MCMC confidence intervals without CLTs more\nwidely applicable.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:46:08 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 22:55:38 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jiang", "Yu Hang", ""], ["Liu", "Tong", ""], ["Lou", "Zhiya", ""], ["Rosenthal", "Jeffrey S.", ""], ["Shangguan", "Shanshan", ""], ["Wang", "Fei", ""], ["Wu", "Zixuan", ""]]}, {"id": "2012.02901", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii M. Ostrovskii, Mohamed Ndaoud, Adel Javanmard, Meisam\n  Razaviyayn", "title": "Near-Optimal Procedures for Model Discrimination with Non-Disclosure\n  Properties", "comments": "52 pages, 2 figures; corrected the proof of the lower bound; added\n  new applications and the Fisher information-based argument in Appendix F", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\theta_0,\\theta_1 \\in \\mathbb{R}^d$ be the population risk minimizers\nassociated to some loss $\\ell:\\mathbb{R}^d\\times \\mathcal{Z}\\to\\mathbb{R}$ and\ntwo distributions $\\mathbb{P}_0,\\mathbb{P}_1$ on $\\mathcal{Z}$. The models\n$\\theta_0,\\theta_1$ are unknown, and $\\mathbb{P}_0,\\mathbb{P}_1$ can be\naccessed by drawing i.i.d samples from them. Our work is motivated by the\nfollowing model discrimination question: \"What sizes of the samples from\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ allow to distinguish between the two\nhypotheses $\\theta^*=\\theta_0$ and $\\theta^*=\\theta_1$ for given\n$\\theta^*\\in\\{\\theta_0,\\theta_1\\}$?\" Making the first steps towards answering\nit in full generality, we first consider the case of a well-specified linear\nmodel with squared loss. Here we provide matching upper and lower bounds on the\nsample complexity as given by $\\min\\{1/\\Delta^2,\\sqrt{r}/\\Delta\\}$ up to a\nconstant factor; here $\\Delta$ is a measure of separation between\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ and $r$ is the rank of the design covariance\nmatrix. We then extend this result in two directions: (i) for general\nparametric models in asymptotic regime; (ii) for generalized linear models in\nsmall samples ($n\\le r$) under weak moment assumptions. In both cases we derive\nsample complexity bounds of a similar form while allowing for model\nmisspecification. In fact, our testing procedures only access $\\theta^*$ via a\ncertain functional of empirical risk. In addition, the number of observations\nthat allows us to reach statistical confidence does not allow to \"resolve\" the\ntwo models $-$ that is, recover $\\theta_0,\\theta_1$ up to $O(\\Delta)$\nprediction accuracy. These two properties allow to use our framework in applied\ntasks where one would like to $\\textit{identify}$ a prediction model, which can\nbe proprietary, while guaranteeing that the model cannot be actually\n$\\textit{inferred}$ by the identifying agent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:52:54 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:56:43 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 12:46:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ostrovskii", "Dmitrii M.", ""], ["Ndaoud", "Mohamed", ""], ["Javanmard", "Adel", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2012.02914", "submitter": "Marios Papamichalis V.", "authors": "Marios Papamichalis, Simon Lunagomez and Patrick J. Wolfe", "title": "Robustness on Networks", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We adopt the statistical framework on robustness proposed by Watson and\nHolmes in 2016 and then tackle the practical challenges that hinder its\napplicability to network models. The goal is to evaluate how the quality of an\ninference for a network feature degrades when the assumed model is\nmisspecified. Decision theory methods aimed to identify model\nmissespecification are applied in the context of network data with the goal of\ninvestigating the stability of optimal actions to perturbations to the assumed\nmodel. Here the modified versions of the model are contained within a well\ndefined neighborhood of model space. Our main challenge is to combine\nstochastic optimization and graph limits tools to explore the model space. As a\nresult, a method for robustness on exchangeable random networks is developed.\nOur approach is inspired by recent developments in the context of robustness\nand recent works in the robust control, macroeconomics and financial\nmathematics literature and more specifically and is based on the concept of\ngraphon approximation through its empirical graphon.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 01:18:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Papamichalis", "Marios", ""], ["Lunagomez", "Simon", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "2012.02985", "submitter": "David Hong", "authors": "David Hong, Yue Sheng, Edgar Dobriban", "title": "Selecting the number of components in PCA via random signflips", "comments": "46 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction via PCA and factor analysis is an important tool of\ndata analysis. A critical step is selecting the number of components. However,\nexisting methods (such as the scree plot, likelihood ratio, parallel analysis,\netc) do not have statistical guarantees in the increasingly common setting\nwhere the data are heterogeneous. There each noise entry can have a different\ndistribution. To address this problem, we propose the Signflip Parallel\nAnalysis (Signflip PA) method: it compares data singular values to those of\n\"empirical null\" data generated by flipping the sign of each entry randomly\nwith probability one-half. We show that Signflip PA consistently selects\nfactors above the noise level in high-dimensional signal-plus-noise models\n(including spiked models and factor models) under heterogeneous settings. Here\nclassical parallel analysis is no longer effective. To do this, we propose to\nleverage recent breakthroughs in random matrix theory, such as dimension-free\noperator norm bounds [Latala et al, 2018, Inventiones Mathematicae], and large\ndeviations for the top eigenvalues of nonhomogeneous matrices [Husson, 2020].\nTo our knowledge, some of these results have not yet been used in statistics.\nWe also illustrate that Signflip PA performs well in numerical simulations and\non empirical data examples.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 09:29:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hong", "David", ""], ["Sheng", "Yue", ""], ["Dobriban", "Edgar", ""]]}, {"id": "2012.03104", "submitter": "Onur Danaci", "authors": "Onur Danaci, Sanjaya Lohani, Brian T. Kirby, Ryan T. Glasser", "title": "Machine learning pipeline for quantum state estimation with incomplete\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-qubit systems typically employ 36 projective measurements for\nhigh-fidelity tomographic estimation. The overcomplete nature of the 36\nmeasurements suggests possible robustness of the estimation procedure to\nmissing measurements. In this paper, we explore the resilience of\nmachine-learning-based quantum state estimation techniques to missing\nmeasurements by creating a pipeline of stacked machine learning models for\nimputation, denoising, and state estimation. When applied to simulated\nnoiseless and noisy projective measurement data for both pure and mixed states,\nwe demonstrate quantum state estimation from partial measurement results that\noutperforms previously developed machine-learning-based methods in\nreconstruction fidelity and several conventional methods in terms of resource\nscaling. Notably, our developed model does not require training a separate\nmodel for each missing measurement, making it potentially applicable to quantum\nstate estimation of large quantum systems where preprocessing is\ncomputationally infeasible due to the exponential scaling of quantum system\ndimension.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:02:00 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Danaci", "Onur", ""], ["Lohani", "Sanjaya", ""], ["Kirby", "Brian T.", ""], ["Glasser", "Ryan T.", ""]]}, {"id": "2012.03376", "submitter": "Giovanni Pistone", "authors": "Giovanni Pistone", "title": "A Lecture About the Use of Orlicz Spaces in Information Geometry", "comments": "Minor revision of version 1. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a revised version of a tutorial lecture that I presented at the\n\\`Ecole de Physique des Houches on July 26-31 2020. Topics include\nNon-parametric Information Geometry, the Statistical bundle, exponential Orlicz\nspaces, and Gaussian Orlicz-Sobolev spaces.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:58:37 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 11:10:18 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Pistone", "Giovanni", ""]]}, {"id": "2012.03451", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Consistent and robust inference in hazard probability and odds models\n  with discrete-time survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete-time survival data, conditional likelihood inference in Cox's\nhazard odds model is theoretically desirable but exact calculation is numerical\nintractable with a moderate to large number of tied events. Unconditional\nmaximum likelihood estimation over both regression coefficients and baseline\nhazard probabilities can be problematic with a large number of time intervals.\nWe develop new methods and theory using numerically simple estimating\nfunctions, along with model-based and model-robust variance estimation, in\nhazard probability and odds models. For the probability hazard model, we derive\nas a consistent estimator the Breslow-Peto estimator, previously known as an\napproximation to the conditional likelihood estimator in the hazard odds model.\nFor the odds hazard model, we propose a weighted Mantel-Haenszel estimator,\nwhich satisfies conditional unbiasedness given the numbers of events in\naddition to the risk sets and covariates, similarly to the conditional\nlikelihood estimator. Our methods are expected to perform satisfactorily in a\nbroad range of settings, with small or large numbers of tied events\ncorresponding to a large or small number of time intervals. The methods are\nimplemented in the R package dSurvival.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:23:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "2012.03593", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "Algebraic geometry of discrete interventional models", "comments": "The previous version of this paper was divided into two distinct\n  papers. This is one of them", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC math.AG math.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:29:21 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:36:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2012.03658", "submitter": "Daniel Schaden", "authors": "Daniel Schaden and Elisabeth Ullmann", "title": "Asymptotic Analysis of Multilevel Best Linear Unbiased Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the computational complexity and variance of multilevel best linear\nunbiased estimators introduced in [D. Schaden and E. Ullmann, SIAM/ASA J.\nUncert. Quantif., (2020)]. We specialize the results in this work to PDE-based\nmodels that are parameterized by a discretization quantity, e.g., the finite\nelement mesh size. In particular, we investigate the asymptotic complexity of\nthe so-called sample allocation optimal best linear unbiased estimators\n(SAOBs). These estimators have the smallest variance given a fixed\ncomputational budget. However, SAOBs are defined implicitly by solving an\noptimization problem and are difficult to analyze. Alternatively, we study a\nclass of auxiliary estimators based on the Richardson extrapolation of the\nparametric model family. This allows us to provide an upper bound for the\ncomplexity of the SAOBs, showing that their complexity is optimal within a\ncertain class of linear unbiased estimators. Moreover, the complexity of the\nSAOBs is not larger than the complexity of Multilevel Monte Carlo. The\ntheoretical results are illustrated by numerical experiments with an elliptic\nPDE.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:16:27 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Schaden", "Daniel", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "2012.03758", "submitter": "Debraj Das", "authors": "Debraj Das", "title": "Uniform Central Limit Theorem for self normalized sums in high\n  dimensions", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we are interested in the normal approximation of the\nself-normalized random vector\n$\\Big(\\frac{\\sum_{i=1}^{n}X_{i1}}{\\sqrt{\\sum_{i=1}^{n}X_{i1}^2}},\\dots,\\frac{\\sum_{i=1}^{n}X_{ip}}{\\sqrt{\\sum_{i=1}^{n}X_{ip}^2}}\\Big)$\nin $\\mathcal{R}^p$ uniformly over the class of hyper-rectangles\n$\\mathcal{A}^{re}=\\{\\prod_{j=1}^{p}[a_j,b_j]\\cap\\mathcal{R}:-\\infty\\leq a_j\\leq\nb_j\\leq \\infty, j=1,\\ldots,p\\}$, where $X_1,\\dots,X_n$ are non-degenerate\nindependent $p-$dimensional random vectors with each having independent and\nidentically distributed (iid) components. We investigate the optimal cut-off\nrate of $\\log p$ in the uniform central limit theorem (UCLT) under variety of\nmoment conditions. When $X_{ij}$'s have $(2+\\delta)$th absolute moment for some\n$0< \\delta\\leq 1$, the optimal rate of $\\log p$ is\n$o\\big(n^{\\delta/(2+\\delta)}\\big)$. When $X_{ij}$'s are independent and\nidentically distributed (iid) across $(i,j)$, even $(2+\\delta)$th absolute\nmoment of $X_{11}$ is not needed. Only under the condition that $X_{11}$ is in\nthe domain of attraction of the normal distribution, the growth rate of $\\log\np$ can be made to be $o(\\eta_n)$ for some $\\eta_n\\rightarrow 0$ as\n$n\\rightarrow \\infty$. We also establish that the rate of $\\log p$ can be\npushed to $\\log p =o(n^{1/2})$ if we assume the existence of fourth moment of\n$X_{ij}$'s. By an example, it is shown however that the rate of growth of $\\log\np$ can not further be improved from $n^{1/2}$ as a power of $n$. As an\napplication, we found respective versions of the high dimensional UCLT for\ncomponent-wise Student's t-statistic. An important aspect of the these UCLTs is\nthat it does not require the existence of some exponential moments even when\ndimension $p$ grows exponentially with some power of $n$, as opposed to the\nUCLT of normalized sums. Only the existence of some absolute moment of order\n$\\in [2,4]$ is sufficient.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:52:09 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Das", "Debraj", ""]]}, {"id": "2012.03780", "submitter": "Benjamin Guedj", "authors": "Th\\'eophile Cantelobre and Benjamin Guedj and Mar\\'ia P\\'erez-Ortiz\n  and John Shawe-Taylor", "title": "A PAC-Bayesian Perspective on Structured Prediction with Implicit Loss\n  Embeddings", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many practical machine learning tasks can be framed as Structured prediction\nproblems, where several output variables are predicted and considered\ninterdependent. Recent theoretical advances in structured prediction have\nfocused on obtaining fast rates convergence guarantees, especially in the\nImplicit Loss Embedding (ILE) framework. PAC-Bayes has gained interest recently\nfor its capacity of producing tight risk bounds for predictor distributions.\nThis work proposes a novel PAC-Bayes perspective on the ILE Structured\nprediction framework. We present two generalization bounds, on the risk and\nexcess risk, which yield insights into the behavior of ILE predictors. Two\nlearning algorithms are derived from these bounds. The algorithms are\nimplemented and their behavior analyzed, with source code available at\n\\url{https://github.com/theophilec/PAC-Bayes-ILE-Structured-Prediction}.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:19:43 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:20:30 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cantelobre", "Th\u00e9ophile", ""], ["Guedj", "Benjamin", ""], ["P\u00e9rez-Ortiz", "Mar\u00eda", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2012.03809", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Independent Elliptical Distributions Minimize Their $\\mathcal{W}_2$\n  Wasserstein Distance from Independent Elliptical Distributions with the Same\n  Density Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note is on a property of the $\\mathcal{W}_2$ Wasserstein distance\nwhich indicates that independent elliptical distributions minimize their\n$\\mathcal{W}_2$ Wasserstein distance from given independent elliptical\ndistributions with the same density generators. Furthermore, we examine the\nimplications of this property in the Gelbrich bound when the distributions are\nnot necessarily elliptical. Meanwhile, we also generalize the results to the\ncases when the distributions are not independent. The primary purpose of this\nnote is for the referencing of papers that need to make use of this property or\nits implications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:52:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2012.03903", "submitter": "Carlos Am\\'endola", "authors": "Carlos Am\\'endola, Piotr Zwiernik", "title": "Likelihood Geometry of Correlation Models", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation matrices are standardized covariance matrices. They form an\naffine space of symmetric matrices defined by setting the diagonal entries to\none. We study the geometry of maximum likelihood estimation for this model and\nlinear submodels that encode additional symmetries. We also consider the\nproblem of minimizing two closely related functions of the covariance matrix:\nthe Stein's loss and the symmetrized Stein's loss. Unlike the Gaussian\nlog-likelihood these two functions are convex and hence admit a unique positive\ndefinite optimum. Some of our results hold for general affine covariance\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:32:15 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 17:44:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "2012.04023", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "The Spectral-Domain $\\mathcal{W}_2$ Wasserstein Distance for Elliptical\n  Processes and the Spectral-Domain Gelbrich Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we introduce the spectral-domain $\\mathcal{W}_2$\nWasserstein distance for elliptical stochastic processes in terms of their\npower spectra. We also introduce the spectral-domain Gelbrich bound for\nprocesses that are not necessarily elliptical.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 20:02:33 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 21:16:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2012.04464", "submitter": "Suzanne Thornton", "authors": "Suzanne Thornton and Minge Xie", "title": "Bridging Bayesian, frequentist and fiducial (BFF) inferences using\n  confidence distribution", "comments": "30 pages, 5 figures, Handbook on Bayesian Fiducial and Frequentist\n  (BFF) Inferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian, frequentist and fiducial (BFF) inferences are much more congruous\nthan they have been perceived historically in the scientific community (cf.,\nReid and Cox 2015; Kass 2011; Efron 1998). Most practitioners are probably more\nfamiliar with the two dominant statistical inferential paradigms, Bayesian\ninference and frequentist inference. The third, lesser known fiducial inference\nparadigm was pioneered by R.A. Fisher in an attempt to define an inversion\nprocedure for inference as an alternative to Bayes' theorem. Although each\nparadigm has its own strengths and limitations subject to their different\nphilosophical underpinnings, this article intends to bridge these different\ninferential methodologies through the lenses of confidence distribution theory\nand Monte-Carlo simulation procedures. This article attempts to understand how\nthese three distinct paradigms, Bayesian, frequentist, and fiducial inference,\ncan be unified and compared on a foundational level, thereby increasing the\nrange of possible techniques available to both statistical theorists and\npractitioners across all fields.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:03:07 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Thornton", "Suzanne", ""], ["Xie", "Minge", ""]]}, {"id": "2012.04505", "submitter": "Nicholas Syring", "authors": "Nicholas Syring and Ryan Martin", "title": "Gibbs posterior concentration rates under sub-exponential type losses", "comments": "60 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian posterior distributions are widely used for inference, but their\ndependence on a statistical model creates some challenges. In particular, there\nmay be lots of nuisance parameters that require prior distributions and\nposterior computations, plus a potentially serious risk of model\nmisspecification bias. Gibbs posterior distributions, on the other hand, offer\ndirect, principled, probabilistic inference on quantities of interest through a\nloss function, not a model-based likelihood. Here we provide simple sufficient\nconditions for establishing Gibbs posterior concentration rates when the loss\nfunction is of a sub-exponential type. We apply these general results in a\nrange of practically relevant examples, including mean regression, quantile\nregression, and sparse high-dimensional classification. We also apply these\ntechniques in an important problem in medical statistics, namely, estimation of\na personalized minimum clinically important difference.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:44:26 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 19:58:45 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 14:19:26 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 17:47:07 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Syring", "Nicholas", ""], ["Martin", "Ryan", ""]]}, {"id": "2012.04576", "submitter": "Dwight Nwaigwe", "authors": "Dwight Nwaigwe, Marek Rychlik", "title": "Convergence Rates for Multi-classs Logistic Regression Near Minimum", "comments": "Lemma 2 from previous versions is removed, and the article expanded", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current paper we provide constructive estimation of the convergence\nrate for training a known class of neural networks: the multi-class logistic\nregression. Despite several decades of successful use, our rigorous results\nappear new, reflective of the gap between practice and theory of machine\nlearning. Training a neural network is typically done via variations of the\ngradient descent method. If a minimum of the loss function exists and gradient\ndescent is used as the training method, we provide an expression that relates\nlearning rate to the rate of convergence to the minimum. The method involves an\nestimate of the condition number of the Hessian of the loss function. We also\ndiscuss the existence of a minimum, as it is not automatic that a minimum\nexists. One method of ensuring convergence is by assigning positive probabiity\nto every class in the training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:21:34 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 16:38:37 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 04:32:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nwaigwe", "Dwight", ""], ["Rychlik", "Marek", ""]]}, {"id": "2012.04646", "submitter": "Yang Feng", "authors": "Sihan Huang, Haolei Weng, Yang Feng", "title": "Spectral clustering via adaptive layer aggregation for multi-layer\n  networks", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in network analysis is detecting community\nstructure in multi-layer networks, of which each layer represents one type of\nedge information among the nodes. We propose integrative spectral clustering\napproaches based on effective convex layer aggregations. Our aggregation\nmethods are strongly motivated by a delicate asymptotic analysis of the\nspectral embedding of weighted adjacency matrices and the downstream $k$-means\nclustering, in a challenging regime where community detection consistency is\nimpossible. In fact, the methods are shown to estimate the optimal convex\naggregation, which minimizes the mis-clustering error under some specialized\nmulti-layer network models. Our analysis further suggests that clustering using\nGaussian mixture models is generally superior to the commonly used $k$-means in\nspectral clustering. Extensive numerical studies demonstrate that our adaptive\naggregation techniques, together with Gaussian mixture model clustering, make\nthe new spectral clustering remarkably competitive compared to several\npopularly used methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:58:18 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Huang", "Sihan", ""], ["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "2012.04741", "submitter": "Sim\\'eon Val\\`ere Bitseki Penda", "authors": "S. Val\\`ere Bitseki Penda and Jean-Fran\\c{c}ois Delmas", "title": "Central limit theorem for bifurcating {M}arkov chains under point-wise\n  ergodic conditions", "comments": "32", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bifurcating Markov chains (BMC) are Markov chains indexed by a full binary\ntree representing the evolution of a trait along a population where each\nindividual has two children. We provide a central limit theorem for general\nadditive functionals of BMC, and prove the existence of three regimes. This\ncorresponds to a competition between the reproducing rate (each individual has\ntwo children) and the ergodicity rate for the evolution of the trait. This is\nin contrast with the work of Guyon (2007), where the considered additive\nfunctionals are sums of martingale increments, and only one regime appears. Our\nresult can be seen as a discrete time version, but with general trait\nevolution, of results in the time continuous setting of branching particle\nsystem from Adamczak and Mi\\l{}o\\'{s} (2015), where the evolution of the trait\nis given by an Ornstein-Uhlenbeck process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:09:11 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 21:57:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Penda", "S. Val\u00e8re Bitseki", ""], ["Delmas", "Jean-Fran\u00e7ois", ""]]}, {"id": "2012.04957", "submitter": "Lasse Vuursteen", "authors": "Botond Szabo, Lasse Vuursteen, Harry van Zanten", "title": "Optimal distributed testing in high-dimensional Gaussian models", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper study the problem of signal detection in Gaussian noise in a\ndistributed setting. We derive a lower bound on the size that the signal needs\nto have in order to be detectable. Moreover, we exhibit optimal distributed\ntesting strategies that attain the lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 10:23:54 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Szabo", "Botond", ""], ["Vuursteen", "Lasse", ""], ["van Zanten", "Harry", ""]]}, {"id": "2012.05041", "submitter": "Bernd Sturmfels", "authors": "Bernd Sturmfels and Simon Telen", "title": "Likelihood Equations and Scattering Amplitudes", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG hep-th math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We relate scattering amplitudes in particle physics to maximum likelihood\nestimation for discrete models in algebraic statistics. The scattering\npotential plays the role of the log-likelihood function, and its critical\npoints are solutions to rational function equations. We study the ML degree of\nlow-rank tensor models in statistics, and we revisit physical theories proposed\nby Arkani-Hamed, Cachazo and their collaborators. Recent advances in numerical\nalgebraic geometry are employed to compute and certify critical points. We also\ndiscuss positive models and how to compute their string amplitudes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 13:41:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sturmfels", "Bernd", ""], ["Telen", "Simon", ""]]}, {"id": "2012.05051", "submitter": "Siegfried H\\\"ormann", "authors": "Siegfried H\\\"ormann and Fatima Jammoul", "title": "Consistently recovering the signal from noisy functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In practice most functional data cannot be recorded on a continuum, but\nrather at discrete time points. It is also quite common that these measurements\ncome with an additive error, which one would like eliminate for the statistical\nanalysis. When the measurements for each functional datum are taken on the same\ngrid, the underlying signal-plus-noise model can be viewed as a factor model.\nThe signals refer to the common components of the factor model, the noise is\nrelated to the idiosyncratic components. We formulate a framework which allows\nto consistently recover the signal by a PCA based factor model estimation\nscheme. Our theoretical results hold under rather mild conditions, in\nparticular we don't require specific smoothness assumptions for the underlying\ncurves and allow for a certain degree of autocorrelation in the noise.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 13:55:28 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Jammoul", "Fatima", ""]]}, {"id": "2012.05187", "submitter": "Kean Ming Tan", "authors": "Xuming He, Xiaoou Pan, Kean Ming Tan, and Wen-Xin Zhou", "title": "Smoothed Quantile Regression with Large-Scale Inference", "comments": "An R package conquer for fitting smoothed quantile regression is\n  available in CRAN, https://cran.r-project.org/web/packages/conquer/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a powerful tool for learning the relationship between\na response variable and a multivariate predictor while exploring heterogeneous\neffects. In this paper, we consider statistical inference for quantile\nregression with large-scale data in the \"increasing dimension\" regime. We\nprovide a comprehensive and in-depth analysis of a convolution-type smoothing\napproach that achieves adequate approximation to computation and inference for\nquantile regression. This method, which we refer to as {\\it{conquer}}, turns\nthe non-differentiable quantile loss function into a twice-differentiable,\nconvex and locally strongly convex surrogate, which admits a fast and scalable\nBarzilai-Borwein gradient-based algorithm to perform optimization, and\nmultiplier bootstrap for statistical inference. Theoretically, we establish\nexplicit non-asymptotic bounds on both estimation and Bahadur-Kiefer\nlinearization errors, from which we show that the asymptotic normality of the\nconquer estimator holds under a weaker requirement on the number of the\nregressors than needed for conventional quantile regression. Moreover, we prove\nthe validity of multiplier bootstrap confidence constructions. Our numerical\nstudies confirm the conquer estimator as a practical and reliable approach to\nlarge-scale inference for quantile regression. Software implementing the\nmethodology is available in the \\texttt{R} package \\texttt{conquer}.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:27:20 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 00:45:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["He", "Xuming", ""], ["Pan", "Xiaoou", ""], ["Tan", "Kean Ming", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "2012.05285", "submitter": "Fernando Castro-Prado", "authors": "Fernando Castro-Prado, Javier Costas, Wenceslao Gonz\\'alez-Manteiga,\n  David R. Penas", "title": "Searching for genetic interactions in complex disease by using distance\n  correlation", "comments": "24 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.GN stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding epistasis (genetic interaction) may shed some light on the\ngenomic basis of common diseases, including disorders of maximum interest due\nto their high socioeconomic burden, like schizophrenia. Distance correlation is\nan association measure that characterises general statistical independence\nbetween random variables, not only the linear one. Here, we propose distance\ncorrelation as a novel tool for the detection of epistasis from case-control\ndata of single nucleotide polymorphisms (SNPs). This approach will be developed\nboth theoretically (mathematical statistics, in a context of high-dimensional\nstatistical inference) and from an applied point of view (simulations and real\ndatasets).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:50:54 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Castro-Prado", "Fernando", ""], ["Costas", "Javier", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Penas", "David R.", ""]]}, {"id": "2012.05299", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Ashwin Pananjady, Martin J. Wainwright", "title": "Optimal oracle inequalities for solving projected fixed-point equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear fixed point equations in Hilbert spaces arise in a variety of\nsettings, including reinforcement learning, and computational methods for\nsolving differential and integral equations. We study methods that use a\ncollection of random observations to compute approximate solutions by searching\nover a known low-dimensional subspace of the Hilbert space. First, we prove an\ninstance-dependent upper bound on the mean-squared error for a linear\nstochastic approximation scheme that exploits Polyak--Ruppert averaging. This\nbound consists of two terms: an approximation error term with an\ninstance-dependent approximation factor, and a statistical error term that\ncaptures the instance-specific complexity of the noise when projected onto the\nlow-dimensional subspace. Using information theoretic methods, we also\nestablish lower bounds showing that both of these terms cannot be improved,\nagain in an instance-dependent sense. A concrete consequence of our\ncharacterization is that the optimal approximation factor in this problem can\nbe much larger than a universal constant. We show how our results precisely\ncharacterize the error of a class of temporal difference learning methods for\nthe policy evaluation problem with linear function approximation, establishing\ntheir optimality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:19:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mou", "Wenlong", ""], ["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2012.05351", "submitter": "Maikol Sol\\'is", "authors": "Alberto J Hern\\'andez and Maikol Sol\\'is and Ronald A.\n  Z\\'u\\~niga-Rojas", "title": "Estimation of first-order sensitivity indices based on symmetric\n  reflected Vietoris-Rips complexes areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we estimate the first-order sensitivity index of random\nvariables within a model by reconstructing the embedding manifold of a\ntwo-dimensional cloud point. The model assumed has p predictors and a\ncontinuous outcome Y . Our method gauges the manifold through a Vietoris-Rips\ncomplex with a fixed radius for each variable. With this object, and using the\narea and its symmetric reflection, we can estimate an index of relevance for\neach predictor. The index reveals the geometric nature of the data points.\nAlso, given the method used, we can decide whether a pair of non-correlated\nrandom variables have some structural pattern in their interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:45:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hern\u00e1ndez", "Alberto J", ""], ["Sol\u00eds", "Maikol", ""], ["Z\u00fa\u00f1iga-Rojas", "Ronald A.", ""]]}, {"id": "2012.05432", "submitter": "Xin Li", "authors": "Xin Li and Dongya Wu", "title": "Low-rank matrix estimation in multi-response regression with measurement\n  errors: Statistical and computational guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the matrix estimation problem in the\nmulti-response regression model with measurement errors. A nonconvex\nerror-corrected estimator based on a combination of the amended loss function\nand the nuclear norm regularizer is proposed to estimate the matrix parameter.\nThen under the (near) low-rank assumption, we analyse statistical and\ncomputational theoretical properties of global solutions of the nonconvex\nregularized estimator from a general point of view. In the statistical aspect,\nwe establish the nonasymptotic recovery bound for any global solution of the\nnonconvex estimator, under restricted strong convexity on the loss function. In\nthe computational aspect, we solve the nonconvex optimization problem via the\nproximal gradient method. The algorithm is proved to converge to a near-global\nsolution and achieve a linear convergence rate. In addition, we also verify\nsufficient conditions for the general results to be held, in order to obtain\nprobabilistic consequences for specific types of measurement errors, including\nthe additive noise and missing data. Finally, theoretical consequences are\ndemonstrated by several numerical experiments on corrupted errors-in-variables\nmulti-response regression models. Simulation results reveal excellent\nconsistency with our theory under high-dimensional scaling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:03:20 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:27:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Xin", ""], ["Wu", "Dongya", ""]]}, {"id": "2012.05450", "submitter": "Abderrazek Karoui", "authors": "Asma Ben Saber and Abderrazek Karoui", "title": "Spectral analysis of some random matrices based schemes for stable and\n  robust nonparametric and functional regression estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the first part of this work, we develop and study a random pseudo-inverse\nbased scheme for a stable and robust solution of nonparametric regression\nproblems. For the interval $I=[-1,1],$ we use a random projection over a\nspecial orthonormal family of a weighted $L^2(I)-$space, given by the Jacobi\nnormalized polynomials. Then, the pseudo-inverse of this random matrix is used\nto compute the different expansion coefficients of the nonparametric regression\nestimator with respect to this orthonormal system. We show that this estimator\nis stable. Then, we combine the RANdom SAmpling Consensus (RANSAC) iterative\nalgorithm with the previous scheme to get a robust and stable nonparametric\nregression estimator. This estimator has also the advantage to provide fairly\naccurate approximations to the true regression functions. In the second part of\nthis work, we extend the random pseudo-inverse scheme technique to build a\nstable and accurate estimator for solving linear functional regression (LFR)\nproblems. A dyadic decomposition approach is used to construct this last stable\nestimator for the LFR problem. The performance of the two proposed estimators\nare illustrated by various numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 04:32:02 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Saber", "Asma Ben", ""], ["Karoui", "Abderrazek", ""]]}, {"id": "2012.05465", "submitter": "Hongxiang Qiu", "authors": "Hongxiang Qiu, Alex Luedtke", "title": "Leveraging vague prior information in general models via iteratively\n  constructed Gamma-minimax estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gamma-minimax estimation is an approach to incorporate prior information into\nan estimation procedure when it is implausible to specify one particular prior\ndistribution. In this approach, we aim for an estimator that minimizes the\nworst-case Bayes risk over a set $\\Gamma$ of prior distributions.\nTraditionally, Gamma-minimax estimation is defined for parametric models. In\nthis paper, we define Gamma-minimaxity for general models and propose iterative\nalgorithms with convergence guarantees to compute Gamma-minimax estimators for\na general model space and a set of prior distributions constrained by\ngeneralized moments. We also propose encoding the space of candidate estimators\nby neural networks to enable flexible estimation. We illustrate our method in\ntwo settings, namely entropy estimation and a problem that arises in\nbiodiversity studies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:39:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Qiu", "Hongxiang", ""], ["Luedtke", "Alex", ""]]}, {"id": "2012.05601", "submitter": "Artur O. Lopes", "authors": "Artur O. Lopes, Silvia R. C. Lopes and Paulo Varandas", "title": "Bayes posterior convergence for loss functions via almost additive\n  Thermodynamic Formalism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical inference can be seen as information processing involving input\ninformation and output information that updates belief about some unknown\nparameters. We consider the Bayesian framework for making inferences about\ndynamical systems from ergodic observations, where the Bayesian procedure is\nbased on the Gibbs posterior inference, a decision process generalization of\nstandard Bayesian inference where the likelihood is replaced by the exponential\nof a loss function. In the case of direct observation and almost-additive loss\nfunctions, we prove an exponential convergence of the a posteriori measures a\nlimit measure. Our estimates on the Bayes posterior convergence for direct\nobservation are related but complementary to those in a recent paper by K.\nMcGoff, S. Mukherjee and A. Nobel. Our approach makes use of non-additive\nthermodynamic formalism and large deviation properties instead of joinings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:42:46 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lopes", "Artur O.", ""], ["Lopes", "Silvia R. C.", ""], ["Varandas", "Paulo", ""]]}, {"id": "2012.05640", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat and Ioana Gavra", "title": "Asymptotic study of stochastic adaptive algorithm in non-convex\n  landscape", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies some asymptotic properties of adaptive algorithms widely\nused in optimization and machine learning, and among them Adagrad and Rmsprop,\nwhich are involved in most of the blackbox deep learning algorithms. Our setup\nis the non-convex landscape optimization point of view, we consider a one time\nscale parametrization and we consider the situation where these algorithms may\nbe used or not with mini-batches. We adopt the point of view of stochastic\nalgorithms and establish the almost sure convergence of these methods when\nusing a decreasing step-size point of view towards the set of critical points\nof the target function. With a mild extra assumption on the noise, we also\nobtain the convergence towards the set of minimizer of the function. Along our\nstudy, we also obtain a \"convergence rate\" of the methods, in the vein of the\nworks of \\cite{GhadimiLan}.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:54:45 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 10:42:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Gavra", "Ioana", ""]]}, {"id": "2012.05784", "submitter": "Nabarun Deb", "authors": "Nabarun Deb, Rajarshi Mukherjee, Sumit Mukherjee, and Ming Yuan", "title": "Detecting Structured Signals in Ising Models", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effect of dependence on detecting a class of\nsignals in Ising models, where the signals are present in a structured way.\nExamples include Ising Models on lattices, and Mean-Field type Ising Models\n(Erd\\H{o}s-R\\'{e}nyi, Random regular, and dense graphs). Our results rely on\ncorrelation decay and mixing type behavior for Ising Models, and demonstrate\nthe beneficial behavior of criticality in the detection of strictly lower\nsignals. As a by-product of our proof technique, we develop sharp control on\nmixing and spin-spin correlation for several Mean-Field type Ising Models in\nall regimes of temperature -- which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:20:39 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Deb", "Nabarun", ""], ["Mukherjee", "Rajarshi", ""], ["Mukherjee", "Sumit", ""], ["Yuan", "Ming", ""]]}, {"id": "2012.05949", "submitter": "David Azriel", "authors": "David Azriel and Yosef Rinott", "title": "Optimal selection of a common subset of covariates for different\n  regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a regression dataset of size $n$, most of the classical model selection\nliterature treats the problem of selecting a subset of covariates to be used\nfor prediction of future responses. In this paper we assume that a sample of\n$\\cal J$ regression datasets of different sizes from a population of datasets\nhaving the same set of covariates and responses is observed. The goal is to\nselect, for each $n$, a single subset of covariates to be used for prediction\nof future responses for any dataset of size $n$ from the population (which may\nor may not be in the sample of datasets). The regression coefficients used in\nthe prediction are estimated using the $n$ observations consisting of\ncovariates and responses in the sample for which prediction of future responses\nis to be done, and thus they differ across different samples. For example, if\nthe response is a diagnosis, and the covariates are medical background\nvariables and measurements, the goal is to select a standard set of\nmeasurements for different clinics, say, where each clinic may estimate and use\nits own coefficients for prediction (depending on local conditions, prevalence,\netc.). The selected subset naturally depends on the sample size $n$, with a\nlarger sample size allowing a more elaborate model. Since we consider\nprediction for any (or a randomly chosen) dataset in the population, it is\nnatural to consider random covariates. If the population consists of datasets\nthat are similar, our approach amounts to borrowing information, leading to a\nsubset selection that is efficient for prediction. On the other hand, if the\ndatasets are dissimilar, then our goal is to find a \"compromise\" subset of\ncovariates for the different regressions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:11:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Azriel", "David", ""], ["Rinott", "Yosef", ""]]}, {"id": "2012.06019", "submitter": "Matthew Roughan", "authors": "Matthew Roughan", "title": "A new non-negative distribution with both finite and infinite support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Tukey-$\\lambda$ distribution has interesting properties including (i) for\nsome parameters values it has finite support, and for others infinite support,\nand (ii) it can mimic several other distributions such that parameter\nestimation for the Tukey distribution is a method for identifying an\nappropriate class of distribution to model a set of data. The Tukey-$\\lambda$\nis, however, symmetric. Here we define a new class of {\\em non-negative}\ndistribution with similar properties to the Tukey-$\\lambda$ distribution. As\nwith the Tukey-$\\lambda$ distribution, our distribution is defined in terms of\nits quantile function, which in this case is given by the polylogarithm\nfunction. We show the support of the distribution to be the Riemann zeta\nfunction (when finite), and we provide a closed form for the expectation,\nprovide simple means to calculate the CDF and PDF, and show that it has\nrelationships to the uniform, exponential, inverse beta and extreme-value\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:18:42 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Roughan", "Matthew", ""]]}, {"id": "2012.06094", "submitter": "Jian Huang", "authors": "Yuan Gao, Jian Huang, Yuling Jiao, Jin Liu, Xiliang Lu and Zhijian\n  Yang", "title": "Generative Learning With Euler Particle Transport", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.02862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an Euler particle transport (EPT) approach for generative\nlearning. The proposed approach is motivated by the problem of finding an\noptimal transport map from a reference distribution to a target distribution\ncharacterized by the Monge-Ampere equation. Interpreting the infinitesimal\nlinearization of the Monge-Ampere equation from the perspective of gradient\nflows in measure spaces leads to a stochastic McKean-Vlasov equation. We use\nthe forward Euler method to solve this equation. The resulting forward Euler\nmap pushes forward a reference distribution to the target. This map is the\ncomposition of a sequence of simple residual maps, which are computationally\nstable and easy to train. The key task in training is the estimation of the\ndensity ratios or differences that determine the residual maps. We estimate the\ndensity ratios (differences) based on the Bregman divergence with a gradient\npenalty using deep density-ratio (difference) fitting. We show that the\nproposed density-ratio (difference) estimators do not suffer from the \"curse of\ndimensionality\" if data is supported on a lower-dimensional manifold. Numerical\nexperiments with multi-mode synthetic datasets and comparisons with the\nexisting methods on real benchmark datasets support our theoretical results and\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 03:10:53 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Gao", "Yuan", ""], ["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Liu", "Jin", ""], ["Lu", "Xiliang", ""], ["Yang", "Zhijian", ""]]}, {"id": "2012.06270", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Handy Formulas for Binomial Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the relevance of the binomial distribution for probability theory and\napplied statistical inference, its higher-order moments are poorly understood.\nThe existing formulas are either not general enough, or not structured or\nsimplified enough for intended applications. This paper introduces novel\nformulas for binomial moments, in terms of \\emph{variance} rather than success\nprobability. The obtained formulas are arguably better structured and simpler\ncompared to prior works. In addition, the paper presents algorithms to derive\nthese formulas along with working implementation in the Python symbolic algebra\npackage. The novel approach is a combinatorial argument coupled with clever\nalgebraic simplifications which rely on symmetrization theory. As an\ninteresting byproduct we establish \\emph{asymptotically sharp estimates for\ncentral binomial moments}, improving upon partial results from prior works.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:04:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2012.06487", "submitter": "Tau Rasethuntsa Mr", "authors": "Tau Raphael Rasethuntsa", "title": "On the UMVUE and Closed-Form Bayes Estimator for $Pr(X<Y<Z)$ and its\n  Generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the parametric estimation of $Pr(X<Y<Z)$ and its\ngeneralizations based on several well-known one-parameter and two-parameter\ncontinuous distributions. It is shown that for some one-parameter distributions\nand when there is a common known parameter in some two-parameter distributions,\nthe uniformly minimum variance unbiased estimator can be expressed as a linear\ncombination of the Appell hypergeometric function of the first type, $F_{1}$\nand the hypergeometric functions $_{2}F_{1}$ and $_{3}F_{2}.$ The Bayes\nestimator based on conjugate gamma priors and Jefferys' non-informative priors\nunder the squared error loss function is also given as a linear combination of\n$_{2}F_{1}$ and $F_{1}.$ Alternatively, a convergent infinite series form of\nthe Bayes estimator involving the $F_{1}$ function is also proposed. In model\ngeneralizations and extensions, it is further shown that the UMVUE can be\nexpressed as a linear combination of a Lauricella series, $F_{D}^{(n)},$ and\nthe generalized hypergeometric function, $_{p}F_{q},$ which are generalizations\nof $F_{1}$ and $_{2}F_{1}$ respectively. The generalized closed-form Bayes\nestimator is also given as a convergent infinite series involving\n$F_{D}^{(n)}.$ To gauge the performances of the UMVUE and the closed-form Bayes\nestimator for $P$ against other well-known estimators, maximum likelihood\nestimates, Lindley approximation estimates and Markov Chain Monte Carlo\nestimates for $P$ are also computed. Additionally, asymptotic confidence\nintervals and Bayesian highest probability density credible intervals are also\nconstructed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:53:20 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Rasethuntsa", "Tau Raphael", ""]]}, {"id": "2012.06603", "submitter": "Remo Kretschmann", "authors": "Tapio Helin, Remo Kretschmann", "title": "Non-asymptotic error estimates for the Laplace approximation in Bayesian\n  inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study properties of the Laplace approximation of the\nposterior distribution arising in nonlinear Bayesian inverse problems. Our work\nis motivated by Schillings et al. (2020), where it is shown that in such a\nsetting the Laplace approximation error in Hellinger distance converges to zero\nin the order of the noise level. Here, we prove novel error estimates for a\ngiven noise level that also quantify the effect due to the nonlinearity of the\nforward mapping and the dimension of the problem. In particular, we are\ninterested in inverse problems, where a linear forward mapping is perturbed by\na small nonlinear mapping. Our results indicate that in this case, the Laplace\napproximation error is of the size of the perturbation. The paper provides\ninsight into Bayesian inference in nonlinear inverse problems, where\nlinearization of the forward mapping has suitable approximation properties.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 19:15:15 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 19:01:30 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Helin", "Tapio", ""], ["Kretschmann", "Remo", ""]]}, {"id": "2012.06750", "submitter": "Philip Thompson", "authors": "Philip Thompson", "title": "Outlier-robust sparse/low-rank least-squares regression and robust\n  matrix completion", "comments": "Correction of typos; addition of simulation results; addition of new\n  oracle inequalities for Lasso and Slope in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study high-dimensional least-squares regression within a subgaussian\nstatistical learning framework with heterogeneous noise. It includes $s$-sparse\nand $r$-low-rank least-squares regression when a fraction $\\epsilon$ of the\nlabels are adversarially contaminated. We also present a novel theory of\ntrace-regression with matrix decomposition based on a new application of the\nproduct process. For these problems, we show novel near-optimal \"subgaussian\"\nestimation rates of the form\n$r(n,d_{e})+\\sqrt{\\log(1/\\delta)/n}+\\epsilon\\log(1/\\epsilon)$, valid with\nprobability at least $1-\\delta$. Here, $r(n,d_{e})$ is the optimal\nuncontaminated rate as a function of the effective dimension $d_{e}$ but\nindependent of the failure probability $\\delta$. These rates are valid\nuniformly on $\\delta$, i.e., the estimators' tuning do not depend on $\\delta$.\nLastly, we consider noisy robust matrix completion with non-uniform sampling.\nIf only the low-rank matrix is of interest, we present a novel near-optimal\nrate that is independent of the corruption level $a$. Our estimators are\ntractable and based on a new \"sorted\" Huber-type loss. No information on\n$(s,r,\\epsilon,a,\\delta)$ are needed to tune these estimators. Our analysis\nmakes use of novel $\\delta$-optimal concentration inequalities for the\nmultiplier and product processes which could be useful elsewhere. For instance,\nthey imply novel sharp oracle inequalities for Lasso and Slope with optimal\ndependence on $\\delta$. Numerical simulations confirm our theoretical\npredictions. In particular, \"sorted\" Huber regression can outperform classical\nHuber regression.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 07:42:47 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:02:32 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Thompson", "Philip", ""]]}, {"id": "2012.06846", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Dario Azzimonti and Dario Piga", "title": "A unified framework for closed-form nonparametric regression,\n  classification, preference and mixed problems with Skew Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal\ndistributions over finite dimensional vectors to distribution over functions.\nSkewGPs are more general and flexible than Gaussian processes, as SkewGPs may\nalso represent asymmetric distributions. In a recent contribution we showed\nthat SkewGP and probit likelihood are conjugate, which allows us to compute the\nexact posterior for non-parametric binary classification and preference\nlearning. In this paper, we generalize previous results and we prove that\nSkewGP is conjugate with both the normal and affine probit likelihood, and more\nin general, with their product. This allows us to (i) handle classification,\npreference, numeric and ordinal regression, and mixed problems in a unified\nframework; (ii) derive closed-form expression for the corresponding posterior\ndistributions. We show empirically that the proposed framework based on SkewGP\nprovides better performance than Gaussian processes in active learning and\nBayesian (constrained) optimization. These two tasks are fundamental for design\nof experiments and in Data Science.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 15:58:16 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 10:47:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Benavoli", "Alessio", ""], ["Azzimonti", "Dario", ""], ["Piga", "Dario", ""]]}, {"id": "2012.06958", "submitter": "Justin Solomon", "authors": "Justin Solomon, Kristjan Greenewald, Haikady N. Nagaraja", "title": "$k$-Variance: A Clustered Notion of Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $k$-variance, a generalization of variance built on the\nmachinery of random bipartite matchings. $K$-variance measures the expected\ncost of matching two sets of $k$ samples from a distribution to each other,\ncapturing local rather than global information about a measure as $k$\nincreases; it is easily approximated stochastically using sampling and linear\nprogramming. In addition to defining $k$-variance and proving its basic\nproperties, we provide in-depth analysis of this quantity in several key cases,\nincluding one-dimensional measures, clustered measures, and measures\nconcentrated on low-dimensional subsets of $\\mathbb R^n$. We conclude with\nexperiments and open problems motivated by this new way to summarize\ndistributional shape.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 04:25:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Solomon", "Justin", ""], ["Greenewald", "Kristjan", ""], ["Nagaraja", "Haikady N.", ""]]}, {"id": "2012.07092", "submitter": "Pengfei Li", "authors": "Meng Yuan, Chunlin Wang, Boxi Lin, and Pengfei Li", "title": "Semiparametric inference on general functionals of two semicontinuous\n  populations", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new semiparametric procedures for making inference\non linear functionals and their functions of two semicontinuous populations.\nThe distribution of each population is usually characterized by a mixture of a\ndiscrete point mass at zero and a continuous skewed positive component, and\nhence such distribution is semicontinuous in the nature. To utilize the\ninformation from both populations, we model the positive components of the two\nmixture distributions via a semiparametric density ratio model. Under this\nmodel setup, we construct the maximum empirical likelihood estimators of the\nlinear functionals and their functions, and establish the asymptotic normality\nof the proposed estimators. We show the proposed estimators of the linear\nfunctionals are more efficient than the fully nonparametric ones. The developed\nasymptotic results enable us to construct confidence regions and perform\nhypothesis tests for the linear functionals and their functions. We further\napply these results to several important summary quantities such as the\nmoments, the mean ratio, the coefficient of variation, and the generalized\nentropy class of inequality measures. Simulation studies demonstrate the\nadvantages of our proposed semiparametric method over some existing methods.\nTwo real data examples are provided for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 16:24:34 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:53:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yuan", "Meng", ""], ["Wang", "Chunlin", ""], ["Lin", "Boxi", ""], ["Li", "Pengfei", ""]]}, {"id": "2012.07133", "submitter": "Prabrisha Rakshit", "authors": "Zijian Guo, Prabrisha Rakshit, Daniel S. Herman and Jinbo Chen", "title": "Inference for the Case Probability in High-dimensional Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling patients in electronic health records with respect to their statuses\nof having a disease or condition, i.e. case or control statuses, has\nincreasingly relied on prediction models using high-dimensional variables\nderived from structured and unstructured electronic health record data. A major\nhurdle currently is a lack of valid statistical inference methods for the case\nprobability. In this paper, considering high-dimensional sparse logistic\nregression models for prediction, we propose a novel bias-corrected estimator\nfor the case probability through the development of linearization and variance\nenhancement techniques. We establish asymptotic normality of the proposed\nestimator for any loading vector in high dimensions. We construct a confidence\ninterval for the case probability and propose a hypothesis testing procedure\nfor patient case-control labelling. We demonstrate the proposed method via\nextensive simulation studies and application to real-world electronic health\nrecord data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 19:58:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 04:08:39 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guo", "Zijian", ""], ["Rakshit", "Prabrisha", ""], ["Herman", "Daniel S.", ""], ["Chen", "Jinbo", ""]]}, {"id": "2012.07167", "submitter": "Jonathan Stewart", "authors": "Jonathan R. Stewart and Michael Schweinberger", "title": "Pseudo-likelihood-based $M$-estimation of random graphs with dependent\n  edges and parameter vectors of increasing dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question in statistical network analysis is how to estimate\nmodels of dependent network data without sacrificing computational scalability\nand statistical guarantees. We demonstrate that scalable estimation of random\ngraph models with dependent edges is possible, by establishing the first\nconsistency results and convergence rates for pseudo-likelihood-based\n$M$-estimators for parameter vectors of increasing dimension based on a single\nobservation of dependent random variables. The main results cover models of\ndependent random variables satisfying weak dependence conditions, and may be of\nindependent interest. To showcase consistency results and convergence rates, we\nintroduce a novel class of generalized $\\beta$-models with dependent edges and\nparameter vectors of increasing dimension. We establish consistency results and\nconvergence rates for pseudo-likelihood-based $M$-estimators of generalized\n$\\beta$-models with dependent edges, in dense- and sparse-graph settings.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 22:07:32 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:46:02 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 00:18:33 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Stewart", "Jonathan R.", ""], ["Schweinberger", "Michael", ""]]}, {"id": "2012.07256", "submitter": "Nicolas Privault", "authors": "Nicolas Privault", "title": "Recursive computation of the Hawkes cumulants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recursive method for the computation of the cumulants of\nself-exciting point processes of Hawkes type, based on standard combinatorial\ntools such as Bell polynomials. This closed-form approach is easier to\nimplement on higher-order cumulants in comparison with existing methods based\non differential equations, tree enumeration or martingale arguments. The\nresults are corroborated by Monte Carlo simulations, and also apply to the\ncomputation of joint cumulants generated by multidimensional self-exciting\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:09:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Privault", "Nicolas", ""]]}, {"id": "2012.07385", "submitter": "Mehdi Dagdoug", "authors": "Mehdi Dagdoug, Camelia Goga, David Haziza", "title": "Model-assisted estimation in high-dimensional settings for survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-assisted estimators have attracted a lot of attention in the last three\ndecades. These estimators attempt to make an efficient use of auxiliary\ninformation available at the estimation stage. A working model linking the\nsurvey variable to the auxiliary variables is specified and fitted on the\nsample data to obtain a set of predictions, which are then incorporated in the\nestimation procedures. A nice feature of model-assisted procedures is that they\nmaintain important design properties such as consistency and asymptotic\nunbiasedness irrespective of whether or not the working model is correctly\nspecified. In this article, we examine several model-assisted estimators from a\ndesign-based point of view and in a high-dimensional setting, including\npenalized estimators and tree-based estimators. We conduct an extensive\nsimulation study using data from the Irish Commission for Energy Regulation\nSmart Metering Project, in order to assess the performance of several\nmodel-assisted estimators in terms of bias and efficiency in this\nhigh-dimensional data set.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:13:58 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:50:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Dagdoug", "Mehdi", ""], ["Goga", "Camelia", ""], ["Haziza", "David", ""]]}, {"id": "2012.07429", "submitter": "David Rossell", "authors": "David Rossell, Oriol Abril, Anirban Bhattacharya", "title": "Approximate Laplace approximations for scalable model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the approximate Laplace approximation (ALA) to evaluate integrated\nlikelihoods, a bottleneck in Bayesian model selection. The Laplace\napproximation (LA) is a popular tool that speeds up such computation and equips\nstrong model selection properties. However, when the sample size is large or\none considers many models the cost of the required optimizations becomes\nimpractical. ALA reduces the cost to that of solving a least-squares problem\nfor each model. Further, it enables efficient computation across models such as\nsharing pre-computed sufficient statistics and certain operations in matrix\ndecompositions. We prove that in generalized (possibly non-linear) models ALA\nachieves a strong form of model selection consistency for a suitably-defined\noptimal model, at the same functional rates as exact computation. We consider\nfixed- and high-dimensional problems, group and hierarchical constraints, and\nthe possibility that all models are misspecified. We also obtain ALA rates for\nGaussian regression under non-local priors, an important example where the LA\ncan be costly and does not consistently estimate the integrated likelihood. Our\nexamples include non-linear regression, logistic, Poisson and survival models.\nWe implement the methodology in the R package mombf.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:29:11 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:06:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rossell", "David", ""], ["Abril", "Oriol", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2012.07774", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Small Covers for Near-Zero Sets of Polynomials and Learning Latent\n  Variable Models", "comments": "Full version of FOCS'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V$ be any vector space of multivariate degree-$d$ homogeneous\npolynomials with co-dimension at most $k$, and $S$ be the set of points where\nall polynomials in $V$ {\\em nearly} vanish. We establish a qualitatively\noptimal upper bound on the size of $\\epsilon$-covers for $S$, in the\n$\\ell_2$-norm. Roughly speaking, we show that there exists an $\\epsilon$-cover\nfor $S$ of cardinality $M = (k/\\epsilon)^{O_d(k^{1/d})}$. Our result is\nconstructive yielding an algorithm to compute such an $\\epsilon$-cover that\nruns in time $\\mathrm{poly}(M)$.\n  Building on our structural result, we obtain significantly improved learning\nalgorithms for several fundamental high-dimensional probabilistic models with\nhidden variables. These include density and parameter estimation for\n$k$-mixtures of spherical Gaussians (with known common covariance), PAC\nlearning one-hidden-layer ReLU networks with $k$ hidden units (under the\nGaussian distribution), density and parameter estimation for $k$-mixtures of\nlinear regressions (with Gaussian covariates), and parameter estimation for\n$k$-mixtures of hyperplanes. Our algorithms run in time {\\em quasi-polynomial}\nin the parameter $k$. Previous algorithms for these problems had running times\nexponential in $k^{\\Omega(1)}$.\n  At a high-level our algorithms for all these learning problems work as\nfollows: By computing the low-degree moments of the hidden parameters, we are\nable to find a vector space of polynomials that nearly vanish on the unknown\nparameters. Our structural result allows us to compute a quasi-polynomial sized\ncover for the set of hidden parameters, which we exploit in our learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:14:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.07793", "submitter": "Philipp Reichenbach", "authors": "Carlos Am\\'endola, Kathl\\'en Kohn, Philipp Reichenbach, Anna Seigal", "title": "Toric invariant theory for maximum likelihood estimation in log-linear\n  models", "comments": "This is a companion paper to arXiv:2003.13662", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish connections between invariant theory and maximum likelihood\nestimation for discrete statistical models. We show that norm minimization over\na torus orbit is equivalent to maximum likelihood estimation in log-linear\nmodels. We use notions of stability under a torus action to characterize the\nexistence of the maximum likelihood estimate, and discuss connections to\nscaling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:27:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Kohn", "Kathl\u00e9n", ""], ["Reichenbach", "Philipp", ""], ["Seigal", "Anna", ""]]}, {"id": "2012.07937", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Lin Zheng", "title": "Template Matching with Ranks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matching a template to a noisy signal. Motivated\nby some recent proposals in the signal processing literature, we suggest a\nrank-based method and study its asymptotic properties using some\nwell-established techniques in empirical process theory combined with H\\'ajek's\nprojection method. The resulting estimator of the shift is shown to achieve a\nparametric rate of convergence and to be asymptotically normal. Some numerical\nsimulations corroborate these findings.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 20:55:20 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Zheng", "Lin", ""]]}, {"id": "2012.08223", "submitter": "Sayar Karmakar", "authors": "Sayar Karmakar, Marek Chudy and Wei Biao Wu", "title": "Long-term prediction intervals with many covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate forecasting is one of the fundamental focus in the literature of\neconometric time-series. Often practitioners and policy makers want to predict\noutcomes of an entire time horizon in the future instead of just a single\n$k$-step ahead prediction. These series, apart from their own possible\nnon-linear dependence, are often also influenced by many external predictors.\nIn this paper, we construct prediction intervals of time-aggregated forecasts\nin a high-dimensional regression setting. Our approach is based on quantiles of\nresiduals obtained by the popular LASSO routine. We allow for general\nheavy-tailed, long-memory, and nonlinear stationary error process and\nstochastic predictors. Through a series of systematically arranged consistency\nresults we provide theoretical guarantees of our proposed quantile-based method\nin all of these scenarios. After validating our approach using simulations we\nalso propose a novel bootstrap based method that can boost the coverage of the\ntheoretical intervals. Finally analyzing the EPEX Spot data, we construct\nprediction intervals for hourly electricity prices over horizons spanning 17\nweeks and contrast them to selected Bayesian and bootstrap interval forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:26:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Karmakar", "Sayar", ""], ["Chudy", "Marek", ""], ["Wu", "Wei Biao", ""]]}, {"id": "2012.08244", "submitter": "Nikita Puchkin", "authors": "Nikita Puchkin, Aleksandr Timofeev, and Vladimir Spokoiny", "title": "Manifold-based time series forecasting", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction for high dimensional time series is a challenging task due to the\ncurse of dimensionality problem. Classical parametric models like ARIMA or VAR\nrequire strong modeling assumptions and time stationarity and are often\noverparametrized. This paper offers a new flexible approach using recent ideas\nof manifold learning. The considered model includes linear models such as the\ncentral subspace model and ARIMA as particular cases. The proposed procedure\ncombines manifold denoising techniques with a simple nonparametric prediction\nby local averaging. The resulting procedure demonstrates a very reasonable\nperformance for real-life econometric time series. We also provide a\ntheoretical justification of the manifold estimation procedure.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:23:30 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Puchkin", "Nikita", ""], ["Timofeev", "Aleksandr", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "2012.08257", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "Ordering results of extreme order statistics from multiple-outlier scale\n  models with dependence", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on stochastic comparisons of extreme order statistics\nstemming from multiple-outlier scale models with dependence. Archimedean copula\nis used to model dependence structure among nonnegative random variables.\nSufficient conditions are obtained for comparison of the largest order\nstatistics in the sense of the usual stochastic, reversed hazard rate, star and\nLorenz orders. The smallest order statistics are also compared with respect to\nthe usual stochastic, hazard rate, star and Lorenz orders. To illustrate the\ntheoretical establishments, some examples are provided.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:47:29 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2012.08294", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Mehmet Niyazi \\c{C}ankaya, Roberto Vila", "title": "Maximum $\\log_q$ Likelihood Estimation for Parameters of Weibull\n  Distribution and Properties: Monte Carlo Simulation", "comments": "36 pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The maximum ${\\log}_q$ likelihood estimation method is a generalization of\nthe known maximum $\\log$ likelihood method to overcome the problem for modeling\nnon-identical observations (inliers and outliers). The parameter $q$ is a\ntuning constant to manage the modeling capability. Weibull is a flexible and\npopular distribution for problems in engineering. In this study, this method is\nused to estimate the parameters of Weibull distribution when non-identical\nobservations exist. Since the main idea is based on modeling capability of\nobjective function\n$\\rho(x;\\boldsymbol{\\theta})=\\log_q\\big[f(x;\\boldsymbol{\\theta})\\big]$, we\nobserve that the finiteness of score functions cannot play a role in the robust\nestimation for inliers. The properties of Weibull distribution are examined. In\nthe numerical experiment, the parameters of Weibull distribution are estimated\nby $\\log_q$ and its special form, $\\log$, likelihood methods if the different\ndesigns of contamination into underlying Weibull distribution are applied. The\noptimization is performed via genetic algorithm. The modeling competence of\n$\\rho(x;\\boldsymbol{\\theta})$ and insensitiveness to non-identical observations\nare observed by Monte Carlo simulation. The value of $q$ can be chosen by use\nof the mean squared error in simulation and the $p$-value of Kolmogorov-Smirnov\ntest statistic used for evaluation of fitting competence. Thus, we can overcome\nthe problem about determining of the value of $q$ for real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:01:05 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["\u00c7ankaya", "Mehmet Niyazi", ""], ["Vila", "Roberto", ""]]}, {"id": "2012.08324", "submitter": "Christopher Strothmann", "authors": "Karl Friedrich Siburg and Christopher Strothmann", "title": "Stochastic monotonicity and the Markov product for copulas", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmaa.2021.125348", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two random variables $X$ and $Y$, stochastic monotonicity describes a\nmonotone influence of $X$ on $Y$. We prove two different characterizations of\nstochastically monotone $2$-copulas using the isomorphism between $2$-copulas\nand Markov operators. The first approach establishes a one-to-one\ncorrespondence between stochastically monotone copulas and\nmonotonicity-preserving Markov operators. The second approach characterizes\nstochastically monotone copulas by their monotonicity property with respect to\nthe Markov product. Applying the latter result, we identify all idempotent\nstochastically monotone copulas as ordinal sums of the independence copula\n$\\Pi$.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:33:26 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 10:43:07 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Siburg", "Karl Friedrich", ""], ["Strothmann", "Christopher", ""]]}, {"id": "2012.08338", "submitter": "Shuya Nagayasu", "authors": "Shuya Nagayasu and Sumio Watanabe", "title": "Asymptotic Behavior of Free Energy When Optimal Probability Distribution\n  Is Not Unique", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian inference is a widely used statistical method. The free energy and\ngeneralization loss, which are used to estimate the accuracy of Bayesian\ninference, are known to be small in singular models that do not have a unique\noptimal parameter. However, their characteristics are not yet known when there\nare multiple optimal probability distributions. In this paper, we theoretically\nderive the asymptotic behaviors of the generalization loss and free energy in\nthe case that the optimal probability distributions are not unique and show\nthat they contain asymptotically different terms from those of the conventional\nasymptotic analysis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:47:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Nagayasu", "Shuya", ""], ["Watanabe", "Sumio", ""]]}, {"id": "2012.08371", "submitter": "Emma Jingfei Zhang", "authors": "Jianwei Hu, Jingfei Zhang, Ji Zhu and Jianhua Guo", "title": "Limiting laws and consistent estimation criteria for fixed and diverging\n  number of spiked eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study limiting laws and consistent estimation criteria for\nthe extreme eigenvalues in a spiked covariance model of dimension $p$. Firstly,\nfor fixed $p$, we propose a generalized estimation criterion that can\nconsistently estimate, $k$, the number of spiked eigenvalues. Compared with the\nexisting literature, we show that consistency can be achieved under weaker\nconditions on the penalty term. Next, allowing both $p$ and $k$ to diverge, we\nderive limiting distributions of the spiked sample eigenvalues using random\nmatrix theory techniques. Notably, our results do not require the spiked\neigenvalues to be uniformly bounded from above or tending to infinity, as have\nbeen assumed in the existing literature. Based on the above derived results, we\nformulate a generalized estimation criterion and show that it can consistently\nestimate $k$, while $k$ can be fixed or grow at an order of $k=o(n^{1/3})$. We\nfurther show that the results in our work continue to hold under a general\npopulation distribution without assuming normality. The efficacy of the\nproposed estimation criteria is illustrated through comparative simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:36:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Hu", "Jianwei", ""], ["Zhang", "Jingfei", ""], ["Zhu", "Ji", ""], ["Guo", "Jianhua", ""]]}, {"id": "2012.08391", "submitter": "Catherine Medlock", "authors": "Catherine Medlock and Alan Oppenheim", "title": "Optimal ROC Curves from Score Variable Threshold Tests", "comments": null, "journal-ref": "In 2019 IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP) (pp. 5327-5330). IEEE", "doi": "10.1109/ICASSP.2019.8683187", "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) is a well-established\nrepresentation of the tradeoff between detection and false alarm probabilities\nin binary hypothesis testing. In many practical contexts ROC's are generated by\nthresholding a measured score variable -- applying score variable threshold\ntests (SVT's). In many cases the resulting curve is different from the\nlikelihood ratio test (LRT) ROC and is therefore not Neyman-Pearson optimal.\nWhile it is well-understood that concavity is a necessary condition for an ROC\nto be Neyman-Pearson optimal, this paper establishes that it is also a\nsufficient condition in the case where the ROC was generated using SVT's. It\nfurther defines a constructive procedure by which the LRT ROC can be generated\nfrom a non-concave SVT ROC, without requiring explicit knowledge of the\nconditional PDF's of the score variable. If the conditional PDF's are known,\nthe procedure implicitly provides a way of redesigning the test so that it is\nequivalent to an LRT.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:08:24 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Medlock", "Catherine", ""], ["Oppenheim", "Alan", ""]]}, {"id": "2012.08443", "submitter": "Adrian Riekert", "authors": "Arnulf Jentzen and Adrian Riekert", "title": "Strong overall error analysis for the training of artificial neural\n  networks via random initializations", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning based approximation algorithms have been applied very\nsuccessfully to numerous problems, at the moment the reasons for their\nperformance are not entirely understood from a mathematical point of view.\nRecently, estimates for the convergence of the overall error have been obtained\nin the situation of deep supervised learning, but with an extremely slow rate\nof convergence. In this note we partially improve on these estimates. More\nspecifically, we show that the depth of the neural network only needs to\nincrease much slower in order to obtain the same rate of approximation. The\nresults hold in the case of an arbitrary stochastic optimization algorithm with\ni.i.d.\\ random initializations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:34:16 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Jentzen", "Arnulf", ""], ["Riekert", "Adrian", ""]]}, {"id": "2012.08444", "submitter": "Fengshi Niu", "authors": "Bryan S. Graham, Fengshi Niu, James L. Powell", "title": "Minimax Risk and Uniform Convergence Rates for Nonparametric Dyadic\n  Regression", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $i=1,\\ldots,N$ index a simple random sample of units drawn from some\nlarge population. For each unit we observe the vector of regressors $X_{i}$\nand, for each of the $N\\left(N-1\\right)$ ordered pairs of units, an outcome\n$Y_{ij}$. The outcomes $Y_{ij}$ and $Y_{kl}$ are independent if their indices\nare disjoint, but dependent otherwise (i.e., \"dyadically dependent\"). Let\n$W_{ij}=\\left(X_{i}',X_{j}'\\right)'$; using the sampled data we seek to\nconstruct a nonparametric estimate of the mean regression function\n$g\\left(W_{ij}\\right)\\overset{def}{\\equiv}\\mathbb{E}\\left[\\left.Y_{ij}\\right|X_{i},X_{j}\\right].$\n  We present two sets of results. First, we calculate lower bounds on the\nminimax risk for estimating the regression function at (i) a point and (ii)\nunder the infinity norm. Second, we calculate (i) pointwise and (ii) uniform\nconvergence rates for the dyadic analog of the familiar Nadaraya-Watson (NW)\nkernel regression estimator. We show that the NW kernel regression estimator\nachieves the optimal rates suggested by our risk bounds when an appropriate\nbandwidth sequence is chosen. This optimal rate differs from the one available\nunder iid data: the effective sample size is smaller and\n$d_W=\\mathrm{dim}(W_{ij})$ influences the rate differently.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:35:07 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 00:28:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Graham", "Bryan S.", ""], ["Niu", "Fengshi", ""], ["Powell", "James L.", ""]]}, {"id": "2012.08496", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma", "title": "Spectral Methods for Data Science: A Statistical Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral methods have emerged as a simple yet surprisingly effective approach\nfor extracting information from massive, noisy and incomplete data. In a\nnutshell, spectral methods refer to a collection of algorithms built upon the\neigenvalues (resp. singular values) and eigenvectors (resp. singular vectors)\nof some properly designed matrices constructed from data. A diverse array of\napplications have been found in machine learning, data science, and signal\nprocessing. Due to their simplicity and effectiveness, spectral methods are not\nonly used as a stand-alone estimator, but also frequently employed to\ninitialize other more sophisticated algorithms to improve performance.\n  While the studies of spectral methods can be traced back to classical matrix\nperturbation theory and methods of moments, the past decade has witnessed\ntremendous theoretical advances in demystifying their efficacy through the lens\nof statistical modeling, with the aid of non-asymptotic random matrix theory.\nThis monograph aims to present a systematic, comprehensive, yet accessible\nintroduction to spectral methods from a modern statistical perspective,\nhighlighting their algorithmic implications in diverse large-scale\napplications. In particular, our exposition gravitates around several central\nquestions that span various applications: how to characterize the sample\nefficiency of spectral methods in reaching a target level of statistical\naccuracy, and how to assess their stability in the face of random noise,\nmissing data, and adversarial corruptions? In addition to conventional $\\ell_2$\nperturbation analysis, we present a systematic $\\ell_{\\infty}$ and\n$\\ell_{2,\\infty}$ perturbation theory for eigenspace and singular subspaces,\nwhich has only recently become available owing to a powerful \"leave-one-out\"\nanalysis framework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:40:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""]]}, {"id": "2012.08498", "submitter": "George Yanev", "authors": "George P. Yanev", "title": "Exponential and Hypoexponential Distributions: Some Characterizations", "comments": null, "journal-ref": "Mathematics 2020, 8, 2207", "doi": "10.3390/math8122207", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The (general) hypoexponential distribution is the distribution of a sum of\nindependent exponential random variables. We consider the particular case when\nthe involved exponential variables have distinct rate parameters. We prove that\nthe following converse result is true. If for some $n\\ge 2$, $X_1,\nX_2,\\,\\ldots,\\,X_n$ are independent copies of a random variable $X$ with\nunknown distribution $F$ and a specific linear combination of $X_j$'s has\nhypoexponential distribution, then $F$ is exponential. Thus, we obtain new\ncharacterizations of the exponential distribution. As corollaries of the main\nresults, we extend some previous characterizations established recently by\nArnold and Villase\\~{n}or (2013) for a particular convolution of two random\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:46:14 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Yanev", "George P.", ""]]}, {"id": "2012.08647", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak and Weicong Yuan", "title": "Computation-free Nonparametric testing for Local and Global Spatial\n  Autocorrelation with application to the Canadian Electorate", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of local and global spatial association are key tools for\nexploratory spatial data analysis. Many such measures exist including Moran's\n$I$, Geary's $C$, and the Getis-Ord $G$ and $G^*$ statistics. A parametric\napproach to testing for significance relies on strong assumptions, which are\noften not met by real world data. Alternatively, the most popular nonparametric\napproach, the permutation test, imposes a large computational burden especially\nfor massive graphical networks. Hence, we propose a computation-free approach\nto nonparametric permutation testing for local and global measures of spatial\nautocorrelation stemming from generalizations of the Khintchine inequality from\nfunctional analysis and the theory of $L^p$ spaces. Our methodology is\ndemonstrated on the results of the 2019 federal Canadian election in the\nprovince of Alberta. We recorded the percentage of the vote gained by the\nconservative candidate in each riding. This data is not normal, and the sample\nsize is fixed at $n=34$ ridings making the parametric approach invalid. In\ncontrast, running a classic permutation test for every riding, for multiple\ntest statistics, with various neighbourhood structures, and multiple testing\ncorrection would require the simulation of millions of permutations. We are\nable to achieve similar statistical power on this dataset to the permutation\ntest without the need for tedious simulation. We also consider data simulated\nacross the entire electoral map of Canada.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:12:25 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""], ["Yuan", "Weicong", ""]]}, {"id": "2012.08810", "submitter": "Robin Henderson", "authors": "Kathryn Garside, Aida Gjoka, Robin Henderson, Hollie Johnson, Irina\n  Makarenko", "title": "Event History and Topological Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Persistent homology is used to track the appearance and disappearance of\nfeatures as we move through a nested sequence of topological spaces. Equating\nthe nested sequence to a filtration and the appearance and disappearance of\nfeatures to events, we show that simple event history methods can be used for\nthe analysis of topological data. We propose a version of the well known\nNelson-Aalen cumulative hazard estimator for the comparison of topological\nfeatures of random fields and for testing parametric assumptions. We suggest a\nCox proportional hazards approach for the analysis of embedded metric trees.\nThe Nelson-Aalen method is illustrated on globally distributed climate data and\non neutral hydrogen distribution in the Milky Way. The Cox method is use to\ncompare vascular patterns in fundus images of the eyes of healthy and diabetic\nretinopathy patients.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:17:37 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Garside", "Kathryn", ""], ["Gjoka", "Aida", ""], ["Henderson", "Robin", ""], ["Johnson", "Hollie", ""], ["Makarenko", "Irina", ""]]}, {"id": "2012.08966", "submitter": "Tania Roa Rojas", "authors": "Tania Roa, Soledad Torres and Ciprian tudor", "title": "Limit distribution of the least square estimator with observations\n  sampled at random times driven by standard Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we study the limit distribution of the least square\nestimator, properly normalized, from a regression model in which observations\nare assumed to be finite ($\\alpha N$) and sampled under two different random\ntimes. Based on the limit behavior of the characteristic function and\nconvergence result we prove the asymptotic normality for the least square\nestimator. We present simulation results to illustrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:04:28 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Roa", "Tania", ""], ["Torres", "Soledad", ""], ["tudor", "Ciprian", ""]]}, {"id": "2012.09141", "submitter": "Julio Castrillon PhD", "authors": "Julio Enrique Castrillon-Candas and Mark Kon", "title": "Change Detection: A functional analysis perspective", "comments": "Keywords: Hilbert spaces, Karhunen-Lo\\`{e}ve Expansions, Stochastic\n  Processes, Random Fields, Multilevel spaces, Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for detecting changes in the behavior of stochastic\nprocesses and random fields based on tensor product representations such as the\nKarhunen-Lo\\`{e}ve expansion. From the associated eigenspaces of the covariance\noperator a series of nested function spaces are constructed, allowing detection\nof signals lying in orthogonal subspaces. In particular this can succeed even\nif the stochastic behavior of the signal changes either in a global or local\nsense. A mathematical approach is developed to locate and measure sizes of\nextraneous components based on construction of multilevel nested subspaces. We\nshow examples in $\\mathbb{R}$ and on a spherical domain $\\mathbb{S}^{2}$.\nHowever, the method is flexible, allowing the detection of orthogonal signals\non general topologies, including spatio-temporal domains.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:26:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Castrillon-Candas", "Julio Enrique", ""], ["Kon", "Mark", ""]]}, {"id": "2012.09317", "submitter": "Matheus De Oliveira Souza", "authors": "Matheus de Oliveira Souza and Pablo Martin Rodriguez", "title": "On a fractional queueing model with catastrophes", "comments": "Revised version accepted for publication at Applied Mathematics and\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $M/M/1$ queue with catastrophes is a modified $M/M/1$ queue model for\nwhich, according to the times of a Poisson process, catastrophes occur leaving\nthe system empty. In this work, we study a fractional $M/M/1$ queue with\ncatastrophes, which is formulated by considering fractional derivatives in the\nKolmogorov's Forward Equations of the original Markov process. For the\nresulting fractional process, we obtain the state probabilities, the mean and\nthe variance for the number of customers at any time. In addition, we discuss\nthe estimation of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 23:22:11 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:33:09 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 23:01:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Souza", "Matheus de Oliveira", ""], ["Rodriguez", "Pablo Martin", ""]]}, {"id": "2012.09422", "submitter": "Andrew Bennett", "authors": "Andrew Bennett, Nathan Kallus", "title": "The Variational Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The conditional moment problem is a powerful formulation for describing\nstructural causal parameters in terms of observables, a prominent example being\ninstrumental variable regression. A standard approach is to reduce the problem\nto a finite set of marginal moment conditions and apply the optimally weighted\ngeneralized method of moments (OWGMM), but this requires we know a finite set\nof identifying moments, can still be inefficient even if identifying, or can be\ntheoretically efficient but practically unwieldy if we use a growing sieve of\nmoment conditions. Motivated by a variational minimax reformulation of OWGMM,\nwe define a very general class of estimators for the conditional moment\nproblem, which we term the variational method of moments (VMM) and which\nnaturally enables controlling infinitely-many moments. We provide a detailed\ntheoretical analysis of multiple VMM estimators, including ones based on kernel\nmethods and neural nets, and provide appropriate conditions under which these\nestimators are consistent, asymptotically normal, and semiparametrically\nefficient in the full conditional moment model. This is in contrast to other\nrecently proposed methods for solving conditional moment problems based on\nadversarial machine learning, which do not incorporate optimal weighting, do\nnot establish asymptotic normality, and are not semiparametrically efficient.\nIn addition, we provide corresponding inference algorithms based on the same\nkind of variational reformulations, both for kernel- and neural net-based\nvarieties. Finally, we demonstrate the strong performance of our proposed\nestimation and inference algorithms in a detailed series of synthetic\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 07:21:06 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:57:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bennett", "Andrew", ""], ["Kallus", "Nathan", ""]]}, {"id": "2012.09436", "submitter": "Irene Gannaz", "authors": "Ir\\`ene Gannaz (PSPM, ICJ)", "title": "Asymptotic normality of wavelet covariances and of multivariate wavelet\n  Whittle estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate processes with long-range memory properties can be encountered\nin many applications fields. Two fundamentals characteristics in such\nframeworks are the long-memory parameters and the correlation between time\nseries. We consider multivariate linear processes, not necessarily Gaussian,\npresenting long-memory dependence. We show that the covariances between the\nwavelet coefficients in this setting are asymptotically Gaussian. We also study\nthe asymptotic distributions of the estimators of the long-memory parameter and\nof the long-run covariance by a wavelet-based Whittle procedure. We prove the\nasymptotic normality of the estimators and we give an explicit expression for\nthe asymptotic covariances. An empirical illustration of this result is\nproposed on a real dataset of a rat brain connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:12:47 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gannaz", "Ir\u00e8ne", "", "PSPM, ICJ"]]}, {"id": "2012.09513", "submitter": "Yuta Koike", "authors": "Victor Chernozhukov, Denis Chetverikov, Yuta Koike", "title": "Nearly optimal central limit theorem and bootstrap approximations in\n  high dimensions", "comments": "60 pages. We corrected a mistake in v1. Lemmas 6.1-6.3 are\n  reformulated for general rectangles", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive new, nearly optimal bounds for the Gaussian\napproximation to scaled averages of $n$ independent high-dimensional centered\nrandom vectors $X_1,\\dots,X_n$ over the class of rectangles in the case when\nthe covariance matrix of the scaled average is non-degenerate. In the case of\nbounded $X_i$'s, the implied bound for the Kolmogorov distance between the\ndistribution of the scaled average and the Gaussian vector takes the form $$C\n(B^2_n \\log^3 d/n)^{1/2} \\log n,$$ where $d$ is the dimension of the vectors\nand $B_n$ is a uniform envelope constant on components of $X_i$'s. This bound\nis sharp in terms of $d$ and $B_n$, and is nearly (up to $\\log n$) sharp in\nterms of the sample size $n$. In addition, we show that similar bounds hold for\nthe multiplier and empirical bootstrap approximations. Moreover, we establish\nbounds that allow for unbounded $X_i$'s, formulated solely in terms of moments\nof $X_i$'s. Finally, we demonstrate that the bounds can be further improved in\nsome special smooth and zero-skewness cases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:23:01 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 10:52:08 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Koike", "Yuta", ""]]}, {"id": "2012.09623", "submitter": "Emma Simpson", "authors": "Emma S. Simpson, Jennifer L. Wadsworth and Jonathan A. Tawn", "title": "A geometric investigation into the tail dependence of vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a type of multivariate dependence model, composed of a\ncollection of bivariate copulas that are combined according to a specific\nunderlying graphical structure. Their flexibility and practicality in moderate\nand high dimensions have contributed to the popularity of vine copulas, but\nrelatively little attention has been paid to their extremal properties. To\naddress this issue, we present results on the tail dependence properties of\nsome of the most widely studied vine copula classes. We focus our study on the\ncoefficient of tail dependence and the asymptotic shape of the sample cloud,\nwhich we calculate using the geometric approach of Nolde (2014). We offer new\ninsights by presenting results for trivariate vine copulas constructed from\nasymptotically dependent and asymptotically independent bivariate copulas,\nfocusing on bivariate extreme value and inverted extreme value copulas, with\nadditional detail provided for logistic and inverted logistic examples. We also\npresent new theory for a class of higher dimensional vine copulas, constructed\nfrom bivariate inverted extreme value copulas.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:49:02 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Simpson", "Emma S.", ""], ["Wadsworth", "Jennifer L.", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "2012.09720", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "Hardness of Learning Halfspaces with Massart Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of PAC learning halfspaces in the presence of Massart\n(bounded) noise. Specifically, given labeled examples $(x, y)$ from a\ndistribution $D$ on $\\mathbb{R}^{n} \\times \\{ \\pm 1\\}$ such that the marginal\ndistribution on $x$ is arbitrary and the labels are generated by an unknown\nhalfspace corrupted with Massart noise at rate $\\eta<1/2$, we want to compute a\nhypothesis with small misclassification error. Characterizing the efficient\nlearnability of halfspaces in the Massart model has remained a longstanding\nopen problem in learning theory.\n  Recent work gave a polynomial-time learning algorithm for this problem with\nerror $\\eta+\\epsilon$. This error upper bound can be far from the\ninformation-theoretically optimal bound of $\\mathrm{OPT}+\\epsilon$. More recent\nwork showed that {\\em exact learning}, i.e., achieving error\n$\\mathrm{OPT}+\\epsilon$, is hard in the Statistical Query (SQ) model. In this\nwork, we show that there is an exponential gap between the\ninformation-theoretically optimal error and the best error that can be achieved\nby a polynomial-time SQ algorithm. In particular, our lower bound implies that\nno efficient SQ algorithm can approximate the optimal error within any\npolynomial factor.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:43:11 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.09828", "submitter": "Joshua Agterberg", "authors": "Joshua Agterberg, Minh Tang, and Carey Priebe", "title": "Nonparametric Two-Sample Hypothesis Testing for Random Graphs with\n  Negative and Repeated Eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a nonparametric two-sample test statistic for low-rank,\nconditionally independent edge random graphs whose edge probability matrices\nhave negative eigenvalues and arbitrarily close eigenvalues. Our proposed test\nstatistic involves using the maximum mean discrepancy applied to suitably\nrotated rows of a graph embedding, where the rotation is estimated using\noptimal transport. We show that our test statistic, appropriately scaled, is\nconsistent for sufficiently dense graphs, and we study its convergence under\ndifferent sparsity regimes. In addition, we provide empirical evidence\nsuggesting that our novel alignment procedure can perform better than the\nna\\\"ive alignment in practice, where the na\\\"ive alignment assumes an eigengap.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:51:02 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 17:35:14 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Agterberg", "Joshua", ""], ["Tang", "Minh", ""], ["Priebe", "Carey", ""]]}, {"id": "2012.09996", "submitter": "Anru R. Zhang", "authors": "Rungang Han, Yuetian Luo, Miaoyan Wang, and Anru R. Zhang", "title": "Exact Clustering in Tensor Block Model: Statistical Optimality and\n  Computational Limit", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-order clustering aims to identify heterogeneous substructure in multiway\ndataset that arises commonly in neuroimaging, genomics, and social network\nstudies. The non-convex and discontinuous nature of the problem poses\nsignificant challenges in both statistics and computation. In this paper, we\npropose a tensor block model and the computationally efficient methods,\n\\emph{high-order Lloyd algorithm} (HLloyd) and \\emph{high-order spectral\nclustering} (HSC), for high-order clustering in tensor block model. The\nconvergence of the proposed procedure is established, and we show that our\nmethod achieves exact clustering under reasonable assumptions. We also give the\ncomplete characterization for the statistical-computational trade-off in\nhigh-order clustering based on three different signal-to-noise ratio regimes.\nFinally, we show the merits of the proposed procedures via extensive\nexperiments on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:48:27 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:55:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Han", "Rungang", ""], ["Luo", "Yuetian", ""], ["Wang", "Miaoyan", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2012.10113", "submitter": "Sebastian Kersting", "authors": "Sebastian Kersting and Michael Kohler", "title": "On the density estimation problem for uncertainty propagation with\n  unknown input distributions", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the problem of quantifying the uncertainty in an\nexperiment with a technical system. We propose new density estimates which\ncombine observed data of the technical system and simulated data from an\n(imperfect) simulation model based on estimated input distributions. We analyze\nthe rate of convergence of these estimates. The finite sample size performance\nof the estimates is illustrated by applying them to simulated data. The\npractical usefulness of the newly proposed estimates is demonstrated by using\nthem to predict the uncertainty of a lateral vibration attenuation system with\npiezo-elastic supports.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:06:13 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kersting", "Sebastian", ""], ["Kohler", "Michael", ""]]}, {"id": "2012.10249", "submitter": "Ranjan Maitra", "authors": "Carlos Llosa-Vite and Ranjan Maitra", "title": "Reduced-Rank Tensor-on-Tensor Regression and Tensor-variate Analysis of\n  Variance", "comments": "30 pages, 12 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting regression models with many multivariate responses and covariates can\nbe challenging, but such responses and covariates sometimes have tensor-variate\nstructure. We extend the classical multivariate regression model to exploit\nsuch structure in two ways: first, we impose four types of low-rank tensor\nformats on the regression coefficients. Second, we model the errors using the\ntensor-variate normal distribution that imposes a Kronecker separable format on\nthe covariance matrix. We obtain maximum likelihood estimators via\nblock-relaxation algorithms and derive their asymptotic distributions. Our\nregression framework enables us to formulate tensor-variate analysis of\nvariance (TANOVA) methodology. Application of our methodology in a one-way\nTANOVA layout enables us to identify cerebral regions significantly associated\nwith the interaction of suicide attempters or non-attemptor ideators and\npositive-, negative- or death-connoting words. A separate application performs\nthree-way TANOVA on the Labeled Faces in the Wild image database to distinguish\nfacial characteristics related to ethnic origin, age group and gender.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:04:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:57:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Llosa-Vite", "Carlos", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2012.10320", "submitter": "Odalric-Ambrym Maillard", "authors": "Maillard Odalric-Ambrym", "title": "Local Dvoretzky-Kiefer-Wolfowitz confidence bands", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we revisit the concentration inequalities for the supremum of\nthe cumulative distribution function (CDF) of a real-valued continuous\ndistribution as established by Dvoretzky, Kiefer, Wolfowitz and revisited later\nby Massart in in two seminal papers. We focus on the concentration of the\n\\textit{local} supremum over a sub-interval, rather than on the full domain.\nThat is, denoting $U$ the CDF of the uniform distribution over $[0,1]$ and\n$U_n$ its empirical version built from $n$ samples, we study\n$\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U_n(u)-U(u) > \\epsilon\\Big)$ for different\nvalues of $\\uu,\\ou\\in[0,1]$. Such local controls naturally appear for instance\nwhen studying estimation error of spectral risk-measures (such as the\nconditional value at risk), where $[\\uu,\\ou]$ is typically $[0,\\alpha]$ or\n$[1-\\alpha,1]$ for a risk level $\\alpha$, after reshaping the CDF $F$ of the\nconsidered distribution into $U$ by the general inverse transform $F^{-1}$.\nExtending a proof technique from Smirnov, we provide exact expressions of the\nlocal quantities $\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U_n(u)-U(u) > \\epsilon\\Big)$\nand $\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U(u)-U_n(u) > \\epsilon\\Big)$ for each\n$n,\\epsilon,\\uu,\\ou$. Interestingly these quantities, seen as a function of\n$\\epsilon$, can be easily inverted numerically into functions of the\nprobability level $\\delta$. Although not explicit, they can be computed and\ntabulated. We plot such expressions and compare them to the classical bound\n$\\sqrt{\\frac{\\ln(1/\\delta)}{2n}}$ provided by Massart inequality. Last, we\nextend the local concentration results holding individually for each $n$ to\ntime-uniform concentration inequalities holding simultaneously for all $n$,\nrevisiting a reflection inequality by James, which is of independent interest\nfor the study of sequential decision making strategies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:04:00 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:35:09 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Odalric-Ambrym", "Maillard", ""]]}, {"id": "2012.10363", "submitter": "Christiane Lemieux", "authors": "C. Lemieux, J. Wiart", "title": "On the distribution of scrambled $(0,m,s)$-nets over unanchored boxes", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a new quality measure to assess randomized low-discrepancy point\nsets of finite size $n$. This new quality measure, which we call \"pairwise\nsampling dependence index\", is based on the concept of negative dependence. A\nnegative value for this index implies that the corresponding point set\nintegrates the indicator function of any unanchored box with smaller variance\nthan the Monte Carlo method. We show that scrambled $(0,m,s)-$nets have a\nnegative pairwise sampling dependence index. We also illustrate through an\nexample that randomizing via a digital shift instead of scrambling may yield a\npositive pairwise sampling dependence index.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:08:15 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 14:11:14 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lemieux", "C.", ""], ["Wiart", "J.", ""]]}, {"id": "2012.10369", "submitter": "Benjamin Guedj", "authors": "Maxime Haddouche and Benjamin Guedj and Omar Rivasplata and John\n  Shawe-Taylor", "title": "Upper and Lower Bounds on the Performance of Kernel PCA", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Principal Component Analysis (PCA) is a popular method for dimension\nreduction and has attracted an unfailing interest for decades. Recently, kernel\nPCA has emerged as an extension of PCA but, despite its use in practice, a\nsound theoretical understanding of kernel PCA is missing. In this paper, we\ncontribute lower and upper bounds on the efficiency of kernel PCA, involving\nthe empirical eigenvalues of the kernel Gram matrix. Two bounds are for fixed\nestimators, and two are for randomized estimators through the PAC-Bayes theory.\nWe control how much information is captured by kernel PCA on average, and we\ndissect the bounds to highlight strengths and limitations of the kernel PCA\nalgorithm. Therefore, we contribute to the better understanding of kernel PCA.\nOur bounds are briefly illustrated on a toy numerical example.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:19:31 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Haddouche", "Maxime", ""], ["Guedj", "Benjamin", ""], ["Rivasplata", "Omar", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2012.10596", "submitter": "Andrew Ledoan", "authors": "Christopher Corley, Andrew Ledoan", "title": "The level crossings of random sums", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\{\\eta_{j}\\}_{j = 0}^{N}$ be a sequence of independent and identically\ndistributed complex normal random variables with mean zero and variances\n$\\{\\sigma_{j}^{2}\\}_{j = 0}^{N}$. Let $\\{f_{j} (z)\\}_{j = 0}^{N}$ be a sequence\nof holomorphic functions that are real-valued on the real line. The purpose of\nthe present study is that of examining the number of times that the random sum\n$\\sum_{j = 0}^{N} \\eta_{j} f_{j} (z)$ crosses the complex level $\\boldsymbol{K}\n= K_{1} + i K_{2}$, where $K_{1}$ and $K_{2}$ are constants independent of $z$.\nMore specifically, we establish an exact formula for the expected density\nfunction for the complex zeros. We then reformulate the problem in terms of\nsuccessive observations of a Brownian motion. We further answer the basic\nquestion about the expected number of complex zeros for coefficients of\nnonvanishing mean values.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 04:54:42 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 20:31:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Corley", "Christopher", ""], ["Ledoan", "Andrew", ""]]}, {"id": "2012.10623", "submitter": "Qijun Tong", "authors": "Qijun Tong, Kei Kobayashi", "title": "Entropy-regularized optimal transport on multivariate normal and\n  q-normal distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The distance and divergence of the probability measures play a central role\nin statistics, machine learning, and many other related fields. The Wasserstein\ndistance has received much attention in recent years because of its\ndistinctions from other distances or divergences. Although~computing the\nWasserstein distance is costly, entropy-regularized optimal transport was\nproposed to computationally efficiently approximate the Wasserstein distance.\nThe purpose of this study is to understand the theoretical aspect of\nentropy-regularized optimal transport. In this paper, we~focus on\nentropy-regularized optimal transport on multivariate normal distributions and\n$q$-normal distributions. We~obtain the explicit form of the\nentropy-regularized optimal transport cost on multivariate normal and\n$q$-normal distributions; this provides a perspective to understand the effect\nof entropy regularization, which was previously known only experimentally.\nFurthermore, we obtain the entropy-regularized Kantorovich estimator for the\nprobability measure that satisfies certain conditions. We also demonstrate how\nthe Wasserstein distance, optimal coupling, geometric structure, and\nstatistical efficiency are affected by entropy regularization in some\nexperiments. In particular, our results about the explicit form of the optimal\ncoupling of the Tsallis entropy-regularized optimal transport on multivariate\n$q$-normal distributions and the entropy-regularized Kantorovich estimator are\nnovel and will become the first step towards the understanding of a more\ngeneral setting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 08:02:55 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 02:52:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tong", "Qijun", ""], ["Kobayashi", "Kei", ""]]}, {"id": "2012.10689", "submitter": "Philip Dawid", "authors": "Philip Dawid", "title": "Fiducial inference then and now", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conduct a review of the fiducial approach to statistical inference,\nfollowing its journey from its initiation by R. A. Fisher, through various\nproblems and criticisms, on to its general neglect, and then to its more recent\nresurgence. Emphasis is laid on the functional model formulation, which helps\nclarify the very limited conditions under which fiducial inference can be\nconducted in an unambiguous and self-consistent way.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 14:00:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dawid", "Philip", ""]]}, {"id": "2012.10745", "submitter": "St\\'ephane Guerrier", "authors": "St\\'ephane Guerrier and Christoph Kuzmics and Maria-Pia Victoria-Feser", "title": "Prevalence Estimation from Random Samples and Census Data with\n  Participation Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Countries officially record the number of COVID-19 cases based on medical\ntests of a subset of the population with unknown participation bias. For\nprevalence estimation, the official information is typically discarded and,\ninstead, small random survey samples are taken. We derive (maximum likelihood\nand method of moment) prevalence estimators, based on a survey sample, that\nadditionally utilize the official information, and that are substantially more\naccurate than the simple sample proportion of positive cases. Put differently,\nusing our estimators, the same level of precision can be obtained with\nsubstantially smaller survey samples. We take into account the possibility of\nmeasurement errors due to the sensitivity and specificity of the medical\ntesting procedure. The proposed estimators and associated confidence intervals\nare implemented in the companion open source R package cape.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 18:02:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 21:40:08 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Kuzmics", "Christoph", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "2012.10861", "submitter": "Lei Liu", "authors": "Lei Liu, Shunqi Huang and Brian M. Kurkoski", "title": "Memory AMP", "comments": "30 pages, 9 figures, submitted to IEEE Trans. on Information Theory\n  for possible publication. [Memory AMP inherits the strengths of AMP and\n  OAMP/VAMP such as low complexity, Bayes optimality and applicability to\n  unitarily-inavariant matrices, while avoiding the weakness of AMP (e.g.\n  limited to IID matrices) and OAMP/VAMP (e.g. needs high-complexity LMMSE).]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG eess.SP math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable\n(e.g. perform poorly or even diverge) for other matrix ensembles, especially\nfor ill-conditioned ones. To handle this difficulty, orthogonal/vector AMP\n(OAMP/VAMP) was proposed for general right-unitarily-invariant matrices.\nHowever, the Bayes-optimal OAMP/VAMP requires high-complexity linear minimum\nmean square error (MMSE) estimator. This limits the application of OAMP/VAMP to\nlarge-scale systems.\n  To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory\nAMP (MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of MAMP is comparable to AMP. The asymptotic\nGaussianity of estimation errors in MAMP is guaranteed by the orthogonality\nprinciple. A state evolution is derived to asymptotically characterize the\nperformance of MAMP. Based on state evolution, the relaxation parameters and\ndamping vector in MAMP are optimized. For all right-unitarily-invariant\nmatrices, the optimized MAMP converges to the high-complexity OAMP/VAMP, and\nthus is Bayes-optimal if it has a unique fixed point. Finally, simulations are\nprovided to verify the validity and accuracy of the theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 07:42:15 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 14:31:38 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 04:08:05 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Lei", ""], ["Huang", "Shunqi", ""], ["Kurkoski", "Brian M.", ""]]}, {"id": "2012.11020", "submitter": "Marko Obradovi\\'c", "authors": "Marija Cupari\\'c, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "Asymptotic distribution of certain degenerate V- and U-statistics with\n  estimated parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The asymptotic distribution of a wide class of V- and U-statistics with\nestimated parameters is derived in the case when the kernel is not necessarily\ndifferentiable along the parameter. The results have their application in\ngoodness-of-fit problems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 20:42:55 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cupari\u0107", "Marija", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "2012.11180", "submitter": "Sunanda Bagchi", "authors": "Bhaskar Bagchi and Sunanda Bagchi", "title": "Aspects of optimality of plans orthogonal through other factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The concept of orthogonality through the block factor (OTB), defined in\nBagchi (2010), is extended here to orthogonality through a set (say S) of other\nfactors. We discuss the impact of such an orthogonality on the precision of the\nestimates as well as on the inference procedure. Concentrating on the case when\n$S$ is of size two, we construct a series of plans in each of which every pair\nof other factors is orthogonal through a given pair of factors.\n  Next we concentrate on plans through the block factors (POTB). We construct\nPOTBs for symmetrical experiments with two and three-level factors. The plans\nfor two factors are E-optimal, while those for three-level factors are\nuniversally optimal.\n  Finally, we construct POTBs for $s^t(s+1)$ experiments, where $s \\equiv 3\n\\pmod 4$ is a prime power. The plan is universally optimal.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 08:41:23 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bagchi", "Bhaskar", ""], ["Bagchi", "Sunanda", ""]]}, {"id": "2012.11222", "submitter": "Gregory Cox", "authors": "Gregory Cox", "title": "Weak Identification with Bounds in a Class of Minimum Distance Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When parameters are weakly identified, bounds on the parameters may provide a\nvaluable source of information. Existing weak identification estimation and\ninference results are unable to combine weak identification with bounds. Within\na class of minimum distance models, this paper proposes identification-robust\ninference that incorporates information from bounds when parameters are weakly\nidentified. The inference is based on limit theory that combines weak\nidentification theory (Andrews and Cheng (2012)) with parameter-on-the-boundary\ntheory (Andrews (1999)) via a new argmax theorem. This paper characterizes weak\nidentification in low-dimensional factor models (due to weak factors) and\ndemonstrates the role of the bounds and identification-robust inference in two\nexample factor models. This paper also demonstrates the identification-robust\ninference in an empirical application: estimating the effects of a randomized\nintervention on parental investments in children, where parental investments\nare modeled by a factor model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:05:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cox", "Gregory", ""]]}, {"id": "2012.11281", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Towards Conditional Path Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend path analysis by giving sufficient conditions for computing the\npartial covariance of two random variables from their covariance. This is\nspecifically done by correcting the covariance with the product of some partial\nvariance ratios. As a result, the partial covariance retains the covariance's\nsalient feature of factorizing over the edges in the paths between the two\nvariables of interest.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:20:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2012.11478", "submitter": "Sunanda Bagchi", "authors": "Sunanda Bagchi and Bhaskar Bagchi", "title": "Optimality of multi-way designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we study optimality aspects of a certain type of designs in a\nmulti-way heterogeneity setting. These are ``duals\" of plans orthogonal through\nthe block factor (POTB). Here by the dual of a main effect plan (say $\\rho$) we\nmean a design in a multi-way heterogeneity setting obtained from $\\rho$ by\ninterchanging the roles of the block factors and the treatment factors.\nSpecifically, we take up two series of universally optimal POTBs for\nsymmetrical experiments constructed in Morgan and Uddin (1996). We show that\nthe duals of these plans, as multi-way designs, satisfy M-optimality.\n  Next, we construct another series of multiway designs and proved their\nM-optimality, thereby generalising the result of Bagchi and Shah (1989). It may\nbe noted that M-optimality includes all commonly used optimality criteria like\nA-, D- and E-optimality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:47:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bagchi", "Sunanda", ""], ["Bagchi", "Bhaskar", ""]]}, {"id": "2012.11530", "submitter": "Dennis Schroers", "authors": "Fred Espen Benth, Giulia Di Nunno and Dennis Schroers", "title": "Copula Measures and Sklar's Theorem in Arbitrary Dimensions", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although copulas are used and defined for various infinite-dimensional\nobjects (e.g. Gaussian processes and Markov processes), there is no prevalent\nnotion of a copula that unifies these concepts. We propose a unified approach\nand define copulas as probability measures on general product spaces. For this\nwe prove Sklar's Theorem in this infinite-dimensional setting. We show how to\ntransfer this result to various function space settings and describe how to\nmodel and approximate dependent probability measures in these spaces in the\nrealm of copulas.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:47:47 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 18:27:25 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Benth", "Fred Espen", ""], ["Di Nunno", "Giulia", ""], ["Schroers", "Dennis", ""]]}, {"id": "2012.11554", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Yufeng Zhang, Yongxin Chen, Zhaoran Wang", "title": "Variational Transport: A Convergent Particle-BasedAlgorithm for\n  Distributional Optimization", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization problem of minimizing a functional defined over\na family of probability distributions, where the objective functional is\nassumed to possess a variational form. Such a distributional optimization\nproblem arises widely in machine learning and statistics, with Monte-Carlo\nsampling, variational inference, policy optimization, and generative\nadversarial network as examples. For this problem, we propose a novel\nparticle-based algorithm, dubbed as variational transport, which approximately\nperforms Wasserstein gradient descent over the manifold of probability\ndistributions via iteratively pushing a set of particles. Specifically, we\nprove that moving along the geodesic in the direction of functional gradient\nwith respect to the second-order Wasserstein distance is equivalent to applying\na pushforward mapping to a probability distribution, which can be approximated\naccurately by pushing a set of particles. Specifically, in each iteration of\nvariational transport, we first solve the variational problem associated with\nthe objective functional using the particles, whose solution yields the\nWasserstein gradient direction. Then we update the current distribution by\npushing each particle along the direction specified by such a solution. By\ncharacterizing both the statistical error incurred in estimating the\nWasserstein gradient and the progress of the optimization algorithm, we prove\nthat when the objective function satisfies a functional version of the\nPolyak-\\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,\nvariational transport converges linearly to the global minimum of the objective\nfunctional up to a certain statistical error, which decays to zero sublinearly\nas the number of particles goes to infinity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:33:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yang", "Zhuoran", ""], ["Zhang", "Yufeng", ""], ["Chen", "Yongxin", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2012.11676", "submitter": "Zhou Fan", "authors": "Xinyi Zhong and Chang Su and Zhou Fan", "title": "Empirical Bayes PCA in high dimensions", "comments": "v2: Shorten exposition, clarify some theoretical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the dimension of data is comparable to or larger than the number of data\nsamples, Principal Components Analysis (PCA) may exhibit problematic\nhigh-dimensional noise. In this work, we propose an Empirical Bayes PCA method\nthat reduces this noise by estimating a joint prior distribution for the\nprincipal components. EB-PCA is based on the classical Kiefer-Wolfowitz\nnonparametric MLE for empirical Bayes estimation, distributional results\nderived from random matrix theory for the sample PCs, and iterative refinement\nusing an Approximate Message Passing (AMP) algorithm. In theoretical \"spiked\"\nmodels, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings\nas an oracle Bayes AMP procedure that knows the true priors. Empirically,\nEB-PCA significantly improves over PCA when there is strong prior structure,\nboth in simulation and on quantitative benchmarks constructed from the 1000\nGenomes Project and the International HapMap Project. An illustration is\npresented for analysis of gene expression data obtained by single-cell RNA-seq.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 20:43:44 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:20:16 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhong", "Xinyi", ""], ["Su", "Chang", ""], ["Fan", "Zhou", ""]]}, {"id": "2012.11809", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou", "title": "Estimation in nonparametric regression model with additive and\n  multiplicative noise via Laguerre series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We look into the nonparametric regression estimation with additive and\nmultiplicative noise and construct adaptive thresholding estimators based on\nLaguerre series. The proposed approach achieves asymptotically near-optimal\nconvergence rates when the unknown function belongs to Laguerre-Sobolev space.\nWe consider the problem under two noise structures; (1) { i.i.d.} Gaussian\nerrors and (2) long-memory Gaussian errors. In the { i.i.d.} case, our\nconvergence rates are similar to those found in the literature. In the\nlong-memory case, the convergence rates depend on the long-memory parameters\nonly when long-memory is strong enough in either noise source, otherwise, the\nrates are identical to those under { i.i.d.} noise.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 03:03:52 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Benhaddou", "Rida", ""]]}, {"id": "2012.11857", "submitter": "Mamikon Gulian", "authors": "Mamikon Gulian, Ari Frankel, Laura Swiler", "title": "Gaussian Process Regression constrained by Boundary Value Problems", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for Gaussian processes regression constrained by\nboundary value problems. The framework may be applied to infer the solution of\na well-posed boundary value problem with a known second-order differential\noperator and boundary conditions, but for which only scattered observations of\nthe source term are available. Scattered observations of the solution may also\nbe used in the regression. The framework combines co-kriging with the linear\ntransformation of a Gaussian process together with the use of kernels given by\nspectral expansions in eigenfunctions of the boundary value problem. Thus, it\nbenefits from a reduced-rank property of covariance matrices. We demonstrate\nthat the resulting framework yields more accurate and stable solution inference\nas compared to physics-informed Gaussian process regression without boundary\ncondition constraints.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:55:15 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Gulian", "Mamikon", ""], ["Frankel", "Ari", ""], ["Swiler", "Laura", ""]]}, {"id": "2012.11920", "submitter": "Anis Mohamed Haddouche", "authors": "Anis M. Haddouche, Dominique Fourdrinier and Fatiha Mezoued", "title": "Covariance matrix estimation under data-based loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the $p\\times p$ scale\nmatrix $\\Sigma$ of a multivariate linear regression model $Y=X\\,\\beta +\n\\mathcal{E}\\,$ when the distribution of the observed matrix $Y$ belongs to a\nlarge class of elliptically symmetric distributions. After deriving the\ncanonical form $(Z^T U^T)^T$ of this model, any estimator $\\hat{ \\Sigma}$ of\n$\\Sigma$ is assessed through the data-based loss tr$(S^{+}\\Sigma\\,\n(\\Sigma^{-1}\\hat{\\Sigma} - I_p)^2 )\\,$ where $S=U^T U$ is the sample covariance\nmatrix and $S^{+}$ is its Moore-Penrose inverse. We provide alternative\nestimators to the usual estimators $a\\,S$, where $a$ is a positive constant,\nwhich present smaller associated risk. Compared to the usual quadratic loss\ntr$(\\Sigma^{-1}\\hat{\\Sigma} - I_p)^2$, we obtain a larger class of estimators\nand a wider class of elliptical distributions for which such an improvement\noccurs. A numerical study illustrates the theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:41:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Haddouche", "Anis M.", ""], ["Fourdrinier", "Dominique", ""], ["Mezoued", "Fatiha", ""]]}, {"id": "2012.12083", "submitter": "Matteo Giordano", "authors": "Matteo Giordano, Kolyan Ray", "title": "Nonparametric Bayesian inference for reversible multi-dimensional\n  diffusions", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric Bayesian models for reversible multi-dimensional\ndiffusions with periodic drift. For continuous observation paths, reversibility\nis exploited to prove a general posterior contraction rate theorem for the\ndrift gradient vector field under approximation-theoretic conditions on the\ninduced prior for the invariant measure. The general theorem is applied to\nGaussian priors and $p$-exponential priors, which are shown to converge to the\ntruth at the minimax optimal rate over Sobolev smoothness classes in any\ndimension\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 15:24:40 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 10:40:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Giordano", "Matteo", ""], ["Ray", "Kolyan", ""]]}, {"id": "2012.12196", "submitter": "Guanhua Fang", "authors": "Guanhua Fang, Xin Xu, Jinxin Guo, Zhiliang Ying, Susu Zhang", "title": "Identifiability of Bifactor Models", "comments": "89 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The bifactor model and its extensions are multidimensional latent variable\nmodels, under which each item measures up to one subdimension on top of the\nprimary dimension(s). Despite their wide applications to educational and\npsychological assessments, this type of multidimensional latent variable models\nmay suffer from non-identifiability, which can further lead to inconsistent\nparameter estimation and invalid inference. The current work provides a\nrelatively complete characterization of identifiability for the linear and\ndichotomous bifactor models and the linear extended bifactor model with\ncorrelated subdimensions. In addition, similar results for the two-tier models\nare also developed. Illustrative examples are provided on checking model\nidentifiability through inspecting the factor loading structure. Simulation\nstudies are reported that examine estimation consistency when the\nidentifiability conditions are/are not satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:34:09 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Fang", "Guanhua", ""], ["Xu", "Xin", ""], ["Guo", "Jinxin", ""], ["Ying", "Zhiliang", ""], ["Zhang", "Susu", ""]]}, {"id": "2012.12562", "submitter": "Max-K. von Renesse", "authors": "Tobias Lehmann, Max-K. von Renesse, Alexander Sambale, Andr\\'e\n  Uschmajew", "title": "A note on overrelaxation in the Sinkhorn algorithm", "comments": "8 pages, 1 figure, some typos corrected with regards to previous\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We derive an a-priori parameter range for overrelaxation of the Sinkhorn\nalgorithm, which guarantees global convergence and a strictly faster asymptotic\nlocal convergence. Guided by the spectral analysis of the linearized problem we\npursue a zero cost procedure to choose a near optimal relaxation parameter.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:48:24 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 13:53:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lehmann", "Tobias", ""], ["von Renesse", "Max-K.", ""], ["Sambale", "Alexander", ""], ["Uschmajew", "Andr\u00e9", ""]]}, {"id": "2012.12670", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Matthew M. Graham, Chris J. Oates, T. J. Sullivan", "title": "Testing whether a Learning Procedure is Calibrated", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A learning procedure takes as input a dataset and performs inference for the\nparameters $\\theta$ of a model that is assumed to have given rise to the\ndataset. Here we consider learning procedures whose output is a probability\ndistribution, representing uncertainty about $\\theta$ after seeing the dataset.\nBayesian inference is a prime example of such a procedure but one can also\nconstruct other learning procedures that return distributional output. This\npaper studies conditions for a learning procedure to be considered calibrated,\nin the sense that the true data-generating parameters are plausible as samples\nfrom its distributional output. A learning procedure that is calibrated need\nnot be statistically efficient and vice versa. A hypothesis-testing framework\nis developed in order to assess, using simulation, whether a learning procedure\nis calibrated. Finally, we exploit our framework to test the calibration of\nsome learning procedures that are motivated as being approximations to Bayesian\ninference but are nevertheless widely used.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:02:58 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 10:22:03 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 07:15:36 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cockayne", "Jon", ""], ["Graham", "Matthew M.", ""], ["Oates", "Chris J.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "2012.12762", "submitter": "Christof Sch\\\"otz", "authors": "Christof Sch\\\"otz", "title": "Strong Laws of Large Numbers for Generalizations of Fr\\'echet Mean Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Fr\\'echet mean of a random variable $Y$ with values in a metric space\n$(\\mathcal Q, d)$ is an element of the metric space that minimizes $q \\mapsto\n\\mathbb E[d(Y,q)^2]$. This minimizer may be non-unique. We study strong laws of\nlarge numbers for sets of generalized Fr\\'echet means. Following\ngeneralizations are considered: the minimizers of $\\mathbb E[d(Y, q)^\\alpha]$\nfor $\\alpha > 0$, the minimizers of $\\mathbb E[H(d(Y, q))]$ for integrals $H$\nof non-decreasing functions, and the minimizers of $\\mathbb E[\\mathfrak c(Y,\nq)]$ for a quite unrestricted class of cost functions $\\mathfrak c$. We show\nconvergence of empirical versions of these sets in outer limit and in one-sided\nHausdorff distance. The derived results require only minimal assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:01:49 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Sch\u00f6tz", "Christof", ""]]}, {"id": "2012.12810", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Chen Lu, Kwangjun Ahn, Xiang Cheng, Thibaut Le Gouic,\n  Philippe Rigollet", "title": "Optimal dimension dependence of the Metropolis-Adjusted Langevin\n  Algorithm", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional wisdom in the sampling literature, backed by a popular diffusion\nscaling limit, suggests that the mixing time of the Metropolis-Adjusted\nLangevin Algorithm (MALA) scales as $O(d^{1/3})$, where $d$ is the dimension.\nHowever, the diffusion scaling limit requires stringent assumptions on the\ntarget distribution and is asymptotic in nature. In contrast, the best known\nnon-asymptotic mixing time bound for MALA on the class of log-smooth and\nstrongly log-concave distributions is $O(d)$. In this work, we establish that\nthe mixing time of MALA on this class of target distributions is\n$\\widetilde\\Theta(d^{1/2})$ under a warm start. Our upper bound proof\nintroduces a new technique based on a projection characterization of the\nMetropolis adjustment which reduces the study of MALA to the well-studied\ndiscretization analysis of the Langevin SDE and bypasses direct computation of\nthe acceptance probability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:14:06 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Chewi", "Sinho", ""], ["Lu", "Chen", ""], ["Ahn", "Kwangjun", ""], ["Cheng", "Xiang", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2012.12859", "submitter": "Steven N. Evans", "authors": "Steven N. Evans and Adam Q. Jaffe", "title": "Strong laws of large numbers for Fr\\'echet means", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $1 \\le p < \\infty$, the Fr\\'echet $p$-mean of a probability distribution\n$\\mu$ on a metric space $(X,d)$ is the set $F_p(\\mu) := {\\arg\\,\\min}_{x\\in\nX}\\int_{X}d^p(x,y)\\, d\\mu(y)$, which is taken to be empty if no minimizer\nexists. Given a sequence $(Y_i)_{i \\in \\mathbb{N}}$ of independent, identically\ndistributed random samples from some probability measure $\\mu$ on $X$, the\nFr\\'echet $p$-means of the empirical measures,\n$F_p(\\frac{1}{n}\\sum_{i=1}^{n}\\delta_{Y_i})$ form a sequence of random closed\nsubsets of $X$. We investigate the senses in which this sequence of random\nclosed sets and related objects converge almost surely as $n \\to \\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:30:58 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Evans", "Steven N.", ""], ["Jaffe", "Adam Q.", ""]]}, {"id": "2012.12891", "submitter": "Sunanda Bagchi", "authors": "Sunanda Bagchi", "title": "New plans orthogonal through the block factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the present paper we construct plans orthogonal through the block factor\n(POTBs). We describe procedures for adding blocks as well as factors to an\ninitial plan and thus generate a bigger plan. Using these procedures we\nconstruct POTBs for symmetrical experiments with factors having three or more\nlevels. We also construct a series of plans inter-class orthogonal through the\nblock factor for two-level factors.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:53:13 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Bagchi", "Sunanda", ""]]}, {"id": "2012.12895", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "A Modern Analysis of Hutchinson's Trace Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper establishes the new state-of-art in the accuracy analysis of\nHutchinson's trace estimator. Leveraging tools that have not been previously\nused in this context, particularly hypercontractive inequalities and\nconcentration properties of sub-gamma distributions, we offer an elegant and\nmodular analysis, as well as numerically superior bounds. Besides these\nimprovements, this work aims to better popularize the aforementioned techniques\nwithin the CS community.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:58:01 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2012.12917", "submitter": "Mattes Mollenhauer", "authors": "Mattes Mollenhauer and P\\'eter Koltai", "title": "Nonparametric approximation of conditional expectation operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the joint distribution of two random variables $X,Y$ on some second\ncountable locally compact Hausdorff space, we investigate the statistical\napproximation of the $L^2$-operator defined by $[Pf](x) := \\mathbb{E}[ f(Y)\n\\mid X = x ]$ under minimal assumptions. By modifying its domain, we prove that\n$P$ can be arbitrarily well approximated in operator norm by Hilbert--Schmidt\noperators acting on a reproducing kernel Hilbert space. This fact allows to\nestimate $P$ uniformly by finite-rank operators over a dense subspace even when\n$P$ is not compact. In terms of modes of convergence, we thereby obtain the\nsuperiority of kernel-based techniques over classically used parametric\nprojection approaches such as Galerkin methods. This also provides a novel\nperspective on which limiting object the nonparametric estimate of $P$\nconverges to. As an application, we show that these results are particularly\nimportant for a large family of spectral analysis techniques for Markov\ntransition operators. Our investigation also gives a new asymptotic perspective\non the so-called kernel conditional mean embedding, which is the theoretical\nfoundation of a wide variety of techniques in kernel-based nonparametric\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 19:06:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:45:41 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mollenhauer", "Mattes", ""], ["Koltai", "P\u00e9ter", ""]]}, {"id": "2012.12948", "submitter": "Karel Hron", "authors": "Karel Hron, Jitka Machalov\\'a, Alessandra Menafoglio", "title": "Bivariate Densities in Bayes Spaces: Orthogonal Decomposition and Spline\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new orthogonal decomposition for bivariate probability densities embedded\nin Bayes Hilbert spaces is derived. It allows one to represent a density into\nindependent and interactive parts, the former being built as the product of\nrevised definitions of marginal densities and the latter capturing the\ndependence between the two random variables being studied. The developed\nframework opens new perspectives for dependence modelling (which is commonly\nperformed through copulas), and allows for the analysis of dataset of bivariate\ndensities, in a Functional Data Analysis perspective. A spline representation\nfor bivariate densities is also proposed, providing a computational cornerstone\nfor the developed theory.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 20:25:05 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Hron", "Karel", ""], ["Machalov\u00e1", "Jitka", ""], ["Menafoglio", "Alessandra", ""]]}, {"id": "2012.13106", "submitter": "Akifumi Okuno", "authors": "Akifumi Okuno, Keisuke Yano", "title": "Dependence of variance on covariate design in nonparametric link\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a nonparametric approach to link regression aiming at\npredicting a mean outcome at a link, i.e., a pair of nodes, based on currently\nobserved data comprising covariates at nodes and outcomes at links. The\nvariance decay rates of nonparametric link regression estimates are\ndemonstrated to depend on covariate designs; namely, whether the covariate\ndesign is random or fixed. This covariate design-dependent nature of variance\nis observed in nonparametric link regression but not in conventional\nnonparametric regression.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:04:11 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:52:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Okuno", "Akifumi", ""], ["Yano", "Keisuke", ""]]}, {"id": "2012.13262", "submitter": "Oliver Dunbar", "authors": "Oliver R. A. Dunbar, Alfredo Garbuno-Inigo, Tapio Schneider, Andrew M.\n  Stuart", "title": "Calibration and Uncertainty Quantification of Convective Parameters in\n  an Idealized GCM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parameters in climate models are usually calibrated manually, exploiting only\nsmall subsets of the available data. This precludes both optimal calibration\nand quantification of uncertainties. Traditional Bayesian calibration methods\nthat allow uncertainty quantification are too expensive for climate models;\nthey are also not robust in the presence of internal climate variability. For\nexample, Markov chain Monte Carlo (MCMC) methods typically require $O(10^5)$\nmodel runs and are sensitive to internal variability noise, rendering them\ninfeasible for climate models. Here we demonstrate an approach to model\ncalibration and uncertainty quantification that requires only $O(10^2)$ model\nruns and can accommodate internal climate variability. The approach consists of\nthree stages: (i) a calibration stage uses variants of ensemble Kalman\ninversion to calibrate a model by minimizing mismatches between model and data\nstatistics; (ii) an emulation stage emulates the parameter-to-data map with\nGaussian processes (GP), using the model runs in the calibration stage for\ntraining; (iii) a sampling stage approximates the Bayesian posterior\ndistributions by sampling the GP emulator with MCMC. We demonstrate the\nfeasibility and computational efficiency of this calibrate-emulate-sample (CES)\napproach in a perfect-model setting. Using an idealized general circulation\nmodel, we estimate parameters in a simple convection scheme from synthetic data\ngenerated with the model. The CES approach generates probability distributions\nof the parameters that are good approximations of the Bayesian posteriors, at a\nfraction of the computational cost usually required to obtain them. Sampling\nfrom this approximate posterior allows the generation of climate predictions\nwith quantified parametric uncertainties.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 14:12:22 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 21:10:24 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Dunbar", "Oliver R. A.", ""], ["Garbuno-Inigo", "Alfredo", ""], ["Schneider", "Tapio", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "2012.13307", "submitter": "Jie Ding", "authors": "Jie Ding, Enmao Diao, Jiawei Zhou, Vahid Tarokh", "title": "On Statistical Efficiency in Learning", "comments": "to be published by the IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2020.3047620", "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central issue of many statistical learning problems is to select an\nappropriate model from a set of candidate models. Large models tend to inflate\nthe variance (or overfitting), while small models tend to cause biases (or\nunderfitting) for a given fixed dataset. In this work, we address the critical\nchallenge of model selection to strike a balance between model fitting and\nmodel complexity, thus gaining reliable predictive power. We consider the task\nof approaching the theoretical limit of statistical learning, meaning that the\nselected model has the predictive performance that is as good as the best\npossible model given a class of potentially misspecified candidate models. We\npropose a generalized notion of Takeuchi's information criterion and prove that\nthe proposed method can asymptotically achieve the optimal out-sample\nprediction loss under reasonable assumptions. It is the first proof of the\nasymptotic property of Takeuchi's information criterion to our best knowledge.\nOur proof applies to a wide variety of nonlinear models, loss functions, and\nhigh dimensionality (in the sense that the models' complexity can grow with\nsample size). The proposed method can be used as a computationally efficient\nsurrogate for leave-one-out cross-validation. Moreover, for modeling streaming\ndata, we propose an online algorithm that sequentially expands the model\ncomplexity to enhance selection stability and reduce computation cost.\nExperimental studies show that the proposed method has desirable predictive\npower and significantly less computational cost than some popular methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 16:08:29 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Ding", "Jie", ""], ["Diao", "Enmao", ""], ["Zhou", "Jiawei", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2012.13332", "submitter": "Christof Sch\\\"otz", "authors": "Christof Sch\\\"otz", "title": "Regression in Nonstandard Spaces with Fr\\'echet and Geodesic Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One approach to tackle regression in nonstandard spaces is Fr\\'echet\nregression, where the value of the regression function at each point is\nestimated via a Fr\\'echet mean calculated from an estimated objective function.\nA second approach is geodesic regression, which builds upon fitting geodesics\nto observations by a least squares method. We compare these two approaches by\nusing them to transform three of the most important regression estimators in\nstatistics - linear regression, local linear regression, and trigonometric\nprojection estimator - to settings where responses live in a metric space. The\nresulting procedures consist of known estimators as well as new methods. We\ninvestigate their rates of convergence in general settings and compare their\nperformance in a simulation study on the sphere.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:08:49 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sch\u00f6tz", "Christof", ""]]}, {"id": "2012.13348", "submitter": "Christophe Ley", "authors": "Corinne Sinner and Yves Dominicy and Julien Trufin and Wout\n  Waterschoot and Patrick Weber and Christophe Ley", "title": "An Interpolating Family of size distributions", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.04430", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power laws and power laws with exponential cut-off are two distinct families\nof size distributions. In the present paper, we propose a unified treatment of\nboth families by building a family of distributions that interpolates between\nthem, hence the name Interpolating Family (IF) of distributions. Our original\nconstruction, which relies on techniques from statistical physics, provides a\nconnection for hitherto unrelated distributions like the Pareto and Weibull\ndistributions, and sheds new light on them. The IF also contains several\ndistributions that are neither of power law nor of power law with exponential\ncut-off type. We calculate quantile-based properties, moments and modes for the\nIF. This allows us to review known properties of famous size distributions and\nto provide in a single sweep these characteristics for various less known (and\nnew) special cases of our Interpolating Family.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:11:36 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Sinner", "Corinne", ""], ["Dominicy", "Yves", ""], ["Trufin", "Julien", ""], ["Waterschoot", "Wout", ""], ["Weber", "Patrick", ""], ["Ley", "Christophe", ""]]}, {"id": "2012.13669", "submitter": "Jiaoyang Huang", "authors": "Jiaoyang Huang and Daniel Z. Huang and Qing Yang and Guang Cheng", "title": "Power Iteration for Tensor PCA", "comments": "Draft version, comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the power iteration algorithm for the spiked tensor\nmodel, as introduced in [44]. We give necessary and sufficient conditions for\nthe convergence of the power iteration algorithm. When the power iteration\nalgorithm converges, for the rank one spiked tensor model, we show the\nestimators for the spike strength and linear functionals of the signal are\nasymptotically Gaussian; for the multi-rank spiked tensor model, we show the\nestimators are asymptotically mixtures of Gaussian. This new phenomenon is\ndifferent from the spiked matrix model. Using these asymptotic results of our\nestimators, we construct valid and efficient confidence intervals for spike\nstrengths and linear functionals of the signals.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 03:09:47 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Huang", "Jiaoyang", ""], ["Huang", "Daniel Z.", ""], ["Yang", "Qing", ""], ["Cheng", "Guang", ""]]}, {"id": "2012.13766", "submitter": "Julien Chhor", "authors": "J. Chhor, A. Carpentier", "title": "Sharp Local Minimax Rates for Goodness-of-Fit Testing in Large Random\n  Graphs, multivariate Poisson families and multinomials", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the identity testing problem in large inhomogeneous random graphs\n(also called goodness-of-fit testing problem), multivariate Poisson families\nand multinomials. Given a known probability distribution $P$ and $n$ iid\nsamples drawn from an unknown probability distribution $Q$, we investigate how\nlarge $\\rho$ should be to distinguish, with high probability, the case $P=Q$\nfrom the case $d(P,Q) \\geq \\rho $. We answer this question in the case of a\nfamily of distances: $d(P,Q) = \\|P-Q\\|_t$ for $t \\in [1,2]$. Besides being\nlocally minimax-optimal (i.e. characterizing the detection threshold associated\nto the known matrix $P$), our tests have simple expressions and are easily\nimplementable. Our results are closely related to important and popular results\nin the multinomial setting, and complete them by providing the missing matching\nupper and lower bounds.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 15:57:19 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chhor", "J.", ""], ["Carpentier", "A.", ""]]}, {"id": "2012.13777", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "General formulas for the central and non-central moments of the\n  multinomial distribution", "comments": "9 pages, 0 figures", "journal-ref": "Stats 4 (2021), no. 1, 18-27", "doi": "10.3390/stats4010002", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first general formulas for the central and non-central moments\nof the multinomial distribution, using a combinatorial argument and the\nfactorial moments previously obtained in Mosimann (1962). We use the formulas\nto give explicit expressions for all the non-central moments up to order 8 and\nall the central moments up to order 4. These results expand significantly on\nthose in Newcomer (2008) and Newcomer et al. (2008), where the non-central\nmoments were calculated up to order 4.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:34:25 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 08:27:47 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2012.13982", "submitter": "Tong Zhang", "authors": "Cong Fang and Hanze Dong and Tong Zhang", "title": "Mathematical Models of Overparameterized Neural Networks", "comments": null, "journal-ref": "Proceedings of the IEEE, 2021", "doi": "10.1109/JPROC.2020.3048020", "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has received considerable empirical successes in recent years.\nHowever, while many ad hoc tricks have been discovered by practitioners, until\nrecently, there has been a lack of theoretical understanding for tricks\ninvented in the deep learning literature. Known by practitioners that\noverparameterized neural networks are easy to learn, in the past few years\nthere have been important theoretical developments in the analysis of\noverparameterized neural networks. In particular, it was shown that such\nsystems behave like convex systems under various restricted settings, such as\nfor two-layer NNs, and when learning is restricted locally in the so-called\nneural tangent kernel space around specialized initializations. This paper\ndiscusses some of these recent progresses leading to significant better\nunderstanding of neural networks. We will focus on the analysis of two-layer\nneural networks, and explain the key mathematical models, with their\nalgorithmic implications. We will then discuss challenges in understanding deep\nneural networks and some current research directions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:48:31 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Fang", "Cong", ""], ["Dong", "Hanze", ""], ["Zhang", "Tong", ""]]}, {"id": "2012.14051", "submitter": "Saeid Sedighi", "authors": "Saeid Sedighi, M. R. Bhavani Shankar, Mojtaba Soltanalian, Bj\\\"orn\n  Ottersten", "title": "On the Performance of One-Bit DoA Estimation via Sparse Linear Arrays", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Direction of Arrival (DoA) estimation using Sparse Linear Arrays (SLAs) has\nrecently gained considerable attention in array processing thanks to their\ncapability to provide enhanced degrees of freedom in resolving uncorrelated\nsource signals. Additionally, deployment of one-bit Analog-to-Digital\nConverters (ADCs) has emerged as an important topic in array processing, as it\noffers both a low-cost and a low-complexity implementation. In this paper, we\nstudy the problem of DoA estimation from one-bit measurements received by an\nSLA. Specifically, we first investigate the identifiability conditions for the\nDoA estimation problem from one-bit SLA data and establish an equivalency with\nthe case when DoAs are estimated from infinite-bit unquantized measurements.\nTowards determining the performance limits of DoA estimation from one-bit\nquantized data, we derive a pessimistic approximation of the corresponding\nCram\\'{e}r-Rao Bound (CRB). This pessimistic CRB is then used as a benchmark\nfor assessing the performance of one-bit DoA estimators. We also propose a new\nalgorithm for estimating DoAs from one-bit quantized data. We investigate the\nanalytical performance of the proposed method through deriving a closed-form\nexpression for the covariance matrix of the asymptotic distribution of the DoA\nestimation errors and show that it outperforms the existing algorithms in the\nliterature. Numerical simulations are provided to validate the analytical\nderivations and corroborate the resulting performance improvement.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 01:24:28 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sedighi", "Saeid", ""], ["Shankar", "M. R. Bhavani", ""], ["Soltanalian", "Mojtaba", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "2012.14081", "submitter": "Pedro Ramos", "authors": "Eduardo Ramos, Osafu A. Egbon, Pedro L. Ramos, Francisco A. Rodrigues,\n  Francisco Louzada", "title": "Objective Bayesian Analysis for the Differential Entropy of the Gamma\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of entropy related concepts goes from physics, such as in statistical\nmechanics, to evolutionary biology. The Shannon entropy is a measure used to\nquantify the amount of information in a system, and its estimation is usually\nmade under the frequentist approach. In the present paper, we introduce an\nfully objective Bayesian analysis to obtain this measure's posterior\ndistribution. Notably, we consider the Gamma distribution, which describes many\nnatural phenomena in physics, engineering, and biology. We reparametrize the\nmodel in terms of entropy, and different objective priors are derived, such as\nJeffreys prior, reference prior, and matching priors. Since the obtained priors\nare improper, we prove that the obtained posterior distributions are proper and\ntheir respective posterior means are finite. An intensive simulation study is\nconducted to select the prior that returns better results in terms of bias,\nmean square error, and coverage probabilities. The proposed approach is\nillustrated in two datasets, where the first one is related to the Achaemenid\ndynasty reign period, and the second data describes the time to failure of an\nelectronic component in the sugarcane harvest machine.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 03:47:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ramos", "Eduardo", ""], ["Egbon", "Osafu A.", ""], ["Ramos", "Pedro L.", ""], ["Rodrigues", "Francisco A.", ""], ["Louzada", "Francisco", ""]]}, {"id": "2012.14118", "submitter": "Jad Beyhum", "authors": "Jad Beyhum", "title": "High-dimensional inference robust to outliers with l1-norm penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference in the high-dimensional linear regression model\nwith outliers. Sparsity constraints are imposed on the vector of coefficients\nof the covariates. The number of outliers can grow with the sample size while\ntheir proportion goes to 0. We propose a two-step procedure for inference on\nthe coefficients of a fixed subset of regressors. The first step is a based on\nseveral square-root lasso l1-norm penalized estimators, while the second step\nis the ordinary least squares estimator applied to a well chosen regression. We\nestablish asymptotic normality of the two-step estimator. The proposed\nprocedure is efficient in the sense that it attains the semiparametric\nefficiency bound when applied to the model without outliers under\nhomoscedasticity. This approach is also computationally advantageous, it\namounts to solving a finite number of convex optimization programs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 06:51:55 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:33:22 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 15:17:57 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Beyhum", "Jad", ""]]}, {"id": "2012.14238", "submitter": "Nirian Mart\\'in", "authors": "Nirian Mart\\'in", "title": "Rao's Score Tests on Correlation Matrices", "comments": "The second version has mainly two important changes with respect to\n  the first one: a) A new paragraph which justifies the completely novel of the\n  current paper. b) Correction of a factor of the test-statistic which depends\n  only on $\\beta$ and p", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Even though the Rao's score tests are classical tests, such as the likelihood\nratio tests, their application has been avoided until now in a multivariate\nframework, in particular high-dimensional setting. We consider they could play\nan important role for testing high-dimensional data, but currently the\nclassical Rao's score tests for an arbitrary but fixed dimension remain being\nstill not very well-known for tests on correlation matrices of multivariate\nnormal distributions. In this paper, we illustrate how to create Rao's score\ntests, focussed on testing correlation matrices, showing their asymptotic\ndistribution. Based on Basu et al. (2021), we do not only develop the classical\nRao's score tests, but also their robust version, Rao's $\\beta$-score tests.\nDespite of tedious calculations, their strength is the final simple expression,\nwhich is valid for any arbitrary but fixed dimension. In addition, we provide\nbasic formulas for creating easily other tests, either for other variants of\ncorrelation tests or for location or variability parameters. We perform a\nsimulation study with high-dimensional data and the results are compared to\nthose of the likelihood ratio test with a variety of distributions, either pure\nand contaminated. The study shows that the classical Rao's score test for\ncorrelation matrices seems to work properly not only under multivariate\nnormality but also under other multivariate distributions. Under perturbed\ndistributions, the Rao's $\\beta$-score tests outperform any classical test.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:12:44 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 04:30:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mart\u00edn", "Nirian", ""]]}, {"id": "2012.14310", "submitter": "Fabien Panloup", "authors": "Gilles Pages (LPSM (UMR\\_8001)), Fabien Panloup (LAREMA)", "title": "Unajusted Langevin algorithm with multiplicative noise: Total variation\n  and Wasserstein bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on non-asymptotic bounds related to the Euler scheme\nof an ergodic diffusion with a possibly multiplicative diffusion term\n(non-constant diffusion coefficient). More precisely, the objective of this\npaper is to control the distance of the standard Euler scheme with decreasing\nstep (usually called Unajusted Langevin Algorithm in the Monte-Carlo\nliterature) to the invariant distribution of such an ergodic diffusion. In an\nappropriate Lyapunov setting and under uniform ellipticity assumptions on the\ndiffusion coefficient, we establish (or improve) such bounds for Total\nVariation and L 1-Wasserstein distances in both multiplicative and additive and\nframeworks. These bounds rely on weak error expansions using Stochastic\nAnalysis adapted to decreasing step setting.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:47:43 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Pages", "Gilles", "", "LPSM"], ["Panloup", "Fabien", "", "LAREMA"]]}, {"id": "2012.14482", "submitter": "Nhat Ho", "authors": "Nhat Ho and Stephen G. Walker", "title": "Multivariate Smoothing via the Fourier Integral Theorem and Fourier\n  Kernel", "comments": "58 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the Fourier integral theorem, we present natural Monte Carlo\nestimators of multivariate functions including densities, mixing densities,\ntransition densities, regression functions, and the search for modes of\nmultivariate density functions (modal regression). Rates of convergence are\nestablished and, in many cases, provide superior rates to current standard\nestimators such as those based on kernels, including kernel density estimators\nand kernel regression functions. Numerical illustrations are presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 20:59:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2012.14521", "submitter": "Jingbo Liu", "authors": "Jingbo Liu", "title": "Minoration via Mixed Volumes and Cover's Problem for General Channels", "comments": "some minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for establishing lower bounds on the supremum of\nprocesses in terms of packing numbers by means of mixed-volume inequalities\n(the Alexandrov-Fenchel inequality). A simple and general bound in terms of\npacking numbers under the convex distance is derived, from which some known\nbounds on the Gaussian processes and the Rademacher processes can be recovered\nwhen the convex set is taken to be the ball or the hypercube. However, the main\nthrust for our study of this approach is to handle non-i.i.d.\\ (noncanonical)\nprocesses (correspondingly, the convex set is not a product set). As an\napplication, we give a complete solution to an open problem of Thomas Cover in\n1987 about the capacity of a relay channel in the general discrete memoryless\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 23:07:47 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 04:17:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Jingbo", ""]]}, {"id": "2012.14530", "submitter": "S. Novak", "authors": "S.Y. Novak", "title": "On the T-test", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $T$-test is probably the most popular statistical test; it is routinely\nrecommended by the textbooks. The applicability of the test relies upon the\nvalidity of normal or Student's approximation to the distribution of Student's\nstatistic $\\,t_n$. However, the latter assumption is not valid as often as\nassumed. We show that normal or Student's approximation to $\\,\\L(t_n)\\,$ does\nnot hold uniformly even in the class $\\,{\\cal P}_n\\,$ of samples from zero-mean\nunit-variance bounded distributions. We present lower bounds to the\ncorresponding error. The fact that a non-parametric test is not applicable\nuniformly to samples from the class $\\,{\\cal P}_n\\,$ seems to be established\nfor the first time. It means the $T$-test can be misleading, and should not be\nrecommended in its present form. We suggest a generalisation of the test that\nallows for variability of possible limiting/approximating distributions to\n$\\,\\L(t_n)$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 23:33:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Novak", "S. Y.", ""]]}, {"id": "2012.14563", "submitter": "Munir Hiabu", "authors": "Munir Hiabu, Enno Mammen, Joseph T. Meyer", "title": "Random Planted Forest: a directly interpretable tree ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel interpretable and tree-based algorithm for prediction in\na regression setting in which each tree in a classical random forest is\nreplaced by a family of planted trees that grow simultaneously. The motivation\nfor our algorithm is to estimate the unknown regression function from a\nfunctional ANOVA decomposition perspective, where each tree corresponds to a\nfunction within that decomposition. Therefore, planted trees are limited in the\nnumber of interaction terms. The maximal order of approximation in the ANOVA\ndecomposition can be specified or left unlimited. If a first order\napproximation is chosen, the result is an additive model. In the other extreme\ncase, if the order of approximation is not limited, the resulting model puts no\nrestrictions on the form of the regression function. In a simulation study we\nfind encouraging prediction and visualisation properties of our random planted\nforest method. We also develop theory for an idealised version of random\nplanted forests in the case of an underlying additive model. We show that in\nthe additive case, the idealised version achieves up to a logarithmic factor\nasymptotically optimal one-dimensional convergence rates of order $n^{-2/5}$.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:51:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hiabu", "Munir", ""], ["Mammen", "Enno", ""], ["Meyer", "Joseph T.", ""]]}, {"id": "2012.14657", "submitter": "Clement Dombry", "authors": "Cl\\'ement Dombry (UBFC, LMB), Youssef Esstafa (ENSAI)", "title": "Behavior of linear L2-boosting algorithms in the vanishing learning rate\n  asymptotic", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic behaviour of gradient boosting algorithms when\nthe learning rate converges to zero and the number of iterations is rescaled\naccordingly. We mostly consider L2-boosting for regression with linear base\nlearner as studied in B{\\\"u}hlmann and Yu (2003) and analyze also a stochastic\nversion of the model where subsampling is used at each step (Friedman 2002). We\nprove a deterministic limit in the vanishing learning rate asymptotic and\ncharacterize the limit as the unique solution of a linear differential equation\nin an infinite dimensional function space. Besides, the training and test error\nof the limiting procedure are thoroughly analyzed. We finally illustrate and\ndiscuss our result on a simple numerical experiment where the linear\nL2-boosting operator is interpreted as a smoothed projection and time is\nrelated to its number of degrees of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 08:37:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dombry", "Cl\u00e9ment", "", "UBFC, LMB"], ["Esstafa", "Youssef", "", "ENSAI"]]}, {"id": "2012.14674", "submitter": "Pierre Bertrand", "authors": "Pierre Bertrand (LPSM), Michel Broniatowski (LPSM), Jean-Fran\\c{c}ois\n  Marcotorchino", "title": "Logical indetermination coupling: a method to minimize drawing matches\n  and its applications", "comments": "arXiv admin note: text overlap with arXiv:2007.08820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IT cs.SI math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While justifying that independence is a canonic coupling, the authors show\nthe existence of a second equilibrium to reduce the information conveyed from\nthe margins to the joined distribution: the so-called indetermination. They use\nthis null information property to apply indetermination to graph clustering.\nFurthermore, they break down a drawing under indetermination to emphasis it is\nthe best construction to reduce couple matchings, meaning, the expected number\nof equal couples drawn in a row. Using this property, they notice that\nindetermination appears in two problems (Guessing and Task Partitioning) where\ncouple matchings reduction is a key objective.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 09:20:12 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bertrand", "Pierre", "", "LPSM"], ["Broniatowski", "Michel", "", "LPSM"], ["Marcotorchino", "Jean-Fran\u00e7ois", ""]]}, {"id": "2012.14844", "submitter": "Anru R. Zhang", "authors": "Dong Xia and Anru R. Zhang and Yuchen Zhou", "title": "Inference for Low-rank Tensors -- No Need to Debias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the statistical inference for several low-rank\ntensor models. Specifically, in the Tucker low-rank tensor PCA or regression\nmodel, provided with any estimates achieving some attainable error rate, we\ndevelop the data-driven confidence regions for the singular subspace of the\nparameter tensor based on the asymptotic distribution of an updated estimate by\ntwo-iteration alternating minimization. The asymptotic distributions are\nestablished under some essential conditions on the signal-to-noise ratio (in\nPCA model) or sample size (in regression model). If the parameter tensor is\nfurther orthogonally decomposable, we develop the methods and theory for\ninference on each individual singular vector. For the rank-one tensor PCA\nmodel, we establish the asymptotic distribution for general linear forms of\nprincipal components and confidence interval for each entry of the parameter\ntensor. Finally, numerical simulations are presented to corroborate our\ntheoretical discoveries.\n  In all these models, we observe that different from many matrix/vector\nsettings in existing work, debiasing is not required to establish the\nasymptotic distribution of estimates or to make statistical inference on\nlow-rank tensors. In fact, due to the widely observed\nstatistical-computational-gap for low-rank tensor estimation, one usually\nrequires stronger conditions than the statistical (or information-theoretic)\nlimit to ensure the computationally feasible estimation is achievable.\nSurprisingly, such conditions ``incidentally\" render a feasible low-rank tensor\ninference without debiasing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:48:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xia", "Dong", ""], ["Zhang", "Anru R.", ""], ["Zhou", "Yuchen", ""]]}, {"id": "2012.14868", "submitter": "Aolin Xu", "authors": "Aolin Xu, Maxim Raginsky", "title": "Minimum Excess Risk in Bayesian Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the best achievable performance of Bayesian learning under\ngenerative models by defining and upper-bounding the minimum excess risk (MER):\nthe gap between the minimum expected loss attainable by learning from data and\nthe minimum expected loss that could be achieved if the model realization were\nknown. The definition of MER provides a principled way to define different\nnotions of uncertainties in Bayesian learning, including the aleatoric\nuncertainty and the minimum epistemic uncertainty. Two methods for deriving\nupper bounds for the MER are presented. The first method, generally suitable\nfor Bayesian learning with a parametric generative model, upper-bounds the MER\nby the conditional mutual information between the model parameters and the\nquantity being predicted given the observed data. It allows us to quantify the\nrate at which the MER decays to zero as more data becomes available. The second\nmethod, particularly suitable for Bayesian learning with a parametric\npredictive model, relates the MER to the deviation of the posterior predictive\ndistribution from the true predictive model, and further to the minimum\nestimation error of the model parameters from data. It explicitly shows how the\nuncertainty in model parameter estimation translates to the MER and to the\nfinal prediction uncertainty. We also extend the definition and analysis of MER\nto the setting with multiple parametric model families and the setting with\nnonparametric models. Along the discussions we draw some comparisons between\nthe MER in Bayesian learning and the excess risk in frequentist learning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:41:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xu", "Aolin", ""], ["Raginsky", "Maxim", ""]]}, {"id": "2012.15047", "submitter": "Linfan Zhang", "authors": "Linfan Zhang and Arash A. Amini", "title": "Adjusted chi-square test for degree-corrected block models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a goodness-of-fit test for degree-corrected stochastic block\nmodels (DCSBM). The test is based on an adjusted chi-square statistic for\nmeasuring equality of means among groups of $n$ multinomial distributions with\n$d_1,\\dots,d_n$ observations. In the context of network models, the number of\nmultinomials, $n$, grows much faster than the number of observations, $d_i$,\nhence the setting deviates from classical asymptotics. We show that a simple\nadjustment allows the statistic to converge in distribution, under null, as\nlong as the harmonic mean of $\\{d_i\\}$ grows to infinity. This result applies\nto large sparse networks where the role of $d_i$ is played by the degree of\nnode $i$. Our distributional results are nonasymptotic, with explicit\nconstants, providing finite-sample bounds on the Kolmogorov-Smirnov distance to\nthe target distribution. When applied sequentially, the test can also be used\nto determine the number of communities. The test operates on a (row) compressed\nversion of the adjacency matrix, conditional on the degrees, and as a result is\nhighly scalable to large sparse networks. We incorporate a novel idea of\ncompressing the columns based on a $(K+1)$-community assignment when testing\nfor $K$ communities. This approach increases the power in sequential\napplications without sacrificing computational efficiency, and we prove its\nconsistency in recovering the number of communities. Since the test statistic\ndoes not rely on a specific alternative, its utility goes beyond sequential\ntesting and can be used to simultaneously test against a wide range of\nalternatives outside the DCSBM family. We show the effectiveness of the\napproach by extensive numerical experiments with simulated and real data. In\nparticular, applying the test to the Facebook-100 dataset, we find that a DCSBM\nwith a small number of communities is far from a good fit in almost all cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 05:20:59 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Linfan", ""], ["Amini", "Arash A.", ""]]}, {"id": "2012.15085", "submitter": "Ying Jin", "authors": "Ying Jin, Zhuoran Yang, Zhaoran Wang", "title": "Is Pessimism Provably Efficient for Offline RL?", "comments": "60 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study offline reinforcement learning (RL), which aims to learn an optimal\npolicy based on a dataset collected a priori. Due to the lack of further\ninteractions with the environment, offline RL suffers from the insufficient\ncoverage of the dataset, which eludes most existing theoretical analysis. In\nthis paper, we propose a pessimistic variant of the value iteration algorithm\n(PEVI), which incorporates an uncertainty quantifier as the penalty function.\nSuch a penalty function simply flips the sign of the bonus function for\npromoting exploration in online RL, which makes it easily implementable and\ncompatible with general function approximators.\n  Without assuming the sufficient coverage of the dataset, we establish a\ndata-dependent upper bound on the suboptimality of PEVI for general Markov\ndecision processes (MDPs). When specialized to linear MDPs, it matches the\ninformation-theoretic lower bound up to multiplicative factors of the dimension\nand horizon. In other words, pessimism is not only provably efficient but also\nminimax optimal. In particular, given the dataset, the learned policy serves as\nthe \"best effort\" among all policies, as no other policies can do better. Our\ntheoretical analysis identifies the critical role of pessimism in eliminating a\nnotion of spurious correlation, which emerges from the \"irrelevant\"\ntrajectories that are less covered by the dataset and not informative for the\noptimal policy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:06:57 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 15:05:39 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Jin", "Ying", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""]]}, {"id": "2012.15467", "submitter": "Zhenzhen Li", "authors": "Thomas Y. Hou, Zhenzhen Li, Ziyun Zhang", "title": "Fast Global Convergence for Low-rank Matrix Recovery via Riemannian\n  Gradient Descent with Random Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new global analysis framework for a class of\nlow-rank matrix recovery problems on the Riemannian manifold. We analyze the\nglobal behavior for the Riemannian optimization with random initialization. We\nuse the Riemannian gradient descent algorithm to minimize a least squares loss\nfunction, and study the asymptotic behavior as well as the exact convergence\nrate. We reveal a previously unknown geometric property of the low-rank matrix\nmanifold, which is the existence of spurious critical points for the simple\nleast squares function on the manifold. We show that under some assumptions,\nthe Riemannian gradient descent starting from a random initialization with high\nprobability avoids these spurious critical points and only converges to the\nground truth in nearly linear convergence rate, i.e.\n$\\mathcal{O}(\\text{log}(\\frac{1}{\\epsilon})+ \\text{log}(n))$ iterations to\nreach an $\\epsilon$-accurate solution. We use two applications as examples for\nour global analysis. The first one is a rank-1 matrix recovery problem. The\nsecond one is a generalization of the Gaussian phase retrieval problem. It only\nsatisfies the weak isometry property, but has behavior similar to that of the\nfirst one except for an extra saddle set. Our convergence guarantee is nearly\noptimal and almost dimension-free, which fully explains the numerical\nobservations. The global analysis can be potentially extended to other data\nproblems with random measurement structures and empirical least squares loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:40:43 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:55:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hou", "Thomas Y.", ""], ["Li", "Zhenzhen", ""], ["Zhang", "Ziyun", ""]]}, {"id": "2012.15526", "submitter": "Artyom Kovalevskii", "authors": "Mikhail Chebunin, Artyom Kovalevskii", "title": "Asymptotics of sums of regression residuals under multiple ordering of\n  regressors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove theorems about the Gaussian asymptotics of an empirical bridge built\nfrom linear model regressors with multiple regressor ordering. We study the\ntesting of the hypothesis of a linear model for the components of a random\nvector: one of the components is a linear combination of the others up to an\nerror that does not depend on the other components of the random vector. The\nresults of observations of independent copies of a random vector are\nsequentially ordered in ascending order of several of its components. The\nresult is a sequence of vectors of higher dimension, consisting of induced\norder statistics (concomitants) corresponding to different orderings. For this\nsequence of vectors, without the assumption of a linear model for the\ncomponents, we prove a lemma of weak convergence of the distributions of an\nappropriately centered and normalized process to a centered Gaussian process\nwith almost surely continuous trajectories. Assuming a linear relationship of\nthe components, standard least squares estimates are used to compute regression\nresiduals -- the differences between response values and the predicted ones by\nthe linear model. We prove a theorem of weak convergence of the process of\nregression residuals under the necessary normalization to a centered Gaussian\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 10:26:17 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 08:47:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""]]}, {"id": "2012.15611", "submitter": "Alexander Kreiss", "authors": "Alexander Kreiss and Ingrid Van Keilegom", "title": "Semi-Parametric Estimation of Incubation and Generation Times by Means\n  of Laguerre Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In epidemics many interesting quantities, like the reproduction number,\ndepend on the incubation period (time from infection to symptom onset) and/or\nthe generation time (time until a new person is infected from another infected\nperson). Therefore, estimation of the distribution of these two quantities is\nof distinct interest. However, this is a challenging problem since it is\nnormally not possible to obtain precise observations of these two variables.\nInstead, in the beginning of a pandemic, it is possible to observe for\ninfection pairs the time of symptom onset for both people as well as a window\nfor infection of the first person (e.g. because of travel to a risk area). In\nthis paper we suggest a simple semi-parametric sieve-estimation method based on\nLaguerre-Polynomials for estimation of these distributions. We provide detailed\ntheory for consistency and illustrate the finite sample performance for small\ndatasets via a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 14:05:34 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 16:49:40 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kreiss", "Alexander", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2012.15664", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Peter W. MacDonald, Daniel Kessler", "title": "Inference post Selection of Group-sparse Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a post-selective Bayesian framework to jointly and consistently\nestimate parameters within automatic group-sparse regression models. Selected\nthrough an indispensable class of learning algorithms, e.g. the Group LASSO,\nthe overlapping Group LASSO, the sparse Group LASSO etc., uncertainty estimates\nfor the matched parameters are unreliable in the absence of adjustments for\nselection bias. Limiting however the application of state of the art tools for\nthe group-sparse problem include estimation strictly tailored to (i)\nreal-valued projections onto very specific selected subspaces, (ii) selection\nevents admitting representations as linear inequalities in the data variables.\nOur Bayesian methods address these gaps by deriving an adjustment factor in an\neasily feasible analytic form that eliminates bias from the selection of\npromising groups. Paying a very nominal price for this adjustment, experiments\non simulated data and the Human Connectome Project demonstrate the efficacy of\nour methods at a joint estimation of group-sparse parameters learned from data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:43:26 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 03:58:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["MacDonald", "Peter W.", ""], ["Kessler", "Daniel", ""]]}, {"id": "2012.15678", "submitter": "Masaaki Imaizumi", "authors": "Masaaki Imaizumi, Taisuke Otsu", "title": "On Gaussian Approximation for M-Estimator", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study develops a non-asymptotic Gaussian approximation theory for\ndistributions of M-estimators, which are defined as maximizers of empirical\ncriterion functions. In existing mathematical statistics literature, numerous\nstudies have focused on approximating the distributions of the M-estimators for\nstatistical inference. In contrast to the existing approaches, which mainly\nfocus on limiting behaviors, this study employs a non-asymptotic approach,\nestablishes abstract Gaussian approximation results for maximizers of empirical\ncriteria, and proposes a Gaussian multiplier bootstrap approximation method.\nOur developments can be considered as extensions of the seminal works\n(Chernozhukov, Chetverikov and Kato (2013, 2014, 2015)) on the approximation\ntheory for distributions of suprema of empirical processes toward their\nmaximizers. Through this work, we shed new lights on the statistical theory of\nM-estimators. Our theory covers not only regular estimators, such as the least\nabsolute deviations, but also some non-regular cases where it is difficult to\nderive or to approximate numerically the limiting distributions such as\nnon-Donsker classes and cube root estimators.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:56:45 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 09:24:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Imaizumi", "Masaaki", ""], ["Otsu", "Taisuke", ""]]}, {"id": "2012.15726", "submitter": "Geovani Rizk", "authors": "Geovani Rizk and Igor Colin and Albert Thomas and Moez Draief", "title": "Refined bounds for randomized experimental design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experimental design is an approach for selecting samples among a given set so\nas to obtain the best estimator for a given criterion. In the context of linear\nregression, several optimal designs have been derived, each associated with a\ndifferent criterion: mean square error, robustness, \\emph{etc}. Computing such\ndesigns is generally an NP-hard problem and one can instead rely on a convex\nrelaxation that considers probability distributions over the samples. Although\ngreedy strategies and rounding procedures have received a lot of attention,\nstraightforward sampling from the optimal distribution has hardly been\ninvestigated. In this paper, we propose theoretical guarantees for randomized\nstrategies on E and G-optimal design. To this end, we develop a new\nconcentration inequality for the eigenvalues of random matrices using a refined\nversion of the intrinsic dimension that enables us to quantify the performance\nof such randomized strategies. Finally, we evidence the validity of our\nanalysis through experiments, with particular attention on the G-optimal design\napplied to the best arm identification problem for linear bandits.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:37:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Rizk", "Geovani", ""], ["Colin", "Igor", ""], ["Thomas", "Albert", ""], ["Draief", "Moez", ""]]}, {"id": "2012.15802", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane", "title": "The Sample Complexity of Robust Covariance Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing the covariance matrix of a high-dimensional\nGaussian in a robust setting, where the input distribution has been corrupted\nin Huber's contamination model. Specifically, we are given i.i.d. samples from\na distribution of the form $Z = (1-\\epsilon) X + \\epsilon B$, where $X$ is a\nzero-mean and unknown covariance Gaussian $\\mathcal{N}(0, \\Sigma)$, $B$ is a\nfixed but unknown noise distribution, and $\\epsilon>0$ is an arbitrarily small\nconstant representing the proportion of contamination. We want to distinguish\nbetween the cases that $\\Sigma$ is the identity matrix versus $\\gamma$-far from\nthe identity in Frobenius norm.\n  In the absence of contamination, prior work gave a simple tester for this\nhypothesis testing task that uses $O(d)$ samples. Moreover, this sample upper\nbound was shown to be best possible, within constant factors. Our main result\nis that the sample complexity of covariance testing dramatically increases in\nthe contaminated setting. In particular, we prove a sample complexity lower\nbound of $\\Omega(d^2)$ for $\\epsilon$ an arbitrarily small constant and $\\gamma\n= 1/2$. This lower bound is best possible, as $O(d^2)$ samples suffice to even\nrobustly {\\em learn} the covariance. The conceptual implication of our result\nis that, for the natural setting we consider, robust hypothesis testing is at\nleast as hard as robust estimation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:24:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""]]}, {"id": "2012.15829", "submitter": "Aolin Xu", "authors": "Aolin Xu", "title": "Continuity of Generalized Entropy and Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the continuity property of the generalized entropy as a functional\nof the underlying probability distribution, defined with an action space and a\nloss function, and use this property to answer the basic questions in\nstatistical learning theory, the excess risk analyses for various learning\nmethods. We first derive upper and lower bounds for the entropy difference of\ntwo distributions in terms of several commonly used $f$-divergences, the\nWasserstein distance, and a distance that depends on the action space and the\nloss function. Examples are given along with the discussion of each general\nresult, comparisons are made with the existing entropy difference bounds, and\nnew mutual information upper bounds are derived based on the new results. We\nthen apply the entropy difference bounds to the theory of statistical learning.\nIt is shown that the excess risks in the two popular learning paradigms, the\nfrequentist learning and the Bayesian learning, both can be studied with the\ncontinuity property of different forms of the generalized entropy. The analysis\nis then extended to the continuity of generalized conditional entropy. The\nextension provides performance bounds for Bayes decision making with mismatched\ndistributions. It also leads to excess risk bounds for a third paradigm of\nlearning, where the decision rule is optimally designed under the projection of\nthe empirical distribution to a predefined family of distributions. We thus\nestablish a unified method of excess risk analysis for the three major\nparadigms of statistical learning, through the continuity of generalized\nentropy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:51:45 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xu", "Aolin", ""]]}]