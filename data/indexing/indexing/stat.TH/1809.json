[{"id": "1809.00411", "submitter": "Yinqiu He", "authors": "Yinqiu He, Gongjun Xu, Chong Wu and Wei Pan", "title": "Asymptotically Independent U-Statistics in High-Dimensional Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high-dimensional hypothesis tests aim to globally examine marginal or\nlow-dimensional features of a high-dimensional joint distribution, such as\ntesting of mean vectors, covariance matrices and regression coefficients. This\npaper constructs a family of U-statistics as unbiased estimators of the\n$\\ell_p$-norms of those features. We show that under the null hypothesis, the\nU-statistics of different finite orders are asymptotically independent and\nnormally distributed. Moreover, they are also asymptotically independent with\nthe maximum-type test statistic, whose limiting distribution is an extreme\nvalue distribution. Based on the asymptotic independence property, we propose\nan adaptive testing procedure which combines $p$-values computed from the\nU-statistics of different orders. We further establish power analysis results\nand show that the proposed adaptive procedure maintains high power against\nvarious alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 23:51:04 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 20:42:38 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 00:37:59 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 06:02:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["He", "Yinqiu", ""], ["Xu", "Gongjun", ""], ["Wu", "Chong", ""], ["Pan", "Wei", ""]]}, {"id": "1809.00463", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Shrinkage for Covariance Estimation: Asymptotics, Confidence Intervals,\n  Bounds and Applications in Sensor Monitoring and Finance", "comments": null, "journal-ref": "Statistical Papers, 2018, Vol. 59, 1441-1462", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When shrinking a covariance matrix towards (a multiple) of the identity\nmatrix, the trace of the covariance matrix arises naturally as the optimal\nscaling factor for the identity target. The trace also appears in other\ncontext, for example when measuring the size of a matrix or the amount of\nuncertainty.\n  Of particular interest is the case when the dimension of the covariance\nmatrix is large. Then the problem arises that the sample covariance matrix is\nsingular if the dimension is larger than the sample size. Another issue is that\nusually the estimation has to based on correlated time series data. We study\nthe estimation of the trace functional allowing for a high-dimensional time\nseries model, where the dimension is allowed to grow with the sample size -\nwithout any constraint. Based on a recent result, we investigate a confidence\ninterval for the trace, which also allows us to propose lower and upper bounds\nfor the shrinkage covariance estimator as well as bounds for the variance of\nprojections. In addition, we provide a novel result dealing with shrinkage\ntowards a diagonal target.\n  We investigate the accuracy of the confidence interval by a simulation study,\nwhich indicates good performance, and analyze three stock market data sets to\nillustrate the proposed bounds, where the dimension (number of stocks) ranges\nbetween $32$ and $475$. Especially, we apply the results to portfolio\noptimization and determine bounds for the risk associated to the\nvariance-minimizing portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 06:45:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1809.00734", "submitter": "Ivana Malenica", "authors": "Mark J. van der Laan and Ivana Malenica", "title": "Robust Estimation of Data-Dependent Causal Effects based on Observing a\n  Single Time-Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the case that one observes a single time-series, where at each time\nt one observes a data record O(t) involving treatment nodes A(t), possible\ncovariates L(t) and an outcome node Y(t). The data record at time t carries\ninformation for an (potentially causal) effect of the treatment A(t) on the\noutcome Y(t), in the context defined by a fixed dimensional summary measure\nCo(t). We are concerned with defining causal effects that can be consistently\nestimated, with valid inference, for sequentially randomized experiments\nwithout further assumptions. More generally, we consider the case when the\n(possibly causal) effects can be estimated in a double robust manner, analogue\nto double robust estimation of effects in the i.i.d. causal inference\nliterature. We propose a general class of averages of conditional\n(context-specific) causal parameters that can be estimated in a double robust\nmanner, therefore fully utilizing the sequential randomization. We propose a\ntargeted maximum likelihood estimator (TMLE) of these causal parameters, and\npresent a general theorem establishing the asymptotic consistency and normality\nof the TMLE. We extend our general framework to a number of typically studied\ncausal target parameters, including a sequentially adaptive design within a\nsingle unit that learns the optimal treatment rule for the unit over time. Our\nwork opens up robust statistical inference for causal questions based on\nobserving a single time-series on a particular unit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 22:02:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Malenica", "Ivana", ""]]}, {"id": "1809.00826", "submitter": "Jing Lv", "authors": "Li Jialiang and Lv Jing", "title": "High-dimensional varying index coefficient quantile regression model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning evolves quickly with more and more sophisticated models\nproposed to incorporate the complicated data structure from modern scientific\nand business problems. Varying index coefficient models extend varying\ncoefficient models and single index models, becoming the latest\nstate-of-the-art for semiparametric regression. This new class of models offers\ngreater flexibility to characterize complicated nonlinear interaction effects\nin regression analysis. To safeguard against outliers and extreme observations,\nwe consider a robust quantile regression approach to estimate the model\nparameters in this paper. High-dimensional loading parameters are allowed in\nour development under reasonable theoretical conditions. In addition, we\npropose a regularized estimation procedure to choose between linear and\nnon-linear forms for interaction terms. We can simultaneously select\nsignificant non-zero loading parameters and identify linear functions in\nvarying index coefficient models, in addition to estimate all the parametric\nand nonparametric components consistently. Under technical assumptions, we show\nthat the proposed procedure is consistent in variable selection as well as in\nlinear function identification, and the proposed parameter estimation enjoys\nthe oracle property. Extensive simulation studies are carried out to assess the\nfinite sample performance of the proposed method. We illustrate our methods\nwith an environmental health data example.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 08:03:19 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 03:16:20 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Jialiang", "Li", ""], ["Jing", "Lv", ""]]}, {"id": "1809.01223", "submitter": "Mansi Garg", "authors": "Mansi Garg and Isha Dewan", "title": "Testing for exponentiality for stationary associated random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of testing for exponentiality against\nunivariate positive ageing when the underlying sample consists of stationary\nassociated random variables. In particular, we discuss the asymptotic behavior\nof the tests by Deshpande (1983), Hollander and Proschan (1972) and Ahmad\n(1992) for testing exponentiality against IFRA, NBU and DMRL, respectively\nunder association. A simulation study illustrates the effect of dependence on\nthe asymptotic normality of the test statistics and on the size and power of\nthe tests.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 19:56:58 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Garg", "Mansi", ""], ["Dewan", "Isha", ""]]}, {"id": "1809.01317", "submitter": "Chengping Gong", "authors": "Chengping Gong, Chengxiu Ling", "title": "Robust estimations for the tail index of Weibull-type distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on suitable left-truncated or censored data, two flexible classes of\n$M$-estimations of Weibull tail coefficient are proposed with two additional\nparameters bounding the impact of extreme contamination. Asymptotic normality\nwith $\\sqrt {n}$-rate of convergence is obtained. Its robustness is discussed\nvia its asymptotic relative efficiency and influence function. It is further\ndemonstrated by a small scale of simulations and an empirical study on CRIX.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:31:13 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 14:13:59 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 01:46:58 GMT"}, {"version": "v4", "created": "Wed, 17 Oct 2018 08:24:05 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Gong", "Chengping", ""], ["Ling", "Chengxiu", ""]]}, {"id": "1809.01364", "submitter": "Chaohui Guo", "authors": "Jingwen Tu, Hu Yang, Chaohui Guo", "title": "Semiparametric model averaging for high dimensional conditional quantile\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a penalized high dimensional semiparametric model\naverage quantile prediction approach that is robust for forecasting the\nconditional quantile of the response. We consider a two-step estimation\nprocedure. In the first step, we use a local linear regression approach to\nestimate the individual marginal quantile functions, and approximate the\nconditional quantile of the response by an affine combination of\none-dimensional marginal quantile regression functions. In the second step,\nbased on the nonparametric kernel estimates of the marginal quantile regression\nfunctions, we utilize a penalized method to estimate the suitable model weights\nvector involved in the approximation. The objective of the second step is to\nselect significant variables whose marginal quantile functions make a\nsignificant contribution to estimating the joint multivariate conditional\nquantile function. Under some mild conditions, we have established the\nasymptotic properties of the proposed robust estimator. Finally, simulations\nand a real data analysis have been used to illustrate the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 07:45:33 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Tu", "Jingwen", ""], ["Yang", "Hu", ""], ["Guo", "Chaohui", ""]]}, {"id": "1809.01412", "submitter": "Lukas Steinberger", "authors": "Lukas Steinberger and Hannes Leeb", "title": "Conditional predictive inference for high-dimensional stable algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate generically applicable and intuitively appealing prediction\nintervals based on leave-one-out residuals. The conditional coverage\nprobability of the proposed interval, given the observations in the training\nsample, is close to the nominal level, provided that the underlying algorithm\nused for computing point predictions is sufficiently stable under the omission\nof single feature-response pairs. Our results are based on a finite sample\nanalysis of the empirical distribution function of the leave-one-out residuals\nand hold in a non-parametric setting with only minimal assumptions on the error\ndistribution. To illustrate our results, we also apply them to high-dimensional\nlinear predictors, where we obtain uniform asymptotic conditional validity as\nboth sample size and dimension tend to infinity at the same rate. These results\nshow that despite the serious problems of resampling procedures for inference\non the unknown parameters (cf. Bickel and Freedman, 1983; El Karoui and Purdom,\n2015; Mammen, 1996), leave-one-out methods can be successfully applied to\nobtain reliable predictive inference even in high dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 09:49:55 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 13:16:18 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Steinberger", "Lukas", ""], ["Leeb", "Hannes", ""]]}, {"id": "1809.01423", "submitter": "Qingqing Wu", "authors": "Qingqing Wu and Rui Zhang", "title": "Intelligent Reflecting Surface Enhanced Wireless Network: Joint Active\n  and Passive Beamforming Design", "comments": "IEEE GLOBECOM 2018. Please attend my presentation if you have the\n  interest in this direction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent reflecting surface (IRS) is envisioned to have abundant\napplications in future wireless networks by smartly reconfiguring the signal\npropagation for performance enhancement. Specifically, an IRS consists of a\nlarge number of low-cost passive elements each reflecting the incident signal\nwith a certain phase shift to collaboratively achieve beamforming and suppress\ninterference at one or more designated receivers. In this paper, we study an\nIRS-enhanced point-to-point multiple-input single-output (MISO) wireless system\nwhere one IRS is deployed to assist in the communication from a multi-antenna\naccess point (AP) to a single-antenna user. As a result, the user\nsimultaneously receives the signal sent directly from the AP as well as that\nreflected by the IRS. We aim to maximize the total received signal power at the\nuser by jointly optimizing the (active) transmit beamforming at the AP and\n(passive) reflect beamforming by the phase shifters at the IRS. We first\npropose a centralized algorithm based on the technique of semidefinite\nrelaxation (SDR) by assuming the global channel state information (CSI)\navailable at the IRS. Since the centralized implementation requires excessive\nchannel estimation and signal exchange overheads, we further propose a\nlow-complexity distributed algorithm where the AP and IRS independently adjust\nthe transmit beamforming and the phase shifts in an alternating manner until\nthe convergence is reached. Simulation results show that significant\nperformance gains can be achieved by the proposed algorithms as compared to\nbenchmark schemes. Moreover, it is verified that the IRS is able to drastically\nenhance the link quality and/or coverage over the conventional setup without\nthe IRS.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 10:30:33 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 09:29:58 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Wu", "Qingqing", ""], ["Zhang", "Rui", ""]]}, {"id": "1809.01455", "submitter": "Luc Pronzato", "authors": "Luc Pronzato (GdR MASCOT-NUM), Henry Wynn (LSE), Anatoly Zhigljavsky", "title": "Bregman divergences based on optimal design criteria and simplicial\n  measures of dispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work the authors defined the k-th order simplicial distance\nbetween probability distributions which arises naturally from a measure of\ndispersion based on the squared volume of random simplices of dimension k. This\ntheory is embedded in the wider theory of divergences and distances between\ndistributions which includes Kullback-Leibler, Jensen-Shannon, Jeffreys-Bregman\ndivergence and Bhattacharyya distance. A general construction is given based on\ndefining a directional derivative of a function $\\phi$ from one distribution to\nthe other whose concavity or strict concavity influences the properties of the\nresulting divergence. For the normal distribution these divergences can be\nexpressed as matrix formula for the (multivariate) means and covariances.\nOptimal experimental design criteria contribute a range of functionals applied\nto non-negative, or positive definite, information matrices. Not all can\ndistinguish normal distributions but sufficient conditions are given. The k-th\norder simplicial distance is revisited from this aspect and the results are\nused to test empirically the identity of means and covariances.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 12:18:33 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Pronzato", "Luc", "", "GdR MASCOT-NUM"], ["Wynn", "Henry", "", "LSE"], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1809.01588", "submitter": "Anna Seigal", "authors": "Max Pfeffer, Anna Seigal, Bernd Sturmfels", "title": "Learning Paths from Signature Tensors", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix congruence extends naturally to the setting of tensors. We apply\nmethods from tensor decomposition, algebraic geometry and numerical\noptimization to this group action. Given a tensor in the orbit of another\ntensor, we compute a matrix which transforms one to the other. Our primary\napplication is an inverse problem from stochastic analysis: the recovery of\npaths from their third order signature tensors. We establish identifiability\nresults, both exact and numerical, for piecewise linear paths, polynomial\npaths, and generic dictionaries. Numerical optimization is applied for recovery\nfrom inexact data. We also compute the shortest path with a given signature\ntensor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 15:52:35 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 10:28:57 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Pfeffer", "Max", ""], ["Seigal", "Anna", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1809.01796", "submitter": "Anru Zhang", "authors": "Anru Zhang and Rungang Han", "title": "Optimal Sparse Singular Value Decomposition for High-dimensional\n  High-order Data", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we consider the sparse tensor singular value decomposition,\nwhich aims for dimension reduction on high-dimensional high-order data with\ncertain sparsity structure. A method named Sparse Tensor Alternating\nThresholding for Singular Value Decomposition (STAT-SVD) is proposed. The\nproposed procedure features a novel double projection \\& thresholding scheme,\nwhich provides a sharp criterion for thresholding in each iteration. Compared\nwith regular tensor SVD model, STAT-SVD permits more robust estimation under\nweaker assumptions. Both the upper and lower bounds for estimation accuracy are\ndeveloped. The proposed procedure is shown to be minimax rate-optimal in a\ngeneral class of situations. Simulation studies show that STAT-SVD performs\nwell under a variety of configurations. We also illustrate the merits of the\nproposed procedure on a longitudinal tensor dataset on European country\nmortality rates.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 02:55:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhang", "Anru", ""], ["Han", "Rungang", ""]]}, {"id": "1809.01903", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock", "title": "Reversible Markov chains: variational representations and ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This pedagogical document explains three variational representations that are\nuseful when comparing the efficiencies of reversible Markov chains: (i) the\nDirichlet form and the associated variational representations of the spectral\ngaps; (ii) a variational representation of the asymptotic variance of an\nergodic average; and (iii) the conductance, and the equivalence of a non-zero\nconductance to a non-zero right spectral gap.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:35:07 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Sherlock", "Chris", ""]]}, {"id": "1809.01975", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel", "title": "A change-point problem and inference for segment signals", "comments": "arXiv admin note: substantial text overlap with arXiv:1404.6224", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of detection and estimation of one or two\nchange-points in the mean of a series of random variables. We use the formalism\nof set estimation in regression: To each point of a design is attached a binary\nlabel that indicates whether that point belongs to an unknown segment and this\nlabel is contaminated with noise. The endpoints of the unknown segment are the\nchange-points. We study the minimal size of the segment which allows\nstatistical detection in different scenarios, including when the endpoints are\nseparated from the boundary of the domain of the design, or when they are\nseparated from one another. We compare this minimal size with the minimax rates\nof convergence for estimation of the segment under the same scenarios. The aim\nof this extensive study of a simple yet fundamental version of the change-point\nproblem is twofold: Understanding the impact of the location and the separation\nof the change points on detection and estimation and bringing insights about\nthe estimation and detection of convex bodies in higher dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 14:58:04 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1809.02089", "submitter": "Russell Bowater", "authors": "Russell J. Bowater and Ludmila E. Guzm\\'an-Pantoja", "title": "Bayesian, classical and hybrid methods of inference when one parameter\n  value is special", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of making statistical inferences about a\nparameter when a narrow interval centred at a given value of the parameter is\nconsidered special, which is interpreted as meaning that there is a substantial\ndegree of prior belief that the true value of the parameter lies in this\ninterval. A clear justification of the practical importance of this problem is\nprovided. The main difficulty with the standard Bayesian solution to this\nproblem is discussed and, as a result, a pseudo-Bayesian solution is put\nforward based on determining lower limits for the posterior probability of the\nparameter lying in the special interval by means of a sensitivity analysis.\nSince it is not assumed that prior beliefs necessarily need to be expressed in\nterms of prior probabilities, nor that post-data probabilities must be Bayesian\nposterior probabilities, hybrid methods of inference are also proposed that are\nbased on specific ways of measuring and interpreting the classical concept of\nsignificance. The various methods that are outlined are compared and contrasted\nat both a foundational level, and from a practical viewpoint by applying them\nto real data from meta-analyses that appeared in a well-known medical article.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 16:57:28 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bowater", "Russell J.", ""], ["Guzm\u00e1n-Pantoja", "Ludmila E.", ""]]}, {"id": "1809.02241", "submitter": "Ouerdia Arkoun", "authors": "Ouerdia Arkoun, Jean-Yves Brua and Serguei Pergamenshchikov", "title": "Sequential Model Selection Method for Nonparametric Autoregression", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper for the first time the nonparametric autoregression estimation\nproblem for the quadratic risks is considered. To this end we develop a new\nadaptive sequential model selection method based on the efficient sequential\nkernel estimators proposed by Arkoun and Pergamenshchikov (2016). Moreover, we\ndevelop a new analytical tool for general regression models to obtain the non\nasymptotic sharp or- acle inequalities for both usual quadratic and robust\nquadratic risks. Then, we show that the constructed sequential model selection\nproce- dure is optimal in the sense of oracle inequalities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 22:34:25 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Arkoun", "Ouerdia", ""], ["Brua", "Jean-Yves", ""], ["Pergamenshchikov", "Serguei", ""]]}, {"id": "1809.02360", "submitter": "Sebastian Holtz", "authors": "Sebastian Holtz", "title": "Asymptotic efficiency for covariance estimation under noise and\n  asynchronicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the covariance structure from a discretely observed\nmultivariate Gaussian process under asynchronicity and noise is analysed under\nhigh-frequency asymptotics. Asymptotic lower and upper bounds are established\nfor a general Gaussian framework which provides benchmark cases for various\nGaussian process models of interest. The parametric bounds give rise to\ninfinite-dimensional convolution theorems for covariation estimation under\nasynchronicity, which is an essential estimation problem in finance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 08:55:06 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 13:05:27 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 19:16:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Holtz", "Sebastian", ""]]}, {"id": "1809.02443", "submitter": "Thomas Staudt", "authors": "Johannes Schmidt-Hieber, Laura Fee Schneider, Thomas Staudt, Andrea\n  Krajina, Timo Aspelmeier, and Axel Munk", "title": "Posterior analysis of $n$ in the binomial $(n,p)$ problem with both\n  parameters unknown -- with applications to quantitative nanoscopy", "comments": "66 pages; 37 pages main text and 29 pages supplement; contains link\n  to a supplementary microscopy video", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the population size $n$ from $k$ i.i.d.\\ binomial observations\nwith unknown success probability $p$ is relevant to a multitude of applications\nand has a long history. Without additional prior information this is a\nnotoriously difficult task when $p$ becomes small, and the Bayesian approach\nbecomes particularly useful. For a large class of priors, we establish\nposterior contraction and a Bernstein-von Mises type theorem in a setting where\n$p\\rightarrow0$ and $n\\rightarrow\\infty$ as $k\\to\\infty$. Furthermore, we\nsuggest a new class of Bayesian estimators for $n$ and provide a comprehensive\nsimulation study in which we investigate their performance. To showcase the\nadvantages of a Bayesian approach on real data, we also benchmark our\nestimators in a novel application from super-resolution microscopy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:44:14 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 16:53:09 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:38:38 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""], ["Schneider", "Laura Fee", ""], ["Staudt", "Thomas", ""], ["Krajina", "Andrea", ""], ["Aspelmeier", "Timo", ""], ["Munk", "Axel", ""]]}, {"id": "1809.02459", "submitter": "Laura Fee Schneider", "authors": "Laura Fee Schneider, Thomas Staudt, and Axel Munk", "title": "Posterior Consistency in the Binomial $(n,p)$ Model with Unknown $n$ and\n  $p$: A Numerical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the parameters from $k$ independent Bin$(n,p)$ random variables,\nwhen both parameters $n$ and $p$ are unknown, is relevant to a variety of\napplications. It is particularly difficult if $n$ is large and $p$ is small.\nOver the past decades, several articles have proposed Bayesian approaches to\nestimate $n$ in this setting, but asymptotic results could only be established\nrecently in \\cite{Schneider}. There, posterior contraction for $n$ is proven in\nthe problematic parameter regime where $n\\rightarrow\\infty$ and $p\\rightarrow0$\nat certain rates. In this article, we study numerically how far the theoretical\nupper bound on $n$ can be relaxed in simulations without losing posterior\nconsistency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 13:25:52 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Schneider", "Laura Fee", ""], ["Staudt", "Thomas", ""], ["Munk", "Axel", ""]]}, {"id": "1809.02686", "submitter": "Karol Dziedziul", "authors": "Bogdan \\'Cmiel, Karol Dziedziul, Natalia Jarz\\k{e}bkowska", "title": "Multiresolution analysis and adaptive estimation on a sphere using\n  stereographic wavelets", "comments": null, "journal-ref": "Nonlinear Analysis Volume 179, February 2019, Pages 41-71", "doi": "10.1016/j.na.2018.08.003", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an adaptive estimator of a density function on $d$ dimensional\nunit sphere $S^d$ ($d \\geq 2 $), using a new type of spherical frames. The\nframes, or as we call them, stereografic wavelets are obtained by transforming\na wavelet system, namely Daubechies, using some stereographic operators. We\nprove that our estimator achieves an optimal rate of convergence on some Besov\ntype class of functions by adapting to unknown smoothness. Our new construction\nof stereografic wavelet system gives us a multiresolution approximation of\n$L^2(S^d)$ which can be used in many approximation and estimation problems. In\nthis paper we also demonstrate how to implement the density estimator in $S^2$\nand we present a finite sample behavior of that estimator in a numerical\nexperiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:37:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 19:46:37 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["\u0106miel", "Bogdan", ""], ["Dziedziul", "Karol", ""], ["Jarz\u0119bkowska", "Natalia", ""]]}, {"id": "1809.02691", "submitter": "Karol Dziedziul", "authors": "Bogdan \\'Cmiel, Karol Dziedziul, Barbara Wolnik", "title": "The smoothness test for a density function", "comments": null, "journal-ref": "Nonlinear Analysis Volume 104, July 2014, Pages 21-39", "doi": "10.1016/j.na.2014.03.004", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing hypothesis that a density function has no more than\n$\\mu$ derivatives versus it has more than $\\mu$ derivatives is considered. For\na solution, the $L^2$ norms of wavelet orthogonal projections on some\northogonal \"differences\" of spaces from a multiresolution analysis is used. For\nthe construction of the smoothness test an asymptotic distribution of a\nsmoothness estimator is used. To analyze that asymptotic distribution, a new\ntechnique of enrichment procedure is proposed. The finite sample behaviour of\nthe smoothness test is demonstrated in a numerical experiment in case of\ndetermination if a density function is continues or discontinues.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 21:51:25 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["\u0106miel", "Bogdan", ""], ["Dziedziul", "Karol", ""], ["Wolnik", "Barbara", ""]]}, {"id": "1809.02741", "submitter": "Roberto Imbuzeiro Oliveira", "authors": "Alexandre Belloni, Roberto I. Oliveira", "title": "A high dimensional Central Limit Theorem for martingales, with\n  applications to context tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a central limit theorem for (a sequence of) multivariate\nmartingales which dimension potentially grows with the length $n$ of the\nmartingale. A consequence of the results are Gaussian couplings and a\nmultiplier bootstrap for the maximum of a multivariate martingale whose\ndimensionality $d$ can be as large as $e^{n^c}$ for some $c>0$. We also develop\nnew anti-concentration bounds for the maximum component of a high-dimensional\nGaussian vector, which we believe is of independent interest.\n  The results are applicable to a variety of settings. We fully develop its use\nto the estimation of context tree models (or variable length Markov chains) for\ndiscrete stationary time series. Specifically, we provide a bootstrap-based\nrule to tune several regularization parameters in a theoretically valid\nLepski-type method. Such bootstrap-based approach accounts for the correlation\nstructure and leads to potentially smaller penalty choices, which in turn\nimprove the estimation of the transition probabilities.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 02:15:18 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Belloni", "Alexandre", ""], ["Oliveira", "Roberto I.", ""]]}, {"id": "1809.02820", "submitter": "Eduardo Horta", "authors": "Eduardo Horta, Flavio Ziegelmann", "title": "Mixing conditions of conjugate processes", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give sufficient conditions ensuring that a $\\psi$-mixing property holds\nfor the sequence of empirical CDFs associated to a conjugate process.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 15:10:39 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 20:46:32 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 17:41:39 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Horta", "Eduardo", ""], ["Ziegelmann", "Flavio", ""]]}, {"id": "1809.02852", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Alain Desgagn\\'e, Pierre Lafaye de Micheaux, Fr\\'ed\\'eric Ouimet", "title": "Asymptotic law of a modified score statistic for the asymmetric power\n  distribution with unknown location and scale parameters", "comments": "15 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an i.i.d. sample of observations, we study a modified score statistic\nthat tests the goodness-of-fit of a given exponential power distribution\nagainst a family of alternatives, called the asymmetric power distribution. The\nfamily of alternatives was introduced in Komunjer (2007) and is a\nreparametrization of the skewed exponential power distribution from Fern\\'andez\net al. (1995) and Kotz et al. (2001). The score is modified in the sense that\nthe location and scale parameters (assumed to be unknown) are replaced by their\nmaximum likelihood estimators. We find the asymptotic law of the modified score\nstatistic under the null hypothesis ($H_0$) and under local alternatives, using\nthe notion of contiguity. Our work generalizes and extends the findings of\nDesgagn\\'e & Lafaye de Micheaux (2018), where the data points were normally\ndistributed under $H_0$. The special case where each data point has a Laplace\ndistribution under $H_0$ is the hardest to treat and requires a recent result\nfrom Lafaye de Micheaux & Ouimet (2018) on a uniform law of large numbers for\nsummands that blow up.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 18:43:12 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Desgagn\u00e9", "Alain", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1809.02857", "submitter": "Vincent Vu", "authors": "Vincent Q. Vu", "title": "Computational Sufficiency, Reflection Groups, and Generalized Lasso\n  Penalties", "comments": "28 page, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimators with generalized lasso penalties within the computational\nsufficiency framework introduced by Vu (2018, arXiv:1807.05985). By\nrepresenting these penalties as support functions of zonotopes and more\ngenerally Minkowski sums of line segments and rays, we show that there is a\nnatural reflection group associated with the underlying optimization problem. A\nconsequence of this point of view is that for large classes of estimators\nsharing the same penalty, the penalized least squares estimator is\ncomputationally minimal sufficient. This means that all such estimators can be\ncomputed by refining the output of any algorithm for the least squares case. An\ninteresting technical component is our analysis of coordinate descent on the\ndual problem. A key insight is that the iterates are obtained by reflecting and\naveraging, so they converge to an element of the dual feasible set that is\nminimal with respect to a ordering induced by the group associated with the\npenalty. Our main application is fused lasso/total variation denoising and\nisotonic regression on arbitrary graphs. In those cases the associated group is\na permutation group.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 19:31:27 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Vu", "Vincent Q.", ""]]}, {"id": "1809.02963", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi", "title": "Variational Approximation Error in Bayesian Non-negative Matrix\n  Factorization", "comments": "21 pages. 1 table. Revision in Neural Networks", "journal-ref": "Neural Networks, Volume 126, June 2020, pp. 65-75", "doi": "10.1016/j.neunet.2020.03.009", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a knowledge discovery method that\nis used in many fields. Variational inference and Gibbs sampling methods for it\nare also wellknown. However, the variational approximation error has not been\nclarified yet, because NMF is not statistically regular and the prior\ndistribution used in variational Bayesian NMF (VBNMF) has zero or divergence\npoints. In this paper, using algebraic geometrical methods, we theoretically\nanalyze the difference in negative log evidence (a.k.a. free energy) between\nVBNMF and Bayesian NMF, i.e., the Kullback-Leibler divergence between the\nvariational posterior and the true posterior. We derive an upper bound for the\nlearning coefficient (a.k.a. the real log canonical threshold) in Bayesian NMF.\nBy using the upper bound, we find a lower bound for the approximation error,\nasymptotically. The result quantitatively shows how well the VBNMF algorithm\ncan approximate Bayesian NMF; the lower bound depends on the hyperparameters\nand the true nonnegative rank. A numerical experiment demonstrates the\ntheoretical result.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 12:56:37 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 11:17:07 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 12:31:34 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 11:00:13 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hayashi", "Naoki", ""]]}, {"id": "1809.03073", "submitter": "Bryon Aragam", "authors": "Chen Dan, Liu Leqi, Bryon Aragam, Pradeep Ravikumar, Eric P. Xing", "title": "Sample Complexity of Nonparametric Semi-Supervised Learning", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of semi-supervised learning (SSL) and\nintroduce new assumptions based on the mismatch between a mixture model learned\nfrom unlabeled data and the true mixture model induced by the (unknown) class\nconditional distributions. Under these assumptions, we establish an\n$\\Omega(K\\log K)$ labeled sample complexity bound without imposing parametric\nassumptions, where $K$ is the number of classes. Our results suggest that even\nin nonparametric settings it is possible to learn a near-optimal classifier\nusing only a few labeled samples. Unlike previous theoretical work which\nfocuses on binary classification, we consider general multiclass classification\n($K>2$), which requires solving a difficult permutation learning problem. This\npermutation defines a classifier whose classification error is controlled by\nthe Wasserstein distance between mixing measures, and we provide finite-sample\nresults characterizing the behaviour of the excess risk of this classifier.\nFinally, we describe three algorithms for computing these estimators based on a\nconnection to bipartite graph matching, and perform experiments to illustrate\nthe superiority of the MLE over the majority vote estimator.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 01:12:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Dan", "Chen", ""], ["Leqi", "Liu", ""], ["Aragam", "Bryon", ""], ["Ravikumar", "Pradeep", ""], ["Xing", "Eric P.", ""]]}, {"id": "1809.03145", "submitter": "Mohamed Ndaoud", "authors": "Mohamed Ndaoud and Alexandre B. Tsybakov", "title": "Optimal variable selection and adaptive noisy Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of high-dimensional linear regression models, we propose an\nalgorithm of exact support recovery in the setting of noisy compressed sensing\nwhere all entries of the design matrix are independent and identically\ndistributed standard Gaussian. This algorithm achieves the same conditions of\nexact recovery as the exhaustive search (maximal likelihood) decoder, and has\nan advantage over the latter of being adaptive to all parameters of the problem\nand computable in polynomial time. The core of our analysis consists in the\nstudy of the non-asymptotic minimax Hamming risk of variable selection. This\nallows us to derive a procedure, which is nearly optimal in a non-asymptotic\nminimax sense. Then, we develop its adaptive version, and propose a robust\nvariant of the method to handle datasets with outliers and heavy-tailed\ndistributions of observations. The resulting polynomial time procedure is near\noptimal, adaptive to all parameters of the problem and also robust.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:56:28 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 21:12:08 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Ndaoud", "Mohamed", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1809.03550", "submitter": "Jakub Mare\\v{c}ek", "authors": "Albert Akhriev and Jakub Marecek and Andrea Simonetto", "title": "Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and\n  Measurement Noise", "comments": "20 pages; camera-ready version + appendices", "journal-ref": "Proceedings of the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence, 2020", "doi": null, "report-no": null, "categories": "math.OC cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tracking of time-varying low-rank models of time-varying matrices, we\npresent a method robust to both uniformly-distributed measurement noise and\narbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking\nerror. In practice, our use of randomised coordinate descent is scalable and\nallows for encouraging results on changedetection net, a benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:00:34 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 13:16:17 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 16:22:33 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Akhriev", "Albert", ""], ["Marecek", "Jakub", ""], ["Simonetto", "Andrea", ""]]}, {"id": "1809.03600", "submitter": "Joel Horowitz", "authors": "Joel L. Horowitz", "title": "Non-Asymptotic Inference in Instrumental Variables Estimation", "comments": "33 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple method for carrying out inference in a wide\nvariety of possibly nonlinear IV models under weak assumptions. The method is\nnon-asymptotic in the sense that it provides a finite sample bound on the\ndifference between the true and nominal probabilities of rejecting a correct\nnull hypothesis. The method is a non-Studentized version of the Anderson-Rubin\ntest but is motivated and analyzed differently. In contrast to the conventional\nAnderson-Rubin test, the method proposed here does not require restrictive\ndistributional assumptions, linearity of the estimated model, or simultaneous\nequations. Nor does it require knowledge of whether the instruments are strong\nor weak. It does not require testing or estimating the strength of the\ninstruments. The method can be applied to quantile IV models that may be\nnonlinear and can be used to test a parametric IV model against a nonparametric\nalternative. The results presented here hold in finite samples, regardless of\nthe strength of the instruments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 21:20:12 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Horowitz", "Joel L.", ""]]}, {"id": "1809.03754", "submitter": "Djihad Benelmadani", "authors": "Djihad Benelmadani, Karim Benhenni, Sana Louhichi", "title": "The reproducing kernel Hilbert space approach in nonparametric\n  regression problems with correlated observations", "comments": "57 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of estimating the regression\nfunction in models with correlated observations. The data is obtained from\nseveral experimental units each of them forms a time series. We propose a new\nestimator based on the inverse of the autocovariance matrix of the\nobservations, assumed known and invertible. Using the properties of the\nReproducing Kernel Hilbert spaces, we give the asymptotic expressions of its\nbias and its variance. In addition, we give a theoretical comparison, by\ncalculating the IMSE, between this new estimator and the classical one proposed\nby Gasser and Muller. Finally, we conduct a simulation study to investigate the\nperformance of the proposed estimator and to compare it to the Gasser and\nMuller's estimator in a finite sample set.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:58:55 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 09:16:07 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Benelmadani", "Djihad", ""], ["Benhenni", "Karim", ""], ["Louhichi", "Sana", ""]]}, {"id": "1809.03759", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana and Fabio Rapallo", "title": "On the aberrations of mixed level Orthogonal Arrays with removed runs", "comments": "13 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an Orthogonal Array we analyze the aberrations of the sub-fractions\nwhich are obtained by the deletion of some of its points. We provide formulae\nto compute the Generalized Word-Length Pattern of any sub-fraction. In the case\nof the deletion of one single point, we provide a simple methodology to find\nwhich the best sub-fractions are according to the Generalized Minimum\nAberration criterion. We also study the effect of the deletion of 1, 2 or 3\npoints on some examples. The methodology does not put any restriction on the\nnumber of levels of each factor. It follows that any mixed level Orthogonal\nArray can be considered.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 09:15:37 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1809.03774", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "A few properties of sample variance", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic result is that the sample variance for i.i.d. observations is an\nunbiased estimator of the variance of the underlying distribution (see for\ninstance Casella and Berger (2002)). But what happens if the observations are\nneither independent nor identically distributed. What can we say? Can we in\nparticular compute explicitly the first two moments of the sample mean and\nhence generalize formulae provided in Tukey (1957a), Tukey (1957b) for the\nfirst two moments of the sample variance? We also know that the sample mean and\nvariance are independent if they are computed on an i.i.d. normal distribution.\nThis is one of the underlying assumption to derive the Student distribution\nStudent alias W. S. Gosset (1908). But does this result hold for any other\nunderlying distribution? Can we still have independent sample mean and variance\nif the distribution is not normal? This paper precisely answers these questions\nand extends previous work of Cho, Cho, and Eltinge (2004). We are able to\nderive a general formula for the first two moments and variance of the sample\nvariance under no specific assumption. We also provide a faster proof of a\nseminal result of Lukacs (1942) by using the log characteristic function of the\nunbiased sample variance estimator.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 10:04:36 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1809.03986", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis and Themis Gouleakis and Christos Tzamos and\n  Manolis Zampetakis", "title": "Efficient Statistics, in High Dimensions, from Truncated Samples", "comments": "Appeared at 59th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an efficient algorithm for the classical problem, going back to\nGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy the\nparameters of a multivariate normal distribution from truncated samples.\nTruncated samples from a $d$-variate normal ${\\cal\nN}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ means a samples is only revealed if it falls\nin some subset $S \\subseteq \\mathbb{R}^d$; otherwise the samples are hidden and\ntheir count in proportion to the revealed samples is also hidden. We show that\nthe mean $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ can be\nestimated with arbitrary accuracy in polynomial-time, as long as we have oracle\naccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variate\nnormal distribution. Additionally we show that without oracle access to $S$,\nany non-trivial estimation is impossible.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:42:43 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:39:09 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1809.04018", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "T-statistic for Autoregressive process", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the distribution of the t-statistic under the\nassumption of normal autoregressive distribution for the underlying discrete\ntime process. This result generalizes the classical result of the traditional\nt-distribution where the underlying discrete time process follows an\nuncorrelated normal distribution. However, for AR(1), the underlying process is\ncorrelated. All traditional results break down and the resulting t-statistic is\na new distribution that converges asymptotically to a normal. We give an\nexplicit formula for this new distribution obtained as the ratio of two\ndependent distribution (a normal and the distribution of the norm of another\nindependent normal distribution). We also provide a modified statistic that\nfollows a non central t-distribution. Its derivation comes from finding an\northogonal basis for the the initial circulant Toeplitz covariance matrix. Our\nfindings are consistent with the asymptotic distribution for the t-statistic\nderived for the asympotic case of large number of observations or zero\ncorrelation. This exact finding of this distribution has applications in\nmultiple fields and in particular provides a way to derive the exact\ndistribution of the Sharpe ratio under normal AR(1) assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 16:42:48 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1809.04090", "submitter": "Alexandra Suvorikova", "authors": "Melf Boeckel, Vladimir Spokoiny and Alexandra Suvorikova", "title": "Multivariate Brenier cumulative distribution functions and their\n  application to non-parametric testing", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel approach of construction of multivariate\ncumulative distribution functions, based on cyclical-monotone mapping of an\noriginal measure $\\mu \\in \\mathcal{P}^{ac}_2(\\mathbb{R}^d)$ to some target\nmeasure $\\nu \\in \\mathcal{P}^{ac}_2(\\mathbb{R}^d)$ , supported on a convex\ncompact subset of $\\mathbb{R}^d$. This map is referred to as $\\nu$-Brenier\ndistribution function ($\\nu$-BDF), whose counterpart under the one-dimensional\nsetting $d = 1$ is an ordinary CDF, with $\\nu$ selected as $\\mathcal{U}[0, 1]$,\na uniform distribution on $[0, 1]$. Following one-dimensional frame-work, a\nmultivariate analogue of Glivenko-Cantelli theorem is provided. A practical\napplicability of the theory is then illustrated by the development of a\nnon-parametric pivotal two-sample test, that is rested on $2$-Wasserstein\ndistance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 18:03:29 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Boeckel", "Melf", ""], ["Spokoiny", "Vladimir", ""], ["Suvorikova", "Alexandra", ""]]}, {"id": "1809.04140", "submitter": "Johannes Schmidt-Hieber", "authors": "Markus Reiss and Johannes Schmidt-Hieber", "title": "Nonparametric Bayesian analysis of the compound Poisson prior for\n  support boundary recovery", "comments": "The first version of arXiv:1703.08358 has been expanded and\n  rewritten. We decided to split it in two separate papers, a new version of\n  arXiv:1703.08358 and this article", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data from a Poisson point process with intensity $(x,y) \\mapsto n\n\\mathbf{1}(f(x)\\leq y),$ frequentist properties for the Bayesian reconstruction\nof the support boundary function $f$ are derived. We mainly study compound\nPoisson process priors with fixed intensity proving that the posterior\ncontracts with nearly optimal rate for monotone and piecewise constant support\nboundaries and adapts to H\\\"older smooth boundaries with smoothness index at\nmost one. We then derive a non-standard Bernstein-von Mises result for a\ncompound Poisson process prior and a function space with increasing parameter\ndimension. As an intermediate result the limiting shape of the posterior for\nrandom histogram type priors is obtained. In both settings, it is shown that\nthe marginal posterior of the functional $\\vartheta =\\int f$ performs an\nautomatic bias correction and contracts with a faster rate than the MLE. In\nthis case, $(1-\\alpha)$-credible sets are also asymptotic\n$(1-\\alpha)$-confidence intervals. As a negative result, it is shown that the\nfrequentist coverage of credible sets is lost for linear functions indicating\nthat credible sets only have frequentist coverage for priors that are\nspecifically constructed to match properties of the underlying true function.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:22:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Reiss", "Markus", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1809.04275", "submitter": "Hannes Leeb", "authors": "Hannes Leeb, Nina Senitschnig", "title": "Prediction out-of-sample using block shrinkage estimators: model\n  selection and predictive inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a linear regression model with random design, we consider a family of\ncandidate models from which we want to select a `good' model for prediction\nout-of-sample. We fit the models using block shrinkage estimators, and we focus\non the challenging situation where the number of explanatory variables can be\nof the same order as sample size and where the number of candidate models can\nbe much larger than sample size. We develop an estimator for the out-of-sample\npredictive performance, and we show that the empirically best model is\nasymptotically as good as the truly best model. Using the estimator\ncorresponding to the empirically best model, we construct a prediction interval\nthat is approximately valid and short with high probability, i.e., we show that\nthe actual coverage probability is close to the nominal one and that the length\nof this prediction interval is close to the length of the shortest but\ninfeasible prediction interval. All results hold uniformly over a large class\nof data-generating processes. These findings extend results of Leeb (2009),\nwhere the models are fit using least-squares estimators, and of Huber (2013),\nwhere the models are fit using shrinkage estimators without block structure.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 06:46:03 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Leeb", "Hannes", ""], ["Senitschnig", "Nina", ""]]}, {"id": "1809.04541", "submitter": "Dootika Vats", "authors": "Dootika Vats and James M. Flegal", "title": "Lugsail lag windows for estimating time-average covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:19:01 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 17:52:15 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 03:56:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Vats", "Dootika", ""], ["Flegal", "James M.", ""]]}, {"id": "1809.04669", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova", "title": "Prediction and estimation consistency of sparse multi-class penalized\n  optimal scoring", "comments": null, "journal-ref": "Bernoulli 2020, Vol. 26, No. 1, 286-322", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear discriminant analysis via penalized optimal scoring is a\nsuccessful tool for classification in high-dimensional settings. While the\nvariable selection consistency of sparse optimal scoring has been established,\nthe corresponding prediction and estimation consistency results have been\nlacking. We bridge this gap by providing probabilistic bounds on out-of-sample\nprediction error and estimation error of multi-class penalized optimal scoring\nallowing for diverging number of classes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 20:52:37 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""]]}, {"id": "1809.04775", "submitter": "Yusuke Uchiyama", "authors": "Yusuke Uchiyama and Takanori Kadoya", "title": "Superstatistics with cut-off tails for financial time series", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2019.04.166", "report-no": null, "categories": "q-fin.ST math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial time series have been investigated to follow fat-tailed\ndistributions. Further, an empirical probability distribution sometimes shows\ncut-off shapes on its tails. To describe this stylized fact, we incorporate the\ncut-off effect in superstatistics. Then we confirm that the presented\nstochastic model is capable of describing the statistical properties of real\nfinancial time series. In addition, we present an option pricing formula with\nrespect to superstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:57:42 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Uchiyama", "Yusuke", ""], ["Kadoya", "Takanori", ""]]}, {"id": "1809.05014", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer and Aryeh Kontorovich", "title": "Statistical Estimation of Ergodic Markov Chain Kernel over Discrete\n  State Space", "comments": "Journal version of the extended abstract (ALT'19), to appear in\n  Bernoulli 2020+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the statistical complexity of estimating the parameters of a\ndiscrete-state Markov chain kernel from a single long sequence of state\nobservations. In the finite case, we characterize (modulo logarithmic factors)\nthe minimax sample complexity of estimation with respect to the operator\ninfinity norm, while in the countably infinite case, we analyze the problem\nwith respect to a natural entry-wise norm derived from total variation. We show\nthat in both cases, the sample complexity is governed by the mixing properties\nof the unknown chain, for which, in the finite-state case, there are known\nfinite-sample estimators with fully empirical confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 15:37:19 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 13:57:14 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:42:03 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2020 08:13:15 GMT"}, {"version": "v5", "created": "Wed, 1 Jul 2020 09:37:22 GMT"}, {"version": "v6", "created": "Thu, 13 Aug 2020 09:04:36 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Wolfer", "Geoffrey", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1809.05032", "submitter": "Yoshimasa Uematsu", "authors": "Yingying Fan, Jinchi Lv, Mahrad Sharifvaghefi and Yoshimasa Uematsu", "title": "IPAD: Stable Interpretable Forecasting with Knockoffs Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability and stability are two important features that are desired in\nmany contemporary big data applications arising in economics and finance. While\nthe former is enjoyed to some extent by many existing forecasting approaches,\nthe latter in the sense of controlling the fraction of wrongly discovered\nfeatures which can enhance greatly the interpretability is still largely\nunderdeveloped in the econometric settings. To this end, in this paper we\nexploit the general framework of model-X knockoffs introduced recently in\nCand\\`{e}s, Fan, Janson and Lv (2018), which is nonconventional for\nreproducible large-scale inference in that the framework is completely free of\nthe use of p-values for significance testing, and suggest a new method of\nintertwined probabilistic factors decoupling (IPAD) for stable interpretable\nforecasting with knockoffs inference in high-dimensional models. The recipe of\nthe method is constructing the knockoff variables by assuming a latent factor\nmodel that is exploited widely in economics and finance for the association\nstructure of covariates. Our method and work are distinct from the existing\nliterature in that we estimate the covariate distribution from data instead of\nassuming that it is known when constructing the knockoff variables, our\nprocedure does not require any sample splitting, we provide theoretical\njustifications on the asymptotic false discovery rate control, and the theory\nfor the power analysis is also established. Several simulation examples and the\nreal data analysis further demonstrate that the newly suggested method has\nappealing finite-sample performance with desired interpretability and stability\ncompared to some popularly used forecasting methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 08:08:10 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Fan", "Yingying", ""], ["Lv", "Jinchi", ""], ["Sharifvaghefi", "Mahrad", ""], ["Uematsu", "Yoshimasa", ""]]}, {"id": "1809.05172", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla", "title": "Deterministic Inequalities for Smooth M-estimators", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever since the proof of asymptotic normality of maximum likelihood estimator\nby Cramer (1946), it has been understood that a basic technique of the Taylor\nseries expansion suffices for asymptotics of $M$-estimators with\nsmooth/differentiable loss function. Although the Taylor series expansion is a\npurely deterministic tool, the realization that the asymptotic normality\nresults can also be made deterministic (and so finite sample) received far less\nattention. With the advent of big data and high-dimensional statistics, the\nneed for finite sample results has increased. In this paper, we use the\n(well-known) Banach fixed point theorem to derive various deterministic\ninequalities that lead to the classical results when studied under randomness.\nIn addition, we provide applications of these deterministic inequalities for\ncrossvalidation/subsampling, marginal screening and uniform-in-submodel results\nthat are very useful for post-selection inference and in the study of\npost-regularization estimators. Our results apply to many classical estimators,\nin particular, generalized linear models, non-linear regression and cox\nproportional hazards model. Extensions to non-smooth and constrained problems\nare also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 20:39:37 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "1809.05224", "submitter": "Rahul Singh", "authors": "Victor Chernozhukov, Whitney K Newey, Rahul Singh", "title": "Automatic Debiased Machine Learning of Causal and Structural Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many causal and structural effects depend on regressions. Examples include\npolicy effects, average derivatives, regression decompositions, average\ntreatment effects, causal mediation, and parameters of economic structural\nmodels. The regressions may be high dimensional, making machine learning\nuseful. Plugging machine learners into identifying equations can lead to poor\ninference due to bias from regularization and/or model selection. This paper\ngives automatic debiasing for linear and nonlinear functions of regressions.\nThe debiasing is automatic in using Lasso and the function of interest without\nthe full form of the bias correction. The debiasing can be applied to any\nregression learner, including neural nets, random forests, Lasso, boosting, and\nother high dimensional methods. In addition to providing the bias correction we\ngive standard errors that are robust to misspecification, convergence rates for\nthe bias correction, and primitive conditions for asymptotic inference for\nestimators of a variety of estimators of structural and causal effects. The\nautomatic debiased machine learning is used to estimate the average treatment\neffect on the treated for the NSW job training data and to estimate demand\nelasticities from Nielsen scanner data while allowing preferences to be\ncorrelated with prices and income.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 02:24:07 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 02:11:29 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 02:05:09 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 12:43:21 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney K", ""], ["Singh", "Rahul", ""]]}, {"id": "1809.05450", "submitter": "Julien Bect", "authors": "Paul Feliot, Julien Bect (L2S, GdR MASCOT-NUM), Emmanuel Vazquez (L2S,\n  GdR MASCOT-NUM)", "title": "User preferences in Bayesian multi-objective optimization: the expected\n  weighted hypervolume improvement criterion", "comments": "To be published in the proceedings of LOD 2018 -- The Fourth\n  International Conference on Machine Learning, Optimization, and Data Science\n  -- September 13-16, 2018 -- Volterra, Tuscany, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a framework for taking into account user\npreferences in multi-objective Bayesian optimization in the case where the\nobjectives are expensive-to-evaluate black-box functions. A novel expected\nimprovement criterion to be used within Bayesian optimization algorithms is\nintroduced. This criterion, which we call the expected weighted hypervolume\nimprovement (EWHI) criterion, is a generalization of the popular expected\nhypervolume improvement to the case where the hypervolume of the dominated\nregion is defined using an absolutely continuous measure instead of the\nLebesgue measure. The EWHI criterion takes the form of an integral for which no\nclosed form expression exists in the general case. To deal with its\ncomputation, we propose an importance sampling approximation method. A sampling\ndensity that is optimal for the computation of the EWHI for a predefined set of\npoints is crafted and a sequential Monte-Carlo (SMC) approach is used to obtain\na sample approximately distributed from this density. The ability of the\ncriterion to produce optimization strategies oriented by user preferences is\ndemonstrated on a simple bi-objective test problem in the cases of a preference\nfor one objective and of a preference for certain regions of the Pareto front.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 14:55:13 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Feliot", "Paul", "", "L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S,\n  GdR MASCOT-NUM"]]}, {"id": "1809.05572", "submitter": "Jonathan Weed", "authors": "Philippe Rigollet, Jonathan Weed", "title": "Entropic optimal transport is maximum-likelihood deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a statistical interpretation of entropic optimal transport by showing\nthat performing maximum-likelihood estimation for Gaussian deconvolution\ncorresponds to calculating a projection with respect to the entropic optimal\ntransport distance. This structural result gives theoretical support for the\nwide adoption of these tools in the machine learning community.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:28:37 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 13:35:53 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Rigollet", "Philippe", ""], ["Weed", "Jonathan", ""]]}, {"id": "1809.05596", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Jaros{\\l}aw B{\\l}asiok", "title": "The Generic Holdout: Preventing False-Discoveries in Adaptive Data\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive data analysis has posed a challenge to science due to its ability to\ngenerate false hypotheses on moderately large data sets. In general, with\nnon-adaptive data analyses (where queries to the data are generated without\nbeing influenced by answers to previous queries) a data set containing $n$\nsamples may support exponentially many queries in $n$. This number reduces to\nlinearly many under naive adaptive data analysis, and even sophisticated\nremedies such as the Reusable Holdout (Dwork et. al 2015) only allow\nquadratically many queries in $n$.\n  In this work, we propose a new framework for adaptive science which\nexponentially improves on this number of queries under a restricted yet\nscientifically relevant setting, where the goal of the scientist is to find a\nsingle (or a few) true hypotheses about the universe based on the samples. Such\na setting may describe the search for predictive factors of some disease based\non medical data, where the analyst may wish to try a number of predictive\nmodels until a satisfactory one is found.\n  Our solution, the Generic Holdout, involves two simple ingredients: (1) a\npartitioning of the data into a exploration set and a holdout set and (2) a\nlimited exposure strategy for the holdout set. An analyst is free to use the\nexploration set arbitrarily, but when testing hypotheses against the holdout\nset, the analyst only learns the answer to the question: \"Is the given\nhypothesis true (empirically) on the holdout set?\" -- and no more information,\nsuch as \"how well\" the hypothesis fits the holdout set. The resulting scheme is\nimmediate to analyze, but despite its simplicity we do not believe our method\nis obvious, as evidenced by the many violations in practice.\n  Our proposal can be seen as an alternative to pre-registration, and allows\nresearchers to get the benefits of adaptive data analysis without the problems\nof adaptivity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 21:28:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Nakkiran", "Preetum", ""], ["B\u0142asiok", "Jaros\u0142aw", ""]]}, {"id": "1809.05638", "submitter": "Eric Janofsky", "authors": "Eric Janofsky", "title": "Learning high-dimensional graphical models with regularized quadratic\n  scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise Markov Random Fields (MRFs) or undirected graphical models are\nparsimonious representations of joint probability distributions. Variables\ncorrespond to nodes of a graph, with edges between nodes corresponding to\nconditional dependencies. Unfortunately, likelihood-based learning and\ninference is hampered by the intractability of computing the normalizing\nconstant. This paper considers an alternative scoring rule to the\nlog-likelihood, which obviates the need to compute the normalizing constant of\nthe distribution. We show that the rule is a quadratic function of the natural\nparameters. We optimize the sum of this scoring rule and a sparsity-inducing\nregularizer. For general continuous-valued exponential families, we provide\ntheoretical results on parameter and edge consistency. As a special case we\ndetail a new approach to sparse precision matrix estimation whose theoretical\nguarantees match that of the graphical lasso of Yuan and Lin (2007), with\nfaster computational performance than the glasso algorithm of Yuan (2010). We\nthen describe results for model selection in the nonparametric pairwise\ngraphical model using exponential series. The regularized score matching\nproblem is shown to be a convex program; we provide scalable algorithms based\non consensus alternating direction method of multipliers (Boyd et al. (2011))\nand coordinate-wise descent.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 03:14:14 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Janofsky", "Eric", ""]]}, {"id": "1809.05870", "submitter": "Jakub Mare\\v{c}ek", "authors": "Mark Kozdoba, Jakub Marecek, Tigran Tchrakian, and Shie Mannor", "title": "On-Line Learning of Linear Dynamical Systems: Exponential Forgetting in\n  Kalman Filters", "comments": null, "journal-ref": "Proceedings of the Thirty-Third AAAI Conference on Artificial\n  Intelligence, 2019. Pages: 4098-4105", "doi": "10.1609/aaai.v33i01.33014098", "report-no": null, "categories": "math.ST cs.AI cs.LG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kalman filter is a key tool for time-series forecasting and analysis. We show\nthat the dependence of a prediction of Kalman filter on the past is decaying\nexponentially, whenever the process noise is non-degenerate. Therefore, Kalman\nfilter may be approximated by regression on a few recent observations.\nSurprisingly, we also show that having some process noise is essential for the\nexponential decay. With no process noise, it may happen that the forecast\ndepends on all of the past uniformly, which makes forecasting more difficult.\n  Based on this insight, we devise an on-line algorithm for improper learning\nof a linear dynamical system (LDS), which considers only a few most recent\nobservations. We use our decay results to provide the first regret bounds\nw.r.t. to Kalman filters within learning an LDS. That is, we compare the\nresults of our algorithm to the best, in hindsight, Kalman filter for a given\nsignal. Also, the algorithm is practical: its per-update run-time is linear in\nthe regression depth.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 13:21:49 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Kozdoba", "Mark", ""], ["Marecek", "Jakub", ""], ["Tchrakian", "Tigran", ""], ["Mannor", "Shie", ""]]}, {"id": "1809.05952", "submitter": "Pietro Lenarda", "authors": "Pietro Lenarda, Giorgio Gnecco, Massimo Riccaboni", "title": "Parameters estimation in a 3-parameters p-star model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important issue in social network analysis refers to the development of\nalgorithms for estimating optimal parameters of a social network model, using\ndata available from the network itself. This entails solving an optimization\nproblem. In the paper, we propose a new method for parameters estimation in a\nspecific social network model, namely, the so-called p-star model with three\nparameters. The method is based on the mean-field approximation of the moments\nassociated with the three subgraphs defining the model, namely: the mean\nnumbers of edges, two-stars, and triangles. A modified gradient ascent method\nis applied to maximize the log-likelihood function of the p-star model, in\nwhich the components of the gradient are computed using approximate values of\nthe moments. Compared to other existing iterative methods for parameters\nestimation, which are computationally very expensive when the number of\nvertices becomes large, such as gradient ascent applied to maximum log-pseudo-\nlikelihood estimation, the proposed approach has the advantage of a much\ncheaper cost per iteration, which is practically independent of the number of\nvertices.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 20:34:13 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Lenarda", "Pietro", ""], ["Gnecco", "Giorgio", ""], ["Riccaboni", "Massimo", ""]]}, {"id": "1809.06019", "submitter": "Guang Cheng", "authors": "Meimei Liu, Jean Honorio, Guang Cheng", "title": "Statistically and Computationally Efficient Variance Estimator for\n  Kernel Ridge Regression", "comments": "To Appear in 2018 Allerton", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a random projection approach to estimate variance\nin kernel ridge regression. Our approach leads to a consistent estimator of the\ntrue variance, while being computationally more efficient. Our variance\nestimator is optimal for a large family of kernels, including cubic splines and\nGaussian kernels. Simulation analysis is conducted to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 04:53:46 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Meimei", ""], ["Honorio", "Jean", ""], ["Cheng", "Guang", ""]]}, {"id": "1809.06092", "submitter": "Kevin Kokot", "authors": "Holger Dette, Kevin Kokot, Stanislav Volgushev", "title": "Testing relevant hypotheses in functional time series via\n  self-normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop methodology for testing relevant hypotheses about\nfunctional time series in a tuning-free way. Instead of testing for exact\nequality, for example for the equality of two mean functions from two\nindependent time series, we propose to test the null hypothesis of no relevant\ndeviation. In the two sample problem this means that an $L^2$-distance between\nthe two mean functions is smaller than a pre-specified threshold. For such\nhypotheses self-normalization, which was introduced by Shao (2010) and Shao and\nZhang (2010) and is commonly used to avoid the estimation of nuisance\nparameters, is not directly applicable. We develop new self-normalized\nprocedures for testing relevant hypotheses in the one sample, two sample and\nchange point problem and investigate their asymptotic properties. Finite sample\nproperties of the proposed tests are illustrated by means of a simulation study\nand data examples. Our main focus is on functional time series, but extensions\nto other settings are also briefly discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:27:04 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 08:18:11 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 07:52:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dette", "Holger", ""], ["Kokot", "Kevin", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1809.06099", "submitter": "Jae Youn Ahn", "authors": "Jae Youn Ahn, Sebastian Fuchs", "title": "On Minimal Copulas under the Concordance Order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we study extreme negative dependence focussing on the\nconcordance order for copulas. With the absence of a least element for\ndimensions $d\\ge$ 3, the set of all minimal elements in the collection of all\ncopulas turns out to be a natural and quite important extreme negative\ndependence concept. We investigate several sufficient conditions and we provide\na necessary condition for a copula to be minimal: The sufficient conditions are\nrelated to the extreme negative dependence concept of d-countermonotonicity and\nthe necessary condition is related to the collection of all copulas minimizing\nmultivariate Kendall's tau. The concept of minimal copulas has already been\nproved to be useful in various continuous and concordance order preserving\noptimization problems including variance minimization and the detection of\nlower bounds for certain measures of concordance. We substantiate this key role\nof minimal copulas by showing that every continuous and concordance order\npreserving functional on copulas is minimized by some minimal copula and, in\nthe case the continuous functional is even strictly concordance order\npreserving, it is minimized by minimal copulas only. Applying the above\nresults, we may conclude that every minimizer of Spearman's rho is also a\nminimizer of Kendall's tau.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:46:15 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 01:49:38 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 14:32:01 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Ahn", "Jae Youn", ""], ["Fuchs", "Sebastian", ""]]}, {"id": "1809.06136", "submitter": "Koen Jochmans", "authors": "Koen Jochmans", "title": "Heteroskedasticity-robust inference in linear regression models with\n  many covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference in linear regression models that is robust to\nheteroskedasticity and the presence of many control variables. When the number\nof control variables increases at the same rate as the sample size the usual\nheteroskedasticity-robust estimators of the covariance matrix are inconsistent.\nHence, tests based on these estimators are size distorted even in large\nsamples. An alternative covariance-matrix estimator for such a setting is\npresented that complements recent work by Cattaneo, Jansson and Newey (2018).\nWe provide high-level conditions for our approach to deliver (asymptotically)\nsize-correct inference as well as more primitive conditions for three special\ncases. Simulation results and an empirical illustration to inference on the\nunion premium are also provided.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:29:21 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 11:29:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Jochmans", "Koen", ""]]}, {"id": "1809.06228", "submitter": "Justin Krometis", "authors": "Jeff Borggaard, Nathan E. Glatt-Holtz, Justin A. Krometis", "title": "On Bayesian Consistency for Flows Observed Through a Passive Scalar", "comments": "Additional remarks/discussion following statement of main result;\n  tweaks to some arguments; fix typos. arXiv admin note: substantial text\n  overlap with arXiv:1808.01084", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical inverse problem of estimating a background fluid\nflow field $\\mathbf{v}$ from the partial, noisy observations of the\nconcentration $\\theta$ of a substance passively advected by the fluid, so that\n$\\theta$ is governed by the partial differential equation \\[\n\\frac{\\partial}{\\partial t}{\\theta}(t,\\mathbf{x}) = -\\mathbf{v}(\\mathbf{x})\n\\cdot \\nabla \\theta(t,\\mathbf{x}) + \\kappa \\Delta \\theta(t,\\mathbf{x}) \\quad\n\\text{ , } \\quad \\theta(0,\\mathbf{x}) = \\theta_0(\\mathbf{x}) \\] for $t \\in\n[0,T], T>0$ and $\\mathbf{x} \\in \\mathbb{T}=[0,1]^2$. The initial condition\n$\\theta_0$ and diffusion coefficient $\\kappa$ are assumed to be known and the\ndata consist of point observations of the scalar field $\\theta$ corrupted by\nadditive, i.i.d. Gaussian noise. We adopt a Bayesian approach to this\nestimation problem and establish that the inference is consistent, i.e., that\nthe posterior measure identifies the true background flow as the number of\nscalar observations grows large. Since the inverse map is ill-defined for some\nclasses of problems even for perfect, infinite measurements of $\\theta$,\nmultiple experiments (initial conditions) are required to resolve the true\nfluid flow. Under this assumption, suitable conditions on the observation\npoints, and given support and tail conditions on the prior measure, we show\nthat the posterior measure converges to a Dirac measure centered on the true\nflow as the number of observations goes to infinity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 19:13:48 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 22:24:56 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 14:44:15 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Borggaard", "Jeff", ""], ["Glatt-Holtz", "Nathan E.", ""], ["Krometis", "Justin A.", ""]]}, {"id": "1809.06464", "submitter": "Sneha Jadhav", "authors": "Sneha Jadhav and Shuangge Ma", "title": "Functional Measurement Error in Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measurement error is an important problem that has not been very well studied\nin the context of Functional Data Analysis. To the best of our knowledge, there\nare no existing methods that address the presence of functional measurement\nerrors in generalized functional linear models. A framework is proposed for\nestimating the slope function in the presence of measurement error in the\ngeneralized functional linear model with a scalar response. This work extends\nthe conditional-score method to the case when both the measurement error and\nthe independent variables lie in an infinite dimensional space. Asymptotic\nresults are obtained for the proposed estimate and its behavior is studied via\nsimulations, when the response is continuous or binary. It's performance on\nreal data is demonstrated through a simulation study based on the Canadian\nWeather data-set, where errors are introduced in the data-set and it is\nobserved that the proposed estimate indeed performs better than a naive\nestimate that ignores the measurement error.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 22:43:34 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Jadhav", "Sneha", ""], ["Ma", "Shuangge", ""]]}, {"id": "1809.06474", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Saeed Ghadimi", "title": "Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints,\n  High-Dimensionality and Saddle-Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyze zeroth-order stochastic approximation\nalgorithms for nonconvex and convex optimization, with a focus on addressing\nconstrained optimization, high-dimensional setting and saddle-point avoiding.\nTo handle constrained optimization, we first propose generalizations of the\nconditional gradient algorithm achieving rates similar to the standard\nstochastic gradient algorithm using only zeroth-order information. To\nfacilitate zeroth-order optimization in high-dimensions, we explore the\nadvantages of structural sparsity assumptions. Specifically, (i) we highlight\nan implicit regularization phenomenon where the standard stochastic gradient\nalgorithm with zeroth-order information adapts to the sparsity of the problem\nat hand by just varying the step-size and (ii) propose a truncated stochastic\ngradient algorithm with zeroth-order information, whose rate of convergence\ndepends only poly-logarithmically on the dimensionality. We next focus on\navoiding saddle-points in non-convex setting. Towards that, we interpret the\nGaussian smoothing technique for estimating gradient based on zeroth-order\ninformation as an instantiation of first-order Stein's identity. Based on this,\nwe provide a novel linear-(in dimension) time estimator of the Hessian matrix\nof a function using only zeroth-order information, which is based on\nsecond-order Stein's identity. We then provide an algorithm for avoiding\nsaddle-points, which is based on a zeroth-order cubic regularization Newton's\nmethod and discuss its convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 23:30:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 02:53:46 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""]]}, {"id": "1809.06522", "submitter": "Jay Mardia", "authors": "Jay Mardia, Jiantao Jiao, Ervin T\\'anczos, Robert D. Nowak, Tsachy\n  Weissman", "title": "Concentration Inequalities for the Empirical Distribution", "comments": "Accepted for publication in Information and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study concentration inequalities for the Kullback--Leibler (KL) divergence\nbetween the empirical distribution and the true distribution. Applying a\nrecursion technique, we improve over the method of types bound uniformly in all\nregimes of sample size $n$ and alphabet size $k$, and the improvement becomes\nmore significant when $k$ is large. We discuss the applications of our results\nin obtaining tighter concentration inequalities for $L_1$ deviations of the\nempirical distribution from the true distribution, and the difference between\nconcentration around the expectation or zero. We also obtain asymptotically\ntight bounds on the variance of the KL divergence between the empirical and\ntrue distribution, and demonstrate their quantitatively different behaviors\nbetween small and large sample sizes compared to the alphabet size.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 04:01:55 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 19:18:30 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 01:12:40 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mardia", "Jay", ""], ["Jiao", "Jiantao", ""], ["T\u00e1nczos", "Ervin", ""], ["Nowak", "Robert D.", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1809.06581", "submitter": "Mario Teixeira Parente", "authors": "Mario Teixeira Parente", "title": "A probabilistic framework for approximating functions in active\n  subspaces", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a comprehensive probabilistic setup to compute\napproximating functions in active subspaces. Constantine et al. proposed the\nactive subspace method in (Constantine et al., 2014) to reduce the dimension of\ncomputational problems. It can be seen as an attempt to approximate a\nhigh-dimensional function of interest $f$ by a low-dimensional one. To do this,\na common approach is to integrate $f$ over the inactive, i.e. non-dominant,\ndirections with a suitable conditional density function. In practice, this can\nbe done with a finite Monte Carlo sum, making not only the resulting\napproximation random in the inactive variable for each fixed input from the\nactive subspace, but also its expectation, i.e. the integral of the\nlow-dimensional function weighted with a probability measure on the active\nvariable. In this regard we develop a fully probabilistic framework extending\nresults from (Constantine et al., 2014, 2016). The results are supported by a\nsimple numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:21:22 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 07:43:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parente", "Mario Teixeira", ""]]}, {"id": "1809.06668", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "Gram Charlier and Edgeworth expansion for sample variance", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a valid Edgeworth expansions for the Bessel\ncorrected empirical variance when data are generated by a strongly mixing\nprocess whose distribution can be arbitrarily. The constraint of strongly\nmixing process makes the problem not easy. Indeed, even for a strongly mixing\nnormal process, the distribution is unknown. Here, we do not assume any other\nassumption than a sufficiently fast decrease of the underlying distribution to\nmake the Edgeworth expansion convergent. This results can obviously apply to\nstrongly mixing normal process and provide an alternative to the work of\nMoschopoulos (1985) and Mathai (1982).\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:26:13 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1809.06790", "submitter": "Wei-Kuo Chen", "authors": "Wei-Kuo Chen, Madeline Handschy, Gilad Lerman", "title": "Phase transition in random tensors with multiple independent spikes", "comments": "46 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a spiked random tensor obtained as a mixture of two components:\nnoise in the form of a symmetric Gaussian $p$-tensor for $p\\geq 3$ and signal\nin the form of a symmetric low-rank random tensor. The latter is defined as a\nlinear combination of $k$ independent symmetric rank-one random tensors,\nreferred to as spikes, with weights referred to as signal-to-noise ratios\n(SNRs). The entries of the vectors that determine the spikes are i.i.d. sampled\nfrom general probability distributions supported on bounded subsets of\n$\\mathbb{R}$. This work focuses on the problem of detecting the presence of\nthese spikes, and establishes the phase transition of this detection problem\nfor any fixed $k \\geq 1$. In particular, it shows that for a set of relatively\nlow SNRs it is impossible to distinguish between the spiked and non-spiked\nGaussian tensors. Furthermore, in the interior of the complement of this set,\nwhere at least one of the $k$ SNRs is relatively high, these two tensors are\ndistinguishable by the likelihood ratio test. In addition, when the total\nnumber of low-rank components, $k$, of the $p$-tensor of size $N$ grows in the\norder $o(N^{(p-2)/4})$ as $N$ tends to infinity, the problem exhibits an\nanalogous phase transition. This theory for spike detection is also shown to\nimply that recovery of the spikes by the minimum mean square error exhibits the\nsame phase transition. The main methods used in this work arise from the study\nof mean field spin glass models, where the phase transition thresholds are\nidentified as the critical inverse temperatures distinguishing the high and\nlow-temperature regimes of the free energies. In particular, our result\nformulates the first full characterization of the high temperature regime for\nvector-valued spin glass models with independent coordinates.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 15:16:18 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 16:11:40 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 19:50:13 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chen", "Wei-Kuo", ""], ["Handschy", "Madeline", ""], ["Lerman", "Gilad", ""]]}, {"id": "1809.07425", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins", "title": "Mean Estimation with Sub-Gaussian Rates in Polynomial Time", "comments": "v4: improvements to exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study polynomial time algorithms for estimating the mean of a heavy-tailed\nmultivariate random vector. We assume only that the random vector $X$ has\nfinite mean and covariance. In this setting, the radius of confidence intervals\nachieved by the empirical mean are large compared to the case that $X$ is\nGaussian or sub-Gaussian.\n  We offer the first polynomial time algorithm to estimate the mean with\nsub-Gaussian-size confidence intervals under such mild assumptions. Our\nalgorithm is based on a new semidefinite programming relaxation of a\nhigh-dimensional median. Previous estimators which assumed only existence of\nfinitely-many moments of $X$ either sacrifice sub-Gaussian performance or are\nonly known to be computable via brute-force search procedures requiring time\nexponential in the dimension.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 23:08:29 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 04:35:35 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 06:59:56 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 00:47:49 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hopkins", "Samuel B.", ""]]}, {"id": "1809.07441", "submitter": "Robin Dunn", "authors": "Robin Dunn, Larry Wasserman, Aaditya Ramdas", "title": "Distribution-Free Prediction Sets with Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing distribution-free prediction sets\nwhen there are random effects. For iid data, prediction sets can be constructed\nusing the method of conformal prediction. The validity of this prediction set\nhinges on the assumption that the data are exchangeable, which is not true when\nthere are random effects. We extend the conformal method so that it is valid\nwith random effects. We develop a CDF pooling approach, a single subsampling\napproach, and a repeated subsampling approach to construct conformal prediction\nsets in unsupervised and supervised settings. We compare these approaches in\nterms of coverage and average set size. We recommend the repeated subsampling\napproach that constructs a conformal set by sampling one observation from each\ndistribution multiple times. Simulations show that this approach has the best\nbalance between coverage and average conformal set size.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 01:00:00 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 19:48:06 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Dunn", "Robin", ""], ["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1809.07541", "submitter": "Hannes Leeb", "authors": "Hannes Leeb and Paul Kabaila", "title": "Admissibility of the usual confidence set for the mean of a univariate\n  or bivariate normal population: The unknown-variance case", "comments": null, "journal-ref": "J. R. Stat. Soc. Ser. B Stat. Methodol., 79:801-813, 2017", "doi": "10.1111/rssb.12186", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Gaussian linear regression model (with unknown mean and variance), we\nshow that the standard confidence set for one or two regression coefficients is\nadmissible in the sense of Joshi (1969). This solves a long-standing open\nproblem in mathematical statistics, and this has important implications on the\nperformance of modern inference procedures post-model-selection or\npost-shrinkage, particularly in situations where the number of parameters is\nlarger than the sample size. As a technical contribution of independent\ninterest, we introduce a new class of conjugate priors for the Gaussian\nlocation-scale model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 09:16:59 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 10:11:41 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Leeb", "Hannes", ""], ["Kabaila", "Paul", ""]]}, {"id": "1809.07570", "submitter": "Laura Scarabosio", "authors": "Ustim Khristenko, Laura Scarabosio, Piotr Swierczynski, Elisabeth\n  Ullmann, Barbara Wohlmuth", "title": "Analysis of boundary effects on PDE-based sampling of Whittle-Mat\\'ern\n  random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the generation of samples of a mean-zero Gaussian random field\nwith Mat\\'ern covariance function. Every sample requires the solution of a\ndifferential equation with Gaussian white noise forcing, formulated on a\nbounded computational domain. This introduces unwanted boundary effects since\nthe stochastic partial differential equation is originally posed on the whole\n$\\mathbb{R}^d$, without boundary conditions. We use a window technique, whereby\none embeds the computational domain into a larger domain, and postulates\nconvenient boundary conditions on the extended domain. To mitigate the\npollution from the artificial boundary it has been suggested in numerical\nstudies to choose a window size that is at least as large as the correlation\nlength of the Mat\\'ern field. We provide a rigorous analysis for the error in\nthe covariance introduced by the window technique, for homogeneous Dirichlet,\nhomogeneous Neumann, and periodic boundary conditions. We show that the error\ndecays exponentially in the window size, independently of the type of boundary\ncondition. We conduct numerical experiments in 1D and 2D space, confirming our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 11:11:28 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Khristenko", "Ustim", ""], ["Scarabosio", "Laura", ""], ["Swierczynski", "Piotr", ""], ["Ullmann", "Elisabeth", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1809.07735", "submitter": "Matthew Colbrook", "authors": "Matthew J. Colbrook, Zdravko I. Botev, Karsten Kuritz and Shev\n  MacNamara", "title": "Kernel Density Estimation with Linked Boundary Conditions", "comments": null, "journal-ref": "Studies in Applied Mathematics 145 (2020) 357-396", "doi": "10.1111/sapm.12322", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel density estimation on a finite interval poses an outstanding challenge\nbecause of the well-recognized bias at the boundaries of the interval.\nMotivated by an application in cancer research, we consider a boundary\nconstraint linking the values of the unknown target density function at the\nboundaries. We provide a kernel density estimator (KDE) that successfully\nincorporates this linked boundary condition, leading to a non-self-adjoint\ndiffusion process and expansions in non-separable generalized eigenfunctions.\nThe solution is rigorously analyzed through an integral representation given by\nthe unified transform (or Fokas method). The new KDE possesses many desirable\nproperties, such as consistency, asymptotically negligible bias at the\nboundaries, and an increased rate of approximation, as measured by the AMISE.\nWe apply our method to the motivating example in biology and provide numerical\nexperiments with synthetic data, including comparisons with state-of-the-art\nKDEs (which currently cannot handle linked boundary constraints). Results\nsuggest that the new method is fast and accurate. Furthermore, we demonstrate\nhow to build statistical estimators of the boundary conditions satisfied by the\ntarget function without apriori knowledge. Our analysis can also be extended to\nmore general boundary conditions that may be encountered in applications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 16:53:19 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 07:39:30 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 07:01:27 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 08:45:02 GMT"}, {"version": "v5", "created": "Mon, 9 Mar 2020 09:30:23 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2020 08:16:29 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Colbrook", "Matthew J.", ""], ["Botev", "Zdravko I.", ""], ["Kuritz", "Karsten", ""], ["MacNamara", "Shev", ""]]}, {"id": "1809.08204", "submitter": "Yuan Cao", "authors": "Yuan Cao, Matey Neykov, Han Liu", "title": "High-Temperature Structure Detection in Ferromagnets", "comments": "51 pages, 4 figures. version 2: a new computational lower bound\n  result is added. version 3: citations are updated", "journal-ref": "Information and Inference: A Journal of the IMA (2020)", "doi": "10.1093/imaiai/iaaa032", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies structure detection problems in high temperature\nferromagnetic (positive interaction only) Ising models. The goal is to\ndistinguish whether the underlying graph is empty, i.e., the model consists of\nindependent Rademacher variables, versus the alternative that the underlying\ngraph contains a subgraph of a certain structure. We give matching upper and\nlower minimax bounds under which testing this problem is possible/impossible\nrespectively. Our results reveal that a key quantity called graph arboricity\ndrives the testability of the problem. On the computational front, under a\nconjecture of the computational hardness of sparse principal component\nanalysis, we prove that, unless the signal is strong enough, there are no\npolynomial time tests which are capable of testing this problem. In order to\nprove this result we exhibit a way to give sharp inequalities for the even\nmoments of sums of i.i.d. Rademacher random variables which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 16:49:23 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 09:02:50 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 03:47:46 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 18:28:44 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cao", "Yuan", ""], ["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1809.08330", "submitter": "Nicolas Verzelen", "authors": "Alexandra Carpentier and Sylvain Delattre and Etienne Roquain and\n  Nicolas Verzelen", "title": "Estimating minimum effect with outlier selection", "comments": "70 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce one-sided versions of Huber's contamination model, in which\ncorrupted samples tend to take larger values than uncorrupted ones. Two\nintertwined problems are addressed: estimation of the mean of uncorrupted\nsamples (minimum effect) and selection of corrupted samples (outliers).\nRegarding the minimum effect estimation, we derive the minimax risks and\nintroduce adaptive estimators to the unknown number of contaminations.\nInterestingly, the optimal convergence rate highly differs from that in\nclassical Huber's contamination model. Also, our analysis uncovers the effect\nof particular structural assumptions on the distribution of the contaminated\nsamples. As for the problem of selecting the outliers, we formulate the problem\nin a multiple testing framework for which the location/scaling of the null\nhypotheses are unknown. We rigorously prove how estimating the null hypothesis\nis possible while maintaining a theoretical guarantee on the amount of the\nfalsely selected outliers, both through false discovery rate (FDR) or post hoc\nbounds. As a by-product, we address a long-standing open issue on FDR control\nunder equi-correlation, which reinforces the interest of removing dependency\nwhen making multiple testing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 22:01:45 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Delattre", "Sylvain", ""], ["Roquain", "Etienne", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1809.08539", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "On the Maximum of Dependent Gaussian Random Variables: A Sharp Bound for\n  the Lower Tail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is an extensive literature on the maxima of Gaussian\nprocesses, there are relatively few non-asymptotic bounds on their lower-tail\nprobabilities. In the context of a finite index set, this paper offers such a\nbound, while also allowing for many types of dependence. Specifically, let\n$(X_1,\\dots,X_n)$ be a centered Gaussian vector, with standardized entries,\nwhose correlation matrix $R$ satisfies $\\max_{i\\neq j} R_{ij}\\leq \\rho_0$ for\nsome constant $\\rho_0\\in (0,1)$. Then, for any $\\epsilon_0\\in\n(0,\\sqrt{1-\\rho_0})$, we establish an upper bound on the probability\n$\\mathbb{P}(\\max_{1\\leq i\\leq n} X_i\\leq \\epsilon_0\\sqrt{2\\log(n)})$ that is a\nfunction of $\\rho_0, \\, \\epsilon_0,$ and $n$. Furthermore, we show the bound is\nsharp, in the sense that it is attained up to a constant, for each $\\rho_0$ and\n$\\epsilon_0$.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 05:45:34 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1809.08732", "submitter": "Hanzhong Liu", "authors": "Hanzhong Liu and Yuehan Yang", "title": "Penalized regression adjusted causal effect estimates in high\n  dimensional randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression adjustments are often considered by investigators to improve the\nestimation efficiency of causal effect in randomized experiments when there\nexists many pre-experiment covariates. In this paper, we provide conditions\nthat guarantee the penalized regression including the Ridge, Elastic Net and\nAdapive Lasso adjusted causal effect estimators are asymptotic normal and we\nshow that their asymptotic variances are no greater than that of the simple\ndifference-in-means estimator, as long as the penalized estimators are risk\nconsistent. We also provide conservative estimators for the asymptotic variance\nwhich can be used to construct asymptotically conservative confidence intervals\nfor the average causal effect (ACE). Our results are obtained under the\nNeyman-Rubin potential outcomes model of randomized experiment when the number\nof covariates is large. Simulation study shows the advantages of the penalized\nregression adjusted ACE estimators over the difference-in-means estimator.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 02:46:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Liu", "Hanzhong", ""], ["Yang", "Yuehan", ""]]}, {"id": "1809.08760", "submitter": "Fang Han", "authors": "Fang Han and Yicheng Li", "title": "Moment bounds for large autocovariance matrices under dependence", "comments": "to appear in Journal of Theoretical Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to obtain expectation bounds for the deviation of\nlarge sample autocovariance matrices from their means under weak data\ndependence. While the accuracy of covariance matrix estimation corresponding to\nindependent data has been well understood, much less is known in the case of\ndependent data. We make a step towards filling this gap, and establish\ndeviation bounds that depend only on the parameters controlling the \"intrinsic\ndimension\" of the data up to some logarithmic terms. Our results have immediate\nimpacts on high dimensional time series analysis, and we apply them to high\ndimensional linear VAR($d$) model, vector-valued ARCH model, and a model used\nin Banna et al. (2016).\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:57:56 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:04:33 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Han", "Fang", ""], ["Li", "Yicheng", ""]]}, {"id": "1809.08818", "submitter": "Sven Wang", "authors": "Richard Nickl, Sara van de Geer, Sven Wang", "title": "Convergence rates for Penalised Least Squares Estimators in\n  PDE-constrained regression problems", "comments": "40 pages, to appear in SIAM/ASA Journal of Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.AP math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider PDE constrained nonparametric regression problems in which the\nparameter $f$ is the unknown coefficient function of a second order elliptic\npartial differential operator $L_f$, and the unique solution $u_f$ of the\nboundary value problem \\[L_fu=g_1\\text{ on } \\mathcal O, \\quad u=g_2 \\text{ on\n}\\partial \\mathcal O,\\] is observed corrupted by additive Gaussian white noise.\nHere $\\mathcal O$ is a bounded domain in $\\mathbb R^d$ with smooth boundary\n$\\partial \\mathcal O$, and $g_1, g_2$ are given functions defined on $\\mathcal\nO, \\partial \\mathcal O$, respectively. Concrete examples include $L_fu=\\Delta\nu-2fu$ (Schr\\\"odinger equation with attenuation potential $f$) and\n$L_fu=\\text{div} (f\\nabla u)$ (divergence form equation with conductivity $f$).\nIn both cases, the parameter space \\[\\mathcal F=\\{f\\in H^\\alpha(\\mathcal O)| f\n> 0\\}, ~\\alpha>0, \\] where $H^\\alpha(\\mathcal O)$ is the usual order $\\alpha$\nSobolev space, induces a set of non-linearly constrained regression functions\n$\\{u_f: f \\in \\mathcal F\\}$.\n  We study Tikhonov-type penalised least squares estimators $\\hat f$ for $f$.\nThe penalty functionals are of squared Sobolev-norm type and thus $\\hat f$ can\nalso be interpreted as a Bayesian `MAP'-estimator corresponding to some\nGaussian process prior. We derive rates of convergence of $\\hat f$ and of\n$u_{\\hat f}$, to $f, u_f$, respectively. We prove that the rates obtained are\nminimax-optimal in prediction loss. Our bounds are derived from a general\nconvergence rate result for non-linear inverse problems whose forward map\nsatisfies a modulus of continuity condition, a result of independent interest\nthat is applicable also to linear inverse problems, illustrated in an example\nwith the Radon transform.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 09:45:53 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 09:40:31 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 09:40:01 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Nickl", "Richard", ""], ["van de Geer", "Sara", ""], ["Wang", "Sven", ""]]}, {"id": "1809.09069", "submitter": "Victor Gaton", "authors": "Javier de Frutos, Victor Gaton", "title": "An extension of Heston's SV model to Stochastic Interest Rates", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics Volume 354, July\n  2019, Pages 174-182", "doi": "10.1016/j.cam.2018.09.010", "report-no": null, "categories": "q-fin.CP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 'A Closed-Form Solution for Options with Stochastic Volatility with\nApplications to Bond and Currency Options', Heston proposes a Stochastic\nVolatility (SV) model with constant interest rate and derives a semi-explicit\nvaluation formula. Heston also describes, in general terms, how the model could\nbe extended to incorporate Stochastic Interest Rates (SIR). This paper is\ndevoted to the construction of an extension of Heston's SV model with a\nparticular stochastic bond model which, just increasing in one the number of\nparameters, allows to incorporate SIR and to derive a semi-explicit formula for\noption pricing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:24:26 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["de Frutos", "Javier", ""], ["Gaton", "Victor", ""]]}, {"id": "1809.09505", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Miko{\\l}aj Kasprzak, Tamara\n  Broderick", "title": "Practical bounds on the error of Bayesian posterior approximations: A\n  nonasymptotic approach", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:11:32 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 21:33:00 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1809.09567", "submitter": "Huiming Zhang", "authors": "Bo Li, Huiming Zhang, Jiao He", "title": "Some Characterizations and Properties of COM-Poisson Random Variables", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces some new characterizations of COM-Poisson random\nvariables. First, it extends Moran-Chatterji characterization and generalizes\nRao-Rubin characterization of Poisson distribution to COM-Poisson distribution.\nThen, it defines the COM-type discrete r.v. ${X_\\nu }$ of the discrete random\nvariable $X$. The probability mass function of ${X_\\nu }$ has a link to the\nR\\'enyi entropy and Tsallis entropy of order $\\nu $ of $X$. And then we can get\nthe characterization of Stam inequality for COM-type discrete version Fisher\ninformation. By using the recurrence formula, the property that COM-Poisson\nrandom variables ($\\nu \\ne 1$) is not closed under addition are obtained.\nFinally, under the property of \"not closed under addition\" of COM-Poisson\nrandom variables, a new characterization of Poisson distribution is found.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:04:02 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Li", "Bo", ""], ["Zhang", "Huiming", ""], ["He", "Jiao", ""]]}, {"id": "1809.09573", "submitter": "Yuxin Chen", "authors": "Yuejie Chi, Yue M. Lu, Yuxin Chen", "title": "Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview", "comments": "Invited overview article", "journal-ref": "IEEE Transactions on Signal Processing, vol. 67, no. 20, pp.\n  5239-5269, October 2019", "doi": "10.1109/TSP.2019.2937282", "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial progress has been made recently on developing provably accurate\nand efficient algorithms for low-rank matrix factorization via nonconvex\noptimization. While conventional wisdom often takes a dim view of nonconvex\noptimization algorithms due to their susceptibility to spurious local minima,\nsimple iterative methods such as gradient descent have been remarkably\nsuccessful in practice. The theoretical footings, however, had been largely\nlacking until recently.\n  In this tutorial-style overview, we highlight the important role of\nstatistical models in enabling efficient nonconvex optimization with\nperformance guarantees. We review two contrasting approaches: (1) two-stage\nalgorithms, which consist of a tailored initialization step followed by\nsuccessive refinement; and (2) global landscape analysis and\ninitialization-free algorithms. Several canonical matrix factorization problems\nare discussed, including but not limited to matrix sensing, phase retrieval,\nmatrix completion, blind deconvolution, robust principal component analysis,\nphase synchronization, and joint alignment. Special care is taken to illustrate\nthe key technical insights underlying their analyses. This article serves as a\ntestament that the integrated consideration of optimization and statistics\nleads to fruitful research findings.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 16:21:07 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 22:56:03 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 17:00:59 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chi", "Yuejie", ""], ["Lu", "Yue M.", ""], ["Chen", "Yuxin", ""]]}, {"id": "1809.09879", "submitter": "Dieter Mitsche", "authors": "Josep Diaz, Colin McDiarmid, Dieter Mitsche", "title": "Learning random points from geometric graphs or orderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that there is a family of $n$ random points $X_v$ for $v \\in V$,\nindependently and uniformly distributed in the square\n$\\left[-\\sqrt{n}/2,\\sqrt{n}/2\\right]^2$ of area $n$. We do not see these\npoints, but learn about them in one of the following two ways.\n  Suppose first that we are given the corresponding random geometric graph $G$,\nwhere distinct vertices $u$ and $v$ are adjacent when the Euclidean distance\n$d_E(X_u,X_v)$ is at most $r$. If the threshold distance $r$ satisfies\n$n^{3/14} \\ll r \\ll n^{1/2}$, then the following holds with high probability.\nGiven the graph $G$ (without any geometric information), in polynomial time we\ncan approximately reconstruct the hidden embedding, in the sense that, `up to\nsymmetries', for each vertex $v$ we find a point within distance about $r$ of\n$X_v$; that is, we find an embedding with `displacement' at most about $r$.\n  Now suppose that, instead of being given the graph $G$, we are given, for\neach vertex $v$, the ordering of the other vertices by increasing Euclidean\ndistance from $v$. Then, with high probability, in polynomial time we can find\nan embedding with the much smaller displacement error $O(\\sqrt{\\log n})$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:55:03 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 21:47:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Diaz", "Josep", ""], ["McDiarmid", "Colin", ""], ["Mitsche", "Dieter", ""]]}, {"id": "1809.09896", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Large sample properties of the regression depth induced median", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notions of depth in regression have been introduced and studied in the\nliterature. Regression depth (RD) of Rousseeuw and Hubert (1999), the most\nfamous one, is a direct extension of Tukey location depth (Tukey (1975)) to\nregression.\n  Like its location counterpart, the most remarkable advantage of the notion of\ndepth in regression is to directly introduce the maximum (or deepest)\nregression depth estimator (aka depth induced median) for regression parameters\nin a multi-dimensional setting. Classical questions for the regression depth\ninduced median include (i) is it a consistent estimator (or rather under what\nsufficient conditions, it is consistent)? and (ii) is there any limiting\ndistribution?\n  Bai and He (1999) (BH99) pioneered an attempt to answer these questions.\nUnder some stringent conditions on (i) the design points, (ii) the conditional\ndistributions of $y$ given $\\bs{x}_i$, and (iii) the error distributions, BH99\nproved the strong consistency of the depth induced median. Under another set of\nconditions, BH99 showed the existence of the limiting distribution of the\nestimator.\n  This article establishes the strong consistency of the depth induced median\nwithout any of the stringent conditions in BH99, and proves the existence of\nthe limiting distribution of the estimator by sufficient conditions and an\napproach different from BH99.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 10:40:31 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 18:47:13 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 01:54:52 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 22:08:29 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "1809.09934", "submitter": "Markus Wageringel", "authors": "Alexandros Grosdos Koutsoumpelias, Markus Wageringel", "title": "Moment ideals of local Dirac mixtures", "comments": "26 pages, 3 figures", "journal-ref": "SIAM J. Appl. Algebra Geometry 4-1 (2020), pp. 1-27", "doi": "10.1137/18M1219783", "report-no": null, "categories": "math.AC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study ideals arising from moments of local Dirac measures\nand their mixtures. We provide generators for the case of first order local\nDiracs and explain how to obtain the moment ideal of the Pareto distribution\nfrom them. We then use elimination theory and Prony's method for parameter\nestimation of finite mixtures. Our results are showcased with applications in\nsignal processing and statistics. We highlight the natural connections to\nalgebraic statistics, combinatorics and applications in analysis throughout the\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 12:27:41 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 15:54:44 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Koutsoumpelias", "Alexandros Grosdos", ""], ["Wageringel", "Markus", ""]]}, {"id": "1809.09953", "submitter": "Max Farrell", "authors": "Max H. Farrell and Tengyuan Liang and Sanjog Misra", "title": "Deep Neural Networks for Estimation and Inference", "comments": null, "journal-ref": "Econometrica, vol 89, no 1, 181-213, 2021", "doi": "10.3982/ECTA16901", "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study deep neural networks and their use in semiparametric inference. We\nestablish novel rates of convergence for deep feedforward neural nets. Our new\nrates are sufficiently fast (in some cases minimax optimal) to allow us to\nestablish valid second-step inference after first-step estimation with deep\nlearning, a result also new to the literature. Our estimation rates and\nsemiparametric inference results handle the current standard architecture:\nfully connected feedforward neural networks (multi-layer perceptrons), with the\nnow-common rectified linear unit activation function and a depth explicitly\ndiverging with the sample size. We discuss other architectures as well,\nincluding fixed-width, very deep networks. We establish nonasymptotic bounds\nfor these deep nets for a general class of nonparametric regression-type loss\nfunctions, which includes as special cases least squares, logistic regression,\nand other generalized linear models. We then apply our theory to develop\nsemiparametric inference, focusing on causal parameters for concreteness, such\nas treatment effects, expected welfare, and decomposition effects. Inference in\nmany other semiparametric contexts can be readily obtained. We demonstrate the\neffectiveness of deep learning with a Monte Carlo analysis and an empirical\napplication to direct mail marketing.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 13:04:23 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 00:03:51 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 14:23:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Farrell", "Max H.", ""], ["Liang", "Tengyuan", ""], ["Misra", "Sanjog", ""]]}, {"id": "1809.10107", "submitter": "Sirio Legramanti", "authors": "Sirio Legramanti", "title": "Mean and dispersion of harmonic measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide and prove exact formulas for the mean and the trace\nof the covariance matrix of harmonic measure, regarded as a parametric\nprobability distribution.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 16:32:01 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Legramanti", "Sirio", ""]]}, {"id": "1809.10272", "submitter": "Tim Austin", "authors": "Tim Austin", "title": "Multi-variate correlation and mixtures of product measures", "comments": "39 pages [v2:] Slight changes in presentation based on feedback from\n  colleagues [v3, v4:] Small revisions during preparation for journal", "journal-ref": "Kybernetika 56 (2020), no. 3, 459--499", "doi": "10.14736/kyb-2020-3-0459", "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total correlation (`TC') and dual total correlation (`DTC') are two classical\nways to quantify the correlation among an $n$-tuple of random variables. They\nboth reduce to mutual information when $n=2$.\n  The first part of this paper sets up the theory of TC and DTC for general\nrandom variables, not necessarily finite-valued. This generality has not been\nexposed in the literature before.\n  The second part considers the structural implications when a joint\ndistribution $\\mu$ has small TC or DTC. If $\\mathrm{TC}(\\mu) = o(n)$, then\n$\\mu$ is close to a product measure according to a suitable transportation\nmetric: this follows directly from Marton's classical transportation-entropy\ninequality. If $\\mathrm{DTC}(\\mu) = o(n)$, then the structural consequence is\nmore complicated: $\\mu$ is a mixture of a controlled number of terms, most of\nthem close to product measures in the transportation metric. This is the main\nnew result of the paper.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 00:05:02 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 23:33:38 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 16:37:58 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2020 16:51:24 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Austin", "Tim", ""]]}, {"id": "1809.10394", "submitter": "Ziying He", "authors": "Ziying He, Jinqiao Duan and Xiujun Cheng", "title": "A parameter estimator based on Smoluchowski-Kramers approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a simplified parameter estimator for a second order stochastic\ndifferential equation by a first order system based on the Smoluchowski-Kramers\napproximation. We establish the consistency of the estimator by using\n{\\Gamma}-convergence theory. We further illus- trate our estimation method by\nan experimentally studied movement model of a colloidal particle immersed in\nwater under conservative force and constant diffusion.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 08:21:29 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["He", "Ziying", ""], ["Duan", "Jinqiao", ""], ["Cheng", "Xiujun", ""]]}, {"id": "1809.10455", "submitter": "H{\\aa}kon Otneim", "authors": "Dag Tj{\\o}stheim, H{\\aa}kon Otneim and B{\\aa}rd St{\\o}ve", "title": "Statistical dependence: Beyond Pearson's $\\rho$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearson's $\\rho$ is the most used measure of statistical dependence. It gives\na complete characterization of dependence in the Gaussian case, and it also\nworks well in some non-Gaussian situations. It is well known, however, that it\nhas a number of shortcomings; in particular for heavy tailed distributions and\nin nonlinear situations, where it may produce misleading, and even disastrous\nresults. In recent years a number of alternatives have been proposed. In this\npaper, we will survey these developments, especially results obtained in the\nlast couple of decades. Among measures discussed are the copula,\ndistribution-based measures, the distance covariance, the HSIC measure popular\nin machine learning, and finally the local Gaussian correlation, which is a\nlocal version of Pearson's $\\rho$. Throughout we put the emphasis on conceptual\ndevelopments and a comparison of these. We point out relevant references to\ntechnical details as well as comparative empirical and simulated experiments.\nThere is a broad selection of references under each topic treated.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:08:10 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Tj\u00f8stheim", "Dag", ""], ["Otneim", "H\u00e5kon", ""], ["St\u00f8ve", "B\u00e5rd", ""]]}, {"id": "1809.10462", "submitter": "Nikita Zhivotovskiy", "authors": "Shahar Mendelson, Nikita Zhivotovskiy", "title": "Robust covariance estimation under $L_4-L_2$ norm equivalence", "comments": "19 pages. Referee's suggestions addressed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a centered random vector taking values in $\\mathbb{R}^d$ and let\n$\\Sigma= \\mathbb{E}(X\\otimes X)$ be its covariance matrix. We show that if $X$\nsatisfies an $L_4-L_2$ norm equivalence, there is a covariance estimator\n$\\hat{\\Sigma}$ that exhibits the optimal performance one would expect had $X$\nbeen a gaussian vector. The procedure also improves the current\nstate-of-the-art regarding high probability bounds in the subgaussian case\n(sharp results were only known in expectation or with constant probability). In\nboth scenarios the new bound does not depend explicitly on the dimension $d$,\nbut rather on the effective rank of the covariance matrix $\\Sigma$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 11:35:49 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 21:50:50 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Mendelson", "Shahar", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "1809.10476", "submitter": "Ke Wang", "authors": "Zhigang Bao, Xiucai Ding, Ke Wang", "title": "Singular vector and singular subspace distribution for the matrix\n  denoising model", "comments": "Final version. Accepted by the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the matrix denosing model $Y=S+X$, where $S$ is a\nlow-rank deterministic signal matrix and $X$ is a random noise matrix, and both\nare $M\\times n$. In the scenario that $M$ and $n$ are comparably large and the\nsignals are supercritical, we study the fluctuation of the outlier singular\nvectors of $Y$. More specifically, we derive the limiting distribution of\nangles between the principal singular vectors of $Y$ and their deterministic\ncounterparts, the singular vectors of $S$. Further, we also derive the\ndistribution of the distance between the subspace spanned by the principal\nsingular vectors of $Y$ and that spanned by the singular vectors of $S$. It\nturns out that the limiting distributions depend on the structure of the\nsingular vectors of $S$ and the distribution of $X$, and thus they are\nnon-universal.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:09:00 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 05:45:50 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bao", "Zhigang", ""], ["Ding", "Xiucai", ""], ["Wang", "Ke", ""]]}, {"id": "1809.10538", "submitter": "Arun Kuchibhotla", "authors": "Arun K. Kuchibhotla, Lawrence D. Brown, Andreas Buja", "title": "Model-free Study of Ordinary Least Squares Linear Regression", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least squares (OLS) linear regression is one of the most basic\nstatistical techniques for data analysis. In the main stream literature and the\nstatistical education, the study of linear regression is typically restricted\nto the case where the covariates are fixed, errors are mean zero Gaussians with\nvariance independent of the (fixed) covariates. Even though OLS has been\nstudied under misspecification from as early as the 1960's, the implications\nhave not yet caught up with the main stream literature and applied sciences.\nThe present article is an attempt at a unified viewpoint that makes the various\nimplications of misspecification stand out.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:25:29 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Brown", "Lawrence D.", ""], ["Buja", "Andreas", ""]]}, {"id": "1809.10652", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Preetam Nandy and Hongzhe Li", "title": "Inference for Individual Mediation Effects and Interventional Effects in\n  Sparse High-Dimensional Causal Graphical Models", "comments": "Revised version; 50 pages, 6 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying intermediate variables (or mediators)\nthat regulate the effect of a treatment on a response variable. While there has\nbeen significant research on this classical topic, little work has been done\nwhen the set of potential mediators is high-dimensional (HD). A further\ncomplication arises when these mediators are interrelated (with unknown\ndependencies). In particular, we assume that the causal structure of the\ntreatment, the confounders, the potential mediators and the response is a\n(possibly unknown) directed acyclic graph (DAG). HD DAG models have previously\nbeen used for the estimation of causal effects from observational data. In\nparticular, methods called IDA and joint-IDA have been developed for estimating\nthe effects of single and multiple simultaneous interventions, respectively. In\nthis paper, we propose an IDA-type method called MIDA for estimating so-called\nindividual mediation effects from HD observational data. Although IDA and\njoint-IDA estimators have been shown to be consistent in certain sparse HD\nsettings, their asymptotic properties such as convergence in distribution and\ninferential tools in such settings have remained unknown. In this paper, we\nprove HD consistency of MIDA for linear structural equation models with\nsub-Gaussian errors. More importantly, we derive distributional convergence\nresults for MIDA in similar HD settings, which are applicable to IDA and\njoint-IDA estimators as well. To our knowledge, these are the first such\ndistributional convergence results facilitating inference for IDA-type\nestimators. These are built on our novel theoretical results regarding uniform\nbounds for linear regression estimators over varying subsets of HD covariates\nwhich may be of independent interest. Finally, we empirically validate our\nasymptotic theory for MIDA and demonstrate its usefulness via simulations and a\nreal data application.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 17:28:07 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:21:30 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 07:27:21 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Nandy", "Preetam", ""], ["Li", "Hongzhe", ""]]}, {"id": "1809.10827", "submitter": "Ji Oon Lee", "authors": "Hye Won Chung and Ji Oon Lee", "title": "Weak detection in the spiked Wigner model", "comments": "45 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the weak detection problem in a rank-one spiked Wigner data\nmatrix where the signal-to-noise ratio is small so that reliable detection is\nimpossible. We propose a hypothesis test on the presence of the signal by\nutilizing the linear spectral statistics of the data matrix. The test is\ndata-driven and does not require prior knowledge about the distribution of the\nsignal or the noise. When the noise is Gaussian, the proposed test is optimal\nin the sense that its error matches that of the likelihood ratio test, which\nminimizes the sum of the Type-I and Type-II errors. If the density of the noise\nis known and non-Gaussian, the error of the test can be lowered by applying an\nentrywise transformation to the data matrix. We establish a central limit\ntheorem for the linear spectral statistics of general rank-one spiked Wigner\nmatrices as an intermediate step.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 02:30:27 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 07:44:17 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 02:53:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""]]}, {"id": "1809.10899", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR)", "title": "Large sample properties of the Midzuno sampling scheme with\n  probabilities proportional to size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Midzuno sampling enables to estimate ratios unbiasedly. We prove the\nasymptotic normality for estimators of totals and ratios under Midzuno\nsampling. We also propose consistent variance estimators.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 07:50:21 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:50:53 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"]]}, {"id": "1809.10925", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy, Carsten Schuett, Elisabeth M. Werner", "title": "Data depth and floating body", "comments": null, "journal-ref": null, "doi": "10.1214/19-SS123", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little known relations of the renown concept of the halfspace depth for\nmultivariate data with notions from convex and affine geometry are discussed.\nHalfspace depth may be regarded as a measure of symmetry for random vectors. As\nsuch, the depth stands as a generalization of a measure of symmetry for convex\nsets, well studied in geometry. Under a mild assumption, the upper level sets\nof the halfspace depth coincide with the convex floating bodies used in the\ndefinition of the affine surface area for convex bodies in Euclidean spaces.\nThese connections enable us to partially resolve some persistent open problems\nregarding theoretical properties of the depth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:11:55 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Nagy", "Stanislav", ""], ["Schuett", "Carsten", ""], ["Werner", "Elisabeth M.", ""]]}, {"id": "1809.11108", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Kari Heine", "title": "Online Inference with Multi-modal Likelihood Functions", "comments": "64 pages (29 pages for the paper and 35 pages for the supplementary\n  material), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(Y_t)_{t\\geq 1}$ be a sequence of i.i.d.\\ observations and\n$\\{f_\\theta,\\theta\\in \\mathbb{R}^d\\}$ be a parametric model. We introduce a new\nonline algorithm for computing a sequence $(\\hat{\\theta}_t)_{t\\geq 1}$ which is\nshown to converge almost surely to $\\text{argmax}_{\\theta\\in\n\\mathbb{R}^d}\\mathbb{E}[\\log f_\\theta(Y_1)]$ at rate $ \\mathcal{O}(\\log\n(t)^{(1+\\varepsilon)/2}t^{-1/2})$, with $\\varepsilon>0$ a user specified\nparameter. This convergence result is obtained under standard conditions on the\nstatistical model and, most notably, we allow the mapping $\\theta\\mapsto\n\\mathbb{E}[\\log f_\\theta(Y_1)]$ to be multi-modal. However, the computational\ncost to process each observation grows exponentially with the dimension of\n$\\theta$, which makes the proposed approach applicable to low or moderate\ndimensional problems only. We also derive a version of the estimator\n$\\hat{\\theta}_t$ which is well suited to Student-t linear regression models.\nThe corresponding estimator of the regression coefficients is robust to the\npresence of outliers, as shown by experiments on simulated and real data, and\nthus, as a by-product of this work, we obtain a new online and adaptive robust\nestimation method for linear regression models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:59:41 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 10:25:53 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 09:54:37 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 08:47:45 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 12:10:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Gerber", "Mathieu", ""], ["Heine", "Kari", ""]]}]