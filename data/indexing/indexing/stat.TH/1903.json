[{"id": "1903.00076", "submitter": "Nooshin Yousefi", "authors": "Nooshin Yousefi, David W. Coit", "title": "Reliability Analysis of Systems Subject To Mutually Dependent Competing\n  Failure Processes With Changing Degradation Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new reliability model has been developed for a single system\ndegrading stochastically which experiences soft and hard failure. Soft failure\noccurs when the physical deterioration level of the system is greater than a\npredefined failure threshold, and hard failure occurs when the instantaneous\nstress caused by a shock process is greater than a critical threshold. It is\nconsidered that the degradation and shock process are mutually dependent. In\nfact, each arriving shock accelerates the degradation process by adding abrupt\nadditional damages to the degradation path and changing the degradation rate\naccording to specific magnitude; also, the cumulative degradation changes the\noccurrence intensity of shock process. A gamma process is used as a stochastic\nprocess to model the degradation path. A realistic numerical example is\npresented to illustrate the proposed reliability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 21:36:28 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yousefi", "Nooshin", ""], ["Coit", "David W.", ""]]}, {"id": "1903.00093", "submitter": "Mar\\'ia Camila V\\'asquez Correa", "authors": "Mar\\'ia Camila V\\'asquez-Correa and Henry Laniado Rodas", "title": "A robust approach for principal component analyisis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we analyze different ways of performing principal component\nanalysis throughout three different approaches: robust covariance and\ncorrelation matrix estimation, projection pursuit approach and non-parametric\nmaximum entropy algorithm. The objective of these approaches is the correction\nof the well known sensitivity to outliers of the classical method for principal\ncomponent analysis. Due to their robustness, they perform very well in\ncontaminated data, while the classical approach fails to preserve the\ncharacteristics of the core information.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 22:35:47 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["V\u00e1squez-Correa", "Mar\u00eda Camila", ""], ["Rodas", "Henry Laniado", ""]]}, {"id": "1903.00147", "submitter": "Hien Nguyen", "authors": "T Tin Nguyen and Hien D Nguyen and Faicel Chamroukhi and Geoffrey J\n  McLachlan", "title": "Approximation by finite mixtures of continuous density functions that\n  vanish at infinity", "comments": null, "journal-ref": "Cogent Mathematics & Statistics. 7:1 (2020)", "doi": "10.1080/25742558.2020.1750861", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given sufficiently many components, it is often cited that finite mixture\nmodels can approximate any other probability density function (pdf) to an\narbitrary degree of accuracy. Unfortunately, the nature of this approximation\nresult is often left unclear. We prove that finite mixture models constructed\nfrom pdfs in $\\mathcal{C}_{0}$ can be used to conduct approximation of various\nclasses of approximands in a number of different modes. That is, we prove\napproximands in $\\mathcal{C}_{0}$ can be uniformly approximated, approximands\nin $\\mathcal{C}_{b}$ can be uniformly approximated on compact sets, and\napproximands in $\\mathcal{L}_{p}$ can be approximated with respect to the\n$\\mathcal{L}_{p}$, for $p\\in\\left[1,\\infty\\right)$. Furthermore, we also prove\nthat measurable functions can be approximated, almost everywhere.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:49:24 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 22:44:11 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Nguyen", "T Tin", ""], ["Nguyen", "Hien D", ""], ["Chamroukhi", "Faicel", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1903.00162", "submitter": "Alan Huang", "authors": "Alan Huang, Andy Sangil Kim", "title": "Are profile likelihoods likelihoods? No, but sometimes they can be", "comments": "short note; 4 pages with references;", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer our two cents to the ongoing discussion on whether profile\nlikelihoods are \"true\" likelihood functions, by showing that the profile\nlikelihood function can in fact be identical to a marginal likelihood in the\nspecial case of normal models. Thus, profile likelihoods can be \"true\"\nlikelihoods insofar as marginal likelihoods are \"true\" likelihoods. The prior\ndistribution that achieves this equivalence turns out to be the Jeffreys prior.\nWe suspect, however, that normal models are the only class of models for which\nsuch an equivalence between maximization and marginalization is exact.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 05:33:17 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 13:39:44 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Huang", "Alan", ""], ["Kim", "Andy Sangil", ""]]}, {"id": "1903.00351", "submitter": "Mohammad Arashi", "authors": "M. Kashani, M. Arashi, and M.R. Rabiei", "title": "Improving efficiency in fuzzy regression modeling by Stein-type\n  shrinkage", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy linear regression (FLR) modeling was first proposed making use of\nlinear programming and then followed by many improvements in a variety of ways.\nIn almost all approaches changing the meters, objective function, and\nrestrictions caused to improve the fuzzy measure of efficiencies (FMEs). In\nthis paper, from a totally different viewpoint, we apply shrinkage estimation\nstrategy to improve FMEs in the FLR modeling. By several illustrative examples,\nwe demonstrate the superiority of the proposed estimation method. In this\nrespect, we show fuzzy shrinkage estimates improve FMEs estimation dramatically\ncompared to the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 15:05:12 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Kashani", "M.", ""], ["Arashi", "M.", ""], ["Rabiei", "M. R.", ""]]}, {"id": "1903.00616", "submitter": "Hongcheng Liu", "authors": "Hongcheng Liu, Yinyu Ye", "title": "High-Dimensional Learning under Approximate Sparsity: A Unifying\n  Framework for Nonsmooth Learning and Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional statistical learning (HDSL) has been widely applied in data\nanalysis, operations research, and stochastic optimization. Despite the\navailability of multiple theoretical frameworks, most HDSL theories stipulate\nthe following two conditions, which are sometimes overly critical: (a) the\nsparsity, and (b) the restricted strong convexity (RSC). This paper generalizes\nboth conditions via the use of the folded concave penalty (FCP); we show that,\nfor an M-estimation problem where (i) the (conventional) sparsity is relaxed\ninto the approximate sparsity and (ii) the RSC is completely absent, the\nFCP-based regularization leads to poly-logarithmic sample complexity: the size\nof the training data is only required to be poly-logarithmic in the problem\ndimensionality. This finding allows us to further understand two important\nparadigms much less discussed formerly: the high-dimensional nonsmooth learning\nand the (deep) neural networks (NN). For both problems, we show that the\npoly-logarithmic sample complexity can be maintained. Furthermore, via\nintegrating the NN with the FCP, the excess risk of a stationary point to the\ntraining formulation for the NN is strictly monotonic with respect to the\nsolution's suboptimality gap, providing the first theoretical evidence for the\nempirically observed consistency between the generalization performance and the\noptimization quality in training an NN.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 03:21:21 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Liu", "Hongcheng", ""], ["Ye", "Yinyu", ""]]}, {"id": "1903.00655", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Viskakh Patel, Kasra Vakiloroayaei and Clement Wan", "title": "A Bayesian Nonparametric Estimation to Entropy", "comments": "20", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian nonparametric estimator to entropy is proposed. The derivation of\nthe new estimator relies on using the Dirichlet process and adapting the\nwell-known frequentist estimators of Vasicek (1976) and Ebrahimi, Pflughoeft\nand Soofi (1994). Several theoretical properties, such as consistency, of the\nproposed estimator are obtained. The quality of the proposed estimator has been\ninvestigated through several examples, in which it exhibits excellent\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 08:40:24 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 04:38:09 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 10:34:19 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 20:33:40 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Patel", "Viskakh", ""], ["Vakiloroayaei", "Kasra", ""], ["Wan", "Clement", ""]]}, {"id": "1903.00669", "submitter": "Luai Labadi Dr.", "authors": "Luai Al-Labadi, Viskakh Patel, Kasra Vakiloroayaei and Clement Wan", "title": "Kullback-Leibler Divergence for Bayesian Nonparametric Model Checking", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric statistics is an area of considerable research\ninterest. While recently there has been an extensive concentration in\ndeveloping Bayesian nonparametric procedures for model checking, the use of the\nDirichlet process, in its simplest form, along with the Kullback-Leibler\ndivergence is still an open problem. This is mainly attributed to the\ndiscreteness property of the Dirichlet process and that the Kullback-Leibler\ndivergence between any discrete distribution and any continuous distribution is\ninfinity. The approach proposed in this paper, which is based on incorporating\nthe Dirichlet process, the Kullback-Leibler divergence and the relative belief\nratio, is considered the first concrete solution to this issue. Applying the\napproach is simple and does not require obtaining a closed form of the relative\nbelief ratio. A Monte Carlo study and real data examples show that the\ndeveloped approach exhibits excellent performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 10:01:03 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 04:36:40 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Patel", "Viskakh", ""], ["Vakiloroayaei", "Kasra", ""], ["Wan", "Clement", ""]]}, {"id": "1903.00673", "submitter": "Marc Hoffmann", "authors": "Alexandre Boumezoued, Marc Hoffmann and Paulien Jeunesse", "title": "Nonparametric adaptive inference of birth and death models in a large\n  population limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by improving mortality tables from human demography databases, we\ninvestigate statistical inference of a stochastic age-evolving density of a\npopulation alimented by time inhomogeneous mortality and fertility. Asymptotics\nare taken as the size of the population grows within a limited time horizon:\nthe observation gets closer to the solution of the Von Foerster Mc Kendrick\nequation, and the difficulty lies in controlling simultaneously the stochastic\napproximation to the limiting PDE in a suitable sense together with an\nappropriate parametrisation of the anisotropic solution. In this setting, we\nprove new concentration inequalities that enable us to implement the\nGoldenshluger-Lepski algorithm and derive oracle inequalities. We obtain\nminimax optimality and adaptation over a wide range of anisotropic H\\\"older\nsmoothness classes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 10:10:52 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Boumezoued", "Alexandre", ""], ["Hoffmann", "Marc", ""], ["Jeunesse", "Paulien", ""]]}, {"id": "1903.00708", "submitter": "Phyllis Wan", "authors": "Phyllis Wan, Richard A. Davis", "title": "Goodness-of-Fit Testing for Time Series Models via Distance Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical modeling frameworks, goodness-of-fit tests are typically\nadministered to the estimated residuals. In the time series setting, whiteness\nof the residuals is assessed using the sample autocorrelation function. For\nmany time series models, especially those used for financial time series, the\nkey assumption on the residuals is that they are in fact independent and not\njust uncorrelated. In this paper, we apply the auto-distance covariance\nfunction (ADCV) to evaluate the serial dependence of the estimated residuals.\nDistance covariance can discriminate between dependence and independence of two\nrandom vectors. The limit behavior of the test statistic based on the ADCV is\nderived for a general class of time series models. One of the key aspects in\nthis theory is adjusting for the dependence that arises due to parameter\nestimation. This adjustment has essentially the same form regardless of the\nmodel specification. We illustrate the results in simulated examples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:25:46 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wan", "Phyllis", ""], ["Davis", "Richard A.", ""]]}, {"id": "1903.00851", "submitter": "Luai Al-Labadi Dr.", "authors": "Ibrahim Abdelrazeq and Luai Al-Labadi", "title": "On one-sample Bayesian tests for the mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a new Bayesian approach to the standard one-sample $z$-\nand $t$- tests. More specifically, let $x_1,\\ldots,x_n$ be an independent\nrandom sample from a normal distribution with mean $\\mu$ and variance\n$\\sigma^2$. The goal is to test the null hypothesis $\\mathcal{H}_0: \\mu=\\mu_1$\nagainst all possible alternatives. The approach is based on using the\nwell-known formula of the Kullbak-Leibler divergence between two normal\ndistributions (sampling and hypothesized distributions selected in an\nappropriate way). The change of the distance from a priori to a posteriori is\ncompared through the relative belief ratio (a measure of evidence). Eliciting\nthe prior, checking for prior-data conflict and bias are also considered. Many\ntheoretical properties of the procedure have been developed. Besides it's\nsimplicity, and unlike the classical approach, the new approach possesses\nattractive and distinctive features such as giving evidence in favor of the\nnull hypothesis. It also avoids several undesirable paradoxes, such as\nLindley's paradox that may be encountered by some existing Bayesian methods.\nThe use of the approach has been illustrated through several examples.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 07:47:52 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 04:32:25 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 09:28:03 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 20:05:53 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Abdelrazeq", "Ibrahim", ""], ["Al-Labadi", "Luai", ""]]}, {"id": "1903.00928", "submitter": "Zikun Yang", "authors": "Andrew Womack, Zikun Yang", "title": "Heavy Tailed Horseshoe Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally adaptive shrinkage in the Bayesian framework is achieved through the\nuse of local-global prior distributions that model both the global level of\nsparsity as well as individual shrinkage parameters for mean structure\nparameters. The most popular of these models is the Horseshoe prior and its\nvariants due to their spike and slab behavior involving an asymptote at the\norigin and heavy tails. In this article, we present an alternative Horseshoe\nprior that exhibits both a sharper asymptote at the origin as well as heavier\ntails, which we call the Heavy-tailed Horseshoe prior. We prove that mixing on\nthe shape parameters provides improved spike and slab behavior as well as\nbetter reconstruction properties than other Horseshoe variants. A simulation\nstudy is provided to show the advantage of the heavy-tailed Horseshoe in terms\nof absolute error to both the truth mean structure as well as the oracle.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 15:51:55 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Womack", "Andrew", ""], ["Yang", "Zikun", ""]]}, {"id": "1903.00961", "submitter": "Ryan Martin", "authors": "Ryan Martin and Yiqi Tang", "title": "Empirical priors for prediction in sparse high-dimensional linear\n  regression", "comments": "29 pages, 1 figure, 7 tables. Comments welcome at\n  https://www.researchers.one/article/2019-03-1", "journal-ref": "Journal of Machine Learning Research, 2020, volume 21, pages 1--30", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we adopt the familiar sparse, high-dimensional linear\nregression model and focus on the important but often overlooked task of\nprediction. In particular, we consider a new empirical Bayes framework that\nincorporates data in the prior in two ways: one is to center the prior for the\nnon-zero regression coefficients and the other is to provide some additional\nregularization. We show that, in certain settings, the asymptotic concentration\nof the proposed empirical Bayes posterior predictive distribution is very fast,\nand we establish a Bernstein--von Mises theorem which ensures that the derived\nempirical Bayes prediction intervals achieve the targeted frequentist coverage\nprobability. The empirical prior has a convenient conjugate form, so posterior\ncomputations are relatively simple and fast. Finally, our numerical results\ndemonstrate the proposed method's strong finite-sample performance in terms of\nprediction accuracy, uncertainty quantification, and computation time compared\nto existing Bayesian methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 18:54:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 14:32:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Martin", "Ryan", ""], ["Tang", "Yiqi", ""]]}, {"id": "1903.01050", "submitter": "Patrick L. Combettes", "authors": "Patrick L. Combettes and Christian L. M\\\"uller", "title": "Regression models for compositional data: General log-contrast\n  formulations, proximal optimization, and microbiome data applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data sets are ubiquitous in science, including geology,\necology, and microbiology. In microbiome research, compositional data primarily\narise from high-throughput sequence-based profiling experiments. These data\ncomprise microbial compositions in their natural habitat and are often paired\nwith covariate measurements that characterize physicochemical habitat\nproperties or the physiology of the host. Inferring parsimonious statistical\nassociations between microbial compositions and habitat- or host-specific\ncovariate data is an important step in exploratory data analysis. A standard\nstatistical model linking compositional covariates to continuous outcomes is\nthe linear log-contrast model. This model describes the response as a linear\ncombination of log-ratios of the original compositions and has been extended to\nthe high-dimensional setting via regularization. In this contribution, we\npropose a general convex optimization model for linear log-contrast regression\nwhich includes many previous proposals as special cases. We introduce a\nproximal algorithm that solves the resulting constrained optimization problem\nexactly with rigorous convergence guarantees. We illustrate the versatility of\nour approach by investigating the performance of several model instances on\nsoil and gut microbiome data analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 02:48:26 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Combettes", "Patrick L.", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "1903.01051", "submitter": "Gard Spreemann", "authors": "Katharine Turner, Gard Spreemann", "title": "Same But Different: Distance Correlations Between Topological Summaries", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology allows us to create topological summaries of complex\ndata. In order to analyse these statistically, we need to choose a topological\nsummary and a relevant metric space in which this topological summary exists.\nWhile different summaries may contain the same information (as they come from\nthe same persistence module), they can lead to different statistical\nconclusions since they lie in different metric spaces. The best choice of\nmetric will often be application-specific. In this paper we discuss distance\ncorrelation, which is a non-parametric tool for comparing data sets that can\nlie in completely different metric spaces. In particular we calculate the\ndistance correlation between different choices of topological summaries. We\ncompare some different topological summaries for a variety of random models of\nunderlying data via the distance correlation between the samples. We also give\nexamples of performing distance correlation between topological summaries and\nother scalar measures of interest, such as a paired random variable or a\nparameter of the random model used to generate the underlying data. This\narticle is meant to be expository in style, and will include the definitions of\nstandard statistical quantities in order to be accessible to non-statisticians.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 02:57:16 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 12:28:31 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Turner", "Katharine", ""], ["Spreemann", "Gard", ""]]}, {"id": "1903.01138", "submitter": "Irene Tubikanec", "authors": "Evelyn Buckwar, Massimiliano Tamborrino, Irene Tubikanec", "title": "Spectral Density-Based and Measure-Preserving ABC for partially observed\n  diffusion processes. An illustration on Hamiltonian SDEs", "comments": "35 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) has become one of the major tools of\nlikelihood-free statistical inference in complex mathematical models.\nSimultaneously, stochastic differential equations (SDEs) have developed to an\nestablished tool for modelling time dependent, real world phenomena with\nunderlying random effects. When applying ABC to stochastic models, two major\ndifficulties arise. First, the derivation of effective summary statistics and\nproper distances is particularly challenging, since simulations from the\nstochastic process under the same parameter configuration result in different\ntrajectories. Second, exact simulation schemes to generate trajectories from\nthe stochastic model are rarely available, requiring the derivation of suitable\nnumerical methods for the synthetic data generation. To obtain summaries that\nare less sensitive to the intrinsic stochasticity of the model, we propose to\nbuild up the statistical method (e.g., the choice of the summary statistics) on\nthe underlying structural properties of the model. Here, we focus on the\nexistence of an invariant measure and we map the data to their estimated\ninvariant density and invariant spectral density. Then, to ensure that these\nmodel properties are kept in the synthetic data generation, we adopt\nmeasure-preserving numerical splitting schemes. The derived property-based and\nmeasure-preserving ABC method is illustrated on the broad class of partially\nobserved Hamiltonian type SDEs, both with simulated data and with real\nelectroencephalography (EEG) data. The proposed ingredients can be incorporated\ninto any type of ABC algorithm and directly applied to all SDEs that are\ncharacterised by an invariant distribution and for which a measure-preserving\nnumerical method can be derived.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 08:59:02 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 16:47:20 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Buckwar", "Evelyn", ""], ["Tamborrino", "Massimiliano", ""], ["Tubikanec", "Irene", ""]]}, {"id": "1903.01253", "submitter": "Michael Vogt", "authors": "Marina Khismatullina, Michael Vogt", "title": "Multiscale inference and long-run variance estimation in nonparametric\n  regression with time series errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop new multiscale methods to test qualitative\nhypotheses about the regression function m in a nonparametric regression model\nwith fixed design points and time series errors. In time series applications, m\nrepresents a nonparametric time trend. Practitioners are often interested in\nwhether the trend m has certain shape properties. For example, they would like\nto know whether m is constant or whether it is increasing/decreasing in certain\ntime regions. Our multiscale methods allow to test for such shape properties of\nthe trend m. In order to perform the methods, we require an estimator of the\nlong-run variance of the error process. We propose a new difference-based\nestimator of the long-run error variance for the case that the error terms form\nan AR(p) process. In the technical part of the paper, we derive asymptotic\ntheory for the proposed multiscale test and the estimator of the long-run error\nvariance. The theory is complemented by a simulation study and an empirical\napplication to climate data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:13:16 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Khismatullina", "Marina", ""], ["Vogt", "Michael", ""]]}, {"id": "1903.01310", "submitter": "Arvind Prasadan", "authors": "Arvind Prasadan and Raj Rao Nadakuditi", "title": "Time Series Source Separation using Dynamic Mode Decomposition", "comments": "Accepted in SIADS (SIAM's Journal of Applied Dynamical Systems)", "journal-ref": null, "doi": "10.5281/zenodo.2656681", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Dynamic Mode Decomposition (DMD) extracted dynamic modes are the\nnon-orthogonal eigenvectors of the matrix that best approximates the one-step\ntemporal evolution of the multivariate samples. In the context of dynamical\nsystem analysis, the extracted dynamic modes are a generalization of global\nstability modes. We apply DMD to a data matrix whose rows are linearly\nindependent, additive mixtures of latent time series. We show that when the\nlatent time series are uncorrelated at a lag of one time-step then, in the\nlarge sample limit, the recovered dynamic modes will approximate, up to a\ncolumn-wise normalization, the columns of the mixing matrix. Thus, DMD is a\ntime series blind source separation algorithm in disguise, but is different\nfrom closely related second order algorithms such as the Second-Order Blind\nIdentification (SOBI) method and the Algorithm for Multiple Unknown Signals\nExtraction (AMUSE). All can unmix mixed stationary, ergodic Gaussian time\nseries in a way that kurtosis-based Independent Components Analysis (ICA)\nfundamentally cannot. We use our insights on single lag DMD to develop a\nhigher-lag extension, analyze the finite sample performance with and without\nrandomly missing data, and identify settings where the higher lag variant can\noutperform the conventional single lag variant. We validate our results with\nnumerical simulations, and highlight how DMD can be used in change point\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 15:37:14 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 17:08:16 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 16:16:05 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 21:57:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Prasadan", "Arvind", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1903.01334", "submitter": "Florian Dumpert", "authors": "Florian Dumpert", "title": "Quantitative Robustness of Localized Support Vector Machines", "comments": "arXiv admin note: text overlap with arXiv:1703.06528", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The huge amount of available data nowadays is a challenge for kernel-based\nmachine learning algorithms like SVMs with respect to runtime and storage\ncapacities. Local approaches might help to relieve these issues and to improve\nstatistical accuracy. It has already been shown that these local approaches are\nconsistent and robust in a basic sense. This article refines the analysis of\nrobustness properties towards the so-called influence function which expresses\nthe differentiability of the learning method: We show that there is a\ndifferentiable dependency of our locally learned predictor on the underlying\ndistribution. The assumptions of the proven theorems can be verified without\nknowing anything about this distribution. This makes the results interesting\nalso from an applied point of view.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 15:12:15 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dumpert", "Florian", ""]]}, {"id": "1903.01395", "submitter": "Adityanand Guntuboyina", "authors": "Billy Fang and Adityanand Guntuboyina and Bodhisattva Sen", "title": "Multivariate extensions of isotonic regression and total variation\n  denoising via entire monotonicity and Hardy-Krause variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of nonparametric regression when the covariate is\n$d$-dimensional, where $d \\geq 1$. In this paper we introduce and study two\nnonparametric least squares estimators (LSEs) in this setting---the entirely\nmonotonic LSE and the constrained Hardy-Krause variation LSE. We show that\nthese two LSEs are natural generalizations of univariate isotonic regression\nand univariate total variation denoising, respectively, to multiple dimensions.\nWe discuss the characterization and computation of these two LSEs obtained from\n$n$ data points. We provide a detailed study of their risk properties under the\nsquared error loss and fixed uniform lattice design. We show that the finite\nsample risk of these LSEs is always bounded from above by $n^{-2/3}$ modulo\nlogarithmic factors depending on $d$; thus these nonparametric LSEs avoid the\ncurse of dimensionality to some extent. We also prove nearly matching minimax\nlower bounds. Further, we illustrate that these LSEs are particularly useful in\nfitting rectangular piecewise constant functions. Specifically, we show that\nthe risk of the entirely monotonic LSE is almost parametric (at most $1/n$ up\nto logarithmic factors) when the true function is well-approximable by a\nrectangular piecewise constant entirely monotone function with not too many\nconstant pieces. A similar result is also shown to hold for the constrained\nHardy-Krause variation LSE for a simple subclass of rectangular piecewise\nconstant functions. We believe that the proposed LSEs yield a novel approach to\nestimating multivariate functions using convex optimization that avoid the\ncurse of dimensionality to some extent.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 17:43:42 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 07:35:35 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 20:07:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Fang", "Billy", ""], ["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1903.01430", "submitter": "Wanli Qiao", "authors": "Wanli Qiao and Wolfgang Polonik", "title": "Nonparametric Confidence Regions for Level Sets: Statistical Properties\n  and Geometry", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies and critically discusses the construction of nonparametric\nconfidence regions for density level sets. Methodologies based on both vertical\nvariation and horizontal variation are considered. The investigations provide\ntheoretical insight into the behavior of these confidence regions via large\nsample theory. We also discuss the geometric relationships underlying the\nconstruction of horizontal and vertical methods, and how finite sample\nperformance of these confidence regions is influenced by geometric or\ntopological aspects. These discussions are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:44:00 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Qiao", "Wanli", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1903.01432", "submitter": "Yi Hao", "authors": "Yi Hao, Alon Orlitsky", "title": "Data Amplification: Instance-Optimal Property Estimation", "comments": "In this new version, we strengthened the previous results by\n  eliminating unnecessary assumptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best-known and most commonly used distribution-property estimation\ntechnique uses a plug-in estimator, with empirical frequency replacing the\nunderlying distribution. We present novel linear-time-computable estimators\nthat significantly \"amplify\" the effective amount of data available. For a\nlarge variety of distribution properties including four of the most popular\nones and for every underlying distribution, they achieve the accuracy that the\nempirical-frequency plug-in estimators would attain using a logarithmic-factor\nmore samples.\n  Specifically, for Shannon entropy and a very broad class of properties\nincluding $\\ell_1$-distance, the new estimators use $n$ samples to achieve the\naccuracy attained by the empirical estimators with $n\\log n$ samples. For\nsupport-size and coverage, the new estimators use $n$ samples to achieve the\nperformance of empirical frequency with sample size $n$ times the logarithm of\nthe property value. Significantly strengthening the traditional min-max\nformulation, these results hold not only for the worst distributions, but for\neach and every underlying distribution. Furthermore, the logarithmic\namplification factors are optimal. Experiments on a wide variety of\ndistributions show that the new estimators outperform the previous\nstate-of-the-art estimators designed for each specific property.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:55:09 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 18:55:10 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Hao", "Yi", ""], ["Orlitsky", "Alon", ""]]}, {"id": "1903.01459", "submitter": "Michael Vogt", "authors": "Michael Vogt, Oliver Linton", "title": "Multiscale clustering of nonparametric regression curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of modern applications, we observe a large number of time\nseries rather than only a single one. It is often natural to suppose that there\nis some group structure in the observed time series. When each time series is\nmodelled by a nonparametric regression equation, one may in particular assume\nthat the observed time series can be partitioned into a small number of groups\nwhose members share the same nonparametric regression function. We develop a\nbandwidth-free clustering method to estimate the unknown group structure from\nthe data. More precisely speaking, we construct multiscale estimators of the\nunknown groups and their unknown number which are free of classical bandwidth\nor smoothing parameters. In the theoretical part of the paper, we analyze the\nstatistical properties of our estimators. Our theoretical results are derived\nunder general conditions which allow the data to be dependent both in time\nseries direction and across different time series. The technical analysis of\nthe paper is complemented by a simulation study and a real-data application.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:34:53 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Vogt", "Michael", ""], ["Linton", "Oliver", ""]]}, {"id": "1903.01500", "submitter": "Wentao Huang", "authors": "Wentao Huang and Kechen Zhang", "title": "Approximations of Shannon Mutual Information for Discrete Variables with\n  Applications to Neural Population Coding", "comments": "31 pages, 6 figures", "journal-ref": "Entropy 2019, 21(3), 243", "doi": "10.3390/e21030243", "report-no": null, "categories": "cs.IT math.IT math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Shannon mutual information has been widely used, its effective\ncalculation is often difficult for many practical problems, including those in\nneural population coding. Asymptotic formulas based on Fisher information\nsometimes provide accurate approximations to the mutual information but this\napproach is restricted to continuous variables because the calculation of\nFisher information requires derivatives with respect to the encoded variables.\nIn this paper, we consider information-theoretic bounds and approximations of\nthe mutual information based on Kullback--Leibler divergence and R\\'{e}nyi\ndivergence. We propose several information metrics to approximate Shannon\nmutual information in the context of neural population coding. While our\nasymptotic formulas all work for discrete variables, one of them has consistent\nperformance and high accuracy regardless of whether the encoded variables are\ndiscrete or continuous. We performed numerical simulations and confirmed that\nour approximation formulas were highly accurate for approximating the mutual\ninformation between the stimuli and the responses of a large neural population.\nThese approximation formulas may potentially bring convenience to the\napplications of information theory to many practical and theoretical problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:33:03 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Huang", "Wentao", ""], ["Zhang", "Kechen", ""]]}, {"id": "1903.01661", "submitter": "Thomas Flynn", "authors": "Thomas Flynn, Shinjae Yoo", "title": "Change Detection with the Kernel Cumulative Sum Algorithm", "comments": "8 pages; Minor changes, as published in CDC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online change detection involves monitoring a stream of data for changes in\nthe statistical properties of incoming observations. A good change detector\nwill detect any changes shortly after they occur, while raising few false\nalarms. Although there are algorithms with confirmed optimality properties for\nthis task, they rely on the exact specifications of the relevant probability\ndistributions and this limits their practicality. In this work we describe a\nkernel-based variant of the Cumulative Sum (CUSUM) change detection algorithm\nthat can detect changes under less restrictive assumptions. Instead of using\nthe likelihood ratio, which is a parametric quantity, the Kernel CUSUM (KCUSUM)\nalgorithm compares incoming data with samples from a reference distribution\nusing a statistic based on the Maximum Mean Discrepancy (MMD) non-parametric\ntesting framework. The KCUSUM algorithm is applicable in settings where there\nis a large amount of background data available and it is desirable to detect a\nchange away from this background setting. Exploiting the random-walk structure\nof the test statistic, we derive bounds on the performance of the algorithm,\nincluding the expected delay and the average time to false alarm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 04:30:17 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 22:04:54 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Flynn", "Thomas", ""], ["Yoo", "Shinjae", ""]]}, {"id": "1903.01679", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen", "title": "Concentration-based confidence intervals for U-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concentration inequalities have become increasingly popular in machine\nlearning, probability, and statistical research. Using concentration\ninequalities, one can construct confidence intervals (CIs) for many quantities\nof interest. Unfortunately, many of these CIs require the knowledge of\npopulation variances, which are generally unknown, making these CIs impractical\nfor numerical application. However, recent results regarding the simultaneous\nbounding of the probabilities of quantities of interest and their variances\nhave permitted the construction of empirical CIs, where variances are replaced\nby their sample estimators. Among these new results are two-sided empirical CIs\nfor U-statistics, which are useful for the construction of CIs for a rich class\nof parameters. In this article, we derive a number of new one-sided empirical\nCIs for U-statistics and their variances. We show that our one-sided CIs can be\nused to construct tighter two-sided CIs for U-statistics, than those currently\nreported. We also demonstrate how our CIs can be used to construct new\nempirical CIs for the mean, which provide tighter bounds than currently known\nCIs for the same number of observations, under various settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 05:35:33 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Nguyen", "Hien D.", ""]]}, {"id": "1903.01696", "submitter": "Michael Evans", "authors": "Michael Evans and Yang Guo", "title": "Measuring and Controlling Bias for Some Bayesian Inferences and the\n  Relation to Frequentist Criteria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common concern with Bayesian methodology in scientific contexts is that\ninferences can be heavily influenced by subjective biases. As presented here,\nthere are two types of bias for some quantity of interest: bias against and\nbias in favor. Based upon the principle of evidence, it is shown how to measure\nand control these biases for both hypothesis assessment and estimation\nproblems. Optimality results are established for the principle of evidence as\nthe basis of the approach to these problems. A close relationship is\nestablished between measuring bias in Bayesian inferences and frequentist\nproperties that hold for any proper prior. This leads to a possible resolution\nto an apparent conflict between these approaches to statistical reasoning.\nFrequentism is seen as establishing a figure of merit for a statistical study,\nwhile Bayesianism plays the key role in determining inferences based upon\nstatistical evidence.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 06:47:13 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Evans", "Michael", ""], ["Guo", "Yang", ""]]}, {"id": "1903.01706", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "Tutorial: Deriving The Efficient Influence Curve for Large Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to provide a tutorial for upper level undergraduate and\ngraduate students in statistics, biostatistics and epidemiology on deriving\ninfluence functions for non-parametric and semi-parametric models. The author\nwill build on previously known efficiency theory and provide a useful identity\nand formulaic technique only relying on the basics of integration which, are\nself-contained in this tutorial and can be used in most any setting one might\nencounter in practice. The paper provides many examples of such derivations for\nwell-known influence functions as well as for new parameters of interest. The\ninfluence function remains a central object for constructing efficient\nestimators for large models, such as the one-step estimator and the targeted\nmaximum likelihood estimator. We will not touch upon these estimators at all\nbut readers familiar with these estimators might find this tutorial of\nparticular use.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:38:17 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:52:51 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 07:20:46 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1903.01927", "submitter": "Martin Kroll", "authors": "Cristina Butucea, Amandine Dubois, Martin Kroll and Adrien Saumard", "title": "Local differential privacy: Elbow effect in optimal density estimation\n  and adaptation over Besov ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of non-parametric density estimation under the\nadditional constraint that only privatised data are allowed to be published and\navailable for inference. For this purpose, we adopt a recent generalisation of\nclassical minimax theory to the framework of local $\\alpha$-differential\nprivacy and provide a lower bound on the rate of convergence over Besov spaces\n$B^s_{pq}$ under mean integrated $\\mathbb L^r$-risk. This lower bound is\ndeteriorated compared to the standard setup without privacy, and reveals a\ntwofold elbow effect. In order to fulfil the privacy requirement, we suggest\nadding suitably scaled Laplace noise to empirical wavelet coefficients. Upper\nbounds within (at most) a logarithmic factor are derived under the assumption\nthat $\\alpha$ stays bounded as $n$ increases: A linear but non-adaptive wavelet\nestimator is shown to attain the lower bound whenever $p \\geq r$ but provides a\nslower rate of convergence otherwise. An adaptive non-linear wavelet estimator\nwith appropriately chosen smoothing parameters and thresholding is shown to\nattain the lower bound within a logarithmic factor for all cases.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 16:33:51 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Butucea", "Cristina", ""], ["Dubois", "Amandine", ""], ["Kroll", "Martin", ""], ["Saumard", "Adrien", ""]]}, {"id": "1903.01944", "submitter": "Chao Gao", "authors": "Chao Gao, Yuan Yao, Weizhi Zhu", "title": "Generative Adversarial Nets for Robust Scatter Estimation: A Proper\n  Scoring Rule Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust scatter estimation is a fundamental task in statistics. The recent\ndiscovery on the connection between robust estimation and generative\nadversarial nets (GANs) by Gao et al. (2018) suggests that it is possible to\ncompute depth-like robust estimators using similar techniques that optimize\nGANs. In this paper, we introduce a general learning via classification\nframework based on the notion of proper scoring rules. This framework allows us\nto understand both matrix depth function and various GANs through the lens of\nvariational approximations of $f$-divergences induced by proper scoring rules.\nWe then propose a new class of robust scatter estimators in this framework by\ncarefully constructing discriminators with appropriate neural network\nstructures. These estimators are proved to achieve the minimax rate of scatter\nestimation under Huber's contamination model. Our numerical results demonstrate\nits good performance under various settings against competitors in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:29:04 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gao", "Chao", ""], ["Yao", "Yuan", ""], ["Zhu", "Weizhi", ""]]}, {"id": "1903.01991", "submitter": "Yair Goldberg", "authors": "Yair Goldberg", "title": "Hoeffding-Type and Bernstein-Type Inequalities for Right Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hoeffding-type and Bernstein-type inequalities for right-censored\ndata. The inequalities bound the difference between an inverse of the\nprobability of censoring weighting (IPCW) estimator and its expectation. We\nfirst discuss the asymptotic properties of the estimator and provide conditions\nfor its efficiency. We present standard, data dependent, and uniform\nHoeffding-type inequalities. We then present a Bernstein-type inequality.\nFinally, we show how to apply these inequalities in an empirical risk\nminimization setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:49:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Goldberg", "Yair", ""]]}, {"id": "1903.02131", "submitter": "David J. Aldous", "authors": "David Aldous", "title": "A Prediction Tournament Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a prediction tournament, contestants \"forecast\" by asserting a numerical\nprobability for each of (say) 100 future real-world events. The scoring system\nis designed so that (regardless of the unknown true probabilities) more\naccurate forecasters will likely score better. This is true for one-on-one\ncomparisons between contestants. But consider a realistic-size tournament with\nmany contestants, with a range of accuracies. It may seem self-evident that the\nwinner will likely be one of the most accurate forecasters. But, in the setting\nwhere the range extends to very accurate forecasters, simulations show this is\nmathematically false, within a somewhat plausible model. Even outside that\nsetting the winner is less likely than intuition suggests to be one of the\nhandful of best forecasters. Though implicit in recent technical papers, this\nparadox has apparently not been explicitly pointed out before, though is easily\nexplained. It perhaps has implications for the ongoing IARPA-sponsored research\nprograms involving forecasting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 15:44:47 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Aldous", "David", ""]]}, {"id": "1903.02364", "submitter": "Radomyra Shevchenko", "authors": "Jan Gairing, Peter Imkeller, Radomyra Shevchenko, Ciprian A. Tudor", "title": "Hurst index estimation in stochastic differential equations driven by\n  fractional Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Hurst index estimation for solutions of stochastic\ndifferential equations driven by an additive fractional Brownian motion. Using\ntechniques of the Malliavin calculus, we analyze the asymptotic behavior of the\nquadratic variations of the solution, defined via higher order increments. Then\nwe apply our results to construct and study estimators for the Hurst index.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:29:14 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Gairing", "Jan", ""], ["Imkeller", "Peter", ""], ["Shevchenko", "Radomyra", ""], ["Tudor", "Ciprian A.", ""]]}, {"id": "1903.02369", "submitter": "Radomyra Shevchenko", "authors": "Radomyra Shevchenko, Meryem Slaoui, Ciprian A. Tudor", "title": "Generalized $k$-variations and Hurst parameter estimation for the\n  fractional wave equation via Malliavin calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the generalized $k$-variations for the solution to the wave\nequation driven by an additive Gaussian noise which behaves as a fractional\nBrownian with Hurst parameter $H>\\frac{1}{2}$ in time and which is white in\nspace. The $k$-variations are defined along {\\it filters} of any order $p\\geq\n1$ and of any length. We show that the sequence of generalized $k$-variation\nsatisfies a Central Limit Theorem when $p> H+\\frac{1}{4}$ and we estimate the\nrate of convergence for it via the Stein-Malliavin calculus. The results are\napplied to the estimation of the Hurst index. We construct several consistent\nestimators for $H$ and these estimators are analyzed theoretically and\nnumerically.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:37:56 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Shevchenko", "Radomyra", ""], ["Slaoui", "Meryem", ""], ["Tudor", "Ciprian A.", ""]]}, {"id": "1903.02376", "submitter": "Radomyra Shevchenko", "authors": "Radomyra Shevchenko, Ciprian A. Tudor", "title": "Parameter estimation for the Rosenblatt Ornstein-Uhlenbeck process with\n  periodic mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the least squares estimator for the drift parameter of the Langevin\nstochastic equation driven by the Rosenblatt process. Using the techniques of\nthe Malliavin calculus and the stochastic integration with respect to the\nRosenblatt process, we analyze the consistency and the asymptotic distribution\nof this estimator. We also introduce alternative estimators, which can be\nsimulated, and we study their asymptotic properties.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:43:47 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Shevchenko", "Radomyra", ""], ["Tudor", "Ciprian A.", ""]]}, {"id": "1903.02603", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "Nonparametric Change Point Detection in Regression", "comments": "Validity result is simplified and improved. Typos fixed, style\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the prominent problem of change-point detection in\nregression. The study suggests a novel testing procedure featuring a fully\ndata-driven calibration scheme. The method is essentially a black box,\nrequiring no tuning from the practitioner. The approach is investigated from\nboth theoretical and practical points of view. The theoretical study\ndemonstrates proper control of first-type error rate under $H_0$ and power\napproaching $1$ under $H_1$. The experiments conducted on synthetic data fully\nsupport the theoretical claims. In conclusion, the method is applied to\nfinancial data, where it detects sensible change-points. Techniques for\nchange-point localization are also suggested and investigated.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 20:34:05 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 22:59:56 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 11:41:17 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "1903.02653", "submitter": "Donald Richards", "authors": "Elena Hadjicosta, Donald Richards", "title": "Integral Transform Methods in Goodness-of-Fit Testing, II: The Wishart\n  Distributions", "comments": "89 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of goodness-of-fit testing when the data consist of\npositive definite matrices. Motivated by the recent appearance of the cone of\npositive definite matrices in numerous areas of applied research, including\ndiffusion tensor imaging, models of the volatility of financial time series,\nwireless communication systems, and the analysis of polarimetric radar images,\nwe apply the method of Hankel transforms of matrix argument to develop\ngoodness-of-fit tests for Wishart distributions with given shape parameter and\nunknown scale matrix. We obtain the limiting null distribution of the test\nstatistic and the corresponding covariance operator. We show that the\neigenvalues of the operator satisfy an interlacing property, and we apply our\ntest to some financial data. Moreover, we establish the consistency of the test\nagainst a large class of alternative distributions and we derive the asymptotic\ndistribution of the test statistic under a sequence of contiguous alternatives.\nWe establish the Bahadur and Pitman efficiency properties of the test statistic\nand we show the validity of a modified Wieand condition.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 23:15:48 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hadjicosta", "Elena", ""], ["Richards", "Donald", ""]]}, {"id": "1903.02676", "submitter": "Junjie Ma", "authors": "Rishabh Dudeja, Milad Bakhshizadeh, Junjie Ma, Arian Maleki", "title": "Analysis of Spectral Methods for Phase Retrieval with Random Orthogonal\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval refers to algorithmic methods for recovering a signal from\nits phaseless measurements. Local search algorithms that work directly on the\nnon-convex formulation of the problem have been very popular recently. Due to\nthe nonconvexity of the problem, the success of these local search algorithms\ndepends heavily on their starting points. The most widely used initialization\nscheme is the spectral method, in which the leading eigenvector of a\ndata-dependent matrix is used as a starting point. Recently, the performance of\nthe spectral initialization was characterized accurately for measurement\nmatrices with independent and identically distributed entries. This paper aims\nto obtain the same level of knowledge for isotropically random\ncolumn-orthogonal matrices, which are substantially better models for practical\nphase retrieval systems. Towards this goal, we consider the asymptotic setting\nin which the number of measurements $m$, and the dimension of the signal, $n$,\ndiverge to infinity with $m/n = \\delta\\in(1,\\infty)$, and obtain a simple\nexpression for the overlap between the spectral estimator and the true signal\nvector.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 01:10:48 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 04:28:42 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Dudeja", "Rishabh", ""], ["Bakhshizadeh", "Milad", ""], ["Ma", "Junjie", ""], ["Maleki", "Arian", ""]]}, {"id": "1903.03089", "submitter": "Matt Wand", "authors": "Tui H. Nolan and Matt P. Wand", "title": "Solutions to Multilevel Sparse Matrix Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and solve classes of sparse matrix problems that arise in\nmultilevel modeling and data analysis. The classes are indexed by the number of\nnested units, with two-level problems corresponding to the common situation in\nwhich data on level 1 units are grouped within a two-level structure. We\nprovide full solutions for two-level and three-level problems and their\nderivations provide blueprints for the challenging, albeit rarer in\napplications, higher level versions of the problem. Whilst our linear system\nsolutions are a concise recasting of existing results, our matrix inverse\nsub-block results are novel and facilitate streamlined computation of standard\nerrors in frequentist inference as well as allowing streamlined mean field\nvariational Bayesian inference for models containing higher level random\neffects.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:32:35 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 05:32:56 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 04:01:14 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Nolan", "Tui H.", ""], ["Wand", "Matt P.", ""]]}, {"id": "1903.03242", "submitter": "Takuma Yoshida", "authors": "Takuma Yoshida", "title": "Nonparametric smoothing for extremal quantile regression with heavy\n  tailed distributions", "comments": "32 pages, 48 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several different fields, there is interest in analyzing the upper or\nlower tail quantile of the underlying distribution rather than mean or center\nquantile. However, the investigation of the tail quantile is difficult because\nof data sparsity. In this paper, we attempt to develop nonparametric quantile\nregression for the extremal quantile level. In extremal quantile regression,\nthere are two types of technical conditions of the order of convergence of the\nquantile level: intermediate order or extreme order. For the intermediate order\nquantile, the ordinary nonparametric estimator is used. On the other hand, for\nthe extreme order quantile, we provide a new estimator by extrapolating the\nintermediate order quantile estimator. The performance of the estimator is\nguaranteed by asymptotic theory and extreme value theory. As a result, we show\nthe asymptotic normality and the rate of convergence of the nonparametric\nquantile regression estimator for both intermediate and extreme order\nquantiles. A simulation is presented to confirm the behavior of the proposed\nestimator. The data application is also assessed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 01:46:33 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 06:07:46 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Yoshida", "Takuma", ""]]}, {"id": "1903.03280", "submitter": "Johannes Krebs", "authors": "Johannes T. N. Krebs and Wolfgang Polonik", "title": "On the asymptotic normality of persistent Betti numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent Betti numbers are a major tool in persistent homology, a subfield\nof topological data analysis. Many tools in persistent homology rely on the\nproperties of persistent Betti numbers considered as a two-dimensional\nstochastic process $ (r,s) \\mapsto n^{-1/2} (\\beta^{r,s}_q (\n\\mathcal{K}(n^{1/d} S_n))-\\mathbb{E}[\\beta^{r,s}_q ( \\mathcal{K}( n^{1/d}\nS_n))])$. So far, pointwise limit theorems have been established in different\nset-ups. In particular, the pointwise asymptotic normality of (persistent)\nBetti numbers has been established for stationary Poisson processes and\nbinomial processes with constant intensity function in the so-called critical\n(or thermodynamic) regime, see Yogeshwaran et al. [2017] and Hiraoka et al.\n[2018].\n  In this contribution, we derive a strong stabilizing property (in the spirit\nof Penrose and Yukich [2001] of persistent Betti numbers and generalize the\nexisting results on the asymptotic normality to the multivariate case and to a\nbroader class of underlying Poisson and binomial processes. Most importantly,\nwe show that the multivariate asymptotic normality holds for all pairs $(r,s)$,\n$0\\le r\\le s<\\infty$, and that it is not affected by percolation effects in the\nunderlying random geometric graph.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:41:20 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 09:04:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Krebs", "Johannes T. N.", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1903.03304", "submitter": "Suparna Biswas", "authors": "Suparna Biswas and Rituparna Sen", "title": "Kernel Based Estimation of Spectral Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral risk measures (SRMs) belong to the family of coherent risk measures.\nA natural estimator for the class of SRMs has the form of L-statistics. Various\nauthors have studied and derived the asymptotic properties of the empirical\nestimator of SRM. We propose a kernel based estimator of SRM. We investigate\nthe large sample properties of general L-statistics based on i.i.d and\ndependent observations and apply them to our estimator. We prove that it is\nstrongly consistent and asymptotically normal. We compare the finite sample\nperformance of our proposed kernel estimator with that of several existing\nestimators for different SRMs using Monte Carlo simulation. We observe that our\nproposed kernel estimator outperforms all the estimators. Based on our\nsimulation study we have estimated the exponential SRM of four future\nindices-that is Nikkei 225, Dax, FTSE 100, and Hang Seng. We also perform a\nbacktesting exercise of SRM.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 06:43:08 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 13:55:53 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 06:43:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Biswas", "Suparna", ""], ["Sen", "Rituparna", ""]]}, {"id": "1903.03440", "submitter": "Simon Holbach", "authors": "Simon Holbach", "title": "Local asymptotic normality for shape and periodicity of a signal in the\n  drift of a degenerate diffusion with internal variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking a multidimensional time-homogeneous dynamical system and adding a\nrandomly perturbed time-dependent deterministic signal to some of its\ncomponents gives rise to a high-dimensional system of stochastic differential\nequations which is driven by possibly very low-dimensional noise. Equations of\nthis type are commonly used in biology for modeling neurons or in statistical\nmechanics for certain Hamiltonian systems. Assuming that the signal depends on\nan unknown shape parameter $\\theta$ and also has an unknown periodicity $T$, we\nprove Local Asymptotic Normality (LAN) jointly in $\\theta$ and $T$ for the\nstatistical experiment arising from (partial) observation of this diffusion in\ncontinuous time. The local scale turns out to be $n^{-1/2}$ for $\\theta$ and\n$n^{-3/2}$ for $T$ which generalizes known results for simpler systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 13:35:10 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 17:14:54 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 13:30:04 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Holbach", "Simon", ""]]}, {"id": "1903.03531", "submitter": "Xuan Cao", "authors": "Xuan Cao, Kshitij Khare, Malay Ghosh", "title": "Consistent Bayesian Sparsity Selection for High-dimensional Gaussian DAG\n  Models with Multiplicative and Beta-mixture Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix for high-dimensional multivariate\ndatasets is a challenging and important problem in modern statistics. In this\npaper, we focus on high-dimensional Gaussian DAG models where sparsity is\ninduced on the Cholesky factor L of the inverse covariance matrix. In recent\nwork, ([Cao, Khare, and Ghosh, 2019]), we established high-dimensional sparsity\nselection consistency for a hierarchical Bayesian DAG model, where an\nErdos-Renyi prior is placed on the sparsity pattern in the Cholesky factor L,\nand a DAG-Wishart prior is placed on the resulting non-zero Cholesky entries.\nIn this paper we significantly improve and extend this work, by (a) considering\nmore diverse and effective priors on the sparsity pattern in L, namely the\nbeta-mixture prior and the multiplicative prior, and (b) establishing sparsity\nselection consistency under significantly relaxed conditions on p, and the\nsparsity pattern of the true model. We demonstrate the validity of our\ntheoretical results via numerical simulations, and also use further simulations\nto demonstrate that our sparsity selection approach is competitive with\nexisting state-of-the-art methods including both frequentist and Bayesian\napproaches in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:20:59 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Cao", "Xuan", ""], ["Khare", "Kshitij", ""], ["Ghosh", "Malay", ""]]}, {"id": "1903.03824", "submitter": "Daniel Rudolf", "authors": "Viacheslav Natarovskii, Daniel Rudolf, Bj\\\"orn Sprungk", "title": "Quantitative spectral gap estimate and Wasserstein contraction of simple\n  slice sampling", "comments": "24 pages, 6 figures, accepted for publication in Ann. Appl. Probab", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove Wasserstein contraction of simple slice sampling for approximate\nsampling w.r.t. distributions with log-concave and rotational invariant\nLebesgue densities. This yields, in particular, an explicit quantitative lower\nbound of the spectral gap of simple slice sampling. Moreover, this lower bound\ncarries over to more general target distributions depending only on the volume\nof the (super-)level sets of their unnormalized density.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 16:32:32 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 13:23:57 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Natarovskii", "Viacheslav", ""], ["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "1903.03919", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho, Vu Dinh, Frederick A. Matsen IV, Marc A. Suchard", "title": "On the convergence of the maximum likelihood estimator for the\n  transition rate under a 2-state symmetric model", "comments": null, "journal-ref": null, "doi": "10.1007/s00285-019-01453-1", "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimators are used extensively to estimate unknown\nparameters of stochastic trait evolution models on phylogenetic trees. Although\nthe MLE has been proven to converge to the true value in the independent-sample\ncase, we cannot appeal to this result because trait values of different species\nare correlated due to shared evolutionary history. In this paper, we consider a\n$2$-state symmetric model for a single binary trait and investigate the\ntheoretical properties of the MLE for the transition rate in the large-tree\nlimit. Here, the large-tree limit is a theoretical scenario where the number of\ntaxa increases to infinity and we can observe the trait values for all species.\nSpecifically, we prove that the MLE converges to the true value under some\nregularity conditions. These conditions ensure that the tree shape is not too\nirregular, and holds for many practical scenarios such as trees with bounded\nedges, trees generated from the Yule (pure birth) process, and trees generated\nfrom the coalescent point process. Our result also provides an upper bound for\nthe distance between the MLE and the true value.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 04:20:33 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 19:18:12 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 03:13:19 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Dinh", "Vu", ""], ["Matsen", "Frederick A.", "IV"], ["Suchard", "Marc A.", ""]]}, {"id": "1903.04059", "submitter": "Ioannis Papastathopoulos", "authors": "Ioannis Papastathopoulos and Jonathan A. Tawn", "title": "Hidden tail chains and recurrence equations for dependence parameters\n  associated with extremes of higher-order Markov chains", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive some key extremal features for kth order Markov chains, which can\nbe used to understand how the process moves between an extreme state and the\nbody of the process. The chains are studied given that there is an exceedance\nof a threshold, as the threshold tends to the upper endpoint of the\ndistribution. Unlike previous studies with k>1 we consider processes where\nstandard limit theory describes each extreme event as a single observation\nwithout any information about the transition to and from the body of the\ndistribution. The extremal properties of the Markov chain at lags up to k are\ndetermined by the kernel of the chain, through a joint initialisation\ndistribution, with the subsequent values determined by the conditional\nindependence structure through a transition behaviour. We study the extremal\nproperties of each of these elements under weak assumptions for broad classes\nof extremal dependence structures. For chains with k>1, these transitions\ninvolve novel functions of the k previous states, in comparison to just the\nsingle value, when k=1. This leads to an increase in the complexity of\ndetermining the form of this class of functions, their properties and the\nmethod of their derivation in applications. We find that it is possible to find\nan affine normalization, dependent on the threshold excess, such that\nnon-degenerate limiting behaviour of the process is assured for all lags. These\nnormalization functions have an attractive structure that has parallels to the\nYule-Walker equations. Furthermore, the limiting process is always linear in\nthe innovations. We illustrate the results with the study of kth order\nstationary Markov chains based on widely studied families of copula dependence\nstructures.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 21:08:55 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:54:37 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 08:42:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Papastathopoulos", "Ioannis", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1903.04194", "submitter": "Yong Sheng Soh", "authors": "Yong Sheng Soh and Venkat Chandrasekaran", "title": "Fitting Tractable Convex Sets to Support Function Evaluations", "comments": "35 pages, 80 figures", "journal-ref": null, "doi": "10.1007/s00454-020-00258-0", "report-no": null, "categories": "math.ST cs.CG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric problem of estimating an unknown compact convex set from\nevaluations of its support function arises in a range of scientific and\nengineering applications. Traditional approaches typically rely on estimators\nthat minimize the error over all possible compact convex sets; in particular,\nthese methods do not allow for the incorporation of prior structural\ninformation about the underlying set and the resulting estimates become\nincreasingly more complicated to describe as the number of measurements\navailable grows. We address both of these shortcomings by describing a\nframework for estimating tractably specified convex sets from support function\nevaluations. Building on the literature in convex optimization, our approach is\nbased on estimators that minimize the error over structured families of convex\nsets that are specified as linear images of concisely described sets -- such as\nthe simplex or the spectraplex -- in a higher-dimensional space that is not\nmuch larger than the ambient space. Convex sets parametrized in this manner are\nsignificant from a computational perspective as one can optimize linear\nfunctionals over such sets efficiently; they serve a different purpose in the\ninferential context of the present paper, namely, that of incorporating\nregularization in the reconstruction while still offering considerable\nexpressive power. We provide a geometric characterization of the asymptotic\nbehavior of our estimators, and our analysis relies on the property that\ncertain sets which admit semialgebraic descriptions are Vapnik-Chervonenkis\n(VC) classes. Our numerical experiments highlight the utility of our framework\nover previous approaches in settings in which the measurements available are\nnoisy or small in number as well as those in which the underlying set to be\nreconstructed is non-polyhedral.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:01:00 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 08:22:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Soh", "Yong Sheng", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1903.04221", "submitter": "Marek Omelka", "authors": "Marek Omelka, \\v{S}\\'arka Hudecov\\'a, Natalie Neumeyer", "title": "Maximum pseudo-likelihood estimation based on estimated residuals in\n  copula semiparametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a situation when one is interested in the dependence\nstructure of a multidimensional response variable in the presence of a\nmultivariate covariate. It is assumed that the covariate affects only the\nmarginal distributions through regression models while the dependence\nstructure, which is described by a copula, is unaffected. A parametric\nestimation of the copula function is considered with focus on the maximum\npseudo-likelihood method. It is proved that under some appropriate regularity\nassumptions the estimator calculated from the residuals is asymptotically\nequivalent to the estimator based on the unobserved errors. In such case one\ncan ignore the fact that the response is first adjusted for the effect of the\ncovariate. A Monte Carlo simulation study explores (among others) situations\nwhere the regularity assumptions are not satisfied and the claimed result does\nnot hold. It shows that in such situations the maximum pseudo-likelihood\nestimator may behave poorly and the moment estimation of the copula parameter\nis of interest. Our results complement the results available for nonparametric\nestimation of the copula function.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:17:43 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Omelka", "Marek", ""], ["Hudecov\u00e1", "\u0160\u00e1rka", ""], ["Neumeyer", "Natalie", ""]]}, {"id": "1903.04306", "submitter": "Lea Longepierre", "authors": "L\\'ea Longepierre (LPSM UMR 8001), Catherine Matias (LPSM UMR 8001)", "title": "Consistency of the maximum likelihood and variational estimators in a\n  dynamic stochastic block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamic version of the stochastic block model, in which the\nnodes are partitioned into latent classes and the connection between two nodes\nis drawn from a Bernoulli distribution depending on the classes of these two\nnodes. The temporal evolution is modeled through a hidden Markov chain on the\nnodes memberships. We prove the consistency (as the number of nodes and time\nsteps increase) of the maximum likelihood and variational estimators of the\nmodel parameters, and obtain upper bounds on the rates of convergence of these\nestimators. We also explore the particular case where the number of time steps\nis fixed and connectivity parameters are allowed to vary.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 13:56:13 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 08:53:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Longepierre", "L\u00e9a", "", "LPSM UMR 8001"], ["Matias", "Catherine", "", "LPSM UMR 8001"]]}, {"id": "1903.04416", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Yun Yang", "title": "Diffusion $K$-means clustering on manifolds: provable exact recovery via\n  semidefinite relaxations", "comments": "accepted to Applied and Computational Harmonic Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2020.03.002", "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the {\\it diffusion $K$-means} clustering method on Riemannian\nsubmanifolds, which maximizes the within-cluster connectedness based on the\ndiffusion distance. The diffusion $K$-means constructs a random walk on the\nsimilarity graph with vertices as data points randomly sampled on the manifolds\nand edges as similarities given by a kernel that captures the local geometry of\nmanifolds. The diffusion $K$-means is a multi-scale clustering tool that is\nsuitable for data with non-linear and non-Euclidean geometric features in mixed\ndimensions. Given the number of clusters, we propose a polynomial-time convex\nrelaxation algorithm via the semidefinite programming (SDP) to solve the\ndiffusion $K$-means. In addition, we also propose a nuclear norm regularized\nSDP that is adaptive to the number of clusters. In both cases, we show that\nexact recovery of the SDPs for diffusion $K$-means can be achieved under\nsuitable between-cluster separability and within-cluster connectedness of the\nsubmanifolds, which together quantify the hardness of the manifold clustering\nproblem. We further propose the {\\it localized diffusion $K$-means} by using\nthe local adaptive bandwidth estimated from the nearest neighbors. We show that\nexact recovery of the localized diffusion $K$-means is fully adaptive to the\nlocal probability density and geometric structures of the underlying\nsubmanifolds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:29:27 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 01:01:56 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 03:27:37 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2020 16:49:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Xiaohui", ""], ["Yang", "Yun", ""]]}, {"id": "1903.04641", "submitter": "Asad Haris", "authors": "Asad Haris, Noah Simon, Ali Shojaie", "title": "Generalized Sparse Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for estimation and analysis of generalized\nadditive models in high dimensions. The framework defines a large class of\npenalized regression estimators, encompassing many existing methods. An\nefficient computational algorithm for this class is presented that easily\nscales to thousands of observations and features. We prove minimax optimal\nconvergence bounds for this class under a weak compatibility condition. In\naddition, we characterize the rate of convergence when this compatibility\ncondition is not met. Finally, we also show that the optimal penalty parameters\nfor structure and sparsity penalties in our framework are linked, allowing\ncross-validation to be conducted over only a single tuning parameter. We\ncomplement our theoretical results with empirical studies comparing some\nexisting methods within this framework.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 22:50:29 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Haris", "Asad", ""], ["Simon", "Noah", ""], ["Shojaie", "Ali", ""]]}, {"id": "1903.04663", "submitter": "Abram Kagan", "authors": "Abram M. Kagan and Gabor J. Sz\\'ekely", "title": "Calibrating dependence between random elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to quantify dependence between random elements X and Y via maximal\ncorrelation go back to Gebelein (1941) and R\\'{e}nyi (1959). After summarizing\nproperties (including some new) of the R\\'{e}nyi measure of dependence, a\ncalibrated scale of dependence is introduced. It is based on the ``complexity``\nof approximating functions of X by functions of Y.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 23:57:42 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Kagan", "Abram M.", ""], ["Sz\u00e9kely", "Gabor J.", ""]]}, {"id": "1903.04684", "submitter": "Rina Barber", "authors": "Rina Foygel Barber, Emmanuel J. Cand\\`es, Aaditya Ramdas, Ryan J.\n  Tibshirani", "title": "The limits of distribution-free conditional predictive inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distribution-free predictive inference, with the\ngoal of producing predictive coverage guarantees that hold conditionally rather\nthan marginally. Existing methods such as conformal prediction offer marginal\ncoverage guarantees, where predictive coverage holds on average over all\npossible test points, but this is not sufficient for many practical\napplications where we would like to know that our predictions are valid for a\ngiven individual, not merely on average over a population. On the other hand,\nexact conditional inference guarantees are known to be impossible without\nimposing assumptions on the underlying distribution. In this work we aim to\nexplore the space in between these two, and examine what types of relaxations\nof the conditional coverage property would alleviate some of the practical\nconcerns with marginal coverage guarantees while still being possible to\nachieve in a distribution-free setting.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 01:23:37 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 00:55:44 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Cand\u00e8s", "Emmanuel J.", ""], ["Ramdas", "Aaditya", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1903.04955", "submitter": "Tuan-Binh Nguyen", "authors": "Tuan-Binh Nguyen, J\\'er\\^ome-Alexis Chevalier and Bertrand Thirion", "title": "ECKO: Ensemble of Clustered Knockoffs for multivariate inference on fMRI\n  data", "comments": "Accepted to 26th International Conference on Information Processing\n  in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous improvement in medical imaging techniques allows the acquisition\nof higher-resolution images. When these are used in a predictive setting, a\ngreater number of explanatory variables are potentially related to the\ndependent variable (the response). Meanwhile, the number of acquisitions per\nexperiment remains limited. In such high dimension/small sample size setting,\nit is desirable to find the explanatory variables that are truly related to the\nresponse while controlling the rate of false discoveries. To achieve this goal,\nnovel multivariate inference procedures, such as knockoff inference, have been\nproposed recently. However, they require the feature covariance to be\nwell-defined, which is impossible in high-dimensional settings. In this paper,\nwe propose a new algorithm, called Ensemble of Clustered Knockoffs, that allows\nto select explanatory variables while controlling the false discovery rate\n(FDR), up to a prescribed spatial tolerance. The core idea is that\nknockoff-based inference can be applied on groups (clusters) of voxels, which\ndrastically reduces the problem's dimension; an ensembling step then removes\nthe dependence on a fixed clustering and stabilizes the results. We benchmark\nthis algorithm and other FDR-controlling methods on brain imaging datasets and\nobserve empirical gains in sensitivity, while the false discovery rate is\ncontrolled at the nominal level.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:37:32 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Nguyen", "Tuan-Binh", ""], ["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Thirion", "Bertrand", ""]]}, {"id": "1903.05046", "submitter": "Ilias Zadik", "authors": "Galen Reeves, Jiaming Xu, Ilias Zadik", "title": "The All-or-Nothing Phenomenon in Sparse Linear Regression", "comments": "40 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering a hidden binary $k$-sparse $p$-dimensional\nvector $\\beta$ from $n$ noisy linear observations $Y=X\\beta+W$ where $X_{ij}$\nare i.i.d. $\\mathcal{N}(0,1)$ and $W_i$ are i.i.d. $\\mathcal{N}(0,\\sigma^2)$. A\nclosely related hypothesis testing problem is to distinguish the pair $(X,Y)$\ngenerated from this structured model from a corresponding null model where\n$(X,Y)$ consist of purely independent Gaussian entries. In the low sparsity\n$k=o(p)$ and high signal to noise ratio $k/\\sigma^2=\\Omega\\left(1\\right)$\nregime, we establish an `All-or-Nothing' information-theoretic phase transition\nat a critical sample size $n^*=2 k\\log \\left(p/k\\right) /\\log\n\\left(1+k/\\sigma^2\\right)$, resolving a conjecture of \\cite{gamarnikzadik}.\nSpecifically, we show that if $\\liminf_{p\\to \\infty} n/n^*>1$, then the maximum\nlikelihood estimator almost perfectly recovers the hidden vector with high\nprobability and moreover the true hypothesis can be detected with a vanishing\nerror probability. Conversely, if $\\limsup_{p\\to \\infty} n/n^*<1$, then it\nbecomes information-theoretically impossible even to recover an arbitrarily\nsmall but fixed fraction of the hidden vector support, or to test hypotheses\nstrictly better than random guess.\n  Our proof of the impossibility result builds upon two key techniques, which\ncould be of independent interest. First, we use a conditional second moment\nmethod to upper bound the Kullback-Leibler (KL) divergence between the\nstructured and the null model. Second, inspired by the celebrated area theorem,\nwe establish a lower bound to the minimum mean squared estimation error of the\nhidden vector in terms of the KL divergence between the two models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:53:27 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Reeves", "Galen", ""], ["Xu", "Jiaming", ""], ["Zadik", "Ilias", ""]]}, {"id": "1903.05083", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov", "title": "Dimension reduction as an optimization problem over a set of generalized\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical dimension reduction problem can be loosely formulated as a problem\nof finding a $k$-dimensional affine subspace of ${\\mathbb R}^n$ onto which data\npoints ${\\mathbf x}_1,\\cdots, {\\mathbf x}_N$ can be projected without loss of\nvaluable information. We reformulate this problem in the language of tempered\ndistributions, i.e. as a problem of approximating an empirical probability\ndensity function $p_{\\rm{emp}}({\\mathbf x}) = \\frac{1}{N} \\sum_{i=1}^N \\delta^n\n(\\bold{x} - \\bold{x}_i)$, where $\\delta^n$ is an $n$-dimensional Dirac delta\nfunction, by another tempered distribution $q({\\mathbf x})$ whose density is\nsupported in some $k$-dimensional subspace. Thus, our problem is reduced to the\nminimization of a certain loss function $I(q)$ measuring the distance from $q$\nto $p_{\\rm{emp}}$ over a pertinent set of generalized functions, denoted\n$\\mathcal{G}_k$.\n  Another classical problem of data analysis is the sufficient dimension\nreduction problem. We show that it can be reduced to the following problem:\ngiven a function $f: {\\mathbb R}^n\\rightarrow {\\mathbb R}$ and a probability\ndensity function $p({\\mathbf x})$, find a function of the form $g({\\mathbf\nw}^T_1{\\mathbf x}, \\cdots, {\\mathbf w}^T_k{\\mathbf x})$ that minimizes the loss\n${\\mathbb E}_{{\\mathbf x}\\sim p} |f({\\mathbf x})-g({\\mathbf w}^T_1{\\mathbf x},\n\\cdots, {\\mathbf w}^T_k{\\mathbf x})|^2$.\n  We first show that search spaces of the latter two problems are in one-to-one\ncorrespondence which is defined by the Fourier transform. We introduce a\nnonnegative penalty function $R(f)$ and a set of ordinary functions\n$\\Omega_\\epsilon = \\{f| R(f)\\leq \\epsilon\\}$ in such a way that\n$\\Omega_\\epsilon$ `approximates' the space $\\mathcal{G}_k$ when $\\epsilon\n\\rightarrow 0$. Then we present an algorithm for minimization of $I(f)+\\lambda\nR(f)$, based on the idea of two-step iterative computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:25:18 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Takhanov", "Rustem", ""]]}, {"id": "1903.05220", "submitter": "Harsha Honnappa", "authors": "Prateek Jaiswal and Harsha Honnappa and Vinayak A. Rao", "title": "Risk-Sensitive Variational Bayes: Formulations and Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data-driven decision-making problems in a parametrized Bayesian\nframework. We adopt a risk-sensitive approach to modeling the interplay between\nstatistical estimation of parameters and optimization, by computing a risk\nmeasure over a loss/disutility function with respect to the posterior\ndistribution over the parameters. While this forms the standard Bayesian\ndecision-theoretic approach, we focus on problems where calculating the\nposterior distribution is intractable, a typical situation in modern\napplications with %high-dimensional parameter space large datasets,\nheterogeneity due to observed covariates and latent group structure. The key\nmethodological innovation we introduce in this paper is to leverage a dual\nrepresentation of the risk measure to introduce an optimization-based framework\nfor approximately computing the posterior risk-sensitive objective, as opposed\nto using standard sampling based methods such as Markov Chain Monte Carlo. Our\nanalytical contributions include rigorously proving finite sample bounds on the\n`optimality gap' of optimizers obtained using the computational methods in this\npaper, from the `true' optimizers of a given decision-making problem. We\nillustrate our results by comparing the theoretical bounds with simulations of\na newsvendor problem on two methods extracted from our computational framework.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 21:20:11 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 12:07:38 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 12:59:24 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Jaiswal", "Prateek", ""], ["Honnappa", "Harsha", ""], ["Rao", "Vinayak A.", ""]]}, {"id": "1903.05315", "submitter": "Gil Kur", "authors": "Gil Kur, Yuval Dagan, Alexander Rakhlin", "title": "Optimality of Maximum Likelihood for Log-Concave Density Estimation and\n  Bounded Convex Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two problems: (1) estimation of a $d$-dimensional\nlog-concave distribution and (2) bounded multivariate convex regression with\nrandom design with an underlying log-concave density or a compactly supported\ndistribution with a continuous density.\n  First, we show that for all $d \\ge 4$ the maximum likelihood estimators of\nboth problems achieve an optimal risk of $\\Theta_d(n^{-2/(d+1)})$ (up to a\nlogarithmic factor) in terms of squared Hellinger distance and $L_2$ squared\ndistance, respectively. Previously, the optimality of both these estimators was\nknown only for $d\\le 3$. We also prove that the $\\epsilon$-entropy numbers of\nthe two aforementioned families are equal up to logarithmic factors. We\ncomplement these results by proving a sharp bound $\\Theta_d(n^{-2/(d+4)})$ on\nthe minimax rate (up to logarithmic factors) with respect to the total\nvariation distance.\n  Finally, we prove that estimating a log-concave density - even a uniform\ndistribution on a convex set - up to a fixed accuracy requires the number of\nsamples \\emph{at least} exponential in the dimension. We do that by improving\nthe dimensional constant in the best known lower bound for the minimax rate\nfrom $2^{-d}\\cdot n^{-2/(d+1)}$ to $c\\cdot n^{-2/(d+1)}$ (when $d\\geq 2$).\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 04:58:36 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 06:19:37 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 13:16:32 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 13:32:11 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Kur", "Gil", ""], ["Dagan", "Yuval", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1903.05589", "submitter": "Nicolas Marie", "authors": "Pierre Alquier and Nicolas Marie", "title": "Matrix factorization for multivariate time series analysis", "comments": "16 pages", "journal-ref": "Electronic Journal of Statistics 13, 2, 4346-4366, 2019", "doi": "10.1214/19-EJS1630", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a powerful data analysis tool. It has been used in\nmultivariate time series analysis, leading to the decomposition of the series\nin a small set of latent factors. However, little is known on the statistical\nperformances of matrix factorization for time series. In this paper, we extend\nthe results known for matrix estimation in the i.i.d setting to time series.\nMoreover, we prove that when the series exhibit some additional structure like\nperiodicity or smoothness, it is possible to improve on the classical rates of\nconvergence.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 16:40:23 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 12:33:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Alquier", "Pierre", ""], ["Marie", "Nicolas", ""]]}, {"id": "1903.05701", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Rejoinder: \"Gene Hunting with Hidden Markov Model Knockoffs\"", "comments": "12 pages, 4 figures", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 35-45", "doi": "10.1093/biomet/asy075", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deepen and enlarge the reflection on the possible advantages\nof a knockoff approach to genome wide association studies (Sesia et al., 2018),\nstarting from the discussions in Bottolo & Richardson (2019); Jewell & Witten\n(2019); Rosenblatt et al. (2019) and Marchini (2019). The discussants bring up\na number of important points, either related to the knockoffs methodology in\ngeneral, or to its specific application to genetic studies. In the following we\noffer some clarifications, mention relevant recent developments and highlight\nsome of the still open problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:58:31 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1903.06026", "submitter": "Gr\u00e9goire Sergeant-Perthuis", "authors": "Gr\\'egoire Sergeant-Perthuis", "title": "Bayesian/Graphoid intersection property for factorisation spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We remark that Pearl's Graphoid intersection property, also called\nintersection property in Bayesian networks, is a particular case of a general\nintersection property, in the sense of intersection of coverings, for\nfactorisation spaces, also coined as factorisation models, factor graphs or by\nLauritzen in his reference book 'Graphical Models' as hierarchical model\nsubspaces. A particular case of this intersection property appears in\nLauritzen's book as a consequence of the decomposition into interaction\nsubspaces; the novel proof that we give of this result allows us to extend it\nin the most general setting. It also allows us to give a direct and new proof\nof the Hammersley-Clifford theorem transposing and reducing it to a\ncorresponding statement for graphs, justifying formally the geometric intuition\nof independency, and extending it to non finite graphs. This intersection\nproperty is the starting point for a generalization of the decomposition into\ninteraction subspaces to collections of vector spaces.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:20:11 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 18:42:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sergeant-Perthuis", "Gr\u00e9goire", ""]]}, {"id": "1903.06092", "submitter": "Richard Samworth", "authors": "Min Xu and Richard J. Samworth", "title": "High-dimensional nonparametric density estimation via symmetry and shape\n  constraints", "comments": "93 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of high-dimensional nonparametric density estimation by\ntaking the class of log-concave densities on $\\mathbb{R}^p$ and incorporating\nwithin it symmetry assumptions, which facilitate scalable estimation algorithms\nand can mitigate the curse of dimensionality. Our main symmetry assumption is\nthat the super-level sets of the density are $K$-homothetic (i.e. scalar\nmultiples of a convex body $K \\subseteq \\mathbb{R}^p$). When $K$ is known, we\nprove that the $K$-homothetic log-concave maximum likelihood estimator based on\n$n$ independent observations from such a density has a worst-case risk bound\nwith respect to, e.g., squared Hellinger loss, of $O(n^{-4/5})$, independent of\n$p$. Moreover, we show that the estimator is adaptive in the sense that if the\ndata generating density admits a special form, then a nearly parametric rate\nmay be attained. We also provide worst-case and adaptive risk bounds in cases\nwhere $K$ is only known up to a positive definite transformation, and where it\nis completely unknown and must be estimated nonparametrically. Our estimation\nalgorithms are fast even when $n$ and $p$ are on the order of hundreds of\nthousands, and we illustrate the strong finite-sample performance of our\nmethods on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 15:54:28 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Xu", "Min", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1903.06110", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Orlando Marigliano, Bernd Sturmfels", "title": "Discrete Statistical Models with Rational Maximum Likelihood Estimator", "comments": "Accepted to Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discrete statistical model is a subset of a probability simplex. Its\nmaximum likelihood estimator (MLE) is a retraction from that simplex onto the\nmodel. We characterize all models for which this retraction is a rational\nfunction. This is a contribution via real algebraic geometry which rests on\nresults due to Huh and Kapranov on Horn uniformization. We present an algorithm\nfor constructing models with rational MLE, and we demonstrate it on a range of\ninstances. Our focus lies on models familiar to statisticians, like Bayesian\nnetworks, decomposable graphical models, and staged trees.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 16:43:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 14:00:15 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Duarte", "Eliana", ""], ["Marigliano", "Orlando", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1903.06165", "submitter": "Francisco J. Beron-Vera", "authors": "P. Miron, F.J. Beron-Vera, M.J. Olascoaga and P. Koltai", "title": "Markov-chain-inspired search for MH370", "comments": "Submitted to Chaos", "journal-ref": "Chaos 29, 041105 (2019)", "doi": "10.1063/1.5092132", "report-no": null, "categories": "stat.AP math.ST physics.ao-ph physics.pop-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov-chain models are constructed for the probabilistic description of the\ndrift of marine debris from Malaysian Airlines flight MH370. En route from\nKuala Lumpur to Beijing, the MH370 mysteriously disappeared in the southeastern\nIndian Ocean on 8 March 2014, somewhere along the arc of the 7th ping ring\naround the Inmarsat-3F1 satellite position when the airplane lost contact. The\nmodels are obtained by discretizing the motion of undrogued satellite-tracked\nsurface drifting buoys from the global historical data bank. A spectral\nanalysis, Bayesian estimation, and the computation of most probable paths\nbetween the Inmarsat arc and confirmed airplane debris beaching sites are shown\nto constrain the crash site, near 25$^{\\circ}$S on the Inmarsat arc.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:36:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Miron", "P.", ""], ["Beron-Vera", "F. J.", ""], ["Olascoaga", "M. J.", ""], ["Koltai", "P.", ""]]}, {"id": "1903.06295", "submitter": "Michael Law", "authors": "Michael Law, Ya'acov Ritov", "title": "Inference Without Compatibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider hypotheses testing problems for three parameters in\nhigh-dimensional linear models with minimal sparsity assumptions of their type\nbut without any compatibility conditions. Under this framework, we construct\nthe first $\\sqrt{n}$-consistent estimators for low-dimensional coefficients,\nthe signal strength, and the noise level. We support our results using\nnumerical simulations and provide comparisons with other estimators.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 23:12:38 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 00:40:13 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Law", "Michael", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1903.06446", "submitter": "Irina Blazhievska", "authors": "Irina Blazhievska, Vladimir Zaiats", "title": "On cross-correlogram IRF's estimators of two-output systems in spaces of\n  continuous functions", "comments": "25 pages, 11 Postscript figures", "journal-ref": "Communications in Statistics - Theory and Methods, 2020", "doi": "10.1080/03610926.2020.1738490", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, single input--double output linear time-invariant systems are\nstudied. Both components of system's impulse response function (IRF) are\nsupposed to be real-valued and square-integrable. One component is unknown\nwhile the second one is controlled. The problem is to estimate the unknown\ncomponent after observations of the other component. We apply cross-correlating\nof the outputs given that the input is a standard Wiener process. Weak\nasymptotic normality of appropriately centred estimators in spaces of\ncontinuous functions is proved. This enables us to construct confidence\nintervals in these spaces. Our results employ techniques related to Gaussian\nprocesses and bilinear forms of Gaussian processes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 10:35:02 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Blazhievska", "Irina", ""], ["Zaiats", "Vladimir", ""]]}, {"id": "1903.06447", "submitter": "Dominique Dehay", "authors": "Dominique Dehay (IRMAR), Khalil El Waled, Vincent Monsan", "title": "Parametric estimation for a signal-plus-noise model from discrete time\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the parametric inference for integrated signals\nembedded in an additive Gaussian noise and observed at deterministic discrete\ninstants which are not necessarily equidistant. The unknown parameter is\nmultidimensional and compounded of a signal-of-interest parameter and a\nvariance parameter of the noise. We state the consistency and the minimax\nefficiency of the maximum likelihood estimator and of the Bayesian estimator\nwhen the time of observation tends to $\\infty$ and the delays between two\nconsecutive observations tend to 0 or are only bounded. The class of signals in\nconsideration contains among others, almost periodic signals and also\nnon-continuous periodic signals. However the problem of frequency estimation is\nnot considered here.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 10:35:10 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dehay", "Dominique", "", "IRMAR"], ["Waled", "Khalil El", ""], ["Monsan", "Vincent", ""]]}, {"id": "1903.06576", "submitter": "Arnak Dalalyan S.", "authors": "Victor-Emmanuel Brunel, Arnak S. Dalalyan, Nicolas Schreuder", "title": "A nonasymptotic law of iterated logarithm for general M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M-estimators are ubiquitous in machine learning and statistical learning\ntheory. They are used both for defining prediction strategies and for\nevaluating their precision. In this paper, we propose the first non-asymptotic\n\"any-time\" deviation bounds for general M-estimators, where \"any-time\" means\nthat the bound holds with a prescribed probability for every sample size. These\nbounds are nonasymptotic versions of the law of iterated logarithm. They are\nestablished under general assumptions such as Lipschitz continuity of the loss\nfunction and (local) curvature of the population risk. These conditions are\nsatisfied for most examples used in machine learning, including those ensuring\nrobustness to outliers and to heavy tailed distributions. As an example of\napplication, we consider the problem of best arm identification in a parametric\nstochastic multi-arm bandit setting. We show that the established bound can be\nconverted into a new algorithm, with provably optimal theoretical guarantees.\nNumerical experiments illustrating the validity of the algorithm are reported.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:37:31 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 12:14:03 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""], ["Dalalyan", "Arnak S.", ""], ["Schreuder", "Nicolas", ""]]}, {"id": "1903.06585", "submitter": "Katerina Papagiannouli", "authors": "Katerina Papagiannouli", "title": "Minimax rates for the covariance estimation of multi-dimensional L\\'evy\n  processes with high-frequency data", "comments": "Important changes have been made in this version. This version\n  supersedes version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies nonparametric methods to estimate the co-integrated\nvolatility for multi-dimensional L\\'evy processes with high frequency data. We\nconstruct a spectral estimator for the co-integrated volatility and prove\nminimax rates for an appropriate bounded nonparametric class of\nsemimartingales. Given $ n $ observations of increments over intervals of\nlength $1/n$, the rates of convergence are $1 / \\sqrt{n} $ if $ r \\leq 1 $ and\n$ (n\\log n)^{(r-2)/2} $ if $ r>1 $, which are optimal in a minimax sense. We\nbound the co-jump index activity from below with the harmonic mean. Finally, we\nassess the efficiency of our estimator by comparing it with estimators in the\nexisting literature.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 15:01:36 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 22:06:10 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Papagiannouli", "Katerina", ""]]}, {"id": "1903.06931", "submitter": "Shovan Chowdhury", "authors": "Shovan Chowdhury, Amarjit Kundu, and Surja Kanta Mishra", "title": "Ordering properties of the smallest order statistic from Weibull G\n  random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare the minimums of two heterogeneous samples each\nfollowing Weibull-G distribution under three scenarios. In the Fifirst\nscenario, the units of the samples are assumed to be independently distributed\nand the comparisons are carried out through vector majorization. The minimums\nof the samples are compared in the second scenario when the independent units\nof the samples also experience random shocks. The last scenario describes the\ncomparison when the units have a dependent structure sharing Archimedean\ncopula.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 14:39:49 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chowdhury", "Shovan", ""], ["Kundu", "Amarjit", ""], ["Mishra", "Surja Kanta", ""]]}, {"id": "1903.06984", "submitter": "Randolf Altmeyer", "authors": "Randolf Altmeyer, Markus Rei{\\ss}", "title": "Nonparametric estimation for linear SPDEs from local measurements", "comments": "corrected version", "journal-ref": "Ann. Appl. Probab. 31 (1) 1 - 38, 2021", "doi": "10.1214/20-AAP1581", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient function of the leading differential operator is estimated\nfrom observations of a linear stochastic partial differential equation (SPDE).\nThe estimation is based on continuous time observations which are localised in\nspace. For the asymptotic regime with fixed time horizon and with the spatial\nresolution of the observations tending to zero, we provide rate-optimal\nestimators and establish scaling limits of the deterministic PDE and of the\nSPDE on growing domains. The estimators are robust to lower order perturbations\nof the underlying differential operator and achieve the parametric rate even in\nthe nonparametric setup with a spatially varying coefficient. A numerical\nexample illustrates the main results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 20:51:21 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 10:48:22 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 10:47:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Altmeyer", "Randolf", ""], ["Rei\u00df", "Markus", ""]]}, {"id": "1903.06991", "submitter": "Glenn Shafer", "authors": "Glenn Shafer", "title": "The Language of Betting as a Strategy for Statistical and Scientific\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The established language for statistical testing --- significance levels,\npower, and p-values --- is overly complicated and deceptively conclusive. Even\nteachers of statistics and scientists who use statistics misinterpret the\nresults of statistical tests, tending to misstate their meaning and exaggerate\ntheir certainty. We can communicate the meaning and limitations of statistical\nevidence more clearly using the language of betting.\n  This paper calls attention to a simple betting interpretation of likelihood\nratios. This interpretation leads to methods that lend themselves to\nmeta-analysis and accounting for multiple testing. It is closely related to the\ninterpretation of probability as frequency, but it does not encourage the\nfallacy that probabilistic models imply the existence of unseen alternative\nworlds.\n  For more on the betting interpretation of probability, see\n\\cite{Shafer/Vovk:2019} and the other working papers at\nwww.probabilityandfinance.com.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 21:44:29 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 16:48:38 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Shafer", "Glenn", ""]]}, {"id": "1903.07028", "submitter": "Nicy Sebastian", "authors": "Nicy Sebastian, Rasin R. S. and Silviya P. O", "title": "Topp-Leone generated q-exponential distribution and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topp-Leone distribution is a continuous model distribution used for modelling\nlifetime phenomena. The main purpose of this paper is to introduce a new\nframework for generating lifetime distributions, called the Topp-Leone\ngenerated q-exponential family of distributions. Parameter estimation using\nmaximum likelihood method and simulation results to assess effectiveness of the\ndistribution are discussed. Different informative and non-informative priors\nare used to estimate the shape parameter of q extended Topp-Leone generated\nexponential distribution under normal approximation technique. We prove\nempirically the importance and flexibility of the new model in model building\nby using a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 05:10:16 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Sebastian", "Nicy", ""], ["S.", "Rasin R.", ""], ["O", "Silviya P.", ""]]}, {"id": "1903.07208", "submitter": "Xi Chen", "authors": "Xi Chen and Wen-Xin Zhou", "title": "Robust Inference via Multiplier Bootstrap", "comments": "81 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the theoretical underpinnings of two fundamental\nstatistical inference problems, the construction of confidence sets and\nlarge-scale simultaneous hypothesis testing, in the presence of heavy-tailed\ndata. With heavy-tailed observation noise, finite sample properties of the\nleast squares-based methods, typified by the sample mean, are suboptimal both\ntheoretically and empirically. In this paper, we demonstrate that the adaptive\nHuber regression, integrated with the multiplier bootstrap procedure, provides\na useful robust alternative to the method of least squares. Our theoretical and\nempirical results reveal the effectiveness of the proposed method, and\nhighlight the importance of having inference methods that are robust to heavy\ntailedness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 00:11:30 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Xi", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1903.07250", "submitter": "Nicy Sebastian", "authors": "Seema S Nair and Nicy Sebastian", "title": "On Generalized q-logistic Distribution and its Characterizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several generalizations of the logistic distribution, and certain related\nmodels, are proposed by many authors for modeling various random phenomena such\nas those encountered in data engineering, pattern recognition, and reliability\nassessment studies. A generalized q-logistic distribution is discussed here in\nthe light of pathway model, in which the new parameter q allows increased\nflexibility for modeling purpose. Also, we discuss different properties of the\ntwo generalizations of the q-logistic distributions, which can be used to model\nthe data exhibiting a unimodal density having some skewness present. The first\ngeneralization is carried out using the basic idea of Azzalini and we call it\nas the skew q-logistic distribution. It is observed that the density function\nof the skew q-logistic distribution is always unimodal and log-concave in\nnature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 04:29:44 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Nair", "Seema S", ""], ["Sebastian", "Nicy", ""]]}, {"id": "1903.07347", "submitter": "Adrien Saumard", "authors": "Adrien Saumard", "title": "Bi-log-concavity: some properties and some remarks towards a\n  multi-dimensional extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bi-log-concavity of probability measures is a univariate extension of the\nnotion of log-concavity that has been recently proposed in a statistical\nliterature. Among other things, it has the nice property from a modelisation\nperspective to admit some multimodal distributions, while preserving some nice\nfeatures of log-concave measures. We compute the isoperimetric constant for a\nbi-log-concave measure, extending a property available for log-concave\nmeasures. This implies that bi-log-concave measures have exponentially\ndecreasing tails. Then we show that the convolution of a bi-log-concave measure\nwith a log-concave one is bi-log-concave. Consequently, infinitely\ndifferentiable, positive densities are dense in the set of bi-log-concave\ndensities for $L_p-$norms, $p \\in [1;+\\infty]$. We also derive a necessary and\nsufficient condition for the convolution of two bi-log-concave measures to be\nbi-log-concave. We conclude this note by discussing ways of defining a\nmulti-dimensional extension of the notion of bi-log-concavity. We propose an\napproach based on a variant of the isoperimetric problem, restricted to\nhalf-spaces.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 10:23:28 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Saumard", "Adrien", ""]]}, {"id": "1903.07349", "submitter": "Arkadi Nemirovski", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "Signal recovery by Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach to signal recovery in Generalized Linear Models (GLM)\nin which the signal estimation problem is reduced to the problem of solving a\nstochastic monotone variational inequality (VI). The solution to the stochastic\nVI can be found in a computationally efficient way, and in the case when the VI\nis strongly monotone we derive finite-time upper bounds on the expected\n$\\|\\cdot\\|_2^2$ error converging to 0 at the rate $O(1/K)$ as the number $K$ of\nobservations grows. Our structural assumptions are essentially weaker than\nthose necessary to ensure convexity of the optimization problem resulting from\nMaximum Likelihood estimation. In hindsight, the approach we promote can be\ntraced back directly to the ideas behind the Rosenblatt's perceptron algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 10:31:25 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1903.07782", "submitter": "Yingrui Yang", "authors": "Yingrui Yang, Molin Wang", "title": "Semiparametric Methods for Exposure Misclassification in Propensity\n  Score-Based Time-to-Event Data Analysis", "comments": "Withdrawn due to grant related requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiology, identifying the effect of exposure variables in relation to\na time-to-event outcome is a classical research area of practical importance.\nIncorporating propensity score in the Cox regression model, as a measure to\ncontrol for confounding, has certain advantages when outcome is rare. However,\nin situations involving exposure measured with moderate to substantial error,\nidentifying the exposure effect using propensity score in Cox models remains a\nchallenging yet unresolved problem. In this paper, we propose an estimating\nequation method to correct for the exposure misclassification-caused bias in\nthe estimation of exposure-outcome associations. We also discuss the asymptotic\nproperties and derive the asymptotic variances of the proposed estimators. We\nconduct a simulation study to evaluate the performance of the proposed\nestimators in various settings. As an illustration, we apply our method to\ncorrect for the misclassification-caused bias in estimating the association of\nPM2.5 level with lung cancer mortality using a nationwide prospective cohort,\nthe Nurses' Health Study (NHS). The proposed methodology can be applied using\nour user-friendly R function published online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 00:57:16 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 07:50:43 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Yang", "Yingrui", ""], ["Wang", "Molin", ""]]}, {"id": "1903.07850", "submitter": "Gopal Basak", "authors": "Gopal K Basak, Samarjit Das, Arijit De and Atanu Biswas", "title": "Relative Efficiency of Higher Normed Estimators Over the Least Squares\n  Estimator", "comments": "32 pages 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the performance of the estimator that minimizes\n$L_{2k}- $ order loss function (for $ k \\ge \\; 2 )$ against the estimators\nwhich minimizes the $L_2-$ order loss function (or the least squares\nestimator). Commonly occurring examples illustrate the differences in\nefficiency between $L_{2k}$ and $L_2 -$ based estimators. We derive an\nempirically testable condition under which the $L_{2k}$ estimator is more\nefficient than the least squares estimator. We construct a simple decision rule\nto choose between $L_{2k}$ and $L_2$ estimator. Special emphasis is provided to\nstudy $L_{4}$ estimator. A detailed simulation study verifies the effectiveness\nof this decision rule. Also, the superiority of the $L_{2k}$ estimator is\ndemonstrated in a real life data set.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 06:06:28 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Basak", "Gopal K", ""], ["Das", "Samarjit", ""], ["De", "Arijit", ""], ["Biswas", "Atanu", ""]]}, {"id": "1903.07870", "submitter": "Samuel Hopkins", "authors": "Samuel B. Hopkins and Jerry Li", "title": "How Hard Is Robust Mean Estimation?", "comments": "Conference on Learning Theory (COLT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust mean estimation is the problem of estimating the mean $\\mu \\in\n\\mathbb{R}^d$ of a $d$-dimensional distribution $D$ from a list of independent\nsamples, an $\\epsilon$-fraction of which have been arbitrarily corrupted by a\nmalicious adversary. Recent algorithmic progress has resulted in the first\npolynomial-time algorithms which achieve \\emph{dimension-independent} rates of\nerror: for instance, if $D$ has covariance $I$, in polynomial-time one may find\n$\\hat{\\mu}$ with $\\|\\mu - \\hat{\\mu}\\| \\leq O(\\sqrt{\\epsilon})$. However, error\nrates achieved by current polynomial-time algorithms, while\ndimension-independent, are sub-optimal in many natural settings, such as when\n$D$ is sub-Gaussian, or has bounded $4$-th moments.\n  In this work we give worst-case complexity-theoretic evidence that improving\non the error rates of current polynomial-time algorithms for robust mean\nestimation may be computationally intractable in natural settings. We show that\nseveral natural approaches to improving error rates of current polynomial-time\nrobust mean estimation algorithms would imply efficient algorithms for the\nsmall-set expansion problem, refuting Raghavendra and Steurer's small-set\nexpansion hypothesis (so long as $P \\neq NP$). We also give the first direct\nreduction to the robust mean estimation problem, starting from a plausible but\nnonstandard variant of the small-set expansion problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 07:48:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 23:17:58 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""]]}, {"id": "1903.08033", "submitter": "Radomyra Shevchenko", "authors": "Radomyra Shevchenko, Jeannette H.C. Woerner", "title": "Inference for fractional Ornstein-Uhlenbeck type processes with periodic\n  mean in the non-ergodic case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we consider the problem of estimating parameters entering the\ndrift of a fractional Ornstein-Uhlenbeck type process in the non-ergodic case,\nwhen the underlying stochastic integral is of Young type. We consider the\nsampling scheme that the process is observed continuously on $[0,T]$ and\n$T\\to\\infty$. For known Hurst parameter $H\\in(0.5, 1)$, i.e. the long range\ndependent case, we construct a least-squares type estimator and establish\nstrong consistency. Furthermore, we prove a second order limit theorem which\nprovides asymptotic normality for the parameters of the periodic function with\na rate depending on $H$ and a non-central Cauchy limit result for the mean\nreverting parameter with exponential rate. For the special case that the\nperiodicity parameter is the weight of a periodic function, which integrates to\nzero over the period, we can even improve the rate to $\\sqrt{T}$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:53:29 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Shevchenko", "Radomyra", ""], ["Woerner", "Jeannette H. C.", ""]]}, {"id": "1903.08072", "submitter": "Samy Blusseau", "authors": "Yunxiang Zhang (CMM, LTCI), Samy Blusseau (CMM), Santiago\n  Velasco-Forero (CMM), Isabelle Bloch (LTCI), Jesus Angulo (CMM)", "title": "Max-plus Operators Applied to Filter Selection and Model Pruning in\n  Neural Networks", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG cs.NE stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent advances in morphological neural networks, we propose to\nstudy in more depth how Max-plus operators can be exploited to define\nmorphological units and how they behave when incorporated in layers of\nconventional neural networks. Besides showing that they can be easily\nimplemented with modern machine learning frameworks , we confirm and extend the\nobservation that a Max-plus layer can be used to select important filters and\nreduce redundancy in its previous layer, without incurring performance loss.\nExperimental results demonstrate that the filter selection strategy enabled by\na Max-plus is highly efficient and robust, through which we successfully\nperformed model pruning on different neural network architectures. We also\npoint out that there is a close connection between Maxout networks and our\npruned Max-plus networks by comparing their respective characteristics. The\ncode for reproducing our experiments is available online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 15:58:43 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 12:51:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Yunxiang", "", "CMM, LTCI"], ["Blusseau", "Samy", "", "CMM"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"], ["Angulo", "Jesus", "", "CMM"]]}, {"id": "1903.08281", "submitter": "Mengxi Yi", "authors": "David E. Tyler and Mengxi Yi", "title": "Shrinking the Sample Covariance Matrix using Convex Penalties on the\n  Matrix-Log Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $q$-dimensional data, penalized versions of the sample covariance matrix\nare important when the sample size is small or modest relative to $q$. Since\nthe negative log-likelihood under multivariate normal sampling is convex in\n$\\Sigma^{-1}$, the inverse of its covariance matrix, it is common to add to it\na penalty which is also convex in $\\Sigma^{-1}$. More recently, Deng-Tsui\n(2013) and Yu et al.(2017) have proposed penalties which are functions of the\neigenvalues of $\\Sigma$, and are convex in $\\log \\Sigma$, but not in\n$\\Sigma^{-1}$. The resulting penalized optimization problem is not convex in\neither $\\log \\Sigma$ or $\\Sigma^{-1}$. In this paper, we note that this\noptimization problem is geodesically convex in $\\Sigma$, which allows us to\nestablish the existence and uniqueness of the corresponding penalized\ncovariance matrices. More generally, we show the equivalence of convexity in\n$\\log \\Sigma$ and geodesic convexity for penalties on $\\Sigma$ which are\nstrictly functions of their eigenvalues. In addition, when using such\npenalties, we show that the resulting optimization problem reduces to to a\n$q$-dimensional convex optimization problem on the eigenvalues of $\\Sigma$,\nwhich can then be readily solved via Newton-Raphson. Finally, we argue that it\nis better to apply these penalties to the shape matrix $\\Sigma/(\\det\n\\Sigma)^{1/q}$ rather than to $\\Sigma$ itself. A simulation study and an\nexample illustrate the advantages of applying the penalty to the shape matrix.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 22:55:00 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Tyler", "David E.", ""], ["Yi", "Mengxi", ""]]}, {"id": "1903.08507", "submitter": "Fran\\c{c}ois Portier", "authors": "Bernard Delyon and Fran\\c{c}ois Portier", "title": "Safe and adaptive importance sampling: a mixture approach", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates adaptive importance sampling algorithms for which the\npolicy, the sequence of distributions used to generate the particles, is a\nmixture distribution between a flexible kernel density estimate (based on the\nprevious particles), and a \"safe\" heavy-tailed density. When the share of\nsamples generated according to the safe density goes to zero but not too\nquickly, two results are established: (i) uniform convergence rates are derived\nfor the policy toward the target density; (ii) a central limit theorem is\nobtained for the resulting integral estimates. The fact that the asymptotic\nvariance is the same as the variance of an \"oracle\" procedure with\nvariance-optimal policy, illustrates the benefits of the approach. In addition,\na subsampling step (among the particles) can be conducted before constructing\nthe kernel estimate in order to decrease the computational effort without\naltering the performance of the method. The practical behavior of the\nalgorithms is illustrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 13:45:23 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 08:41:19 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 16:26:08 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 12:53:59 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Delyon", "Bernard", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1903.08560", "submitter": "Andrea Montanari", "authors": "Trevor Hastie and Andrea Montanari and Saharon Rosset and Ryan J.\n  Tibshirani", "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation", "comments": "68 pages; 16 figures. This revision contains non-asymptotic version\n  of earlier results, and results for general coefficients", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolators -- estimators that achieve zero training error -- have\nattracted growing attention in machine learning, mainly because state-of-the\nart neural networks appear to be models of this type. In this paper, we study\nminimum $\\ell_2$ norm (``ridgeless'') interpolation in high-dimensional least\nsquares regression. We consider two different models for the feature\ndistribution: a linear model, where the feature vectors $x_i \\in {\\mathbb R}^p$\nare obtained by applying a linear transform to a vector of i.i.d.\\ entries,\n$x_i = \\Sigma^{1/2} z_i$ (with $z_i \\in {\\mathbb R}^p$); and a nonlinear model,\nwhere the feature vectors are obtained by passing the input through a random\none-layer neural network, $x_i = \\varphi(W z_i)$ (with $z_i \\in {\\mathbb R}^d$,\n$W \\in {\\mathbb R}^{p \\times d}$ a matrix of i.i.d.\\ entries, and $\\varphi$ an\nactivation function acting componentwise on $W z_i$). We recover -- in a\nprecise quantitative way -- several phenomena that have been observed in\nlarge-scale neural networks and kernel machines, including the \"double descent\"\nbehavior of the prediction risk, and the potential benefits of\noverparametrization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 16:53:11 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 16:34:19 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 00:37:59 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 16:47:40 GMT"}, {"version": "v5", "created": "Mon, 7 Dec 2020 17:59:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hastie", "Trevor", ""], ["Montanari", "Andrea", ""], ["Rosset", "Saharon", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1903.08573", "submitter": "Carlos Matr\\'an", "authors": "Eustasio del Barrio, Hristo Inouzhe and Carlos Matr\\'an", "title": "Box-constrained monotone $L_\\infty$-approximations to Lipschitz\n  regularizations, with applications to robust testing", "comments": "16 pages, 29 references", "journal-ref": "Journal of Optimization Theory and Applications 2020", "doi": "10.1007/s10957-020-01743-5", "report-no": null, "categories": "math.OC math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests of fit to exact models in statistical analysis often lead to rejections\neven when the model is a useful approximate description of the random generator\nof the data. Among possible relaxations of a fixed model, the one defined by\ncontamination neighbourhoods, namely,\n$\\mathcal{V}_\\alpha(P_0)=\\{(1-\\alpha)P_0+\\alpha Q: Q \\in \\mathcal{P}\\}$, where\n$\\mathcal{P}$ is the set of all probabilities in the sample space, has received\nmuch attention, from its central role in Robust Statistics. For probabilities\non the real line, consistent tests of fit to $\\mathcal{V}_\\alpha(P_0)$ can be\nbased on $d_K(P_0,R_\\alpha(P))$, the minimal Kolmogorov distance between $P_0$\nand the set of trimmings of $P$, $R_\\alpha(P)=\\big\\{\\tilde\nP\\in\\mathcal{P}:\\tilde P\\ll P,\\,{\\textstyle \\frac{d\\tilde\nP}{dP}\\leq\\frac{1}{1-\\alpha}}\\, P\\text{-a.s.}\\big\\}$. We show that this\nfunctional admits equivalent formulations in terms of, either best\napproximation in uniform norm by $L$-Lipschitz functions satisfying a box\nconstraint, or as the best monotone approximation in uniform norm to the\n$L$-Lipschitz regularization, which is seen to be expressable in terms of the\naverage of the Pasch-Hausdorff envelopes. This representation for the solution\nof the variational problem allows to obtain results showing stability of the\nfunctional $d_K(P_0,R_\\alpha(P))$, as well as directional differentiability,\nproviding the basis for a Central Limit Theorem for that functional.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 15:58:30 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 18:40:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Inouzhe", "Hristo", ""], ["Matr\u00e1n", "Carlos", ""]]}, {"id": "1903.08611", "submitter": "Carlos Am\\'endola", "authors": "Carlos Am\\'endola and Viet Son Pham", "title": "Autocovariance Varieties of Moving Average Random Fields", "comments": "20 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the autocovariance functions of moving average random fields over\nthe integer lattice $\\mathbb{Z}^d$ from an algebraic perspective. These\nautocovariances are parametrized polynomially by the moving average\ncoefficients, hence tracing out algebraic varieties. We derive dimension and\ndegree of these varieties and we use their algebraic properties to obtain\nstatistical consequences such as identifiability of model parameters. We\nconnect the problem of parameter estimation to the algebraic invariants known\nas euclidean distance degree and maximum likelihood degree. Throughout, we\nillustrate the results with concrete examples. In our computations we use tools\nfrom commutative algebra and numerical algebraic geometry.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:53:30 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Pham", "Viet Son", ""]]}, {"id": "1903.08640", "submitter": "Frederik Heber", "authors": "Frederik Heber, Zofia Trstanova, Benedict Leimkuhler", "title": "TATi-Thermodynamic Analytics ToolkIt: TensorFlow-based software for\n  posterior sampling in machine learning applications", "comments": "25 pages: textual improvements with results unchanged, sections on\n  TATi architecture and software performance removed for size constraints,\n  extended EQN parts, added MNIST nonlinear perceptron example", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of GPU-assisted hardware and maturing high-efficiency\nsoftware platforms such as TensorFlow and PyTorch, Bayesian posterior sampling\nfor neural networks becomes plausible. In this article we discuss Bayesian\nparametrization in machine learning based on Markov Chain Monte Carlo methods,\nspecifically discretized stochastic differential equations such as Langevin\ndynamics and extended system methods in which an ensemble of walkers is\nemployed to enhance sampling. We provide a glimpse of the potential of the\nsampling-intensive approach by studying (and visualizing) the loss landscape of\na neural network applied to the MNIST data set. Moreover, we investigate how\nthe sampling efficiency itself can be significantly enhanced through an\nensemble quasi-Newton preconditioning method. This article accompanies the\nrelease of a new TensorFlow software package, the Thermodynamic Analytics\nToolkIt, which is used in the computational experiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:56:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 20:26:23 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Heber", "Frederik", ""], ["Trstanova", "Zofia", ""], ["Leimkuhler", "Benedict", ""]]}, {"id": "1903.08645", "submitter": "Amadou Diadie Ba", "authors": "Amadou Diadie Ba and Gane Samb Lo", "title": "Entropies and their Asymptotic Theory in the discrete case", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some new nonparametric estimators of entropies and we establish\nalmost sure consistency and central limit Theorems for some of the most\nimportant entropies in the discrete case. Our theorical results are validated\nby simulations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 21:12:14 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Ba", "Amadou Diadie", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1903.08656", "submitter": "Michael Trosset", "authors": "Michael W. Trosset, Carey E. Priebe", "title": "Approximate Information Tests on Statistical Submanifolds", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric inference posits a statistical model that is a specified family of\nprobability distributions. Restricted inference, e.g., restricted likelihood\nratio testing, attempts to exploit the structure of a statistical submodel that\nis a subset of the specified family. We consider the problem of testing a\nsimple hypothesis against alternatives from such a submodel. In the case of an\nunknown submodel, it is not clear how to realize the benefits of restricted\ninference. To do so, we first construct information tests that are locally\nasymptotically equivalent to likelihood ratio tests. Information tests are\nconceptually appealing but (in general) computationally intractable. However,\nunlike restricted likelihood ratio tests, restricted information tests can be\napproximated even when the statistical submodel is unknown. We construct\napproximate information tests using manifold learning procedures to extract\ninformation from samples of an unknown (or intractable) submodel, thereby\nproviding a roadmap for computational solutions to a class of previously\nimpenetrable problems in statistical inference. Examples illustrate the\nefficacy of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:01:43 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Trosset", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1903.08687", "submitter": "Carlos Matr\\'an", "authors": "Eustasio del Barrio, Hristo Inouzhe and Carlos Matr\\'an", "title": "On approximate validation of models: A Kolmogorov-Smirnov based approach", "comments": "14 figures, 32 pages", "journal-ref": null, "doi": "10.1007/s11749-019-00691-1", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical tests of fit typically reject a model for large enough real data\nsamples. In contrast, often in statistical practice a model offers a good\ndescription of the data even though it is not the \"true\" random generator. We\nconsider a more flexible approach based on contamination neighbourhoods around\na model. Using trimming methods and the Kolmogorov metric we introduce a\nfunctional statistic measuring departures from a contaminated model and the\nassociated estimator corresponding to its sample version. We show how this\nestimator allows testing of fit for the (slightly) contaminated model vs\nsensible deviations from it, with uniformly exponentially small type I and type\nII error probabilities. We also address the asymptotic behavior of the\nestimator showing that, under suitable regularity conditions, it asymptotically\nbehaves as the supremum of a Gaussian process. As an application we explore\nmethods of comparison between descriptive models based on the paradigm of model\nfalseness. We also include some connections of our approach with the\nFalse-Discovery-Rate setting, showing competitive behavior when estimating the\ncontamination level, although applicable in a wider framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 18:30:59 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Inouzhe", "Hristo", ""], ["Matr\u00e1n", "Carlos", ""]]}, {"id": "1903.08704", "submitter": "Kaspar Wuthrich", "authors": "Kaspar Wuthrich and Ying Zhu", "title": "Omitted variable bias of Lasso-based inference methods: A finite sample\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the finite sample behavior of Lasso-based inference methods such as\npost double Lasso and debiased Lasso. We show that these methods can exhibit\nsubstantial omitted variable biases (OVBs) due to Lasso not selecting relevant\ncontrols. This phenomenon can occur even when the coefficients are sparse and\nthe sample size is large and larger than the number of controls. Therefore,\nrelying on the existing asymptotic inference theory can be problematic in\nempirical applications. We compare the Lasso-based inference methods to modern\nhigh-dimensional OLS-based methods and provide practical guidance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 19:05:35 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 22:07:38 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 17:45:34 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 06:26:44 GMT"}, {"version": "v5", "created": "Mon, 13 Jan 2020 02:54:00 GMT"}, {"version": "v6", "created": "Thu, 10 Sep 2020 01:28:56 GMT"}, {"version": "v7", "created": "Fri, 18 Jun 2021 00:25:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wuthrich", "Kaspar", ""], ["Zhu", "Ying", ""]]}, {"id": "1903.08743", "submitter": "Hanbaek Lyu", "authors": "Sam Dittmer, Hanbaek Lyu, Igor Pak", "title": "Phase transition in random contingency tables with non-uniform margins", "comments": "24 pages, 4 figures. Earlier version is accepted for publication in\n  Transactions of the AMS. This version contains an appendix on asymptotic\n  independence of the entries", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For parameters $n,\\delta,B,$ and $C$, let $X=(X_{k\\ell})$ be the random\nuniform contingency table whose first $\\lfloor n^{\\delta} \\rfloor $ rows and\ncolumns have margin $\\lfloor BCn \\rfloor$ and the last $n$ rows and columns\nhave margin $\\lfloor Cn \\rfloor$. For every $0<\\delta<1$, we establish a sharp\nphase transition of the limiting distribution of each entry of $X$ at the\ncritical value $B_{c}=1+\\sqrt{1+1/C}$. In particular, for $1/2<\\delta<1$, we\nshow that the distribution of each entry converges to a geometric distribution\nin total variation distance, whose mean depends sensitively on whether\n$B<B_{c}$ or $B>B_{c}$. Our main result shows that $\\mathbb{E}[X_{11}]$ is\nuniformly bounded for $B<B_{c}$, but has sharp asymptotic $C(B-B_{c})\nn^{1-\\delta}$ for $B>B_{c}$. We also establish a strong law of large numbers\nfor the row sums in top right and top left blocks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:05:53 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 23:22:11 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 10:48:55 GMT"}, {"version": "v4", "created": "Fri, 18 Oct 2019 02:16:45 GMT"}, {"version": "v5", "created": "Sat, 12 Sep 2020 03:38:07 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Dittmer", "Sam", ""], ["Lyu", "Hanbaek", ""], ["Pak", "Igor", ""]]}, {"id": "1903.08939", "submitter": "So Takao", "authors": "Alexis Arnaudon, Alessandro Barp, So Takao", "title": "Irreversible Langevin MCMC on Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that irreversible MCMC algorithms converge faster to their\nstationary distributions than reversible ones. Using the special geometric\nstructure of Lie groups $\\mathcal G$ and dissipation fields compatible with the\nsymplectic structure, we construct an irreversible HMC-like MCMC algorithm on\n$\\mathcal G$, where we first update the momentum by solving an OU process on\nthe corresponding Lie algebra $\\mathfrak g$, and then approximate the\nHamiltonian system on $\\mathcal G \\times \\mathfrak g$ with a reversible\nsymplectic integrator followed by a Metropolis-Hastings correction step. In\nparticular, when the OU process is simulated over sufficiently long times, we\nrecover HMC as a special case. We illustrate this algorithm numerically using\nthe example $\\mathcal G = SO(3)$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 12:02:59 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Arnaudon", "Alexis", ""], ["Barp", "Alessandro", ""], ["Takao", "So", ""]]}, {"id": "1903.08987", "submitter": "Angshuman Roy", "authors": "Angshuman Roy, Anil Ghosh, Alok Goswami, C. A. Murthy", "title": "Some New Copula Based Distribution-free Tests of Independence among\n  Several Random Variables", "comments": "arXiv admin note: text overlap with arXiv:1708.07485", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last couple of decades, several copula based methods have been\nproposed in the literature to test for the independence among several random\nvariables. But these existing tests are not invariant under monotone\ntransformations of the variables, and they often perform poorly if the\ndependence among the variables is highly non-monotone in nature. In this\narticle, we propose a copula based measure of dependency and use it to\nconstruct some new distribution-free tests of independence. The proposed\nmeasure and the resulting tests, all are invariant under permutations and\nmonotone transformations of the variables. Our dependency measure involves a\nkernel function, and we use the Gaussian kernel for that purpose. We adopt a\nmulti-scale approach, where we look at the results obtained for several choices\nof the bandwidth parameter associated with the Gaussian kernel and aggregate\nthem judiciously. Large sample properties of the dependency measure and the\nresulting tests are derived under appropriate regularity conditions. Several\nsimulated and real data sets are analyzed to compare the performance of the\nproposed tests with some popular tests available in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:15:57 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 10:03:09 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Roy", "Angshuman", ""], ["Ghosh", "Anil", ""], ["Goswami", "Alok", ""], ["Murthy", "C. A.", ""]]}, {"id": "1903.09019", "submitter": "Tobias Siems", "authors": "Tobias Siems", "title": "Markov Chain Monte Carlo on Finite State Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We elaborate the idea behind Markov chain Monte Carlo (MCMC) methods in a\nmathematically coherent, yet simple and understandable way. To this end, we\nproof a pivotal convergence theorem for finite Markov chains and a minimal\nversion of the Perron-Frobenius theorem. Subsequently, we briefly discuss two\nfundamental MCMC methods, the Gibbs and Metropolis-Hastings sampler. Only very\nbasic knowledge about matrices, convergence of real sequences and probability\ntheory is required.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:01:18 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 20:37:43 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 12:38:30 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 13:40:17 GMT"}, {"version": "v5", "created": "Sun, 28 Jul 2019 17:04:06 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Siems", "Tobias", ""]]}, {"id": "1903.09235", "submitter": "Taiyao Wang", "authors": "Taiyao Wang, Ioannis Ch. Paschalidis", "title": "Convergence of Parameter Estimates for Regularized Mixed Linear\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider {\\em Mixed Linear Regression (MLR)}, where training data have\nbeen generated from a mixture of distinct linear models (or clusters) and we\nseek to identify the corresponding coefficient vectors. We introduce a {\\em\nMixed Integer Programming (MIP)} formulation for MLR subject to regularization\nconstraints on the coefficient vectors. We establish that as the number of\ntraining samples grows large, the MIP solution converges to the true\ncoefficient vectors in the absence of noise. Subject to slightly stronger\nassumptions, we also establish that the MIP identifies the clusters from which\nthe training samples were generated. In the special case where training data\ncome from a single cluster, we establish that the corresponding MIP yields a\nsolution that converges to the true coefficient vector even when training data\nare perturbed by (martingale difference) noise. We provide a counterexample\nindicating that in the presence of noise, the MIP may fail to produce the true\ncoefficient vectors for more than one clusters. We also provide numerical\nresults testing the MIP solutions in synthetic examples with noise.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 20:44:20 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 15:30:59 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Taiyao", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1903.09321", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "WONDER: Weighted one-shot distributed ridge regression in high\n  dimensions", "comments": "Gave the name \"Wonder\" to the algorithm, updated title, added\n  algorithm for general non-isotropic design", "journal-ref": null, "doi": null, "report-no": "Journal of Machine Learning Research 21(66) p. 1-52 2020. Short\n  version at ICML 2020", "categories": "math.ST cs.DC cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas, practitioners need to analyze large datasets that challenge\nconventional single-machine computing. To scale up data analysis, distributed\nand parallel computing approaches are increasingly needed.\n  Here we study a fundamental and highly important problem in this area: How to\ndo ridge regression in a distributed computing environment? Ridge regression is\nan extremely popular method for supervised learning, and has several optimality\nproperties, thus it is important to study. We study one-shot methods that\nconstruct weighted combinations of ridge regression estimators computed on each\nmachine. By analyzing the mean squared error in a high dimensional\nrandom-effects model where each predictor has a small effect, we discover\nseveral new phenomena.\n  1. Infinite-worker limit: The distributed estimator works well for very large\nnumbers of machines, a phenomenon we call \"infinite-worker limit\".\n  2. Optimal weights: The optimal weights for combining local estimators sum to\nmore than unity, due to the downward bias of ridge. Thus, all averaging methods\nare suboptimal.\n  We also propose a new Weighted ONe-shot DistributEd Ridge regression (WONDER)\nalgorithm. We test WONDER in simulation studies and using the Million Song\nDataset as an example. There it can save at least 100x in computation time,\nwhile nearly preserving test accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 02:26:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:25:58 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1903.09367", "submitter": "Peng Zhao", "authors": "Peng Zhao, Yun Yang and Qiao-Chu He", "title": "Implicit Regularization via Hadamard Product Over-Parametrization in\n  High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Hadamard product parametrization as a change-of-variable\n(over-parametrization) technique for solving least square problems in the\ncontext of linear regression. Despite the non-convexity and exponentially many\nsaddle points induced by the change-of-variable, we show that under certain\nconditions, this over-parametrization leads to implicit regularization: if we\ndirectly apply gradient descent to the residual sum of squares with\nsufficiently small initial values, then under proper early stopping rule, the\niterates converge to a nearly sparse rate-optimal solution with relatively\nbetter accuracy than explicit regularized approaches. In particular, the\nresulting estimator does not suffer from extra bias due to explicit penalties,\nand can achieve the parametric root-$n$ rate (independent of the dimension)\nunder proper conditions on the signal-to-noise ratio. We perform simulations to\ncompare our methods with high dimensional linear regression with explicit\nregularizations. Our results illustrate advantages of using implicit\nregularization via gradient descent after over-parametrization in sparse vector\nestimation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:56:04 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Yun", ""], ["He", "Qiao-Chu", ""]]}, {"id": "1903.09488", "submitter": "S. Jalil Kazemitabar", "authors": "S. Jalil Kazemitabar, Arash A. Amini and Ameet Talwalkar", "title": "On the support recovery of marginal regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leading methods for support recovery in high-dimensional regression, such as\nLasso, have been well-studied and their limitations in the context of\ncorrelated design have been characterized with precise incoherence conditions.\nIn this work, we present a similar treatment of selection consistency for\nmarginal regression (MR), a computationally efficient family of methods with\nconnections to decision trees. Selection based on marginal regression is also\nreferred to as covariate screening or independence screening and is a popular\napproach in applied work, especially in ultra high-dimensional settings. We\nidentify the underlying factors---which we denote as \\emph{MR\nincoherence}---affecting MR's support recovery performance. Our near complete\ncharacterization provides a much more nuanced and optimistic view of MR in\ncomparison to previous works. To ground our results, we provide a broad\ntaxonomy of results for leading feature selection methods, relating the\nbehavior of Lasso, OMP, SIS, and MR. We also lay the foundation for interesting\ngeneralizations of our analysis, e.g., to non-linear feature selection methods\nand to more general regression frameworks such as a general additive models.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 13:19:26 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Kazemitabar", "S. Jalil", ""], ["Amini", "Arash A.", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1903.09592", "submitter": "Zhou Fan", "authors": "Zhou Fan and Yi Sun and Zhichao Wang", "title": "Principal components in linear mixed models with general bulk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the principal components of covariance estimators in multivariate\nmixed-effects linear models. We show that, in high dimensions, the principal\neigenvalues and eigenvectors may exhibit bias and aliasing effects that are not\npresent in low-dimensional settings. We derive the first-order limits of the\nprincipal eigenvalue locations and eigenvector projections in a\nhigh-dimensional asymptotic framework, allowing for general population spectral\ndistributions for the random effects and extending previous results from a more\nrestrictive spiked model. Our analysis uses free probability techniques, and we\ndevelop two general tools of independent interest-- strong asymptotic freeness\nof GOE and deterministic matrices and a free deterministic equivalent\napproximation for bilinear forms of resolvents.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 16:37:39 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 19:34:11 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 20:21:44 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Fan", "Zhou", ""], ["Sun", "Yi", ""], ["Wang", "Zhichao", ""]]}, {"id": "1903.09631", "submitter": "Parthe Pandit", "authors": "Parthe Pandit, Mojtaba Sahraee-Ardakan, Arash A. Amini, Sundeep\n  Rangan, Alyson K. Fletcher", "title": "High-Dimensional Bernoulli Autoregressive Process with Long-Range\n  Dependence", "comments": "To appear at AISTATS 2019 titled \"Sparse Multivariate Bernoulli\n  Processes in High Dimensions\"", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR:\n  Volume 89", "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of a multivariate\nBernoulli process with auto-regressive feedback in the high-dimensional setting\nwhere the number of samples available is much less than the number of\nparameters. This problem arises in learning interconnections of networks of\ndynamical systems with spiking or binary-valued data. We allow the process to\ndepend on its past up to a lag $p$, for a general $p \\ge 1$, allowing for more\nrealistic modeling in many applications. We propose and analyze an\n$\\ell_1$-regularized maximum likelihood estimator (MLE) under the assumption\nthat the parameter tensor is approximately sparse. Rigorous analysis of such\nestimators is made challenging by the dependent and non-Gaussian nature of the\nprocess as well as the presence of the nonlinearities and multi-level feedback.\nWe derive precise upper bounds on the mean-squared estimation error in terms of\nthe number of samples, dimensions of the process, the lag $p$ and other key\nstatistical properties of the model. The ideas presented can be used in the\nhigh-dimensional analysis of regularized $M$-estimators for other sparse\nnonlinear and non-Gaussian processes with long-range dependence.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 06:06:27 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Pandit", "Parthe", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Amini", "Arash A.", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "1903.09741", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Bai Jiang, Qiang Sun", "title": "Bayesian Factor-adjusted Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the high-dimensional linear regression with highly\ncorrelated covariates. In this setup, the traditional sparsity assumption on\nthe regression coefficients often fails to hold, and consequently many model\nselection procedures do not work. To address this challenge, we model the\nvariations of covariates by a factor structure. Specifically, strong\ncorrelations among covariates are explained by common factors and the remaining\nvariations are interpreted as idiosyncratic components of each covariate. This\nleads to a factor-adjusted regression model with both common factors and\nidiosyncratic components as covariates. We generalize the traditional sparsity\nassumption accordingly and assume that all common factors but only a small\nnumber of idiosyncratic components contribute to the response. A Bayesian\nprocedure with a spike-and-slab prior is then proposed for parameter estimation\nand model selection. Simulation studies show that our Bayesian method\noutperforms its lasso analogue, manifests insensitivity to the overestimates of\nthe number of common factors, pays a negligible price in the no correlation\ncase, and scales up well with increasing sample size, dimensionality and\nsparsity. Numerical results on a real dataset of U.S. bond risk premia and\nmacroeconomic indicators lend strong support to our methodology.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 01:01:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Fan", "Jianqing", ""], ["Jiang", "Bai", ""], ["Sun", "Qiang", ""]]}, {"id": "1903.09859", "submitter": "Viktor Bengs", "authors": "Viktor Bengs, Matthias Eulert and Hajo Holzmann", "title": "Asymptotic confidence sets for the jump curve in bivariate regression\n  problems", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 173, 291-312, 2019", "doi": "10.1016/j.jmva.2019.02.017", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We construct uniform and point-wise asymptotic confidence sets for the single\nedge in an otherwise smooth image function which are based on rotated\ndifferences of two one-sided kernel estimators. Using methods from\nM-estimation, we show consistency of the estimators of location, slope and\nheight of the edge function and develop a uniform linearization of the contrast\nprocess. The uniform confidence bands then rely on a Gaussian approximation of\nthe score process together with anti-concentration results for suprema of\nGaussian processes, while point-wise bands are based on asymptotic normality.\nThe finite-sample performance of the point-wise proposed methods is\ninvestigated in a simulation study. An illustration to real-world image\nprocessing is also given.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 18:15:14 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bengs", "Viktor", ""], ["Eulert", "Matthias", ""], ["Holzmann", "Hajo", ""]]}, {"id": "1903.09873", "submitter": "Emil Aas Stoltenberg", "authors": "Emil A. Stoltenberg, Per A. Mykland and Lan Zhang", "title": "A CLT for second difference estimators with an application to volatility\n  and intensity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a general method for estimating the quadratic\ncovariation of one or more spot parameters processes associated with continuous\ntime semimartingales. This estimator is applicable to a wide range of spot\nparameter processes, and may also be used to estimate the leverage effect of\nstochastic volatility models. The estimator we introduce is based on sums of\nsquared increments of second differences of the observed process, and the\nintervals over which the differences are computed are rolling and overlapping.\nThis latter feature lets us take full advantage of the data, and, by\nsufficiency considerations, ought to outperform estimators that are only based\non one partition of the observational window. The main result of the paper is a\ncentral limit theorem for such triangular array rolling quadratic variations.\nWe highlight the wide applicability of this theorem by showcasing how it might\nbe applied to a novel leverage effect estimator. The principal motivation for\nthe present study, however, is that the discrete times at which a continuous\ntime semimartingale is observed might depend on features of the observable\nprocess other than its level, such as its (non-observable) spot-volatility\nprocess. As the main application of our estimator, we therefore show how it may\nbe used to estimate the quadratic covariation between the spot-volatility\nprocess and the intensity process of the observation times, when both of these\nare taken to be semimartingales. The finite sample properties of this estimator\nare studied by way of a simulation experiment, and we also apply this estimator\nin an empirical analysis of the Apple stock. Our analysis of the Apple stock\nindicates a rather strong correlation between the spot volatility process of\nthe log-prices process and the times at which this stock is traded (hence\nobserved).\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:00:13 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 21:44:35 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 15:51:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Stoltenberg", "Emil A.", ""], ["Mykland", "Per A.", ""], ["Zhang", "Lan", ""]]}, {"id": "1903.09935", "submitter": "Dominik Sieradzki", "authors": "Dominik Sieradzki, Wojciech Zieli\\'nski", "title": "Cost Issue in Estimation of Proportion in a Finite Population Divided\n  Among Two Strata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimation of the proportion of units with a given attribute\nin a~finite population is considered. From the population a sample is drawn due\nto the simple random sampling without replacement. There are limited funds for\nconducting survey sample. Suppose that the population is divided into two\nstrata. The question now arises: how should sample sizes be chosen from each\nstrata to obtain the best estimation of proportion without exceeding the budget\nplanned. In the paper it is shown, that with the appropriate sample allocation\nthe variance of the stratified estimator may be reduced up to 30% off of the\nstandard, unstratified estimator.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 06:51:52 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Sieradzki", "Dominik", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.10047", "submitter": "Kenta Oono", "authors": "Kenta Oono, Taiji Suzuki", "title": "Approximation and Non-parametric Estimation of ResNet-type Convolutional\n  Neural Networks", "comments": "8 pages + References 2 pages + Supplemental material 18 pages", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been shown to achieve optimal\napproximation and estimation error rates (in minimax sense) in several function\nclasses. However, previous analyzed optimal CNNs are unrealistically wide and\ndifficult to obtain via optimization due to sparse constraints in important\nfunction classes, including the H\\\"older class. We show a ResNet-type CNN can\nattain the minimax optimal error rates in these classes in more plausible\nsituations -- it can be dense, and its width, channel size, and filter size are\nconstant with respect to sample size. The key idea is that we can replicate the\nlearning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as\nlong as the FNNs have \\textit{block-sparse} structures. Our theory is general\nin a sense that we can automatically translate any approximation rate achieved\nby block-sparse FNNs into that by CNNs. As an application, we derive\napproximation and estimation error rates of the aformentioned type of CNNs for\nthe Barron and H\\\"older classes with the same strategy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 19:42:39 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 12:22:39 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 13:23:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Oono", "Kenta", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1903.10063", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Moulinath Banerjee and Ya'acov Ritov", "title": "Optimal Linear Discriminators For The Discrete Choice Model In Growing\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manski's celebrated maximum score estimator for the discrete choice model,\nwhich is an optimal linear discriminator, has been the focus of much\ninvestigation in both the econometrics and statistics literatures, but its\nbehavior under growing dimension scenarios largely remains unknown. This paper\naddresses that gap. Two different cases are considered: $p$ grows with $n$ but\nat a slow rate, i.e. $p/n \\rightarrow 0$; and $p \\gg n$ (fast growth). In the\nbinary response model, we recast Manski's score estimation as empirical risk\nminimization for a classification problem, and derive the $\\ell_2$ rate of\nconvergence of the score estimator under a \\emph{transition condition} in terms\nof our margin parameter that calibrates the level of difficulty of the\nestimation problem. We also establish upper and lower bounds for the minimax\n$\\ell_2$ error in the binary choice model that differ by a logarithmic factor,\nand construct a minimax-optimal estimator in the slow growth regime. Some\nextensions to the general case -- the multinomial response model -- are also\nconsidered. Last but not least, we use a variety of learning algorithms to\ncompute the maximum score estimator in growing dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 21:39:34 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 23:42:10 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 21:44:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1903.10099", "submitter": "Yi Zhang", "authors": "Nobuki Takayama, Lin Jiu, Satoshi Kuriki, and Yi Zhang", "title": "Computation of the Expected Euler Characteristic for the Largest\n  Eigenvalue of a Real Non-central Wishart Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an approximate formula for the distribution of the largest eigenvalue\nof real Wishart matrices by the expected Euler characteristic method for the\ngeneral dimension. The formula is expressed in terms of a definite integral\nwith parameters. We derive a differential equation satisfied by the integral\nfor the $2 \\times 2$ matrix case and perform a numerical analysis of it.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 02:06:17 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 02:07:36 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Takayama", "Nobuki", ""], ["Jiu", "Lin", ""], ["Kuriki", "Satoshi", ""], ["Zhang", "Yi", ""]]}, {"id": "1903.10101", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Inequalities between $L^p$-norms for log-concave distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.FA math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-concave distributions include some important distributions such as normal\ndistribution, exponential distribution and so on. In this note, we show\ninequalities between two Lp-norms for log-concave distributions on the\nEuclidean space. These inequalities are the generalizations of the upper and\nlower bound of the differential entropy and are also interpreted as a kind of\nexpansion of the inequality between two Lp-norms on the measurable set with\nfinite measure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 02:09:50 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1903.10233", "submitter": "Yang Wang", "authors": "Yang Wang and Zhangsheng Yu", "title": "A kernel regression model for panel count data with time-varying\n  coefficients", "comments": "33 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the conditional mean function of panel count model with time-varying\ncoefficients, we propose to use local kernel regression method for estimation.\nPartial log-likelihood with local polynomial is formed for estimation. Under\nsome regularity conditions, strong uniform consistency rates are obtained for\nthe local estimator. At target time point, we show that the local estimator\nconverges in distribution to normal distribution. The baseline mean function\nestimator is also shown to be consistent. Simulation studies show that the\ntime-varying coefficient estimator is close to the true value, the empirical\ncoverage probabilities of the confidence interval is close to the nominal\nlevel. We also applied the proposed method to analyze a clinical study on\nchildhood wheezing.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:53:09 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wang", "Yang", ""], ["Yu", "Zhangsheng", ""]]}, {"id": "1903.10603", "submitter": "Andrea Montanari", "authors": "Michael Celentano and Andrea Montanari", "title": "Fundamental Barriers to High-Dimensional Regression with Convex\n  Penalties", "comments": "112 pages; 1 pdf figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional regression, we attempt to estimate a parameter vector\n${\\boldsymbol \\beta}_0\\in{\\mathbb R}^p$ from $n\\lesssim p$ observations\n$\\{(y_i,{\\boldsymbol x}_i)\\}_{i\\le n}$ where ${\\boldsymbol x}_i\\in{\\mathbb\nR}^p$ is a vector of predictors and $y_i$ is a response variable. A\nwell-estabilished approach uses convex regularizers to promote specific\nstructures (e.g. sparsity) of the estimate $\\widehat{\\boldsymbol \\beta}$, while\nallowing for practical algorithms. Theoretical analysis implies that convex\npenalization schemes have nearly optimal estimation properties in certain\nsettings. However, in general the gaps between statistically optimal estimation\n(with unbounded computational resources) and convex methods are poorly\nunderstood.\n  We show that, in general, a large gap exists between the best performance\nachieved by \\emph{any convex regularizer} and the optimal statistical error.\nRemarkably, we demonstrate that this gap is generic as soon as we try to\nincorporate very simple structural information about the empirical distribution\nof the entries of ${\\boldsymbol \\beta}_0$. Our results follow from a detailed\nstudy of standard Gaussian designs, a setting that is normally considered\nparticularly friendly to convex regularization schemes such as the Lasso. We\nprove a lower bound on the estimation error achieved by any convex regularizer\nwhich is invariant under permutations of the coordinates of its argument. This\nbound is expected to be generally tight, and indeed we prove tightness under\ncertain conditions. Further, it implies a gap with respect to Bayes-optimal\nestimation that can be precisely quantified and persists if the prior\ndistribution of the signal ${\\boldsymbol \\beta}_0$ is known to the\nstatistician.\n  Our results provide rigorous evidence towards a broad conjecture regarding\ncomputational-statistical gaps in high-dimensional estimation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 21:42:27 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""]]}, {"id": "1903.10769", "submitter": "Maylis Varvenne", "authors": "Fabien Panloup (LAREMA), Samy Tindel, Maylis Varvenne (IMT)", "title": "A general drift estimation procedure for stochastic differential\n  equations with additive fractional noise", "comments": null, "journal-ref": "Electronic journal of statistics , Shaker Heights, OH : Institute\n  of Mathematical Statistics, 2020, 14 (1), pp.1075-1136", "doi": "10.1214/20-EJS1685", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the drift estimation problem for a general\ndifferential equation driven by an additive multidimensional fractional\nBrownian motion, under ergodic assumptions on the drift coefficient. Our\nestimation procedure is based on the identification of the invariant measure,\nand we provide consistency results as well as some information about the\nconvergence rate. We also give some examples of coefficients for which the\nidentifiability assumption for the invariant measure is satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 10:03:15 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 08:53:12 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Panloup", "Fabien", "", "LAREMA"], ["Tindel", "Samy", "", "IMT"], ["Varvenne", "Maylis", "", "IMT"]]}, {"id": "1903.10914", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny", "title": "Estimation of a regular conditional functional by conditional\n  U-statistics regression", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-statistics constitute a large class of estimators, generalizing the\nempirical mean of a random variable $X$ to sums over every $k$-tuple of\ndistinct observations of $X$. They may be used to estimate a regular functional\n$\\theta(P_{X})$ of the law of $X$. When a vector of covariates $Z$ is\navailable, a conditional U-statistic may describe the effect of $z$ on the\nconditional law of $X$ given $Z=z$, by estimating a regular conditional\nfunctional $\\theta(P_{X|Z=\\cdot})$. We prove concentration inequalities for\nconditional U-statistics. Assuming a parametric model of the conditional\nfunctional of interest, we propose a regression-type estimator based on\nconditional U-statistics. Its theoretical properties are derived, first in a\nnon-asymptotic framework and then in two different asymptotic regimes. Some\nexamples are given to illustrate our methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:18:51 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Derumigny", "Alexis", ""]]}, {"id": "1903.11142", "submitter": "Shota Gugushvili", "authors": "Shota Gugushvili, Ester Mariucci, Frank van der Meulen", "title": "Decompounding discrete distributions: A non-parametric Bayesian approach", "comments": "27 pages, 7 figures", "journal-ref": "Scand J. Statist., 47(2):464-492, 2020", "doi": "10.1111/sjos.12413", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a compound Poisson process is observed discretely in time and\nassume that its jump distribution is supported on the set of natural numbers.\nIn this paper we propose a non-parametric Bayesian approach to estimate the\nintensity of the underlying Poisson process and the distribution of the jumps.\nWe provide a MCMC scheme for obtaining samples from the posterior. We apply our\nmethod on both simulated and real data examples, and compare its performance\nwith the frequentist plug-in estimator proposed by Buchmann and Gr\\\"ubel. On a\ntheoretical side, we study the posterior from the frequentist point of view and\nprove that as the sample size $n\\rightarrow\\infty$, it contracts around the\n`true', data-generating parameters at rate $1/\\sqrt{n}$, up to a $\\log n$\nfactor.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 20:17:21 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 09:59:11 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Gugushvili", "Shota", ""], ["Mariucci", "Ester", ""], ["van der Meulen", "Frank", ""]]}, {"id": "1903.11220", "submitter": "Lifeng Lai", "authors": "Erhan Bayraktar and Lifeng Lai", "title": "On the Adversarial Robustness of Multivariate Robust Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the adversarial robustness of multivariate\n$M$-Estimators. In the considered model, after observing the whole dataset, an\nadversary can modify all data points with the goal of maximizing inference\nerrors. We use adversarial influence function (AIF) to measure the asymptotic\nrate at which the adversary can change the inference result. We first\ncharacterize the adversary's optimal modification strategy and its\ncorresponding AIF. From the defender's perspective, we would like to design an\nestimator that has a small AIF. For the case of joint location and scale\nestimation problem, we characterize the optimal $M$-estimator that has the\nsmallest AIF. We further identify a tradeoff between robustness against\nadversarial modifications and robustness against outliers, and derive the\noptimal $M$-estimator that achieves the best tradeoff.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 01:54:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Bayraktar", "Erhan", ""], ["Lai", "Lifeng", ""]]}, {"id": "1903.11370", "submitter": "Harsha Honnappa", "authors": "Remco van der Hofstad and Harsha Honnappa", "title": "Large Deviations of Bivariate Gaussian Extrema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish sharp tail asymptotics for component-wise extreme values of\nbivariate Gaussian random vectors with arbitrary correlation between the\ncomponents. We consider two scaling regimes for the tail event in which we\ndemonstrate the existence of a restricted large deviations principle, and\nidentify the unique rate function associated with these asymptotics. Our\nresults identify when the maxima of both coordinates are typically attained by\ntwo different vs. the same index, and how this depends on the correlation\nbetween the coordinates of the bivariate Gaussian random vectors. Our results\ncomplement a growing body of work on the extremes of Gaussian processes. The\nresults are also relevant for steady-state performance and simulation analysis\nof networks of infinite server queues.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:13:19 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["van der Hofstad", "Remco", ""], ["Honnappa", "Harsha", ""]]}, {"id": "1903.11555", "submitter": "Wojciech Zieli\\'nski", "authors": "Stanis{\\l}aw Jaworski, Wojciech Zieli\\'nski", "title": "The shortest confidence interval for the weighted sum of two Binomial\n  proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval estimation of the probability of success in a Binomial model is\nconsidered. Zieli\\'nski (2018) showed that the confidence interval which uses\ninformation about non-homogeneity of the sample is better than the classical\none. In the following paper the shortest confidence interval for non-homogenous\nsample is constructed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:16:39 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Jaworski", "Stanis\u0142aw", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.11582", "submitter": "Hong Hu", "authors": "Hong Hu, Yue M. Lu", "title": "SLOPE for Sparse Linear Regression:Asymptotics and Optimal\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse linear regression, the SLOPE estimator generalizes LASSO by\npenalizing different coordinates of the estimate according to their magnitudes.\nIn this paper, we present a precise performance characterization of SLOPE in\nthe asymptotic regime where the number of unknown parameters grows in\nproportion to the number of observations. Our asymptotic characterization\nenables us to derive the fundamental limits of SLOPE in both estimation and\nvariable selection settings. We also provide a computational feasible way to\noptimally design the regularizing sequences such that the fundamental limits\nare reached. In both settings, we show that the optimal design problem can be\nformulated as certain infinite-dimensional convex optimization problems, which\nhave efficient and accurate finite-dimensional approximations. Numerical\nsimulations verify all our asymptotic predictions. They demonstrate the\nsuperiority of our optimal regularizing sequences over other designs used in\nthe existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:48:32 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 14:38:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hu", "Hong", ""], ["Lu", "Yue M.", ""]]}, {"id": "1903.11867", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen", "title": "Classification of sparse binary vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider a problem of multi-label classification, where each\ninstance is associated with some binary vector. Our focus is to find a\nclassifier which minimizes false negative discoveries under constraints.\nDepending on the considered set of constraints we propose plug-in methods and\nprovide non-asymptotic analysis under margin type assumptions. Specifically, we\nanalyze two particular examples of constraints that promote sparse predictions:\nin the first one, we focus on classifiers with $\\ell_0$-type constraints and in\nthe second one, we address classifiers with bounded false positive discoveries.\nBoth formulations lead to different Bayes rules and, thus, different plug-in\napproaches. The first considered scenario is the popular multi-label top-$K$\nprocedure: a label is predicted to be relevant if its score is among the $K$\nlargest ones. For this case, we provide an excess risk bound that achieves so\ncalled `fast' rates of convergence under a generalization of the margin\nassumption to this settings. The second scenario differs significantly from the\ntop-$K$ settings, as the constraints are distribution dependent. We demonstrate\nthat in this scenario the almost sure control of false positive discoveries is\nimpossible without extra assumptions. To alleviate this issue we propose a\nsufficient condition for the consistent estimation and provide non-asymptotic\nupper-bound.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 09:57:17 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Chzhen", "Evgenii", ""]]}, {"id": "1903.12035", "submitter": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak", "authors": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak, Jesper M{\\o}ller, Tom\\'a\\v{s} Mrkvi\\v{c}ka\n  and Samuel Soubeyrand", "title": "Quick inference for log Gaussian Cox processes with non-stationary\n  underlying random fields", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2019.100388", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For point patterns observed in natura, spatial heterogeneity is more the rule\nthan the exception. In numerous applications, this can be mathematically\nhandled by the flexible class of log Gaussian Cox processes (LGCPs); in brief,\na LGCP is a Cox process driven by an underlying log Gaussian random field (log\nGRF). This allows the representation of point aggregation, point vacuum and\nintermediate situations, with more or less rapid transitions between these\ndifferent states depending on the properties of GRF. Very often, the covariance\nfunction of the GRF is assumed to be stationary. In this article, we give two\nexamples where the sizes (that is, the number of points) and the spatial\nextents of point clusters are allowed to vary in space. To tackle such\nfeatures, we propose parametric and semiparametric models of non-stationary\nLGCPs where the non-stationarity is included in both the mean function and the\ncovariance function of the GRF. Thus, in contrast to most other work on\ninhomogeneous LGCPs, second-order intensity-reweighted stationarity is not\nsatisfied and the usual two step procedure for parameter estimation based on\ne.g. composite likelihood does not easily apply. Instead we propose a fast\nthree step procedure based on composite likelihood. We apply our modelling and\nestimation framework to analyse datasets dealing with fish aggregation in a\nreservoir and with dispersal of biological particles.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:07:39 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 13:34:30 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""], ["M\u00f8ller", "Jesper", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""], ["Soubeyrand", "Samuel", ""]]}, {"id": "1903.12077", "submitter": "Ke Zhu", "authors": "Jiayuan Zhou, Feiyu Jiang, Ke Zhu, Wai Keung Li", "title": "Time series models for realized covariance matrices based on the\n  matrix-F distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Conditional BEKK matrix-F (CBF) model for the time-varying\nrealized covariance (RCOV) matrices. This CBF model is capable of capturing\nheavy-tailed RCOV, which is an important stylized fact but could not be handled\nadequately by the Wishart-based models. To further mimic the long memory\nfeature of the RCOV, a special CBF model with the conditional heterogeneous\nautoregressive (HAR) structure is introduced. Moreover, we give a systematical\nstudy on the probabilistic properties and statistical inferences of the CBF\nmodel, including exploring its stationarity, establishing the asymptotics of\nits maximum likelihood estimator, and giving some new inner-product-based tests\nfor its model checking. In order to handle a large dimensional RCOV matrix, we\nconstruct two reduced CBF models -- the variance-target CBF model (for moderate\nbut fixed dimensional RCOV matrix) and the factor CBF model (for high\ndimensional RCOV matrix). For both reduced models, the asymptotic theory of the\nestimated parameters is derived. The importance of our entire methodology is\nillustrated by simulation results and two real examples.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 05:13:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:02:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhou", "Jiayuan", ""], ["Jiang", "Feiyu", ""], ["Zhu", "Ke", ""], ["Li", "Wai Keung", ""]]}, {"id": "1903.12299", "submitter": "Omid Shams Solari", "authors": "Farzad Pourbabaee and Omid Shams Solari", "title": "Large Deviations of Factor Models with Regularly-Varying Tails:\n  Asymptotics and Efficient Estimation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the \\textit{Large Deviation Probability (LDP)} of linear factor\nmodels generated from non-identically distributed components with\n\\textit{regularly-varying} tails, a large subclass of heavy tailed\ndistributions. An efficient sampling method for LDP estimation of this class is\nintroduced and theoretically shown to exponentially outperform the crude\nMonte-Carlo estimator, in terms of the coverage probability and the confidence\ninterval's length. The theoretical results are empirically validated through\nstochastic simulations on independent non-identically Pareto distributed\nfactors. The proposed estimator is available as part of a more comprehensive\n\\texttt{Betta} package.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 23:26:53 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 18:50:32 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 00:27:36 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pourbabaee", "Farzad", ""], ["Solari", "Omid Shams", ""]]}, {"id": "1903.12414", "submitter": "Angelina Roche", "authors": "Angelina Roche (CEREMADE)", "title": "Variable Selection and Estimation in Multivariate Functional Linear\n  Regression via the LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In more and more applications, a quantity of interest may depend on several\ncovariates, with at least one of them infinite-dimensional (e.g. a curve). To\nselect the relevant covariates in this context, we propose an adaptation of the\nLasso method. Two estimation methods are defined. The first one consists in the\nminimisation of a criterion inspired by classical Lasso inference under group\nsparsity (Yuan and Lin, 2006; Lounici et al., 2011) on the whole multivariate\nfunctional space H. The second one minimises the same criterion but on a\nfinite-dimensional subspace of H which dimension is chosen by a penalized\nleasts-squares method base on the work of Barron et al. (1999). Sparsity-oracle\ninequalities are proven in case of fixed or random design in our\ninfinite-dimensional context. To calculate the solutions of both criteria, we\npropose a coordinate-wise descent algorithm, inspired by the glmnet algorithm\n(Friedman et al., 2007). A numerical study on simulated and experimental\ndatasets illustrates the behavior of the estimators.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 09:28:20 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 11:59:18 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Roche", "Angelina", "", "CEREMADE"]]}, {"id": "1903.12423", "submitter": "Helene Flourent", "authors": "H\\'el\\`ene Flourent (LMBA), Emmanuel Fr\\'enod (LMBA), Vincent\n  Sincholle", "title": "An innovating Statistical Learning Tool based on Partial Differential\n  Equations, intending livestock Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The realistic modeling intended to quantify precisely some biological\nmechanisms is a task requiering a lot of a priori knowledge and generally\nleading to heavy mathematical models. On the other hand, the structure of the\nclassical Machine Learning algorithms, such as Neural Networks, limits their\nflexibility and the possibility to take into account the existence of complex\nunderlying phenomena, such as delay, saturation and accumulation. The aim of\nthis paper is to reach a compromise between precision, parsimony and\nflexibility to design an efficient biomimetic predictive tool extracting\nknowledge from livestock data. To achieve this, we build a Mathematical Model\nbased on Partial Differential Equations (PDE) embarking the mathematical\nexpression of biological determinants. We made the hypothesis that all the\nphysico-chemical phenomena occurring in animal body can be summarized by the\nevolution of a global information. Therefore the developed PDE system describes\nthe evolution and the action of an information circulating in an Avatar of the\nReal Animal. This Avatar outlines the dynamics of the biological reactions of\nanimal body in the framework of a specific problem. Each PDE contains\nparameters corresponding to biological-like factors which can be learnt from\ndata by the developed Statistical Learning Tool.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 09:55:11 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 07:23:15 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 11:03:52 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Flourent", "H\u00e9l\u00e8ne", "", "LMBA"], ["Fr\u00e9nod", "Emmanuel", "", "LMBA"], ["Sincholle", "Vincent", ""]]}, {"id": "1903.12488", "submitter": "Timoth\\'ee Tabouy", "authors": "Mahendra Mariadassou and Timoth\\'ee Tabouy", "title": "Consistency and Asymptotic Normality of Stochastic Block Models\n  Estimators from Sampled Data", "comments": "32 pages, 0 figures. arXiv admin note: text overlap with\n  arXiv:1704.06629", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of network is an active research area and the literature\ncounts a lot of papers concerned with network models and statistical analysis\nof networks. However, very few papers deal with missing data in network\nanalysis and we reckon that, in practice, networks are often observed with\nmissing values. In this paper we focus on the Stochastic Block Model with\nvalued edges and consider a MCAR setting by assuming that every dyad (pair of\nnodes) is sampled identically and independently of the others with probability\n$\\rho > 0$. We prove that maximum likelihood estimators and its variational\napproximations are consistent and asymptotically normal in the presence of\nmissing data as soon as the sampling probability $\\rho$ satisfies\n$\\rho\\gg\\log(n)/n$.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 10:38:33 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 07:40:42 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 15:08:25 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 15:27:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mariadassou", "Mahendra", ""], ["Tabouy", "Timoth\u00e9e", ""]]}]