[{"id": "1412.0313", "submitter": "Chao Gao", "authors": "Chao Gao, Harrison H. Zhou", "title": "Bernstein-von Mises Theorems for Functionals of Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general theoretical framework to derive Bernstein-von Mises\ntheorems for matrix functionals. The conditions on functionals and priors are\nexplicit and easy to check. Results are obtained for various functionals\nincluding entries of covariance matrix, entries of precision matrix, quadratic\nforms, log-determinant, eigenvalues in the Bayesian Gaussian\ncovariance/precision matrix estimation setting, as well as for Bayesian linear\nand quadratic discriminant analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 00:35:24 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Gao", "Chao", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1412.0379", "submitter": "Svenja Fischer", "authors": "Svenja Fischer, Roland Fried and Martin Wendler", "title": "Multivariate Generalized Linear-statistics of short range dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear (GL-) statistics are defined as functionals of an\nU-quantile process and unify different classes of statistics such as\nU-statistics and L-statistics. We derive a central limit theorem for\nGL-statistics of strongly mixing sequences and arbitrary dimension of the\nunderlying kernel. For this purpose we establish a limit theorem for\nU-statistics and an invariance principle for U-processes together with a\nconvergence rate for the remaining term of the Bahadur representation. An\napplication is given by the generalized median estimator for the tail-parameter\nof the Pareto distribution, which is commonly used to model exceedances of high\nthresholds. We use subsampling to calculate confidence intervals and\ninvestigate its behaviour under independence and strong mixing in simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 09:05:19 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Fischer", "Svenja", ""], ["Fried", "Roland", ""], ["Wendler", "Martin", ""]]}, {"id": "1412.0391", "submitter": "Ir\\`ene Gannaz", "authors": "Sophie Achard and Ir\\`ene Gannaz", "title": "Multivariate wavelet Whittle estimation in long-range dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate processes with long-range dependent properties are found in a\nlarge number of applications including finance, geophysics and neuroscience.\nFor real data applications, the correlation between time series is crucial.\nUsual estimations of correlation can be highly biased due to phase-shifts\ncaused by the differences in the properties of autocorrelation in the\nprocesses. To address this issue, we introduce a semiparametric estimation of\nmultivariate long-range dependent processes. The parameters of interest in the\nmodel are the vector of the long-range dependence parameters and the long-run\ncovariance matrix, also called functional connectivity in neuroscience. This\nmatrix characterizes coupling between time series. The proposed multivariate\nwavelet-based Whittle estimation is shown to be consistent for the estimation\nof both the long-range dependence and the covariance matrix and to encompass\nboth stationary and nonstationary processes. A simulation study and a real data\nexample are presented to illustrate the finite sample behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 09:27:59 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 08:49:50 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Achard", "Sophie", ""], ["Gannaz", "Ir\u00e8ne", ""]]}, {"id": "1412.0442", "submitter": "M{\\AA}ns Thulin", "authors": "M{\\AA}ns Thulin, Silvelyn Zwanzig", "title": "Exact confidence intervals and hypothesis tests for parameters of\n  discrete distributions", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ750 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2017, Vol. 23, No. 1, 479-502", "doi": "10.3150/15-BEJ750", "report-no": "IMS-BEJ-BEJ750", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact confidence intervals and two-sided hypothesis tests for\nunivariate parameters of stochastically increasing discrete distributions, such\nas the binomial and Poisson distributions. It is shown that several popular\nmethods for constructing short intervals lack strict nestedness, meaning that\naccepting a lower confidence level not always will lead to a shorter confidence\ninterval. These intervals correspond to a class of tests that are shown to\nassign differing $p$-values to indistinguishable models. Finally, we show that\namong strictly nested intervals, fiducial intervals, including the\nClopper-Pearson interval for a binomial proportion and the Garwood interval for\na Poisson mean, are optimal.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:49:42 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 06:44:50 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 08:09:11 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Thulin", "M\u00c5ns", ""], ["Zwanzig", "Silvelyn", ""]]}, {"id": "1412.0446", "submitter": "Johannes Tewes", "authors": "Olimjon Sharipov, Johannes Tewes, Martin Wendler", "title": "Sequential block bootstrap in a Hilbert space with application to change\n  point analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new test for structural changes in functional data is investigated. It is\nbased on Hilbert space theory and critical values are deduced from bootstrap\niterations. Thus a new functional central limit theorem for the block bootstrap\nin a Hilbert space is required. The test can also be used to detect changes in\nthe marginal distribution of random vectors, which is supplemented by a\nsimulation study. Our methods are applied to hydrological data from Germany.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 12:13:33 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 11:56:52 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Sharipov", "Olimjon", ""], ["Tewes", "Johannes", ""], ["Wendler", "Martin", ""]]}, {"id": "1412.0459", "submitter": "Botond Szabo", "authors": "Botond Szab\\'o", "title": "On Bayesian based adaptive confidence sets for linear functionals", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing Bayesian based confidence sets for\nlinear functionals in the inverse Gaussian white noise model. We work with a\nscale of Gaussian priors indexed by a regularity hyper-parameter and apply the\ndata-driven (slightly modified) marginal likelihood empirical Bayes method for\nthe choice of this hyper-parameter. We show by theory and simulations that the\ncredible sets constructed by this method have sub-optimal behaviour in general.\nHowever, by assuming \"self-similarity\" the credible sets have rate-adaptive\nsize and optimal coverage. As an application of these results we construct\n$L_{\\infty}$-credible bands for the true functional parameter with adaptive\nsize and optimal coverage under self-similarity constraint.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 12:58:16 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 08:32:28 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Szab\u00f3", "Botond", ""]]}, {"id": "1412.0607", "submitter": "Cristian Rojas", "authors": "Cristian R. Rojas and Bo Wahlberg", "title": "How to monitor and mitigate stair-casing in l1 trend filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the estimation of changing trends in time-series using\n$\\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV)\ndenoising for detection of step changes in means to detecting changes in\ntrends, and it relies on a convex optimization problem for which there are very\nefficient numerical algorithms. It is known that TV denoising suffers from the\nso-called stair-case effect, which leads to detecting false change points. The\nobjective of this paper is to show that $\\ell_1$ trend filtering also suffers\nfrom a certain stair-case problem. The analysis is based on an interpretation\nof the dual variables of the optimization problem in the method as integrated\nrandom walk. We discuss consistency conditions for $\\ell_1$ trend filtering,\nhow to monitor their fulfillment, and how to modify the algorithm to avoid the\nstair-case false detection problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:31:51 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1412.0614", "submitter": "Francesco Renna", "authors": "Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves,\n  Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues", "title": "Classification and Reconstruction of High-Dimensional Signals from\n  Low-Dimensional Features in the Presence of Side Information", "comments": "62 pages, 11 figures, submitted to IEEE Transactions on Information\n  Theory. The abstract of the paper is not reported entirely in the metadata\n  due to length limitations", "journal-ref": null, "doi": "10.1109/TIT.2016.2606646", "report-no": null, "categories": "cs.IT cs.CV math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers a characterization of fundamental limits on the\nclassification and reconstruction of high-dimensional signals from\nlow-dimensional features, in the presence of side information. We consider a\nscenario where a decoder has access both to linear features of the signal of\ninterest and to linear features of the side information signal; while the side\ninformation may be in a compressed form, the objective is recovery or\nclassification of the primary signal, not the side information. The signal of\ninterest and the side information are each assumed to have (distinct) latent\ndiscrete labels; conditioned on these two labels, the signal of interest and\nside information are drawn from a multivariate Gaussian distribution. With\njoint probabilities on the latent labels, the overall signal-(side information)\nrepresentation is defined by a Gaussian mixture model. We then provide sharp\nsufficient and/or necessary conditions for these quantities to approach zero\nwhen the covariance matrices of the Gaussians are nearly low-rank. These\nconditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Ziv\nconditions, are a function of the number of linear features extracted from the\nsignal of interest, the number of linear features extracted from the side\ninformation signal, and the geometry of these signals and their interplay.\nMoreover, on assuming that the signal of interest and the side information obey\nsuch an approximately low-rank model, we derive expansions of the\nreconstruction error as a function of the deviation from an exactly low-rank\nmodel; such expansions also allow identification of operational regimes where\nthe impact of side information on signal reconstruction is most relevant. Our\nframework, which offers a principled mechanism to integrate side information in\nhigh-dimensional data problems, is also tested in the context of imaging\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 19:53:25 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 13:35:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Renna", "Francesco", ""], ["Wang", "Liming", ""], ["Yuan", "Xin", ""], ["Yang", "Jianbo", ""], ["Reeves", "Galen", ""], ["Calderbank", "Robert", ""], ["Carin", "Lawrence", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1412.0620", "submitter": "Anil Aswani", "authors": "Anil Aswani", "title": "Low-Rank Approximation and Completion of Positive Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the matrix case, computing low-rank approximations of tensors is\nNP-hard and numerically ill-posed in general. Even the best rank-1\napproximation of a tensor is NP-hard. In this paper, we use convex optimization\nto develop polynomial-time algorithms for low-rank approximation and completion\nof positive tensors. Our approach is to use algebraic topology to define a new\n(numerically well-posed) decomposition for positive tensors, which we show is\nequivalent to the standard tensor decomposition in important cases. Though\ncomputing this decomposition is a nonconvex optimization problem, we prove it\ncan be exactly reformulated as a convex optimization problem. This allows us to\nconstruct polynomial-time randomized algorithms for computing this\ndecomposition and for solving low-rank tensor approximation problems. Among the\nconsequences is that best rank-1 approximations of positive tensors can be\ncomputed in polynomial time. Our framework is next extended to the tensor\ncompletion problem, where noisy entries of a tensor are observed and then used\nto estimate missing entries. We provide a polynomial-time algorithm that for\nspecific cases requires a polynomial (in tensor order) number of measurements,\nin contrast to existing approaches that require an exponential number of\nmeasurements. These algorithms are extended to exploit sparsity in the tensor\nto reduce the number of measurements needed. We conclude by providing a novel\ninterpretation of statistical regression problems with categorical variables as\ntensor completion problems, and numerical examples with synthetic data and data\nfrom a bioengineered metabolic network show the improved performance of our\napproach on this problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 20:06:39 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 17:55:12 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 19:55:33 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2015 08:14:41 GMT"}, {"version": "v5", "created": "Sat, 9 Jul 2016 20:15:45 GMT"}, {"version": "v6", "created": "Tue, 13 Sep 2016 18:25:42 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Aswani", "Anil", ""]]}, {"id": "1412.0705", "submitter": "Mohamed Eliwa", "authors": "M.A. EL-Damcese, Abdelfattah Mustafa and M.S. Eliwa", "title": "Exponentaited generalized Weibull Gompertz distribution", "comments": "16 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces studies on exponentaited generalized Weibull Gompertz\ndistribution EGWGD which generalizes a lot of distributions. Several properties\nof the EGWGD such as reversed (hazard) function, moments, maximum likelihood\nestimation, mean residual (past) lifetime, MTTF, MTTR, MTBF, maintainability,\navailability and order statistics are studied in this paper. A real data set is\nanalyzed and it is observed that the present distribution can provide a better\nfit than some other very well known distributions\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 21:57:10 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 10:28:38 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 23:28:57 GMT"}, {"version": "v4", "created": "Tue, 13 Jan 2015 15:35:15 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["EL-Damcese", "M. A.", ""], ["Mustafa", "Abdelfattah", ""], ["Eliwa", "M. S.", ""]]}, {"id": "1412.0753", "submitter": "Gourab Mukherjee", "authors": "Peter Radchenko, Gourab Mukherjee", "title": "Convex clustering via $\\ell_1$ fusion penalization", "comments": "final journal version with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the large sample behavior of a convex clustering framework, which\nminimizes the sample within cluster sum of squares under an~$\\ell_1$ fusion\nconstraint on the cluster centroids. This recently proposed approach has been\ngaining in popularity, however, its asymptotic properties have remained mostly\nunknown. Our analysis is based on a novel representation of the sample\nclustering procedure as a sequence of cluster splits determined by a sequence\nof maximization problems. We use this representation to provide a simple and\nintuitive formulation for the population clustering procedure. We then\ndemonstrate that the sample procedure consistently estimates its population\nanalog, and derive the corresponding rates of convergence. The proof conducts a\ncareful simultaneous analysis of a collection of M-estimation problems, whose\ncardinality grows together with the sample size. Based on the new perspectives\ngained from the asymptotic investigation, we propose a key post-processing\nmodification of the original clustering framework. We show, both theoretically\nand empirically, that the resulting approach can be successfully used to\nestimate the number of clusters in the population. Using simulated data, we\ncompare the proposed method with existing number of clusters and modality\nassessment approaches, and obtain encouraging results. We also demonstrate the\napplicability of our clustering method for the detection of cellular\nsubpopulations in a single-cell virology study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 01:10:11 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 03:30:15 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2015 01:26:51 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2016 20:46:40 GMT"}, {"version": "v5", "created": "Wed, 28 Dec 2016 01:42:44 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Radchenko", "Peter", ""], ["Mukherjee", "Gourab", ""]]}, {"id": "1412.0836", "submitter": "Chih-Hao Chang", "authors": "Chih-Hao Chang, Hsin-Cheng Huang, Ching-Kang Ing", "title": "Asymptotic theory of generalized information criterion for\n  geostatistical regression model selection", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1258 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2441-2468", "doi": "10.1214/14-AOS1258", "report-no": "IMS-AOS-AOS1258", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information criteria, such as Akaike's information criterion and Bayesian\ninformation criterion are often applied in model selection. However, their\nasymptotic behaviors for selecting geostatistical regression models have not\nbeen well studied, particularly under the fixed domain asymptotic framework\nwith more and more data observed in a bounded fixed region. In this article, we\nstudy the generalized information criterion (GIC) for selecting geostatistical\nregression models under a more general mixed domain asymptotic framework. Via\nuniform convergence developments of some statistics, we establish the selection\nconsistency and the asymptotic loss efficiency of GIC under some regularity\nconditions, regardless of whether the covariance model is correctly or wrongly\nspecified. We further provide specific examples with different types of\nexplanatory variables that satisfy the conditions. For example, in some\nsituations, GIC is selection consistent, even when some spatial covariance\nparameters cannot be estimated consistently. On the other hand, GIC fails to\nselect the true polynomial order consistently under the fixed domain asymptotic\nframework. Moreover, the growth rate of the domain and the degree of smoothness\nof candidate regressors in space are shown to play key roles for model\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 10:10:08 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Chang", "Chih-Hao", ""], ["Huang", "Hsin-Cheng", ""], ["Ing", "Ching-Kang", ""]]}, {"id": "1412.0972", "submitter": "Jacek Wesolowski", "authors": "Helene Massam, Jacek Wesolowski", "title": "A new prior for the discrete DAG models with a restricted set of\n  directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first develop a new family of conjugate prior distributions\nfor the cell parameters of discrete graphical models Markov with respect to a\nset P of moral directed acyclic graphs with skeleton a given decomposable graph\nG. Such families arise when the set of conditional independences between\ndiscrete variables is given and can be represented by a decomposable graph and\nadditionally, the direction of certain edges is imposed by the practitioner.\nThis family, which we call the P-Dirichlet, is a generalization of the hyper\nDirichlet given in Dawid and Lauritzen (1993): it keeps the strong directed\nhyper Markov property for every DAG in P but increases the flexibility in the\nchoice of its parameters, i.e. the hyper parameters. Our second contribution is\na characterization of the P-Dirichlet, which yields, as a corollary, a\ncharacterization of the hyper Dirichlet and a characterization of the Dirichlet\nalso. Like that given by Geiger and Heckerman (1997), our characterization of\nthe Dirichlet is based on local and global independence of the probability\nparameters but we need not make the assumption of the existence of a positive\ndensity function. We use the method of moments for our proofs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 16:45:54 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Massam", "Helene", ""], ["Wesolowski", "Jacek", ""]]}, {"id": "1412.0995", "submitter": "Jacek Wesolowski", "authors": "Jan Kowalski, Jacek Wesolowski", "title": "Exploring recursion for optimal estimators under cascade rotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with optimal linear estimation of means on subsequent\noccasions under sample rotation where evolution of samples in time is designed\nthrough a cascade pattern. It has been known since the seminal paper of\nPatterson (1950) that when the units are not allowed to return to the sample\nafter leaving it for certain period (there are no gaps in the rotation\npattern), one step recursion for optimal estimator holds. However, in some\nimportant real surveys, e.g. Current Population Survey in the US or Labour\nForce Survey in many countries in Europe, units return to the sample after\nbeing absent in the sample for several occasions (there are gaps in rotation\npatterns). In such situations difficulty of the question of the form of the\nrecurrence for optimal estimator increases drastically. This issue has not been\nresolved yet. Instead alternative sub-optimal approaches were developed, as\nK-composite estimation (see e.g. Hansen et al. (1955)), AK-composite estimation\n(see e.g. Gurney and Daly (1965) or time series approach (see e.g. Binder and\nHidiroglou (1988)).\n  In the present paper we overcome this long-standing difficulty, that is, we\npresent analytical recursion formulas for the optimal linear estimator of the\nmean for schemes with gaps in rotation patterns. It is achieved under some\ntechnical conditions: ASSUMPTION I and ASSUMPTION II (numerical experiments\nsuggest that these assumptions might be universally satisfied). To attain the\ngoal we develop an algebraic operator approach which allows to reduce the\nproblem of recursion for the optimal linear estimator to two issues: (1)\nlocalization of roots (possibly complex) of a polynomial Q_p defined in terms\nof the rotation pattern (Q_p happens to be conveniently expressed through\nChebyshev polynomials of the first kind), (2) rank of a matrix S defined in\nterms of the rotation pattern and the roots of the polynomial Q_p.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 17:57:28 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Kowalski", "Jan", ""], ["Wesolowski", "Jacek", ""]]}, {"id": "1412.1411", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen, Hironori Fujisawa, Su-Yun Huang, Chii-Ruey Hwang", "title": "On the Weak Convergence and Central Limit Theorem of Blurring and\n  Nonblurring Processes with Application to Robust Location Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the weak convergence and associated Central Limit\nTheorem for blurring and nonblurring processes. Then, they are applied to the\nestimation of location parameter. Simulation studies show that the location\nestimation based on the convergence point of blurring process is more robust\nand often more efficient than that of nonblurring process.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 17:42:43 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 15:40:52 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 09:01:40 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Chen", "Ting-Li", ""], ["Fujisawa", "Hironori", ""], ["Huang", "Su-Yun", ""], ["Hwang", "Chii-Ruey", ""]]}, {"id": "1412.1414", "submitter": "Matthias De Lozzo", "authors": "Matthias De Lozzo (DER), Amandine Marrel (DER)", "title": "New improvements in the use of dependence measures for sensitivity\n  analysis and screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical phenomena are commonly modeled by numerical simulators. Such codes\ncan take as input a high number of uncertain parameters and it is important to\nidentify their influences via a global sensitivity analysis (GSA). However,\nthese codes can be time consuming which prevents a GSA based on the classical\nSobol' indices, requiring too many simulations. This is especially true as the\nnumber of inputs is important. To address this limitation, we consider recent\nadvances in dependence measures, focusing on the distance correlation and the\nHilbert-Schmidt independence criterion (HSIC). Our objective is to study these\nindices and use them for a screening purpose. Numerical tests reveal some\ndifferences between dependence measures and classical Sobol' indices, and\npreliminary answers to \"What sensitivity indices to what situation?\" are\nderived. Then, two approaches are proposed to use the dependence measures for a\nscreening purpose. The first one directly uses these indices with independence\ntests; asymptotic tests and their spectral extensions exist and are detailed.\nFor a higher accuracy in presence of small samples, we propose a non-asymptotic\nversion based on bootstrap sampling. The second approach is based on a linear\nmodel associating two simulations, which explains their output difference as a\nweighed sum of their input differences. From this, a bootstrap method is\nproposed for the selection of the influential inputs. We also propose a\nheuristic approach for the calibration of the HSIC Lasso method. Numerical\nexperiments are performed and show the potential of these approaches for\nscreening when many inputs are not influential.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 17:48:00 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["De Lozzo", "Matthias", "", "DER"], ["Marrel", "Amandine", "", "DER"]]}, {"id": "1412.1490", "submitter": "Walter Dempsey", "authors": "Walter Dempsey and Peter McCullagh", "title": "The pilgrim process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pilgrim's monopoly is a probabilistic process giving rise to a non-negative\nsequence $T_1, T_2,\\ldots$ that is infinitely exchangeable, a natural model for\ntime-to-event data. The one-dimensional marginal distributions are exponential.\nThe rules are simple, the process is easy to generate sequentially, and a\nsimple expression is available for both the joint density and the multivariate\nsurvivor function. There is a close connection with the Kaplan-Meier estimator\nof the survival distribution. Embedded within the process is an infinitely\nexchangeable ordered partition processes connected to Markov branching\nprocesses in neutral evolutionary theory. Some aspects of the process, such as\nthe distribution of the number of blocks, can be investigated analytically and\nconfirmed by simulation. By ignoring the order, the embedded process can be\nconsidered as an infinitely exchangeable partition process, shown to be closely\nrelated to the Chinese restaurant process. Further connection to the Indian\nbuffet process is also provided. Thus we establish a previously unknown link\nbetween the well-known Kaplan-Meier estimator and the important Ewens sampling\nformula.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 21:05:27 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 19:18:53 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 17:09:35 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Dempsey", "Walter", ""], ["McCullagh", "Peter", ""]]}, {"id": "1412.1530", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Strength of Connections in a Random Graph: Definition, Characterization,\n  and Estimation", "comments": "18 pages, 6 Figures. Third version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can the `affinity' or `strength' of ties of a random graph be\ncharacterized and compactly represented? How can concepts like Fourier and\ninverse-Fourier like transform be developed for graph data? To do so, we\nintroduce a new graph-theoretic function called `Graph Correlation Density\nField' (or in short GraField), which differs from the traditional edge\nprobability density-based approaches, to completely characterize tie-strength\nbetween graph nodes. Our approach further allows frequency domain analysis,\napplicable for both directed and undirected random graphs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 01:08:39 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 00:40:50 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 22:29:27 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1412.1563", "submitter": "Ian McKeague", "authors": "Ian W. McKeague and Bruce Levin", "title": "Convergence of empirical distributions in an interpretation of quantum\n  mechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From its beginning, there have been attempts by physicists to formulate\nquantum mechanics without requiring the use of wave functions. An interesting\nrecent approach takes the point of view that quantum effects arise solely from\nthe interaction of finitely many classical \"worlds.\" The wave function is then\nrecovered (as a secondary object) from observations of particles in these\nworlds, without knowing the world from which any particular observation\noriginates. Hall, Deckert and Wiseman [Physical Review X 4 (2014) 041013] have\nintroduced an explicit many-interacting-worlds harmonic oscillator model to\nprovide support for this approach. In this note we provide a proof of their\nclaim that the particle configuration is asymptotically Gaussian, thus matching\nthe stationary ground-state solution of Schroedinger's equation when the number\nof worlds goes to infinity. We also construct a Markov chain based on\nresampling from the particle configuration and show that it converges to an\nOrnstein-Uhlenbeck process, matching the time-dependent solution as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 06:39:44 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 17:42:53 GMT"}, {"version": "v3", "created": "Mon, 16 Feb 2015 21:31:16 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2015 19:38:22 GMT"}, {"version": "v5", "created": "Tue, 14 Jul 2015 15:53:48 GMT"}, {"version": "v6", "created": "Thu, 29 Oct 2015 21:06:54 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["McKeague", "Ian W.", ""], ["Levin", "Bruce", ""]]}, {"id": "1412.1567", "submitter": "Oliver Lang", "authors": "Mario Huemer, Oliver Lang", "title": "CWCU LMMSE Estimation: Prerequisites and Properties", "comments": "4 pages of content, 1 page with references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical unbiasedness condition utilized e.g. by the best linear\nunbiased estimator (BLUE) is very stringent. By softening the \"global\"\nunbiasedness condition and introducing component-wise conditional unbiasedness\nconditions instead, the number of constraints limiting the estimator's\nperformance can in many cases significantly be reduced. In this work we\ninvestigate the component-wise conditionally unbiased linear minimum mean\nsquare error (CWCU LMMSE) estimator for different model assumptions. The\nprerequisites in general differ from the ones of the LMMSE estimator. We first\nderive the CWCU LMMSE estimator under the jointly Gaussian assumption of the\nmeasurements and the parameters. Then we focus on the linear model and discuss\nthe CWCU LMMSE estimator for jointly Gaussian parameters, and for mutually\nindependent (and otherwise arbitrarily distributed) parameters, respectively.\nIn all these cases the CWCU LMMSE estimator incorporates the prior mean and the\nprior covariance matrix of the parameter vector. For the remaining cases\noptimum linear CWCU estimators exist, but they may correspond to globally\nunbiased estimators that do not make use of prior statistical knowledge about\nthe parameters. Finally, the beneficial properties of the CWCU LMMSE estimator\nare demonstrated with the help of a well-known channel estimation application.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:16:01 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Huemer", "Mario", ""], ["Lang", "Oliver", ""]]}, {"id": "1412.1605", "submitter": "Anatoli Juditsky B.", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "On sequential hypotheses testing via convex optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.6765", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to sequential testing which is an adaptive\n(on-line) extension of the (off-line) framework developed in [10]. It relies\nupon testing of pairs of hypotheses in the case where each hypothesis states\nthat the vector of parameters underlying the dis- tribution of observations\nbelongs to a convex set. The nearly optimal under appropriate conditions test\nis yielded by a solution to an efficiently solvable convex optimization prob-\nlem. The proposed methodology can be seen as a computationally friendly\nreformulation of the classical sequential testing.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 10:11:03 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 14:57:52 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1412.1701", "submitter": "Helmut Rieder F.", "authors": "Helmut Rieder", "title": "One-Sided Confidence About Functionals Over Tangent Cones", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": "Discussion Paper 26, Sonderforschungsbereich 373", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setup of i.i.d.~observations and a real valued differentiable\nfunctional~$T$, locally asymptotic upper bounds are derived for the power of\none-sided tests (simple, versus large values of~$T$)and for the confidence\nprobability of lower confidence limits (for the value of~$T$), in the case that\nthe tangent set is only a convex cone. The bounds, and the tests and estimators\nwhich achieve the bounds, are based on the projection of the influence curve of\nthe functional on the closed convex cone, as opposed to its closed linear span.\nThe higher efficiency comes along with some weaker, only one-sided, regularity\nand stability.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 15:30:46 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Rieder", "Helmut", ""]]}, {"id": "1412.1711", "submitter": "Helmut Rieder F.", "authors": "Helmut Rieder", "title": "Neighborhoods as Nuisance Parameters? Robustness vs. Semiparametrics", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": "Discussion Paper 25, Sonderforschungsbereich 373", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deviations from the center within a robust neighborhood of a parametric model\ndistribution may naturally be considered an infinite dimensional nuisance\nparameter. Thus, the semiparametric method may be tried, which is to compute\nthe scores function for the main parameter minus its orthogonal projection on\nthe closed linear tangent space for the nuisance parameter, and then rescale\nfor Fisher consistency. In this paper, we derive such a semiparametric\ninfluence curve by nonlinear projection on the tangent balls arising in robust\nstatistics.\n  This semiparametric influence curve is then compared with the optimally\nrobust influence curve that minimizes maximum weighted mean square error of the\ncorresponding asymptotically linear estimators over infinitesimal\nneighborhoods. For Hellinger balls, the two coincide (with the classical one).\nIn the total variation model, the semiparametric IC solves the robust MSE\nproblem for a particular bias weight. In the case of contamination\nneighborhoods, the semiparametric IC is bounded only from above. Due to an\ninterchange of truncation and linear combination, the discrepancy increases\nwith the dimension.\n  While there is coincidence for Hellinger balls, at least clipping is achieved\nfor total variation and contamination neighborhoods, but the semiparametric\nmethod in general falls short to solve the minimax MSE estimation problem for\nthe gross error models.\n  The semiparametric approach is carried further to testing contaminated\nhypotheses. In the one-sided case, for testing hypotheses defined by any two\nclosed convex sets of tangents, a saddle point is furnished by projection on\nthe set of differences of these sets. For total variation and contamination\nneighborhoods, we thus recover the robust asymptotic tests based on least\nfavorable pairs. So the two approaches agree in the testing context.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:03:27 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Rieder", "Helmut", ""]]}, {"id": "1412.1716", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry\n  Wasserman", "title": "Nonparametric modal regression", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1373 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 2, 489-514", "doi": "10.1214/15-AOS1373", "report-no": "IMS-AOS-AOS1373", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal regression estimates the local modes of the distribution of $Y$ given\n$X=x$, instead of the mean, as in the usual regression sense, and can hence\nreveal important structure missed by usual regression methods. We study a\nsimple nonparametric method for modal regression, based on a kernel density\nestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic\nerror bounds for this method, and propose techniques for constructing\nconfidence sets and prediction sets. The latter is used to select the smoothing\nbandwidth of the underlying KDE. The idea behind modal regression is connected\nto many others, such as mixture regression and density ridge estimation, and we\ndiscuss these ties as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 16:18:42 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 00:50:10 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 11:54:46 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Tibshirani", "Ryan J.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1412.1926", "submitter": "Fran\\c{c}ois Bachoc", "authors": "Fran\\c{c}ois Bachoc", "title": "Asymptotic analysis of covariance parameter estimation for Gaussian\n  processes in the misspecified case", "comments": "A supplementary material (pdf) is available in the arXiv sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parametric estimation of covariance function of Gaussian processes, it is\noften the case that the true covariance function does not belong to the\nparametric set used for estimation. This situation is called the misspecified\ncase. In this case, it has been shown that, for irregular spatial sampling of\nobservation points, Cross Validation can yield smaller prediction errors than\nMaximum Likelihood. Motivated by this observation, we provide a general\nasymptotic analysis of the misspecified case, for independent and uniformly\ndistributed observation points. We prove that the Maximum Likelihood estimator\nasymptotically minimizes a Kullback-Leibler divergence, within the misspecified\nparametric set, while Cross Validation asymptotically minimizes the integrated\nsquare prediction error. In a Monte Carlo simulation, we show that the\ncovariance parameters estimated by Maximum Likelihood and Cross Validation, and\nthe corresponding Kullback-Leibler divergences and integrated square prediction\nerrors, can be strongly contrasting. On a more technical level, we provide new\nincreasing-domain asymptotic results for independent and uniformly distributed\nobservation points.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 09:14:54 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 14:55:41 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""]]}, {"id": "1412.2129", "submitter": "Diana Cai", "authors": "Diana Cai and Nathanael Ackerman and Cameron Freer", "title": "An iterative step-function estimator for graphons", "comments": "27 pages, 8 figures. Updated and expanded throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable graphs arise via a sampling procedure from measurable functions\nknown as graphons. A natural estimation problem is how well we can recover a\ngraphon given a single graph sampled from it. One general framework for\nestimating a graphon uses step-functions obtained by partitioning the nodes of\nthe graph according to some clustering algorithm. We propose an iterative\nstep-function estimator (ISFE) that, given an initial partition, iteratively\nclusters nodes based on their edge densities with respect to the previous\niteration's partition. We analyze ISFE and demonstrate its performance in\ncomparison with other graphon estimation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 20:59:21 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 00:18:50 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Cai", "Diana", ""], ["Ackerman", "Nathanael", ""], ["Freer", "Cameron", ""]]}, {"id": "1412.2143", "submitter": "Zhengyuan Gao", "authors": "Zhengyuan Gao and Antonio Galvao", "title": "Minimum Integrated Distance Estimation in Simultaneous Equation Models", "comments": "arXiv admin note: text overlap with arXiv:1109.1516 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers estimation and inference in semiparametric econometric\nmodels. Standard procedures estimate the model based on an independence\nrestriction that induces a minimum distance between a joint cumulative\ndistribution function and the product of the marginal cumulative distribution\nfunctions. This paper develops a new estimator which generalizes estimation by\nallowing endogeneity of the weighting measure and estimating the optimal\nmeasure nonparametrically. The optimality corresponds to the minimum of the\nintegrated distance. To accomplish this aim we use Kantorovich's formulation of\nthe optimal transportation problem. The minimizing distance is equivalent to\nthe total variation distance and thus characterizes finer topological\nstructures of the distributions. The estimation also provides greater\ngenerality by dealing with probability measures on compact metric spaces\nwithout assuming existence of densities. Asymptotic statistics of the empirical\nestimates have standard convergent results and are available for different\nstatistical analyses. In addition, we provide a tractable implementation for\ncomputing the estimator in practice.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 21:06:34 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Gao", "Zhengyuan", ""], ["Galvao", "Antonio", ""]]}, {"id": "1412.2314", "submitter": "Bo Waggoner", "authors": "Bo Waggoner", "title": "$\\ell_p$ Testing and Learning of Discrete Distributions", "comments": "This is the full version of the paper appearing at ITCS 2015. Two\n  columns. 24 pages, of which 14 appendix", "journal-ref": null, "doi": "10.1145/2688073.2688095", "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic problems of testing uniformity of and learning a discrete\ndistribution, given access to independent samples from it, are examined under\ngeneral $\\ell_p$ metrics. The intuitions and results often contrast with the\nclassic $\\ell_1$ case. For $p > 1$, we can learn and test with a number of\nsamples that is independent of the support size of the distribution: With an\n$\\ell_p$ tolerance $\\epsilon$, $O(\\max\\{ \\sqrt{1/\\epsilon^q}, 1/\\epsilon^2 \\})$\nsamples suffice for testing uniformity and $O(\\max\\{ 1/\\epsilon^q,\n1/\\epsilon^2\\})$ samples suffice for learning, where $q=p/(p-1)$ is the\nconjugate of $p$. As this parallels the intuition that $O(\\sqrt{n})$ and $O(n)$\nsamples suffice for the $\\ell_1$ case, it seems that $1/\\epsilon^q$ acts as an\nupper bound on the \"apparent\" support size.\n  For some $\\ell_p$ metrics, uniformity testing becomes easier over larger\nsupports: a 6-sided die requires fewer trials to test for fairness than a\n2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,\nthis inverse dependence on support size holds if and only if $p > \\frac{4}{3}$.\nThe uniformity testing algorithm simply thresholds the number of \"collisions\"\nor \"coincidences\" and has an optimal sample complexity up to constant factors\nfor all $1 \\leq p \\leq 2$. Another algorithm gives order-optimal sample\ncomplexity for $\\ell_{\\infty}$ uniformity testing. Meanwhile, the most natural\nlearning algorithm is shown to have order-optimal sample complexity for all\n$\\ell_p$ metrics.\n  The author thanks Cl\\'{e}ment Canonne for discussions and contributions to\nthis work.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 03:57:29 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 17:53:34 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 13:34:20 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2015 17:30:44 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Waggoner", "Bo", ""]]}, {"id": "1412.2557", "submitter": "Jean-Francois Coeurjolly", "authors": "Jean-Fran\\c{c}ois Coeurjolly, Fr\\'ed\\'eric Lavancier (SERPICO)", "title": "Parametric estimation of pairwise Gibbs point processes with infinite\n  range interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with statistical inference for infinite range\ninteraction Gibbs point processes and in particular for the large class of\nRuelle superstable and lower regular pairwise interaction models. We extend\nclassical statistical methodologies such as the pseudolikelihood and the\nlogistic regression methods, originally defined and studied for finite range\nmodels. Then we prove that the associated estimators are strongly consistent\nand satisfy a central limit theorem, provided the pairwise interaction function\ntends sufficiently fast to zero. To this end, we introduce a new central limit\ntheorem for almost conditionally centered triangular arrays of random fields.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 13:53:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 16:16:52 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2015 06:46:39 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", "", "SERPICO"], ["Lavancier", "Fr\u00e9d\u00e9ric", "", "SERPICO"]]}, {"id": "1412.2619", "submitter": "Bertrand Iooss", "authors": "Serge Kucherenko, Bertrand Iooss (GdR MASCOT-NUM)", "title": "Derivative based global sensitivity measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of derivative based global sensitivity measures (DGSM) has\nrecently become popular among practitioners. It has a strong link with the\nMorris screening method and Sobol' sensitivity indices and has several\nadvantages over them. DGSM are very easy to implement and evaluate numerically.\nThe computational time required for numerical evaluation of DGSM is generally\nmuch lower than that for estimation of Sobol' sensitivity indices. This paper\npresents a survey of recent advances in DGSM concerning lower and upper bounds\non the values of Sobol' total sensitivity indices $S\\_{i}^{tot}$. Using these\nbounds it is possible in most cases to get a good practical estimation of the\nvalues of $S\\_{i}^{tot} $. Several examples are used to illustrate an\napplication of DGSM.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:46:52 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 07:09:54 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2015 12:05:32 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Kucherenko", "Serge", "", "GdR MASCOT-NUM"], ["Iooss", "Bertrand", "", "GdR MASCOT-NUM"]]}, {"id": "1412.2632", "submitter": "Joseph Salmon", "authors": "Jean Lafond (LTCI), Olga Klopp (MODAL'X, CREST-INSEE), Eric Moulines\n  (LTCI), Jospeh Salmon (LTCI)", "title": "Probabilistic low-rank matrix completion on finite alphabets", "comments": "arXiv admin note: text overlap with arXiv:1408.6218", "journal-ref": "NIPS, Dec 2014, Montreal, Canada", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of reconstructing a matrix given a sample of observedentries is\nknown as the matrix completion problem. It arises ina wide range of problems,\nincluding recommender systems, collaborativefiltering, dimensionality\nreduction, image processing, quantum physics or multi-class classificationto\nname a few. Most works have focused on recovering an unknown real-valued\nlow-rankmatrix from randomly sub-sampling its entries.Here, we investigate the\ncase where the observations take a finite number of values, corresponding for\nexamples to ratings in recommender systems or labels in multi-class\nclassification.We also consider a general sampling scheme (not necessarily\nuniform) over the matrix entries.The performance of a nuclear-norm penalized\nestimator is analyzed theoretically.More precisely, we derive bounds for the\nKullback-Leibler divergence between the true and estimated distributions.In\npractice, we have also proposed an efficient algorithm based on lifted\ncoordinate gradient descent in order to tacklepotentially high dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 15:57:40 GMT"}], "update_date": "2014-12-20", "authors_parsed": [["Lafond", "Jean", "", "LTCI"], ["Klopp", "Olga", "", "MODAL'X, CREST-INSEE"], ["Moulines", "Eric", "", "LTCI"], ["Salmon", "Jospeh", "", "LTCI"]]}, {"id": "1412.2817", "submitter": "Mohammad Reza Gholami", "authors": "Mohammad Reza Gholami, Magnus Jansson, Erik G. Str\\\"om, and Ali H.\n  Sayed", "title": "Diffusion Estimation Over Cooperative Multi-Agent Networks With Missing\n  Data", "comments": "To appear in IEEE Transactions on Signal and Information Processing\n  Over Networks", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2570679", "report-no": null, "categories": "math.ST cs.SY stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields, and especially in the medical and social sciences and in\nrecommender systems, data are gathered through clinical studies or targeted\nsurveys. Participants are generally reluctant to respond to all questions in a\nsurvey or they may lack information to respond adequately to some questions.\nThe data collected from these studies tend to lead to linear regression models\nwhere the regression vectors are only known partially: some of their entries\nare either missing completely or replaced randomly by noisy values. In this\nwork, assuming missing positions are replaced by noisy values, we examine how a\nconnected network of agents, with each one of them subjected to a stream of\ndata with incomplete regression information, can cooperate with each other\nthrough local interactions to estimate the underlying model parameters in the\npresence of missing data. We explain how to adjust the distributed diffusion\nthrough (de)regularization in order to eliminate the bias introduced by the\nincomplete model. We also propose a technique to recursively estimate the\n(de)regularization parameter and examine the performance of the resulting\nstrategy. We illustrate the results by considering two applications: one\ndealing with a mental health survey and the other dealing with a household\nconsumption survey.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 00:11:44 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 19:55:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Gholami", "Mohammad Reza", ""], ["Jansson", "Magnus", ""], ["Str\u00f6m", "Erik G.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1412.2821", "submitter": "Xiu-Li Wang", "authors": "Xiuli Wang", "title": "Zipf's Law and the Frequency of Characters or Words of Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article discusses the frequency of characters of Oracle,concluding that\nthe frequency and the rank of a word or character is fit to Zipf-Mandelboit Law\nor Zipf's law with three parameters,and figuring out the parameters based on\nthe frequency,and pointing out that what some researchers of Oracle call the\nassembling on the two ends is just a description by their impression about the\nOracle data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 00:39:16 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Wang", "Xiuli", ""]]}, {"id": "1412.2984", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (LM-Orsay,(M\\'ethodes d'Analyse Stochastique des Codes\n  et Traitements Num\\'eriques)), Ma\\\"elle Nodet (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann), Christophe Prieur, Cl\\'ementine Prieur\n  ((M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques),\n  INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann)", "title": "Global sensitivity analysis for the boundary control of an open channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to solve the global sensitivity analysis for a\nparticular control problem. More precisely, the boundary control problem of an\nopen-water channel is considered, where the boundary conditions are defined by\nthe position of a down stream overflow gate and an upper stream underflow gate.\nThe dynamics of the water depth and of the water velocity are described by the\nShallow Water equations, taking into account the bottom and friction slopes.\nSince some physical parameters are unknown, a stabilizing boundary control is\nfirst computed for their nominal values, and then a sensitivity anal-ysis is\nperformed to measure the impact of the uncertainty in the parameters on a given\nto-be-controlled output. The unknown physical parameters are de-scribed by some\nprobability distribution functions. Numerical simulations are performed to\nmeasure the first-order and total sensitivity indices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:57:04 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Janon", "Alexandre", "", "LM-Orsay,"], ["Nodet", "Ma\u00eblle", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"], ["Prieur", "Christophe", ""], ["Prieur", "Cl\u00e9mentine", ""]]}, {"id": "1412.3092", "submitter": "Daniel Waltner", "authors": "Daniel Waltner, Tim Wirtz, Thomas Guhr", "title": "Eigenvalue Density of the Doubly Correlated Wishart Model: Exact Results", "comments": "16 pages, 3 figures", "journal-ref": "J. Phys. A: Math. Theor. 48 (2015) 175204", "doi": "10.1088/1751-8113/48/17/175204", "report-no": null, "categories": "math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sets collected at different times and different observing points can\npossess correlations at different times $and$ at different positions. The\ndoubly correlated Wishart model takes both into account. We calculate the\neigenvalue density of the Wishart correlation matrices using supersymmetry. In\nthe complex case we obtain a new closed form expression which we compare to\nprevious results in the literature. In the more relevant and much more\ncomplicated real case we derive an expression for the density in terms of a\nfourfold integral. Finally, we calculate the density in the limit of large\ncorrelation matrices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 20:38:58 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Waltner", "Daniel", ""], ["Wirtz", "Tim", ""], ["Guhr", "Thomas", ""]]}, {"id": "1412.3442", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Patrick Rubin-Delanchy, Daniel John Lawson", "title": "Posterior predictive p-values and the convex order", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior predictive p-values are a common approach to Bayesian\nmodel-checking. This article analyses their frequency behaviour, that is, their\ndistribution when the parameters and the data are drawn from the prior and the\nmodel respectively. We show that the family of possible distributions is\nexactly described as the distributions that are less variable than uniform on\n[0,1], in the convex order. In general, p-values with such a property are not\nconservative, and we illustrate how the theoretical worst-case error rate for\nfalse rejection can occur in practice. We describe how to correct the p-values\nto recover conservatism in several common scenarios, for example, when\ninterpreting a single p-value or when combining multiple p-values into an\noverall score of significance. We also handle the case where the p-value is\nestimated from posterior samples obtained from techniques such as Markov Chain\nor Sequential Monte Carlo. Our results place posterior predictive p-values in a\nmuch clearer theoretical framework, allowing them to be used with more\nassurance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 20:39:58 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 18:16:00 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2015 15:14:34 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Rubin-Delanchy", "Patrick", ""], ["Lawson", "Daniel John", ""]]}, {"id": "1412.3661", "submitter": "Denis Chetverikov", "authors": "Victor Chernozhukov, Denis Chetverikov, and Kengo Kato", "title": "Central Limit Theorems and Bootstrap in High Dimensions", "comments": "43 pages; minor revision of the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives central limit and bootstrap theorems for probabilities\nthat sums of centered high-dimensional random vectors hit hyperrectangles and\nsparsely convex sets. Specifically, we derive Gaussian and bootstrap\napproximations for probabilities $\\Pr(n^{-1/2}\\sum_{i=1}^n X_i\\in A)$ where\n$X_1,\\dots,X_n$ are independent random vectors in $\\mathbb{R}^p$ and $A$ is a\nhyperrectangle, or, more generally, a sparsely convex set, and show that the\napproximation error converges to zero even if $p=p_n\\to \\infty$ as $n \\to\n\\infty$ and $p \\gg n$; in particular, $p$ can be as large as $O(e^{Cn^c})$ for\nsome constants $c,C>0$. The result holds uniformly over all hyperrectangles, or\nmore generally, sparsely convex sets, and does not require any restriction on\nthe correlation structure among coordinates of $X_i$. Sparsely convex sets are\nsets that can be represented as intersections of many convex sets whose\nindicator functions depend only on a small subset of their arguments, with\nhyperrectangles being a special case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 14:45:47 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 06:26:39 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2015 19:10:43 GMT"}, {"version": "v4", "created": "Tue, 8 Mar 2016 17:33:48 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Kato", "Kengo", ""]]}, {"id": "1412.3730", "submitter": "Thijs van Ommen", "authors": "Peter Gr\\\"unwald and Thijs van Ommen", "title": "Inconsistency of Bayesian Inference for Misspecified Linear Models, and\n  a Proposal for Repairing It", "comments": "70 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We empirically show that Bayesian inference can be inconsistent under\nmisspecification in simple linear regression problems, both in a model\naveraging/selection and in a Bayesian ridge regression setting. We use the\nstandard linear model, which assumes homoskedasticity, whereas the data are\nheteroskedastic, and observe that the posterior puts its mass on ever more\nhigh-dimensional models as the sample size increases. To remedy the problem, we\nequip the likelihood in Bayes' theorem with an exponent called the learning\nrate, and we propose the Safe Bayesian method to learn the learning rate from\nthe data. SafeBayes tends to select small learning rates as soon the standard\nposterior is not `cumulatively concentrated', and its results on our data are\nquite encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 17:16:06 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 16:23:39 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 13:14:47 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["van Ommen", "Thijs", ""]]}, {"id": "1412.3731", "submitter": "Yong Sheng Soh", "authors": "Yong Sheng Soh and Venkat Chandrasekaran", "title": "High-Dimensional Change-Point Estimation: Combining Filtering with\n  Convex Optimization", "comments": "27 pages, 4 figures, minor typo in Theorem 3.1 corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider change-point estimation in a sequence of high-dimensional signals\ngiven noisy observations. Classical approaches to this problem such as the\nfiltered derivative method are useful for sequences of scalar-valued signals,\nbut they have undesirable scaling behavior in the high-dimensional setting.\nHowever, many high-dimensional signals encountered in practice frequently\npossess latent low-dimensional structure. Motivated by this observation, we\npropose a technique for high-dimensional change-point estimation that combines\nthe filtered derivative approach from previous work with convex optimization\nmethods based on atomic norm regularization, which are useful for exploiting\nstructure in high-dimensional data. Our algorithm is applicable in online\nsettings as it operates on small portions of the sequence of observations at a\ntime, and it is well-suited to the high-dimensional setting both in terms of\ncomputational scalability and of statistical efficiency. The main result of\nthis paper shows that our method performs change-point estimation reliably as\nlong as the product of the smallest-sized change (the Euclidean-norm-squared of\nthe difference between signals at a change-point) and the smallest distance\nbetween change-points (number of time instances) is larger than a Gaussian\nwidth parameter that characterizes the low-dimensional complexity of the\nunderlying signal sequence.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 17:18:06 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 00:50:36 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Soh", "Yong Sheng", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1412.3972", "submitter": "Isabel Fraga Alves", "authors": "Isabel Fraga Alves, Cl\\'audia Neves and Pedro Ros\\'ario", "title": "A general estimator for the right endpoint - with an application to\n  supercentenarian women's records", "comments": "Another version published in Extremes Journal", "journal-ref": null, "doi": "10.1007/s10687-016-0260-6", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the setting of the right endpoint estimator introduced in Fraga\nAlves and Neves (Statist. Sinica 24:1811--1835, 2014) to the broader class of\nlight-tailed distributions with finite endpoint, belonging to some domain of\nattraction induced by the extreme value theorem. This stretch enables a general\nestimator for the finite endpoint, which does not require estimation of the\n(supposedly non-positive) extreme value index. A new testing procedure for\nselecting max-domains of attraction also arises in connection with asymptotic\nproperties of the general endpoint estimator. The simulation study conveys that\nthe general endpoint estimator is a valuable complement to the most usual\nendpoint estimators, particularly when the true extreme value index stays above\n$-1/2$, embracing the most common cases in practical applications. An\nillustration is provided via an extreme value analysis of supercentenarian\nwomen data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 12:44:10 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 17:28:11 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 20:14:29 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Alves", "Isabel Fraga", ""], ["Neves", "Cl\u00e1udia", ""], ["Ros\u00e1rio", "Pedro", ""]]}, {"id": "1412.4170", "submitter": "Ritwik Mitra", "authors": "Ritwik Mitra and Cun-Hui Zhang", "title": "The Benefit of Group Sparsity in Group Inference with De-biased Scaled\n  Group Lasso", "comments": "39 Pages, 2 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study confidence regions and approximate chi-squared tests for variable\ngroups in high-dimensional linear regression. When the size of the group is\nsmall, low-dimensional projection estimators for individual coefficients can be\ndirectly used to construct efficient confidence regions and p-values for the\ngroup. However, the existing analyses of low-dimensional projection estimators\ndo not directly carry through for chi-squared-based inference of a large group\nof variables without inflating the sample size by a factor of the group size.\nWe propose to de-bias a scaled group Lasso for chi-squared-based statistical\ninference for potentially very large groups of variables. We prove that the\nproposed methods capture the benefit of group sparsity under proper conditions,\nfor statistical inference of the noise level and variable groups, large and\nsmall. Such benefit is especially strong when the group size is large.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 00:41:22 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 00:48:11 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Mitra", "Ritwik", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1412.4182", "submitter": "Stefan Wager", "authors": "Jacob Steinhardt, Stefan Wager, and Percy Liang", "title": "The Statistics of Streaming Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse analogue to stochastic gradient descent that is\nguaranteed to perform well under similar conditions to the lasso. In the linear\nregression setup with irrepresentable noise features, our algorithm recovers\nthe support set of the optimal parameter vector with high probability, and\nachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),\nwhere k is the sparsity of the solution, d is the number of features, and T is\nthe number of training examples. Meanwhile, our algorithm does not require any\nmore computational resources than stochastic gradient descent. In our\nexperiments, we find that our method substantially out-performs existing\nstreaming algorithms on both real and simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:32:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Wager", "Stefan", ""], ["Liang", "Percy", ""]]}, {"id": "1412.4222", "submitter": "Jairo Cugliari", "authors": "Anestis Antoniadis (LJK), Xavier Brossat, Jairo Cugliari (ERIC),\n  Jean-Michel Poggi (LM-Orsay, INRIA Saclay - Ile de France)", "title": "A prediction interval for a function-valued forecast model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the information contained in the shape of the load curves, we\nhave proposed a flexible nonparametric function-valued fore-cast model called\nKWF (Kernel+Wavelet+Functional) well suited to handle nonstationary series. The\npredictor can be seen as a weighted average of futures of past situations,\nwhere the weights increase with the similarity between the past situations and\nthe actual one. In addi-tion, this strategy provides with a simultaneous\nmultiple horizon pre-diction. These weights induce a probability distribution\nthat can be used to produce bootstrap pseudo predictions. Prediction intervals\nare constructed after obtaining the corresponding bootstrap pseudo pre-diction\nresiduals. We develop two propositions following directly the KWF strategy and\ncompare it to two alternative ways coming from proposals of econometricians.\nThey construct simultaneous prediction intervals using multiple comparison\ncorrections through the control of the family wise error (FWE) or the false\ndiscovery rate. Alternatively, such prediction intervals can be constructed\nbootstrapping joint prob-ability regions. In this work we propose to obtain\nprediction intervals for the KWF model that are simultaneously valid for the H\npredic-tion horizons that corresponds with the corresponding path forecast,\nmaking a connection between functional time series and the econome-tricians'\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 10:31:31 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Antoniadis", "Anestis", "", "LJK"], ["Brossat", "Xavier", "", "ERIC"], ["Cugliari", "Jairo", "", "ERIC"], ["Poggi", "Jean-Michel", "", "LM-Orsay, INRIA Saclay - Ile de France"]]}, {"id": "1412.4451", "submitter": "John Duchi", "authors": "Rina Foygel Barber and John C. Duchi", "title": "Privacy and Statistical Risk: Formalisms and Minimax Bounds", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore and compare a variety of definitions for privacy and disclosure\nlimitation in statistical estimation and data analysis, including (approximate)\ndifferential privacy, testing-based definitions of privacy, and posterior\nguarantees on disclosure risk. We give equivalence results between the\ndefinitions, shedding light on the relationships between different formalisms\nfor privacy. We also take an inferential perspective, where---building off of\nthese definitions---we provide minimax risk bounds for several estimation\nproblems, including mean estimation, estimation of the support of a\ndistribution, and nonparametric density estimation. These bounds highlight the\nstatistical consequences of different definitions of privacy and provide a\nsecond lens for evaluating the advantages and disadvantages of different\ntechniques for disclosure limitation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 03:13:15 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Duchi", "John C.", ""]]}, {"id": "1412.4605", "submitter": "Benedikt M. P\\\"otscher", "authors": "Fran\\c{c}ois Bachoc, Hannes Leeb, and Benedikt M. P\\\"otscher", "title": "Valid confidence intervals for post-model-selection predictors", "comments": "Some material added. Some restructuring of the paper. Some minor\n  errors corrected", "journal-ref": "Annals of Statistics 47 (2019), 1475-1504", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference post-model-selection in linear regression. In this\nsetting, Berk et al.(2013) recently introduced a class of confidence sets, the\nso-called PoSI intervals, that cover a certain non-standard quantity of\ninterest with a user-specified minimal coverage probability, irrespective of\nthe model selection procedure that is being used. In this paper, we generalize\nthe PoSI intervals to post-model-selection predictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 14:19:05 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 14:36:34 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 12:01:41 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Leeb", "Hannes", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1412.4736", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Inductive Bias of Dropout", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16, 3403-3454 (2015). (See\n  http://jmlr.org/papers/volume16/helmbold15a/helmbold15a.pdf.)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a simple but effective technique for learning in neural networks\nand other settings. A sound theoretical understanding of dropout is needed to\ndetermine when dropout should be applied and how to use it most effectively. In\nthis paper we continue the exploration of dropout as a regularizer pioneered by\nWager, et.al. We focus on linear classification where a convex proxy to the\nmisclassification loss (i.e. the logistic loss used in logistic regression) is\nminimized. We show: (a) when the dropout-regularized criterion has a unique\nminimizer, (b) when the dropout-regularization penalty goes to infinity with\nthe weights, and when it remains bounded, (c) that the dropout regularization\ncan be non-monotonic as individual weights increase from 0, and (d) that the\ndropout regularization penalty may not be convex. This last point is\nparticularly surprising because the combination of dropout regularization with\nany convex loss proxy is always a convex function.\n  In order to contrast dropout regularization with $L_2$ regularization, we\nformalize the notion of when different sources are more compatible with\ndifferent regularizers. We then exhibit distributions that are provably more\ncompatible with dropout regularization than $L_2$ regularization, and vice\nversa. These sources provide additional insight into how the inductive biases\nof dropout and $L_2$ regularization differ. We provide some similar results for\n$L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:40:46 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 02:58:50 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:22:30 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 18:59:20 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1412.4857", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "A goodness-of-fit test for stochastic block models", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1370 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 401-424", "doi": "10.1214/15-AOS1370", "report-no": "IMS-AOS-AOS1370", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is a popular tool for studying community\nstructures in network data. We develop a goodness-of-fit test for the\nstochastic block model. The test statistic is based on the largest singular\nvalue of a residual matrix obtained by subtracting the estimated block mean\neffect from the adjacency matrix. Asymptotic null distribution is obtained\nusing recent advances in random matrix theory. The test is proved to have full\npower against alternative models with finer structures. These results naturally\nlead to a consistent sequential testing estimate of the number of communities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 02:07:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 14:30:13 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1412.4870", "submitter": "Sirin Nitinawarat", "authors": "Sirin Nitinawarat and Venugopal V. Veeravalli", "title": "Universal Scheme for Optimal Search and Stop", "comments": "Submitted for journal publication, December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of universal search and stop using an adaptive search policy is\nconsidered. When the target location is searched, the observation is\ndistributed according to the target distribution, otherwise it is distributed\naccording to the absence distribution. A universal sequential scheme for search\nand stop is proposed using only the knowledge of the absence distribution, and\nits asymptotic performance is analyzed. The universal test is shown to yield a\nvanishing error probability, and to achieve the optimal reliability when the\ntarget is present, universally for every target distribution. Consequently, it\nis established that the knowledge of the target distribution is only useful for\nimproving the reliability for detecting a missing target. It is also shown that\na multiplicative gain for the search reliability equal to the number of\nsearched locations is achieved by allowing adaptivity in the search.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 03:49:16 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Nitinawarat", "Sirin", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1412.4912", "submitter": "Sylvain Le Corff", "authors": "Rainer Dahlhaus, Thierry Dumont (MODAL'X), Sylvain Le Corff\n  (LM-Orsay), Jan C. Neddermeyer", "title": "Statistical Inference for Oscillation Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model for time series with a specific oscillation pattern is proposed.\nThe model consists of a hidden phase process controlling the speed of polling\nand a nonparametric curve characterizing the pattern, leading together to a\ngeneralized state space model. Identifiability of the model is proved and a\nmethod for statistical inference based on a particle smoother and a\nnonparametric EM algorithm is developed. In particular, the oscillation pattern\nand the unobserved phase process are estimated. The proposed algorithms are\ncomputationally efficient and their performance is assessed through simulations\nand an application to human electrocardiogram recordings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 08:19:01 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 09:07:48 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 08:37:56 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Dahlhaus", "Rainer", "", "MODAL'X"], ["Dumont", "Thierry", "", "MODAL'X"], ["Corff", "Sylvain Le", "", "LM-Orsay"], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1412.5059", "submitter": "Hai Shu", "authors": "Hai Shu and Bin Nan", "title": "Estimation of Large Covariance and Precision Matrices from Temporally\n  Dependent Observations", "comments": "The result for banding estimator of covariance matrix is given in the\n  version 2 of this article. See arXiv:1412.5059v2", "journal-ref": "The Annals of Statistics, 2019, 47(3): 1321-1350", "doi": "10.1214/18-AOS1716", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of large covariance and precision matrices from\nhigh-dimensional sub-Gaussian or heavier-tailed observations with slowly\ndecaying temporal dependence. The temporal dependence is allowed to be\nlong-range so with longer memory than those considered in the current\nliterature. We show that several commonly used methods for independent\nobservations can be applied to the temporally dependent data. In particular,\nthe rates of convergence are obtained for the generalized thresholding\nestimation of covariance and correlation matrices, and for the constrained\n$\\ell_1$ minimization and the $\\ell_1$ penalized likelihood estimation of\nprecision matrix. Properties of sparsistency and sign-consistency are also\nestablished. A gap-block cross-validation method is proposed for the tuning\nparameter selection, which performs well in simulations. As a motivating\nexample, we study the brain functional connectivity using resting-state fMRI\ntime series data with long-range temporal dependence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 16:05:07 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2015 02:29:34 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 20:04:52 GMT"}, {"version": "v4", "created": "Tue, 15 Mar 2016 03:36:05 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 17:51:41 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Shu", "Hai", ""], ["Nan", "Bin", ""]]}, {"id": "1412.5136", "submitter": "Delphine Blanke", "authors": "Mohammed El Asri, Delphine Blanke and Edith Gabriel", "title": "Weighted M-estimators for multivariate clustered data: theory and\n  simulation results", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study weighted M-estimators for $\\mathbb{R}^d$-valued clustered data and\ngive sufficient conditions for their consistency. Their asymptotic normality is\nestablished with estimation of the asymptotic covariance matrix. We address the\nrobustness of these estimators in terms of their breakdown point. Comparison\nwith the unweighted case is performed with some numerical studies. They\nhighlight that optimal weights maximizing the relative efficiency have a bad\nimpact on the breakdown point.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 19:41:27 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 09:00:55 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Asri", "Mohammed El", ""], ["Blanke", "Delphine", ""], ["Gabriel", "Edith", ""]]}, {"id": "1412.5139", "submitter": "Ryan Martin", "authors": "Ryan Martin, Huiping Xu, Zuoyi Zhang, Chuanhai Liu", "title": "Valid uncertainty quantification about the model in a linear regression\n  setting", "comments": "24 pages, 4 figures, 2 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific applications, there often are several competing models that\ncould be fit to the observed data, so quantification of the model uncertainty\nis of fundamental importance. In this paper, we develop an inferential model\n(IM) approach for simultaneously valid probabilistic inference over a\ncollection of assertions of interest without requiring any prior input. Our\nconstruction guarantees that the approach is optimal in the sense that it is\nthe most efficient among those which are valid. Connections between the IM's\nsimultaneous validity and post-selection inference are also made. We apply the\ngeneral results to obtain valid uncertainty quantification about the set of\npredictor variables to be included in a linear regression model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 19:53:35 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 18:59:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Martin", "Ryan", ""], ["Xu", "Huiping", ""], ["Zhang", "Zuoyi", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1412.5158", "submitter": "Han Liu", "authors": "Ethan X. Fang, Yang Ning, Han Liu", "title": "Testing and Confidence Intervals for High Dimensional Proportional\n  Hazards Model", "comments": "42 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decorrelation-based approach to test hypotheses and\nconstruct confidence intervals for the low dimensional component of high\ndimensional proportional hazards models. Motivated by the geometric projection\nprinciple, we propose new decorrelated score, Wald and partial likelihood ratio\nstatistics. Without assuming model selection consistency, we prove the\nasymptotic normality of these test statistics, establish their semiparametric\noptimality. We also develop new procedures for constructing pointwise\nconfidence intervals for the baseline hazard function and baseline survival\nfunction. Thorough numerical results are provided to back up our theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 20:36:26 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Fang", "Ethan X.", ""], ["Ning", "Yang", ""], ["Liu", "Han", ""]]}, {"id": "1412.5262", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Rheanna Mainzer and Davide Farchione", "title": "The impact of a Hausman pretest, applied to panel data, on the coverage\n  probability of confidence intervals", "comments": "The exposition has been improved", "journal-ref": "The impact of a Hausman pretest, applied to panel data, on the\n  coverage probability of confidence intervals. Economics Letters, 131, 12-15\n  (2015)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of panel data that includes a time-varying covariate, a\nHausman pretest is commonly used to decide whether subsequent inference is made\nusing the random effects model or the fixed effects model. We consider the\neffect of this pretest on the coverage probability of a confidence interval for\nthe slope parameter. We prove three new finite sample theorems that make it\neasy to assess, for a wide variety of circumstances, the effect of the Hausman\npretest on the minimum coverage probability of this confidence interval. Our\nresults show that for the small levels of significance of the Hausman pretest\ncommonly used in applications, the minimum coverage probability of the\nconfidence interval for the slope parameter can be far below nominal.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 05:50:49 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 05:02:51 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""], ["Farchione", "Davide", ""]]}, {"id": "1412.5632", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh and Martin J. Wainwright", "title": "Support recovery without incoherence: A case for nonconvex\n  regularization", "comments": "51 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the primal-dual witness proof method may be used to\nestablish variable selection consistency and $\\ell_\\infty$-bounds for sparse\nregression problems, even when the loss function and/or regularizer are\nnonconvex. Using this method, we derive two theorems concerning support\nrecovery and $\\ell_\\infty$-guarantees for the regression estimator in a general\nsetting. Our results provide rigorous theoretical justification for the use of\nnonconvex regularization: For certain nonconvex regularizers with vanishing\nderivative away from the origin, support recovery consistency may be guaranteed\nwithout requiring the typical incoherence conditions present in $\\ell_1$-based\nmethods. We then derive several corollaries that illustrate the wide\napplicability of our method to analyzing composite objective functions\ninvolving losses such as least squares, nonconvex modified least squares for\nerrors-in variables linear regression, the negative log likelihood for\ngeneralized linear models, and the graphical Lasso. We conclude with empirical\nstudies to corroborate our theoretical predictions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 21:39:35 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1412.5663", "submitter": "Pasquale Erto", "authors": "Pasquale Erto and Antonio Lepore", "title": "Plotting positions close to the exact unbiased solution: application to\n  the Pozzuoli's bradeysism earthquake data", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical techniques are recommended for critical applications in order to\nshare information with non-statisticians, since they allow for a visual\nanalysis and helpful understanding of the results. However, graphical\nestimation methods are often underestimated because of their minor efficiency\nwith respect to the analytical ones. Therefore, finding unbiased plotting\npositions can contribute to rise their reputation and to encourage their\nstrategic use. This paper proposes a new general plotting position formula\nwhich can be as close as needed to the exact unbiased plotting positions. The\nability of the new solution in estimating quantiles for both symmetrical and\nskewed location-scale distributions is shown via Monte Carlo simulation. An\napplicative example shows how the proposed formula enables to perform, with\nknown accuracy, the graphical analysis of critical data, such as the earthquake\nmagnitudes registered during the serious 1983-1984 bradyseismic crisis in Campi\nFlegrei (Italy). Moreover, the proposed formula gives a unified look at\nexisting plotting positions and a definitive insight into plotting position\ncontroversies recently renewed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:48:26 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Erto", "Pasquale", ""], ["Lepore", "Antonio", ""]]}, {"id": "1412.5816", "submitter": "Tapio Helin", "authors": "Tapio Helin and Martin Burger", "title": "Maximum a posteriori probability estimates in infinite-dimensional\n  Bayesian inverse problems", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A demanding challenge in Bayesian inversion is to efficiently characterize\nthe posterior distribution. This task is problematic especially in\nhigh-dimensional non-Gaussian problems, where the structure of the posterior\ncan be very chaotic and difficult to analyse. Current inverse problem\nliterature often approaches the problem by considering suitable point\nestimators for the task. Typically the choice is made between the maximum a\nposteriori (MAP) or the conditional mean (CM) estimate.\n  The benefits of either choice are not well-understood from the perspective of\ninfinite-dimensional theory. Most importantly, there exists no general scheme\nregarding how to connect the topological description of a MAP estimate to a\nvariational problem. The results by Dashti et. al. (2013) resolve this issue\nfor non-linear inverse problems in Gaussian framework. In this work we improve\nthe current understanding by introducing a novel concept called the weak MAP\n(wMAP) estimate. We show that any MAP estimate in the sense of Dashti et. al.\n(2013) is a wMAP estimate and, moreover, how in general infinite-dimensional\nnon-Gaussian problems the wMAP estimate connects to a variational formulation.\nSuch a formulation yields many properties of the estimate that were earlier\nimpossible to study.\n  In a recent work by Burger and Lucka (2014) the MAP estimator was studied in\nthe context of Bayes cost method. Using Bregman distances, proper convex Bayes\ncost functions were introduced for which the MAP estimator is the Bayes\nestimator. Here, we generalize these results to the infinite-dimensional\nsetting. Moreover, we discuss the implications of our results for some examples\nof prior models such as the Besov prior and hierarchical prior.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 11:32:36 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 21:53:17 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2015 21:22:29 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Helin", "Tapio", ""], ["Burger", "Martin", ""]]}, {"id": "1412.5844", "submitter": "Housen Li", "authors": "Housen Li, Axel Munk, and Hannes Sieling", "title": "FDR-Control in Multiscale Change-point Segmentation", "comments": "An R-package \"FDRSeg\" is available at\n  http://www.stochastik.math.uni-goettingen.de/fdrs", "journal-ref": "Electron. J. Stat. 10 (2016) 918-959", "doi": "10.1214/16-EJS1131", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast multiple change-point segmentation methods, which additionally provide\nfaithful statistical statements on the number, locations and sizes of the\nsegments, have recently received great attention. In this paper, we propose a\nmultiscale segmentation method, FDRSeg, which controls the false discovery rate\n(FDR) in the sense that the number of false jumps is bounded linearly by the\nnumber of true jumps. In this way, it adapts the detection power to the number\nof true jumps. We prove a non-asymptotic upper bound for its FDR in a Gaussian\nsetting, which allows to calibrate the only parameter of FDRSeg properly.\nChange-point locations, as well as the signal, are shown to be estimated in a\nuniform sense at optimal minimax convergence rates up to a log-factor. The\nlatter is w.r.t. $L^p$-risk, $p \\ge 1$, over classes of step functions with\nbounded jump sizes and either bounded, or possibly increasing, number of\nchange-points. FDRSeg can be efficiently computed by an accelerated dynamic\nprogram; its computational complexity is shown to be linear in the number of\nobservations when there are many change-points. The performance of the proposed\nmethod is examined by comparisons with some state of the art methods on both\nsimulated and real datasets. An R-package is available online.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 13:01:36 GMT"}, {"version": "v2", "created": "Fri, 19 Dec 2014 12:28:14 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2015 21:59:41 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2015 15:14:51 GMT"}, {"version": "v5", "created": "Sun, 25 Oct 2015 17:16:12 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Li", "Housen", ""], ["Munk", "Axel", ""], ["Sieling", "Hannes", ""]]}, {"id": "1412.5875", "submitter": "Ivan Kojadinovic", "authors": "Axel B\\\"ucher and Ivan Kojadinovic", "title": "Dependent multiplier bootstraps for non-degenerate $U$-statistics under\n  mixing conditions with applications", "comments": "35 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic validity of a resampling method for two sequential processes\nconstructed from non-degenerate $U$-statistics is established under mixing\nconditions. The resampling schemes, referred to as {\\em dependent multiplier\nbootstraps}, result from an adaptation of the seminal approach of\n\\cite{GomHor02} to mixing sequences. The proofs exploit recent results of\n\\cite{DehWen10b} on degenerate $U$-statistics. A data-driven procedure for\nestimating a key bandwidth parameter involved in the resampling schemes is also\nsuggested, making the use of the studied dependent multiplier bootstraps fully\nautomatic. The derived results are applied to the construction of confidence\nintervals and to test for change-point detection. For such applications, Monte\nCarlo experiments suggest that the use of the proposed resampling approaches\ncan have advantages over that of estimated asymptotic distributions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 14:35:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 13:41:46 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1412.6020", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen and Timothy Christensen", "title": "Optimal Uniform Convergence Rates and Asymptotic Normality for Series\n  Estimators Under Weak Dependence and Weak Conditions", "comments": "forthcoming in Journal of Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that spline and wavelet series regression estimators for weakly\ndependent regressors attain the optimal uniform (i.e. sup-norm) convergence\nrate $(n/\\log n)^{-p/(2p+d)}$ of Stone (1982), where $d$ is the number of\nregressors and $p$ is the smoothness of the regression function. The optimal\nrate is achieved even for heavy-tailed martingale difference errors with finite\n$(2+(d/p))$th absolute moment for $d/p<2$. We also establish the asymptotic\nnormality of t statistics for possibly nonlinear, irregular functionals of the\nconditional mean function under weak conditions. The results are proved by\nderiving a new exponential inequality for sums of weakly dependent random\nmatrices, which is of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 19:26:09 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 17:27:16 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""]]}, {"id": "1412.6185", "submitter": "Piotr Zwiernik", "authors": "Mateusz Micha{\\l}ek, Bernd Sturmfels, Caroline Uhler, Piotr Zwiernik", "title": "Exponential Varieties", "comments": null, "journal-ref": null, "doi": "10.1112/plms/pdv066", "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential varieties arise from exponential families in statistics. These\nreal algebraic varieties have strong positivity and convexity properties,\nfamiliar from toric varieties and their moment maps. Among them are varieties\nof inverses of symmetric matrices satisfying linear constraints. This class\nincludes Gaussian graphical models. We develop a general theory of exponential\nvarieties. These are derived from hyperbolic polynomials and their integral\nrepresentations. We compare the multidegrees and ML degrees of the gradient map\nfor hyperbolic polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 23:58:29 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 03:45:52 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Micha\u0142ek", "Mateusz", ""], ["Sturmfels", "Bernd", ""], ["Uhler", "Caroline", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1412.6697", "submitter": "Alice Cleynen", "authors": "Alice Cleynen and Emilie Lebarbier", "title": "Model selection for the segmentation of multiparameter exponential\n  family distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the segmentation problem of univariate distributions from the\nexponential family with multiple parameters. In segmentation, the choice of the\nnumber of segments remains a difficult issue due to the discrete nature of the\nchange-points. In this general exponential family distribution framework, we\npropose a penalized log-likelihood estimator where the penalty is inspired by\npapers of L. Birg\\'e and P. Massart. The resulting estimator is proved to\nsatisfy an oracle inequality. We then further study the particular case of\ncategorical variables by comparing the values of the key constants when derived\nfrom the specification of our general approach and when obtained by working\ndirectly with the characteristics of this distribution. Finally, a simulation\nstudy is conducted to assess the performance of our criterion for the\nexponential distribution, and an application on real data modelled by the\ncategorical distribution is provided.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 22:04:46 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 18:08:15 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Cleynen", "Alice", ""], ["Lebarbier", "Emilie", ""]]}, {"id": "1412.6829", "submitter": "Leonid Sirota", "authors": "E.Ostrovsky, L.Sirota", "title": "Well Posedness of the Problem of Estimation Fractional Derivative for a\n  Distribution Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric estimation of the fractional derivative\nof unknown distribution function and of spectral function and show that these\nproblems are well posed when the order of derivative is less than 0.5.\n  We prove also the unbiaseness and asymptotical normality of offered estimates\nwith optimal speed of convergence. For the construction of the confidence\nregion in some functional norm we establish the Central Limit Theorem in\ncorrespondent Lebesgue-Riesz space for offered estimates, and deduce also the\nnon-asymptotical deviation of our estimates in these spaces.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 20:14:31 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ostrovsky", "E.", ""], ["Sirota", "L.", ""]]}, {"id": "1412.6966", "submitter": "Franck Picard", "authors": "S. Ivanoff, F. Picard, V. Rivoirard", "title": "Adaptive Lasso and group-Lasso for functional Poisson regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional Poisson regression has become a standard framework for the\nanalysis of massive counts datasets. In this work we estimate the intensity\nfunction of the Poisson regression model by using a dictionary approach, which\ngeneralizes the classical basis approach, combined with a Lasso or a\ngroup-Lasso procedure. Selection depends on penalty weights that need to be\ncalibrated. Standard methodologies developed in the Gaussian framework can not\nbe directly applied to Poisson models due to heteroscedasticity. Here we\nprovide data-driven weights for the Lasso and the group-Lasso derived from\nconcentration inequalities adapted to the Poisson case. We show that the\nassociated Lasso and group-Lasso procedures are theoretically optimal in the\noracle approach. Simulations are used to assess the empirical performance of\nour procedure, and an original application to the analysis of Next Generation\nSequencing data is provided.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 13:00:36 GMT"}, {"version": "v2", "created": "Fri, 26 Dec 2014 20:13:11 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Ivanoff", "S.", ""], ["Picard", "F.", ""], ["Rivoirard", "V.", ""]]}, {"id": "1412.7103", "submitter": "Mathias Trabs", "authors": "Jakob S\\\"ohl and Mathias Trabs", "title": "Adaptive confidence bands for Markov chains and diffusions: Estimating\n  the invariant measure and the drift", "comments": "to appear in ESAIM: Probability and Statistics", "journal-ref": "ESAIM: PS 20 (2016) 432-462", "doi": "10.1051/ps/2016017", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a starting point we prove a functional central limit theorem for\nestimators of the invariant measure of a geometrically ergodic Harris-recurrent\nMarkov chain in a multi-scale space. This allows to construct confidence bands\nfor the invariant density with optimal (up to undersmoothing)\n$L^{\\infty}$-diameter by using wavelet projection estimators. In addition our\nsetting applies to the drift estimation of diffusions observed discretely with\nfixed observation distance. We prove a functional central limit theorem for\nestimators of the drift function and finally construct adaptive confidence\nbands for the drift by using a completely data-driven estimator.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 19:15:55 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 08:02:05 GMT"}, {"version": "v3", "created": "Mon, 23 May 2016 15:03:44 GMT"}, {"version": "v4", "created": "Thu, 7 Jul 2016 14:37:33 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["S\u00f6hl", "Jakob", ""], ["Trabs", "Mathias", ""]]}, {"id": "1412.7138", "submitter": "Ning Hao", "authors": "Ning Hao and Hao Helen Zhang", "title": "A Note on High Dimensional Linear Regression with Interactions", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of interaction selection has recently caught much attention in\nhigh dimensional data analysis. This note aims to address and clarify several\nfundamental issues in interaction selection for linear regression models,\nespecially when the input dimension p is much larger than the sample size n. We\nfirst discuss issues such as a valid way of defining importance for the main\neffects and interaction effects, the invariance principle, and the strong\nheredity condition. Then we focus on two-stage methods, which are\ncomputationally attractive for large p problems but regarded heuristic in the\nliterature. We will revisit the counterexample of Turlach (2004) and provide\nnew insight to justify two-stage methods from a theoretical perspective. In the\nend, we suggest some new strategies for interaction selection under the\nmarginality principle, which is followed by a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 20:34:01 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 16:47:47 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Hao", "Ning", ""], ["Zhang", "Hao Helen", ""]]}, {"id": "1412.7197", "submitter": "Fabrizio Lecci", "authors": "Fr\\'ed\\'eric Chazal, Brittany T. Fasy, Fabrizio Lecci, Bertrand\n  Michel, Alessandro Rinaldo, Larry Wasserman", "title": "Robust Topological Inference: Distance To a Measure and Kernel Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG math.AT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let P be a distribution with support S. The salient features of S can be\nquantified with persistent homology, which summarizes topological features of\nthe sublevel sets of the distance function (the distance of any point x to S).\nGiven a sample from P we can infer the persistent homology using an empirical\nversion of the distance function. However, the empirical distance function is\nhighly non-robust to noise and outliers. Even one outlier is deadly. The\ndistance-to-a-measure (DTM), introduced by Chazal et al. (2011), and the kernel\ndistance, introduced by Phillips et al. (2014), are smooth functions that\nprovide useful topological information but are robust to noise and outliers.\nChazal et al. (2014) derived concentration bounds for DTM. Building on these\nresults, we derive limiting distributions and confidence sets, and we propose a\nmethod for choosing tuning parameters.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 22:50:15 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Fasy", "Brittany T.", ""], ["Lecci", "Fabrizio", ""], ["Michel", "Bertrand", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1412.7206", "submitter": "Fan Wu", "authors": "Min Tsao and Fan Wu", "title": "Two-sample extended empirical likelihood for estimating equations", "comments": "26 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample extended empirical likelihood for inference on the\ndifference between two p-dimensional parameters defined by estimating\nequations. The standard two-sample empirical likelihood for the difference is\nBartlett correctable but its domain is a bounded subset of the parameter space.\nWe expand its domain through a composite similarity transformation to derive\nthe two-sample extended empirical likelihood which is defined on the full\nparameter space. The extended empirical likelihood has the same asymptotic\ndistribution as the standard one and can also achieve the second order accuracy\nof the Bartlett correction. We include two applications to illustrate the use\nof two-sample empirical likelihood methods and to demonstrate the superior\ncoverage accuracy of the extended empirical likelihood confidence regions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:21:19 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Tsao", "Min", ""], ["Wu", "Fan", ""]]}, {"id": "1412.7216", "submitter": "Mathieu Rosenbaum", "authors": "Alexandre Belloni, Mathieu Rosenbaum and Alexandre B. Tsybakov", "title": "An $\\{l_1,l_2,l_{\\infty}\\}$-Regularization Approach to High-Dimensional\n  Errors-in-variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several new estimation methods have been recently proposed for the linear\nregression model with observation error in the design. Different assumptions on\nthe data generating process have motivated different estimators and analysis.\nIn particular, the literature considered (1) observation errors in the design\nuniformly bounded by some $\\bar \\delta$, and (2) zero mean independent\nobservation errors. Under the first assumption, the rates of convergence of the\nproposed estimators depend explicitly on $\\bar \\delta$, while the second\nassumption has been applied when an estimator for the second moment of the\nobservational error is available. This work proposes and studies two new\nestimators which, compared to other procedures for regression models with\nerrors in the design, exploit an additional $l_{\\infty}$-norm regularization.\nThe first estimator is applicable when both (1) and (2) hold but does not\nrequire an estimator for the second moment of the observational error. The\nsecond estimator is applicable under (2) and requires an estimator for the\nsecond moment of the observation error. Importantly, we impose no assumption on\nthe accuracy of this pilot estimator, in contrast to the previously known\nprocedures. As the recent proposals, we allow the number of covariates to be\nmuch larger than the sample size. We establish the rates of convergence of the\nestimators and compare them with the bounds obtained for related estimators in\nthe literature. These comparisons show interesting insights on the interplay of\nthe assumptions and the achievable rates of convergence.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 23:57:56 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Belloni", "Alexandre", ""], ["Rosenbaum", "Mathieu", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1412.7332", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Caterina May and Chiara Tommasi", "title": "Best estimation of functional linear models", "comments": "the best information from the two samples of functions and\n  derivatives: a strong version of the Gauss-Markov theorem. Relaxed an hidden\n  hypothesis on linear independence of the Riesz representation of the\n  Karhunen-Loeve base", "journal-ref": "J.Multivariate Anal. 151 (2016) 54-68", "doi": "10.1016/j.jmva.2016.07.005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations which are realizations from some continuous process are frequent\nin sciences, engineering, economics, and other fields. We consider linear\nmodels, with possible random effects, where the responses are random functions\nin a suitable Sobolev space. The processes cannot be observed directly. With\nsmoothing procedures from the original data, both the response curves and their\nderivatives can be reconstructed, even separately. From both these samples of\nfunctions, just one sample of representatives is obtained to estimate the\nvector of functional parameters. A simulation study shows the benefits of this\napproach over the common method of using information either on curves or\nderivatives. The main theoretical result is a strong functional version of the\nGauss-Markov theorem. This ensures that the proposed functional estimator is\nmore efficient than the best linear unbiased estimator based only on curves or\nderivatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 11:54:47 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 12:13:07 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 12:30:42 GMT"}, {"version": "v4", "created": "Wed, 27 Apr 2016 12:56:24 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Aletti", "Giacomo", ""], ["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1412.7392", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan", "title": "Theoretical guarantees for approximate sampling from smooth and\n  log-concave densities", "comments": "To appear in JRSS B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from various kinds of distributions is an issue of paramount\nimportance in statistics since it is often the key ingredient for constructing\nestimators, test procedures or confidence intervals. In many situations, the\nexact sampling from a given distribution is impossible or computationally\nexpensive and, therefore, one needs to resort to approximate sampling\nstrategies. However, there is no well-developed theory providing meaningful\nnonasymptotic guarantees for the approximate sampling procedures, especially in\nthe high-dimensional problems. This paper makes some progress in this direction\nby considering the problem of sampling from a distribution having a smooth and\nlog-concave density defined on \\(\\RR^p\\), for some integer \\(p>0\\). We\nestablish nonasymptotic bounds for the error of approximating the target\ndistribution by the one obtained by the Langevin Monte Carlo method and its\nvariants. We illustrate the effectiveness of the established guarantees with\nvarious experiments. Underlying our analysis are insights from the theory of\ncontinuous-time diffusion processes, which may be of interest beyond the\nframework of log-concave densities considered in the present work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 15:00:57 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 03:15:50 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 03:29:23 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 16:02:53 GMT"}, {"version": "v5", "created": "Fri, 19 Feb 2016 23:19:39 GMT"}, {"version": "v6", "created": "Sat, 3 Dec 2016 08:41:19 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dalalyan", "Arnak S.", ""]]}, {"id": "1412.7468", "submitter": "Yang Feng", "authors": "Pallavi Basu, Yang Feng and Jinchi Lv", "title": "Model Selection in High-Dimensional Misspecified Models", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is indispensable to high-dimensional sparse modeling in\nselecting the best set of covariates among a sequence of candidate models. Most\nexisting work assumes implicitly that the model is correctly specified or of\nfixed dimensions. Yet model misspecification and high dimensionality are common\nin real applications. In this paper, we investigate two classical\nKullback-Leibler divergence and Bayesian principles of model selection in the\nsetting of high-dimensional misspecified models. Asymptotic expansions of these\nprinciples reveal that the effect of model misspecification is crucial and\nshould be taken into account, leading to the generalized AIC and generalized\nBIC in high dimensions. With a natural choice of prior probabilities, we\nsuggest the generalized BIC with prior probability which involves a logarithmic\nfactor of the dimensionality in penalizing model complexity. We further\nestablish the consistency of the covariance contrast matrix estimator in a\ngeneral setting. Our results and new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 18:49:19 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Basu", "Pallavi", ""], ["Feng", "Yang", ""], ["Lv", "Jinchi", ""]]}, {"id": "1412.7739", "submitter": "Shota Gugushvili", "authors": "Shota Gugushvili, Frank van der Meulen, Peter Spreij", "title": "Nonparametric Bayesian inference for multidimensional compound Poisson\n  processes", "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA20 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 1,\n  1-15", "doi": "10.15559/15-VMSTA20", "report-no": "VTeX-VMSTA-VMSTA20", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample from a discretely observed multidimensional compound Poisson\nprocess, we study the problem of nonparametric estimation of its jump size\ndensity $r_0$ and intensity $\\lambda_0$. We take a nonparametric Bayesian\napproach to the problem and determine posterior contraction rates in this\ncontext, which, under some assumptions, we argue to be optimal posterior\ncontraction rates. In particular, our results imply the existence of Bayesian\npoint estimates that converge to the true parameter pair $(r_0,\\lambda_0)$ at\nthese rates. To the best of our knowledge, construction of nonparametric\ndensity estimators for inference in the class of discretely observed\nmultidimensional L\\'{e}vy processes, and the study of their rates of\nconvergence is a new contribution to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 19:22:04 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 05:47:19 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Spreij", "Peter", ""]]}, {"id": "1412.7794", "submitter": "Mutsuki Kojima", "authors": "Mutsuki Kojima, Fumiyasu Komaki", "title": "Relations Between the Conditional Normalized Maximum Likelihood\n  Distributions and the Latent Information Priors", "comments": null, "journal-ref": "IEEE Transactions on Information Theory, Volume: 62, Issue 1, 2016", "doi": "10.1109/TIT.2015.2496581", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reveal the relations between the conditional normalized maximum likelihood\n(CNML) distributions and Bayesian predictive densities based on the latent\ninformation priors (LIPs). In particular, CNML3, which is one type of CNML\ndistributions, is investigated. The Bayes projection of a predictive density,\nwhich is an information projection of the predictive density on a set of\nBayesian predictive densities, is considered. We prove that the sum of the\nBayes projection divergence of CNML3 and the conditional mutual information is\nasymptotically constant. This result implies that the Bayes projection of CNML3\n(BPCNML3) is asymptotically identical to the Bayesian predictive density based\non LIP. In addition, under some stronger assumptions, we show that BPCNML3\nexactly coincides with the Bayesian predictive density based on LIP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 06:22:32 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kojima", "Mutsuki", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1412.7890", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Justin Romberg", "title": "Compressive Deconvolution in Random Mask Imaging", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging 1(4):236--246, 2015", "doi": "10.1109/TCI.2015.2485941", "report-no": null, "categories": "cs.IT math.FA math.IT math.NA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reconstructing signals from a subsampled\nconvolution of their modulated versions and a known filter. The problem is\nstudied as applies to specific imaging systems relying on spatial phase\nmodulation by randomly coded \"masks.\" The diversity induced by the random masks\nis deemed to improve the conditioning of the deconvolution problem while\nmaintaining sampling efficiency.\n  We analyze a linear model of the system, where the joint effect of the\nspatial modulation, blurring, and spatial subsampling is represented by a\nmeasurement matrix. We provide a bound on the conditioning of this measurement\nmatrix in terms of the number of masks, the dimension of the image, and certain\ncharacteristics of the blurring kernel and subsampling operator. The derived\nbound shows that stable deconvolution is possible with high probability even if\nthe total number of (scalar) measurements is within a logarithmic factor of the\nimage size. Furthermore, beyond a critical number of masks determined by the\nextent of blurring and subsampling, every additional mask improves the\nconditioning of the measurement matrix.\n  We also consider a more interesting scenario where the target image is\nsparse. We show that under mild conditions on the blurring kernel, with high\nprobability the measurement matrix is a restricted isometry when the number of\nmasks is within a logarithmic factor of the sparsity of the image. Therefore,\nthe image can be reconstructed using many sparse recovery algorithms such as\nthe basis pursuit. The bound on the required number of masks is linear in\nsparsity of the image but it is logarithmic in its dimension. The bound\nprovides a quantitative view of the effect of the blurring and subsampling on\nthe required number of masks, which is critical for designing efficient imaging\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 03:56:16 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 20:53:52 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Bahmani", "Sohail", ""], ["Romberg", "Justin", ""]]}, {"id": "1412.7907", "submitter": "Ashwini Maurya", "authors": "Ashwini Maurya", "title": "A Well-Conditioned and Sparse Estimation of Covariance and Inverse\n  Covariance Matrices Using a Joint Penalty", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for estimating well-conditioned and sparse covariance and\ninverse covariance matrices from a sample of vectors drawn from a sub-gaussian\ndistribution in high dimensional setting. The proposed estimators are obtained\nby minimizing the quadratic loss function and joint penalty of `1 norm and\nvariance of its eigenvalues. In contrast to some of the existing methods of\ncovariance and inverse covariance matrix estimation, where often the interest\nis to estimate a sparse matrix, the proposed method is flexible in estimating\nboth a sparse and well-conditioned covariance matrix simultaneously. The\nproposed estimators are optimal in the sense that they achieve the minimax rate\nof estimation in operator norm for the underlying class of covariance and\ninverse covariance matrices. We give a very fast algorithm for computation of\nthese covariance and inverse covariance matrices which is easily scalable to\nlarge scale data analysis problems. The simulation study for varying sample\nsizes and variables shows that the proposed estimators performs better than\nseveral other estimators for various choices of structured covariance and\ninverse covariance matrices. We also use our proposed estimator for tumor\ntissues classification using gene expression data and compare its performance\nwith some other classification methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 07:34:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 04:13:12 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 18:11:00 GMT"}, {"version": "v4", "created": "Sun, 3 Apr 2016 23:07:03 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Maurya", "Ashwini", ""]]}, {"id": "1412.8016", "submitter": "Madhuresh Madhuresh", "authors": "Madhuresh", "title": "Some results on contraction rates for Bayesian inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a general lemma for deriving contraction rates for linear inverse\nproblems with non parametric nonconjugate priors. We then apply it to get\ncontraction rates for both mildly and severely ill posed linear inverse\nproblems with Gaussian priors in non conjugate cases. In the severely illposed\ncase, our rates match the minimax rates using scalable priors with scales which\ndo not depend upon the smoothness of true solution. In the mildly illposed\ncase, our rates match the minimax rates using scalable priors when the true\nsolution is not too smooth. Further, using the lemma, we find contraction rates\nfor inversion of a semilinear operator with Gaussian priors. We find the\ncontraction rates for a compactly supported prior. We also discuss the minimax\nrates applicable to our examples when the Sobolev balls in which the true\nsolution lies, are different from the usual Sobolev balls defined via the basis\nof forward operator.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 05:20:11 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 11:15:39 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 22:07:34 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Madhuresh", "", ""]]}, {"id": "1412.8132", "submitter": "Olga Klopp", "authors": "Olga Klopp (MODAL'X, CREST), Karim Lounici, Alexandre B. Tsybakov\n  (CREST)", "title": "Robust Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of recovery of a low-rank matrix in the\nsituation when most of its entries are not observed and a fraction of observed\nentries are corrupted. The observations are noisy realizations of the sum of a\nlow rank matrix, which we wish to recover, with a second matrix having a\ncomplementary sparse structure such as element-wise or column-wise sparsity. We\nanalyze a class of estimators obtained by solving a constrained convex\noptimization problem that combines the nuclear norm and a convex relaxation for\na sparse constraint. Our results are obtained for the simultaneous presence of\nrandom and deterministic patterns in the sampling scheme. We provide guarantees\nfor recovery of low-rank and sparse components from partial and corrupted\nobservations in the presence of noise and show that the obtained rates of\nconvergence are minimax optimal.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 08:45:05 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 09:27:10 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Klopp", "Olga", "", "MODAL'X, CREST"], ["Lounici", "Karim", "", "CREST"], ["Tsybakov", "Alexandre B.", "", "CREST"]]}, {"id": "1412.8161", "submitter": "Prasenjit Ghosh", "authors": "Prasenjit Ghosh and Arijit Chakrabarti", "title": "Posterior Concentration Properties of a General Class of Shrinkage\n  Priors around Nearly Black Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have data generated according to a multivariate normal\ndistribution with a fixed unknown mean vector that is sparse in the sense of\nbeing nearly black. Optimality of Bayes estimates and posterior concentration\nproperties in terms of the minimax risk in the $l_2$ norm corresponding to a\nvery general class of continuous shrinkage priors are studied in this work. The\nclass of priors considered is rich enough to include a great variety of heavy\ntailed prior distributions, such as, the three parameter beta normal mixtures\n(including the horseshoe), the generalized double Pareto, the inverse gamma and\nthe normal-exponential-gamma priors. Assuming that the number of non-zero\ncomponents of the mean vector is known, we show that the Bayes estimators\ncorresponding to this general class of priors attain the minimax risk in the\n$l_2$ norm (possibly up to a multiplicative constant) and the corresponding\nposterior distributions contract around the true mean vector at the minimax\noptimal rate for appropriate choice of the global shrinkage parameter.\nMoreover, we provide conditions for which these posterior distributions\ncontract around the corresponding Bayes estimates at least as fast as the\nminimax risk in the $l_2$ norm. We also provide a lower bound to the total\nposterior variance for an important subclass of this general class of shrinkage\npriors that includes the generalized double Pareto priors with shape parameter\n$\\alpha=0.5$ and the three parameter beta normal mixtures with parameters\n$a=0.5$ and $b>0$ (including the horseshoe) in particular. The present work is\ninspired by the recent work of van der Pas et al. (2014) on the posterior\ncontraction properties of the horseshoe prior under the present set-up. We\nextend their results for this general class of priors and come up with novel\nunifying proofs which work for a very broad class of one-group continuous\nshrinkage priors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 13:11:23 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 15:31:45 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 19:06:38 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2015 10:53:58 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Ghosh", "Prasenjit", ""], ["Chakrabarti", "Arijit", ""]]}, {"id": "1412.8173", "submitter": "Teppei Ogihara", "authors": "Teppei Ogihara", "title": "Parametric Inference for Nonsynchronously Observed Diffusion Processes\n  in the Presence of Market Microstructure Noise", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parametric inference for diffusion processes when observations occur\nnonsynchronously and are contaminated by market microstructure noise. We\nconstruct a quasi-likelihood function and study asymptotic mixed normality of\nmaximum-likelihood- and Bayes-type estimators based on it. We also prove the\nlocal asymptotic normality of the model and asymptotic efficiency of our\nestimator when the diffusion coefficients are constant and noise follows a\nnormal distribution. We conjecture that our estimator is asymptotically\nefficient even when the latent process is a general diffusion process. An\nestimator for the quadratic covariation of the latent process is also\nconstructed. Some numerical examples show that this estimator performs better\ncompared to existing estimators of the quadratic covariation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 14:44:56 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 08:37:47 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2015 13:10:44 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Ogihara", "Teppei", ""]]}, {"id": "1412.8285", "submitter": "Piotr Zwiernik", "authors": "Mathias Drton, Shaowei Lin, Luca Weihs and Piotr Zwiernik", "title": "Marginal likelihood and model selection for Gaussian latent tree and\n  forest models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian latent tree models, or more generally, Gaussian latent forest models\nhave Fisher-information matrices that become singular along interesting\nsubmodels, namely, models that correspond to subforests. For these\nsingularities, we compute the real log-canonical thresholds (also known as\nstochastic complexities or learning coefficients) that quantify the\nlarge-sample behavior of the marginal likelihood in Bayesian inference. This\nprovides the information needed for a recently introduced generalization of the\nBayesian information criterion. Our mathematical developments treat the general\nsetting of Laplace integrals whose phase functions are sums of squared\ndifferences between monomials and constants. We clarify how in this case real\nlog-canonical thresholds can be computed using polyhedral geometry, and we show\nhow to apply the general theory to the Laplace integrals associated with\nGaussian latent tree and forest models. In simulations and a data example, we\ndemonstrate how the mathematical knowledge can be applied in model selection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 09:10:17 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 02:19:35 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Drton", "Mathias", ""], ["Lin", "Shaowei", ""], ["Weihs", "Luca", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1412.8340", "submitter": "Kammoun Abla", "authors": "Abla Kammoun and M. S. Alouini", "title": "On the Smallest Eigenvalue of General correlated Gaussian Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the behaviour of the spectrum of generally correlated\nGaussian random matrices whose columns are zero-mean independent vectors but\nhave different correlations, under the specific regime where the number of\ntheir columns and that of their rows grow at infinity with the same pace. This\nwork is, in particular, motivated by applications from statistical signal\nprocessing and wireless communications, where this kind of matrices naturally\narise. Following the approach proposed in [1], we prove that under some\nspecific conditions, the smallest singular value of generally correlated\nGaussian matrices is almost surely away from zero.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:46:13 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kammoun", "Abla", ""], ["Alouini", "M. S.", ""]]}, {"id": "1412.8344", "submitter": "Kammoun Abla", "authors": "Abla Kammoun and Mohamed-Slim Alouini", "title": "The random matrix regime of Maronna's M-estimator for observations\n  corrupted by elliptical noises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the behavior of the Maronna robust scatter estimator\n$\\hat{C}_N\\in \\mathbb{C}^{N\\times N}$ of a sequence of observations\n$y_1,...,y_n$ which is composed of a $K$ dimensional signal drown in a heavy\ntailed noise, i.e $y_i=A_N s_i+x_i$ where $A_N \\in \\mathbb{C}^{N\\times K}$ and\n$x_i$ is drawn from elliptical distribution.\n  In particular, we prove that as the population dimension $N$, the number of\nobservations $n$ and the rank of $A_N$ grow to infinity at the same pace and\nunder some mild assumptions, the robust scatter matrix can be characterized by\na random matrix $\\hat{S}_N$ that follows a standard random model. Our analysis\ncan be very useful for many applications of the fields of statistical inference\nand signal processing.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 13:56:24 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Kammoun", "Abla", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1412.8397", "submitter": "Chanchal Kundu", "authors": "Chanchal Kundu and Amit Ghosh", "title": "Inequalities involving expectations of selected functions in reliability\n  theory to characterize distributions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, authors have studied inequalities involving expectations of\nselected functions viz. failure rate, mean residual life, aging intensity\nfunction and log-odds rate which are defined for left truncated random\nvariables in reliability theory to characterize some well-known distributions.\nHowever, there has been growing interest in the study of these functions in\nreversed time and their applications. In the present work we consider reversed\nhazard rate, expected inactivity time and reversed aging intensity function to\ndeal with right truncated random variables and characterize a few statistical\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 17:22:44 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 05:49:23 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Kundu", "Chanchal", ""], ["Ghosh", "Amit", ""]]}, {"id": "1412.8401", "submitter": "George Yanev", "authors": "George P. Yanev and Santanu Chakraborty", "title": "A characterization of Exponential Distribution and the Sukhatme-Renyi\n  Decomposition of Exponential Maxima", "comments": null, "journal-ref": "Statistics and Probability Letters 110 (2016) 94-102", "doi": "10.1016/j.spl.2015.12.004", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new characterization of the exponential distribution is established. It is\nproven that the well-known Sukhatme-Renyi necessary condition is also\nsufficient for exponentiality. A method of proof due to Arnold and Villasenor\nbased on the Maclaurin series expansion of the density is utilized.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 17:37:07 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 17:35:03 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2015 16:40:37 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Yanev", "George P.", ""], ["Chakraborty", "Santanu", ""]]}, {"id": "1412.8434", "submitter": "Victor Chernozhukov", "authors": "Victor Chernozhukov, Alfred Galichon, Marc Hallin, Marc Henry", "title": "Monge-Kantorovich Depth, Quantiles, Ranks, and Signs", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new concepts of statistical depth, multivariate quantiles, ranks\nand signs, based on canonical transportation maps between a distribution of\ninterest on $R^d$ and a reference distribution on the $d$-dimensional unit\nball. The new depth concept, called Monge-Kantorovich depth, specializes to\nhalfspace depth in the case of spherical distributions, but, for more general\ndistributions, differs from the latter in the ability for its contours to\naccount for non convex features of the distribution of interest. We propose\nempirical counterparts to the population versions of those Monge-Kantorovich\ndepth contours, quantiles, ranks and signs, and show their consistency by\nestablishing a uniform convergence property for empirical transport maps, which\nis of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 19:32:34 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 20:12:16 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 18:18:59 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2015 20:55:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Galichon", "Alfred", ""], ["Hallin", "Marc", ""], ["Henry", "Marc", ""]]}, {"id": "1412.8604", "submitter": "JInglai Li", "authors": "Jinglai Li", "title": "A note on the Karhunen-Lo\\`eve expansions for infinite-dimensional\n  Bayesian inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we consider the truncated Karhunen-Lo\\`eve expansion for\napproximating solutions to infinite dimensional inverse problems. We show that,\nunder certain conditions, the bound of the error between a solution and its\nfinite-dimensional approximation can be estimated without the knowledge of the\nsolution.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 11:24:59 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Li", "Jinglai", ""]]}, {"id": "1412.8652", "submitter": "Anna Ben-Hamou", "authors": "Anna Ben-Hamou, St\\'ephane Boucheron, Mesrob I. Ohannessian", "title": "Concentration inequalities in the infinite urn scheme for occupancy\n  counts and the missing mass, with applications", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ743 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2017, Vol. 23, No. 1, 249-287", "doi": "10.3150/15-BEJ743", "report-no": "IMS-BEJ-BEJ743", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An infinite urn scheme is defined by a probability mass function\n$(p_j)_{j\\geq1}$ over positive integers. A random allocation consists of a\nsample of $N$ independent drawings according to this probability distribution\nwhere $N$ may be deterministic or Poisson-distributed. This paper is concerned\nwith occupancy counts, that is with the number of symbols with $r$ or at least\n$r$ occurrences in the sample, and with the missing mass that is the total\nprobability of all symbols that do not occur in the sample. Without any further\nassumption on the sampling distribution, these random quantities are shown to\nsatisfy Bernstein-type concentration inequalities. The variance factors in\nthese concentration inequalities are shown to be tight if the sampling\ndistribution satisfies a regular variation property. This regular variation\nproperty reads as follows. Let the number of symbols with probability larger\nthan $x$ be $\\vec{\\nu}(x)=|\\{j:p_j\\geq x\\}|$. In a regularly varying urn\nscheme, $\\vec{\\nu}$ satisfies $\\lim_{\\tau \\rightarrow0}\\vec{\\nu}(\\tau\nx)/\\vec{\\nu}(\\tau)=x^{-\\alpha}$ for $\\alpha\\in[0,1]$ and the variance of the\nnumber of distinct symbols in a sample tends to infinity as the sample size\ntends to infinity. Among other applications, these concentration inequalities\nallow us to derive tight confidence intervals for the Good--Turing estimator of\nthe missing mass.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 15:20:04 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 15:27:11 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 13:49:22 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Ben-Hamou", "Anna", ""], ["Boucheron", "St\u00e9phane", ""], ["Ohannessian", "Mesrob I.", ""]]}, {"id": "1412.8690", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, SIERRA)", "title": "Breaking the Curse of Dimensionality with Convex Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider neural networks with a single hidden layer and non-decreasing\nhomogeneous activa-tion functions like the rectified linear units. By letting\nthe number of hidden units grow unbounded and using classical non-Euclidean\nregularization tools on the output weights, we provide a detailed theoretical\nanalysis of their generalization performance, with a study of both the\napproximation and the estimation errors. We show in particular that they are\nadaptive to unknown underlying linear structures, such as the dependence on the\nprojection of the input variables onto a low-dimensional subspace. Moreover,\nwhen using sparsity-inducing norms on the input weights, we show that\nhigh-dimensional non-linear variable selection may be achieved, without any\nstrong assumption regarding the data and with a total number of variables\npotentially exponential in the number of ob-servations. In addition, we provide\na simple geometric interpretation to the non-convex problem of addition of a\nnew unit, which is the core potentially hard computational element in the\nframework of learning from continuously many basis functions. We provide simple\nconditions for convex relaxations to achieve the same generalization error\nbounds, even when constant-factor approxi-mations cannot be found (e.g.,\nbecause it is NP-hard such as for the zero-homogeneous activation function). We\nwere not able to find strong enough convex relaxations and leave open the\nexistence or non-existence of polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 17:08:00 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 08:38:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Bach", "Francis", "", "LIENS, SIERRA"]]}]