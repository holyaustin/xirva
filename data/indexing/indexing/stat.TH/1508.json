[{"id": "1508.00076", "submitter": "Alexander Goldenshluger", "authors": "A. Goldenshluger", "title": "Nonparametric estimation of service time distribution in the\n  $M/G/\\infty$ queue and related estimation problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this paper is the problem of estimating service time\ndistribution of the $M/G/\\infty$ queue from incomplete data on the queue. The\ngoal is to estimate $G$ from observations of the queue--length process at the\npoints of the regular grid on a fixed time interval. We propose an estimator\nand analyze its accuracy over a family of target service time distributions.\nThe original $M/G/\\infty$ problem is closely related to the problem of\nestimating derivatives of the covariance function of a stationary Gaussian\nprocess. We consider the latter problem and derive lower bounds on the minimax\nrisk. The obtained results strongly suggest that the proposed estimator of the\nservice time distribution is rate optimal.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 04:56:53 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Goldenshluger", "A.", ""]]}, {"id": "1508.00144", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva, Julie Henriques, Juan-Pablo Ortega", "title": "Quantitative evaluation of the performance of discrete-time reservoir\n  computers in the forecasting, filtering, and reconstruction of stochastic\n  stationary signals", "comments": "21 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the notion of information processing capacity for\nnon-independent input signals in the context of reservoir computing (RC). The\npresence of input autocorrelation makes worthwhile the treatment of forecasting\nand filtering problems for which we explicitly compute this generalized\ncapacity as a function of the reservoir parameter values using a streamlined\nmodel. The reservoir model leading to these developments is used to show that,\nwhenever that approximation is valid, this computational paradigm satisfies the\nso called separation and fading memory properties that are usually associated\nwith good information processing performances. We show that several standard\nmemory, forecasting, and filtering problems that appear in the parametric\nstochastic time series context can be readily formulated and tackled via RC\nwhich, as we show, significantly outperforms standard techniques in some\ninstances.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 16:42:38 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 10:21:19 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 14:44:12 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Henriques", "Julie", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1508.00219", "submitter": "Ali Akbar Jafari", "authors": "Ali Akbar Jafari and Rasool Roozegar", "title": "On Bivariate Generalized Exponential-Power Series Class of Distributions", "comments": "arXiv admin note: text overlap with arXiv:1507.07535", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of bivariate distributions by\ncompounding the bivariate generalized exponential and power-series\ndistributions.\n  This new class contains some new sub-models such as the bivariate generalized\nexponential distribution, the bivariate generalized exponential-poisson,\n-logarithmic, -binomial and -negative binomial distributions. We derive\ndifferent properties of the new class of distributions. The EM algorithm is\nused to determine the maximum likelihood estimates of the parameters. We\nillustrate the usefulness of the new distributions by means of an application\nto a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 11:06:33 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Jafari", "Ali Akbar", ""], ["Roozegar", "Rasool", ""]]}, {"id": "1508.00249", "submitter": "Rajarshi Mukherjee", "authors": "Rajarshi Mukherjee, Eric Tchetgen Tchetgen and James Robins", "title": "Lepski's Method and Adaptive Estimation of Nonlinear Integral\n  Functionals of Density", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adaptive minimax estimation of non-linear integral functionals\nof a density and extend the results obtained for linear and quadratic\nfunctionals to general functionals. The typical rate optimal non-adaptive\nminimax estimators of \"smooth\" non-linear functionals are higher order\nU-statistics. Since Lepski's method requires tight control of tails of such\nestimators, we bypass such calculations by a modification of Lepski's method\nwhich is applicable in such situations. As a necessary ingredient, we also\nprovide a method to control higher order moments of minimax estimator of cubic\nintegral functionals. Following a standard constrained risk inequality method,\nwe also show the optimality of our adaptation rates.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 15:05:26 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 11:45:28 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Robins", "James", ""]]}, {"id": "1508.00506", "submitter": "Tobias Sutter", "authors": "Tobias Sutter, Arnab Ganguly, Heinz Koeppl", "title": "A variational approach to path estimation and parameter inference of\n  hidden diffusion processes", "comments": "37 pages, 2 figures, revised", "journal-ref": "JMLR, volume 17, number 190, year 2016", "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hidden Markov model, where the signal process, given by a\ndiffusion, is only indirectly observed through some noisy measurements. The\narticle develops a variational method for approximating the hidden states of\nthe signal process given the full set of observations. This, in particular,\nleads to systematic approximations of the smoothing densities of the signal\nprocess. The paper then demonstrates how an efficient inference scheme, based\non this variational approach to the approximation of the hidden states, can be\ndesigned to estimate the unknown parameters of stochastic differential\nequations. Two examples at the end illustrate the efficacy and the accuracy of\nthe presented method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 18:01:13 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 06:39:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 12:11:46 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 09:15:29 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Sutter", "Tobias", ""], ["Ganguly", "Arnab", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1508.00655", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance\n  based High Dimensional Two Sample Testing", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing is a decision theoretic problem that\ninvolves identifying differences between two random variables without making\nparametric assumptions about their underlying distributions. We refer to the\nmost common settings as mean difference alternatives (MDA), for testing\ndifferences only in first moments, and general difference alternatives (GDA),\nwhich is about testing for any difference in distributions. A large number of\ntest statistics have been proposed for both these settings. This paper connects\nthree classes of statistics - high dimensional variants of Hotelling's t-test,\nstatistics based on Reproducing Kernel Hilbert Spaces, and energy statistics\nbased on pairwise distances. We ask the question: how much statistical power do\npopular kernel and distance based tests for GDA have when the unknown\ndistributions differ in their means, compared to specialized tests for MDA?\n  We formally characterize the power of popular tests for GDA like the Maximum\nMean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent\nvariants of the Energy Distance with the Euclidean norm (eED) in the\nhigh-dimensional MDA regime. Some practically important properties include (a)\neED and gMMD have asymptotically equal power; furthermore they enjoy a free\nlunch because, while they are additionally consistent for GDA, they also have\nthe same power as specialized high-dimensional t-test variants for MDA. All\nthese tests are asymptotically optimal (including matching constants) under MDA\nfor spherical covariances, according to simple lower bounds, (b) The power of\ngMMD is independent of the kernel bandwidth, as long as it is larger than the\nchoice made by the median heuristic, (c) There is a clear and smooth\ncomputation-statistics tradeoff for linear-time, subquadratic-time and\nquadratic-time versions of these tests, with more computation resulting in\nhigher power.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 04:10:05 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1508.00689", "submitter": "Pascal Vontobel", "authors": "Hans-Andrea Loeliger and Pascal O. Vontobel", "title": "Factor Graphs for Quantum Probabilities", "comments": "To appear in IEEE Transactions on Information Theory, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A factor-graph representation of quantum-mechanical probabilities (involving\nany number of measurements) is proposed. Unlike standard statistical models,\nthe proposed representation uses auxiliary variables (state variables) that are\nnot random variables. All joint probability distributions are marginals of some\ncomplex-valued function $q$, and it is demonstrated how the basic concepts of\nquantum mechanics relate to factorizations and marginals of $q$.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 07:38:51 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 13:16:37 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 04:29:07 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Loeliger", "Hans-Andrea", ""], ["Vontobel", "Pascal O.", ""]]}, {"id": "1508.00934", "submitter": "Jingshu Wang", "authors": "Jingshu Wang, Art B. Owen", "title": "Admissibility in Partial Conjunction Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis combines results from multiple studies aiming to increase power\nin finding their common effect. It would typically reject the null hypothesis\nof no effect if any one of the studies shows strong significance. The partial\nconjunction null hypothesis is rejected only when at least $r$ of $n$ component\nhypotheses are non-null with $r = 1$ corresponding to a usual meta-analysis.\nCompared with meta-analysis, it can encourage replicable findings across\nstudies. A by-product of it when applied to different $r$ values is a\nconfidence interval of $r$ quantifying the proportion of non-null studies.\nBenjamini and Heller (2008) provided a valid test for the partial conjunction\nnull by ignoring the $r - 1$ smallest p-values and applying a valid\nmeta-analysis p-value to the remaining $n - r + 1$ p-values. We provide\nsufficient and necessary conditions of admissible combined p-value for the\npartial conjunction hypothesis among monotone tests. Non-monotone tests always\ndominate monotone tests but are usually too unreasonable to be used in\npractice. Based on these findings, we propose a generalized form of Benjamini\nand Heller's test which allows usage of various types of meta-analysis\np-values, and apply our method to an example in assessing replicable benefit of\nnew anticoagulants across subgroups of patients for stroke prevention.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 22:52:16 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:14:02 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 09:49:37 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Wang", "Jingshu", ""], ["Owen", "Art B.", ""]]}, {"id": "1508.00936", "submitter": "Yang-Hui He", "authors": "Rachael L. Bond, Yang-Hui He, and Thomas C. Ormerod", "title": "A quantum framework for likelihood ratios", "comments": null, "journal-ref": null, "doi": "10.1142/S0219749918500028", "report-no": null, "categories": "math.ST hep-th quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to calculate precise likelihood ratios is fundamental to many\nSTEM areas, such as decision-making theory, biomedical science, and\nengineering. However, there is no assumption-free statistical methodology to\nachieve this. For instance, in the absence of data relating to covariate\noverlap, the widely used Bayes' theorem either defaults to the marginal\nprobability driven \"naive Bayes' classifier\", or requires the use of\ncompensatory expectation-maximization techniques. Equally, the use of\nalternative statistical approaches, such as multivariate logistic regression,\nmay be confounded by other axiomatic conditions, e.g., low levels of\nco-linearity. This article takes an information-theoretic approach in\ndeveloping a new statistical formula for the calculation of likelihood ratios\nbased on the principles of quantum entanglement. In doing so, it is argued that\nthis quantum approach demonstrates: that the likelihood ratio is a real quality\nof statistical systems; that the naive Bayes' classifier is a special case of a\nmore general quantum mechanical expression; and that only a quantum mechanical\napproach can overcome the axiomatic limitations of classical statistics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 23:00:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bond", "Rachael L.", ""], ["He", "Yang-Hui", ""], ["Ormerod", "Thomas C.", ""]]}, {"id": "1508.00947", "submitter": "Bala Rajaratnam", "authors": "Bala Rajaratnam and Doug Sparks", "title": "MCMC-Based Inference in the Era of Big Data: A Fundamental Analysis of\n  the Convergence Complexity of High-Dimensional Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) lies at the core of modern Bayesian\nmethodology, much of which would be impossible without it. Thus, the\nconvergence properties of MCMCs have received significant attention, and in\nparticular, proving (geometric) ergodicity is of critical interest. Trust in\nthe ability of MCMCs to sample from modern-day high-dimensional posteriors,\nhowever, has been limited by a widespread perception that these chains\ntypically experience serious convergence problems. In this paper, we first\ndemonstrate that contemporary methods for obtaining convergence rates have\nserious limitations when the dimension grows. We then propose a framework for\nrigorously establishing the convergence behavior of commonly used\nhigh-dimensional MCMCs. In particular, we demonstrate theoretically the precise\nnature and severity of the convergence problems of popular MCMCs when\nimplemented in high dimensions, including phase transitions in the convergence\nrates in various $n$ and $p$ regimes, and a universality result across an\nentire spectrum of models. We also show that convergence problems effectively\neliminate the apparent safeguard of geometric ergodicity. We then demonstrate\ntheoretical principles by which MCMCs can be constructed and analyzed to yield\nbounded geometric convergence rates even as the dimension $p$ grows without\nbound. Additionally, we propose a diagnostic tool for establishing convergence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 01:11:51 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 01:15:22 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Rajaratnam", "Bala", ""], ["Sparks", "Doug", ""]]}, {"id": "1508.01090", "submitter": "Alina Astrakova", "authors": "Alina Astrakova and Dean S. Oliver and Christian Lantu\\'ejoul", "title": "Truncation map estimation based on bivariate probabilities and\n  validation for the truncated plurigaussian model", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The truncated plurigaussian model is often used to simulate the spatial\ndistribution of random categorical variables such as geological facies. The\nproblems addressed in this paper are the estimation of parameters of the\ntruncation map for the truncated plurigaussian model. Unlike standard\ntruncation maps, in this paper a colored Voronoi tessellation with number of\nnodes, locations of nodes, and category associated with each node all treated\nas unknowns in the optimization. Parameters were adjusted to match categorical\nbivariate unit-lag probabilities, which were obtained from a larger pattern\njoint distribution estimates from the Bayesian maximum-entropy approach\nconditioned to the unit-lag probabilities. The distribution of categorical\nvariables generated from the estimated truncation map was close to the target\nunit-lag bivariate probabilities. The validation of the predictive performance\nof the model is evaluated using scoring rules, and conditioning of the latent\nGaussian fields to log-data is generalized for the case when the truncated\nbigaussian model is governed by a colored Voronoi tessellation of the\ntruncation map.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:40:22 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 07:21:22 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Astrakova", "Alina", ""], ["Oliver", "Dean S.", ""], ["Lantu\u00e9joul", "Christian", ""]]}, {"id": "1508.01101", "submitter": "Kamil Jurczak", "authors": "Kamil Jurczak", "title": "Weak convergence of the empirical spectral distribution of\n  ultra-high-dimensional banded sample covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we investigate high-dimensional banded sample covariance\nmatrices under the regime that the sample size $n$, the dimension $p$ and the\nbandwidth $d$ tend simultaneously to infinity such that $$n/p\\to 0 \\ \\\n\\text{and} \\ \\ 2d/n\\to y>0.$$ It is shown that the empirical spectral\ndistribution of those matrices almost surely converges weakly to some\ndeterministic probability measure which is characterized by its moments.\nCertain restricted compositions of natural numbers play a crucial role in the\nevaluation of the expected moments of the empirical spectral distribution.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 15:17:04 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 07:02:54 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Jurczak", "Kamil", ""]]}, {"id": "1508.01131", "submitter": "Ruiyan Luo", "authors": "Ruiyan Luo and Xin Qi", "title": "Asymptotic optimality of sparse linear discriminant analysis with\n  arbitrary number of classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sparse linear discriminant analysis (LDA) methods have been proposed to\novercome the major problems of the classic LDA in high-dimensional settings.\nHowever, the asymptotic optimality results are limited to the case that there\nare only two classes, which is due to the fact that the classification boundary\nof LDA is a hyperplane and explicit formulas exist for the classification error\nin this case. In the situation where there are more than two classes, the\nclassification boundary is usually complicated and no explicit formulas for the\nclassification errors exist. In this paper, we consider the asymptotic\noptimality in the high-dimensional settings for a large family of linear\nclassification rules with arbitrary number of classes under the situation of\nmultivariate normal distribution. Our main theorem provides easy-to-check\ncriteria for the asymptotic optimality of a general classification rule in this\nfamily as dimensionality and sample size both go to infinity and the number of\nclasses is arbitrary. We establish the corresponding convergence rates. The\ngeneral theory is applied to the classic LDA and the extensions of two recently\nproposed sparse LDA methods to obtain the asymptotic optimality. We conduct\nsimulation studies on the extended methods in various settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 17:04:29 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Luo", "Ruiyan", ""], ["Qi", "Xin", ""]]}, {"id": "1508.01146", "submitter": "Michael Beyer", "authors": "Michael E. Beyer", "title": "Minimizing the CDF Path Length: A Novel Perspective on Uniformity and\n  Uncertainty of Bounded Distributions", "comments": "Draft Paper/Idea Sketch. 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An index of uniformity is developed as an alternative to the maximum-entropy\nprinciple for selecting continuous, differentiable probability distributions\n$\\mathcal{P}$ subject to constraints $C$. The uniformity index developed in\nthis paper is motivated by the observation that among all differentiable\nprobability distributions defined on a finite interval $[a,b] \\in \\mathbb{R}$,\nit is the uniform probability distribution that minimizes the path length of\nthe associated cumulative distribution function $F_{\\mathcal{P}}$ on $[a,b]$.\nThis intuition is extended to situations where there are constraints on the\nallowable probability distributions. In particular, constraints on the first\nand second raw moments of a distribution are discussed in detail, including the\nanalytical form of the solutions and numerical studies of particular examples.\nThe resulting \"shortest path\" distributions are found to be decidedly more\nheavy-tailed than the associated maximum-entropy distributions, suggesting that\nentropy and \"CDF path length\" measure two different aspects of uncertainty for\nbounded distributions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 17:45:41 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 13:15:26 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Beyer", "Michael E.", ""]]}, {"id": "1508.01264", "submitter": "Michael Kane", "authors": "Michelle DeVeaux, Michael J. Kane, and Daniel Zelterman", "title": "A Stopped Negative Binomial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new discrete distribution suggested by curtailed\nsampling rules common in early-stage clinical trials. We derive the\ndistribution of the smallest number of independent Bernoulli(p) trials needed\nin order to observe either s successes or t failures. The closed form\nexpression for the distribution as well as the compound distribution are\nderived. Properties of the distribution are shown and discussed. A case study\nis presented showing how the distribution can be used to monitor sequential\nenrollment of clinical trials with binary outcomes as well as providing\npost-hoc analysis of completed trials.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 02:24:27 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 17:47:39 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 15:28:45 GMT"}, {"version": "v4", "created": "Tue, 16 Aug 2016 20:00:23 GMT"}, {"version": "v5", "created": "Fri, 19 Aug 2016 19:32:55 GMT"}, {"version": "v6", "created": "Thu, 8 Sep 2016 15:40:18 GMT"}, {"version": "v7", "created": "Tue, 8 Aug 2017 19:35:02 GMT"}, {"version": "v8", "created": "Wed, 14 Feb 2018 20:59:59 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["DeVeaux", "Michelle", ""], ["Kane", "Michael J.", ""], ["Zelterman", "Daniel", ""]]}, {"id": "1508.01358", "submitter": "Lu Wei Dr.", "authors": "Lu Wei and Jukka Corander", "title": "Asymptotic Matrix Variate von-Mises Fisher and Bingham Distributions\n  with Applications", "comments": "This paper has been withdrawn by the authors due to a crucial error\n  in numerical experiment setup", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability distributions in Stiefel manifold such as the von-Mises Fisher\nand Bingham distributions find diverse applications in signal processing and\nother applied sciences. Use of these statistical models in practice is\ncomplicated by the difficulties in numerical evaluation of their normalization\nconstants. In this letter, we derive asymptotical approximations to the\nnormalization constants via recent results in random matrix theory. The derived\napproximations take simple forms and are reasonably accurate in regimes of\npractical interest. As an application, we show that the proposed analytical\nresults lead to a remarkably reduction of the sampling complexity compared to\nexisting simulation based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 10:55:27 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 13:41:48 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wei", "Lu", ""], ["Corander", "Jukka", ""]]}, {"id": "1508.01661", "submitter": "Leopold S\\\"ogner", "authors": "Jaroslava Hlouskova and Leopold S\\\"ogner", "title": "GMM Estimation of Affine Term Structure Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates parameter estimation of affine term structure\nmodels by means of the generalized method of moments. Exact moments of the\naffine latent process as well as of the yields are obtained by using results\nderived for p-polynomial processes. Then the generalized method of moments,\ncombined with Quasi-Bayesian methods, is used to get reliable parameter\nestimates and to perform inference. After a simulation study, the estimation\nprocedure is applied to empirical interest rate data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 11:29:53 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Hlouskova", "Jaroslava", ""], ["S\u00f6gner", "Leopold", ""]]}, {"id": "1508.01681", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Tianwen Wei and Basad Ali Hussain Al-sarray", "title": "Joint estimation and model order selection for one dimensional ARMA\n  models via convex optimization: a nuclear norm penalization approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating ARMA models is computationally interesting due to\nthe nonconcavity of the log-likelihood function. Recent results were based on\nthe convex minimization. Joint model selection using penalization by a convex\nnorm, e.g. the nuclear norm of a certain matrix related to the state space\nformulation was extensively studied from a computational viewpoint. The goal of\nthe present short note is to present a theoretical study of a nuclear norm\npenalization based variant of the method of\n\\cite{Bauer:Automatica05,Bauer:EconTh05} under the assumption of a Gaussian\nnoise process.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 13:21:15 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Wei", "Tianwen", ""], ["Al-sarray", "Basad Ali Hussain", ""]]}, {"id": "1508.01731", "submitter": "Rasool Roozegar Dr", "authors": "Rasool Roozegar", "title": "Certain Family of Some Beta Distributions Arising from Distribution of\n  Randomly Weighted Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We give the exact distribution of the average of n independent beta random\nvariables weighted by the selected cuts of (0, 1) by the order statistics of a\nrandom sample of size n-1 from the uniform distribution U(0,1), for each n. A\nnew integral transformation that is similar to generalized Stieltjes transform\nis given with various properties. The result of Soltani and Roozegar [On\ndistribution of randomly ordered uniform incremental weighted averages: Divided\ndifference approach. Statist Probab Lett. 2012, 82(5):1012-1020] with this new\ntransform and also integral representation of the Gauss-hypergeometric function\nin some parts are employed to achieve the exact distribution. Several examples\nof the new family are investigated.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:46:09 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Roozegar", "Rasool", ""]]}, {"id": "1508.01819", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Peter J. Bickel", "title": "Spectral Clustering and Block Models: A Review And A New Algorithm", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on spectral clustering of unlabeled graphs and review some results\non clustering methods which achieve weak or strong consistent identification in\ndata generated by such models. We also present a new algorithm which appears to\nperform optimally both theoretically using asymptotic theory and empirically.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 21:11:41 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1508.01902", "submitter": "Teo Sharia", "authors": "Teo Sharia and Lei Zhong", "title": "Rate of Convergence of Truncated Stochastic Approximation Procedures\n  with Moving Bounds", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with stochastic approximation procedures having three\nmain characteristics: truncations with random moving bounds, a matrix valued\nrandom step-size sequence, and a dynamically changing random regression\nfunction. We study convergence and rate of convergence. Main results are\nsupplemented with corollaries to establish various sets of sufficient\nconditions, with the main emphases on the parametric statistical estimation.\nThe theory is illustrated by examples and special cases.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 13:29:12 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 13:22:32 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Sharia", "Teo", ""], ["Zhong", "Lei", ""]]}, {"id": "1508.01922", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Peter Radchenko", "title": "The Discrete Dantzig Selector: Estimating Sparse Linear Models via Mixed\n  Integer Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel high-dimensional linear regression estimator: the Discrete\nDantzig Selector, which minimizes the number of nonzero regression coefficients\nsubject to a budget on the maximal absolute correlation between the features\nand residuals. Motivated by the significant advances in integer optimization\nover the past 10-15 years, we present a Mixed Integer Linear Optimization\n(MILO) approach to obtain certifiably optimal global solutions to this\nnonconvex optimization problem. The current state of algorithmics in integer\noptimization makes our proposal substantially more computationally attractive\nthan the least squares subset selection framework based on integer quadratic\noptimization, recently proposed in [8] and the continuous nonconvex quadratic\noptimization framework of [33]. We propose new discrete first-order methods,\nwhich when paired with state-of-the-art MILO solvers, lead to good solutions\nfor the Discrete Dantzig Selector problem for a given computational budget. We\nillustrate that our integrated approach provides globally optimal solutions in\nsignificantly shorter computation times, when compared to off-the-shelf MILO\nsolvers. We demonstrate both theoretically and empirically that in a wide range\nof regimes the statistical properties of the Discrete Dantzig Selector are\nsuperior to those of popular $\\ell_{1}$-based approaches. We illustrate that\nour approach can handle problem instances with p = 10,000 features with\ncertifiable optimality making it a highly scalable combinatorial variable\nselection approach in sparse linear modeling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 16:13:07 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 00:52:56 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 05:48:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "1508.01928", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicol\\'as Garc\\'ia Trillos and Dejan Slep\\v{c}ev", "title": "A variational approach to the consistency of spectral clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the consistency of spectral approaches to data\nclustering. We consider clustering of point clouds obtained as samples of a\nground-truth measure. A graph representing the point cloud is obtained by\nassigning weights to edges based on the distance between the points they\nconnect. We investigate the spectral convergence of both unnormalized and\nnormalized graph Laplacians towards the appropriate operators in the continuum\ndomain. We obtain sharp conditions on how the connectivity radius can be scaled\nwith respect to the number of sample points for the spectral convergence to\nhold.\n  We also show that the discrete clusters obtained via spectral clustering\nconverge towards a continuum partition of the ground truth measure. Such\ncontinuum partition minimizes a functional describing the continuum analogue of\nthe graph-based spectral partitioning. Our approach, based on variational\nconvergence, is general and flexible.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 17:14:51 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Trillos", "Nicol\u00e1s Garc\u00eda", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1508.01939", "submitter": "Xi Luo", "authors": "Florentina Bunea, Christophe Giraud, Xi Luo, Martin Royer, Nicolas\n  Verzelen", "title": "Model Assisted Variable Clustering: Minimax-optimal Recovery and\n  Algorithms", "comments": "Maintext: 38 pages; supplementary information: 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering defines population level clusters relative to a model\nthat embeds notions of similarity. Algorithms tailored to such models yield\nestimated clusters with a clear statistical interpretation. We take this view\nhere and introduce the class of G-block covariance models as a background model\nfor variable clustering. In such models, two variables in a cluster are deemed\nsimilar if they have similar associations will all other variables. This can\narise, for instance, when groups of variables are noise corrupted versions of\nthe same latent factor. We quantify the difficulty of clustering data generated\nfrom a G-block covariance model in terms of cluster proximity, measured with\nrespect to two related, but different, cluster separation metrics. We derive\nminimax cluster separation thresholds, which are the metric values below which\nno algorithm can recover the model-defined clusters exactly, and show that they\nare different for the two metrics. We therefore develop two algorithms, COD and\nPECOK, tailored to G-block covariance models, and study their\nminimax-optimality with respect to each metric. Of independent interest is the\nfact that the analysis of the PECOK algorithm, which is based on a corrected\nconvex relaxation of the popular K-means algorithm, provides the first\nstatistical analysis of such algorithms for variable clustering. Additionally,\nwe contrast our methods with another popular clustering method, spectral\nclustering, specialized to variable clustering, and show that ensuring exact\ncluster recovery via this method requires clusters to have a higher separation,\nrelative to the minimax threshold. Extensive simulation studies, as well as our\ndata analyses, confirm the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 18:25:16 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 18:30:48 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 03:13:09 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 03:32:02 GMT"}, {"version": "v5", "created": "Thu, 13 Dec 2018 03:33:17 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Bunea", "Florentina", ""], ["Giraud", "Christophe", ""], ["Luo", "Xi", ""], ["Royer", "Martin", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1508.01964", "submitter": "Sebastian Roch", "authors": "Sebastien Roch and Allan Sly", "title": "Phase transition in the sample complexity of likelihood-based phylogeny\n  inference", "comments": "To appear in Probability Theory and Related Fields", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing evolutionary trees from molecular sequence data is a\nfundamental problem in computational biology. Stochastic models of sequence\nevolution are closely related to spin systems that have been extensively\nstudied in statistical physics and that connection has led to important\ninsights on the theoretical properties of phylogenetic reconstruction\nalgorithms as well as the development of new inference methods. Here, we study\nmaximum likelihood, a classical statistical technique which is perhaps the most\nwidely used in phylogenetic practice because of its superior empirical\naccuracy.\n  At the theoretical level, except for its consistency, that is, the guarantee\nof eventual correct reconstruction as the size of the input data grows, much\nremains to be understood about the statistical properties of maximum likelihood\nin this context. In particular, the best bounds on the sample complexity or\nsequence-length requirement of maximum likelihood, that is, the amount of data\nrequired for correct reconstruction, are exponential in the number, $n$, of\ntips---far from known lower bounds based on information-theoretic arguments.\nHere we close the gap by proving a new upper bound on the sequence-length\nrequirement of maximum likelihood that matches up to constants the known lower\nbound for some standard models of evolution.\n  More specifically, for the $r$-state symmetric model of sequence evolution on\na binary phylogeny with bounded edge lengths, we show that the sequence-length\nrequirement behaves logarithmically in $n$ when the expected amount of mutation\nper edge is below what is known as the Kesten-Stigum threshold. In general, the\nsequence-length requirement is polynomial in $n$. Our results imply moreover\nthat the maximum likelihood estimator can be computed efficiently on randomly\ngenerated data provided sequences are as above.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 22:30:14 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 19:07:38 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Roch", "Sebastien", ""], ["Sly", "Allan", ""]]}, {"id": "1508.01967", "submitter": "Ezequiel Smucler", "authors": "Ezequiel Smucler and V\\'ictor J. Yohai", "title": "Robust and sparse estimators for linear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression estimators are a popular tool for the analysis of sparse\nand high-dimensional data sets. However, penalized regression estimators\ndefined using an unbounded loss function can be very sensitive to the presence\nof outlying observations, especially high leverage outliers. Moreover, it can\nbe particularly challenging to detect outliers in high-dimensional data sets.\nThus, robust estimators for sparse and high-dimensional linear regression\nmodels are in need. In this paper, we study the robust and asymptotic\nproperties of MM-Bridge and adaptive MM-Bridge estimators: $\\ell_q$-penalized\nMM-estimators of regression and MM-estimators with an adaptive $\\ell_t$\npenalty. For the case of a fixed number of covariates, we derive the asymptotic\ndistribution of MM-Bridge estimators for all $q>0$. We prove that for $q<1$\nMM-Bridge estimators can have the oracle property defined in Fan and Li (2001).\nWe prove that for all $t\\leq 1$ adaptive MM-Bridge estimators can have the\noracle property. The advantages of our proposed estimators are demonstrated\nthrough an extensive simulation study and the analysis of a real\nhigh-dimensional data set.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 22:48:10 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 16:41:01 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 21:02:42 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 19:29:53 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Smucler", "Ezequiel", ""], ["Yohai", "V\u00edctor J.", ""]]}, {"id": "1508.02043", "submitter": "Shankar Bhamidi", "authors": "Shankar Bhamidi and Jimmy Jin and Andrew Nobel", "title": "Change point detection in Network models: Preferential attachment and\n  long range dependence", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by empirical data on real world complex networks, the last few years\nhave seen an explosion in proposed generative models to understand and explain\nobserved properties of real world networks, including power law degree\ndistribution and \"small world\" distance scaling. In this context, a natural\nquestion is the phenomenon of {\\it change point}, understanding how abrupt\nchanges in parameters driving the network model change structural properties of\nthe network. We study this phenomenon in one popular class of dynamically\nevolving networks: preferential attachment models. We derive asymptotic\nproperties of various functionals of the network including the degree\ndistribution as well as maximal degree asymptotics, in essence showing that the\nchange point does effect the degree distribution but does {\\bf not} change the\ndegree exponent. This provides further evidence for long range dependence and\nsensitive dependence of the evolution of the process on the initial evolution\nof the process in such self-reinforced systems. We then propose an estimator\nfor the change point and prove consistency properties of this estimator. The\nmethodology developed highlights the effect of the non-ergodic nature of the\nevolution of the network on classical change point estimators.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 15:44:22 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Bhamidi", "Shankar", ""], ["Jin", "Jimmy", ""], ["Nobel", "Andrew", ""]]}, {"id": "1508.02201", "submitter": "Lizhen Lin", "authors": "Lizhen Lin, Brian St. Thomas, Hongtu Zhu, and David B. Dunson", "title": "Extrinsic local regression on manifold-valued data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extrinsic regression framework for modeling data with manifold\nvalued responses and Euclidean predictors. Regression with manifold responses\nhas wide applications in shape analysis, neuroscience, medical imaging and many\nother areas. Our approach embeds the manifold where the responses lie onto a\nhigher dimensional Euclidean space, obtains a local regression estimate in that\nspace, and then projects this estimate back onto the image of the manifold.\nOutside the regression setting both intrinsic and extrinsic approaches have\nbeen proposed for modeling i.i.d manifold-valued data. However, to our\nknowledge our work is the first to take an extrinsic approach to the regression\nproblem. The proposed extrinsic regression framework is general,\ncomputationally efficient and theoretically appealing. Asymptotic distributions\nand convergence rates of the extrinsic regression estimates are derived and a\nlarge class of examples are considered indicating the wide applicability of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 11:08:14 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Lin", "Lizhen", ""], ["Thomas", "Brian St.", ""], ["Zhu", "Hongtu", ""], ["Dunson", "David B.", ""]]}, {"id": "1508.02449", "submitter": "Houman Owhadi", "authors": "Houman Owhadi and Clint Scovel", "title": "Towards Machine Wald", "comments": "37 pages", "journal-ref": null, "doi": "10.1007/978-3-319-11259-6_3-1", "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past century has seen a steady increase in the need of estimating and\npredicting complex systems and making (possibly critical) decisions with\nlimited information. Although computers have made possible the numerical\nevaluation of sophisticated statistical models, these models are still designed\n\\emph{by humans} because there is currently no known recipe or algorithm for\ndividing the design of a statistical model into a sequence of arithmetic\noperations. Indeed enabling computers to \\emph{think} as \\emph{humans} have the\nability to do when faced with uncertainty is challenging in several major ways:\n(1) Finding optimal statistical models remains to be formulated as a well posed\nproblem when information on the system of interest is incomplete and comes in\nthe form of a complex combination of sample data, partial knowledge of\nconstitutive relations and a limited description of the distribution of input\nrandom variables. (2) The space of admissible scenarios along with the space of\nrelevant information, assumptions, and/or beliefs, tend to be infinite\ndimensional, whereas calculus on a computer is necessarily discrete and finite.\nWith this purpose, this paper explores the foundations of a rigorous framework\nfor the scientific computation of optimal statistical estimators/models and\nreviews their connections with Decision Theory, Machine Learning, Bayesian\nInference, Stochastic Optimization, Robust Optimization, Optimal Uncertainty\nQuantification and Information Based Complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 22:48:51 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 22:41:50 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Owhadi", "Houman", ""], ["Scovel", "Clint", ""]]}, {"id": "1508.02473", "submitter": "Jie Ding", "authors": "Jie Ding, Vahid Tarokh, Yuhong Yang", "title": "Bridging AIC and BIC: a new criterion for autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new criterion to determine the order of an autoregressive\nmodel fitted to time series data. It has the benefits of the two well-known\nmodel selection techniques, the Akaike information criterion and the Bayesian\ninformation criterion. When the data is generated from a finite order\nautoregression, the Bayesian information criterion is known to be consistent,\nand so is the new criterion. When the true order is infinity or suitably high\nwith respect to the sample size, the Akaike information criterion is known to\nbe efficient in the sense that its prediction performance is asymptotically\nequivalent to the best offered by the candidate models; in this case, the new\ncriterion behaves in a similar manner. Different from the two classical\ncriteria, the proposed criterion adaptively achieves either consistency or\nefficiency depending on the underlying true model. In practice where the\nobserved time series is given without any prior information about the model\nspecification, the proposed order selection criterion is more flexible and\nrobust compared with classical approaches. Numerical results are presented\ndemonstrating the adaptivity of the proposed technique when applied to various\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 02:49:45 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 03:19:05 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 14:50:08 GMT"}, {"version": "v4", "created": "Wed, 24 Aug 2016 16:10:33 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Ding", "Jie", ""], ["Tarokh", "Vahid", ""], ["Yang", "Yuhong", ""]]}, {"id": "1508.02489", "submitter": "Zheng Zhang", "authors": "Zheng Zhang and Hung Dinh Nguyen and Konstantin Turitsyn and Luca\n  Daniel", "title": "Probabilistic Power Flow Computation via Low-Rank and Sparse Tensor\n  Recovery", "comments": "8 pages, 10 figures, submitted to IEEE Trans. Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tensor-recovery method to solve probabilistic power\nflow problems. Our approach generates a high-dimensional and sparse generalized\npolynomial-chaos expansion that provides useful statistical information. The\nresult can also speed up other essential routines in power systems (e.g.,\nstochastic planning, operations and controls).\n  Instead of simulating a power flow equation at all quadrature points, our\napproach only simulates an extremely small subset of samples. We suggest a\nmodel to exploit the underlying low-rank and sparse structure of\nhigh-dimensional simulation data arrays, making our technique applicable to\npower systems with many random parameters. We also present a numerical method\nto solve the resulting nonlinear optimization problem.\n  Our algorithm is implemented in MATLAB and is verified by several benchmarks\nin MATPOWER $5.1$. Accurate results are obtained for power systems with up to\n$50$ independent random parameters, with a speedup factor up to $9\\times\n10^{20}$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 05:08:25 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Zhang", "Zheng", ""], ["Nguyen", "Hung Dinh", ""], ["Turitsyn", "Konstantin", ""], ["Daniel", "Luca", ""]]}, {"id": "1508.02651", "submitter": "Konstantinos Spiliopoulos", "authors": "Alexandra Chronopoulou and Konstantinos Spiliopoulos", "title": "Sequential Monte Carlo for fractional Stochastic Volatility Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a fractional stochastic volatility model, that is a\nmodel in which the volatility may exhibit a long-range dependent or a\nrough/antipersistent behavior. We propose a dynamic sequential Monte Carlo\nmethodology that is applicable to both long memory and antipersistent processes\nin order to estimate the volatility as well as the unknown parameters of the\nmodel. We establish a central limit theorem for the state and parameter filters\nand we study asymptotic properties (consistency and asymptotic normality) for\nthe filter. We illustrate our results with a simulation study and we apply our\nmethod to estimating the volatility and the parameters of a long-range\ndependent model for S&P 500 data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 16:36:53 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 20:14:32 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chronopoulou", "Alexandra", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1508.02757", "submitter": "Andrea Montanari", "authors": "Adel Javanmard and Andrea Montanari", "title": "De-biasing the Lasso: Optimal Sample Size for Gaussian Designs", "comments": "57 pages, 4 pdf figures (v3 contains stronger statement for unknown\n  covariance, and several new applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing statistical inference in high-dimension is an outstanding\nchallenge. A major source of difficulty is the absence of precise information\non the distribution of high-dimensional estimators. Here, we consider linear\nregression in the high-dimensional regime $p\\gg n$. In this context, we would\nlike to perform inference on a high-dimensional parameters vector\n$\\theta^*\\in{\\mathbb R}^p$. Important progress has been achieved in computing\nconfidence intervals for single coordinates $\\theta^*_i$. A key role in these\nnew methods is played by a certain debiased estimator $\\hat{\\theta}^{\\rm d}$\nthat is constructed from the Lasso. Earlier work establishes that, under\nsuitable assumptions on the design matrix, the coordinates of\n$\\hat{\\theta}^{\\rm d}$ are asymptotically Gaussian provided $\\theta^*$ is\n$s_0$-sparse with $s_0 = o(\\sqrt{n}/\\log p )$. The condition $s_0 = o(\\sqrt{n}/\n\\log p )$ is stronger than the one for consistent estimation, namely $s_0 =\no(n/ \\log p)$. We study Gaussian designs with known or unknown population\ncovariance. When the covariance is known, we prove that the debiased estimator\nis asymptotically Gaussian under the nearly optimal condition $s_0 = o(n/ (\\log\np)^2)$. Note that earlier work was limited to $s_0 = o(\\sqrt{n}/\\log p)$ even\nfor perfectly known covariance. The same conclusion holds if the population\ncovariance is unknown but can be estimated sufficiently well, e.g. under the\nsame sparsity conditions on the inverse covariance as assumed by earlier work.\nFor intermediate regimes, we describe the trade-off between sparsity in the\ncoefficients and in the inverse covariance of the design. We further discuss\nseveral applications of our results to high-dimensional inference. In\nparticular, we propose a new estimator that is minimax optimal up to a factor\n$1+o_n(1)$ for i.i.d. Gaussian designs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:38:13 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 00:24:06 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 20:07:22 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1508.02758", "submitter": "Enkelejd Hashorva", "authors": "P. Albin, E. Hashorva, L. Ji, C. Ling", "title": "Extremes and Limit Theorems for Difference of Chi-type processes", "comments": "To appear in ESAIM P&S", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{\\zeta_{m,k}^{(\\kappa)}(t), t \\ge0\\}, \\kappa>0$ be random processes\ndefined as the differences of two independent stationary chi-type processes\nwith $m$ and $k$ degrees of freedom. In applications such as physical sciences\nand engineering dealing with structure reliability, of interest is the\napproximation of the probability that the random process\n$\\zeta_{m,k}^{(\\kappa)}$ stays in some safety region up to a fixed time $T$. In\nthis paper we derive the asymptotics of $\\mathbb{P}\\{\\sup_{t\\in[0,\nT]}\\zeta_{m,k}^{(\\kappa)}(t)> u\\}, {u\\to\\infty}$ under some assumptions on the\ncovariance structures of the underlying Gaussian processes. Further, we\nestablish a Berman sojourn limit theorem and a Gumbel limit result.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 21:41:04 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 07:44:09 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Albin", "P.", ""], ["Hashorva", "E.", ""], ["Ji", "L.", ""], ["Ling", "C.", ""]]}, {"id": "1508.02925", "submitter": "Hao Han", "authors": "Hao Han, Wei Zhu", "title": "RCR: Robust Compound Regression for Robust Estimation of\n  Errors-in-Variables Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The errors-in-variables (EIV) regression model, being more realistic by\naccounting for measurement errors in both the dependent and the independent\nvariables, is widely adopted in applied sciences. The traditional EIV model\nestimators, however, can be highly biased by outliers and other departures from\nthe underlying assumptions. In this paper, we develop a novel nonparametric\nregression approach - the robust compound regression (RCR) analysis method for\nthe robust estimation of EIV models. We first introduce a robust and efficient\nestimator called least sine squares (LSS). Taking full advantage of both the\nnew LSS method and the compound regression analysis method developed in our own\ngroup, we subsequently propose the RCR approach as a generalization of those\ntwo, which provides a robust counterpart of the entire class of the maximum\nlikelihood estimation (MLE) solutions of the EIV model, in a 1-1 mapping.\nTechnically, our approach gives users the flexibility to select from a class of\nRCR estimates the optimal one with a predefined regression efficiency criterion\nsatisfied. Simulation studies and real-life examples are provided to illustrate\nthe effectiveness of the RCR approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:19:24 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Han", "Hao", ""], ["Zhu", "Wei", ""]]}, {"id": "1508.02933", "submitter": "Robert Nishihara", "authors": "Robert Nishihara, David Lopez-Paz, L\\'eon Bottou", "title": "No Regret Bound for Extreme Bandits", "comments": "11 pages, International Conference on Artificial Intelligence and\n  Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for hyperparameter optimization abound, all of which work well\nunder different and often unverifiable assumptions. Motivated by the general\nchallenge of sequentially choosing which algorithm to use, we study the more\nspecific task of choosing among distributions to use for random hyperparameter\noptimization. This work is naturally framed in the extreme bandit setting,\nwhich deals with sequentially choosing which distribution from a collection to\nsample in order to minimize (maximize) the single best cost (reward). Whereas\nthe distributions in the standard bandit setting are primarily characterized by\ntheir means, a number of subtleties arise when we care about the minimal cost\nas opposed to the average cost. For example, there may not be a well-defined\n\"best\" distribution as there is in the standard bandit setting. The best\ndistribution depends on the rewards that have been obtained and on the\nremaining time horizon. Whereas in the standard bandit setting, it is sensible\nto compare policies with an oracle which plays the single best arm, in the\nextreme bandit setting, there are multiple sensible oracle models. We define a\nsensible notion of \"extreme regret\" in the extreme bandit setting, which\nparallels the concept of regret in the standard bandit setting. We then prove\nthat no policy can asymptotically achieve no extreme regret.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 14:31:49 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 21:45:51 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 18:20:50 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Nishihara", "Robert", ""], ["Lopez-Paz", "David", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1508.02973", "submitter": "Max Farrell", "authors": "Sebastian Calonico and Matias D. Cattaneo and Max H. Farrell", "title": "On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric\n  Inference", "comments": null, "journal-ref": "Journal of the American Statistical Association, 113(522), pp.\n  767-779, 2018", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric methods play a central role in modern empirical work. While\nthey provide inference procedures that are more robust to parametric\nmisspecification bias, they may be quite sensitive to tuning parameter choices.\nWe study the effects of bias correction on confidence interval coverage in the\ncontext of kernel density and local polynomial regression estimation, and prove\nthat bias correction can be preferred to undersmoothing for minimizing coverage\nerror and increasing robustness to tuning parameter choice. This is achieved\nusing a novel, yet simple, Studentization, which leads to a new way of\nconstructing kernel-based bias-corrected confidence intervals. In addition, for\npractical cases, we derive coverage error optimal bandwidths and discuss\neasy-to-implement bandwidth selectors. For interior points, we show that the\nMSE-optimal bandwidth for the original point estimator (before bias correction)\ndelivers the fastest coverage error decay rate after bias correction when\nsecond-order (equivalent) kernels are employed, but is otherwise suboptimal\nbecause it is too \"large\". Finally, for odd-degree local polynomial regression,\nwe show that, as with point estimation, coverage error adapts to boundary\npoints automatically when appropriate Studentization is used; however, the\nMSE-optimal bandwidth for the original point estimator is suboptimal. All the\nresults are established using valid Edgeworth expansions and illustrated with\nsimulated data. Our findings have important consequences for empirical work as\nthey indicate that bias-corrected confidence intervals, coupled with\nappropriate standard errors, have smaller coverage error and are less sensitive\nto tuning parameter choices in practically relevant cases where additional\nsmoothness is available.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:13:30 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 15:12:50 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 19:59:42 GMT"}, {"version": "v4", "created": "Thu, 10 Nov 2016 16:42:13 GMT"}, {"version": "v5", "created": "Wed, 8 Nov 2017 21:56:35 GMT"}, {"version": "v6", "created": "Wed, 7 Mar 2018 22:07:48 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Calonico", "Sebastian", ""], ["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""]]}, {"id": "1508.03002", "submitter": "Rui Castro", "authors": "Ery Arias-Castro, Rui M. Castro, Ervin T\\'anczos, Meng Wang", "title": "Distribution-Free Detection of Structured Anomalies: Permutation and\n  Rank-Based Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scan statistic is by far the most popular method for anomaly detection,\nbeing popular in syndromic surveillance, signal and image processing, and\ntarget detection based on sensor networks, among other applications. The use of\nthe scan statistics in such settings yields a hypothesis testing procedure,\nwhere the null hypothesis corresponds to the absence of anomalous behavior. If\nthe null distribution is known, then calibration of a scan-based test is\nrelatively easy, as it can be done by Monte Carlo simulation. When the null\ndistribution is unknown, it is less straightforward. We investigate two\nprocedures. The first one is a calibration by permutation and the other is a\nrank-based scan test, which is distribution-free and less sensitive to\noutliers. Furthermore, the rank scan test requires only a one-time calibration\nfor a given data size making it computationally much more appealing. In both\ncases, we quantify the performance loss with respect to an oracle scan test\nthat knows the null distribution. We show that using one of these calibration\nprocedures results in only a very small loss of power in the context of a\nnatural exponential family. This includes the classical normal location model,\npopular in signal processing, and the Poisson model, popular in syndromic\nsurveillance. We perform numerical experiments on simulated data further\nsupporting our theory and also on a real dataset from genomics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 17:15:35 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 22:58:42 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 09:49:37 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Castro", "Rui M.", ""], ["T\u00e1nczos", "Ervin", ""], ["Wang", "Meng", ""]]}, {"id": "1508.03283", "submitter": "JInglai Li", "authors": "Zhe Feng, Jinglai Li", "title": "An adaptive independence sampler MCMC algorithm for infinite dimensional\n  Bayesian inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering problems require to perform Bayesian\ninferences in function spaces, in which the unknowns are of infinite dimension.\nIn such problems, many standard Markov Chain Monte Carlo (MCMC) algorithms\nbecome arbitrary slow under the mesh refinement, which is referred to as being\ndimension dependent. In this work we develop an independence sampler based MCMC\nmethod for the infinite dimensional Bayesian inferences. We represent the\nproposal distribution as a mixture of a finite number of specially parametrized\nGaussian measures. We show that under the chosen parametrization, the resulting\nMCMC algorithm is dimension independent. We also design an efficient adaptive\nalgorithm to adjust the parameter values of the mixtures from the previous\nsamples. Finally we provide numerical examples to demonstrate the efficiency\nand robustness of the proposed method, even for problems with multimodal\nposterior distributions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:50:05 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 13:38:06 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Feng", "Zhe", ""], ["Li", "Jinglai", ""]]}, {"id": "1508.03323", "submitter": "Aurelien Alfonsi", "authors": "Aur\\'elien Alfonsi and Ahmed Kebaier and Cl\\'ement Rey", "title": "Maximum Likelihood Estimation for Wishart processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, there has been a growing interest to use Wishart\nprocesses for modelling, especially for financial applications. However, there\nare still few studies on the estimation of its parameters. Here, we study the\nMaximum Likelihood Estimator (MLE) in order to estimate the drift parameters of\na Wishart process. We obtain precise convergence rates and limits for this\nestimator in the ergodic case and in some nonergodic cases. We check that the\nMLE achieves the optimal convergence rate in each case. Motivated by this\nstudy, we also present new results on the Laplace transform that extend the\nrecent findings of Gnoatto and Grasselli and are of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 19:45:48 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 15:13:08 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Alfonsi", "Aur\u00e9lien", ""], ["Kebaier", "Ahmed", ""], ["Rey", "Cl\u00e9ment", ""]]}, {"id": "1508.03387", "submitter": "James Johndrow", "authors": "James E. Johndrow, Jonathan C. Mattingly, Sayan Mukherjee, David\n  Dunson", "title": "Optimal approximating Markov chains for Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov Chain Monte Carlo method is the dominant paradigm for posterior\ncomputation in Bayesian analysis. It is common to control computation time by\nmaking approximations to the Markov transition kernel. Comparatively little\nattention has been paid to computational optimality in these approximating\nMarkov Chains, or when such approximations are justified relative to obtaining\nshorter paths from the exact kernel. We give simple, sharp bounds for uniform\napproximations of uniformly mixing Markov chains. We then suggest a notion of\noptimality that incorporates computation time and approximation error, and use\nour bounds to make generalizations about properties of good approximations in\nthe uniformly mixing setting. The relevance of these properties is demonstrated\nin applications to a minibatching-based approximate MCMC algorithm for large\n$n$ logistic regression and low-rank approximations for Gaussian processes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 23:50:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 22:37:46 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 04:28:10 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Johndrow", "James E.", ""], ["Mattingly", "Jonathan C.", ""], ["Mukherjee", "Sayan", ""], ["Dunson", "David", ""]]}, {"id": "1508.03416", "submitter": "Nanang Susyanto", "authors": "Chris A.J. Klaassen and Nanang Susyanto", "title": "Semiparametrically Efficient Estimation of Constrained Euclidean\n  Parameters", "comments": "22 pages with 20 main pages and 2 pages supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a quite arbitrary (semi)parametric model with a Euclidean parameter\nof interest and assume that an asymptotically (semi)parametrically efficient\nestimator of it is given. If the parameter of interest is known to lie on a\ngeneral surface (image of a continuously differentiable vector valued\nfunction), we have a submodel in which this constrained Euclidean parameter may\nbe rewritten in terms of a lower-dimensional Euclidean parameter of interest.\nAn estimator of this underlying parameter is constructed based on the original\nestimator, and it is shown to be (semi)parametrically efficient. It is proved\nthat the efficient score function for the underlying parameter is determined by\nthe efficient score function for the original parameter and the Jacobian of the\nfunction defining the general surface, via a chain rule for score functions.\nEfficient estimation of the constrained Euclidean parameter itself is\nconsidered as well.\n  Our general estimation method is applied to location-scale, Gaussian copula\nand semiparametric regression models, and to parametric models under linear\nrestrictions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 04:03:32 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Klaassen", "Chris A. J.", ""], ["Susyanto", "Nanang", ""]]}, {"id": "1508.03606", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar and Johannes Rauh", "title": "Hierarchical Models as Marginals of Hierarchical Models", "comments": "18 pages, 4 figures, 2 tables, WUPES'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the representation of hierarchical models in terms of\nmarginals of other hierarchical models with smaller interactions. We focus on\nbinary variables and marginals of pairwise interaction models whose hidden\nvariables are conditionally independent given the visible variables. In this\ncase the problem is equivalent to the representation of linear subspaces of\npolynomials by feedforward neural networks with soft-plus computational units.\nWe show that every hidden variable can freely model multiple interactions among\nthe visible variables, which allows us to generalize and improve previous\nresults. In particular, we show that a restricted Boltzmann machine with less\nthan $[ 2(\\log(v)+1) / (v+1) ] 2^v-1$ hidden binary variables can approximate\nevery distribution of $v$ visible binary variables arbitrarily well, compared\nto $2^{v-1}-1$ from the best previously known result.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:56:00 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 19:48:07 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""]]}, {"id": "1508.03712", "submitter": "Ingo Steinwart", "authors": "Philipp Thomann, Ingo Steinwart, Nico Schmid", "title": "Towards an Axiomatic Approach to Hierarchical Clustering of Measures", "comments": null, "journal-ref": "Journal of Machine Learning Research. 16(Sep):1949-2002, 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some axioms for hierarchical clustering of probability measures\nand investigate their ramifications. The basic idea is to let the user\nstipulate the clusters for some elementary measures. This is done without the\nneed of any notion of metric, similarity or dissimilarity. Our main results\nthen show that for each suitable choice of user-defined clustering on\nelementary measures we obtain a unique notion of clustering on a large set of\ndistributions satisfying a set of additivity and continuity axioms. We\nillustrate the developed theory by numerous examples including some with and\nsome without a density.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 09:07:01 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Thomann", "Philipp", ""], ["Steinwart", "Ingo", ""], ["Schmid", "Nico", ""]]}, {"id": "1508.03744", "submitter": "Adityanand Guntuboyina", "authors": "Tony Cai and Adityanand Guntuboyina and Yuting Wei", "title": "Adaptive estimation of planar convex sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider adaptive estimation of an unknown planar compact,\nconvex set from noisy measurements of its support function on a uniform grid.\nBoth the problem of estimating the support function at a point and that of\nestimating the convex set are studied. Data-driven adaptive estimators are\nproposed and their optimality properties are established. For pointwise\nestimation, it is shown that the estimator optimally adapts to every compact,\nconvex set instead of a collection of large parameter spaces as in the\nconventional minimax theory of nonparametric estimation. For set estimation,\nthe estimators adaptively achieve the optimal rate of convergence. In both\nthese problems, our analysis makes no smoothness assumptions on the unknown\nsets.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 15:50:11 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Cai", "Tony", ""], ["Guntuboyina", "Adityanand", ""], ["Wei", "Yuting", ""]]}, {"id": "1508.03808", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Quantifying information transfer and mediation along causal pathways in\n  complex systems", "comments": "20 pages, 6 figures", "journal-ref": "Phys. Rev. E 92, 062829 (2015)", "doi": "10.1103/PhysRevE.92.062829", "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of information transfer have become a popular approach to analyze\ninteractions in complex systems such as the Earth or the human brain from\nmeasured time series. Recent work has focused on causal definitions of\ninformation transfer excluding effects of common drivers and indirect\ninfluences. While the former clearly constitutes a spurious causality, the aim\nof the present article is to develop measures quantifying different notions of\nthe strength of information transfer along indirect causal paths, based on\nfirst reconstructing the multivariate causal network (\\emph{Tigramite}\napproach). Another class of novel measures quantifies to what extent different\nintermediate processes on causal paths contribute to an interaction mechanism\nto determine pathways of causal information transfer. A rigorous mathematical\nframework allows for a clear information-theoretic interpretation that can also\nbe related to the underlying dynamics as proven for certain classes of\nprocesses. Generally, however, estimates of information transfer remain hard to\ninterpret for nonlinearly intertwined complex systems. But, if experiments or\nmathematical models are not available, measuring pathways of information\ntransfer within the causal dependency structure allows at least for an\nabstraction of the dynamics. The measures are illustrated on a climatological\nexample to disentangle pathways of atmospheric flow over Europe.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 10:30:57 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 12:46:43 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "1508.03852", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Venkat Chandrasekaran", "title": "Sufficient Dimension Reduction and Modeling Responses Conditioned on\n  Covariates: An Integrated Approach via Convex Optimization", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given observations of a collection of covariates and responses $(Y, X) \\in\n\\mathbb{R}^p \\times \\mathbb{R}^q$, sufficient dimension reduction (SDR)\ntechniques aim to identify a mapping $f: \\mathbb{R}^q \\rightarrow \\mathbb{R}^k$\nwith $k \\ll q$ such that $Y|f(X)$ is independent of $X$. The image $f(X)$\nsummarizes the relevant information in a potentially large number of covariates\n$X$ that influence the responses $Y$. In many contemporary settings, the number\nof responses $p$ is also quite large, in addition to a large number $q$ of\ncovariates. This leads to the challenge of fitting a succinctly parameterized\nstatistical model to $Y|f(X)$, which is a problem that is usually not addressed\nin a traditional SDR framework. In this paper, we present a computationally\ntractable convex relaxation based estimator for simultaneously (a) identifying\na linear dimension reduction $f(X)$ of the covariates that is sufficient with\nrespect to the responses, and (b) fitting several types of structured\nlow-dimensional models -- factor models, graphical models, latent-variable\ngraphical models -- to the conditional distribution of $Y|f(X)$. We analyze the\nconsistency properties of our estimator in a high-dimensional scaling regime.\nWe also illustrate the performance of our approach on a newsgroup dataset and\non a dataset consisting of financial asset prices.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 17:57:02 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 02:37:22 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Taeb", "Armeen", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1508.04070", "submitter": "Malgorzata Wojtys", "authors": "M. Wojty\\'s and G. Marra", "title": "Copula based generalized additive models with non-random sample\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-random sample selection is a commonplace amongst many empirical studies\nand it appears when an output variable of interest is available only for a\nrestricted non-random sub-sample of data. We introduce an extension of the\ngeneralized additive model which accounts for non-random sample selection by\nusing a selection equation. The proposed approach allows for different\ndistributions of the outcome variable, various dependence structures between\nthe (outcome and selection) equations through the use of copulae, and\nnonparametric effects on the responses. Parameter estimation is carried out\nwithin a penalized likelihood and simultaneous equation framework. We establish\nasymptotic theory for the proposed penalized spline estimators, which extends\nthe recent theoretical results for penalized splines in generalized additive\nmodels, such as those by Kauermann et al. (2009) and Yoshida & Naito (2014).\nThe empirical effectiveness of the approach is demonstrated through a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 15:57:53 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Wojty\u015b", "M.", ""], ["Marra", "G.", ""]]}, {"id": "1508.04175", "submitter": "Guang Cheng", "authors": "Zuofeng Shang, Botao Hao, Guang Cheng", "title": "Nonparametric Bayesian Aggregation for Massive Data", "comments": "To appear in Journal of Machine Learning Research. arXiv admin note:\n  text overlap with arXiv:1411.3686", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a set of scalable Bayesian inference procedures for a general\nclass of nonparametric regression models. Specifically, nonparametric Bayesian\ninferences are separately performed on each subset randomly split from a\nmassive dataset, and then the obtained local results are aggregated into global\ncounterparts. This aggregation step is explicit without involving any\nadditional computation cost. By a careful partition, we show that our\naggregated inference results obtain an oracle rule in the sense that they are\nequivalent to those obtained directly from the entire data (which are\ncomputationally prohibitive). For example, an aggregated credible ball achieves\ndesirable credibility level and also frequentist coverage while possessing the\nsame radius as the oracle ball.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 23:26:31 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:04:03 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 16:34:53 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shang", "Zuofeng", ""], ["Hao", "Botao", ""], ["Cheng", "Guang", ""]]}, {"id": "1508.04178", "submitter": "Qingyuan Zhao", "authors": "Jingshu Wang, Qingyuan Zhao, Trevor Hastie, Art B. Owen", "title": "Confounder Adjustment in Multiple Hypothesis Testing", "comments": "The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale studies in which thousands of significance tests are\nperformed simultaneously. In some of these studies, the multiple testing\nprocedure can be severely biased by latent confounding factors such as batch\neffects and unmeasured covariates that correlate with both primary variable(s)\nof interest (e.g. treatment variable, phenotype) and the outcome. Over the past\ndecade, many statistical methods have been proposed to adjust for the\nconfounders in hypothesis testing. We unify these methods in the same\nframework, generalize them to include multiple primary variables and multiple\nnuisance variables, and analyze their statistical properties. In particular, we\nprovide theoretical guarantees for RUV-4 and LEAPP, which correspond to two\ndifferent identification conditions in the framework: the first requires a set\nof \"negative controls\" that are known a priori to follow the null distribution;\nthe second requires the true non-nulls to be sparse. Two different estimators\nwhich are based on RUV-4 and LEAPP are then applied to these two scenarios. We\nshow that if the confounding factors are strong, the resulting estimators can\nbe asymptotically as powerful as the oracle estimator which observes the latent\nconfounding factors. For hypothesis testing, we show the asymptotic z-tests\nbased on the estimators can control the type I error. Numerical experiments\nshow that the false discovery rate is also controlled by the Benjamini-Hochberg\nprocedure when the sample size is reasonably large.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 23:45:02 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 03:37:10 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 03:28:20 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Wang", "Jingshu", ""], ["Zhao", "Qingyuan", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "1508.04216", "submitter": "Viktor Todorov", "authors": "Viktor Todorov", "title": "Jump activity estimation for pure-jump semimartingales via\n  self-normalized statistics", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1327 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1831-1864", "doi": "10.1214/15-AOS1327", "report-no": "IMS-AOS-AOS1327", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a nonparametric estimator of the jump-activity index $\\beta$ of a\n\"locally-stable\" pure-jump It\\^{o} semimartingale from discrete observations of\nthe process on a fixed time interval with mesh of the observation grid\nshrinking to zero. The estimator is based on the empirical characteristic\nfunction of the increments of the process scaled by local power variations\nformed from blocks of increments spanning shrinking time intervals preceding\nthe increments to be scaled. The scaling serves two purposes: (1) it controls\nfor the time variation in the jump compensator around zero, and (2) it ensures\nself-normalization, that is, that the limit of the characteristic\nfunction-based estimator converges to a nondegenerate limit which depends only\non $\\beta$. The proposed estimator leads to nontrivial efficiency gains over\nexisting estimators based on power variations. In the L\\'{e}vy case, the\nasymptotic variance decreases multiple times for higher values of $\\beta$. The\nlimiting asymptotic variance of the proposed estimator, unlike that of the\nexisting power variation based estimators, is constant. This leads to further\nefficiency gains in the case when the characteristics of the semimartingale are\nstochastic. Finally, in the limiting case of $\\beta=2$, which corresponds to\njump-diffusion, our estimator of $\\beta$ can achieve a faster rate than\nexisting estimators.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:31:28 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Todorov", "Viktor", ""]]}, {"id": "1508.04217", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu and Shinya Tanaka", "title": "Macroeconomic Forecasting and Variable Selection with a Very Large\n  Number of Predictors: A Penalized Regression Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies macroeconomic forecasting and variable selection using a\nfolded-concave penalized regression with a very large number of predictors. The\npenalized regression approach leads to sparse estimates of the regression\ncoefficients, and is applicable even if the dimensionality of the model is much\nlarger than the sample size. The first half of the paper discusses the\ntheoretical aspects of a folded-concave penalized regression when the model\nexhibits time series dependence. Specifically, we show the oracle inequality\nand the oracle property for ultrahigh-dimensional time-dependent regressors.\nThe latter half of the paper shows the validity of the penalized regression\nusing two motivating empirical applications. The first forecasts U.S. GDP with\nthe FRED-MD data using the MIDAS regression framework, where there are more\nthan 1000 covariates, while the sample size is at most 200. The second examines\nhow well the penalized regression screens the hidden portfolio with around 40\nstocks from more than 1800 potential stocks using NYSE stock price data. Both\napplications reveal that the penalized regression provides remarkable results\nin terms of forecasting performance and variable selection.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:33:06 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 06:44:29 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Uematsu", "Yoshimasa", ""], ["Tanaka", "Shinya", ""]]}, {"id": "1508.04520", "submitter": "Shuyang Bai", "authors": "Shuyang Bai, Murad S. Taqqu", "title": "Short-range dependent processes subordinated to the Gaussian may not be\n  strong mixing", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are all kinds of weak dependence. For example, strong mixing.\nShort-range dependence (SRD) is also a form of weak dependence. It occurs in\nthe context of processes that are subordinated to the Gaussian. Is a SRD\nprocess strong mixing if the underlying Gaussian process is long-range\ndependent? We show that this is not necessarily the case.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 03:57:08 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Bai", "Shuyang", ""], ["Taqqu", "Murad S.", ""]]}, {"id": "1508.04671", "submitter": "Amor Keziou", "authors": "Amor Keziou and Philippe Regnault", "title": "Semiparametric estimation of mutual information and related criteria :\n  optimal test of independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive independence tests by means of dependence measures thresholding in\na semiparametric context. Precisely, estimates of phi-mutual informations,\nassociated to phi-divergences between a joint distribution and the product\ndistribution of its margins, are derived through the dual representation of\nphi-divergences. The asymptotic properties of the proposed estimates are\nestablished, including consistency, asymptotic distributions and large\ndeviations principle. The obtained tests of independence are compared via their\nrelative asymptotic Bahadur efficiency and numerical simulations. It follows\nthat the proposed semiparametric Kullback-Leibler Mutual information test is\nthe optimal one. On the other hand, the proposed approach provides a new method\nfor estimating the Kullback-Leibler mutual information in a semiparametric\nsetting, as well as a model selection procedure in large class of dependency\nmodels including semiparametric copulas.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 15:08:23 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Keziou", "Amor", ""], ["Regnault", "Philippe", ""]]}, {"id": "1508.04720", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Hamed Firouzi, and Alfred O. Hero III", "title": "Quickest Detection for Changes in Maximal kNN Coherence of Random\n  Matrices", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of quickest detection of a change in the\nmaximal coherence between columns of a $n\\times p$ random matrix based on a\nsequence of matrix observations having a single unknown change point. The\nrandom matrix is assumed to have identically distributed rows and the maximal\ncoherence is defined as the largest of the $p \\choose 2$ correlation\ncoefficients associated with any row. Likewise, the $k$ nearest neighbor (kNN)\ncoherence is defined as the $k$-th largest of these correlation coefficients.\nThe forms of the pre- and post-change distributions of the observed matrices\nare assumed to belong to the family of elliptically contoured densities with\nsparse dispersion matrices but are otherwise unknown. A non-parametric stopping\nrule is proposed that is based on the maximal k-nearest neighbor sample\ncoherence between columns of each observed random matrix. This is a summary\nstatistic that is related to a test of the existence of a hub vertex in a\nsample correlation graph having a degree at least $k$. Performance bounds on\nthe delay and false alarm performance of the proposed stopping rule are\nobtained in the purely high dimensional regime where $p\\rightarrow \\infty$ and\n$n$ is fixed. When the pre-change dispersion matrix is diagonal it is shown\nthat, among all functions of the proposed summary statistic, the proposed\nstopping rule is asymptotically optimal under a minimax quickest change\ndetection (QCD) model as the stopping threshold approaches infinity. The theory\ndeveloped also applies to sequential hypothesis testing and fixed sample size\ntests.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 17:51:26 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 23:07:15 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 14:09:27 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Banerjee", "Taposh", ""], ["Firouzi", "Hamed", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1508.04742", "submitter": "Salimeh Yasaei Sekeh", "authors": "Salimeh Yasaei Sekeh", "title": "A short note on estimation of WCRE and WCE", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note the author uses order statistics to estimate WCRE and WCE in\nterms of empirical and survival functions. An example in both cases normal and\nexponential WFs is analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 19:02:01 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 11:59:57 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Sekeh", "Salimeh Yasaei", ""]]}, {"id": "1508.04812", "submitter": "Linxi Liu", "authors": "Linxi Liu and Wing Hung Wong", "title": "Multivariate Density Estimation via Adaptive Partitioning (II):\n  Posterior Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a class of non-parametric density estimators under\nBayesian settings. The estimators are piecewise constant functions on binary\npartitions. We analyze the concentration rate of the posterior distribution\nunder a suitable prior, and demonstrate that the rate does not directly depend\non the dimension of the problem. This paper can be viewed as an extension of a\nparallel work where the convergence rate of a related sieve MLE was\nestablished. Compared to the sieve MLE, the main advantage of the Bayesian\nmethod is that it can adapt to the unknown complexity of the true density\nfunction, thus achieving the optimal convergence rate without artificial\nconditions on the density.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 22:06:43 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Liu", "Linxi", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1508.04905", "submitter": "Alain Celisse", "authors": "Alain Celisse (MODAL, LPP), Tristan Mary-Huard (GQE-Le Moulon)", "title": "Theoretical analysis of cross-validation for estimating the risk of the\n  k-Nearest Neighbor classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work aims at deriving theoretical guaranties on the behavior of\nsome cross-validation procedures applied to the $k$-nearest neighbors ($k$NN)\nrule in the context of binary classification. Here we focus on the\nleave-$p$-out cross-validation (L$p$O) used to assess the performance of the\n$k$NN classifier. Remarkably this L$p$O estimator can be efficiently computed\nin this context using closed-form formulas derived by\n\\cite{CelisseMaryHuard11}. We describe a general strategy to derive moment and\nexponential concentration inequalities for the L$p$O estimator applied to the\n$k$NN classifier. Such results are obtained first by exploiting the connection\nbetween the L$p$O estimator and U-statistics, and second by making an intensive\nuse of the generalized Efron-Stein inequality applied to the L$1$O estimator.\nOne other important contribution is made by deriving new quantifications of the\ndiscrepancy between the L$p$O estimator and the classification error/risk of\nthe $k$NN classifier. The optimality of these bounds is discussed by means of\nseveral lower bounds as well as simulation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 07:50:32 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 11:12:34 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Celisse", "Alain", "", "MODAL, LPP"], ["Mary-Huard", "Tristan", "", "GQE-Le Moulon"]]}, {"id": "1508.04948", "submitter": "Andreas Anastasiou Mr", "authors": "Andreas Anastasiou and Christophe Ley", "title": "Bounds for the asymptotic normality of the maximum likelihood estimator\n  using the Delta method", "comments": "15 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic normality of the Maximum Likelihood Estimator (MLE) is a\ncornerstone of statistical theory. In the present paper, we provide sharp\nexplicit upper bounds on Zolotarev-type distances between the exact, unknown\ndistribution of the MLE and its limiting normal distribution. Our approach to\nthis fundamental issue is based on a sound combination of the Delta method,\nStein's method, Taylor expansions and conditional expectations, for the\nclassical situations where the MLE can be expressed as a function of a sum of\nindependent and identically distributed terms. This encompasses in particular\nthe broad exponential family of distributions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:50:24 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 14:51:01 GMT"}, {"version": "v3", "created": "Sun, 17 Apr 2016 10:48:16 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Ley", "Christophe", ""]]}, {"id": "1508.05249", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart", "title": "Representation of Quasi-Monotone Functionals by Families of Separating\n  Hyperplanes", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize when the level sets of a continuous quasi-monotone functional\ndefined on a suitable convex subset of a normed space can be uniquely\nrepresented by a family of bounded continuous functionals. Furthermore, we\ninvestigate how regularly these functionals depend on the parameterizing level.\nFinally, we show how this question relates to the recent problem of property\nelicitation that simultaneously attracted interest in machine learning,\nstatistical evaluation of forecasts, and finance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 11:58:42 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Steinwart", "Ingo", ""]]}, {"id": "1508.05476", "submitter": "Vivian Viallon", "authors": "Edouard Ollier and Vivian Viallon", "title": "Regression modeling on stratified data with the lasso", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of regression models on strata defined using a\ncategorical covariate, in order to identify interactions between this\ncategorical covariate and the other predictors. A basic approach requires the\nchoice of a reference stratum. We show that the performance of a penalized\nversion of this approach depends on this arbitrary choice. We propose a refined\napproach that bypasses this arbitrary choice, at almost no additional\ncomputational cost. Regarding model selection consistency, our proposal mimics\nthe strategy based on an optimal and covariate-specific choice for the\nreference stratum. Results from an empirical study confirm that our proposal\ngenerally outperforms the basic approach in the identification and description\nof the interactions. An illustration on gene expression data is provided.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 07:33:37 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 19:02:14 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Ollier", "Edouard", ""], ["Viallon", "Vivian", ""]]}, {"id": "1508.05503", "submitter": "Simon Byrne", "authors": "Simon Byrne", "title": "Empirical AUC for evaluating probabilistic forecasts", "comments": "15 pages", "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 380-393", "doi": "10.1214/16-EJS1109", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring functions are used to evaluate and compare partially probabilistic\nforecasts. We investigate the use of rank-sum functions such as empirical Area\nUnder the Curve (AUC), a widely-used measure of classification performance, as\na scoring function for the prediction of probabilities of a set of binary\noutcomes. It is shown that the AUC is not generally a proper scoring function,\nthat is, under certain circumstances it is possible to improve on the expected\nAUC by modifying the quoted probabilities from their true values. However with\nsome restrictions, or with certain modifications, it can be made proper.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 12:32:34 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Byrne", "Simon", ""]]}, {"id": "1508.05538", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin", "title": "Optimal Algorithms and Lower Bounds for Testing Closeness of Structured\n  Distributions", "comments": "27 pages, to appear in FOCS'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a general unified method that can be used for $L_1$ {\\em closeness\ntesting} of a wide range of univariate structured distribution families. More\nspecifically, we design a sample optimal and computationally efficient\nalgorithm for testing the equivalence of two unknown (potentially arbitrary)\nunivariate distributions under the $\\mathcal{A}_k$-distance metric: Given\nsample access to distributions with density functions $p, q: I \\to \\mathbb{R}$,\nwe want to distinguish between the cases that $p=q$ and\n$\\|p-q\\|_{\\mathcal{A}_k} \\ge \\epsilon$ with probability at least $2/3$. We show\nthat for any $k \\ge 2, \\epsilon>0$, the {\\em optimal} sample complexity of the\n$\\mathcal{A}_k$-closeness testing problem is $\\Theta(\\max\\{\nk^{4/5}/\\epsilon^{6/5}, k^{1/2}/\\epsilon^2 \\})$. This is the first $o(k)$\nsample algorithm for this problem, and yields new, simple $L_1$ closeness\ntesters, in most cases with optimal sample complexity, for broad classes of\nstructured distributions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 18:49:50 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Nikishkin", "Vladimir", ""]]}, {"id": "1508.05680", "submitter": "Junxiong Jia", "authors": "Junxiong Jia and Jigen Peng and Jinghuai Gao", "title": "Bayesian approach to inverse problems for functions with variable index\n  Besov prior", "comments": "31 pages. arXiv admin note: text overlap with arXiv:1302.6989 by\n  other authors", "journal-ref": "Inverse Problems, 32(8), 2016, 085006", "doi": "10.1088/0266-5611/32/8/085006", "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopt Bayesian approach to consider the inverse problem of estimate a\nfunction from noisy observations. One important component of this approach is\nthe prior measure. Total variation prior has been proved with no discretization\ninvariant property, so Besov prior has been proposed recently. Different prior\nmeasures usually connect to different regularization terms. Variable index TV,\nvariable index Besov regularization terms have been proposed in image analysis,\nhowever, there are no such prior measure in Bayesian theory. So in this paper,\nwe propose a variable index Besov prior measure which is a Non-Guassian\nmeasure. Based on the variable index Besov prior measure, we build the Bayesian\ninverse theory. Then applying our theory to integer and fractional order\nbackward diffusion problems. Although there are many researches about\nfractional order backward diffusion problems, we firstly apply Bayesian inverse\ntheory to this problem which provide an opportunity to quantify the\nuncertainties for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 01:30:29 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jia", "Junxiong", ""], ["Peng", "Jigen", ""], ["Gao", "Jinghuai", ""]]}, {"id": "1508.05733", "submitter": "Tom Sterkenburg", "authors": "Tom F. Sterkenburg", "title": "A generalized characterization of algorithmic probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.LO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An a priori semimeasure (also known as \"algorithmic probability\" or \"the\nSolomonoff prior\" in the context of inductive inference) is defined as the\ntransformation, by a given universal monotone Turing machine, of the uniform\nmeasure on the infinite strings. It is shown in this paper that the class of a\npriori semimeasures can equivalently be defined as the class of\ntransformations, by all compatible universal monotone Turing machines, of any\ncontinuous computable measure in place of the uniform measure. Some\nconsideration is given to possible implications for the prevalent association\nof algorithmic probability with certain foundational statistical principles.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 09:39:10 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 19:24:32 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Sterkenburg", "Tom F.", ""]]}, {"id": "1508.05847", "submitter": "Meng Li", "authors": "Meng Li and Subhashis Ghosal", "title": "Bayesian Detection of Image Boundaries", "comments": null, "journal-ref": null, "doi": "10.1214/16-AOS1523", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting boundary of an image based on noisy observations is a fundamental\nproblem of image processing and image segmentation. For a $d$-dimensional image\n($d = 2, 3, \\ldots$), the boundary can often be described by a closed smooth\n$(d - 1)$-dimensional manifold. In this paper, we propose a nonparametric\nBayesian approach based on priors indexed by $\\mathbb{S}^{d - 1}$, the unit\nsphere in $\\mathbb{R}^d$. We derive optimal posterior contraction rates using\nGaussian processes or finite random series priors using basis functions such as\ntrigonometric polynomials for 2-dimensional images and spherical harmonics for\n3-dimensional images. For 2-dimensional images, we show a rescaled squared\nexponential Gaussian process on $\\mathbb{S}^1$ achieves four goals of\nguaranteed geometric restriction, (nearly) minimax rate optimal and adaptive to\nthe smoothness level, convenient for joint inference and computationally\nefficient. We conduct an extensive study of its reproducing kernel Hilbert\nspace, which may be of interest by its own and can also be used in other\ncontexts. Simulations confirm excellent performance of the proposed method and\nindicate its robustness under model misspecification at least under the\nsimulated settings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 15:40:03 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 19:45:16 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 16:12:11 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Li", "Meng", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1508.05904", "submitter": "Ulhas Dixit Dr", "authors": "U. J. Dixit and M. Jabbari Nooghabi", "title": "Comments on the estimate for Pareto Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dixit and Jabbari Nooghabi (2010) had derived the MLE and UMVUE of the\nprobability density function (pdf) and cumulative distributive function (cdf).\nFurther, it had been shown that MLE is more efficient than UMVUE. He, Zhou and\nZhang (2014) have also derived the same and made a remark that the work of\nDixit and Jabbari Nooghabi (2010) is not correct. We have made a comments with\ndetail algebra that our results are correct. Further, we have also given the R\ncode.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 18:15:49 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Dixit", "U. J.", ""], ["Nooghabi", "M. Jabbari", ""]]}, {"id": "1508.05994", "submitter": "Tatiane Melo", "authors": "Tatiane F. N. Melo, Silvia L. P. Ferrari and Alexandre G. Patriota", "title": "Improved estimation in a general multivariate elliptical model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of reducing the bias of maximum likelihood estimator in a general\nmultivariate elliptical regression model is considered. The model is very\nflexible and allows the mean vector and the dispersion matrix to have\nparameters in common. Many frequently used models are special cases of this\ngeneral formulation, namely: errors-in-variables models, nonlinear\nmixed-effects models, heteroscedastic nonlinear models, among others. In any of\nthese models, the vector of the errors may have any multivariate elliptical\ndistribution. We obtain the second-order bias of the maximum likelihood\nestimator, a bias-corrected estimator, and a bias-reduced estimator. Simulation\nresults indicate the effectiveness of the bias correction and bias reduction\nschemes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 23:05:55 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 17:01:05 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Melo", "Tatiane F. N.", ""], ["Ferrari", "Silvia L. P.", ""], ["Patriota", "Alexandre G.", ""]]}, {"id": "1508.06025", "submitter": "Yury Polyanskiy", "authors": "Yury Polyanskiy and Yihong Wu", "title": "Strong data-processing inequalities for channels and Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-processing inequality, that is, $I(U;Y) \\le I(U;X)$ for a Markov\nchain $U \\to X \\to Y$, has been the method of choice for proving impossibility\n(converse) results in information theory and many other disciplines. Various\nchannel-dependent improvements (called strong data-processing inequalities, or\nSDPIs) of this inequality have been proposed both classically and more\nrecently. In this note we first survey known results relating various notions\nof contraction for a single channel. Then we consider the basic extension:\ngiven SDPI for each constituent channel in a Bayesian network, how to produce\nan end-to-end SDPI?\n  Our approach is based on the (extract of the) Evans-Schulman method, which is\ndemonstrated for three different kinds of SDPIs, namely, the usual\nAhslwede-G\\'acs type contraction coefficients (mutual information), Dobrushin's\ncontraction coefficients (total variation), and finally the $F_I$-curve (the\nbest possible non-linear SDPI for a given channel). Resulting bounds on the\ncontraction coefficients are interpreted as probability of site percolation. As\nan example, we demonstrate how to obtain SDPI for an $n$-letter memoryless\nchannel with feedback given an SDPI for $n=1$.\n  Finally, we discuss a simple observation on the equivalence of a linear SDPI\nand comparison to an erasure channel (in the sense of \"less noisy\" order). This\nleads to a simple proof of a curious inequality of Samorodnitsky (2015), and\nsheds light on how information spreads in the subsets of inputs of a memoryless\nchannel.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 04:29:28 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 03:48:35 GMT"}, {"version": "v3", "created": "Fri, 20 May 2016 01:23:34 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 20:57:45 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Polyanskiy", "Yury", ""], ["Wu", "Yihong", ""]]}, {"id": "1508.06465", "submitter": "Jean-Michel Loubes", "authors": "Eustasio Del Barrio, H\\'el\\`ene Lescornel (IMT), Jean-Michel Loubes\n  (IMT)", "title": "A statistical analysis of a deformation model with Wasserstein\n  barycenters : estimation procedure and goodness of fit test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a study of a distribution registration model for general\ndeformation functions. In this framework, we provide estimators of the\ndeformations as well as a goodness of fit test of the model. For this, we\nconsider a criterion which studies the Fr{\\'e}chet mean (or barycenter) of the\nwarped distributions whose study enables to make inference on the model. In\nparticular we obtain the asymptotic distribution and a bootstrap procedure for\nthe Wasserstein variation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 12:29:38 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 09:46:24 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Del Barrio", "Eustasio", "", "IMT"], ["Lescornel", "H\u00e9l\u00e8ne", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1508.06505", "submitter": "Aurelien Garivier", "authors": "Tatiana Labopin-Richard (IMT), Fabrice Gamboa (IMT), Aur\\'elien\n  Garivier (UMPA-ENSL, MC2), Jerome Stenger (EDF R&D)", "title": "Conditional quantile sequential estimation for stochastic codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze an algorithm for the sequential estimation of a\nconditional quantile in the context of real stochastic codes with vectorvalued\ninputs. Our algorithm is based on k-nearest neighbors smoothing within a\nRobbins-Monro estimator. We discuss the convergence of the algorithm under some\nconditions on the stochastic code. We provide non-asymptotic rates of\nconvergence of the mean squared error and we discuss the tuning of the\nalgorithm's parameters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 14:20:52 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 13:32:32 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 12:46:32 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2016 13:56:05 GMT"}, {"version": "v5", "created": "Fri, 20 May 2016 11:15:47 GMT"}, {"version": "v6", "created": "Mon, 5 Aug 2019 07:00:31 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Labopin-Richard", "Tatiana", "", "IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Garivier", "Aur\u00e9lien", "", "UMPA-ENSL, MC2"], ["Stenger", "Jerome", "", "EDF R&D"]]}, {"id": "1508.06550", "submitter": "Irene Crimaldi", "authors": "Patrizia Berti, Irene Crimaldi, Luca Pratelli and Pietro Rigo", "title": "Asymptotics for randomly reinforced urns with random barriers", "comments": "13 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An urn contains black and red balls. Let $Z_n$ be the proportion of black\nballs at time $n$ and $0\\leq L<U\\leq 1$ random barriers. At each time $n$, a\nball $b_n$ is drawn. If $b_n$ is black and $Z_{n-1}<U$, then $b_n$ is replaced\ntogether with a random number $B_n$ of black balls. If $b_n$ is red and\n$Z_{n-1}>L$, then $b_n$ is replaced together with a random number $R_n$ of red\nballs. Otherwise, no additional balls are added, and $b_n$ alone is replaced.\nIn this paper, we assume $R_n=B_n$. Then, under mild conditions, it is shown\nthat $Z_n\\overset{a.s.}\\longrightarrow Z$ for some random variable $Z$, and\n\\begin{gather*}\nD_n:=\\sqrt{n}\\,(Z_n-Z)\\longrightarrow\\mathcal{N}(0,\\sigma^2)\\quad\\text{conditionally\na.s.} \\end{gather*} where $\\sigma^2$ is a certain random variance. Almost sure\nconditional convergence means that \\begin{gather*}\nP\\bigl(D_n\\in\\cdot\\mid\\mathcal{G}_n\\bigr)\\overset{weakly}\\longrightarrow\\mathcal{N}(0,\\,\\sigma^2)\\quad\\text{a.s.}\n\\end{gather*} where $P\\bigl(D_n\\in\\cdot\\mid\\mathcal{G}_n\\bigr)$ is a regular\nversion of the conditional distribution of $D_n$ given the past\n$\\mathcal{G}_n$. Thus, in particular, one obtains\n$D_n\\longrightarrow\\mathcal{N}(0,\\sigma^2)$ stably. It is also shown that\n$L<Z<U$ a.s. and $Z$ has non-atomic distribution.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 16:14:34 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Berti", "Patrizia", ""], ["Crimaldi", "Irene", ""], ["Pratelli", "Luca", ""], ["Rigo", "Pietro", ""]]}, {"id": "1508.06558", "submitter": "Jay Beder", "authors": "Jay H. Beder and Margaret Ann McComack", "title": "A note on the minimum size of an orthogonal array", "comments": "10 pages (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is an elementary fact that the size of an orthogonal array of strength t\non k factors must be a multiple of a certain number, say L_t, that depends on\nthe orders of the factors. Thus L_t is a lower bound on the size of arrays of\nstrength t on those factors, and is no larger than L_k, the size of the\ncomplete factorial design. We investigate the relationship between the numbers\nL_t, and two questions in particular: For what t is L_t < L_k? And when L_t =\nL_k, is the complete factorial design the only array of that size and strength\nt? Arrays are assumed to be mixed-level.\n  We refer to an array of size less than L_k as a proper fraction. Guided by\nour main result, we construct a variety of mixed-level proper fractions of\nstrength k-1 that also satisfy a certain group-theoretic condition.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 21:47:16 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Beder", "Jay H.", ""], ["McComack", "Margaret Ann", ""]]}, {"id": "1508.06624", "submitter": "Ke Li", "authors": "Ke Li", "title": "Discriminating quantum states: the multiple Chernoff distance", "comments": "v2: minor changes", "journal-ref": "Annals of Statistics 44 (4): 1661-1679 (2016)", "doi": "10.1214/16-AOS1436", "report-no": null, "categories": "quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing multiple quantum hypotheses\n$\\{\\rho_1^{\\otimes n},\\ldots,\\rho_r^{\\otimes n}\\}$, where an arbitrary prior\ndistribution is given and each of the $r$ hypotheses is $n$ copies of a quantum\nstate. It is known that the average error probability $P_e$ decays\nexponentially to zero, that is, $P_e=\\exp\\{-\\xi n+o(n)\\}$. However, this error\nexponent $\\xi$ is generally unknown, except for the case that $r=2$.\n  In this paper, we solve the long-standing open problem of identifying the\nabove error exponent, by proving Nussbaum and Szko\\l a's conjecture that\n$\\xi=\\min_{i\\neq j}C(\\rho_i,\\rho_j)$. The right-hand side of this equality is\ncalled the multiple quantum Chernoff distance, and\n$C(\\rho_i,\\rho_j):=\\max_{0\\leq s\\leq\n1}\\{-\\log\\operatorname{Tr}\\rho_i^s\\rho_j^{1-s}\\}$ has been previously\nidentified as the optimal error exponent for testing two hypotheses,\n$\\rho_i^{\\otimes n}$ versus $\\rho_j^{\\otimes n}$.\n  The main ingredient of our proof is a new upper bound for the average error\nprobability, for testing an ensemble of finite-dimensional, but otherwise\ngeneral, quantum states. This upper bound, up to a states-dependent factor,\nmatches the multiple-state generalization of Nussbaum and Szko\\l a's lower\nbound. Specialized to the case $r=2$, we give an alternative proof to the\nachievability of the binary-hypothesis Chernoff distance, which was originally\nproved by Audenaert et al.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 19:47:18 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 18:21:17 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Li", "Ke", ""]]}, {"id": "1508.06660", "submitter": "Cristina Butucea", "authors": "Cristina Butucea and Natalia Stepanova", "title": "Adaptive variable selection in nonparametric sparse additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovery of an unknown multivariate signal $f$\nobserved in a $d$-dimensional Gaussian white noise model of intensity\n$\\varepsilon$. We assume that $f$ belongs to a class of smooth functions ${\\cal\nF}^d\\subset L_2([0,1]^d)$ and has an additive sparse structure determined by\nthe parameter $s$, the number of non-zero univariate components contributing to\n$f$. We are interested in the case when $d=d_\\varepsilon \\to \\infty$ as\n$\\varepsilon \\to 0$ and the parameter $s$ stays \"small\" relative to $d$. With\nthese assumptions, the recovery problem in hand becomes that of determining\nwhich sparse additive components are non-zero. Attempting to reconstruct most\nnon-zero components of $f$, but not all of them, we arrive at the problem of\nalmost full variable selection in high-dimensional regression. For two\ndifferent choices of ${\\cal F}^d$, we establish conditions under which almost\nfull variable selection is possible, and provide a procedure that gives almost\nfull variable selection. The procedure does the best (in the asymptotically\nminimax sense) in selecting most non-zero components of $f$. Moreover, it is\nadaptive in the parameter $s$.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 20:51:52 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Butucea", "Cristina", ""], ["Stepanova", "Natalia", ""]]}, {"id": "1508.06675", "submitter": "Henry Cohn", "authors": "Christian Borgs, Jennifer T. Chayes, Henry Cohn, Shirshendu Ganguly", "title": "Consistent nonparametric estimation for heavy-tailed sparse graphs", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graphons as a non-parametric generalization of stochastic block\nmodels, and show how to obtain compactly represented estimators for sparse\nnetworks in this framework. Our algorithms and analysis go beyond previous work\nin several ways. First, we relax the usual boundedness assumption for the\ngenerating graphon and instead treat arbitrary integrable graphons, so that we\ncan handle networks with long tails in their degree distributions. Second,\nagain motivated by real-world applications, we relax the usual assumption that\nthe graphon is defined on the unit interval, to allow latent position graphs\nwhere the latent positions live in a more general space, and we characterize\nidentifiability for these graphons and their underlying position spaces.\n  We analyze three algorithms. The first is a least squares algorithm, which\ngives an approximation we prove to be consistent for all square-integrable\ngraphons, with errors expressed in terms of the best possible stochastic block\nmodel approximation to the generating graphon. Next, we analyze a\ngeneralization based on the cut norm, which works for any integrable graphon\n(not necessarily square-integrable). Finally, we show that clustering based on\ndegrees works whenever the underlying degree distribution is atomless. Unlike\nthe previous two algorithms, this third one runs in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 22:17:32 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 18:06:09 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer T.", ""], ["Cohn", "Henry", ""], ["Ganguly", "Shirshendu", ""]]}, {"id": "1508.06958", "submitter": "Bernd Sturmfels", "authors": "Carlos Am\\'endola, Mathias Drton, Bernd Sturmfels", "title": "Maximum Likelihood Estimates for Gaussian Mixtures Are Transcendental", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": "10.1007/978-3-319-32859-1", "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models are central to classical statistics, widely used in\nthe information sciences, and have a rich mathematical structure. We examine\ntheir maximum likelihood estimates through the lens of algebraic statistics.\nThe MLE is not an algebraic function of the data, so there is no notion of ML\ndegree for these models. The critical points of the likelihood function are\ntranscendental, and there is no bound on their number, even for mixtures of two\nunivariate Gaussians.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 18:12:31 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 10:44:41 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Drton", "Mathias", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1508.07036", "submitter": "Danna Zhang", "authors": "Danna Zhang and Wei Biao Wu", "title": "Gaussian Approximation for High Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating sums of high-dimensional stationary\ntime series by Gaussian vectors, using the framework of functional dependence\nmeasure. The validity of the Gaussian approximation depends on the sample size\n$n$, the dimension $p$, the moment condition and the dependence of the\nunderlying processes. We also consider an estimator for long-run covariance\nmatrices and study its convergence properties. Our results allow constructing\nsimultaneous confidence intervals for mean vectors of high-dimensional time\nseries with asymptotically correct coverage probabilities. A Gaussian\nmultiplier bootstrap method is proposed. A simulation study indicates the\nquality of Gaussian approximation with different $n$, $p$ under different\nmoment and dependence conditions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 21:15:43 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Zhang", "Danna", ""], ["Wu", "Wei Biao", ""]]}, {"id": "1508.07448", "submitter": "Ryan Martin", "authors": "P. Richard Hahn, Ryan Martin, Stephen G. Walker", "title": "On recursive Bayesian predictive distributions", "comments": "22 pages, 3 figures, 3 tables", "journal-ref": "Journal of the American Statistical Association, 2018, volume 113,\n  number 523, pages 1085--1093", "doi": "10.1080/01621459.2017.1304219", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian framework is attractive in the context of prediction, but a fast\nrecursive update of the predictive distribution has apparently been out of\nreach, in part because Monte Carlo methods are generally used to compute the\npredictive. This paper shows that online Bayesian prediction is possible by\ncharacterizing the Bayesian predictive update in terms of a bivariate copula,\nmaking it unnecessary to pass through the posterior to update the predictive.\nIn standard models, the Bayesian predictive update corresponds to familiar\nchoices of copula but, in nonparametric problems, the appropriate copula may\nnot have a closed-form expression. In such cases, our new perspective suggests\na fast recursive approximation to the predictive density, in the spirit of\nNewton's predictive recursion algorithm, but without requiring evaluation of\nnormalizing constants. Consistency of the new algorithm is shown, and numerical\nexamples demonstrate its quality performance in finite-samples compared to\nfully Bayesian and kernel methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 13:27:06 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 12:23:26 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 15:27:53 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 18:26:12 GMT"}, {"version": "v5", "created": "Mon, 1 May 2017 02:20:25 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hahn", "P. Richard", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1508.07530", "submitter": "Bhaswar Bhattacharya", "authors": "Bhaswar B. Bhattacharya", "title": "A General Asymptotic Framework for Distribution-Free Graph-Based\n  Two-Sample Tests", "comments": "Major updates. New results and simulations added. 54 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing equality of two multivariate distributions is a classical problem for\nwhich many non-parametric tests have been proposed over the years. Most of the\npopular two-sample tests, which are asymptotically distribution-free, are based\neither on geometric graphs constructed using inter-point distances between the\nobservations (multivariate generalizations of the Wald-Wolfowitz's runs test)\nor on multivariate data-depth (generalizations of the Mann-Whitney rank test).\n  This paper introduces a general notion of distribution-free graph-based\ntwo-sample tests, and provides a unified framework for analyzing and comparing\ntheir asymptotic properties. The asymptotic (Pitman) efficiency of a general\ngraph-based test is derived, which include tests based on geometric graphs,\nsuch as the Friedman-Rafsky test (1979), the test based on the $K$-nearest\nneighbor graph, the cross-match test (2005), the generalized edge-count test\n(2017), as well as tests based on multivariate depth functions (the Liu-Singh\nrank sum statistic (1993)). The results show how the combinatorial properties\nof the underlying graph effect the performance of the associated two-sample\ntest, and can be used to validate and decide which tests to use in practice.\nApplications of the results are illustrated both on synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 05:27:32 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2015 17:29:21 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 05:51:13 GMT"}, {"version": "v4", "created": "Sat, 26 Aug 2017 07:32:28 GMT"}, {"version": "v5", "created": "Tue, 16 Apr 2019 05:24:04 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "1508.07537", "submitter": "Marie-Luce Taupin", "authors": "Marius Kwemou (LERSTAD, LaMME), Marie-Luce Taupin (Unit\\'e MIAJ,\n  LaMME), Anne-Sophie Tocquet (LaMME)", "title": "Model selection in logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to model selection in logistic regression. We extend\nthe model selection principle introduced by Birg\\'e and Massart (2001) to\nlogistic regression model. This selection is done by using penalized maximum\nlikelihood criteria. We propose in this context a completely data-driven\ncriteria based on the slope heuristics. We prove non asymptotic oracle\ninequalities for selected estimators. Theoretical results are illustrated\nthrough simulation studies.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 07:03:45 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Kwemou", "Marius", "", "LERSTAD, LaMME"], ["Taupin", "Marie-Luce", "", "Unit\u00e9 MIAJ,\n  LaMME"], ["Tocquet", "Anne-Sophie", "", "LaMME"]]}, {"id": "1508.07636", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "A characterization of best unbiased estimators", "comments": "4 pages. Version 2: the paper is thoroughly reworked, even the title\n  and the abstract have changed; the method remains the same. Version 3: a\n  corollary, a proposition, and two examples (on Bernoulli and Beta-Bernoulli\n  trials) are added; two typos are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple characterization of uniformly minimum variance unbiased estimators\n(UMVUEs) is provided (in the case when the sample space is finite) in terms of\na linear independence condition on the likelihood functions corresponding to\nthe possible samples. The crucial observation in the proof is that, if a UMVUE\nexists, then, after an appropriate cleaning of the parameter space, the nonzero\nlikelihood functions are eigenvectors of an \"artificial\" matrix of Lagrange\nmultipliers, and the values of the UMVUE are eigenvalues of that matrix. The\ncharacterization is then extended to best unbiased estimators with respect to\narbitrary convex loss functions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 21:02:53 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 15:18:57 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 21:36:25 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1508.07929", "submitter": "Yves Atchade F", "authors": "Yves F. Atchad\\'e", "title": "On the contraction properties of some high-dimensional quasi-posterior\n  distributions", "comments": "38 pages. Minor modifications from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the contraction properties of a quasi-posterior distribution\n$\\check\\Pi_{n,d}$ obtained by combining a quasi-likelihood function and a\nsparsity inducing prior distribution on $\\rset^d$, as both $n$ (the sample\nsize), and $d$ (the dimension of the parameter) increase. We derive some\ngeneral results that highlight a set of sufficient conditions under which\n$\\check\\Pi_{n,d}$ puts increasingly high probability on sparse subsets of\n$\\rset^d$, and contracts towards the true value of the parameter. We apply\nthese results to the analysis of logistic regression models, and binary\ngraphical models, in high-dimensional settings. For the logistic regression\nmodel, we shows that for well-behaved design matrices, the posterior\ndistribution contracts at the rate $O(\\sqrt{s_\\star\\log(d)/n})$, where\n$s_\\star$ is the number of non-zero components of the parameter. For the binary\ngraphical model, under some regularity conditions, we show that a\nquasi-posterior analog of the neighborhood selection of \\cite{meinshausen06}\ncontracts in the Frobenius norm at the rate $O(\\sqrt{(p+S)\\log(p)/n})$, where\n$p$ is the number of nodes, and $S$ the number of edges of the true graph.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 17:38:40 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 17:20:18 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2016 13:06:05 GMT"}, {"version": "v4", "created": "Sat, 19 Nov 2016 20:48:33 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Atchad\u00e9", "Yves F.", ""]]}]