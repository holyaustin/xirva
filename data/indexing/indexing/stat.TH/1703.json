[{"id": "1703.00167", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier and Nicolas Verzelen", "title": "Adaptive estimation of the sparsity in the Gaussian vector model", "comments": "76 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the Gaussian vector model with mean value {\\theta}. We study the\ntwin problems of estimating the number |{\\theta}|_0 of non-zero components of\n{\\theta} and testing whether |{\\theta}|_0 is smaller than some value. For\ntesting, we establish the minimax separation distances for this model and\nintroduce a minimax adaptive test. Extensions to the case of unknown variance\nare also discussed. Rewriting the estimation of |{\\theta}|_0 as a multiple\ntesting problem of all hypotheses {|{\\theta}|_0 <= q}, we both derive a new way\nof assessing the optimality of a sparsity estimator and we exhibit such an\noptimal procedure. This general approach provides a roadmap for estimating the\ncomplexity of the signal in various statistical models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 07:57:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1703.00329", "submitter": "Clement Bouttier", "authors": "Cl\\'ement Bouttier (ENAC, IMT), Ioana Gavra (IMT)", "title": "Convergence rate of a simulated annealing algorithm with noisy\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a modified version of the simulated annealing\nalgorithm for solving a stochastic global optimization problem. More precisely,\nwe address the problem of finding a global minimizer of a function with noisy\nevaluations. We provide a rate of convergence and its optimized parametrization\nto ensure a minimal number of evaluations for a given accuracy and a confidence\nlevel close to 1. This work is completed with a set of numerical\nexperimentations and assesses the practical performance both on benchmark test\ncases and on real world examples.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 15:03:04 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Bouttier", "Cl\u00e9ment", "", "ENAC, IMT"], ["Gavra", "Ioana", "", "IMT"]]}, {"id": "1703.00353", "submitter": "Pierre Del Moral", "authors": "Pierre Del Moral, Adrian N. Bishop", "title": "Matrix product moments in normal variables", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\cal X }=XX^{\\prime}$ be a random matrix associated with a centered\n$r$-column centered Gaussian vector $X$ with a covariance matrix $P$. In this\narticle we compute expectations of matrix-products of the form $\\prod_{1\\leq\ni\\leq n}({\\cal X } P^{v_i})$ for any $n\\geq 1$ and any multi-index parameters\n$v_i\\in\\mathbb{N}$. We derive closed form formulae and a simple sequential\nalgorithm to compute these matrices w.r.t. the parameter $n$. The second part\nof the article is dedicated to a non commutative binomial formula for the\ncentral matrix-moments $\\mathbb{E}\\left(\\left[{\\cal X }-P\\right]^n\\right)$. The\nmatrix product moments discussed in this study are expressed in terms of\npolynomial formulae w.r.t. the powers of the covariance matrix, with\ncoefficients depending on the trace of these matrices. We also derive a series\nof estimates w.r.t. the Loewner order on quadratic forms. For instance we shall\nprove the rather crude estimate $\\mathbb{E}\\left(\\left[{\\cal X\n}-P\\right]^n\\right)\\leq \\mathbb{E}\\left({\\cal X }^n-P^n\\right)$, for any $n\\geq\n1$\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 15:49:03 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Del Moral", "Pierre", ""], ["Bishop", "Adrian N.", ""]]}, {"id": "1703.00469", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov and Abhishek Kaul", "title": "Confidence Bands for Coefficients in High Dimensional Linear Models with\n  Error-in-variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional linear models with error-in-variables. Such models\nare motivated by various applications in econometrics, finance and genetics.\nThese models are challenging because of the need to account for measurement\nerrors to avoid non-vanishing biases in addition to handle the high\ndimensionality of the parameters. A recent growing literature has proposed\nvarious estimators that achieve good rates of convergence. Our main\ncontribution complements this literature with the construction of simultaneous\nconfidence regions for the parameters of interest in such high-dimensional\nlinear models with error-in-variables.\n  These confidence regions are based on the construction of moment conditions\nthat have an additional orthogonal property with respect to nuisance\nparameters. We provide a construction that requires us to estimate an\nadditional high-dimensional linear model with error-in-variables for each\ncomponent of interest. We use a multiplier bootstrap to compute critical values\nfor simultaneous confidence intervals for a subset $S$ of the components. We\nshow its validity despite of possible model selection mistakes, and allowing\nfor the cardinality of $S$ to be larger than the sample size.\n  We apply and discuss the implications of our results to two examples and\nconduct Monte Carlo simulations to illustrate the performance of the proposed\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 19:08:00 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Kaul", "Abhishek", ""]]}, {"id": "1703.00471", "submitter": "Erik Agrell", "authors": "Erik Agrell and Bal\\'azs Cs\\'ebfalvi", "title": "Multidimensional Sampling of Isotropically Bandlimited Signals", "comments": null, "journal-ref": "IEEE Signal Processing Letters, vol. 25, no. 3, pp. 383-387, Mar.\n  2018", "doi": "10.1109/LSP.2017.2720143", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new lower bound on the average reconstruction error variance of\nmultidimensional sampling and reconstruction is presented. It applies to\nsampling on arbitrary lattices in arbitrary dimensions, assuming a stochastic\nprocess with constant, isotropically bandlimited spectrum and reconstruction by\nthe best linear interpolator. The lower bound is exact for any lattice at\nsufficiently high and low sampling rates. The two threshold rates where the\nerror variance deviates from the lower bound gives two optimality criteria for\nsampling lattices. It is proved that at low rates, near the first threshold,\nthe optimal lattice is the dual of the best sphere-covering lattice, which for\nthe first time establishes a rigorous relation between optimal sampling and\noptimal sphere covering. A previously known result is confirmed at high rates,\nnear the second threshold, namely, that the optimal lattice is the dual of the\nbest sphere-packing lattice. Numerical results quantify the performance of\nvarious lattices for sampling and support the theoretical optimality criteria.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 19:25:14 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Agrell", "Erik", ""], ["Cs\u00e9bfalvi", "Bal\u00e1zs", ""]]}, {"id": "1703.00539", "submitter": "Victor-Emmanuel Brunel", "authors": "John Urschel, Victor-Emmanuel Brunel, Ankur Moitra, Philippe Rigollet", "title": "Learning Determinantal Point Processes with Moments and Cycles", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal Point Processes (DPPs) are a family of probabilistic models\nthat have a repulsive behavior, and lend themselves naturally to many tasks in\nmachine learning where returning a diverse set of objects is important. While\nthere are fast algorithms for sampling, marginalization and conditioning, much\nless is known about learning the parameters of a DPP. Our contribution is\ntwofold: (i) we establish the optimal sample complexity achievable in this\nproblem and show that it is governed by a natural parameter, which we call the\n\\emph{cycle sparsity}; (ii) we propose a provably fast combinatorial algorithm\nthat implements the method of moments efficiently and achieves optimal sample\ncomplexity. Finally, we give experimental results that confirm our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 22:53:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Urschel", "John", ""], ["Brunel", "Victor-Emmanuel", ""], ["Moitra", "Ankur", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1703.00542", "submitter": "Adityanand Guntuboyina", "authors": "Xi Chen, Adityanand Guntuboyina and Yuchen Zhang", "title": "A note on the approximate admissibility of regularized estimators in the\n  Gaussian sequence model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating an unknown vector $\\theta$ from an\nobservation $X$ drawn according to the normal distribution with mean $\\theta$\nand identity covariance matrix under the knowledge that $\\theta$ belongs to a\nknown closed convex set $\\Theta$. In this general setting, Chatterjee (2014)\nproved that the natural constrained least squares estimator is \"approximately\nadmissible\" for every $\\Theta$. We extend this result by proving that the same\nproperty holds for all convex penalized estimators as well. Moreover, we\nsimplify and shorten the original proof considerably. We also provide explicit\nupper and lower bounds for the universal constant underlying the notion of\napproximate admissibility.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:08:33 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Chen", "Xi", ""], ["Guntuboyina", "Adityanand", ""], ["Zhang", "Yuchen", ""]]}, {"id": "1703.00647", "submitter": "Chun Yip Yau", "authors": "Wai Leong Ng, Shenyi Pan, Chun Yip Yau", "title": "Inference for Multiple Change-points in Linear and Non-linear Time\n  Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a generalized likelihood ratio scan method (GLRSM)\nfor multiple change-points inference in piecewise stationary time series, which\nestimates the number and positions of change-points and provides a confidence\ninterval for each change-point. The computational complexity of using GLRSM for\nmultiple change-points detection is as low as $O(n(\\log n)^3)$ for a series of\nlength $n$. Consistency of the estimated numbers and positions of the\nchange-points is established. Extensive simulation studies are provided to\ndemonstrate the effectiveness of the proposed methodology under different\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:31:13 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Ng", "Wai Leong", ""], ["Pan", "Shenyi", ""], ["Yau", "Chun Yip", ""]]}, {"id": "1703.00871", "submitter": "Alexey Naumov", "authors": "Alexey Naumov, Vladimir Spokoiny, Vladimir Ulyanov", "title": "Bootstrap confidence sets for spectral projectors of sample covariance", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1},\\ldots,X_{n}$ be i.i.d. sample in $\\mathbb{R}^{p}$ with zero mean\nand the covariance matrix $\\mathbf{\\Sigma}$. The problem of recovering the\nprojector onto an eigenspace of $\\mathbf{\\Sigma}$ from these observations\nnaturally arises in many applications. Recent technique from [Koltchinskii,\nLounici, 2015] helps to study the asymptotic distribution of the distance in\nthe Frobenius norm $\\| \\mathbf{P}_r - \\widehat{\\mathbf{P}}_r \\|_{2}$ between\nthe true projector $\\mathbf{P}_r$ on the subspace of the $r$-th eigenvalue and\nits empirical counterpart $\\widehat{\\mathbf{P}}_r$ in terms of the effective\nrank of $\\mathbf{\\Sigma}$. This paper offers a bootstrap procedure for building\nsharp confidence sets for the true projector $\\mathbf{P}_r$ from the given\ndata. This procedure does not rely on the asymptotic distribution of $\\|\n\\mathbf{P}_r - \\widehat{\\mathbf{P}}_r \\|_{2}$ and its moments. It could be\napplied for small or moderate sample size $n$ and large dimension $p$. The main\nresult states the validity of the proposed procedure for finite samples with an\nexplicit error bound for the error of bootstrap approximation. This bound\ninvolves some new sharp results on Gaussian comparison and Gaussian\nanti-concentration in high-dimensional spaces. Numeric results confirm a good\nperformance of the method in realistic examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 17:48:18 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Naumov", "Alexey", ""], ["Spokoiny", "Vladimir", ""], ["Ulyanov", "Vladimir", ""]]}, {"id": "1703.00918", "submitter": "Marcin Pitera", "authors": "Piotr Jaworski and Marcin Pitera", "title": "A note on conditional covariance matrices for elliptical distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we provide an analytical formula for the conditional\ncovariance matrices of the elliptically distributed random vectors, when the\nconditioning is based on the values of any linear combination of the marginal\nrandom variables. We show that one could introduce the univariate invariant\ndepending solely on the conditioning set, which greatly simplifies the\ncalculations. As an application, we show that one could define uniquely defined\nquantile-based sets on which conditional covariance matrices must be equal to\neach other if only the vector is multivariate normal. The similar results are\nobtained for conditional correlation matrices of the general elliptic case.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 19:05:00 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jaworski", "Piotr", ""], ["Pitera", "Marcin", ""]]}, {"id": "1703.01232", "submitter": "Loic Devilliers", "authors": "Lo\\\"ic Devilliers (ASCLEPIOS), Xavier Pennec (ASCLEPIOS), St\\'ephanie\n  Allassonni\\`ere", "title": "Inconsistency of Template Estimation with the Fr{\\'e}chet mean in\n  Quotient Space", "comments": null, "journal-ref": "Information Processing in Medical Imaging 2017, Jun 2017, Boone,\n  United States", "doi": null, "report-no": "hal-01481074", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of template estimation when data have been randomly\ntransformed under an isometric group action in the presence of noise. In order\nto estimate the template, one often minimizes the variance when the influence\nof the transformations have been removed (computation of the Fr{\\'e}chet mean\nin quotient space). The consistency bias is defined as the distance (possibly\nzero) between the orbit of the template and the orbit of one element which\nminimizes the variance. In this article we establish an asymptotic behavior of\nthe consistency bias with respect to the noise level. This behavior is linear\nwith respect to the noise level. As a result the inconsistency is unavoidable\nas soon as the noise is large enough. In practice, the template estimation with\na finite sample is often done with an algorithm called max-max. We show the\nconvergence of this algorithm to an empirical Karcher mean. Finally, our\nnumerical experiments show that the bias observed in practice cannot be\nattributed to the small sample size or to a convergence problem but is indeed\ndue to the previously studied inconsistency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:27:57 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 08:29:02 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Devilliers", "Lo\u00efc", "", "ASCLEPIOS"], ["Pennec", "Xavier", "", "ASCLEPIOS"], ["Allassonni\u00e8re", "St\u00e9phanie", ""]]}, {"id": "1703.01237", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "How real is the random censorship model in medical studies?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis the random censorship model refers to censoring and\nsurvival times being independent of each other. It is one of the fundamental\nassumptions in the theory of survival analysis. We explain the reason for it\nbeing so ubiquitous, and we investigate its presence in medical studies. We\ndifferentiate two types of censoring in medical studies (dropout and\nadministrative), and we explain their importance in examining the existence of\nthe random censorship model. We show that in order to presume the random\ncensorship model it is not enough to have a design study which conforms to it,\nbut that one needs to provide evidence for its presence in the results. Blindly\npresuming the random censorship model might lead to the Kaplan-Meier estimator\nproducing biased results, which might have serious consequences when estimating\nsurvival in medical studies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:38:28 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "1703.01326", "submitter": "Rui Tuo", "authors": "Rui Tuo and C. F. Jeff Wu", "title": "Prediction based on the Kennedy-O'Hagan calibration model: asymptotic\n  consistency and other properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kennedy and O'Hagan (2001) propose a model for calibrating some unknown\nparameters in a computer model and estimating the discrepancy between the\ncomputer output and physical response. This model is known to have certain\nidentifiability issues. Tuo and Wu (2016) show that there are examples for\nwhich the Kennedy-O'Hagan method renders unreasonable results in calibration.\nIn spite of its unstable performance in calibration, the Kennedy-O'Hagan\napproach has a more robust behavior in predicting the physical response. In\nthis work, we present some theoretical analysis to show the consistency of\npredictor based on their calibration model in the context of radial basis\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 20:14:59 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Tuo", "Rui", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1703.01332", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec", "title": "Optimistic lower bounds for convex regularized least-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimax lower bounds are pessimistic in nature: for any given estimator,\nminimax lower bounds yield the existence of a worst-case target vector\n$\\beta^*_{worst}$ for which the prediction error of the given estimator is\nbounded from below. However, minimax lower bounds shed no light on the\nprediction error of the given estimator for target vectors different than\n$\\beta^*_{worst}$. A characterization of the prediction error of any convex\nregularized least-squares is given. This characterization provide both a lower\nbound and an upper bound on the prediction error. This produces lower bounds\nthat are applicable for any target vector and not only for a single, worst-case\n$\\beta^*_{worst}$. Finally, these lower and upper bounds on the prediction\nerror are applied to the Lasso is sparse linear regression. We obtain a lower\nbound involving the compatibility constant for any tuning parameter, matching\nupper and lower bounds for the universal choice of the tuning parameter, and a\nlower bound for the Lasso with small tuning parameter.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 20:33:58 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 22:44:25 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 22:53:41 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Bellec", "Pierre C", ""]]}, {"id": "1703.01364", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "A Matrix Variate Skew-t Distribution", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.143", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is ample work in the literature dealing with skewness in the\nmultivariate setting, there is a relative paucity of work in the matrix variate\nparadigm. Such work is, for example, useful for modelling three-way data. A\nmatrix variate skew-t distribution is derived based on a mean-variance matrix\nnormal mixture. An expectation-conditional maximization algorithm is developed\nfor parameter estimation. Simulated data are used for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 00:16:54 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 20:57:57 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 01:29:16 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.01421", "submitter": "Zhou Fan", "authors": "Zhou Fan and Leying Guan", "title": "Approximate $l_0$-penalized estimation of piecewise-constant signals on\n  graphs", "comments": "v2: Title change, renumbering of sections and theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study recovery of piecewise-constant signals on graphs by the estimator\nminimizing an $l_0$-edge-penalized objective. Although exact minimization of\nthis objective may be computationally intractable, we show that the same\nstatistical risk guarantees are achieved by the $\\alpha$-expansion algorithm\nwhich computes an approximate minimizer in polynomial time. We establish that\nfor graphs with small average vertex degree, these guarantees are minimax\nrate-optimal over classes of edge-sparse signals. For spatially inhomogeneous\ngraphs, we propose minimization of an edge-weighted objective where each edge\nis weighted by its effective resistance or another measure of its contribution\nto the graph's connectivity. We establish minimax optimality of the resulting\nestimators over corresponding edge-weighted sparsity classes. We show\ntheoretically that these risk guarantees are not always achieved by the\nestimator minimizing the $l_1$/total-variation relaxation, and empirically that\nthe $l_0$-based estimates are more accurate in high signal-to-noise settings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 09:12:42 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 17:16:01 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Fan", "Zhou", ""], ["Guan", "Leying", ""]]}, {"id": "1703.01474", "submitter": "Anindya De", "authors": "Anindya De and Ryan O'Donnell and Rocco Servedio", "title": "Sharp bounds for population recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population recovery problem is a basic problem in noisy unsupervised\nlearning that has attracted significant research attention in recent years\n[WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this\nproblem have been studied, often under assumptions on the unknown distribution\n(such as that it has restricted support size). In this work we study the sample\ncomplexity and algorithmic complexity of the most general version of the\nproblem, under both bit-flip noise and erasure noise model. We give essentially\nmatching upper and lower sample complexity bounds for both noise models, and\nefficient algorithms matching these sample complexity bounds up to polynomial\nfactors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 15:13:41 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["De", "Anindya", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco", ""]]}, {"id": "1703.01525", "submitter": "Tan Le Thanh", "authors": "Le Thanh Tan, Lei Ying, Daniel W. Bliss", "title": "Power Control and Relay Selection in Full-Duplex Cognitive Relay\n  Networks: Coherent versus Non-coherent Scenarios", "comments": "The 51st Annual Conference on Information Systems and Sciences 2017\n  (IEEE CISS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates power control and relay selection in Full Duplex\nCognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can\nsimultaneously receive and forward the signal from the SU source. We study both\nnon-coherent and coherent scenarios. In the non-coherent case, the SU relay\nforwards the signal from the SU source without regulating the phase, while in\nthe coherent scenario, the SU relay regulates the phase when forwarding the\nsignal to minimize the interference at the primary-user (PU) receiver. We\nconsider the problem of maximizing the transmission rate from the SU source to\nthe SU destination subject to the interference constraint at the PU receiver\nand power constraints at both the SU source and SU relay. We develop\nlow-complexity and high-performance joint power control and relay selection\nalgorithms. The superior performance of the proposed algorithms are confirmed\nusing extensive numerical evaluation. In particular, we demonstrate the\nsignificant gain of phase regulation at the SU relay (i.e., the gain of the\ncoherent mechanism over the noncoherent mechanism).\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 21:48:41 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Tan", "Le Thanh", ""], ["Ying", "Lei", ""], ["Bliss", "Daniel W.", ""]]}, {"id": "1703.01527", "submitter": "Tan Le Thanh", "authors": "Le Thanh Tan, Lei Ying, Daniel W. Bliss", "title": "Power Allocation for Full-Duplex Relay Selection in Underlay Cognitive\n  Radio Networks: Coherent versus Non-Coherent Scenarios", "comments": "Submitted to TSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates power control and relay selection in Full Duplex\nCognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can\nsimultaneously receive data from the SU source and forward them to the SU\ndestination. We study both non-coherent and coherent scenarios. In the\nnon-coherent case, the SU relay forwards the signal from the SU source without\nregulating the phase; while in the coherent scenario, the SU relay regulates\nthe phase when forwarding the signal to minimize the interference at the\nprimary-user (PU) receiver. We consider the problem of maximizing the\ntransmission rate from the SU source to the SU destination subject to the\ninterference constraint at the PU receiver and power constraints at both the SU\nsource and SU relay. We then develop a mathematical model to analyze the data\nrate performance of the FDCRN considering the self-interference effects at the\nFD relay. We develop low-complexity and high-performance joint power control\nand relay selection algorithms. Extensive numerical results are presented to\nillustrate the impacts of power level parameters and the self-interference\ncancellation quality on the rate performance. Moreover, we demonstrate the\nsignificant gain of phase regulation at the SU relay.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 22:15:27 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Tan", "Le Thanh", ""], ["Ying", "Lei", ""], ["Bliss", "Daniel W.", ""]]}, {"id": "1703.01654", "submitter": "Lucien Birg\\'e", "authors": "Yannick Baraud and Lucien Birg\\'e", "title": "Une alternative robuste au maximum de vraisemblance: la\n  $\\rho$-estimation", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is based on our personal notes for the short course we gave on\nJanuary 5, 2017 at Institut Henri Poincar\\'e, after an invitation of the SFdS.\nOur purpose is to give an overview of the method of $\\rho$-estimation and of\nthe optimality and robustness properties of the estimators built according to\nthis procedure. This method can be viewed as the sequel of a long series of\nresearches which were devoted to the construction of estimators with good\nproperties in various statistical frameworks. We shall emphasize the connection\nbetween the $\\rho$-estimators and the previous ones, in particular the maximum\nlikelihood estimator, and we shall show, via some typical examples, that the\n$\\rho$-estimators perform better from various points of view.\n  ------\n  Cet article est fond\\'e sur les notes du mini-cours que nous avons donn\\'e le\n5 janvier 2017 \\`a l'Institut Henri Poincar\\'e \\`a l'occasion d'une journ\\'ee\norganis\\'ee par la SFdS et consacr\\'ee \\`a la Statistique Math\\'ematique. Il\nvise \\`a donner un aper\\c{c}u de la m\\'ethode de $\\rho$-estimation ainsi que\ndes propri\\'et\\'es d'optimalit\\'e et de robustesse des estimateurs construits\nselon cette proc\\'edure. Cette m\\'ethode s'inscrit dans une longue lign\\'ee de\nrecherches dont l'objectif a \\'et\\'e de produire des estimateurs poss\\'edant de\nbonnes propri\\'et\\'es pour un ensemble de cadres statistiques aussi vaste que\npossible. Nous mettrons en lumi\\`ere les liens forts qui existent entre les\n$\\rho$-estimateurs et ces pr\\'ed\\'ecesseurs, notamment les estimateurs du\nmaximum de vraisemblance, mais montrerons \\'egalement, au travers d'exemples\nchoisis, que les $\\rho$-estimateurs les surpassent sur bien des aspects.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 19:37:52 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 10:22:37 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Baraud", "Yannick", ""], ["Birg\u00e9", "Lucien", ""]]}, {"id": "1703.01658", "submitter": "Nicolai Baldin", "authors": "Nicolai Baldin", "title": "The wrapping hull and a unified framework for volume estimation", "comments": "Condensed version with a new example", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a unified framework for estimating the volume of a set in\n$\\mathbb{R}^d$ based on observations of points uniformly distributed over the\nset. The framework applies to all classes of sets satisfying one simple axiom:\na class is assumed to be intersection stable. No further hypotheses on the\nboundary of the set are imposed; in particular, the convex sets and the\nso-called weakly-convex sets are covered by the framework. The approach rests\nupon a homogeneous Poisson point process model. We introduce the so-called\nwrapping hull, a generalization of the convex hull, and prove that it is a\nsufficient and complete statistic. The proposed estimator of the volume is\nsimply the volume of the wrapping hull scaled with an appropriate factor. It is\nshown to be consistent for all classes of sets satisfying the axiom and mimics\nan unbiased estimator with uniformly minimal variance. The construction and\nproofs hinge upon an interplay between probabilistic and geometric arguments.\nThe tractability of the framework is numerically confirmed in a variety of\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 20:09:44 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 19:06:38 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Baldin", "Nicolai", ""]]}, {"id": "1703.01721", "submitter": "Jon A. Wellner", "authors": "Jon A. Wellner", "title": "The Bennett-Orlicz norm", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lederer and van de Geer (2013) introduced a new Orlicz norm, the\nBernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here\nwe introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected\nto Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities\nfor expectations of maxima which are potentially somewhat tighter than those\nresulting from the Bernstein-Orlicz norm when they are both applicable. We\ndiscuss cross connections between these norms, exponential inequalities of the\nBernstein, Bennett, and Prokhorov types, and make comparisons with results of\nTalagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 04:31:36 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Wellner", "Jon A.", ""]]}, {"id": "1703.01777", "submitter": "Didier Henrion", "authors": "Yohann De Castro (LM-Orsay), F Gamboa (IMT), D Henrion (LAAS-MAC,\n  CTU), R Hess (LAAS-MAC), J.-B Lasserre (LAAS-MAC, IMT)", "title": "D-optimal design for multivariate polynomial regression via the\n  Christoffel function and semidefinite relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the design of D-optimal experiments with\nmultivariate polynomial regressions on compact semi-algebraic design spaces. We\napply the moment-sum-of-squares hierarchy of semidefinite programming problems\nto solve numerically and approximately the optimal design problem. The geometry\nof the design is recovered with semidefinite programming duality theory and the\nChristoffel polynomial.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:25:09 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["De Castro", "Yohann", "", "LM-Orsay"], ["Gamboa", "F", "", "IMT"], ["Henrion", "D", "", "LAAS-MAC,\n  CTU"], ["Hess", "R", "", "LAAS-MAC"], ["Lasserre", "J. -B", "", "LAAS-MAC, IMT"]]}, {"id": "1703.01913", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vladimir Nikishkin", "title": "Near-Optimal Closeness Testing of Discrete Histogram Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of testing the equivalence between two discrete\nhistograms. A {\\em $k$-histogram} over $[n]$ is a probability distribution that\nis piecewise constant over some set of $k$ intervals over $[n]$. Histograms\nhave been extensively studied in computer science and statistics. Given a set\nof samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to\ndistinguish (with high probability) between the cases that $p = q$ and\n$\\|p-q\\|_1 \\geq \\epsilon$. The main contribution of this paper is a new\nalgorithm for this testing problem and a nearly matching information-theoretic\nlower bound. Specifically, the sample complexity of our algorithm matches our\nlower bound up to a logarithmic factor, improving on previous work by\npolynomial factors in the relevant parameters. Our algorithmic approach applies\nin a more general setting and yields improved sample upper bounds for testing\ncloseness of other structured distributions as well.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 15:03:55 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Nikishkin", "Vladimir", ""]]}, {"id": "1703.02251", "submitter": "Martin Helmer", "authors": "Carlos Am\\'endola, Nathan Bliss, Isaac Burke, Courtney R. Gibbons,\n  Martin Helmer, Serkan Ho\\c{s}ten, Evan D. Nash, Jose Israel Rodriguez, Daniel\n  Smolkin", "title": "The Maximum Likelihood Degree of Toric Varieties", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.jsc.2018.04.016", "report-no": null, "categories": "math.AG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood degree (ML degree) of toric varieties, known\nas discrete exponential models in statistics. By introducing scaling\ncoefficients to the monomial parameterization of the toric variety, one can\nchange the ML degree. We show that the ML degree is equal to the degree of the\ntoric variety for generic scalings, while it drops if and only if the scaling\nvector is in the locus of the principal $A$-determinant. We also illustrate how\nto compute the ML estimate of a toric variety numerically via homotopy\ncontinuation from a scaled toric variety with low ML degree. Throughout, we\ninclude examples motivated by algebraic geometry and statistics. We compute the\nML degree of rational normal scrolls and a large class of Veronese-type\nvarieties. In addition, we investigate the ML degree of scaled Segre varieties,\nhierarchical loglinear models, and graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 07:44:52 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:36:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Bliss", "Nathan", ""], ["Burke", "Isaac", ""], ["Gibbons", "Courtney R.", ""], ["Helmer", "Martin", ""], ["Ho\u015ften", "Serkan", ""], ["Nash", "Evan D.", ""], ["Rodriguez", "Jose Israel", ""], ["Smolkin", "Daniel", ""]]}, {"id": "1703.02307", "submitter": "Pierre Neuvial", "authors": "Gilles Blanchard, Pierre Neuvial (IMT, LaMME), Etienne Roquain (LPMA)", "title": "Post hoc inference via joint family-wise error rate control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general methodology for post hoc inference in a large-scale\nmultiple testing framework. The approach is called \"user-agnostic\" in the sense\nthat the statistical guarantee on the number of correct rejections holds for\nany set of candidate items selected by the user (after having seen the data).\nThis task is investigated by defining a suitable criterion, named the\njoint-family-wise-error rate (JER for short). We propose several procedures for\ncontrolling the JER, with a special focus on incorporating dependencies while\nadapting to the unknown quantity of signal (via a step-down approach). We show\nthat our proposed setting incorporates as particular cases a version of the\nhigher criticism as well as the closed testing based approach of Goeman and\nSolari (2011). Our theoretical statements are supported by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:12:22 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 15:44:35 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 16:27:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Blanchard", "Gilles", "", "IMT, LaMME"], ["Neuvial", "Pierre", "", "IMT, LaMME"], ["Roquain", "Etienne", "", "LPMA"]]}, {"id": "1703.02376", "submitter": "Gyula Pap", "authors": "Be\\'ata Bolyog and Gyula Pap", "title": "On conditional least squares estimation for affine diffusions based on\n  continuous time observations", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic properties of conditional least squares estimators for\nthe drift parameters of two-factor affine diffusions based on continuous time\nobservations. We distinguish three cases: subcritical, critical and\nsupercritical. For all the drift parameters, in the subcritical and\nsupercritical cases, asymptotic normality and asymptotic mixed normality is\nproved, while in the critical case, non-standard asymptotic behavior is\ndescribed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 13:44:56 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 15:08:47 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Bolyog", "Be\u00e1ta", ""], ["Pap", "Gyula", ""]]}, {"id": "1703.02462", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, and Fr\\'ed\\'erique\n  Letu\\'e", "title": "Convex and non-convex regularization methods for spatial point processes\n  intensity estimation", "comments": null, "journal-ref": null, "doi": "10.1214/18-EJS1408", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with feature selection procedures for spatial point\nprocesses intensity estimation. We consider regularized versions of estimating\nequations based on Campbell theorem derived from two classical functions:\nPoisson likelihood and logistic regression likelihood. We provide general\nconditions on the spatial point processes and on penalty functions which ensure\nconsistency, sparsity and asymptotic normality. We discuss the numerical\nimplementation and assess finite sample properties in a simulation study.\nFinally, an application to tropical forestry datasets illustrates the use of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 16:38:11 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Letu\u00e9", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1703.02625", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Theodore Willke, Ryan A. Rossi", "title": "On Sampling from Massive Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Graph Priority Sampling (GPS), a new paradigm for order-based\nreservoir sampling from massive streams of graph edges. GPS provides a general\nway to weight edge sampling according to auxiliary and/or size variables so as\nto accomplish various estimation goals of graph properties. In the context of\nsubgraph counting, we show how edge sampling weights can be chosen so as to\nminimize the estimation variance of counts of specified sets of subgraphs. In\ndistinction with many prior graph sampling schemes, GPS separates the functions\nof edge sampling and subgraph estimation. We propose two estimation frameworks:\n(1) Post-Stream estimation, to allow GPS to construct a reference sample of\nedges to support retrospective graph queries, and (2) In-Stream estimation, to\nallow GPS to obtain lower variance estimates by incrementally updating the\nsubgraph count estimates during stream processing. Unbiasedness of subgraph\nestimators is established through a new Martingale formulation of graph stream\norder sampling, which shows that subgraph estimators, written as a product of\nconstituent edge estimators are unbiased, even when computed at different\npoints in the stream. The separation of estimation and sampling enables\nsignificant resource savings relative to previous work. We illustrate our\nframework with applications to triangle and wedge counting. We perform a\nlarge-scale experimental study on real-world graphs from various domains and\ntypes. GPS achieves high accuracy with less than 1% error for triangle and\nwedge counting, while storing a small fraction of the graph with average update\ntimes of a few microseconds per edge. Notably, for a large Twitter graph with\nmore than 260M edges, GPS accurately estimates triangle counts with less than\n1% error, while storing only 40K edges.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:18:35 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Willke", "Theodore", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1703.02679", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Matt Barnes, Willie Neiswanger", "title": "Performance Bounds for Graphical Record Linkage", "comments": "11 pages with supplement; 4 figures and 2 tables; to appear in\n  AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage involves merging records in large, noisy databases to remove\nduplicate entities. It has become an important area because of its widespread\noccurrence in bibliometrics, public health, official statistics production,\npolitical science, and beyond. Traditional linkage methods directly linking\nrecords to one another are computationally infeasible as the number of records\ngrows. As a result, it is increasingly common for researchers to treat record\nlinkage as a clustering task, in which each latent entity is associated with\none or more noisy database records. We critically assess performance bounds\nusing the Kullback-Leibler (KL) divergence under a Bayesian record linkage\nframework, making connections to Kolchin partition models. We provide an upper\nbound using the KL divergence and a lower bound on the minimum probability of\nmisclassifying a latent entity. We give insights for when our bounds hold using\nsimulated data and provide practical user guidance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:07:37 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Barnes", "Matt", ""], ["Neiswanger", "Willie", ""]]}, {"id": "1703.02720", "submitter": "Yubo Tao", "authors": "Yubo Tao and Jun Yu", "title": "Model Selection for Explosive Models", "comments": null, "journal-ref": "Advances in Econometrics, Vol. 41, 2020, pp. 73-103", "doi": "10.1108/S0731-905320200000041003", "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the limit properties of information criteria (such as\nAIC, BIC, HQIC) for distinguishing between the unit root model and the various\nkinds of explosive models. The explosive models include the local-to-unit-root\nmodel, the mildly explosive model and the regular explosive model. Initial\nconditions with different order of magnitude are considered. Both the OLS\nestimator and the indirect inference estimator are studied. It is found that\nBIC and HQIC, but not AIC, consistently select the unit root model when data\ncome from the unit root model. When data come from the local-to-unit-root\nmodel, both BIC and HQIC select the wrong model with probability approaching 1\nwhile AIC has a positive probability of selecting the right model in the limit.\nWhen data come from the regular explosive model or from the mildly explosive\nmodel in the form of $1+n^{\\alpha }/n$ with $\\alpha \\in (0,1)$, all three\ninformation criteria consistently select the true model. Indirect inference\nestimation can increase or decrease the probability for information criteria to\nselect the right model asymptotically relative to OLS, depending on the\ninformation criteria and the true model. Simulation results confirm our\nasymptotic results in finite sample.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:16:28 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tao", "Yubo", ""], ["Yu", "Jun", ""]]}, {"id": "1703.02724", "submitter": "Anru Zhang", "authors": "Anru Zhang and Dong Xia", "title": "Tensor SVD: Statistical and Computational Limits", "comments": "Typos fixed", "journal-ref": null, "doi": null, "report-no": "IEEE Transactions on Information Theory 64 (11), 7311-7338", "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a general framework for tensor singular value\ndecomposition (tensor SVD), which focuses on the methodology and theory for\nextracting the hidden low-rank structure from high-dimensional tensor data.\nComprehensive results are developed on both the statistical and computational\nlimits for tensor SVD. This problem exhibits three different phases according\nto the signal-to-noise ratio (SNR). In particular, with strong SNR, we show\nthat the classical higher-order orthogonal iteration achieves the minimax\noptimal rate of convergence in estimation; with weak SNR, the\ninformation-theoretical lower bound implies that it is impossible to have\nconsistent estimation in general; with moderate SNR, we show that the\nnon-convex maximum likelihood estimation provides optimal solution, but with\nNP-hard computational cost; moreover, under the hardness hypothesis of\nhypergraphic planted clique detection, there are no polynomial-time algorithms\nperforming consistently in general.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:22:56 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 18:54:51 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 17:29:56 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 12:35:34 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Anru", ""], ["Xia", "Dong", ""]]}, {"id": "1703.02736", "submitter": "Linglong Kong", "authors": "Qingguo Tang, Linglong Kong, David Ruppert and Rohana J. Karunamuni", "title": "Profile Estimation for Partial Functional Partially Linear Single-Index\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a \\textit{partial functional partially linear single-index\nmodel} that consists of a functional linear component as well as a linear\nsingle-index component. This model generalizes many well-known existing models\nand is suitable for more complicated data structures. However, its estimation\ninherits the difficulties and complexities from both components and makes it a\nchallenging problem, which calls for new methodology. We propose a novel\nprofile B-spline method to estimate the parameters by approximating the unknown\nnonparametric link function in the single-index component part with B-spline,\nwhile the linear slope function in the functional component part is estimated\nby the functional principal component basis. The consistency and asymptotic\nnormality of the parametric estimators are derived, and the global convergence\nof the proposed estimator of the linear slope function is also established.\nMore excitingly, the latter convergence is optimal in the minimax sense. A\ntwo-stage procedure is implemented to estimate the nonparametric link function,\nand the resulting estimator possesses the optimal global rate of convergence.\nFurthermore, the convergence rate of the mean squared prediction error for a\npredictor is also obtained. Empirical properties of the proposed procedures are\nstudied through Monte Carlo simulations. A real data example is also analyzed\nto illustrate the power and flexibility of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 07:41:13 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tang", "Qingguo", ""], ["Kong", "Linglong", ""], ["Ruppert", "David", ""], ["Karunamuni", "Rohana J.", ""]]}, {"id": "1703.02834", "submitter": "Pierre-Alexandre Mattei", "authors": "Charles Bouveyron (EPIONE, JAD), Pierre Latouche (MAP5 - UMR 8145),\n  Pierre-Alexandre Mattei", "title": "Exact Dimensionality Selection for Bayesian PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 13:47:17 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 13:00:55 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bouveyron", "Charles", "", "EPIONE, JAD"], ["Latouche", "Pierre", "", "MAP5 - UMR 8145"], ["Mattei", "Pierre-Alexandre", ""]]}, {"id": "1703.02907", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny", "title": "Improved bounds for Square-Root Lasso and Square-Root Slope", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the results of Bellec, Lecu\\'e and Tsybakov to the setting of\nsparse high-dimensional linear regression with unknown variance, we show that\ntwo estimators, the Square-Root Lasso and the Square-Root Slope can achieve the\noptimal minimax prediction rate, which is $(s/n) \\log (p/s)$, up to some\nconstant, under some mild conditions on the design matrix. Here, $n$ is the\nsample size, $p$ is the dimension and $s$ is the sparsity parameter. We also\nprove optimality for the estimation error in the $l_q$-norm, with $q \\in [1,2]$\nfor the Square-Root Lasso, and in the $l_2$ and sorted $l_1$ norms for the\nSquare-Root Slope. Both estimators are adaptive to the unknown variance of the\nnoise. The Square-Root Slope is also adaptive to the sparsity $s$ of the true\nparameter. Next, we prove that any estimator depending on $s$ which attains the\nminimax rate admits an adaptive to $s$ version still attaining the same rate.\nWe apply this result to the Square-root Lasso. Moreover, for both estimators,\nwe obtain valid rates for a wide range of confidence levels, and improved\nconcentration properties as in [Bellec, Lecu\\'e and Tsybakov, 2017] where the\ncase of known variance is treated. Our results are non-asymptotic.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 16:45:03 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 16:57:54 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 17:58:37 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Derumigny", "Alexis", ""]]}, {"id": "1703.03031", "submitter": "Zuofeng Shang", "authors": "Shunan Zhao, Ruiqi Liu, Zuofeng Shang", "title": "Statistical Inference on Panel Data Models: A Kernel Ridge Regression\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose statistical inferential procedures for panel data models with\ninteractive fixed effects in a kernel ridge regression framework.Compared with\ntraditional sieve methods, our method is automatic in the sense that it does\nnot require the choice of basis functions and truncation parameters.Model\ncomplexity is controlled by a continuous regularization parameter which can be\nautomatically selected by generalized cross validation. Based on empirical\nprocesses theory and functional analysis tools, we derive joint asymptotic\ndistributions for the estimators in the heterogeneous setting. These joint\nasymptotic results are then used to construct confidence intervals for the\nregression means and prediction intervals for the future observations, both\nbeing the first provably valid intervals in literature. Marginal asymptotic\nnormality of the functional estimators in homogeneous setting is also obtained.\nSimulation and real data analysis demonstrate the advantages of our method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 21:03:22 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Zhao", "Shunan", ""], ["Liu", "Ruiqi", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1703.03165", "submitter": "Debraj Das", "authors": "Debraj Das, Karl Gregory, S. N. Lahiri", "title": "Perturbation Bootstrap in Adaptive Lasso", "comments": "43 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adaptive Lasso(Alasso) was proposed by Zou [\\textit{J. Amer. Statist.\nAssoc. \\textbf{101} (2006) 1418-1429}] as a modification of the Lasso for the\npurpose of simultaneous variable selection and estimation of the parameters in\na linear regression model. Zou (2006) established that the Alasso estimator is\nvariable-selection consistent as well as asymptotically Normal in the indices\ncorresponding to the nonzero regression coefficients in certain\nfixed-dimensional settings. In an influential paper, Minnier, Tian and Cai\n[\\textit{J. Amer. Statist. Assoc. \\textbf{106} (2011) 1371-1382}] proposed a\nperturbation bootstrap method and established its distributional consistency\nfor the Alasso estimator in the fixed-dimensional setting. In this paper,\nhowever, we show that this (naive) perturbation bootstrap fails to achieve\nsecond order correctness in approximating the distribution of the Alasso\nestimator. We propose a modification to the perturbation bootstrap objective\nfunction and show that a suitably studentized version of our modified\nperturbation bootstrap Alasso estimator achieves second-order correctness even\nwhen the dimension of the model is allowed to grow to infinity with the sample\nsize. As a consequence, inferences based on the modified perturbation bootstrap\nwill be more accurate than the inferences based on the oracle Normal\napproximation. We give simulation studies demonstrating good finite-sample\nproperties of our modified perturbation bootstrap method as well as an\nillustration of our method on a real data set.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 07:27:33 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 19:55:40 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Das", "Debraj", ""], ["Gregory", "Karl", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1703.03167", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (SELECT, LMO)", "title": "Cross-validation", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is a survey on cross-validation. We define all classical\ncross-validation procedures, and we study their properties for two different\ngoals: estimating the risk of a given estimator, and selecting the best\nestimator among a given family. For the risk estimation problem, we compute the\nbias (which can also be corrected) and the variance of cross-validation\nmethods. For estimator selection, we first provide a first-order analysis\n(based on expectations). Then, we explain how to take into account second-order\nterms (from variance computations, and by taking into account the usefulness of\noverpenalization). This allows, in the end, to provide some guidelines for\nchoosing the best cross-validation method for a given learning problem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 07:40:53 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Arlot", "Sylvain", "", "SELECT, LMO"]]}, {"id": "1703.03237", "submitter": "Weihua Deng Professor", "authors": "Pengbo Xu, Weihua Deng", "title": "Fractional compound Poisson processes with multiple internal states", "comments": "5 pages, 2 figures", "journal-ref": "Mathematical Modelling of Natural Phenomena, 13(1), 10, 2018", "doi": "10.1051/mmnp/2018001", "report-no": null, "categories": "math.ST cond-mat.stat-mech stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the particles undergoing the anomalous diffusion with different waiting\ntime distributions for different internal states, we derive the Fokker-Planck\nand Feymann-Kac equations, respectively, describing positions of the particles\nand functional distributions of the trajectories of particles; in particular,\nthe equations governing the functional distribution of internal states are also\nobtained. The dynamics of the stochastic processes are analyzed and the\napplications, calculating the distribution of the first passage time and the\ndistribution of the fraction of the occupation time, of the equations are\ngiven.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 11:42:34 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Xu", "Pengbo", ""], ["Deng", "Weihua", ""]]}, {"id": "1703.03282", "submitter": "Tom Boot", "authors": "Tom Boot, Didier Nibbering", "title": "Scalable simultaneous inference in high-dimensional linear regression\n  models", "comments": "35 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of simultaneous inference methods in\nhigh-dimensional linear regression models quickly increases with the number\nvariables. This paper proposes a computationally efficient method based on the\nMoore-Penrose pseudoinverse. Under a symmetry assumption on the available\nregressors, the estimators are normally distributed and accompanied by a\nclosed-form expression for the standard errors that is free of tuning\nparameters. We study the numerical performance in Monte Carlo experiments that\nmimic the size of modern applications for which existing methods are\ncomputationally infeasible. We find close to nominal coverage, even in settings\nwhere the imposed symmetry assumption does not hold. Regularization of the\npseudoinverse via a ridge adjustment is shown to yield possible efficiency\ngains.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 15:13:49 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 16:16:31 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 19:14:58 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 08:30:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Boot", "Tom", ""], ["Nibbering", "Didier", ""]]}, {"id": "1703.03353", "submitter": "Philip Dawid", "authors": "A. Philip Dawid, Monica Musio and Silvia Columbu", "title": "A Note on Bayesian Model Selection for Discrete Data Using Proper\n  Scoring Rules", "comments": "8 pages, 2 figures", "journal-ref": "Statistics & Probability Letters 129 (2017), 101-106", "doi": "10.1016/j.spl.2017.05.010", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of choosing between parametric models for a discrete\nobservable, taking a Bayesian approach in which the within-model prior\ndistributions are allowed to be improper. In order to avoid the ambiguity in\nthe marginal likelihood function in such a case, we apply a homogeneous scoring\nrule. For the particular case of distinguishing between Poisson and Negative\nBinomial models, we conduct simulations that indicate that, applied\nprequentially, the method will consistently select the true model.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 17:18:21 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. Philip", ""], ["Musio", "Monica", ""], ["Columbu", "Silvia", ""]]}, {"id": "1703.03412", "submitter": "Peter Orbanz", "authors": "Isma\\\"el Castillo and Peter Orbanz", "title": "Uniform estimation of a class of random graph functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of certain functionals of random graphs. The random\ngraph is generated by a possibly sparse stochastic block model (SBM). The\nnumber of classes is fixed or grows with the number of vertices. Minimax lower\nand upper bounds of estimation along specific submodels are derived. The\nresults are nonasymptotic and imply that uniform estimation of a single\nconnectivity parameter is much slower than the expected asymptotic pointwise\nrate. Specifically, the uniform quadratic rate does not scale as the number of\nedges, but only as the number of vertices. The lower bounds are local around\nany possible SBM. An analogous result is derived for functionals of a class of\nsmooth graphons.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 19:00:01 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 19:05:28 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Castillo", "Isma\u00ebl", ""], ["Orbanz", "Peter", ""]]}, {"id": "1703.03658", "submitter": "Alexandra Suvorikova", "authors": "Johannes Ebert, Vladimir Spokoiny and Alexandra Suvorikova", "title": "Construction of Non-asymptotic Confidence Sets in 2-Wasserstein Space", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a probabilistic setting where the probability\nmeasures are considered to be random objects. We propose a procedure of\nconstruction non-asymptotic confidence sets for empirical barycenters in\n2-Wasserstein space and develop the idea further to construction of a\nnon-parametric two-sample test that is then applied to the detection of\nstructural breaks in data with complex geometry. Both procedures mainly rely on\nthe idea of multiplier bootstrap (Spokoiny and Zhilova (2015), Chernozhukov et\nal. (2014)). The main focus lies on probability measures that have commuting\ncovariance matrices and belong to the same scatter-location family: we proof\nthe validity of a bootstrap procedure that allows to compute confidence sets\nand critical values for a Wasserstein-based two-sample test.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 12:42:32 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Ebert", "Johannes", ""], ["Spokoiny", "Vladimir", ""], ["Suvorikova", "Alexandra", ""]]}, {"id": "1703.03680", "submitter": "Han Cheng Lie", "authors": "H. C. Lie and A. M. Stuart and T. J. Sullivan", "title": "Strong convergence rates of probabilistic integrators for ordinary\n  differential equations", "comments": "25 pages", "journal-ref": "Statistics and Computing (2019)", "doi": "10.1007/s11222-019-09898-6", "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic integration of a continuous dynamical system is a way of\nsystematically introducing model error, at scales no larger than errors\nintroduced by standard numerical discretisation, in order to enable thorough\nexploration of possible responses of the system to inputs. It is thus a\npotentially useful approach in a number of applications such as forward\nuncertainty quantification, inverse problems, and data assimilation. We extend\nthe convergence analysis of probabilistic integrators for deterministic\nordinary differential equations, as proposed by Conrad et al.\\ (\\textit{Stat.\\\nComput.}, 2017), to establish mean-square convergence in the uniform norm on\ndiscrete- or continuous-time solutions under relaxed regularity assumptions on\nthe driving vector fields and their induced flows. Specifically, we show that\nrandomised high-order integrators for globally Lipschitz flows and randomised\nEuler integrators for dissipative vector fields with polynomially-bounded local\nLipschitz constants all have the same mean-square convergence rate as their\ndeterministic counterparts, provided that the variance of the integration noise\nis not of higher order than the corresponding deterministic integrator. These\nand similar results are proven for probabilistic integrators where the random\nperturbations may be state-dependent, non-Gaussian, or non-centred random\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 13:43:06 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 09:21:47 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 15:23:43 GMT"}, {"version": "v4", "created": "Thu, 27 Sep 2018 13:50:16 GMT"}, {"version": "v5", "created": "Tue, 8 Jan 2019 21:19:24 GMT"}, {"version": "v6", "created": "Sat, 26 Oct 2019 18:01:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "H. C.", ""], ["Stuart", "A. M.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1703.03965", "submitter": "Lihu Xu", "authors": "Jinzhu Jia, Fang Xie, Lihu Xu", "title": "Sparse Poisson Regression with Penalized Weighted Score Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a new penalized method in this paper to solve sparse Poisson\nRegression problems. Being different from $\\ell_1$ penalized log-likelihood\nestimation, our new method can be viewed as penalized weighted score function\nmethod. We show that under mild conditions, our estimator is $\\ell_1$\nconsistent and the tuning parameter can be pre-specified, which shares the same\ngood property of the square-root Lasso.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 12:39:33 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Jia", "Jinzhu", ""], ["Xie", "Fang", ""], ["Xu", "Lihu", ""]]}, {"id": "1703.04058", "submitter": "Nan Wu", "authors": "Hau-Tieng Wu and Nan Wu", "title": "Think globally, fit locally under the Manifold Setup: Asymptotic\n  Analysis of Locally Linear Embedding", "comments": "78 pages, 4 figures. We add a short discussion about thr relation\n  between espilon and the intrinsic geometry of the manifold. We add a new\n  section about K nearest neighborhood (KNN) and a new subsection about error\n  in variable. We provide more numerical examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction in 2000, the locally linear embedding (LLE) has been\nwidely applied in data science. We provide an asymptotical analysis of the LLE\nunder the manifold setup. We show that for the general manifold, asymptotically\nwe may not obtain the Laplace-Beltrami operator, and the result may depend on\nthe non-uniform sampling, unless a correct regularization is chosen. We also\nderive the corresponding kernel function, which indicates that the LLE is not a\nMarkov process. A comparison with the other commonly applied nonlinear\nalgorithms, particularly the diffusion map, is provided, and its relationship\nwith the locally linear regression is also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 02:16:11 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 02:45:42 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "1703.04320", "submitter": "Holger Dette", "authors": "Ria van Hecke, Stanislav Volgushev, Holger Dette", "title": "Fourier analysis of serial dependence measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical spectral analysis is based on the discrete Fourier transform of the\nauto-covariances. In this paper we investigate the asymptotic properties of new\nfrequency domain methods where the auto-covariances in the spectral density are\nreplaced by alternative dependence measures which can be estimated by\nU-statistics. An interesting example is given by Kendall{'}s $\\tau$ , for which\nthe limiting variance exhibits a surprising behavior.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 10:34:44 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["van Hecke", "Ria", ""], ["Volgushev", "Stanislav", ""], ["Dette", "Holger", ""]]}, {"id": "1703.04419", "submitter": "Idir Arab", "authors": "Idir Arab and Paulo Eduardo Oliveira", "title": "Iterated failure rate monotonicity and ordering relations within Gamma\n  and Weibull distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic ordering of distributions of random variables may be defined by\nthe relative convexity of the tail functions. This has been extended to higher\norder stochastic orderings, by iteratively reassigning tail-weights. The actual\nverification of those stochastic orderings is not simple, as this depends on\ninverting distribution functions for which there may be no explicit expression.\nThe iterative definition of distributions, of course, contributes to make that\nverification even harder. We have a look at the stochastic ordering,\nintroducing a method that allows for explicit usage, applying it to the Gamma\nand Weibull distributions, giving a complete description of the order relations\nwithin each of those families.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:36:48 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Arab", "Idir", ""], ["Oliveira", "Paulo Eduardo", ""]]}, {"id": "1703.04517", "submitter": "Fulgence Eyi Obiang", "authors": "Alban Mbina Mbina, Guy Martial Nkiet and Fulgence Eyi Obiang", "title": "Variable selection in discriminant analysis for mixed variables and\n  several groups", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for variable selection in discriminant analysis with\nmixed categorical and continuous variables. This method is based on a criterion\nthat permits to reduce the variable selection problem to a problem of\nestimating suitable permutation and dimensionality. Then, estimators for these\nparameters are proposed and the resulting method for selecting variables is\nshown to be consistent. A simulation study that permits to study several\npoperties of the proposed approach and to compare it with an existing method is\ngiven.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 17:54:07 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Mbina", "Alban Mbina", ""], ["Nkiet", "Guy Martial", ""], ["Obiang", "Fulgence Eyi", ""]]}, {"id": "1703.04661", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and David Draper", "title": "A Noninformative Prior on a Space of Distribution Functions", "comments": null, "journal-ref": "Entropy 19(8):391, 2017", "doi": "10.3390/e19080391", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a given problem, the Bayesian statistical paradigm requires the\nspecification of a prior distribution that quantifies relevant information\nabout the unknowns of main interest external to the data. In cases where little\nsuch information is available, the problem under study may possess an\ninvariance under a transformation group that encodes a lack of information,\nleading to a unique prior---this idea was explored at length by E.T. Jaynes.\nPrevious successful examples have included location-scale invariance under\nlinear transformation, multiplicative invariance of the rate at which events in\na counting process are observed, and the derivation of the Haldane prior for a\nBernoulli success probability. In this paper we show that this method can be\nextended, by generalizing Jaynes, in two ways: (1) to yield families of\napproximately invariant priors, and (2) to the infinite-dimensional setting,\nyielding families of priors on spaces of distribution functions. Our results\ncan be used to describe conditions under which a particular Dirichlet Process\nposterior arises from an optimal Bayesian analysis, in the sense that\ninvariances in the prior and likelihood lead to one and only one posterior\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 18:48:35 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 01:23:09 GMT"}, {"version": "v3", "created": "Sat, 29 Jul 2017 15:34:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Terenin", "Alexander", ""], ["Draper", "David", ""]]}, {"id": "1703.04697", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Joseph Salmon", "title": "On the benefits of output sparsity for multi-label classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-label classification framework, where each observation can be\nassociated with a set of labels, has generated a tremendous amount of attention\nover recent years. The modern multi-label problems are typically large-scale in\nterms of number of observations, features and labels, and the amount of labels\ncan even be comparable with the amount of observations. In this context,\ndifferent remedies have been proposed to overcome the curse of dimensionality.\nIn this work, we aim at exploiting the output sparsity by introducing a new\nloss, called the sparse weighted Hamming loss. This proposed loss can be seen\nas a weighted version of classical ones, where active and inactive labels are\nweighted separately. Leveraging the influence of sparsity in the loss function,\nwe provide improved generalization bounds for the empirical risk minimizer, a\nsuitable property for large-scale problems. For this new loss, we derive rates\nof convergence linear in the underlying output-sparsity rather than linear in\nthe number of labels. In practice, minimizing the associated risk can be\nperformed efficiently by using convex surrogates and modern convex optimization\nalgorithms. We provide experiments on various real-world datasets demonstrating\nthe pertinence of our approach when compared to non-weighted techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 20:19:08 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Denis", "Christophe", ""], ["Hebiri", "Mohamed", ""], ["Salmon", "Joseph", ""]]}, {"id": "1703.04790", "submitter": "Junbo Zhao", "authors": "Junbo Zhao and Lamine Mili", "title": "Robust Power System Dynamic State Estimator with Non-Gaussian\n  Measurement Noise: Part I--Theory", "comments": "Submitted to IEEE Transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY eess.SY stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops the theoretical framework and the equations of a new\nrobust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF)\nthat is able to suppress observation and innovation outliers while filtering\nout non-Gaussian measurement noise. Because the errors of the real and reactive\npower measurements calculated using Phasor Measurement Units (PMUs) follow\nlong-tailed probability distributions, the conventional UKF provides strongly\nbiased state estimates since it relies on the weighted least squares estimator.\nBy contrast, the state estimates and residuals of our GM-UKF are proved to be\nroughly Gaussian, allowing the sigma points to reliably approximate the mean\nand the covariance matrices of the predicted and corrected state vectors. To\ndevelop our GM-UKF, we first derive a batch-mode regression form by processing\nthe predictions and observations simultaneously, where the statistical\nlinearization approach is used. We show that the set of equations so derived\nare equivalent to those of the unscented transformation. Then, a robust\nGM-estimator that minimizes a convex Huber cost function while using weights\ncalculated via Projection Statistics (PS's) is proposed. The PS's are applied\nto a two-dimensional matrix that consists of serially correlated predicted\nstate and innovation vectors to detect observation and innovation outliers.\nThese outliers are suppressed by the GM-estimator using the iteratively\nreweighted least squares algorithm. Finally, the asymptotic error covariance\nmatrix of the GM-UKF state estimates is derived from the total influence\nfunction. In the companion paper, extensive simulation results will be shown to\nverify the effectiveness and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:36:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhao", "Junbo", ""], ["Mili", "Lamine", ""]]}, {"id": "1703.04799", "submitter": "Guangyu Zhu", "authors": "Guangyu Zhu and Jiahua Chen", "title": "Multi-parameter One-Sided Monitoring Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-parameter one-sided hypothesis test problems arise naturally in many\napplications. We are particularly interested in effective tests for monitoring\nmultiple quality indices in forestry products. Our search reveals that there\nare many effective statistical methods in the literature for normal data, and\nthat they can easily be adapted for non-normal data. We find that the beautiful\nlikelihood ratio test is unsatisfactory, because in order to control the size,\nit must cope with the least favorable distributions at the cost of power. In\nthis paper, we find a novel way to slightly ease the size control, obtaining a\nmuch more powerful test. Simulation confirms that the new test retains good\ncontrol of the type I error and is markedly more powerful than the likelihood\nratio test as well as many competitors based on normal data. The new method\nperforms well in the context of monitoring multiple quality indices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:49:50 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Zhu", "Guangyu", ""], ["Chen", "Jiahua", ""]]}, {"id": "1703.04886", "submitter": "Marc Vuffray", "authors": "Sidhant Misra, Marc Vuffray, Andrey Y. Lokhov", "title": "Information Theoretic Optimal Learning of Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the optimal number of independent observations from which a sparse\nGaussian Graphical Model can be correctly recovered? Information-theoretic\narguments provide a lower bound on the minimum number of samples necessary to\nperfectly identify the support of any multivariate normal distribution as a\nfunction of model parameters. For a model defined on a sparse graph with $p$\nnodes, a maximum degree $d$ and minimum normalized edge strength $\\kappa$, this\nnecessary number of samples scales at least as $d \\log p/\\kappa^2$. The sample\ncomplexity requirements of existing methods for perfect graph reconstruction\nexhibit dependency on additional parameters that do not enter in the lower\nbound. The question of whether the lower bound is tight and achievable by a\npolynomial time algorithm remains open. In this paper, we constructively answer\nthis question and propose an algorithm, termed DICE, whose sample complexity\nmatches the information-theoretic lower bound up to a universal constant\nfactor. We also propose a related algorithm SLICE that has a slightly higher\nsample complexity, but can be implemented as a mixed integer quadratic program\nwhich makes it attractive in practice. Importantly, SLICE retains a critical\nadvantage of DICE in that its sample complexity only depends on quantities\npresent in the information theoretic lower bound. We anticipate that this\nresult will stimulate future search of computationally efficient sample-optimal\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:25:31 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 05:46:43 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 04:32:34 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Misra", "Sidhant", ""], ["Vuffray", "Marc", ""], ["Lokhov", "Andrey Y.", ""]]}, {"id": "1703.04955", "submitter": "James Johndrow", "authors": "James E. Johndrow, Kristian Lum, David B. Dunson", "title": "Theoretical Limits of Record Linkage and Microclustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been substantial recent interest in record linkage, attempting to\ngroup the records pertaining to the same entities from a large database lacking\nunique identifiers. This can be viewed as a type of \"microclustering,\" with few\nobservations per cluster and a very large number of clusters. A variety of\nmethods have been proposed, but there is a lack of literature providing\ntheoretical guarantees on performance. We show that the problem is\nfundamentally hard from a theoretical perspective, and even in idealized cases,\naccurate entity resolution is effectively impossible when the number of\nentities is small relative to the number of records and/or the separation among\nrecords from different entities is not extremely large. To characterize the\nfundamental difficulty, we focus on entity resolution based on multivariate\nGaussian mixture models, but our conclusions apply broadly and are supported by\nsimulation studies inspired by human rights applications. These results suggest\nconservatism in interpretation of the results of record linkage, support\ncollection of additional data to more accurately disambiguate the entities, and\nmotivate a focus on coarser inference. For example, results from a simulation\nstudy suggest that sometimes one may obtain accurate results for population\nsize estimation even when fine scale entity resolution is inaccurate.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:26:43 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Johndrow", "James E.", ""], ["Lum", "Kristian", ""], ["Dunson", "David B.", ""]]}, {"id": "1703.04956", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee, Trisha Maitra and Sourabh Bhattacharya", "title": "A Short Note on Almost Sure Convergence of Bayes Factors in the General\n  Set-Up", "comments": "To appear in The American Statistician", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is a significant literature on the asymptotic theory of Bayes\nfactor, the set-ups considered are usually specialized and often involves\nindependent and identically distributed data. Even in such specialized cases,\nmostly weak consistency results are available. In this article, for the first\ntime ever, we derive the almost sure convergence theory of Bayes factor in the\ngeneral set-up that includes even dependent data and misspecified models.\nSomewhat surprisingly, the key to the proof of such a general theory is a\nsimple application of a result of Shalizi (2009) to a well-known identity\nsatisfied by the Bayes factor.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:34:38 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 17:14:14 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 13:46:13 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 11:30:01 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 11:09:53 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1703.05101", "submitter": "Olga Klopp", "authors": "Olga Klopp (CREST), Nicolas Verzelen (MISTEA)", "title": "Optimal graphon estimation in cut distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the twin problems of estimating the connection probability matrix of\nan inhomogeneous random graph and the graphon of a W-random graph. We establish\nthe minimax estimation rates with respect to the cut metric for classes of\nblock constant matrices and step function graphons. Surprisingly, our results\nimply that, from the minimax point of view, the raw data, that is, the\nadjacency matrix of the observed graph, is already optimal and more involved\nprocedures cannot improve the convergence rates for this metric. This\nphenomenon contrasts with optimal rates of convergence with respect to other\nclassical distances for graphons such as the l 1 or l 2 metrics.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:04:34 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 10:13:40 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Klopp", "Olga", "", "CREST"], ["Verzelen", "Nicolas", "", "MISTEA"]]}, {"id": "1703.05619", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Nonparametric intensity estimation from noisy observations of a Poisson\n  process under unknown error distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the nonparametric estimation of the intensity function of a\nPoisson point process in a circular model from indirect observations\n$N_1,\\ldots,N_n$. These observations emerge from hidden point process\nrealizations with the target intensity through contamination with additive\nerror. In case that the error distribution can only be estimated from an\nadditional sample $Y_1,\\ldots,Y_m$ we derive minimax rates of convergence with\nrespect to the sample sizes $n$ and $m$ under abstract smoothness conditions\nand propose an orthonormal series estimator which attains the optimal rate of\nconvergence. The performance of the estimator depends on the correct\nspecification of a dimension parameter whose optimal choice relies on\nsmoothness characteristics of both the intensity and the error density. We\npropose a data-driven choice of the dimension parameter based on model\nselection and show that the adaptive estimator attains the minimax optimal\nrate.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 13:48:45 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 10:05:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "1703.05757", "submitter": "Abdelfattah Mustafa", "authors": "Beih S. El-Desouky, Abdelfattah Mustafa and Shamsan AL-Garash", "title": "The Beta Flexible Weibull Distribution", "comments": "14 pages and 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce in this paper a new generalization of the flexible Weibull\ndistribution with four parameters. This model based on the Beta generalized\n(BG) distribution, Eugene et al. \\cite{Eugeneetal2002}, they first using the BG\ndistribution for generating new generalizations. This new model is called the\nbeta flexible Weibull BFW distribution. Some statistical properties such as the\nmode, the $r$th moment, skewness and kurtosis are derived. The moment\ngenerating function and the order statistics are obtained. Moreover, the\nestimations of the parameters are given by maximum likelihood method and the\nFisher's information matrix is derived. Finally, we study the advantage of the\nBFW distribution by an application using real data set.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 21:04:10 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["El-Desouky", "Beih S.", ""], ["Mustafa", "Abdelfattah", ""], ["AL-Garash", "Shamsan", ""]]}, {"id": "1703.06222", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Rina Foygel Barber, Martin J. Wainwright, Michael I.\n  Jordan", "title": "A unified treatment of multiple testing with prior knowledge using the\n  p-filter", "comments": "36 pages, 1 figure, accepted for publication at the Annals of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:08:59 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 01:30:58 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 06:10:07 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 20:17:20 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 07:25:11 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Barber", "Rina Foygel", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1703.06336", "submitter": "Wenge Guo", "authors": "Wenge Guo and Joseph P. Romano", "title": "Analysis of error control in large scale two-stage multiple hypothesis\n  testing", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with the problem of simultaneously testing a large number of\nnull hypotheses, a natural testing strategy is to first reduce the number of\ntested hypotheses by some selection (screening or filtering) process, and then\nto simultaneously test the selected hypotheses. The main advantage of this\nstrategy is to greatly reduce the severe effect of high dimensions. However,\nthe first screening or selection stage must be properly accounted for in order\nto maintain some type of error control. In this paper, we will introduce a\nselection rule based on a selection statistic that is independent of the test\nstatistic when the tested hypothesis is true. Combining this selection rule and\nthe conventional Bonferroni procedure, we can develop a powerful and valid\ntwo-stage procedure. The introduced procedure has several nice properties: (i)\nit completely removes the selection effect; (ii) it reduces the multiplicity\neffect; (iii) it does not \"waste\" data while carrying out both selection and\ntesting. Asymptotic power analysis and simulation studies illustrate that this\nproposed method can provide higher power compared to usual multiple testing\nmethods while controlling the Type 1 error rate. Optimal selection thresholds\nare also derived based on our asymptotic analysis.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 19:00:23 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1703.06367", "submitter": "Annie Liang", "authors": "Annie Liang, Xiaosheng Mu, Vasilis Syrgkanis", "title": "Optimal and Myopic Information Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal dynamic information acquisition from many\ncorrelated information sources. Each period, the decision-maker jointly takes\nan action and allocates a fixed number of observations across the available\nsources. His payoff depends on the actions taken and on an unknown state. In\nthe canonical setting of jointly normal information sources, we show that the\noptimal dynamic information acquisition rule proceeds myopically after finitely\nmany periods. If signals are acquired in large blocks each period, then the\noptimal rule turns out to be myopic from period 1. These results demonstrate\nthe possibility of robust and \"simple\" optimal information acquisition, and\nsimplify the analysis of dynamic information acquisition in a widely used\ninformational environment.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 23:22:23 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 18:47:43 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 21:45:31 GMT"}, {"version": "v4", "created": "Mon, 14 May 2018 13:01:06 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Liang", "Annie", ""], ["Mu", "Xiaosheng", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1703.06610", "submitter": "David Hong", "authors": "David Hong, Laura Balzano and Jeffrey A. Fessler", "title": "Asymptotic performance of PCA for high-dimensional heteroscedastic data", "comments": "34 pages (including supplement), 17 figures", "journal-ref": "J. Multivariate Analysis 167:435-52 Sep 2018", "doi": "10.1016/j.jmva.2018.06.002", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a classical method for reducing the\ndimensionality of data by projecting them onto a subspace that captures most of\ntheir variation. Effective use of PCA in modern applications requires\nunderstanding its performance for data that are both high-dimensional and\nheteroscedastic. This paper analyzes the statistical performance of PCA in this\nsetting, i.e., for high-dimensional data drawn from a low-dimensional subspace\nand degraded by heteroscedastic noise. We provide simplified expressions for\nthe asymptotic PCA recovery of the underlying subspace, subspace amplitudes and\nsubspace coefficients; the expressions enable both easy and efficient\ncalculation and reasoning about the performance of PCA. We exploit the\nstructure of these expressions to show that, for a fixed average noise\nvariance, the asymptotic recovery of PCA for heteroscedastic data is always\nworse than that for homoscedastic data (i.e., for noise variances that are\nequal across samples). Hence, while average noise variance is often a\npractically convenient measure for the overall quality of data, it gives an\noverly optimistic estimate of the performance of PCA for heteroscedastic data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 05:37:48 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 07:31:58 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 05:12:19 GMT"}, {"version": "v4", "created": "Sat, 23 Jun 2018 16:28:05 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hong", "David", ""], ["Balzano", "Laura", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1703.06810", "submitter": "Yuting Wei", "authors": "Yuting Wei, Martin J. Wainwright, Adityanand Guntuboyina", "title": "The geometry of hypothesis testing over convex cones: Generalized\n  likelihood tests and minimax radii", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a compound testing problem within the Gaussian sequence model in\nwhich the null and alternative are specified by a pair of closed, convex cones.\nSuch cone testing problem arise in various applications, including detection of\ntreatment effects, trend detection in econometrics, signal detection in radar\nprocessing, and shape-constrained inference in non-parametric statistics. We\nprovide a sharp characterization of the GLRT testing radius up to a universal\nmultiplicative constant in terms of the geometric structure of the underlying\nconvex cones. When applied to concrete examples, this result reveals some\ninteresting phenomena that do not arise in the analogous problems of estimation\nunder convex constraints. In particular, in contrast to estimation error, the\ntesting error no longer depends purely on the problem complexity via a\nvolume-based measure (such as metric entropy or Gaussian complexity), other\ngeometric properties of the cones also play an important role. To address the\nissue of optimality, we prove information-theoretic lower bounds for minimax\ntesting radius again in terms of geometric quantities. Our general theorems are\nillustrated by examples including the cases of monotone and orthant cones, and\ninvolve some results of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 15:55:05 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 05:27:09 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wei", "Yuting", ""], ["Wainwright", "Martin J.", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "1703.07044", "submitter": "Jiwoong Kim", "authors": "Jiwoong Kim", "title": "The Minimum Distance Estimation with Multiple Integral in Panel Data", "comments": "Minimum distance estimation; panel data", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the minimum distance estimation problem for panel data\nmodel. We propose the minimum distance estimators of regression parameters of\nthe panel data model and investigate their asymptotic distributions. This paper\ncontains two main contributions. First, the domain of application of the\nminimum distance estimation method is extended to the panel data model. Second,\nthe proposed estimators are more efficient than other existing ones. Simulation\nstudies compare performance of the proposed estimators with performance of\nothers and demonstrate some superiority of our estimators.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 03:58:47 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Kim", "Jiwoong", ""]]}, {"id": "1703.07072", "submitter": "Moritz Von Rohrscheidt", "authors": "Cornelia Wichelhaus and Moritz von Rohrscheidt", "title": "Bayesian Nonparametric Inference for M/G/1 Queueing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, nonparametric statistical inference is provided for the\ncontinuous-time M/G/1 queueing model from a Bayesian point of view. The\ninference is based on observations of the inter-arrival and service times.\nBeside other characteristics of the system, particular interest is in the\nwaiting time distribution which is not accessible in closed form. Thus, we use\nan indirect statistical approach by exploiting the Pollaczek-Khinchine\ntransform formula for the Laplace transform of the waiting time distribution.\nDue to this, an estimator is defined and its frequentist validation in terms of\nposterior consistency and posterior normality is studied. It will turn out that\nwe can hereby make inference for the observables separately and compose the\nresults subsequently by suitable techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 06:24:47 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Wichelhaus", "Cornelia", ""], ["von Rohrscheidt", "Moritz", ""]]}, {"id": "1703.07233", "submitter": "Joseph Mur\\'e", "authors": "Joseph Mur\\'e", "title": "Optimal compromise between incompatible conditional probability\n  distributions, with application to Objective Bayesian Kriging", "comments": null, "journal-ref": null, "doi": "10.1051/ps/2018023", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models are often defined through conditional rather than joint distributions,\nbut it can be difficult to check whether the conditional distributions are\ncompatible, i.e. whether there exists a joint probability distribution which\ngenerates them. When they are compatible, a Gibbs sampler can be used to sample\nfrom this joint distribution. When they are not, the Gibbs sampling algorithm\nmay still be applied, resulting in a \"pseudo-Gibbs sampler\". We show its\nstationary probability distribution to be the optimal compromise between the\nconditional distributions, in the sense that it minimizes a mean squared misfit\nbetween them and its own conditional distributions. This allows us to perform\nObjective Bayesian analysis of correlation parameters in Kriging models by\nusing univariate conditional Jeffreys-rule posterior distributions instead of\nthe widely used multivariate Jeffreys-rule posterior. This strategy makes the\nfull-Bayesian procedure tractable. Numerical examples show it has near-optimal\nfrequentist performance in terms of prediction interval coverage.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 14:20:44 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 13:16:57 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 13:06:39 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 09:44:13 GMT"}, {"version": "v5", "created": "Mon, 17 Dec 2018 16:46:31 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Mur\u00e9", "Joseph", ""]]}, {"id": "1703.07281", "submitter": "Davide Giraudo", "authors": "Davide Giraudo", "title": "Convergence rates in the central limit theorem for weighted sums of\n  Bernoulli random fields", "comments": null, "journal-ref": "Mod. Stoch. Theory Appl. 6 (2019)", "doi": "10.15559/18-VMSTA121", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove moment inequalities for a class of functionals of i.i.d. random\nfields. We then derive rates in the central limit theorem for weighted sums of\nsuch randoms fields via an approximation by $m$-dependent random fields.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:38:51 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 14:00:13 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 08:32:37 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Giraudo", "Davide", ""]]}, {"id": "1703.07303", "submitter": "Ansgar Steland", "authors": "Annabel Prause and Ansgar Steland", "title": "Sequential Detection of Three-Dimensional Signals under Dependent Noise", "comments": null, "journal-ref": "Sequential Analysis, 36, 2017 - Issue 2", "doi": "10.1080/07474946.2017.1319674", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study detection methods for multivariable signals under dependent noise.\nThe main focus is on three-dimensional signals, i.e. on signals in the\nspace-time domain. Examples for such signals are multifaceted. They include\ngeographic and climatic data as well as image data, that are observed over a\nfixed time horizon. We assume that the signal is observed as a finite block of\nnoisy samples whereby we are interested in detecting changes from a given\nreference signal. Our detector statistic is based on a sequential partial sum\nprocess, related to classical signal decomposition and reconstruction\napproaches applied to the sampled signal. We show that this detector process\nconverges weakly under the no change null hypothesis that the signal coincides\nwith the reference signal, provided that the spatial-temporal partial sum\nprocess associated to the random field of the noise terms disturbing the\nsampled signal con- verges to a Brownian motion. More generally, we also\nestablish the limiting distribution under a wide class of local alternatives\nthat allows for smooth as well as discontinuous changes. Our results also cover\nextensions to the case that the reference signal is unknown. We conclude with\nan extensive simulation study of the detection algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:36:30 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Prause", "Annabel", ""], ["Steland", "Ansgar", ""]]}, {"id": "1703.07809", "submitter": "Frank Werner", "authors": "Housen Li and Frank Werner", "title": "Empirical Risk Minimization as Parameter Choice Rule for General Linear\n  Regularization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical inverse problem to recover $f$ from noisy\nmeasurements $Y = Tf + \\sigma \\xi$ where $\\xi$ is Gaussian white noise and $T$\na compact operator between Hilbert spaces. Considering general reconstruction\nmethods of the form $\\hat f_\\alpha = q_\\alpha \\left(T^*T\\right)T^*Y$ with an\nordered filter $q_\\alpha$, we investigate the choice of the regularization\nparameter $\\alpha$ by minimizing an unbiased estimate of the predictive risk\n$\\mathbb E\\left[\\Vert Tf - T\\hat f_\\alpha\\Vert^2\\right]$. The corresponding\nparameter $\\alpha_{\\mathrm{pred}}$ and its usage are well-known in the\nliterature, but oracle inequalities and optimality results in this general\nsetting are unknown. We prove a (generalized) oracle inequality, which relates\nthe direct risk $\\mathbb E\\left[\\Vert f - \\hat\nf_{\\alpha_{\\mathrm{pred}}}\\Vert^2\\right]$ with the oracle prediction risk\n$\\inf_{\\alpha>0}\\mathbb E\\left[\\Vert Tf - T\\hat f_{\\alpha}\\Vert^2\\right]$. From\nthis oracle inequality we are then able to conclude that the investigated\nparameter choice rule is of optimal order.\n  Finally we also present numerical simulations, which support the order\noptimality of the method and the quality of the parameter choice in finite\nsample situations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 18:32:47 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 18:12:15 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Li", "Housen", ""], ["Werner", "Frank", ""]]}, {"id": "1703.07879", "submitter": "Simone Carlo Surace", "authors": "Simone Carlo Surace and Anna Kutschireiter and Jean-Pascal Pfister", "title": "How to avoid the curse of dimensionality: scalability of particle\n  filters with and without importance weights", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are a popular and flexible class of numerical algorithms to\nsolve a large class of nonlinear filtering problems. However, standard particle\nfilters with importance weights have been shown to require a sample size that\nincreases exponentially with the dimension D of the state space in order to\nachieve a certain performance, which precludes their use in very\nhigh-dimensional filtering problems. Here, we focus on the dynamic aspect of\nthis curse of dimensionality (COD) in continuous time filtering, which is\ncaused by the degeneracy of importance weights over time. We show that the\ndegeneracy occurs on a time-scale that decreases with increasing D. In order to\nsoften the effects of weight degeneracy, most particle filters use particle\nresampling and improved proposal functions for the particle motion. We explain\nwhy neither of the two can prevent the COD in general. In order to address this\nfundamental problem, we investigate an existing filtering algorithm based on\noptimal feedback control that sidesteps the use of importance weights. We use\nnumerical experiments to show that this Feedback Particle Filter (FPF) by Yang\net al. (2013) does not exhibit a COD.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 22:59:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 14:04:40 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Surace", "Simone Carlo", ""], ["Kutschireiter", "Anna", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1703.08044", "submitter": "Francois Malgouyres", "authors": "Francois Malgouyres (IMT), Joseph Landsberg", "title": "Multilinear compressive sensing and an application to convolutional\n  linear networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a deep linear network endowed with a structure. It takes the form of\na matrix $X$ obtained by multiplying $K$ matrices (called factors and\ncorresponding to the action of the layers). The action of each layer (i.e. a\nfactor) is obtained by applying a fixed linear operator to a vector of\nparameters satisfying a constraint. The number of layers is not limited.\nAssuming that $X$ is given and factors have been estimated, the error between\nthe product of the estimated factors and $X$ (i.e. the reconstruction error) is\neither the statistical or the empirical risk. In this paper, we provide\nnecessary and sufficient conditions on the network topology under which a\nstability property holds. The stability property requires that the error on the\nparameters defining the factors (i.e. the stability of the recovered\nparameters) scales linearly with the reconstruction error (i.e. the risk).\nTherefore, under these conditions on the network topology, any successful\nlearning task leads to stably defined features and therefore interpretable\nlayers/network.In order to do so, we first evaluate how the Segre embedding and\nits inverse distort distances. Then, we show that any deep structured linear\nnetwork can be cast as a generic multilinear problem (that uses the Segre\nembedding). This is the {\\em tensorial lifting}. Using the tensorial lifting,\nwe provide necessary and sufficient conditions for the identifiability of the\nfactors (up to a scale rearrangement). We finally provide the necessary and\nsufficient condition called \\NSPlong~(because of the analogy with the usual\nNull Space Property in the compressed sensing framework) which guarantees that\nthe stability property holds. We illustrate the theory with a practical example\nwhere the deep structured linear network is a convolutional linear network. As\nexpected, the conditions are rather strong but not empty. A simple test on the\nnetwork topology can be implemented to test if the condition holds.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:40:28 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 09:39:59 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 06:33:34 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Malgouyres", "Francois", "", "IMT"], ["Landsberg", "Joseph", ""]]}, {"id": "1703.08190", "submitter": "Jos\\'e Luis Romero", "authors": "Lu\\'is Daniel Abreu and Jos\\'e Luis Romero", "title": "MSE estimates for multitaper spectral estimation and off-grid\n  compressive sensing", "comments": "16 pages, 2 figures. (This article replaces arXiv: 1503.02991.)", "journal-ref": "IEEE Transactions on Information Theory, 63(12):7770-7776, 2017", "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain estimates for the Mean Squared Error (MSE) for the multitaper\nspectral estimator and certain compressive acquisition methods for multi-band\nsignals. We confirm a fact discovered by Thomson [Spectrum estimation and\nharmonic analysis, Proc. IEEE, 1982]: assuming bandwidth $W$ and $N$ time\ndomain observations, the average of the square of the first $K=2NW$ Slepian\nfunctions approaches, as $K$ grows, an ideal band-pass kernel for the interval\n$[-W,W]$. We provide an analytic proof of this fact and measure the\ncorresponding rate of convergence in the $L^{1}$ norm. This validates a\nheuristic approximation used to control the MSE of the multitaper estimator.\nThe estimates have also consequences for the method of compressive acquisition\nof multi-band signals introduced by Davenport and Wakin, giving MSE\napproximation bounds for the dictionary formed by modulation of the critical\nnumber of prolates.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 18:01:39 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 13:01:42 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Abreu", "Lu\u00eds Daniel", ""], ["Romero", "Jos\u00e9 Luis", ""]]}, {"id": "1703.08285", "submitter": "Hock Peng Chan", "authors": "Hock Peng Chan", "title": "The Multi-Armed Bandit Problem: An Efficient Non-Parametric Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lai and Robbins (1985) and Lai (1987) provided efficient parametric solutions\nto the multi-armed bandit problem, showing that arm allocation via upper\nconfidence bounds (UCB) achieves minimum regret. These bounds are constructed\nfrom the Kullback-Leibler information of the reward distributions, estimated\nfrom specified parametric families. In recent years there has been renewed\ninterest in the multi-armed bandit problem due to new applications in machine\nlearning algorithms and data analytics. Non-parametric arm allocation\nprocedures like $\\epsilon$-greedy, Boltzmann exploration and BESA were studied,\nand modified versions of the UCB procedure were also analyzed under\nnon-parametric settings. However unlike UCB these non-parametric procedures are\nnot efficient under general parametric settings. In this paper we propose\nefficient non-parametric procedures.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 04:51:03 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 06:22:40 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 05:47:32 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 05:14:10 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Chan", "Hock Peng", ""]]}, {"id": "1703.08358", "submitter": "Johannes Schmidt-Hieber", "authors": "Markus Reiss and Johannes Schmidt-Hieber", "title": "Posterior contraction rates for support boundary recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample of a Poisson point process with intensity $\\lambda_f(x,y) = n\n\\mathbf{1}(f(x) \\leq y),$ we study recovery of the boundary function $f$ from a\nnonparametric Bayes perspective. Because of the irregularity of this model, the\nanalysis is non-standard. We establish a general result for the posterior\ncontraction rate with respect to the $L^1$-norm based on entropy and one-sided\nsmall probability bounds. From this, specific posterior contraction results are\nderived for Gaussian process priors and priors based on random wavelet series.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:16:37 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 20:00:41 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 16:02:18 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Reiss", "Markus", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1703.08487", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Giandomenico Nollo, Sebastiano Stramaglia, Daniele\n  Marinazzo", "title": "Multiscale Granger causality", "comments": null, "journal-ref": "Phys. Rev. E 96, 042150 (2017)", "doi": "10.1103/PhysRevE.96.042150", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of complex physical and biological systems represented by\nmultivariate stochastic processes, an issue of great relevance is the\ndescription of the system dynamics spanning multiple temporal scales. While\nmethods to assess the dynamic complexity of individual processes at different\ntime scales are well-established, multiscale analysis of directed interactions\nhas never been formalized theoretically, and empirical evaluations are\ncomplicated by practical issues such as filtering and downsampling. Here we\nextend the very popular measure of Granger causality (GC), a prominent tool for\nassessing directed lagged interactions between joint processes, to quantify\ninformation transfer across multiple time scales. We show that the multiscale\nprocessing of a vector autoregressive (AR) process introduces a moving average\n(MA) component, and describe how to represent the resulting ARMA process using\nstate space (SS) models and to combine the SS model parameters for computing\nexact GC values at arbitrarily large time scales. We exploit the theoretical\nformulation to identify peculiar features of multiscale GC in basic AR\nprocesses, and demonstrate with numerical simulations the much larger\nestimation accuracy of the SS approach compared with pure AR modeling of\nfiltered and downsampled data. The improved computational reliability is\nexploited to disclose meaningful multiscale patterns of information transfer\nbetween global temperature and carbon dioxide concentration time series, both\nin paleoclimate and in recent years.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:08:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:02:12 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Faes", "Luca", ""], ["Nollo", "Giandomenico", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1703.08570", "submitter": "Feng Ruan", "authors": "John Duchi and Feng Ruan", "title": "Stochastic Methods for Composite and Weakly Convex Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider minimization of stochastic functionals that are compositions of a\n(potentially) non-smooth convex function $h$ and smooth function $c$ and, more\ngenerally, stochastic weakly-convex functionals. We develop a family of\nstochastic methods---including a stochastic prox-linear algorithm and a\nstochastic (generalized) sub-gradient procedure---and prove that, under mild\ntechnical conditions, each converges to first-order stationary points of the\nstochastic objective. We provide experiments further investigating our methods\non non-smooth phase retrieval problems; the experiments indicate the practical\neffectiveness of the procedures.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 18:45:06 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 00:23:45 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 20:47:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Duchi", "John", ""], ["Ruan", "Feng", ""]]}, {"id": "1703.08596", "submitter": "David N. Levin", "authors": "David N. Levin (University of Chicago, Chicago, IL)", "title": "The Inner Structure of Time-Dependent Signals", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SD math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how a time series of measurements of an evolving system can\nbe processed to create an inner time series that is unaffected by any\ninstantaneous invertible, possibly nonlinear transformation of the\nmeasurements. An inner time series contains information that does not depend on\nthe nature of the sensors, which the observer chose to monitor the system.\nInstead, it encodes information that is intrinsic to the evolution of the\nobserved system. Because of its sensor-independence, an inner time series may\nproduce fewer false negatives when it is used to detect events in the presence\nof sensor drift. Furthermore, if the observed physical system is comprised of\nnon-interacting subsystems, its inner time series is separable; i.e., it\nconsists of a collection of time series, each one being the inner time series\nof an isolated subsystem. Because of this property, an inner time series can be\nused to detect a specific behavior of one of the independent subsystems without\nusing blind source separation to disentangle that subsystem from the others.\nThe method is illustrated by applying it to: 1) an analytic example; 2) the\naudio waveform of one speaker; 3) video images from a moving camera; 4)\nmixtures of audio waveforms of two speakers.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 20:59:52 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Levin", "David N.", "", "University of Chicago, Chicago, IL"]]}, {"id": "1703.08843", "submitter": "Xi Chen", "authors": "Xi Chen, Weidong Liu", "title": "Testing independence with high-dimensional correlated samples", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing independence among a number of (ultra) high-dimensional random\nsamples is a fundamental and challenging problem. By arranging $n$ identically\ndistributed $p$-dimensional random vectors into a $p \\times n$ data matrix, we\ninvestigate the problem of testing independence among columns under the\nmatrix-variate normal modeling of data. We propose a computationally simple and\ntuning-free test statistic, characterize its limiting null distribution,\nanalyze the statistical power and prove its minimax optimality. As an important\nby-product of the test statistic, a ratio-consistent estimator for the\nquadratic functional of a covariance matrix from correlated samples is\ndeveloped. We further study the effect of correlation among samples to an\nimportant high-dimensional inference problem --- large-scale multiple testing\nof Pearson's correlation coefficients. Indeed, blindly using classical\ninference results based on the assumed independence of samples will lead to\nmany false discoveries, which suggests the need for conducting independence\ntesting before applying existing methods. To address the challenge arising from\ncorrelation among samples, we propose a \"sandwich estimator\" of Pearson's\ncorrelation coefficient by de-correlating the samples. Based on this approach,\nthe resulting multiple testing procedure asymptotically controls the overall\nfalse discovery rate at the nominal level while maintaining good statistical\npower. Both simulated and real data experiments are carried out to demonstrate\nthe advantages of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:37:52 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""]]}, {"id": "1703.09269", "submitter": "Ilya Molchanov", "authors": "Ignacio Cascos and Ilya Molchanov", "title": "Band depths based on multiple time instances", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bands of vector-valued functions $f:T\\mapsto\\mathbb{R}^d$ are defined by\nconsidering convex hulls generated by their values concatenated at $m$\ndifferent values of the argument. The obtained $m$-bands are families of\nfunctions, ranging from the conventional band in case the time points are\nindividually considered (for $m=1$) to the convex hull in the functional space\nif the number $m$ of simultaneously considered time points becomes large enough\nto fill the whole time domain. These bands give rise to a depth concept that is\nnew both for real-valued and vector-valued functions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 19:02:09 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Cascos", "Ignacio", ""], ["Molchanov", "Ilya", ""]]}, {"id": "1703.09344", "submitter": "Huong Ha", "authors": "Huong Ha, James S. Welsh, Mazen Alamir", "title": "Useful redundancy in parameter and time delay estimation for\n  continuous-time models", "comments": null, "journal-ref": "Automatica 95, 455-462 (2018)", "doi": "10.1016/j.automatica.2018.06.023", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an algorithm to estimate the parameters, including time\ndelay, of continuous time systems based on instrumental variable identification\nmethods. To overcome the multiple local minima of the cost function associated\nwith the estimation of a time delay system, we utilise the useful redundancy\ntechnique. Specifically, the cost function is filtered through a set of\nlow-pass filters to improve convexity with the useful redundancy technique\nexploited to achieve convergence to the global minimum of the optimization\nproblem. Numerical examples are presented to demonstrate the effectiveness of\nthe proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 23:37:46 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ha", "Huong", ""], ["Welsh", "James S.", ""], ["Alamir", "Mazen", ""]]}, {"id": "1703.09351", "submitter": "Huong Ha", "authors": "Huong Ha, James S. Welsh, Cristian R. Rojas, Bo Wahlberg", "title": "An analysis of the SPARSEVA estimate for the finite sample data case", "comments": null, "journal-ref": "Automatica 96, 141-149, 2018", "doi": "10.1016/j.automatica.2018.06.046", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an upper bound for the SPARSEVA (SPARSe Estimation\nbased on a VAlidation criterion) estimation error in a general scheme, i.e.,\nwhen the cost function is strongly convex and the regularized norm is\ndecomposable for a pair of subspaces. We show how this general bound can be\napplied to a sparse regression problem to obtain an upper bound for the\ntraditional SPARSEVA problem. Numerical results are used to illustrate the\neffectiveness of the suggested bound.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 23:46:31 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 01:06:38 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 04:11:50 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Ha", "Huong", ""], ["Welsh", "James S.", ""], ["Rojas", "Cristian R.", ""], ["Wahlberg", "Bo", ""]]}, {"id": "1703.09372", "submitter": "Hongjuan Zhou", "authors": "Yaozhong Hu, David Nualart, Hongjuan Zhou", "title": "Parameter estimation for fractional Ornstein-Uhlenbeck processes of\n  general Hurst parameter", "comments": "39 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides several statistical estimators for the drift and\nvolatility parameters of an Ornstein-Uhlenbeck process driven by fractional\nBrownian motion, whose observations can be made either continuously or at\ndiscrete time instants. First and higher order power variations are used to\nestimate the volatility parameter. The almost sure convergence of the\nestimators and the corresponding central limit theorems are obtained for all\nthe Hurst parameter range $H\\in (0, 1)$. The least squares estimator is used\nfor the drift parameter. A central limit theorem is proved when the Hurst\nparameter $H \\in (0, 1/2)$ and a noncentral limit theorem is proved for\n$H\\in[3/4, 1)$. Thus, the open problem left in the paper by Hu and Nualart\n(2010) is completely solved, where a central limit theorem for least squares\nestimator is proved for $H\\in [1/2, 3/4)$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 02:19:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Hu", "Yaozhong", ""], ["Nualart", "David", ""], ["Zhou", "Hongjuan", ""]]}, {"id": "1703.09403", "submitter": "HongVan Le", "authors": "J\\\"urgen Jost, H\\^ong V\\^an L\\^e and Lorenz Schwachh\\\"ofer", "title": "The Cram\\'er-Rao inequality on singular statistical models I", "comments": "v.2: 28 p, New subsections: 4.4, 4.5, 4.6", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of the essential tangent bundle of a parametrized\nmeasure model and the notion of reduced Fisher metric on a (possibly singular)\n2-integrable measure model. Using these notions and a new characterization of\n$k$-integrable parametrized measure models, we extend the Cram\\'er-Rao\ninequality to $2$-integrable (possibly singular) statistical models for general\n$\\varphi$-estimations, where $\\varphi$ is a $V$-valued feature function and $V$\nis a topological vector space. Thus we derive an intrinsic Cram\\'er-Rao\ninequality in the most general terms of parametric statistics.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 05:16:35 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 13:36:29 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Jost", "J\u00fcrgen", ""], ["L\u00ea", "H\u00f4ng V\u00e2n", ""], ["Schwachh\u00f6fer", "Lorenz", ""]]}, {"id": "1703.09531", "submitter": "Ester Mariucci", "authors": "Ester Mariucci, Kolyan Ray, Botond Szabo", "title": "A Bayesian nonparametric approach to log-concave density estimation", "comments": "39 pages, 17 figures. Simulation studies were significantly expanded\n  and one more theorem has been added", "journal-ref": "Bernoulli 26 (2020), no. 2, 1070-1097", "doi": "10.3150/19-BEJ1139", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of a log-concave density on $\\mathbb{R}$ is a canonical\nproblem in the area of shape-constrained nonparametric inference. We present a\nBayesian nonparametric approach to this problem based on an exponentiated\nDirichlet process mixture prior and show that the posterior distribution\nconverges to the log-concave truth at the (near-) minimax rate in Hellinger\ndistance. Our proof proceeds by establishing a general contraction result based\non the log-concave maximum likelihood estimator that prevents the need for\nfurther metric entropy calculations. We also present two computationally more\nfeasible approximations and a more practical empirical Bayes approach, which\nare illustrated numerically via simulations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 12:15:26 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:55:04 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mariucci", "Ester", ""], ["Ray", "Kolyan", ""], ["Szabo", "Botond", ""]]}, {"id": "1703.09940", "submitter": "Salem Said", "authors": "Salem Said, Nicolas Le Bihan, Jonathan H. Manton", "title": "Riemannian Gaussian distributions on the space of positive-definite\n  quaternion matrices", "comments": "8 pages, submitted to GSI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Riemannian Gaussian distributions were defined on spaces of\npositive-definite real and complex matrices. The present paper extends this\ndefinition to the space of positive-definite quaternion matrices. In order to\ndo so, it develops the Riemannian geometry of the space of positive-definite\nquaternion matrices, which is shown to be a Riemannian symmetric space of\nnon-positive curvature. The paper gives original formulae for the Riemannian\nmetric of this space, its geodesics, and distance function. Then, it develops\nthe theory of Riemannian Gaussian distributions, including the exact expression\nof their probability density, their sampling algorithm and statistical\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 09:16:26 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Said", "Salem", ""], ["Bihan", "Nicolas Le", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "1703.09965", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "Estimable group effects for strongly correlated variables in linear\n  models", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that parameters for strongly correlated predictor variables\nin a linear model cannot be accurately estimated. We look for linear\ncombinations of these parameters that can be. Under a uniform model, we find\nsuch linear combinations in a neighborhood of a simple variability weighted\naverage of these parameters. Surprisingly, this variability weighted average is\nmore accurately estimated when the variables are more strongly correlated, and\nit is the only linear combination with this property. It can be easily computed\nfor strongly correlated predictor variables in all linear models and has\napplications in inference and estimation concerning parameters of such\nvariables.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 10:54:11 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 03:51:51 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 17:16:24 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "1703.10107", "submitter": "Yo Sheena", "authors": "Yo Sheena", "title": "Asymptotic Expansion of Risk for a Regression Model with respect to\n  $\\alpha$-Divergence with an Application to the Sample Size Problem --\n  Complete Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a regression model, we consider the risk of the maximum likelihood\nestimator with respect to $\\alpha$-divergence, which includes the special cases\nof Kullback-Leibler divergence, Hellinger distance and $\\chi^2$ divergence. The\nasymptotic expansion of the risk with respect to the sample size $n$ is given\nup to the order $n^{-2}$. We are interested in how the risk convergence speed\n(to zero) is affected by the error term distributions of the regression model\nand the magnitude of the joint moments of the standardized explanatory\nvariables. Besides the general result (which is given by Mathematica program),\nwe consider three concrete error term distributions; a normal distribution, a\nt-distribution and a skew-normal distribution. We use the (approximated) risk\nof m.l.e. as a measure of the difficulty of estimation for the regression\nmodel. Especially comparing the value of the (approximated) risk with that of a\nbinomial distribution, we can give a certain standard for the sample size\nrequired to estimate the regression model.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 15:46:34 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 06:28:13 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Sheena", "Yo", ""]]}, {"id": "1703.10127", "submitter": "Gautam Kamath", "authors": "Bryan Cai, Constantinos Daskalakis, Gautam Kamath", "title": "Priv'IT: Private and Sample Efficient Identity Testing", "comments": "To appear in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop differentially private hypothesis testing methods for the small\nsample regime. Given a sample $\\cal D$ from a categorical distribution $p$ over\nsome domain $\\Sigma$, an explicitly described distribution $q$ over $\\Sigma$,\nsome privacy parameter $\\varepsilon$, accuracy parameter $\\alpha$, and\nrequirements $\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ for the type I and type II\nerrors of our test, the goal is to distinguish between $p=q$ and\n$d_{\\rm{TV}}(p,q) \\geq \\alpha$.\n  We provide theoretical bounds for the sample size $|{\\cal D}|$ so that our\nmethod both satisfies $(\\varepsilon,0)$-differential privacy, and guarantees\n$\\beta_{\\rm I}$ and $\\beta_{\\rm II}$ type I and type II errors. We show that\ndifferential privacy may come for free in some regimes of parameters, and we\nalways beat the sample complexity resulting from running the $\\chi^2$-test with\nnoisy counts, or standard approaches such as repetition for endowing\nnon-private $\\chi^2$-style statistics with differential privacy guarantees. We\nexperimentally compare the sample complexity of our method to that of recently\nproposed methods for private hypothesis testing.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:42:21 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 14:53:34 GMT"}, {"version": "v3", "created": "Wed, 7 Jun 2017 02:46:11 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Cai", "Bryan", ""], ["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1703.10143", "submitter": "Richard Samworth", "authors": "Richard A. Lockhart and Richard J. Samworth", "title": "Comments on `High-dimensional simultaneous inference with the bootstrap'", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide some comments on the article `High-dimensional simultaneous\ninference with the bootstrap' by Ruben Dezeure, Peter Buhlmann and Cun-Hui\nZhang.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 17:18:59 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Lockhart", "Richard A.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1703.10283", "submitter": "Muneya Matsui", "authors": "Muneya Matsui, Thomas Mikosch and Gennady Samorodnitsky", "title": "Distance covariance for stochastic processes", "comments": "Contribution to T. Rolski special issue of Probability and\n  Mathematical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distance covariance of two random vectors is a measure of their\ndependence. The empirical distance covariance and correlation can be used as\nstatistical tools for testing whether two random vectors are independent. We\npropose an analogs of the distance covariance for two stochastic processes\ndefined on some interval. Their empirical analogs can be used to test the\nindependence of two processes.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 01:25:09 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Matsui", "Muneya", ""], ["Mikosch", "Thomas", ""], ["Samorodnitsky", "Gennady", ""]]}, {"id": "1703.10381", "submitter": "Joni Virta", "authors": "Joni Virta and Klaus Nordhausen", "title": "Blind source separation of tensor-valued time series", "comments": "26 pages, 6 figures", "journal-ref": "Signal Processing, Vol 141, 204-216 (2017)", "doi": "10.1016/j.sigpro.2017.06.008", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blind source separation model for multivariate time series generally\nassumes that the observed series is a linear transformation of an unobserved\nseries with temporally uncorrelated or independent components. Given the\nobservations, the objective is to find a linear transformation that recovers\nthe latent series. Several methods for accomplishing this exist and three\nparticular ones are the classic SOBI and the recently proposed generalized FOBI\n(gFOBI) and generalized JADE (gJADE), each based on the use of joint lagged\nmoments. In this paper we generalize the methodologies behind these algorithms\nfor tensor-valued time series. We assume that our data consists of a tensor\nobserved at each time point and that the observations are linear\ntransformations of latent tensors we wish to estimate. The tensorial\ngeneralizations are shown to have particularly elegant forms and we show that\neach of them is Fisher consistent and orthogonal equivariant. Comparing the new\nmethods with the original ones in various settings shows that the tensorial\nextensions are superior to both their vector-valued counterparts and to two\nexisting tensorial dimension reduction methods for i.i.d. data. Finally,\napplications to fMRI-data and video processing show that the methods are\ncapable of extracting relevant information from noisy high-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 09:43:35 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Virta", "Joni", ""], ["Nordhausen", "Klaus", ""]]}, {"id": "1703.10393", "submitter": "Hisayuki Tsukuma", "authors": "Hisayuki Tsukuma and Tatsuya Kubokawa", "title": "Proper Bayes and Minimax Predictive Densities for a Matrix-variate\n  Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of estimating predictive densities of a\nmatrix-variate normal distribution with known covariance matrix. Our main aim\nis to establish some Bayesian predictive densities related to matricial\nshrinkage estimators of the normal mean matrix. The Kullback-Leibler loss is\nused for evaluating decision-theoretical optimality of predictive densities. It\nis shown that a proper hierarchical prior yields an admissible and minimax\npredictive density. Also, superharmonicity of prior densities is paid attention\nto for finding out a minimax predictive density with good numerical\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 10:35:42 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 00:29:39 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Tsukuma", "Hisayuki", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1703.10940", "submitter": "Oksana Chernova", "authors": "Alexander Kukush, Oksana Chernova", "title": "Consistent estimation in Cox proportional hazards model with measurement\n  errors and unbounded parameter set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox proportional hazards model with measurement error is investigated. In\nKukush et al. (2011) [Journal of Statistical Research 45, 77-94] and Chimisov\nand Kukush (2014) [Modern Stochastics: Theory and Applications 1, 13-32]\nasymptotic properties of simultaneous estimator $\\lambda_n(\\cdot)$, $\\beta_n$\nwere studied for baseline hazard rate $\\lambda(\\cdot)$ and regression parameter\n$\\beta$, at that the parameter set $\\Theta=\\Theta_{\\lambda}\\times\n\\Theta_{\\beta}$ was assumed bounded. In the present paper, the set\n$\\Theta_{\\lambda}$ is unbounded from above and not separated away from $0$. We\nconstruct the estimator in two steps: first we derive a strongly consistent\nestimator and then modify it to provide its asymptotic normality.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:25:18 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Kukush", "Alexander", ""], ["Chernova", "Oksana", ""]]}]