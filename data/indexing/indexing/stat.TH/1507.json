[{"id": "1507.00065", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Alberto Rodr\\'iguez Casal", "title": "On Estimating the Perimeter Using the Alpha-Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the perimeter of a smooth domain in the\nplane based on a sample from the uniform distribution over the domain. We study\nthe performance of the estimator defined as the perimeter of the alpha-shape of\nthe sample. Some numerical experiments corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 23:30:05 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Casal", "Alberto Rodr\u00edguez", ""]]}, {"id": "1507.00070", "submitter": "Stephen DeSalvo", "authors": "Stephen DeSalvo and James Y. Zhao", "title": "Random Sampling of Contingency Tables via Probabilistic\n  Divide-and-Conquer", "comments": "32 Pages, rewritten", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for random sampling of contingency tables of any\nsize and constraints based on a recently introduced $\\textit{probabilistic\ndivide-and-conquer}$ technique. A simple exact sampling algorithm is presented\nfor $2\\times n$ tables, as well as a generalization where each entry of the\ntable has a specified marginal distribution.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 00:06:21 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 07:34:45 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 14:50:54 GMT"}, {"version": "v4", "created": "Mon, 29 Feb 2016 19:14:46 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["DeSalvo", "Stephen", ""], ["Zhao", "James Y.", ""]]}, {"id": "1507.00123", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Ami Wiesel", "title": "Joint Covariance Estimation with Mutual Linear Structure", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2502556", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint estimation of structured covariance\nmatrices. Assuming the structure is unknown, estimation is achieved using\nheterogeneous training sets. Namely, given groups of measurements coming from\ncentered populations with different covariances, our aim is to determine the\nmutual structure of these covariance matrices and estimate them. Supposing that\nthe covariances span a low dimensional affine subspace in the space of\nsymmetric matrices, we develop a new efficient algorithm discovering the\nstructure and using it to improve the estimation. Our technique is based on the\napplication of principal component analysis in the matrix space. We also derive\nan upper performance bound of the proposed algorithm in the Gaussian scenario\nand compare it with the Cramer-Rao lower bound. Numerical simulations are\npresented to illustrate the performance benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 06:54:11 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1507.00171", "submitter": "Kevin Bleakley", "authors": "G\\'erard Biau (LSTA), Kevin Bleakley (LMO, SELECT), Benoit Cadre (ENS\n  Rennes, UEB, IRMAR)", "title": "The Statistical Performance of Collaborative Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of massive and complex data sets will require the\ndevelopment of algorithms that depend on distributed computing and\ncollaborative inference. Inspired by this, we propose a collaborative framework\nthat aims to estimate the unknown mean $\\theta$ of a random variable $X$. In\nthe model we present, a certain number of calculation units, distributed across\na communication network represented by a graph, participate in the estimation\nof $\\theta$ by sequentially receiving independent data from $X$ while\nexchanging messages via a stochastic matrix $A$ defined over the graph. We give\nprecise conditions on the matrix $A$ under which the statistical precision of\nthe individual units is comparable to that of a (gold standard) virtual\ncentralized estimate, even though each unit does not have access to all of the\ndata. We show in particular the fundamental role played by both the non-trivial\neigenvalues of $A$ and the Ramanujan class of expander graphs, which provide\nremarkable performance for moderate algorithmic cost.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 10:05:29 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Bleakley", "Kevin", "", "LMO, SELECT"], ["Cadre", "Benoit", "", "ENS\n  Rennes, UEB, IRMAR"]]}, {"id": "1507.00421", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie", "title": "Categorical Matrix Completion", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of completing a matrix with categorical-valued\nentries from partial observations. This is achieved by extending the\nformulation and theory of one-bit matrix completion. We recover a low-rank\nmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclear\nnorm of $X$, and the observations are mapped from entries of $X$ through\nmultiple link functions. We establish theoretical upper and lower bounds on the\nrecovery error, which meet up to a constant factor $\\mathcal{O}(K^{3/2})$ where\n$K$ is the fixed number of categories. The upper bound in our case depends on\nthe number of categories implicitly through a maximization of terms that\ninvolve the smoothness of the link functions. In contrast to one-bit matrix\ncompletion, our bounds for categorical matrix completion are optimal up to a\nfactor on the order of the square root of the number of categories, which is\nconsistent with an intuition that the problem becomes harder when the number of\ncategories increases. By comparing the performance of our method with the\nconventional matrix completion method on the MovieLens dataset, we demonstrate\nthe advantage of our method.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 03:58:47 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1507.00513", "submitter": "Mokhtar Zahdi Alaya", "authors": "Mokhtar Zahdi Alaya (LSTA), St\\'ephane Ga\\\"iffas (CMAP), Agathe\n  Guilloux (LSTA)", "title": "Learning the intensity of time events with change-points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the inhomogeneous intensity of a counting\nprocess, under a sparse segmentation assumption. We introduce a weighted\ntotal-variation penalization, using data-driven weights that correctly scale\nthe penalization along the observation interval. We prove that this leads to a\nsharp tuning of the convex relaxation of the segmentation prior, by stating\noracle inequalities with fast rates of convergence, and consistency for\nchange-points detection. This provides first theoretical guarantees for\nsegmentation with a convex proxy beyond the standard i.i.d signal + white noise\nsetting. We introduce a fast algorithm to solve this convex problem. Numerical\nexperiments illustrate our approach on simulated and on a high-frequency\ngenomics dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 10:46:45 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Alaya", "Mokhtar Zahdi", "", "LSTA"], ["Ga\u00efffas", "St\u00e9phane", "", "CMAP"], ["Guilloux", "Agathe", "", "LSTA"]]}, {"id": "1507.00710", "submitter": "Rasmus J Kyng", "authors": "Rasmus Kyng and Anup Rao and Sushant Sachdeva", "title": "Fast, Provable Algorithms for Isotonic Regression in all\n  $\\ell_{p}$-norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,\nthe Isotonic Regression of $y$ is a vector $x$ that respects the partial order\ndescribed by $G,$ and minimizes $||x-y||,$ for a specified norm. This paper\ngives improved algorithms for computing the Isotonic Regression for all\nweighted $\\ell_{p}$-norms with rigorous performance guarantees. Our algorithms\nare quite practical, and their variants can be implemented to run fast in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2015 19:42:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:14:21 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Kyng", "Rasmus", ""], ["Rao", "Anup", ""], ["Sachdeva", "Sushant", ""]]}, {"id": "1507.00802", "submitter": "Khalifa Es-Sebaiy", "authors": "Mohamed El Machkouri, Khalifa Es-Sebaiy, Youssef Ouknine", "title": "Least squares estimator for non-ergodic Ornstein-Uhlenbeck processes\n  driven by Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis for equations driven by fractional Gaussian process\n(fGp) is relatively recent. The development of stochastic calculus with respect\nto the fGp allowed to study such models. In the present paper we consider the\ndrift parameter estimation problem for the non-ergodic Ornstein-Uhlenbeck\nprocess defined as $dX_t=\\theta X_tdt+dG_t,\\ t\\geq0$ with an unknown parameter\n$\\theta>0$, where $G$ is a Gaussian process. We provide sufficient conditions,\nbased on the properties of $G$, ensuring the strong consistency and the\nasymptotic distribution of our estimator $\\widetilde{\\theta}_t$ of $\\theta$\nbased on the observation $\\{X_s,\\ s\\in[0,t]\\}$ as $t\\rightarrow\\infty$. Our\napproach offers an elementary, unifying proof of \\cite{BEO}, and it allows to\nextend the result of \\cite{BEO} to the case when $G$ is a fractional Brownian\nmotion with Hurst parameter $H\\in(0,1)$. We also discuss the cases of\nsubfractional Brownian motion and bifractional Brownian motion.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 01:41:05 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 22:06:14 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Machkouri", "Mohamed El", ""], ["Es-Sebaiy", "Khalifa", ""], ["Ouknine", "Youssef", ""]]}, {"id": "1507.00827", "submitter": "Can Le", "authors": "Can M. Le and Elizaveta Levina", "title": "Estimating the number of communities in networks by spectral methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental problem in network analysis with many\nmethods available to estimate communities. Most of these methods assume that\nthe number of communities is known, which is often not the case in practice. We\nstudy a simple and very fast method for estimating the number of communities\nbased on the spectral properties of certain graph operators, such as the\nnon-backtracking matrix and the Bethe Hessian matrix. We show that the method\nperforms well under several models and a wide range of parameters, and is\nguaranteed to be consistent under several asymptotic regimes. We compare this\nmethod to several existing methods for estimating the number of communities and\nshow that it is both more accurate and more computationally efficient.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 06:32:12 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 01:46:05 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Le", "Can M.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1507.00832", "submitter": "Stefan Wager", "authors": "Stefan Wager", "title": "The Efficiency of Density Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density deconvolution problem involves recovering a target density g from\na sample that has been corrupted by noise. From the perspective of Le Cam's\nlocal asymptotic normality theory, we show that non-parametric density\ndeconvolution with Gaussian noise behaves similarly to a low-dimensional\nparametric problem that can easily be solved by maximum likelihood. This\nframework allows us to give a simple account of the statistical efficiency of\ndensity deconvolution and to concisely describe the effect of Gaussian noise on\nour ability to estimate g, all while relying on classical maximum likelihood\ntheory instead of the kernel estimators typically used to study density\ndeconvolution.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 07:13:13 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Wager", "Stefan", ""]]}, {"id": "1507.01037", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Han Liu, Qiang Sun, Tong Zhang", "title": "I-LAMM for Sparse Learning: Simultaneous Control of Algorithmic\n  Complexity and Statistical Error", "comments": "66 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework named iterative local adaptive\nmajorize-minimization (I-LAMM) to simultaneously control algorithmic complexity\nand statistical error when fitting high dimensional models. I-LAMM is a\ntwo-stage algorithmic implementation of the local linear approximation to a\nfamily of folded concave penalized quasi-likelihood. The first stage solves a\nconvex program with a crude precision tolerance to obtain a coarse initial\nestimator, which is further refined in the second stage by iteratively solving\na sequence of convex programs with smaller precision tolerances. Theoretically,\nwe establish a phase transition: the first stage has a sublinear iteration\ncomplexity, while the second stage achieves an improved linear rate of\nconvergence. Though this framework is completely algorithmic, it provides\nsolutions with optimal statistical performances and controlled algorithmic\ncomplexity for a large family of nonconvex optimization problems. The iteration\neffects on statistical errors are clearly demonstrated via a contraction\nproperty. Our theory relies on a localized version of the sparse/restricted\neigenvalue condition, which allows us to analyze a large family of loss and\npenalty functions and provide optimality guarantees under very weak assumptions\n(For example, I-LAMM requires much weaker minimal signal strength than other\nprocedures). Thorough numerical results are provided to support the obtained\ntheory.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 21:44:43 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 18:37:16 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 00:14:02 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Sun", "Qiang", ""], ["Zhang", "Tong", ""]]}, {"id": "1507.01175", "submitter": "Khalil Said", "authors": "V\\'eronique Maume-Deschamps (ICJ), Didier Rulli\\`ere (SAF), Khalil\n  Said (SAF)", "title": "Impact of dependence on some multivariate risk indicators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimization of some multivariate risk indicators may be used as an\nallocation method, as proposed in C\\'enac et al. [6]. The aim of capital\nallocation is to choose a point in a simplex, according to a given criterion.\nIn a previous paper [17] we proved that the proposed allocation technique\nsatisfies a set of coherence axioms. In the present one, we study the\nproperties and asymptotic behavior of the allocation for some distribution\nmodels. We analyze also the impact of the dependence structure on the\nallocation using some copulas.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 08:08:53 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Maume-Deschamps", "V\u00e9ronique", "", "ICJ"], ["Rulli\u00e8re", "Didier", "", "SAF"], ["Said", "Khalil", "", "SAF"]]}, {"id": "1507.01234", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis and Maria Skoularidou", "title": "Estimating the Directed Information and Testing for Causality", "comments": "Minor typos corrected, reviewers' comments addressed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the directed information rate between two discrete\nprocesses $\\{X_n\\}$ and $\\{Y_n\\}$ via the plug-in (or maximum-likelihood)\nestimator is considered. When the joint process $\\{(X_n,Y_n)\\}$ is a Markov\nchain of a given memory length, the plug-in estimator is shown to be\nasymptotically Gaussian and to converge at the optimal rate $O(1/\\sqrt{n})$\nunder appropriate conditions; this is the first estimator that has been shown\nto achieve this rate. An important connection is drawn between the problem of\nestimating the directed information rate and that of performing a hypothesis\ntest for the presence of causal influence between the two processes. Under\nfairly general conditions, the null hypothesis, which corresponds to the\nabsence of causal influence, is equivalent to the requirement that the directed\ninformation rate be equal to zero. In that case a finer result is established,\nshowing that the plug-in converges at the faster rate $O(1/n)$ and that it is\nasymptotically $\\chi^2$-distributed. This is proved by showing that this\nestimator is equal to (a scalar multiple of) the classical likelihood ratio\nstatistic for the above hypothesis test. Finally it is noted that these results\nfacilitate the design of an actual likelihood ratio test for the presence or\nabsence of causal influence.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 15:51:45 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 12:48:51 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 15:24:58 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Skoularidou", "Maria", ""]]}, {"id": "1507.01279", "submitter": "Shuang Li", "authors": "Shuang Li, Yao Xie, Hanjun Dai, and Le Song", "title": "Scan $B$-Statistic for Kernel Change-Point Detection", "comments": "Submitted for journal publication. Partial results appeared in NIPS\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the emergence of an abrupt change-point is a classic problem in\nstatistics and machine learning. Kernel-based nonparametric statistics have\nbeen used for this task which enjoy fewer assumptions on the distributions than\nthe parametric approach and can handle high-dimensional data. In this paper we\nfocus on the scenario when the amount of background data is large, and propose\ntwo related computationally efficient kernel-based statistics for change-point\ndetection, which are inspired by the recently developed $B$-statistics. A novel\ntheoretical result of the paper is the characterization of the tail probability\nof these statistics using the change-of-measure technique, which focuses on\ncharacterizing the tail of the detection statistics rather than obtaining its\nasymptotic distribution under the null distribution. Such approximations are\ncrucial to control the false alarm rate, which corresponds to the significance\nlevel in offline change-point detection and the average-run-length in online\nchange-point detection. Our approximations are shown to be highly accurate.\nThus, they provide a convenient way to find detection thresholds for both\noffline and online cases without the need to resort to the more expensive\nsimulations or bootstrapping. We show that our methods perform well on both\nsynthetic data and real data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 21:46:03 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 18:41:51 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 14:36:53 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 04:22:51 GMT"}, {"version": "v5", "created": "Mon, 12 Nov 2018 20:57:24 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Dai", "Hanjun", ""], ["Song", "Le", ""]]}, {"id": "1507.01296", "submitter": "Qing Zhou", "authors": "Qing Zhou and Seunghyun Min", "title": "Uncertainty Quantification Under Group Sparsity", "comments": "44 pages", "journal-ref": "Biometrika, 104: 613-632, 2017", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the uncertainty in penalized regression under group sparsity is\nan important open question. We establish, under a high-dimensional scaling, the\nasymptotic validity of a modified parametric bootstrap method for the group\nlasso, assuming a Gaussian error model and mild conditions on the design matrix\nand the true coefficients. Simulation of bootstrap samples provides\nsimultaneous inferences on large groups of coefficients. Through extensive\nnumerical comparisons, we demonstrate that our bootstrap method performs much\nbetter than popular competitors, highlighting its practical utility. The\ntheoretical result is generalized to other block norm penalization and\nsub-Gaussian errors, which further broadens the potential applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 23:23:16 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 18:04:24 GMT"}, {"version": "v3", "created": "Sat, 10 Sep 2016 19:58:22 GMT"}, {"version": "v4", "created": "Sun, 4 Jun 2017 21:41:04 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhou", "Qing", ""], ["Min", "Seunghyun", ""]]}, {"id": "1507.01427", "submitter": "Alexei Stepanov", "authors": "Alexei Stepanov", "title": "On the Kendall Correlation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we first discuss the Kendall rank correlation\ncoefficient. In continuous case, we define the Kendall rank correlation\ncoefficient in terms of the concomitants of order statistics, find the expected\nvalue of the Kendall rank correlation coefficient and show that the later is\nfree of n. We also prove that in continuous case the Kendall correlation\ncoefficient converges in probability to its expected value. We then propose to\nconsider the expected value of the Kendall rank correlation coefficient as a\nnew theoretical correlation coefficient which can be an alternative to the\nclassical Pearson product-moment correlation coefficient. At the end of this\nwork we analyze illustrative examples.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 12:56:27 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Stepanov", "Alexei", ""]]}, {"id": "1507.01454", "submitter": "Katharine Turner", "authors": "Vanessa Robins, Katharine Turner", "title": "Principal Component Analysis of Persistent Homology Rank Functions with\n  case studies of Spatial Point Patterns, Sphere Packing and Colloids", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.physd.2016.03.007", "report-no": null, "categories": "math.ST math.AT stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology, while ostensibly measuring changes in topology, captures\nmultiscale geometrical information. It is a natural tool for the analysis of\npoint patterns. In this paper we explore the statistical power of the\n(persistent homology) rank functions. For a point pattern $X$ we construct a\nfiltration of spaces by taking the union of balls of radius $a$ centered on\npoints in $X$, $X_a = \\cup_{x\\in X}B(x,a)$. The rank function\n${\\beta}_k(X):{\\{(a,b)\\in \\mathbb{R}^2: a\\leq b\\}} \\to \\mathbb{R}$ is then\ndefined by ${\\beta}_k(X)(a,b) = rank ( \\iota_*:H_k(X_a) \\to H_k(X_b))$ where\n$\\iota_*$ is the induced map on homology from the inclusion map on spaces. We\nconsider the rank functions as lying in a Hilbert space and show that under\nreasonable conditions the rank functions from multiple simulations or\nexperiments will lie in an affine subspace. This enables us to perform\nfunctional principal component analysis which we apply to experimental data\nfrom colloids at different effective temperatures and of sphere packings with\ndifferent volume fractions. We also investigate the potential of rank functions\nin providing a test of complete spatial randomness of 2D point patterns using\nthe distances to an empirically computed mean rank function of binomial point\npatterns in the unit square.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:41:57 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Robins", "Vanessa", ""], ["Turner", "Katharine", ""]]}, {"id": "1507.01494", "submitter": "Dario Trevisan", "authors": "Eni Musta, Maurizio Pratelli and Dario Trevisan", "title": "Functional Cramer-Rao bounds and Stein estimators in Sobolev spaces, for\n  Brownian motion and Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problems of drift estimation for a shifted Brownian motion\nand intensity estimation for a Cox process on a finite interval $[0,T]$, when\nthe risk is given by the energy functional associated to some fractional\nSobolev space $H^1_0\\subset W^{\\alpha,2}\\subset L^2$. In both situations,\nCramer-Rao lower bounds are obtained, entailing in particular that no unbiased\nestimators with finite risk in $H^1_0$ exist. By Malliavin calculus techniques,\nwe also study super-efficient Stein type estimators (in the Gaussian case).\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 15:21:44 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Musta", "Eni", ""], ["Pratelli", "Maurizio", ""], ["Trevisan", "Dario", ""]]}, {"id": "1507.01524", "submitter": "Emilija Perkovi\\'c", "authors": "Emilija Perkovi\\'c, Johannes Textor, Markus Kalisch, Marloes H.\n  Maathuis", "title": "A Complete Generalized Adjustment Criterion", "comments": "10 pages, 6 figures, To appear in Proceedings of the 31st Conference\n  on Uncertainty in Artificial Intelligence (UAI2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate adjustment is a widely used approach to estimate total causal\neffects from observational data. Several graphical criteria have been developed\nin recent years to identify valid covariates for adjustment from graphical\ncausal models. These criteria can handle multiple causes, latent confounding,\nor partial knowledge of the causal structure; however, their diversity is\nconfusing and some of them are only sufficient, but not necessary. In this\npaper, we present a criterion that is necessary and sufficient for four\ndifferent classes of graphical causal models: directed acyclic graphs (DAGs),\nmaximum ancestral graphs (MAGs), completed partially directed acyclic graphs\n(CPDAGs), and partial ancestral graphs (PAGs). Our criterion subsumes the\nexisting ones and in this way unifies adjustment set construction for a large\nset of graph classes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 16:21:57 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Perkovi\u0107", "Emilija", ""], ["Textor", "Johannes", ""], ["Kalisch", "Markus", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1507.01548", "submitter": "Abdelhakim Necir", "authors": "Souad Benchaira, Djamel Meraghni, Abdelhakim Necir", "title": "Tail product-limit process for truncated data with application to\n  extreme value index estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A weighted Gaussian approximation to tail product-limit process for\nPareto-like distributions of randomly right-truncated data is provided and a\nnew consistent and asymptotically normal estimator of the extreme value index\nis derived. A simulation study is carried out to evaluate the finite sample\nbehavior of the proposed estimator.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 17:42:38 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Benchaira", "Souad", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1507.01615", "submitter": "Kamil Jurczak", "authors": "Kamil Jurczak, Angelika Rohde", "title": "Spectral analysis of high-dimensional sample covariance matrices with\n  missing observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional sample covariance matrices based on independent\nrandom vectors with missing coordinates. The presence of missing observations\nis common in modern applications such as climate studies or gene expression\nmicro-arrays. A weak approximation on the spectral distribution in the \"large\ndimension $d$ and large sample size $n$\" asymptotics is derived for possibly\ndifferent observation probabilities in the coordinates. The spectral\ndistribution turns out to be strongly influenced by the missingness mechanism.\nIn the null case under the missing at random scenario where each component is\nobserved with the same probability $p$, the limiting spectral distribution is a\nMar\\v{c}enko-Pastur law shifted by $(1-p)/p$ to the left. As $d/n\\rightarrow y<\n1$, the almost sure convergence of the extremal eigenvalues to the respective\nboundary points of the support of the limiting spectral distribution is proved,\nwhich are explicitly given in terms of $y$ and $p$. Eventually, the sample\ncovariance matrix is positive definite if $p$ is larger than $$\n1-\\left(1-\\sqrt{y}\\right)^2, $$ whereas this is not true any longer if $p$ is\nsmaller than this quantity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 20:32:25 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 14:29:08 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2015 07:14:20 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2015 12:29:19 GMT"}, {"version": "v5", "created": "Mon, 29 Feb 2016 13:20:54 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Jurczak", "Kamil", ""], ["Rohde", "Angelika", ""]]}, {"id": "1507.01760", "submitter": "Salem Said", "authors": "Salem Said, Lionel Bombrun, Yannick Berthoumieu, Jonathan Manton", "title": "Riemannian Gaussian Distributions on the Space of Symmetric Positive\n  Definite Matrices", "comments": "21 pages, 1 table; accepted for publication in IEEE Trans Inf Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data which lie in the space $\\mathcal{P}_{m\\,}$, of $m \\times m$ symmetric\npositive definite matrices, (sometimes called tensor data), play a fundamental\nrole in applications including medical imaging, computer vision, and radar\nsignal processing. An open challenge, for these applications, is to find a\nclass of probability distributions, which is able to capture the statistical\nproperties of data in $\\mathcal{P}_{m\\,}$, as they arise in real-world\nsituations. The present paper meets this challenge by introducing Riemannian\nGaussian distributions on $\\mathcal{P}_{m\\,}$. Distributions of this kind were\nfirst considered by Pennec in $2006$. However, the present paper gives an exact\nexpression of their probability density function for the first time in existing\nliterature. This leads to two original contributions. First, a detailed study\nof statistical inference for Riemannian Gaussian distributions, uncovering the\nconnection between maximum likelihood estimation and the concept of Riemannian\ncentre of mass, widely used in applications. Second, the derivation and\nimplementation of an expectation-maximisation algorithm, for the estimation of\nmixtures of Riemannian Gaussian distributions. The paper applies this new\nalgorithm, to the classification of data in $\\mathcal{P}_{m\\,}$, (concretely,\nto the problem of texture classification, in computer vision), showing that it\nyields significantly better performance, in comparison to recent approaches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 11:43:36 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 12:15:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Said", "Salem", ""], ["Bombrun", "Lionel", ""], ["Berthoumieu", "Yannick", ""], ["Manton", "Jonathan", ""]]}, {"id": "1507.01772", "submitter": "Hanne Kekkonen", "authors": "Hanne Kekkonen, Matti Lassas and Samuli Siltanen", "title": "Posterior consistency and convergence rates for Bayesian inversion with\n  hypoelliptic operators", "comments": null, "journal-ref": null, "doi": "10.1088/0266-5611/32/8/085005", "report-no": null, "categories": "math.ST math.FA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approach to inverse problems is studied in the case where the\nforward map is a linear hypoelliptic pseudodifferential operator and\nmeasurement error is additive white Gaussian noise. The measurement model for\nan unknown Gaussian random variable $U(x,\\omega)$ is \\begin{eqnarray*}\nM(y,\\omega) = A(U(x,\\omega) )+ \\delta\\hspace{.2mm}\\mathcal{E}(y,\\omega),\n\\end{eqnarray*} where $A$ is a finitely many times smoothing linear\nhypoelliptic operator and $\\delta>0$ is the noise magnitude. The covariance\noperator $C_U$ of $U$ is $2r$ times smoothing, self-adjoint, injective and\nelliptic pseudodifferential operator.\n  If $\\mathcal{E}$ was taking values in $L^2$ then in Gaussian case solving the\nconditional mean (and maximum a posteriori) estimate is linked to solving the\nminimisation problem \\begin{eqnarray*} T_\\delta(M) = \\text{argmin}_{u\\in H^r}\n  \\big\\{\\|A u-m\\|_{L^2}^2+ \\delta^2\\|C_U^{-1/2}u\\|_{L^2}^2 \\big\\}.\n\\end{eqnarray*} However, Gaussian white noise does not take values in $L^2$ but\nin $H^{-s}$ where $s>0$ is big enough. A modification of the above approach to\nsolve the inverse problem is presented, covering the case of white Gaussian\nmeasurement noise. Furthermore, the convergence of conditional mean estimate to\nthe correct solution as $\\delta\\rightarrow 0$ is proven in appropriate function\nspaces using microlocal analysis. Also the contraction of the confidence\nregions is studied.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2015 12:09:53 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 07:11:44 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Kekkonen", "Hanne", ""], ["Lassas", "Matti", ""], ["Siltanen", "Samuli", ""]]}, {"id": "1507.02018", "submitter": "Magali Champion", "authors": "Magali Champion (1), Victor Picheny (2), Matthieu Vignes ((1) MAP5,\n  (2) INRA Toulouse)", "title": "Inferring large graphs using l1-penalized likelihood", "comments": null, "journal-ref": "Statistics and Computing, Springer Verlag (Germany), 2017", "doi": "10.1198/jasa.2011.ap10346", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of recovering the structure of large sparse directed\nacyclic graphs from noisy observations of the system. We propose a novel\nprocedure based on a specific formulation of the l1-norm regularized maximum\nlikelihood, which decomposes the graph estimation into two optimization\nsub-problems: topological structure and node order learning. We provide oracle\ninequalities for the graph estimator, as well as an algorithm to solve the\ninduced optimization problem, in the form of a convex program embedded in a\ngenetic algorithm. We apply our method to various data sets (including data\nfrom the DREAM4 challenge) and show that it compares favorably to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:21:57 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 09:56:53 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 08:39:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Champion", "Magali", ""], ["Picheny", "Victor", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1507.02061", "submitter": "Jana Jankova", "authors": "Jana Jankov\\'a and Sara van de Geer", "title": "Honest confidence regions and optimality in high-dimensional precision\n  matrix estimation", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methodology for estimation of sparse precision matrices and\nstatistical inference for their low-dimensional parameters in a\nhigh-dimensional setting where the number of parameters $p$ can be much larger\nthan the sample size. We show that the novel estimator achieves minimax rates\nin supremum norm and the low-dimensional components of the estimator have a\nGaussian limiting distribution. These results hold uniformly over the class of\nprecision matrices with row sparsity of small order $\\sqrt{n}/\\log p$ and\nspectrum uniformly bounded, under a sub-Gaussian tail assumption on the margins\nof the true underlying distribution. Consequently, our results lead to\nuniformly valid confidence regions for low-dimensional parameters of the\nprecision matrix. Thresholding the estimator leads to variable selection\nwithout imposing irrepresentability conditions. The performance of the method\nis demonstrated in a simulation study and on real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 08:23:12 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 17:18:48 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Jankov\u00e1", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1507.02074", "submitter": "Daniel Nevo", "authors": "Daniel Nevo and Ya'acov Ritov", "title": "On Bayesian robust regression with diverging number of predictors", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the robust regression model when the number of predictors\nand the number of observations grow in a similar rate. Theory for M-estimators\nin this regime has been recently developed by several authors [El Karoui et\nal., 2013, Bean et al., 2013, Donoho and Montanari, 2013].\n  Motivated by the inability of M-estimators to successfully estimate the\nEuclidean norm of the coefficient vector, we consider a Bayesian framework for\nthis model. We suggest a two-component mixture of normals prior for the\ncoefficients and develop a Gibbs sampler procedure for sampling from relevant\nposterior distributions, while utilizing a scale mixture of normal\nrepresentation for the error distribution . Unlike M-estimators, the proposed\nBayes estimator is consistent in the Euclidean norm sense. Simulation results\ndemonstrate the superiority of the Bayes estimator over traditional estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 09:20:50 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 20:59:56 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Nevo", "Daniel", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1507.02493", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Michael Jansson, Whitney K. Newey", "title": "Inference in Linear Regression Models with Many Covariates and\n  Heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear regression model is widely used in empirical work in Economics,\nStatistics, and many other disciplines. Researchers often include many\ncovariates in their linear model specification in an attempt to control for\nconfounders. We give inference methods that allow for many covariates and\nheteroskedasticity. Our results are obtained using high-dimensional\napproximations, where the number of included covariates are allowed to grow as\nfast as the sample size. We find that all of the usual versions of Eicker-White\nheteroskedasticity consistent standard error estimators for linear models are\ninconsistent under this asymptotics. We then propose a new heteroskedasticity\nconsistent standard error formula that is fully automatic and robust to both\n(conditional)\\ heteroskedasticity of unknown form and the inclusion of possibly\nmany covariates. We apply our findings to three settings: parametric linear\nmodels with many covariates, linear panel models with many fixed effects, and\nsemiparametric semi-linear models with many technical regressors. Simulation\nevidence consistent with our theoretical results is also provided. The proposed\nmethods are also illustrated with an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 13:13:47 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 16:25:20 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Newey", "Whitney K.", ""]]}, {"id": "1507.02608", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Alain Hauser and Marloes H. Maathuis", "title": "High-dimensional consistency in score-based and hybrid structure\n  learning", "comments": "37 pages, 5 figures, 41 pages supplement (available as an ancillary\n  file)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main approaches for learning Bayesian networks can be classified as\nconstraint-based, score-based or hybrid methods. Although high-dimensional\nconsistency results are available for constraint-based methods like the PC\nalgorithm, such results have not been proved for score-based or hybrid methods,\nand most of the hybrid methods have not even shown to be consistent in the\nclassical setting where the number of variables remains fixed and the sample\nsize tends to infinity. In this paper, we show that consistency of hybrid\nmethods based on greedy equivalence search (GES) can be achieved in the\nclassical setting with adaptive restrictions on the search space that depend on\nthe current state of the algorithm. Moreover, we prove consistency of GES and\nadaptively restricted GES (ARGES) in several sparse high-dimensional settings.\nARGES scales well to sparse graphs with thousands of variables and our\nsimulation study indicates that both GES and ARGES generally outperform the PC\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 17:31:52 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:14:02 GMT"}, {"version": "v3", "created": "Mon, 9 May 2016 11:46:51 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 18:47:46 GMT"}, {"version": "v5", "created": "Sat, 4 Nov 2017 20:36:18 GMT"}, {"version": "v6", "created": "Sat, 3 Feb 2018 17:38:36 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Nandy", "Preetam", ""], ["Hauser", "Alain", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1507.02769", "submitter": "Yaakov Malinovsky", "authors": "Abram M. Kagan and Yaakov Malinovsky", "title": "On the structure of UMVUEs", "comments": "Accepted for publication in Sankhya A", "journal-ref": null, "doi": "10.1007/s13171-015-0076-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In all setups when the structure of UMVUEs is known, there exists a\nsubalgebra $\\cal U$ (MVE-algebra) of the basic $\\sigma$-algebra such that all\n$\\cal U$-measurable statistics with finite second moments are UMVUEs. It is\nshown that MVE-algebras are, in a sense, similar to the subalgebras generated\nby complete sufficient statistics. Examples are given when these subalgebras\ndiffer, in these cases a new statistical structure arises.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 02:48:53 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Kagan", "Abram M.", ""], ["Malinovsky", "Yaakov", ""]]}, {"id": "1507.02838", "submitter": "Dennis Dobler", "authors": "Dennis Dobler and Jan Beyersmann and Markus Pauly", "title": "Non-strange Weird Resampling for Complex Survival Data", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the new data-dependent multiplier bootstrap for\nnon-parametric analysis of survival data, possibly subject to competing risks.\nThe new resampling procedure includes both the general wild bootstrap and the\nweird bootstrap as special cases. The data may be subject to independent\nright-censoring and left-truncation. We rigorously prove asymptotic correctness\nwhich has in particular been pending for the weird bootstrap. As a consequence,\npointwise as well as time-simultaneous inference procedures for, amongst\nothers, the classical survival setting are deduced. We report simulation\nresults and a real data analysis of the cumulative cardiovascular event\nprobability. The simulation results suggest that both the weird bootstrap and\nuse of non-standard multipliers in the wild bootstrap may perform preferably.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 10:36:33 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 07:59:09 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Dobler", "Dennis", ""], ["Beyersmann", "Jan", ""], ["Pauly", "Markus", ""]]}, {"id": "1507.02848", "submitter": "Akihiko Inoue", "authors": "Akihiko Inoue, Yukio Kasahara and Mohsen Pourahmadi", "title": "Baxter's inequality for finite predictor coefficients of multivariate\n  long-memory stationary processes", "comments": "To appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a multivariate stationary process, we develop explicit representations\nfor the finite predictor coefficient matrices, the finite prediction error\ncovariance matrices and the partial autocorrelation function (PACF) in terms of\nthe Fourier coefficients of its phase function in the spectral domain. The\nderivation is based on a novel alternating projection technique and the use of\nthe forward and backward innovations corresponding to the predictions based on\nthe infinite past and future, respectively. We show that such representations\nare ideal for studying the rates of convergence of the finite predictor\ncoefficients, prediction error covariances, and the PACF as well as for proving\na multivariate version of Baxter's inequality for a multivariate FARIMA process\nwith a common fractional differencing order for all components of the process.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 11:05:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 09:30:34 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 06:03:50 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2016 06:35:51 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Inoue", "Akihiko", ""], ["Kasahara", "Yukio", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "1507.02887", "submitter": "Sylvain Delattre Mr", "authors": "Sylvain Delattre and Nicolas Fournier", "title": "Statistical inference versus mean field limit for Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a population of $N$ individuals, of which we observe the number\nof actions as time evolves. For each couple of individuals $(i,j)$, $j$ may or\nnot influence $i$, which we model by i.i.d. Bernoulli$(p)$-random variables,\nfor some unknown parameter $p\\in (0,1]$. Each individual acts autonomously at\nsome unknown rate $\\mu>0$ and acts by mimetism at some rate depending on the\nnumber of recent actions of the individuals which influence him, the age of\nthese actions being taken into account through an unknown function $\\varphi$\n(roughly, decreasing and with fast decay). The goal of this paper is to\nestimate $p$, which is the main charateristic of the graph of interactions, in\nthe asymptotic $N\\to\\infty$, $t\\to\\infty$. The main issue is that the mean\nfield limit (as $N \\to \\infty$) of this model is unidentifiable, in that it\nonly depends on the parameters $\\mu$ and $p\\varphi$. Fortunately, this mean\nfield limit is not valid for large times. We distinguish the subcritical case,\nwhere, roughly, the mean number $m_t$ of actions per individual increases\nlinearly and the supercritical case, where $m_t$ increases exponentially.\nAlthough the nuisance parameter $\\varphi$ is non-parametric, we are able, in\nboth cases, to estimate $p$ without estimating $\\varphi$ in a nonparametric\nway, with a precision of order $N^{-1/2}+N^{1/2}m_t^{-1}$, up to some\narbitrarily small loss. We explain, using a Gaussian toy model, the reason why\nthis rate of convergence might be (almost) optimal.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 13:16:10 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 08:56:37 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Delattre", "Sylvain", ""], ["Fournier", "Nicolas", ""]]}, {"id": "1507.02904", "submitter": "Jeremie Kellner", "authors": "J\\'er\\'emie Kellner (INRIA Lille - Nord Europe), Alain Celisse (INRIA\n  Lille - Nord Europe)", "title": "A One-Sample Test for Normality with Kernel Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new one-sample test for normality in a Reproducing Kernel\nHilbert Space (RKHS). Namely, we test the null-hypothesis of belonging to a\ngiven family of Gaussian distributions. Hence our procedure may be applied\neither to test data for normality or to test parameters (mean and covariance)\nif data are assumed Gaussian. Our test is based on the same principle as the\nMMD (Maximum Mean Discrepancy) which is usually used for two-sample tests such\nas homogeneity or independence testing. Our method makes use of a special kind\nof parametric bootstrap (typical of goodness-of-fit tests) which is\ncomputationally more efficient than standard parametric bootstrap. Moreover, an\nupper bound for the Type-II error highlights the dependence on influential\nquantities. Experiments illustrate the practical improvement allowed by our\ntest in high-dimensional settings where common normality tests are known to\nfail. We also consider an application to covariance rank selection through a\nsequential procedure.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 13:56:03 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Kellner", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe"], ["Celisse", "Alain", "", "INRIA\n  Lille - Nord Europe"]]}, {"id": "1507.03003", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban and Stefan Wager", "title": "High-Dimensional Asymptotics of Prediction: Ridge Regression and\n  Classification", "comments": "Added a section on prediction versus estimation for ridge regression.\n  Rewrote introduction. Other results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unified analysis of the predictive risk of ridge regression and\nregularized discriminant analysis in a dense random effects model. We work in a\nhigh-dimensional asymptotic regime where $p, n \\to \\infty$ and $p/n \\to \\gamma\n\\in (0, \\, \\infty)$, and allow for arbitrary covariance among the features. For\nboth methods, we provide an explicit and efficiently computable expression for\nthe limiting predictive risk, which depends only on the spectrum of the\nfeature-covariance matrix, the signal strength, and the aspect ratio $\\gamma$.\nEspecially in the case of regularized discriminant analysis, we find that\npredictive accuracy has a nuanced dependence on the eigenvalue distribution of\nthe covariance matrix, suggesting that analyses based on the operator norm of\nthe covariance matrix may not be sharp. Our results also uncover several\nqualitative insights about both methods: for example, with ridge regression,\nthere is an exact inverse relation between the limiting predictive risk and the\nlimiting estimation risk given a fixed signal strength. Our analysis builds on\nrecent advances in random matrix theory.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 19:48:07 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 20:38:45 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Dobriban", "Edgar", ""], ["Wager", "Stefan", ""]]}, {"id": "1507.03111", "submitter": "Boumediene Hamzi", "authors": "Fritz Colonius and Boumediene Hamzi", "title": "Kernel Methods for Linear Discrete-Time Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from learning theory are used in the state space of linear dynamical\nand control systems in order to estimate the system matrices. An application to\nstabilization via algebraic Riccati equations is included. The approach is\nillustrated via a series of numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 14:49:12 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 20:12:37 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Colonius", "Fritz", ""], ["Hamzi", "Boumediene", ""]]}, {"id": "1507.03178", "submitter": "Abdelhakim Necir", "authors": "Louiza Soltane, Djamel Meraghni, Abdelhakim Necir", "title": "Estimating the mean of a heavy-tailed distribution under random\n  censoring", "comments": "arXiv admin note: text overlap with arXiv:1302.1666", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The central limit theorem introduced by Stute [The central limit theorem\nunder random censorship. Ann. Statist. 1995; 23: 422-439] does not hold for\nsome class of heavy-tailed distributions. In this paper, we make use of the\nextreme value theory to propose an alternative estimating approach of the mean\nensuring the asymptotic normality property. A simulation study is carried out\nto evaluate the performance of this estimation procedure\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 02:38:12 GMT"}], "update_date": "2015-07-19", "authors_parsed": [["Soltane", "Louiza", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1507.03245", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "A Geometric Approach for Bounding Average Stopping Time", "comments": "39 pages, no figure, Published in Proceedings of SPIE Conferences,\n  April 2015, simplify proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric approach for bounding average stopping times for\nstopped random walks in discrete and continuous time. We consider stopping\ntimes in the hyperspace of time indexes and stochastic processes. Our\ntechniques relies on exploring geometric properties of continuity or stopping\nregions. Especially, we make use of the concepts of convex sets and supporting\nhyperplane. Explicit formulae and efficiently computable bounds are obtained\nfor average stopping times. Our techniques can be applied to bound average\nstopping times involving random vectors, nonlinear stopping boundary, and\nconstraints of time indexes. Moreover, we establish a stochastic characteristic\nof convex sets and generalize Jensen's inequality, Wald's equations and\nLorden's inequality, which are useful for investigating average stopping times.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 16:20:53 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 21:26:18 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 00:07:00 GMT"}, {"version": "v4", "created": "Sun, 24 Jun 2018 17:58:35 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1507.03263", "submitter": "P. J. C. Spreij", "authors": "Shota Gugushvili, Frank van der Meulen and Peter Spreij", "title": "A non-parametric Bayesian approach to decompounding from high frequency\n  data", "comments": null, "journal-ref": "Stat. Inference Stoch. Process. 21 (2018), no. 1, 53-79", "doi": "10.1007/s11203-016-9153-1", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample from a discretely observed compound Poisson process, we\nconsider non-parametric estimation of the density $f_0$ of its jump sizes, as\nwell as of its intensity $\\lambda_0.$ We take a Bayesian approach to the\nproblem and specify the prior on $f_0$ as the Dirichlet location mixture of\nnormal densities. An independent prior for $\\lambda_0$ is assumed to be\ncompactly supported and to possess a positive density with respect to the\nLebesgue measure. We show that under suitable assumptions the posterior\ncontracts around the pair $(\\lambda_0,f_0)$ at essentially (up to a logarithmic\nfactor) the $\\sqrt{n\\Delta}$-rate, where $n$ is the number of observations and\n$\\Delta$ is the mesh size at which the process is sampled. The emphasis is on\nhigh frequency data, $\\Delta\\to 0$, but the obtained results are also valid for\nfixed $\\Delta$. In either case we assume that $n\\Delta\\rightarrow\\infty$. Our\nmain result implies existence of Bayesian point estimates converging (in the\nfrequentist sense, in probability) to $(\\lambda_0,f_0)$ at the same rate.\n  We also discuss a practical implementation of our approach. The computational\nproblem is dealt with by inclusion of auxiliary variables and we develop a\nMarkov Chain Monte Carlo algorithm that samples from the joint distribution of\nthe unknown parameters in the mixture density and the introduced auxiliary\nvariables. Numerical examples illustrate the feasibility of this approach.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2015 19:56:26 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 12:54:02 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 08:29:39 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Spreij", "Peter", ""]]}, {"id": "1507.03315", "submitter": "Jos\\'e A. D\\'iaz-Garc\\'ia", "authors": "Jos\\'e A. D\\'iaz-Garc\\'ia and Francisco J. Caro-Lopera", "title": "Estimation of mean form and mean form difference under elliptical laws", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some ideas studied by Lele (1993), under a Gaussian perturbation model, are\ngeneralised in the setting of matrix multivariate elliptical distributions. In\nparticular, several inaccuracies in the published statistical perturbation\nmodel are revised. In addition, a number of aspects about identifiability and\nestimability are also considered. Instead of using the Euclidean distance\nmatrix for proposing consistent estimates, this paper determines exact formulae\nfor the moments of matrix $\\mathbf{B} =\n\\mathbf{X}^{c}\\left(\\mathbf{X}^{c}\\right)^{T}$, where $\\mathbf{X}^{c}$ is the\ncentered landmarks matrix. Consistent estimation of mean form difference under\nelliptical laws is also studied. Finally, the main results of the paper and\nsome methodologies for selecting models and hypothesis testing are applied to a\nreal landmark data. comparing correlation shape structure is proposed and\napplied in handwritten differentiation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 03:03:38 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["D\u00edaz-Garc\u00eda", "Jos\u00e9 A.", ""], ["Caro-Lopera", "Francisco J.", ""]]}, {"id": "1507.03429", "submitter": "Lukas Martig", "authors": "Lukas Martig, J\\\"urg H\\\"usler", "title": "On consistency of the likelihood moment estimators for a linear process\n  with regularly varying innovations", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1975 James Pickands III showed that the excesses over a high threshold are\napproximatly Generalized Pareto distributed. Since then, a variety of\nestimators for the parameters of this cdf have been studied, but always\nassuming the underlying data to be independent. In this paper we consider the\nspecial case where the underlying data arises from a linear process with\nregularly varying (i.e. heavy-tailed) innovations. Using this setup, we then\nshow that the likelihood moment estimators introduced by Zhang (2007) are\nconsistent estimators for the parameters of the Generalized Pareto\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 12:45:33 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:19:55 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Martig", "Lukas", ""], ["H\u00fcsler", "J\u00fcrg", ""]]}, {"id": "1507.03558", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Themis Gouleakis, and Ronitt\n  Rubinfeld", "title": "Testing Shape Restrictions of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the question of testing structured properties (classes) of discrete\ndistributions. Specifically, given sample access to an arbitrary distribution\n$D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish between\n$D\\in\\mathcal{P}$ and $\\ell_1(D,\\mathcal{P})>\\varepsilon$. We develop a general\nalgorithm for this question, which applies to a large range of\n\"shape-constrained\" properties, including monotone, log-concave, $t$-modal,\npiecewise-polynomial, and Poisson Binomial distributions. Moreover, for all\ncases considered, our algorithm has near-optimal sample complexity with regard\nto the domain size and is computationally efficient. For most of these classes,\nwe provide the first non-trivial tester in the literature. In addition, we also\ndescribe a generic method to prove lower bounds for this problem, and use it to\nshow our upper bounds are nearly tight. Finally, we extend some of our\ntechniques to tolerant testing, deriving nearly-tight upper and lower bounds\nfor the corresponding questions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 19:22:41 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 16:28:19 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 19:56:27 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1507.03652", "submitter": "Adam Bloniarz", "authors": "Adam Bloniarz, Hanzhong Liu, Cun-Hui Zhang, Jasjeet Sekhon and Bin Yu", "title": "Lasso adjustments of treatment effect estimates in randomized\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a principled way for investigators to analyze randomized\nexperiments when the number of covariates is large. Investigators often use\nlinear multivariate regression to analyze randomized experiments instead of\nsimply reporting the difference of means between treatment and control groups.\nTheir aim is to reduce the variance of the estimated treatment effect by\nadjusting for covariates. If there are a large number of covariates relative to\nthe number of observations, regression may perform poorly because of\noverfitting. In such cases, the Lasso may be helpful. We study the resulting\nLasso-based treatment effect estimator under the Neyman-Rubin model of\nrandomized experiments. We present theoretical conditions that guarantee that\nthe estimator is more efficient than the simple difference-of-means estimator,\nand we provide a conservative estimator of the asymptotic variance, which can\nyield tighter confidence intervals than the difference-of-means estimator.\nSimulation and data examples show that Lasso-based adjustment can be\nadvantageous even when the number of covariates is less than the number of\nobservations. Specifically, a variant using Lasso for selection and OLS for\nestimation performs particularly well, and it chooses a smoothing parameter\nbased on combined performance of Lasso and OLS.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 23:24:17 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 21:30:11 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2015 08:16:30 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 03:48:51 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Bloniarz", "Adam", ""], ["Liu", "Hanzhong", ""], ["Zhang", "Cun-Hui", ""], ["Sekhon", "Jasjeet", ""], ["Yu", "Bin", ""]]}, {"id": "1507.03829", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier, Richard Nickl", "title": "On signal detection and confidence sets for low rank inference problems", "comments": "This paper will appear in the Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the signal detection problem in the Gaussian design trace\nregression model with low rank alternative hypotheses. We derive the precise\n(Ingster-type) detection boundary for the Frobenius and the nuclear norm. We\nthen apply these results to show that honest confidence sets for the unknown\nmatrix parameter that adapt to all low rank sub-models in nuclear norm do not\nexist. This shows that recently obtained positive results in (Carpentier,\nEisert, Gross and Nickl, 2015) for confidence sets in low rank recovery\nproblems are essentially optimal.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 12:49:28 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 12:00:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Nickl", "Richard", ""]]}, {"id": "1507.03895", "submitter": "Zhigen Zhao", "authors": "Qian Lin, Zhigen Zhao, Jun S. Liu", "title": "On consistency and sparsity for sliced inverse regression in high\n  dimensions", "comments": "49 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide here a framework to analyze the phase transition phenomenon of\nslice inverse regression (SIR), a supervised dimension reduction technique\nintroduced by \\cite{Li:1991}. Under mild conditions, the asymptotic ratio\n$\\rho= \\lim p/n$ is the phase transition parameter and the SIR estimator is\nconsistent if and only if $\\rho= 0$. When dimension $p$ is greater than $n$, we\npropose a diagonal thresholding screening SIR (DT-SIR) algorithm. This method\nprovides us with an estimate of the eigen-space of the covariance matrix of the\nconditional expectation $var(\\mathbf{E}[\\boldsymbol{x}|y])$. The desired\ndimension reduction space is then obtained by multiplying the inverse of the\ncovariance matrix on the eigen-space. Under certain sparsity assumptions on\nboth the covariance matrix of predictors and the loadings of the directions, we\nprove the consistency of DT-SIR in estimating the dimension reduction space in\nhigh dimensional data analysis. Extensive numerical experiments demonstrate\nsuperior performances of the proposed method in comparison to its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 15:42:07 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 05:40:55 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Lin", "Qian", ""], ["Zhao", "Zhigen", ""], ["Liu", "Jun S.", ""]]}, {"id": "1507.03984", "submitter": "Peng Ding", "authors": "Peng Ding and Tyler VanderWeele", "title": "Sensitivity Analysis Without Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding may undermine the validity of causal inference with\nobservational studies. Sensitivity analysis provides an attractive way to\npartially circumvent this issue by assessing the potential influence of\nunmeasured confounding on the causal conclusions. However, previous sensitivity\nanalysis approaches often make strong and untestable assumptions such as having\na confounder that is binary, or having no interaction between the effects of\nthe exposure and the confounder on the outcome, or having only one confounder.\nWithout imposing any assumptions on the confounder or confounders, we derive a\nbounding factor and a sharp inequality such that the sensitivity analysis\nparameters must satisfy the inequality if an unmeasured confounder is to\nexplain away the observed effect estimate or reduce it to a particular level.\nOur approach is easy to implement and involves only two sensitivity parameters.\nSurprisingly, our bounding factor, which makes no simplifying assumptions, is\nno more conservative than a number of previous sensitivity analysis techniques\nthat do make assumptions. Our new bounding factor implies not only the\ntraditional Cornfield conditions that both the relative risk of the exposure on\nthe confounder and that of the confounder on the outcome must satisfy, but also\na high threshold that the maximum of these relative risks must satisfy.\nFurthermore, this new bounding factor can be viewed as a measure of the\nstrength of confounding between the exposure and the outcome induced by a\nconfounder.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 19:57:33 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Ding", "Peng", ""], ["VanderWeele", "Tyler", ""]]}, {"id": "1507.04090", "submitter": "Thomas Rippl", "authors": "Thomas Rippl, Axel Munk and Anja Sturm", "title": "Limit laws of the empirical Wasserstein distance: Gaussian distributions", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive central limit theorems for the Wasserstein distance between the\nempirical distributions of Gaussian samples. The cases are distinguished\nwhether the underlying laws are the same or different. Results are based on the\n(quadratic) Frechet differentiability of the Wasserstein distance in the\nGaussian case. Extensions to elliptically symmetric distributions are discussed\nas well as several applications such as bootstrap and statistical testing.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 05:42:44 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 16:59:19 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Rippl", "Thomas", ""], ["Munk", "Axel", ""], ["Sturm", "Anja", ""]]}, {"id": "1507.04118", "submitter": "Olga Klopp", "authors": "Olga Klopp (1), Alexandre B. Tsybakov (2), Nicolas Verzelen (3) ((1)\n  MODAL'X, CREST, (2) CREST, (3) MISTEA)", "title": "Oracle inequalities for network models and sparse graphon estimation", "comments": "Annals of Statistics, Institute of Mathematical Statistics, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inhomogeneous random graph models encompass many network models such as\nstochastic block models and latent position models. We consider the problem of\nstatistical estimation of the matrix of connection probabilities based on the\nobservations of the adjacency matrix of the network. Taking the stochastic\nblock model as an approximation, we construct estimators of network connection\nprobabilities -- the ordinary block constant least squares estimator, and its\nrestricted version. We show that they satisfy oracle inequalities with respect\nto the block constant oracle. As a consequence, we derive optimal rates of\nestimation of the probability matrix. Our results cover the important setting\nof sparse networks. Another consequence consists in establishing upper bounds\non the minimax risks for graphon estimation in the $L\\_2$ norm when the\nprobability matrix is sampled according to a graphon model. These bounds\ninclude an additional term accounting for the \"agnostic\" error induced by the\nvariability of the latent unobserved variables of the graphon model. In this\nsetting, the optimal rates are influenced not only by the bias and variance\ncomponents as in usual nonparametric problems but also include the third\ncomponent, which is the agnostic error. The results shed light on the\ndifferences between estimation under the empirical loss (the probability matrix\nestimation) and under the integrated loss (the graphon estimation).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:30:15 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 14:40:56 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 07:26:35 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 12:33:15 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Klopp", "Olga", ""], ["Tsybakov", "Alexandre B.", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1507.04121", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Solomonoff Induction Violates Nicod's Criterion", "comments": "ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nicod's criterion states that observing a black raven is evidence for the\nhypothesis H that all ravens are black. We show that Solomonoff induction does\nnot satisfy Nicod's criterion: there are time steps in which observing black\nravens decreases the belief in H. Moreover, while observing any computable\ninfinite string compatible with H, the belief in H decreases infinitely often\nwhen using the unnormalized Solomonoff prior, but only finitely often when\nusing the normalized Solomonoff prior. We argue that the fault is not with\nSolomonoff induction; instead we should reject Nicod's criterion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:37:52 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1507.04189", "submitter": "Julien Worms", "authors": "Julien Worms (LM-Versailles), Rym Worms (LAMA)", "title": "A Lynden-Bell integral estimator for extremes of randomly truncated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with the estimation of the extreme value index and extreme\nquantiles for heavy tailed data,randomly right truncated by another heavy\ntailed variable. Under mild assumptions and the condition thatthe truncated\nvariable is less heavy-tailed than the truncating variable, asymptotic\nnormality is proved for bothestimators. The proposed estimator of the extreme\nvalue index is an adaptation of the Hill estimator, in thenatural form of a\nLynden-Bell integral. Simulations illustrate the quality of the estimators\nunder a variety ofsituations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 12:35:15 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Worms", "Julien", "", "LM-Versailles"], ["Worms", "Rym", "", "LAMA"]]}, {"id": "1507.04228", "submitter": "Radu Stoica", "authors": "R. S. Stoica, A. Philippe, P. Gregori, J. Mateu", "title": "ABC Shadow algorithm: a tool for statistical analysis of spatial\n  patterns", "comments": "34 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an original ABC algorithm, \"ABC Shadow\", that can be\napplied to sample posterior densities that are continuously differentiable. The\nproposed method uses the ideas given by the auxiliary variable MH of (M\\o ller\nand Waagepetersen, 2004). The obtained algorithm solves the main condition to\nbe fulfilled by any ABC algorithm, in order to be useful in practice. This\ncondition requires enough samples in the parameter space region, induced by the\nobserved statistics (Blum, 2010). The algorithm is tuned on the posterior of a\nGaussian model which is entirely known, and then it is applied for the\nstatistical analysis of several spatial patterns. These patterns are issued or\nassumed to be outcomes of point processes. The considered models are: Strauss,\nCandy and area-interaction.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 14:16:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 13:54:05 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 12:43:25 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Stoica", "R. S.", ""], ["Philippe", "A.", ""], ["Gregori", "P.", ""], ["Mateu", "J.", ""]]}, {"id": "1507.04313", "submitter": "Jonas Kahn", "authors": "Philippe Heinrich and Jonas Kahn", "title": "Optimal rates for finite mixture estimation", "comments": "48 pages, 1 figure, submitted to The Annals of Statistics, as a main\n  article (30 pages) and the appendices (19 pages) as supplemental material.\n  Part of the material appears in an earlier version appears as\n  arXiv:1504.03506, but without any result on pointwise rates, any figure, much\n  less bibliography and explanations, and overall different presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the rates of estimation of finite mixing distributions, that is, the\nparameters of the mixture. We prove that under some regularity and strong\nidentifiability conditions, around a given mixing distribution with $m_0$\ncomponents, the optimal local minimax rate of estimation of a mixing\ndistribution with $m$ components is $n^{-1/(4(m-m_0) + 2)}$. This corrects a\nprevious paper by Chen (1995) in The Annals of Statistics.\n  By contrast, it turns out that there are estimators with a (non-uniform)\npointwise rate of estimation of $n^{-1/2}$ for all mixing distributions with a\nfinite number of components.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 18:20:17 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Heinrich", "Philippe", ""], ["Kahn", "Jonas", ""]]}, {"id": "1507.04398", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e R. Berrendero, Antonio Cuevas and Jos\\'e L. Torrecilla", "title": "On the use of reproducing kernel Hilbert spaces in functional\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The H\\'ajek-Feldman dichotomy establishes that two Gaussian measures are\neither mutually absolutely continuous with respect to each other (and hence\nthere is a Radon-Nikodym density for each measure with respect to the other\none) or mutually singular. Unlike the case of finite dimensional Gaussian\nmeasures, there are non-trivial examples of both situations when dealing with\nGaussian stochastic processes. This paper provides:\n  (a) Explicit expressions for the optimal (Bayes) rule and the minimal\nclassification error probability in several relevant problems of supervised\nbinary classification of mutually absolutely continuous Gaussian processes. The\napproach relies on some classical results in the theory of Reproducing Kernel\nHilbert Spaces (RKHS).\n  (b) An interpretation, in terms of mutual singularity, for the \"near perfect\nclassification\" phenomenon described by Delaigle and Hall (2012). We show that\nthe asymptotically optimal rule proposed by these authors can be identified\nwith the sequence of optimal rules for an approximating sequence of\nclassification problems in the absolutely continuous case.\n  (c) A new model-based method for variable selection in binary classification\nproblems, which arises in a very natural way from the explicit knowledge of the\nRN-derivatives and the underlying RKHS structure. Different classifiers might\nbe used from the selected variables. In particular, the classical, linear\nfinite-dimensional Fisher rule turns out to be consistent under some standard\nconditions on the underlying functional model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 21:43:09 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 19:06:25 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 16:04:50 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""], ["Torrecilla", "Jos\u00e9 L.", ""]]}, {"id": "1507.04528", "submitter": "Ilaria Bianchini", "authors": "Raffaele Argiento, Ilaria Bianchini and Alessandra Guglielmi", "title": "A priori truncation method for posterior sampling from homogeneous\n  normalized completely random measure mixture models", "comments": "32 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adopts a Bayesian nonparametric mixture model where the mixing\ndistribution belongs to the wide class of normalized homogeneous completely\nrandom measures. We propose a truncation method for the mixing distribution by\ndiscarding the weights of the unnormalized measure smaller than a threshold. We\nprove convergence in law of our approximation, provide some theoretical\nproperties and characterize its posterior distribution so that a blocked Gibbs\nsampler is devised. The versatility of the approximation is illustrated by two\ndifferent applications. In the first the normalized Bessel random measure,\nencompassing the Dirichlet process, is introduced; goodness of fit indexes show\nits good performances as mixing measure for density estimation. The second\ndescribes how to incorporate covariates in the support of the normalized\nmeasure, leading to a linear dependent model for regression and clustering.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 11:18:01 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Argiento", "Raffaele", ""], ["Bianchini", "Ilaria", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "1507.04793", "submitter": "Mahdi Soltanolkotabi", "authors": "Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi", "title": "Sharp Time--Data Tradeoffs for Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we characterize sharp time-data tradeoffs for optimization\nproblems used for solving linear inverse problems. We focus on the minimization\nof a least-squares objective subject to a constraint defined as the sub-level\nset of a penalty function. We present a unified convergence analysis of the\ngradient projection algorithm applied to such problems. We sharply characterize\nthe convergence rate associated with a wide variety of random measurement\nensembles in terms of the number of measurements and structural complexity of\nthe signal with respect to the chosen penalty function. The results apply to\nboth convex and nonconvex constraints, demonstrating that a linear convergence\nrate is attainable even though the least squares objective is not strongly\nconvex in these settings. When specialized to Gaussian measurements our results\nshow that such linear convergence occurs when the number of measurements is\nmerely 4 times the minimal number required to recover the desired signal at all\n(a.k.a. the phase transition). We also achieve a slower but geometric rate of\nconvergence precisely above the phase transition point. Extensive numerical\nresults suggest that the derived rates exactly match the empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 23:03:00 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 06:04:43 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1507.05021", "submitter": "Alain Durmus", "authors": "Alain Durmus (LTCI), Eric Moulines (CMAP)", "title": "Non-asymptotic convergence analysis for the Unadjusted Langevin\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a method to sample from a target distribution $\\pi$\nover $\\mathbb{R}^d$ having a positive density with respect to the Lebesgue\nmeasure, known up to a normalisation factor. This method is based on the Euler\ndiscretization of the overdamped Langevin stochastic differential equation\nassociated with $\\pi$. For both constant and decreasing step sizes in the Euler\ndiscretization, we obtain non-asymptotic bounds for the convergence to the\ntarget distribution $\\pi$ in total variation distance. A particular attention\nis paid to the dependency on the dimension $d$, to demonstrate the\napplicability of this method in the high dimensional setting. These bounds\nimprove and extend the results of (Dalalyan 2014).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 16:23:23 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 17:07:11 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 14:27:42 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Durmus", "Alain", "", "LTCI"], ["Moulines", "Eric", "", "CMAP"]]}, {"id": "1507.05034", "submitter": "Vladimir Spokoiny", "authors": "Vladimir Spokoiny and Niklas Willrich", "title": "Bootstrap tuning in ordered model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of model selection for a given family of linear estimators,\nordered by their variance, we offer a new \"smallest accepted\" approach\nmotivated by Lepski's method and multiple testing theory. The procedure selects\nthe smallest model which satisfies an acceptance rule based on comparison with\nall larger models. The method is completely data-driven and does not use any\nprior information about the variance structure of the noise: its parameters are\nadjusted to the underlying possibly heterogeneous noise by the so-called\n\"propagation condition\" using a wild bootstrap method. The validity of the\nbootstrap calibration is proved for finite samples with an explicit error\nbound. We provide a comprehensive theoretical study of the method and describe\nin detail the set of possible values of the selector \\( \\hat{m} \\). We also\nestablish some precise oracle error bounds for the corresponding estimator \\(\n\\hat{\\theta} = \\tilde{\\theta}_{\\hat{m}} \\) which equally applies to estimation\nof the whole parameter vectors, some subvector or linear mapping, as well as\nthe estimation of a linear functional.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 17:18:20 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Spokoiny", "Vladimir", ""], ["Willrich", "Niklas", ""]]}, {"id": "1507.05185", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Lijun Zhang, Qihang Lin, Rong Jin", "title": "Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a fast approximation method for {\\it large-scale\nhigh-dimensional} sparse least-squares regression problem by exploiting the\nJohnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional\nvectors into a low-dimensional space. In particular, we propose to apply the JL\ntransforms to the data matrix and the target vector and then to solve a sparse\nleast-squares problem on the compressed data with a {\\it slightly larger\nregularization parameter}. Theoretically, we establish the optimization error\nbound of the learned model for two different sparsity-inducing regularizers,\ni.e., the elastic net and the $\\ell_1$ norm. Compared with previous relevant\nwork, our analysis is {\\it non-asymptotic and exhibits more insights} on the\nbound, the sample complexity and the regularization. As an illustration, we\nalso provide an error bound of the {\\it Dantzig selector} under JL transforms.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 13:16:09 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhang", "Lijun", ""], ["Lin", "Qihang", ""], ["Jin", "Rong", ""]]}, {"id": "1507.05270", "submitter": "Denis Chetverikov", "authors": "Denis Chetverikov and Daniel Wilhelm", "title": "Nonparametric instrumental variable estimation under monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ill-posedness of the inverse problem of recovering a regression function\nin a nonparametric instrumental variable model leads to estimators that may\nsuffer from a very slow, logarithmic rate of convergence. In this paper, we\nshow that restricting the problem to models with monotone regression functions\nand monotone instruments significantly weakens the ill-posedness of the\nproblem. In stark contrast to the existing literature, the presence of a\nmonotone instrument implies boundedness of our measure of ill-posedness when\nrestricted to the space of monotone functions. Based on this result we derive a\nnovel non-asymptotic error bound for the constrained estimator that imposes\nmonotonicity of the regression function. For a given sample size, the bound is\nindependent of the degree of ill-posedness as long as the regression function\nis not too steep. As an implication, the bound allows us to show that the\nconstrained estimator converges at a fast, polynomial rate, independently of\nthe degree of ill-posedness, in a large, but slowly shrinking neighborhood of\nconstant functions. Our simulation study demonstrates significant finite-sample\nperformance gains from imposing monotonicity even when the regression function\nis rather far from being a constant. We apply the constrained estimator to the\nproblem of estimating gasoline demand functions from U.S. data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 10:04:41 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Chetverikov", "Denis", ""], ["Wilhelm", "Daniel", ""]]}, {"id": "1507.05313", "submitter": "Ye Zhang", "authors": "Anderson Y. Zhang, Harrison H. Zhou", "title": "Minimax Rates of Community Detection in Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently network analysis has gained more and more attentions in statistics,\nas well as in computer science, probability, and applied mathematics. Community\ndetection for the stochastic block model (SBM) is probably the most studied\ntopic in network analysis. Many methodologies have been proposed. Some\nbeautiful and significant phase transition results are obtained in various\nsettings. In this paper, we provide a general minimax theory for community\ndetection. It gives minimax rates of the mis-match ratio for a wide rage of\nsettings including homogeneous and inhomogeneous SBMs, dense and sparse\nnetworks, finite and growing number of communities. The minimax rates are\nexponential, different from polynomial rates we often see in statistical\nliterature. An immediate consequence of the result is to establish threshold\nphenomenon for strong consistency (exact recovery) as well as weak consistency\n(partial recovery). We obtain the upper bound by a range of penalized\nlikelihood-type approaches. The lower bound is achieved by a novel reduction\nfrom a global mis-match ratio to a local clustering problem for one node\nthrough an exchangeability property.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 17:57:37 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 20:38:34 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1507.05315", "submitter": "Ulrike Schneider", "authors": "Karl Ewald and Ulrike Schneider", "title": "Uniformly Valid Confidence Sets Based on the Lasso", "comments": "Some typos corrected, updated references", "journal-ref": "Electron. J. Statist. 12 (2018), 1358-1387", "doi": "10.1214/18-EJS1425", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a linear regression model of fixed dimension $p \\leq n$, we construct\nconfidence regions for the unknown parameter vector based on the Lasso\nestimator that uniformly and exactly hold the prescribed in finite samples as\nwell as in an asymptotic setup. We thereby quantify estimation uncertainty as\nwell as the \"post-model selection error\" of this estimator. More concretely, in\nfinite samples with Gaussian errors and asymptotically in the case where the\nLasso estimator is tuned to perform conservative model selection, we derive\nexact formulas for computing the minimal coverage probability over the entire\nparameter space for a large class of shapes for the confidence sets, thus\nenabling the construction of valid confidence regions based on the Lasso\nestimator in these settings. The choice of shape for the confidence sets and\ncomparison with the confidence ellipse based on the least-squares estimator is\nalso discussed. Moreover, in the case where the Lasso estimator is tuned to\nenable consistent model selection, we give a simple confidence region with\nminimal coverage probability converging to one. Finally, we also treat the case\nof unknown error variance and present some ideas for extensions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 18:20:05 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:19:11 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 13:10:02 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Ewald", "Karl", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1507.05343", "submitter": "Zhou Fan", "authors": "Zhou Fan and Andrea Montanari", "title": "The Spectral Norm of Random Inner-Product Kernel Matrices", "comments": "This revision clarifies the proofs and statistical motivation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an \"inner-product kernel\" random matrix model, whose empirical\nspectral distribution was shown by Xiuyuan Cheng and Amit Singer to converge to\na deterministic measure in the large $n$ and $p$ limit. We provide an\ninterpretation of this limit measure as the additive free convolution of a\nsemicircle law and a Marcenko-Pastur law. By comparing the tracial moments of\nthis random matrix to those of a deformed GUE matrix with the same limiting\nspectrum, we establish that for odd kernel functions, the spectral norm of this\nmatrix convergences almost surely to the edge of the limiting spectrum. Our\nstudy is motivated by the analysis of a covariance thresholding procedure for\nthe statistical detection and estimation of sparse principal components, and\nour results characterize the limit of the largest eigenvalue of the thresholded\nsample covariance matrix in the null setting.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2015 21:29:18 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 22:03:59 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Fan", "Zhou", ""], ["Montanari", "Andrea", ""]]}, {"id": "1507.05366", "submitter": "Hau-tieng Wu", "authors": "Ingrid Daubechies, Yi Wang, Hau-tieng Wu", "title": "ConceFT: Concentration of Frequency and Time via a multitapered\n  synchrosqueezed transform", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2015.0193", "report-no": null, "categories": "math.ST cs.NA stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to determine the time-frequency content of\ntime-dependent signals consisting of multiple oscillatory components, with\ntime-varying amplitudes and instantaneous frequencies. Numerical experiments as\nwell as a theoretical analysis are presented to assess its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:05:14 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Daubechies", "Ingrid", ""], ["Wang", "Yi", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1507.05442", "submitter": "Pierpaolo Vivo", "authors": "Pierpaolo Vivo", "title": "Large deviations of the maximum of independent and identically\n  distributed random variables", "comments": "14 pag., 1 fig. - Accepted for publication in European Journal of\n  Physics", "journal-ref": null, "doi": "10.1088/0143-0807/36/5/055037", "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pedagogical account of some aspects of Extreme Value Statistics (EVS) is\npresented from the somewhat non-standard viewpoint of Large Deviation Theory.\nWe address the following problem: given a set of $N$ i.i.d. random variables\n$\\{X_1,\\ldots,X_N\\}$ drawn from a parent probability density function (pdf)\n$p(x)$, what is the probability that the maximum value of the set\n$X_{\\mathrm{max}}=\\max_i X_i$ is \"atypically larger\" than expected? The cases\nof exponential and Gaussian distributed variables are worked out in detail, and\nthe right rate function for a general pdf in the Gumbel basin of attraction is\nderived. The Gaussian case convincingly demonstrates that the full rate\nfunction cannot be determined from the knowledge of the limiting distribution\n(Gumbel) alone, thus implying that it indeed carries additional information.\nGiven the simplicity and richness of the result and its derivation, its absence\nfrom textbooks, tutorials and lecture notes on EVS for physicists appears\ninexplicable.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 10:45:50 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Vivo", "Pierpaolo", ""]]}, {"id": "1507.05723", "submitter": "Cheng Li", "authors": "Wenxin Jiang, Cheng Li", "title": "On Bayesian Oracle Properties", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When model uncertainty is handled by Bayesian model averaging (BMA) or\nBayesian model selection (BMS), the posterior distribution possesses a\ndesirable \"oracle property\" for parametric inference, if for large enough data\nit is nearly as good as the oracle posterior, obtained by assuming\nunrealistically that the true model is known and only the true model is used.\nWe study the oracle properties in a very general context of quasi-posterior,\nwhich can accommodate non-regular models with cubic root asymptotics and\npartial identification. Our approach for proving the oracle properties is based\non a unified treatment that bounds the posterior probability of model\nmis-selection. This theoretical framework can be of interest to Bayesian\nstatisticians who would like to theoretically justify their new model selection\nor model averaging methods in addition to empirical results. Furthermore, for\nnon-regular models, we obtain nontrivial conclusions on the choice of prior\npenalty on model complexity, the temperature parameter of the quasi-posterior,\nand the advantage of BMA over BMS.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 06:46:22 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 02:07:34 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Jiang", "Wenxin", ""], ["Li", "Cheng", ""]]}, {"id": "1507.05731", "submitter": "Maximilian Kasy", "authors": "Maximilian Kasy", "title": "Uniformity and the delta method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When are asymptotic approximations using the delta-method uniformly valid? We\nprovide sufficient conditions as well as closely related necessary conditions\nfor uniform negligibility of the remainder of such approximations. These\nconditions are easily verified and permit to identify settings and parameter\nregions where pointwise asymptotic approximations perform poorly. Our framework\nallows for a unified and transparent discussion of uniformity issues in various\nsub-fields of econometrics. Our conditions involve uniform bounds on the\nremainder of a first-order approximation for the function of interest.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 07:51:57 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Kasy", "Maximilian", ""]]}, {"id": "1507.05780", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone", "title": "Geometric ergodicity of the Random Walk Metropolis with\n  position-dependent proposal covariance", "comments": "13 pages including appendices, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Metropolis--Hastings method with proposal $\\mathcal{N}(x,\nhG(x)^{-1})$, where $x$ is the current state, and study its ergodicity\nproperties. We show that suitable choices of $G(x)$ can change these compared\nto the Random Walk Metropolis case $\\mathcal{N}(x, h\\Sigma)$, either for better\nor worse. We find that if the proposal variance is allowed to grow unboundedly\nin the tails of the distribution then geometric ergodicity can be established\nwhen the target distribution for the algorithm has tails that are heavier than\nexponential, but that the growth rate must be carefully controlled to prevent\nthe rejection rate approaching unity. We also illustrate that a judicious\nchoice of $G(x)$ can result in a geometrically ergodic chain when probability\nconcentrates on an ever narrower ridge in the tails, something that is not true\nfor the Random Walk Metropolis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 10:49:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 10:09:40 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 11:32:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Livingstone", "Samuel", ""]]}, {"id": "1507.05935", "submitter": "Peng Ding", "authors": "Zhichao Jiang, Peng Ding, Zhi Geng", "title": "Principal causal effect identification and surrogate endpoint evaluation\n  by multiple trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal stratification is a causal framework to analyze randomized\nexperiments with a post-treatment variable between the treatment and endpoint\nvariables. Because the principal strata defined by the potential outcomes of\nthe post-treatment variable are not observable, we generally cannot identify\nthe causal effects within principal strata. Motivated by a real data set of\nphase III adjuvant colon clinical trials, we propose approaches to identifying\nand estimating the principal causal effects via multiple trials. For the\nidentifiability, we remove the commonly-used exclusion restriction assumption\nby stipulating that the principal causal effects are homogeneous across these\ntrials. To remove another commonly-used monotonicity assumption, we give a\nnecessary condition for the local identifiability, which requires at least\nthree trials. Applying our approaches to the data from adjuvant colon clinical\ntrials, we find that the commonly-used monotonicity assumption is untenable,\nand disease-free survival with three-year follow-up is a valid surrogate\nendpoint for overall survival with five-year follow-up, which satisfies both\nthe causal necessity and the causal sufficiency. We also propose a sensitivity\nanalysis approach based on Bayesian hierarchical models to investigate the\nimpact of the deviation from the homogeneity assumption.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:19:01 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""], ["Geng", "Zhi", ""]]}, {"id": "1507.05952", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath", "title": "Optimal Testing for Properties of Distributions", "comments": "31 pages, extended abstract appeared as a spotlight in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown distribution $p$, is it possible to distinguish\nwhether $p$ belongs to some class of distributions $\\mathcal{C}$ versus $p$\nbeing far from every distribution in $\\mathcal{C}$? This fundamental question\nhas received tremendous attention in statistics, focusing primarily on\nasymptotic analysis, and more recently in information theory and theoretical\ncomputer science, where the emphasis has been on small sample size and\ncomputational complexity. Nevertheless, even for basic properties of\ndistributions such as monotonicity, log-concavity, unimodality, independence,\nand monotone-hazard rate, the optimal sample complexity is unknown.\n  We provide a general approach via which we obtain sample-optimal and\ncomputationally efficient testers for all these distribution families. At the\ncore of our approach is an algorithm which solves the following problem: Given\nsamples from an unknown distribution $p$, and a known distribution $q$, are $p$\nand $q$ close in $\\chi^2$-distance, or far in total variation distance?\n  The optimality of our testers is established by providing matching lower\nbounds with respect to both $n$ and $\\varepsilon$. Finally, a necessary\nbuilding block for our testers and an important byproduct of our work are the\nfirst known computationally efficient proper learners for discrete log-concave\nand monotone hazard rate distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 19:52:56 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 14:12:33 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 20:00:11 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Acharya", "Jayadev", ""], ["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1507.06115", "submitter": "James Duffy", "authors": "Marianne Bruins, James A. Duffy, Michael P. Keane, Anthony A. Smith Jr", "title": "Generalized Indirect Inference for Discrete Choice Models", "comments": "ii + 36 pp., with 18 pp. appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops and implements a practical simulation-based method for\nestimating dynamic discrete choice models. The method, which can accommodate\nlagged dependent variables, serially correlated errors, unobserved variables,\nand many alternatives, builds on the ideas of indirect inference. The main\ndifficulty in implementing indirect inference in discrete choice models is that\nthe objective surface is a step function, rendering gradient-based optimization\nmethods useless. To overcome this obstacle, this paper shows how to smooth the\nobjective surface. The key idea is to use a smoothed function of the latent\nutilities as the dependent variable in the auxiliary model. As the smoothing\nparameter goes to zero, this function delivers the discrete choice implied by\nthe latent utilities, thereby guaranteeing consistency. We establish conditions\non the smoothing such that our estimator enjoys the same limiting distribution\nas the indirect inference estimator, while at the same time ensuring that the\nsmoothing facilitates the convergence of gradient-based optimization methods. A\nset of Monte Carlo experiments shows that the method is fast, robust, and\nnearly as efficient as maximum likelihood when the auxiliary model is\nsufficiently rich.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 09:49:59 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Bruins", "Marianne", ""], ["Duffy", "James A.", ""], ["Keane", "Michael P.", ""], ["Smith", "Anthony A.", "Jr"]]}, {"id": "1507.06128", "submitter": "Trisha Maitra Mrs", "authors": "Trisha Maitra and Sourabh Bhattacharya", "title": "On Classical and Bayesian Asymptotics in State Space Stochastic\n  Differential Equations", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we investigate consistency and asymptotic normality of the\nmaximum likelihood and the posterior distribution of the parameters in the\ncontext of state space stochastic differential equations (SDEs). We then extend\nour asymptotic theory to random effects models based on systems of state space\nSDEs, covering both independent and identical and independent but non-identical\ncollections of state space SDEs. We also address asymptotic inference in the\ncase of multidimensional linear random effects, and in situations where the\ndata are available in discretized forms. It is important to note that\nasymptotic inference, either in the classical or in the Bayesian paradigm, has\nnot been hitherto investigated in state space SDEs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 11:05:02 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 08:53:28 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 11:11:35 GMT"}, {"version": "v4", "created": "Sun, 11 Nov 2018 17:28:47 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1507.06145", "submitter": "Adam Charles", "authors": "Adam Charles, Aurele Balavoine, Christopher Rozell", "title": "Dynamic Filtering of Time-Varying Sparse Signals via l1 Minimization", "comments": "26 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1208.0325", "journal-ref": null, "doi": "10.1109/TSP.2016.2586745", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of sparsity signal models and the increasing\nprevalence of high-dimensional streaming data, there are relatively few\nalgorithms for dynamic filtering of time-varying sparse signals. Of the\nexisting algorithms, fewer still provide strong performance guarantees. This\npaper examines two algorithms for dynamic filtering of sparse signals that are\nbased on efficient l1 optimization methods. We first present an analysis for\none simple algorithm (BPDN-DF) that works well when the system dynamics are\nknown exactly. We then introduce a novel second algorithm (RWL1-DF) that is\nmore computationally complex than BPDN-DF but performs better in practice,\nespecially in the case where the system dynamics model is inaccurate.\nRobustness to model inaccuracy is achieved by using a hierarchical\nprobabilistic data model and propagating higher-order statistics from the\nprevious estimate (akin to Kalman filtering) in the sparse inference process.\nWe demonstrate the properties of these algorithms on both simulated data as\nwell as natural video sequences. Taken together, the algorithms presented in\nthis paper represent the first strong performance analysis of dynamic filtering\nalgorithms for time-varying sparse signals as well as state-of-the-art\nperformance in this emerging application.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 12:10:16 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 16:20:48 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Charles", "Adam", ""], ["Balavoine", "Aurele", ""], ["Rozell", "Christopher", ""]]}, {"id": "1507.06227", "submitter": "Joscha Prochno", "authors": "Richard Lechner and Markus Passenbrunner and Joscha Prochno", "title": "Estimating averages of order statistics of bivariate functions", "comments": null, "journal-ref": "J Theor Probab (2017) 30: 1445", "doi": "10.1007/s10959-016-0702-8", "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove uniform estimates for the expected value of averages of order\nstatistics of bivariate functions in terms of their largest values by a direct\nanalysis. As an application, uniform estimates for the expected value of\naverages of order statistics of sequences of independent random variables in\nterms of Orlicz norms are obtained. In the case where the bivariate functions\nare matrices, we provide a \"minimal\" probability space which allows us to\n$C$-embed certain Orlicz spaces $\\ell_M^n$ into $\\ell_1^{cn^3}$, $c,C>0$ being\nabsolute constants.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 15:25:12 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Lechner", "Richard", ""], ["Passenbrunner", "Markus", ""], ["Prochno", "Joscha", ""]]}, {"id": "1507.06245", "submitter": "Anna Bonnet", "authors": "Anna Bonnet, C\\'eline L\\'evy-Leduc, Elisabeth Gassiat, Roberto Toro,\n  Thomas Bourgeron", "title": "Improving heritability estimation by a variable selection approach in\n  sparse high dimensional linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in neuroanatomy, we propose a novel methodology for\nestimating the heritability which corresponds to the proportion of phenotypic\nvariance which can be explained by genetic factors. Estimating this quantity\nfor neuroanatomical features is a fundamental challenge in psychiatric disease\nresearch. Since the phenotypic variations may only be due to a small fraction\nof the available genetic information, we propose an estimator of the\nheritability that can be used in high dimensional sparse linear mixed models.\nOur method consists of three steps. Firstly, a variable selection stage is\nperformed in order to recover the support of the genetic effects -- also called\ncausal variants -- that is to find the genetic effects which really explain the\nphenotypic variations. Secondly, we propose a maximum likelihood strategy for\nestimating the heritability which only takes into account the causal genetic\neffects found in the first step. Thirdly, we compute the standard error and the\n95% confidence interval associated to our heritability estimator thanks to a\nnonparametric bootsrap approach. Our main contribution consists in providing an\nestimation of the heritability with standard errors substantially smaller than\nmethods without variable selection when the genetic effects are very sparse.\nSince the real genetic architecture is in general unknown in practice, we also\npropose an empirical criterion which allows the user to decide whether it is\nrelevant to apply a variable selection based approach or not. We illustrate the\nperformance of our methodology on synthetic and real neuroanatomic data coming\nfrom the Imagen project. We also show that our approach has a very low\ncomputational burden and is very efficient from a statistical point of view.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 16:33:19 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 16:45:20 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:42:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Bonnet", "Anna", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Gassiat", "Elisabeth", ""], ["Toro", "Roberto", ""], ["Bourgeron", "Thomas", ""]]}, {"id": "1507.06348", "submitter": "George Savvidy K", "authors": "George Savvidy", "title": "Anosov C-systems and random number generators", "comments": "LaTex file, 25 pages, 6 figures; references added", "journal-ref": null, "doi": "10.1134/S004057791608002X", "report-no": "NRCPS-HE-24-2015", "categories": "hep-th math.DS math.ST nlin.CD physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing further our earlier suggestion to use hyperbolic Anosov\nC-systems for the Monte-Carlo simulations in high energy particle physics. The\nhyperbolic dynamical systems have homogeneous instability of all trajectories\nand as such they have mixing of all orders, countable Lebesgue spectrum and\npositive Kolmogorov entropy. These extraordinary ergodic properties follow from\nthe C-condition introduced by Anosov. The C-condition defines a rich class of\ndynamical systems which span an open set in the space of all dynamical systems.\nThe important property of C-systems is that they have a countable set of\neverywhere dense periodic trajectories and that their density exponentially\nincreases with entropy. Of special interest are C-systems that are defined on a\nhigh dimensional torus. The C-systems on a torus are perfect candidates to be\nused for Monte-Carlo simulations. Recently an efficient algorithm was found,\nwhich allows very fast generation of long trajectories of the C-systems. These\ntrajectories have high quality statistical properties and we are suggesting to\nuse them for the QCD lattice simulations and at high energy particle physics.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 22:22:57 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 10:15:58 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 08:18:22 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Savvidy", "George", ""]]}, {"id": "1507.06352", "submitter": "David Choi", "authors": "David Choi", "title": "Co-clustering of Nonsmooth Graphons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance bounds are given for exploratory co-clustering/ blockmodeling of\nbipartite graph data, where we assume the rows and columns of the data matrix\nare samples from an arbitrary population. This is equivalent to assuming that\nthe data is generated from a nonsmooth graphon. It is shown that co-clusters\nfound by any method can be extended to the row and column populations, or\nequivalently that the estimated blockmodel approximates a blocked version of\nthe generative graphon, with estimation error bounded by $O_P(n^{-1/2})$.\nAnalogous performance bounds are also given for degree-corrected blockmodels\nand random dot product graphs, with error rates depending on the dimensionality\nof the latent variable space.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 22:35:54 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Choi", "David", ""]]}, {"id": "1507.06370", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Avi Wigderson", "title": "Sum-of-Squares Lower Bounds for Sparse PCA", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a statistical versus computational trade-off for\nsolving a basic high-dimensional machine learning problem via a basic convex\nrelaxation method. Specifically, we consider the {\\em Sparse Principal\nComponent Analysis} (Sparse PCA) problem, and the family of {\\em\nSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was well\nknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em\nin principle} detected using only $n \\approx k\\log p$ (Gaussian or Bernoulli)\nsamples, but all {\\em efficient} (polynomial time) algorithms known require $n\n\\approx k^2$ samples. It was also known that this quadratic gap cannot be\nimproved by the the most basic {\\em semi-definite} (SDP, aka spectral)\nrelaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also\ndegree-4 SoS algorithms cannot improve this quadratic gap. This average-case\nlower bound adds to the small collection of hardness results in machine\nlearning for this powerful family of convex relaxation algorithms. Moreover,\nour design of moments (or \"pseudo-expectations\") for this lower bound is quite\ndifferent than previous lower bounds. Establishing lower bounds for higher\ndegree SoS algorithms for remains a challenging problem.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:43 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 05:50:16 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ma", "Tengyu", ""], ["Wigderson", "Avi", ""]]}, {"id": "1507.06371", "submitter": "Hongshuai Dai", "authors": "Chunhong Li, Xinxing Wei, Hongshuai Dai", "title": "Adaptive Elastic Net Method for Cox Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Adaptive Elastic Net method for the Cox model. We\nprove the grouping effect and oracle property of its estimators. Finally, we\nshow these two properties by an empirical analysis and a numerical simulation,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 01:50:45 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Li", "Chunhong", ""], ["Wei", "Xinxing", ""], ["Dai", "Hongshuai", ""]]}, {"id": "1507.06400", "submitter": "Abdelfattah  Mustafa AM", "authors": "M.A. El-Damcese, Abdelfattah Mustafa, B.S. El-Desouky and M. E.\n  Mustafa", "title": "The Odd Generalized Exponential Gompertz", "comments": "14 Pages, 7 figures(11 Images), 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we propose a new lifetime model, called the odd generalized\nexponential gompertz distribution, We obtain some of its mathematical\nproperties. Some structural properties of the new distribution are studied. The\nmethod of maximum likelihood method is used for estimating the model parameters\nand the observed Fisher's information matrix is derived. We illustrate the\nusefulness of the proposed model by applications to real data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 07:43:24 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 05:07:05 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 17:45:02 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["El-Damcese", "M. A.", ""], ["Mustafa", "Abdelfattah", ""], ["El-Desouky", "B. S.", ""], ["Mustafa", "M. E.", ""]]}, {"id": "1507.06506", "submitter": "Christophe Biscio", "authors": "Christophe Ange Napol\\'eon Biscio (LMJL), Fr\\'ed\\'eric Lavancier\n  (SERPICO, LMJL)", "title": "Brillinger mixing of determinantal point processes and statistical\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary determinantal point processes are proved to be Brillinger mixing.\nThis property is an important step towards asymptotic statistics for these\nprocesses. As an important example, a central limit theorem for a wide class of\nfunctionals of determinantal point processes is established. This result yields\nin particular the asymptotic normality of the estimator of the intensity of a\nstationary determinantal point process and of the kernel estimator of its pair\ncorrelation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:12:49 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Biscio", "Christophe Ange Napol\u00e9on", "", "LMJL"], ["Lavancier", "Fr\u00e9d\u00e9ric", "", "SERPICO, LMJL"]]}, {"id": "1507.06510", "submitter": "Sylvain Le Corff", "authors": "Yohann De Castro, Elisabeth Gassiat and Sylvain Le Corff", "title": "Consistent estimation of the filtering and marginal smoothing\n  distributions in nonparametric hidden Markov models", "comments": "27 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1501.04787", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the filtering and smoothing recursions in\nnonparametric finite state space hidden Markov models (HMMs) when the\nparameters of the model are unknown and replaced by estimators. We provide an\nexplicit and time uniform control of the filtering and smoothing errors in\ntotal variation norm as a function of the parameter estimation errors. We prove\nthat the risk for the filtering and smoothing errors may be uniformly upper\nbounded by the risk of the estimators. It has been proved very recently that\nstatistical inference for finite state space nonparametric HMMs is possible. We\nstudy how the recent spectral methods developed in the parametric setting may\nbe extended to the nonparametric framework and we give explicit upper bounds\nfor the L2-risk of the nonparametric spectral estimators. When the observation\nspace is compact, this provides explicit rates for the filtering and smoothing\nerrors in total variation norm. The performance of the spectral method is\nassessed with simulated data for both the estimation of the (nonparametric)\nconditional distribution of the observations and the estimation of the marginal\nsmoothing distributions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 14:18:25 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["De Castro", "Yohann", ""], ["Gassiat", "Elisabeth", ""], ["Corff", "Sylvain Le", ""]]}, {"id": "1507.06597", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and David Draper", "title": "Cox's Theorem and the Jaynesian Interpretation of Probability", "comments": "This work is withdrawn due to a critical error which we are unable to\n  repair without completely changing the framework. The first author deeply\n  regrets this error, which was committed when he was still obtaining his\n  master's degree and had yet to learn a proper degree of carefulness needed\n  when devising theoretical arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple proposed interpretations of probability theory: one such\ninterpretation is true-false logic under uncertainty. Cox's Theorem is a\nrepresentation theorem that states, under a certain set of axioms describing\nthe meaning of uncertainty, that every true-false logic under uncertainty is\nisomorphic to conditional probability theory. This result was used by Jaynes to\ndevelop a philosophical framework in which statistical inference under\nuncertainty should be conducted through the use of probability, via Bayes'\nRule. Unfortunately, most existing correct proofs of Cox's Theorem require\nrestrictive assumptions: for instance, many do not apply even to the simple\nexample of rolling a pair of fair dice. We offer a new axiomatization by\nreplacing various technical conditions with an axiom stating that our theory\nmust be consistent with respect to repeated events. We discuss the implications\nof our results, both for the philosophy of probability and for the philosophy\nof statistics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 18:21:22 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 04:00:32 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 14:28:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Terenin", "Alexander", ""], ["Draper", "David", ""]]}, {"id": "1507.06710", "submitter": "Gilad Lerman Dr", "authors": "Xu Wang and Gilad Lerman", "title": "Nonparametric Bayesian Regression on Manifolds via Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for manifold-valued regression and\nestablishes its consistency as well as its contraction rate. It assumes a\npredictor with values in the interval $[0,1]$ and response with values in a\ncompact Riemannian manifold $M$. This setting is useful for applications such\nas modeling dynamic scenes or shape deformations, where the visual scene or the\ndeformed objects can be modeled by a manifold. The proposed framework is\nnonparametric and uses the heat kernel (and its associated Brownian motion) on\nmanifolds as an averaging procedure. It directly generalizes the use of the\nGaussian kernel (as a natural model of additive noise) in vector-valued\nregression problems. In order to avoid explicit dependence on estimates of the\nheat kernel, we follow a Bayesian setting, where Brownian motion on $M$ induces\na prior distribution on the space of continuous functions $C([0,1], M)$. For\nthe case of discretized Brownian motion, we establish the consistency of the\nposterior distribution in terms of the $L_{q}$ distances for any $1 \\leq q <\n\\infty$. Most importantly, we establish contraction rate of order\n$O(n^{-1/4+\\epsilon})$ for any fixed $\\epsilon>0$, where $n$ is the number of\nobservations. For the continuous Brownian motion we establish weak consistency.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 00:33:04 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Wang", "Xu", ""], ["Lerman", "Gilad", ""]]}, {"id": "1507.06739", "submitter": "Xiaoying Tian Harris", "authors": "Xiaoying Tian and Jonathan E. Taylor", "title": "Selective inference with a randomized response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by sample splitting and the reusable holdout introduced in the field\nof differential privacy, we consider selective inference with a randomized\nresponse. We discuss two major advantages of using a randomized response for\nmodel selection. First, the selectively valid tests are more powerful after\nrandomized selection. Second, it allows consistent estimation and weak\nconvergence of selective inference procedures. Under independent sampling, we\nprove a selective (or privatized) central limit theorem that transfers\nprocedures valid under asymptotic normality without selection to their\ncorresponding selective counterparts. This allows selective inference in\nnonparametric settings. Finally, we propose a framework of inference after\ncombining multiple randomized selection procedures. We focus on the classical\nasymptotic setting, leaving the interesting high-dimensional asymptotic\nquestions for future work.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 04:28:16 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 01:34:36 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2015 01:43:56 GMT"}, {"version": "v4", "created": "Thu, 10 Mar 2016 20:52:38 GMT"}, {"version": "v5", "created": "Wed, 30 Nov 2016 18:13:12 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Tian", "Xiaoying", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1507.06792", "submitter": "Michael S{\\o}rensen", "authors": "Nina Munkholt Jakobsen and Michael S{\\o}rensen", "title": "Efficient estimation for diffusions sampled at high frequency over a\n  fixed time interval", "comments": "Published at http://projecteuclid.org/euclid.bj/1489737628 in the\n  Bernoulli Journal ( see\n  http://www.bernoulli-society.org/index.php/publications/bernoulli-journal/bernoulli-journal\n  ) by the International Statistical Institute/Bernoulli Society ( see\n  http://www.bernoulli-society.org )", "journal-ref": "Bernoulli 2017, Vol. 23, No. 3, 1874-1910", "doi": "10.3150/15-BEJ799", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric estimation for diffusion processes is considered for high\nfrequency observations over a fixed time interval. The processes solve\nstochastic differential equations with an unknown parameter in the diffusion\ncoefficient. We find easily verified conditions on approximate martingale\nestimating functions under which estimators are consistent, rate optimal, and\nefficient under high frequency (in-fill) asymptotics. The asymptotic\ndistributions of the estimators are shown to be normal variance-mixtures, where\nthe mixing distribution generally depends on the full sample path of the\ndiffusion process over the observation time interval. Utilising the concept of\nstable convergence, we also obtain the more easily applicable result that for a\nsuitable data dependent normalisation, the estimators converge in distribution\nto a standard normal distribution. The theory is illustrated by a simulation\nstudy comparing an efficient and a non-efficient estimating function for an\nergodic and a non-ergodic model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 09:33:14 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 15:22:13 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Jakobsen", "Nina Munkholt", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1507.07050", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky, Daniell Toth", "title": "Bayesian Estimation Under Informative Sampling", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis is increasingly popular for use in social science and other\napplication areas where the data are observations from an informative sample.\nAn informative sampling design leads to inclusion probabilities that are\ncorrelated with the response variable of interest. Model inference performed on\nthe observed sample taken from the population will be biased for the population\ngenerative model under informative sampling since the balance of information in\nthe sample data is different from that for the population. Typical approaches\nto account for an informative sampling design under Bayesian estimation are\noften difficult to implement because they require re-parameterization of the\nhypothesized generating model, or focus on design, rather than model-based,\ninference. We propose to construct a pseudo-posterior distribution that\nutilizes sampling weights based on the marginal inclusion probabilities to\nexponentiate the likelihood contribution of each sampled unit, which weights\nthe information in the sample back to the population. Our approach provides a\nnearly automated estimation procedure applicable to any model specified by the\ndata analyst for the population and retains the population model\nparameterization and posterior sampling geometry. We construct conditions on\nknown marginal and pairwise inclusion probabilities that define a class of\nsampling designs where $L_{1}$ consistency of the pseudo posterior is\nguaranteed. We demonstrate our method on an application concerning the Bureau\nof Labor Statistics Job Openings and Labor Turnover Survey.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 01:15:55 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 13:55:52 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 20:35:07 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 22:08:29 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Toth", "Daniell", ""]]}, {"id": "1507.07055", "submitter": "Bhaswar Bhattacharya", "authors": "Bhaswar B. Bhattacharya and Sumit Mukherjee", "title": "Inference in Ising Models", "comments": "Revised. 33 pages, 4 figures, To appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising spin glass is a one-parameter exponential family model for binary\ndata with quadratic sufficient statistic. In this paper, we show that given a\nsingle realization from this model, the maximum pseudolikelihood estimate\n(MPLE) of the natural parameter is $\\sqrt {a_N}$-consistent at a point whenever\nthe log-partition function has order $a_N$ in a neighborhood of that point.\nThis gives consistency rates of the MPLE for ferromagnetic Ising models on\ngeneral weighted graphs in all regimes, extending the results of Chatterjee\n(2007) where only $\\sqrt N$-consistency of the MPLE was shown. It is also shown\nthat consistent testing, and hence estimation, is impossible in the high\ntemperature phase in ferromagnetic Ising models on a converging sequence of\nsimple graphs, which include the Curie--Weiss model. In this regime, the\nsufficient statistic is distributed as a weighted sum of independent $\\chi^2_1$\nrandom variables, and the asymptotic power of the most powerful test is\ndetermined. We also illustrate applications of our results on synthetic and\nreal-world network data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 02:37:51 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 05:52:38 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 18:08:50 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""], ["Mukherjee", "Sumit", ""]]}, {"id": "1507.07089", "submitter": "Peter Harremo\\\"es", "authors": "Peter Harremo\\\"es", "title": "Proper Scoring and Sufficiency", "comments": "Proceedings WITMSE 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logarithmic score and information divergence appear in both information\ntheory, statistics, statistical mechanics, and portfolio theory. We demonstrate\nthat all these topics involve some kind of optimization that leads directly to\nthe use of Bregman divergences. If a sufficiency condition is also fulfilled\nthe Bregman divergence must be proportional to information divergence. The\nsufficiency condition has quite different consequences in the different areas\nof application, and often it is not fulfilled. Therefore the sufficiency\ncondition can be used to explain when results from one area can be transferred\ndirectly from one area to another and when one will experience differences.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 10:41:35 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Harremo\u00ebs", "Peter", ""]]}, {"id": "1507.07094", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Unknown sparsity in compressed sensing: Denoising and inference", "comments": "The title of the previous tech report has been updated so that it\n  matches the published version. The published version contains additional\n  material", "journal-ref": "IEEE Transactions on Information Theory 62.9 (2016): 5145-5166", "doi": "10.1109/TIT.2016.2587772", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of Compressed Sensing (CS) asserts that an unknown signal\n$x\\in\\mathbb{R}^p$ can be accurately recovered from an underdetermined set of\n$n$ linear measurements with $n\\ll p$, provided that $x$ is sufficiently\nsparse. However, in applications, the degree of sparsity $\\|x\\|_0$ is typically\nunknown, and the problem of directly estimating $\\|x\\|_0$ has been a\nlongstanding gap between theory and practice. A closely related issue is that\n$\\|x\\|_0$ is a highly idealized measure of sparsity, and for real signals with\nentries not equal to 0, the value $\\|x\\|_0=p$ is not a useful description of\ncompressibility. In our previous conference paper [Lop13] that examined these\nproblems, we considered an alternative measure of \"soft\" sparsity,\n$\\|x\\|_1^2/\\|x\\|_2^2$, and designed a procedure to estimate\n$\\|x\\|_1^2/\\|x\\|_2^2$ that does not rely on sparsity assumptions.\n  The present work offers a new deconvolution-based method for estimating\nunknown sparsity, which has wider applicability and sharper theoretical\nguarantees. In particular, we introduce a family of entropy-based sparsity\nmeasures $s_q(x):=\\big(\\frac{\\|x\\|_q}{\\|x\\|_1}\\big)^{\\frac{q}{1-q}}$\nparameterized by $q\\in[0,\\infty]$. This family interpolates between\n$\\|x\\|_0=s_0(x)$ and $\\|x\\|_1^2/\\|x\\|_2^2=s_2(x)$ as $q$ ranges over $[0,2]$.\nFor any $q\\in (0,2]\\setminus\\{1\\}$, we propose an estimator $\\hat{s}_q(x)$\nwhose relative error converges at the dimension-free rate of $1/\\sqrt{n}$, even\nwhen $p/n\\to\\infty$. Our main results also describe the limiting distribution\nof $\\hat{s}_q(x)$, as well as some connections to Basis Pursuit Denosing, the\nLasso, deterministic measurement matrices, and inference problems in CS.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 11:17:03 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 23:06:31 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 09:52:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1507.07106", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson", "title": "Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in\n  Ultrahigh-Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection procedures based on nonlocal alternative prior\ndensities are extended to ultrahigh dimensional settings and compared to other\nvariable selection procedures using precision-recall curves. Variable selection\nprocedures included in these comparisons include methods based on $g$-priors,\nreciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria.\nThe use of precision-recall curves eliminates the sensitivity of our\nconclusions to the choice of tuning parameters. We find that Bayesian variable\nselection procedures based on nonlocal priors are competitive to all other\nprocedures in a range of simulation scenarios, and we subsequently explain this\nfavorable performance through a theoretical examination of their consistency\nproperties. When certain regularity conditions apply, we demonstrate that the\nnonlocal procedures are consistent for linear models even when the number of\ncovariates $p$ increases sub-exponentially with the sample size $n$. A model\nselection procedure based on Zellner's $g$-prior is also found to be\ncompetitive with penalized likelihood methods in identifying the true model,\nbut the posterior distribution on the model space induced by this method is\nmuch more dispersed than the posterior distribution induced on the model space\nby the nonlocal prior methods. We investigate the asymptotic form of the\nmarginal likelihood based on the nonlocal priors and show that it attains a\nunique term that cannot be derived from the other Bayesian model selection\nprocedures. We also propose a scalable and efficient algorithm called\nSimplified Shotgun Stochastic Search with Screening (S5) to explore the\nenormous model space, and we show that S5 dramatically reduces the computing\ntime without losing the capacity to search the interesting region in the model\nspace. The S5 algorithm is available in an \\verb R ~package {\\it BayesS5} on\n\\texttt{CRAN}.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 15:03:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 21:14:52 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:57:31 GMT"}, {"version": "v4", "created": "Wed, 18 Jan 2017 05:14:06 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Shin", "Minsuk", ""], ["Bhattacharya", "Anirban", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1507.07235", "submitter": "Mohamed Hebiri", "authors": "Christophe Denis (LAMA), Mohamed Hebiri (LAMA)", "title": "Consistency of plug-in confidence sets for classification in\n  semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confident prediction is highly relevant in machine learning; for example, in\napplications such as medical diagnoses, wrong prediction can be fatal. For\nclassification, there already exist procedures that allow to not classify data\nwhen the confidence in their prediction is weak. This approach is known as\nclassification with reject option. In the present paper, we provide new\nmethodology for this approach. Predicting a new instance via a confidence set,\nwe ensure an exact control of the probability of classification. Moreover, we\nshow that this methodology is easily implementable and entails attractive\ntheoretical and numerical properties.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2015 18:58:26 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Denis", "Christophe", "", "LAMA"], ["Hebiri", "Mohamed", "", "LAMA"]]}, {"id": "1507.07304", "submitter": "Haim Shore", "authors": "Haim Shore", "title": "The Two Ignored Components of Random Variation", "comments": "20 pages;no figures or tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random phenomenon may have two sources of random variation: an unstable\nidentity and a set of external variation-generating factors. When only a single\nsource is active, two mutually exclusive extreme scenarios may ensue that\nresult in the exponential or the normal, the only truly univariate\ndistributions. All other supposedly univariate random variation observed in\nnature is truly bivariate. In this article, we elaborate on this new paradigm\nfor random variation and develop a general bivariate distribution to reflect\nit. It is shown that numerous current univariate distributions are special\ncases of an approximation to the new bivariate distribution. We first show that\nthe exponential and the normal are special cases of a single distribution\nrepresented by a Response Modeling Methodology model. We then develop a general\nbivariate distribution commensurate with the new paradigm, its properties are\ndiscussed and its moments developed. An approximating assumption results in a\nunivariate general distribution that is shown to include as exact special cases\nwidely used distributions like generalized gamma, log-normal, F, t, and Cauchy.\nCompound distributions and their relationship to the new paradigm are\naddressed. Empirical observations that comply with predictions derived from the\nnew paradigm corroborate its scientific validity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 05:52:29 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Shore", "Haim", ""]]}, {"id": "1507.07357", "submitter": "Debashis Mondal", "authors": "Debashis Mondal", "title": "Applying Dynkin's isomorphism: An alternative approach to understand the\n  Markov property of the de Wijs process", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ541 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1289-1303", "doi": "10.3150/13-BEJ541", "report-no": "IMS-BEJ-BEJ541", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynkin's (Bull. Amer. Math. Soc. 3 (1980) 975-999) seminal work associates a\nmultidimensional transient symmetric Markov process with a multidimensional\nGaussian random field. This association, known as Dynkin's isomorphism, has\nprofoundly influenced the studies of Markov properties of generalized Gaussian\nrandom fields. Extending Dykin's isomorphism, we study here a particular\ngeneralized Gaussian Markov random field, namely, the de Wijs process that\noriginated in Georges Matheron's pioneering work on mining geostatistics and,\nfollowing McCullagh (Ann. Statist. 30 (2002) 1225-1310), is now receiving\nrenewed attention in spatial statistics. This extension of Dynkin's theory\nassociates the de Wijs process with the (recurrent) Brownian motion on the two\ndimensional plane, grants us further insight into Matheron's kriging formula\nfor the de Wijs process and highlights previously unexplored relationships of\nthe central Markov models in spatial statistics with Markov processes on the\nplane.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 10:38:23 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Mondal", "Debashis", ""]]}, {"id": "1507.07412", "submitter": "Fengnan Gao", "authors": "Fengnan Gao and Aad van der Vaart", "title": "Posterior contraction rates for deconvolution of Dirichlet-Laplace\n  mixtures", "comments": "19 pages. Minor revisions", "journal-ref": null, "doi": "10.1214/16-EJS1119", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric Bayesian inference with location mixtures of the\nLaplace density and a Dirichlet process prior on the mixing distribution. We\nderive a contraction rate of the corresponding posterior distribution, both for\nthe mixing distribution relative to the Wasserstein metric and for the mixed\ndensity relative to the Hellinger and $L_q$ metrics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 14:25:45 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 10:05:50 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Gao", "Fengnan", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1507.07443", "submitter": "Katar\\'ina Burclov\\'a", "authors": "Katar\\'ina Burclov\\'a and Andrej P\\'azman", "title": "Optimum design via I-divergence for stable estimation in generalized\n  regression models", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimum designs for parameter estimation in generalized regression models are\nstandardly based on the Fisher information matrix (cf. Atkinson et al (2014)\nfor a recent exposition). The corresponding optimality criteria are related to\nthe asymptotic properties of maximal likelihood (ML) estimators in such models.\nHowever, in finite sample experiments there could be problems with\nidentifiability, stability and uniqueness of the ML estimate, which are not\nreflected by the information matrices. In P\\'azman and Pronzato (2014) is\ndiscussed how to solve some of these estimability issues on the design stage of\nan experiment in standard nonlinear regression. Here we want to extend this\ndesign methodology to more general models based on exponential families of\ndistributions (binomial, Poisson, normal with parametrized variances, etc.).\nThe main tool for that is the information (or Kullback-Leibler) divergence,\nwhich is closely related to the ML estimation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 15:18:14 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Burclov\u00e1", "Katar\u00edna", ""], ["P\u00e1zman", "Andrej", ""]]}, {"id": "1507.07495", "submitter": "Asif Shakeel", "authors": "David A. Meyer and Asif Shakeel", "title": "Estimating an Activity Driven Hidden Markov Model", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a Hidden Markov Model (HMM) in which each hidden state has\ntime-dependent $\\textit{activity levels}$ that drive transitions and emissions,\nand show how to estimate its parameters. Our construction is motivated by the\nproblem of inferring human mobility on sub-daily time scales from, for example,\nmobile phone records.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 17:37:27 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Meyer", "David A.", ""], ["Shakeel", "Asif", ""]]}, {"id": "1507.07653", "submitter": "Jonathan B. Hill", "authors": "Jonathan B. Hill", "title": "Robust estimation and inference for heavy tailed GARCH", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ616 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1629-1669", "doi": "10.3150/14-BEJ616", "report-no": "IMS-BEJ-BEJ616", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two new estimators for a general class of stationary GARCH models\nwith possibly heavy tailed asymmetrically distributed errors, covering\nprocesses with symmetric and asymmetric feedback like GARCH, Asymmetric GARCH,\nVGARCH and Quadratic GARCH. The first estimator arises from negligibly trimming\nQML criterion equations according to error extremes. The second imbeds\nnegligibly transformed errors into QML score equations for a Method of Moments\nestimator. In this case, we exploit a sub-class of redescending transforms that\nincludes tail-trimming and functions popular in the robust estimation\nliterature, and we re-center the transformed errors to minimize small sample\nbias. The negligible transforms allow both identification of the true parameter\nand asymptotic normality. We present a consistent estimator of the covariance\nmatrix that permits classic inference without knowledge of the rate of\nconvergence. A simulation study shows both of our estimators trump existing\nones for sharpness and approximate normality including QML, Log-LAD, and two\ntypes of non-Gaussian QML (Laplace and Power-Law). Finally, we apply the\ntail-trimmed QML estimator to financial data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 05:55:11 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Hill", "Jonathan B.", ""]]}, {"id": "1507.07664", "submitter": "Harry Crane", "authors": "Harry Crane", "title": "Time-varying network models", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ617 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1670-1696", "doi": "10.3150/14-BEJ617", "report-no": "IMS-BEJ-BEJ617", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the exchangeable rewiring process for modeling time-varying\nnetworks. The process fulfills fundamental mathematical and statistical\nproperties and can be easily constructed from the novel operation of random\nrewiring. We derive basic properties of the model, including consistency under\nsubsampling, exchangeability, and the Feller property. A reversible sub-family\nrelated to the Erd\\H{o}s-R\\'{e}nyi model arises as a special case.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 06:46:45 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Crane", "Harry", ""]]}, {"id": "1507.07669", "submitter": "Hermine Bierm\\'{e}", "authors": "Hermine Bierm\\'e, C\\'eline Lacaux", "title": "Modulus of continuity of some conditionally sub-Gaussian fields,\n  application to stable random fields", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ619 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1719-1759", "doi": "10.3150/14-BEJ619", "report-no": "IMS-BEJ-BEJ619", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study modulus of continuity and rate of convergence of\nseries of conditionally sub-Gaussian random fields. This framework includes\nboth classical series representations of Gaussian fields and LePage series\nrepresentations of stable fields. We enlighten their anisotropic properties by\nusing an adapted quasi-metric instead of the classical Euclidean norm. We\nspecify our assumptions in the case of shot noise series where arrival times of\na Poisson process are involved. This allows us to state unified results for\nharmonizable (multi)operator scaling stable random fields through their LePage\nseries representation, as well as to study sample path properties of their\nmultistable analogous.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 07:19:19 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Bierm\u00e9", "Hermine", ""], ["Lacaux", "C\u00e9line", ""]]}, {"id": "1507.07673", "submitter": "Jinzhu Li", "authors": "Jinzhu Li, Qihe Tang", "title": "Interplay of insurance and financial risks in a discrete-time model with\n  strongly regular variation", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ625 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1800-1823", "doi": "10.3150/14-BEJ625", "report-no": "IMS-BEJ-BEJ625", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an insurance company exposed to a stochastic economic environment\nthat contains two kinds of risk. The first kind is the insurance risk caused by\ntraditional insurance claims, and the second kind is the financial risk\nresulting from investments. Its wealth process is described in a standard\ndiscrete-time model in which, during each period, the insurance risk is\nquantified as a real-valued random variable $X$ equal to the total amount of\nclaims less premiums, and the financial risk as a positive random variable $Y$\nequal to the reciprocal of the stochastic accumulation factor. This risk model\nbuilds an efficient platform for investigating the interplay of the two kinds\nof risk. We focus on the ruin probability and the tail probability of the\naggregate risk amount. Assuming that every convex combination of the\ndistributions of $X$ and $Y$ is of strongly regular variation, we derive some\nprecise asymptotic formulas for these probabilities with both finite and\ninfinite time horizons, all in the form of linear combinations of the tail\nprobabilities of $X$ and $Y$. Our treatment is unified in the sense that no\ndominating relationship between $X$ and $Y$ is required.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 07:57:39 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Li", "Jinzhu", ""], ["Tang", "Qihe", ""]]}, {"id": "1507.07754", "submitter": "Marc Hallin", "authors": "Marc Hallin, Zudi Lu, Davy Paindaveine, Miroslav \\v{S}iman", "title": "Local bilinear multiple-output quantile/depth regression", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ610 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1435-1466", "doi": "10.3150/14-BEJ610", "report-no": "IMS-BEJ-BEJ610", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new quantile regression concept, based on a directional version of Koenker\nand Bassett's traditional single-output one, has been introduced in [Ann.\nStatist. (2010) 38 635-669] for multiple-output location/linear regression\nproblems. The polyhedral contours provided by the empirical counterpart of that\nconcept, however, cannot adapt to unknown nonlinear and/or heteroskedastic\ndependencies. This paper therefore introduces local constant and local linear\n(actually, bilinear) versions of those contours, which both allow to\nasymptotically recover the conditional halfspace depth contours that completely\ncharacterize the response's conditional distributions. Bahadur representation\nand asymptotic normality results are established. Illustrations are provided\nboth on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 13:14:30 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Hallin", "Marc", ""], ["Lu", "Zudi", ""], ["Paindaveine", "Davy", ""], ["\u0160iman", "Miroslav", ""]]}, {"id": "1507.08140", "submitter": "Sarah Ouadah", "authors": "Sarah Ouadah, St\\'ephane Robin, Pierre Latouche", "title": "Degree-based goodness-of-fit tests for heterogeneous random graph models\n  : independent and exchangeable cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degrees are a classical and relevant way to study the topology of a\nnetwork. They can be used to assess the goodness-of-fit for a given random\ngraph model. In this paper we introduce goodness-of-fit tests for two classes\nof models. First, we consider the case of independent graph models such as the\nheterogeneous Erd\\\"os-R\\'enyi model in which the edges have different\nconnection probabilities. Second, we consider a generic model for exchangeable\nrandom graphs called the W-graph. The stochastic block model and the expected\ndegree distribution model fall within this framework. We prove the asymptotic\nnormality of the degree mean square under these independent and exchangeable\nmodels and derive formal tests. We study the power of the proposed tests and we\nprove the asymptotic normality under specific sparsity regimes. The tests are\nillustrated on real networks from social sciences and ecology, and their\nperformances are assessed via a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 13:31:25 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 17:30:13 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 14:25:19 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ouadah", "Sarah", ""], ["Robin", "St\u00e9phane", ""], ["Latouche", "Pierre", ""]]}, {"id": "1507.08254", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Justin Romberg", "title": "Efficient Compressive Phase Retrieval with Constrained Sensing Vectors", "comments": "Accepted for the 29th Annual Conference on Neural Information\n  Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust and efficient approach to the problem of compressive\nphase retrieval in which the goal is to reconstruct a sparse vector from the\nmagnitude of a number of its linear measurements. The proposed framework relies\non constrained sensing vectors and a two-stage reconstruction method that\nconsists of two standard convex programs that are solved sequentially.\n  In recent years, various methods are proposed for compressive phase\nretrieval, but they have suboptimal sample complexity or lack robustness\nguarantees. The main obstacle has been that there is no straightforward convex\nrelaxations for the type of structure in the target. Given a set of\nunderdetermined measurements, there is a standard framework for recovering a\nsparse matrix, and a standard framework for recovering a low-rank matrix.\nHowever, a general, efficient method for recovering a jointly sparse and\nlow-rank matrix has remained elusive.\n  Deviating from the models with generic measurements, in this paper we show\nthat if the sensing vectors are chosen at random from an incoherent subspace,\nthen the low-rank and sparse structures of the target signal can be effectively\ndecoupled. We show that a recovery algorithm that consists of a low-rank\nrecovery stage followed by a sparse recovery stage will produce an accurate\nestimate of the target when the number of measurements is\n$\\mathsf{O}(k\\,\\log\\frac{d}{k})$, where $k$ and $d$ denote the sparsity level\nand the dimension of the input signal. We also evaluate the algorithm through\nnumerical simulation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 18:38:36 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 01:03:22 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Bahmani", "Sohail", ""], ["Romberg", "Justin", ""]]}, {"id": "1507.08266", "submitter": "Dootika Vats", "authors": "Dootika Vats, James M. Flegal and Galin L. Jones", "title": "Strong Consistency of Multivariate Spectral Variance Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are used to estimate features of\ninterest of a distribution. The Monte Carlo error in estimation has an\nasymptotic normal distribution whose multivariate nature has so far been\nignored in the MCMC community. We present a class of multivariate spectral\nvariance estimators for the asymptotic covariance matrix in the Markov chain\ncentral limit theorem and provide conditions for strong consistency. We examine\nthe finite sample properties of the multivariate spectral variance estimators\nand its eigenvalues in the context of a vector autoregressive process of order\n1.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 19:08:09 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:10:49 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2016 16:00:03 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Vats", "Dootika", ""], ["Flegal", "James M.", ""], ["Jones", "Galin L.", ""]]}, {"id": "1507.08298", "submitter": "Jon A. Wellner", "authors": "Evan Greene and Jon A. Wellner", "title": "Exponential bounds for the hypergeometric distribution", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish exponential bounds for the hypergeometric distribution which\ninclude a finite sampling correction factor, but are otherwise analogous to\nbounds for the binomial distribution due to Le\\'on and Perron (2003) and\nTalagrand (1994). We also establish a convex ordering for sampling without\nreplacement from populations of real numbers between zero and one: a population\nof all zeros or ones (and hence yielding a hypergeometric distribution in the\nupper bound) gives the extreme case.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 20:20:38 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 23:07:43 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 03:55:12 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Greene", "Evan", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1507.08394", "submitter": "Michael J Lew", "authors": "Michael J. Lew", "title": "The likelihood principle does not entail a `sure thing', `evil demon' or\n  `determinist' hypothesis", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood principle makes strong claims about the nature of statistical\nevidence but is controversial. Its claims are undermined by the existence of\nseveral examples that are assumed to show that it allows, with unity\nprobability, domination of all other hypotheses by the uninteresting,\ndeterminist hypothesis that whatever happened had to happen. Such examples are\ngenerally assumed to be important obstacles to the application of the\nlikelihood principle: they are counter-examples to the principle. A re-analysis\nof Birnbaum's 1969 `counter-example', demonstrates that the standardly reported\nanalyses of such examples involves an inappropriate treatment of a nuisance\nparameter and that, when the nuisance parameter is adequately considered, there\nis no conflict between the evidential consequences of the likelihood principle\nand the intuitive evidential account of the problem. It also shows that the\nconclusion that the likelihood principle allows the determinist hypothesis to\ndominate with unity probability requires a misconception about the scope of the\nlikelihood principle or an inappropriately specified statistical model.\nWhatever happened did \\textit{not} have to happen.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 06:33:58 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 01:55:56 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Lew", "Michael J.", ""]]}, {"id": "1507.08436", "submitter": "Alain Desgagn\\'{e}", "authors": "Alain Desgagn\\'e", "title": "Robustness to outliers in location-scale parameter model using\n  log-regularly varying distributions", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1316 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1568-1595", "doi": "10.1214/15-AOS1316", "report-no": "IMS-AOS-AOS1316", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the location and scale parameters is common in statistics, using,\nfor instance, the well-known sample mean and standard deviation. However,\ninference can be contaminated by the presence of outliers if modeling is done\nwith light-tailed distributions such as the normal distribution. In this paper,\nwe study robustness to outliers in location-scale parameter models using both\nthe Bayesian and frequentist approaches. We find sufficient conditions (e.g.,\non tail behavior of the model) to obtain whole robustness to outliers, in the\nsense that the impact of the outliers gradually decreases to nothing as the\nconflict grows infinitely. To this end, we introduce the family of\nlog-Pareto-tailed symmetric distributions that belongs to the larger family of\nlog-regularly varying distributions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 09:52:07 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Desgagn\u00e9", "Alain", ""]]}, {"id": "1507.08441", "submitter": "Kang Li", "authors": "Kang Li, Wei Zheng, Mingyao Ai", "title": "Optimal designs for the proportional interference model", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1317 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1596-1616", "doi": "10.1214/15-AOS1317", "report-no": "IMS-AOS-AOS1317", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interference model has been widely used and studied in block experiments\nwhere the treatment for a particular plot has effects on its neighbor plots. In\nthis paper, we study optimal circular designs for the proportional interference\nmodel, in which the neighbor effects of a treatment are proportional to its\ndirect effect. Kiefer's equivalence theorems for estimating both the direct and\ntotal treatment effects are developed with respect to the criteria of A, D, E\nand T. Parallel studies are carried out for the undirectional model, where the\nneighbor effects do not depend on whether they are from the left or right.\nMoreover, the connection between optimal designs for the directional and\nundiretional models is built. Importantly, one can easily develop a computer\nprogram for finding optimal designs based on these theorems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 10:17:03 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Li", "Kang", ""], ["Zheng", "Wei", ""], ["Ai", "Mingyao", ""]]}, {"id": "1507.08473", "submitter": "Jia Chen", "authors": "Jia Chen, Degui Li, Hua Liang, Suojin Wang", "title": "Semiparametric GEE analysis in partially linear single-index models for\n  longitudinal data", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1320 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1682-1715", "doi": "10.1214/15-AOS1320", "report-no": "IMS-AOS-AOS1320", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a partially linear single-index model for\nlongitudinal data under a general framework which includes both the sparse and\ndense longitudinal data cases. A semiparametric estimation method based on a\ncombination of the local linear smoothing and generalized estimation equations\n(GEE) is introduced to estimate the two parameter vectors as well as the\nunknown link function. Under some mild conditions, we derive the asymptotic\nproperties of the proposed parametric and nonparametric estimators in different\nscenarios, from which we find that the convergence rates and asymptotic\nvariances of the proposed estimators for sparse longitudinal data would be\nsubstantially different from those for dense longitudinal data. We also discuss\nthe estimation of the covariance (or weight) matrices involved in the\nsemiparametric GEE method. Furthermore, we provide some numerical studies\nincluding Monte Carlo simulation and an empirical application to illustrate our\nmethodology and theory.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 12:13:19 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Chen", "Jia", ""], ["Li", "Degui", ""], ["Liang", "Hua", ""], ["Wang", "Suojin", ""]]}, {"id": "1507.08566", "submitter": "Vinay Chakravarthi Gogineni", "authors": "Vinay Chakravarthi Gogineni and Mrityunjoy Chakraborty", "title": "Diffusion Adaptation Over Clustered Multitask Networks Based on the\n  Affine Projection Algorithm", "comments": "Under Communication. arXiv admin note: substantial text overlap with\n  arXiv:1311.4894 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed adaptive networks achieve better estimation performance by\nexploiting temporal and as well spatial diversity while consuming few\nresources. Recent works have studied the single task distributed estimation\nproblem, in which the nodes estimate a single optimum parameter vector\ncollaboratively. However, there are many important applications where the\nmultiple vectors have to estimated simultaneously, in a collaborative manner.\nThis paper presents multi-task diffusion strategies based on the Affine\nProjection Algorithm (APA), usage of APA makes the algorithm robust against the\ncorrelated input. The performance analysis of the proposed multi-task diffusion\nAPA algorithm is studied in mean and mean square sense. And also a modified\nmulti-task diffusion strategy is proposed that improves the performance in\nterms of convergence rate and steady state EMSE as well. Simulations are\nconducted to verify the analytical results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 09:27:38 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 13:33:33 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 15:12:47 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 09:23:55 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Gogineni", "Vinay Chakravarthi", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1507.08645", "submitter": "Reza Solgi", "authors": "Luke Bornn, Neil Shephard and Reza Solgi", "title": "Moment conditions and Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models phrased though moment conditions are central to much of modern\ninference. Here these moment conditions are embedded within a nonparametric\nBayesian setup. Handling such a model is not probabilistically straightforward\nas the posterior has support on a manifold. We solve the relevant issues,\nbuilding new probability and computational tools using Hausdorff measures to\nanalyze them on real and simulated data. These new methods which involve\nsimulating on a manifold can be applied widely, including providing Bayesian\nanalysis of quasi-likelihoods, linear and nonlinear regression, missing data\nand hierarchical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 19:45:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 18:58:42 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Bornn", "Luke", ""], ["Shephard", "Neil", ""], ["Solgi", "Reza", ""]]}, {"id": "1507.08685", "submitter": "Andrea Montanari", "authors": "Yash Deshpande, Emmanuel Abbe, Andrea Montanari", "title": "Asymptotic Mutual Information for the Two-Groups Stochastic Block Model", "comments": "41 pages, 3 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cond-mat.stat-mech math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an information-theoretic view of the stochastic block model, a\npopular statistical model for the large-scale structure of complex networks. A\ngraph $G$ from such a model is generated by first assigning vertex labels at\nrandom from a finite alphabet, and then connecting vertices with edge\nprobabilities depending on the labels of the endpoints. In the case of the\nsymmetric two-group model, we establish an explicit `single-letter'\ncharacterization of the per-vertex mutual information between the vertex labels\nand the graph.\n  The explicit expression of the mutual information is intimately related to\nestimation-theoretic quantities, and --in particular-- reveals a phase\ntransition at the critical point for community detection. Below the critical\npoint the per-vertex mutual information is asymptotically the same as if edges\nwere independent. Correspondingly, no algorithm can estimate the partition\nbetter than random guessing. Conversely, above the threshold, the per-vertex\nmutual information is strictly smaller than the independent-edges upper bound.\nIn this regime there exists a procedure that estimates the vertex labels better\nthan random guessing.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 20:56:38 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Deshpande", "Yash", ""], ["Abbe", "Emmanuel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1507.08726", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Robustness in sparse linear models: relative efficiency based on robust\n  approximate message passing", "comments": "49 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Volume 10, Number 2 (2016),\n  3894-3944", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding efficiency in high dimensional linear models is a longstanding\nproblem of interest. Classical work with smaller dimensional problems dating\nback to Huber and Bickel has illustrated the benefits of efficient loss\nfunctions. When the number of parameters $p$ is of the same order as the sample\nsize $n$, $p \\approx n$, an efficiency pattern different from the one of Huber\nwas recently established. In this work, we consider the effects of model\nselection on the estimation efficiency of penalized methods. In particular, we\nexplore whether sparsity, results in new efficiency patterns when $p > n$. In\nthe interest of deriving the asymptotic mean squared error for regularized\nM-estimators, we use the powerful framework of approximate message passing. We\npropose a novel, robust and sparse approximate message passing algorithm\n(RAMP), that is adaptive to the error distribution. Our algorithm includes many\nnon-quadratic and non-differentiable loss functions. We derive its asymptotic\nmean squared error and show its convergence, while allowing $p, n, s \\to\n\\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new\npatterns of relative efficiency regarding a number of penalized $M$ estimators,\nwhen $p$ is much larger than $n$. We show that the classical information bound\nis no longer reachable, even for light--tailed error distributions. We show\nthat the penalized least absolute deviation estimator dominates the penalized\nleast square estimator, in cases of heavy--tailed distributions. We observe\nthis pattern for all choices of the number of non-zero parameters $s$, both $s\n\\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$,\nthe opposite regime holds. Therefore, we discover that the presence of model\nselection significantly changes the efficiency patterns.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:31:24 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 00:27:46 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1507.08727", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Large Scale Signal Detection: A Unified Perspective", "comments": "Online link of supplementary materials added; copyediting typos\n  corrected", "journal-ref": "Biometrics (2016), 72, 2, 325-334", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an overwhelmingly large literature and algorithms already available\non `large scale inference problems' based on different modeling techniques and\ncultures. Our primary goal in this paper is \\emph{not to add one more new\nmethodology} to the existing toolbox but instead (a) to clarify the mystery how\nthese different simultaneous inference methods are \\emph{connected}, (b) to\nprovide an alternative more intuitive derivation of the formulas that leads to\n\\emph{simpler} expressions, and (c) to develop a \\emph{unified} algorithm for\npractitioners. A detailed discussion on representation, estimation, inference,\nand model selection is given. Applications to a variety of real and simulated\ndatasets show promise. We end with several future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 01:37:02 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2015 18:40:36 GMT"}, {"version": "v3", "created": "Fri, 31 Mar 2017 14:10:50 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}]