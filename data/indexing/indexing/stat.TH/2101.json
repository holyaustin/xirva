[{"id": "2101.00089", "submitter": "Nakahiro Yoshida", "authors": "Nakahiro Yoshida", "title": "Asymptotic expansion of a variation with anticipative weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic expansion of a variation with anticipative weights is derived by\nthe theory of asymptotic expansion for Skorohod integrals having a mixed normal\nlimit. The expansion formula is expressed with the quasi-torsion, quasi-tangent\nand other random symbols. To specify these random symbols, it is necessary to\nclassify the level of the effect of each term appearing in the stochastic\nexpansion of the variable in question. To solve this problem, we consider a\nclass ${\\cal L}$ of certain sequences $({\\cal I}_n)_{n\\in{\\mathbb N}}$ of\nWiener functionals and we give a systematic way of estimation of the order of\n$({\\cal I}_n)_{n\\in{\\mathbb N}}$. Based on this method, we introduce a notion\nof exponent of the sequence $({\\cal I}_n)_{n\\in{\\mathbb N}}$, and investigate\nthe stability and contraction effect of the operators $D_{u_n}$ and $D$ on\n${\\cal L}$, where $u_n$ is the integrand of a Skorohod integral. After\nconstructed these machineries, we derive asymptotic expansion of the variation\nhaving anticipative weights. An application to robust volatility estimation is\nmentioned.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:59:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yoshida", "Nakahiro", ""]]}, {"id": "2101.00136", "submitter": "Yan Wang", "authors": "Yan Wang", "title": "Sub-Gaussian Error Bounds for Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret likelihood-based test functions from a geometric perspective\nwhere the Kullback-Leibler (KL) divergence is adopted to quantify the distance\nfrom a distribution to another. Such a test function can be seen as a\nsub-Gaussian random variable, and we propose a principled way to calculate its\ncorresponding sub-Gaussian norm. Then an error bound for binary hypothesis\ntesting can be obtained in terms of the sub-Gaussian norm and the KL\ndivergence, which is more informative than Pinsker's bound when the\nsignificance level is prescribed. For $M$-ary hypothesis testing, we also\nderive an error bound which is complementary to Fano's inequality by being more\ninformative when the number of hypotheses or the sample size is not large.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 02:00:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Yan", ""]]}, {"id": "2101.00360", "submitter": "Pingyi Fan Prof.", "authors": "Pingyi Fan", "title": "New-Type Hoeffding's Inequalities and Application in Tail Bounds", "comments": "8 pages, 1 figure", "journal-ref": "Open Journal of Mathematical Sciences Vol.5 No.1 pp.248 -261, 2021", "doi": "10.30538/oms2021.0161", "report-no": "ISSN: 2523-0212 (Online) 2616-4906 (Print)", "categories": "math.ST cs.AI cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Hoeffding's inequality has a lot of applications in the\nsignal and information processing fields. How to improve Hoeffding's inequality\nand find the refinements of its applications have always attracted much\nattentions. An improvement of Hoeffding inequality was recently given by Hertz\n\\cite{r1}. Eventhough such an improvement is not so big, it still can be used\nto update many known results with original Hoeffding's inequality, especially\nfor Hoeffding-Azuma inequality for martingales. However, the results in\noriginal Hoeffding's inequality and its refinement one by Hertz only considered\nthe first order moment of random variables. In this paper, we present a new\ntype of Hoeffding's inequalities, where the high order moments of random\nvariables are taken into account. It can get some considerable improvements in\nthe tail bounds evaluation compared with the known results. It is expected that\nthe developed new type Hoeffding's inequalities could get more interesting\napplications in some related fields that use Hoeffding's results.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:19:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fan", "Pingyi", ""]]}, {"id": "2101.00366", "submitter": "Suman Bhattacharya", "authors": "Suman K. Bhattacharya, Kshitij Khare and Subhadip Pal", "title": "Geometric ergodicity of Gibbs samplers for the Horseshoe and its\n  regularized variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Horseshoe is a widely used and popular continuous shrinkage prior for\nhigh-dimensional Bayesian linear regression. Recently, regularized versions of\nthe Horseshoe prior have also been introduced in the literature. Various Gibbs\nsampling Markov chains have been developed in the literature to generate\napproximate samples from the corresponding intractable posterior densities.\nEstablishing geometric ergodicity of these Markov chains provides crucial\ntechnical justification for the accuracy of asymptotic standard errors for\nMarkov chain based estimates of posterior quantities. In this paper, we\nestablish geometric ergodicity for various Gibbs samplers corresponding to the\nHorseshoe prior and its regularized variants in the context of linear\nregression. First, we establish geometric ergodicity of a Gibbs sampler for the\noriginal Horseshoe posterior under strictly weaker conditions than existing\nanalyses in the literature. Second, we consider the regularized Horseshoe prior\nintroduced in Piironen and Vehtari (2017), and prove geometric ergodicity for a\nGibbs sampling Markov chain to sample from the corresponding posterior without\nany truncation constraint on the global and local shrinkage parameters.\nFinally, we consider a variant of this regularized Horseshoe prior introduced\nin Nishimura and Suchard (2020), and again establish geometric ergodicity for a\nGibbs sampling Markov chain to sample from the corresponding posterior.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 04:17:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bhattacharya", "Suman K.", ""], ["Khare", "Kshitij", ""], ["Pal", "Subhadip", ""]]}, {"id": "2101.00514", "submitter": "Liliana Forzani", "authors": "Dennis Cook, Liliana Forzani, Lan Liu", "title": "Envelopes for multivariate linear regression with linearly constrained\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A constrained multivariate linear model is a multivariate linear model with\nthe columns of its coefficient matrix constrained to lie in a known subspace.\nThis class of models includes those typically used to study growth curves and\nlongitudinal data. Envelope methods have been proposed to improve estimation\nefficiency in the class of unconstrained multivariate linear models, but have\nnot yet been developed for constrained models that we develop in this article.\nWe first compare the standard envelope estimator based on an unconstrained\nmultivariate model with the standard estimator arising from a constrained\nmultivariate model in terms of bias and efficiency. Then, to further improve\nefficiency, we propose a novel envelope estimator based on a constrained\nmultivariate model. Novel envelope-based testing methods are also proposed. We\nprovide support for our proposals by simulations and by studying the classical\ndental data and data from the China Health and Nutrition Survey and a study of\nprobiotic capacity to reduced Salmonella infection.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 20:44:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cook", "Dennis", ""], ["Forzani", "Liliana", ""], ["Liu", "Lan", ""]]}, {"id": "2101.00527", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai", "title": "Statistical Inference on the Hilbert Sphere with Application to Random\n  Densities", "comments": "N/A", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinite-dimensional Hilbert sphere $S^\\infty$ has been widely employed\nto model density functions and shapes, extending the finite-dimensional\ncounterpart. We consider the Fr\\'echet mean as an intrinsic summary of the\ncentral tendency of data lying on $S^\\infty$. To break a path for sound\nstatistical inference, we derive properties of the Fr\\'echet mean on $S^\\infty$\nby establishing its existence and uniqueness as well as a root-$n$ central\nlimit theorem (CLT) for the sample version, overcoming obstructions from\ninfinite-dimensionality and lack of compactness on $S^\\infty$. Intrinsic CLTs\nfor the estimated tangent vectors and covariance operator are also obtained.\nAsymptotic and bootstrap hypothesis tests for the Fr\\'echet mean based on\nprojection and norm are then proposed and are shown to be consistent. The\nproposed two-sample tests are applied to make inference for daily taxi demand\npatterns over Manhattan modeled as densities, of which the square roots are\nanalyzed on the Hilbert sphere. Numerical properties of the proposed hypothesis\ntests which utilize the spherical geometry are studied in the real data\napplication and simulations, where we demonstrate that the tests based on the\nintrinsic geometry compare favorably to those based on an extrinsic or flat\ngeometry.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 22:52:14 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dai", "Xiongtao", ""]]}, {"id": "2101.00548", "submitter": "Xiaogang Duan", "authors": "X.G. Duan", "title": "Better understanding of the multivariate hypergeometric distribution\n  with implications in design-based survey sampling", "comments": "9 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate hypergeometric distribution arises frequently in elementary\nstatistics and probability courses, for simultaneously studying the occurence\nlaw of specified events, when sampling without replacement from a finite\npopulation with fixed number of classification. Covariance matrix of this\ndistribution is well known to be identical to its multinomial counterpart\nmultiplied by 1-(n-1)/(N-1), with N and n being population and sample sizes,\nrespectively. It appears to however, have been less discussed in the literature\nabout the meaning of this relationship, especially regarding the specific form\nof the multiplier. Based on an augmenting argument together with probabilistic\nsymmetry, we present a more transparent understanding for the covariance\nstructure of the multivariate hypergeometric distribution. We discuss\nimplications of these combined techniques and provide a unified description\nabout the relative efficiency for estimating population mean based on simple\nrandom sampling, probability proportional-to-size sampling and adaptive cluster\nsampling, with versus without replacement. We also provide insight into the\nclassic random group method for variance estimation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 03:10:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Duan", "X. G.", ""]]}, {"id": "2101.00575", "submitter": "Nimrod Segol", "authors": "Nimrod Segol, Boaz Nadler", "title": "Improved Convergence Guarantees for Learning Gaussian Mixture Models by\n  EM and Gradient EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters a Gaussian Mixture Model\nwith K components of known weights, all with an identity covariance matrix. We\nmake two contributions. First, at the population level, we present a sharper\nanalysis of the local convergence of EM and gradient EM, compared to previous\nworks. Assuming a separation of $\\Omega(\\sqrt{\\log K})$, we prove convergence\nof both methods to the global optima from an initialization region larger than\nthose of previous works. Specifically, the initial guess of each component can\nbe as far as (almost) half its distance to the nearest Gaussian. This is\nessentially the largest possible contraction region. Our second contribution\nare improved sample size requirements for accurate estimation by EM and\ngradient EM. In previous works, the required number of samples had a quadratic\ndependence on the maximal separation between the K components, and the\nresulting error estimate increased linearly with this maximal separation. In\nthis manuscript we show that both quantities depend only logarithmically on the\nmaximal separation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:10:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Segol", "Nimrod", ""], ["Nadler", "Boaz", ""]]}, {"id": "2101.00767", "submitter": "Yassine El Maazouz", "authors": "Yassine El Maazouz", "title": "The Gaussian entropy map in valued fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We exhibit the analog of the entropy map for multivariate Gaussian\ndistributions on local fields. As in the real case, the image of this map lies\nin the supermodular cone and it determines the distribution of the valuation\nvector. In general, this map can be defined for non-archimedian valued fields\nwhose valuation group is an additive subgroup of the real line, and it remains\nsupermodular. We also explicitly compute the image of this map in dimension 3.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:38:45 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 01:47:36 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 01:24:56 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 01:35:19 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Maazouz", "Yassine El", ""]]}, {"id": "2101.00914", "submitter": "Zong Shang", "authors": "Zong Shang", "title": "Benign overfitting without concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We obtain a sufficient condition for benign overfitting of linear regression\nproblem. Our result does not rely on concentration argument but on small-ball\nassumption and thus can holds in heavy-tailed case. The basic idea is to\nestablish a coordinate small-ball estimate in terms of effective rank so that\nwe can calibrate the balance of epsilon-Net and exponential probability. Our\nresult indicates that benign overfitting is not depending on concentration\nproperty of the input vector. Finally, we discuss potential difficulties for\nbenign overfitting beyond linear model and a benign overfitting result without\ntruncated effective rank.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:18:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shang", "Zong", ""]]}, {"id": "2101.01011", "submitter": "Christian P. Robert", "authors": "Christian P. Robert and Gareth O. Roberts (University of Warwick)", "title": "Rao-Blackwellization in the MCMC era", "comments": "This paper is to appear in The International Statistical Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rao-Blackwellization is a notion often occurring in the MCMC literature, with\npossibly different meanings and connections with the original Rao--Blackwell\ntheorem (Rao, 1945 and Blackwell,1947), including a reduction of the variance\nof the resulting Monte Carlo approximations. This survey reviews some of the\nmeanings of the term.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:44:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Robert", "Christian P.", "", "University of Warwick"], ["Roberts", "Gareth O.", "", "University of Warwick"]]}, {"id": "2101.01590", "submitter": "Matyas Barczy", "authors": "Matyas Barczy, Gyula Pap", "title": "Mixing convergence of LSE for supercritical Gaussian AR(2) processes\n  using random scaling", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove mixing convergence of least squares estimator of autoregressive\nparameters for supercritical Gaussian autoregressive processes of order 2\nhaving real characteristic roots with different absolute values. We use an\nappropriate random scaling such that the limit distribution is a\ntwo-dimensional normal distribution concentrated on a one-dimensional ray\ndetermined by the characteristic root having the larger absolute value.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:30:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Barczy", "Matyas", ""], ["Pap", "Gyula", ""]]}, {"id": "2101.01708", "submitter": "Yulong Lu", "authors": "Jianfeng Lu, Yulong Lu, Min Wang", "title": "A Priori Generalization Analysis of the Deep Ritz Method for Solving\n  High Dimensional Elliptic Equations", "comments": "Revised the definition of Barron space and updated the proofs induced\n  by the changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the a priori generalization analysis of the Deep Ritz\nMethod (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for\nsolving high dimensional partial differential equations. We derive the\ngeneralization error bounds of two-layer neural networks in the framework of\nthe DRM for solving two prototype elliptic PDEs: Poisson equation and static\nSchr\\\"odinger equation on the $d$-dimensional unit hypercube. Specifically, we\nprove that the convergence rates of generalization errors are independent of\nthe dimension $d$, under the a priori assumption that the exact solutions of\nthe PDEs lie in a suitable low-complexity space called spectral Barron space.\nMoreover, we give sufficient conditions on the forcing term and the potential\nfunction which guarantee that the solutions are spectral Barron functions. We\nachieve this by developing a new solution theory for the PDEs on the spectral\nBarron space, which can be viewed as an analog of the classical Sobolev\nregularity theory for PDEs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:50:59 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:58:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Jianfeng", ""], ["Lu", "Yulong", ""], ["Wang", "Min", ""]]}, {"id": "2101.01839", "submitter": "Ricardo Carrizo Vergara", "authors": "R. Carrizo Vergara", "title": "Generalized Stochastic Processes as Linear Transformations of White\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that any (real) generalized stochastic process over $\\mathbb{R}^{d}$\ncan be expressed as a linear transformation of a White Noise process over\n$\\mathbb{R}^{d}$. The procedure is done by using the regularity theorem for\ntempered distributions to obtain a mean-square continuous stochastic process\nwhich is then expressed in a Karhunen-Lo\\`eve expansion with respect to a\nconvenient Hilbert space. This result also allows to conclude that any\ngeneralized stochastic process can be expressed as a series expansion of\ndeterministic tempered distributions weighted by uncorrelated random variables\nwith square-summable variances. A result specifying when a generalized\nstochastic process can be linearly transformed into a White Noise is also\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 01:11:03 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Vergara", "R. Carrizo", ""]]}, {"id": "2101.01908", "submitter": "Bo Zhang", "authors": "Bo Zhang, Guangming Pan, Qiwei Yao and Wang Zhou", "title": "Factor Modelling for Clustering High-dimensional Time Series", "comments": "10 figures, 12 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new unsupervised learning method for clustering a large number\nof time series based on a latent factor structure. Each cluster is\ncharacterized by its own cluster-specific factors in addition to some common\nfactors which impact on all the time series concerned. Our setting also offers\nthe flexibility that some time series may not belong to any clusters. The\nconsistency with explicit convergence rates is established for the estimation\nof the common factors, the cluster-specific factors, the latent clusters.\nNumerical illustration with both simulated data as well as a real data example\nis also reported. As a spin-off, the proposed new approach also advances\nsignificantly the statistical inference for the factor model of Lam and Yao\n(2012).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:55:15 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Bo", ""], ["Pan", "Guangming", ""], ["Yao", "Qiwei", ""], ["Zhou", "Wang", ""]]}, {"id": "2101.02094", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Bernstein-Type Bounds for Beta Distribution", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes Bernstein-type closed-form concentration inequalities\nfor the beta distribution, with optimal variance proxy. For skewed\ndistributions, these bounds are demonstrated to be more accurate then\nsub-gaussian and sub-gamma inequalities from prior works.\n  The approach builds on the recursion obtained from a hyper-geometric\nrepresentation of the central moments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:36:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 18:58:22 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2101.02347", "submitter": "Chao Gao", "authors": "Chao Gao and Anderson Y. Zhang", "title": "SDP Achieves Exact Minimax Optimality in Phase Synchronization", "comments": "arXiv admin note: text overlap with arXiv:2010.04345", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the phase synchronization problem with noisy measurements\n$Y=z^*z^{*H}+\\sigma W\\in\\mathbb{C}^{n\\times n}$, where $z^*$ is an\n$n$-dimensional complex unit-modulus vector and $W$ is a complex-valued\nGaussian random matrix. It is assumed that each entry $Y_{jk}$ is observed with\nprobability $p$. We prove that an SDP relaxation of the MLE achieves the error\nbound $(1+o(1))\\frac{\\sigma^2}{2np}$ under a normalized squared $\\ell_2$ loss.\nThis result matches the minimax lower bound of the problem, and even the\nleading constant is sharp. The analysis of the SDP is based on an equivalent\nnon-convex programming whose solution can be characterized as a fixed point of\nthe generalized power iteration lifted to a higher dimensional space. This\nviewpoint unifies the proofs of the statistical optimality of three different\nmethods: MLE, SDP, and generalized power method. The technique is also applied\nto the analysis of the SDP for $\\mathbb{Z}_2$ synchronization, and we achieve\nthe minimax optimal error $\\exp\\left(-(1-o(1))\\frac{np}{2\\sigma^2}\\right)$ with\na sharp constant in the exponent.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:14:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.02416", "submitter": "Yongdao Zhou", "authors": "Mei Zhang, Feng Yang and Yongdao Zhou", "title": "Uniformity criterion for designs with both qualitative and quantitative\n  factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments with both qualitative and quantitative factors occur frequently\nin practical applications. Many construction methods for this kind of designs,\nsuch as marginally coupled designs, were proposed to pursue some good\nspace-filling structures. However, few criteria can be adapted to quantify the\nspace-filling property of designs involving both qualitative and quantitative\nfactors. As the uniformity is an important space-filling property of a design,\nin this paper, a new uniformity criterion, qualitative-quantitative discrepancy\n(QQD), is proposed for assessing the uniformity of designs with both types of\nfactors. The closed form and lower bounds of the QQD are presented to calculate\nthe exact QQD values of designs and recognize the uniform designs directly. In\naddition, a connection between the QQD and the balance pattern is derived,\nwhich not only helps to obtain a new lower bound but also provides a\nstatistical justification of the QQD. Several examples show that the proposed\ncriterion is reasonable and useful since it can distinguish distinct designs\nvery well.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:43:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhang", "Mei", ""], ["Yang", "Feng", ""], ["Zhou", "Yongdao", ""]]}, {"id": "2101.02417", "submitter": "Xin Tong Thomson", "authors": "Tiangang Cui and Xin T. Tong", "title": "A unified performance analysis of likelihood-informed subspace methods", "comments": "48 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood-informed subspace (LIS) method offers a viable route to\nreducing the dimensionality of high-dimensional probability distributions\narisen in Bayesian inference. LIS identifies an intrinsic low-dimensional\nlinear subspace where the target distribution differs the most from some\ntractable reference distribution. Such a subspace can be identified using the\nleading eigenvectors of a Gram matrix of the gradient of the log-likelihood\nfunction. Then, the original high-dimensional target distribution is\napproximated through various forms of ridge approximations of the likelihood\nfunction, in which the approximated likelihood only has support on the\nintrinsic low-dimensional subspace. This approximation enables the design of\ninference algorithms that can scale sub-linearly with the apparent\ndimensionality of the problem. Intuitively, the accuracy of the approximation,\nand hence the performance of the inference algorithms, are influenced by three\nfactors -- the dimension truncation error in identifying the subspace, Monte\nCarlo error in estimating the Gram matrices, and Monte Carlo error in\nconstructing ridge approximations. This work establishes a unified framework to\nanalysis each of these three factors and their interplay. Under mild technical\nassumptions, we establish error bounds for a range of existing dimension\nreduction techniques based on the principle of LIS. Our error bounds also\nprovide useful insights into the accuracy comparison of these methods. In\naddition, we analyze the integration of LIS with sampling methods such as\nMarkov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC). We also\ndemonstrate our analyses on a linear inverse problem with Gaussian prior, which\nshows that all the estimates can be dimension-independent if the prior\ncovariance is a trace-class operator.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:48:42 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cui", "Tiangang", ""], ["Tong", "Xin T.", ""]]}, {"id": "2101.02491", "submitter": "Taeho Kim", "authors": "Alexander Goldenshluger and Taeho Kim", "title": "Density Deconvolution with Non-Standard Error Distributions: Rates of\n  Convergence and Adaptive Estimation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a typical standard assumption in the density deconvolution problem that\nthe characteristic function of the measurement error distribution is non-zero\non the real line. While this condition is assumed in the majority of existing\nworks on the topic, there are many problem instances of interest where it is\nviolated. In this paper we focus on non--standard settings where the\ncharacteristic function of the measurement errors has zeros, and study how\nzeros multiplicity affects the estimation accuracy. For a prototypical problem\nof this type we demonstrate that the best achievable estimation accuracy is\ndetermined by the multiplicity of zeros, the rate of decay of the error\ncharacteristic function, as well as by the smoothness and the tail behavior of\nthe estimated density. We derive lower bounds on the minimax risk and develop\noptimal in the minimax sense estimators. In addition, we consider the problem\nof adaptive estimation and propose a data-driven estimator that automatically\nadapts to unknown smoothness and tail behavior of the density to be estimated.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:20:46 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Goldenshluger", "Alexander", ""], ["Kim", "Taeho", ""]]}, {"id": "2101.02553", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Fernando Amat Gil, Ashok Chandrashekar", "title": "Off-Policy Evaluation of Slate Policies under Bayes Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation for slate bandits, for the\ntypical case in which the logging policy factorizes over the slots of the\nslate. We slightly depart from the existing literature by taking Bayes risk as\nthe criterion by which to evaluate estimators, and we analyze the family of\n'additive' estimators that includes the pseudoinverse (PI) estimator of\nSwaminathan et al.\\ (2017; arXiv:1605.04812). Using a control variate approach,\nwe identify a new estimator in this family that is guaranteed to have lower\nrisk than PI in the above class of problems. In particular, we show that the\nrisk improvement over PI grows linearly with the number of slots, and linearly\nwith the gap between the arithmetic and the harmonic mean of a set of\nslot-level divergences between the logging and the target policy. In the\ntypical case of a uniform logging policy and a deterministic target policy,\neach divergence corresponds to slot size, showing that maximal gains can be\nobtained for slate problems with diverse numbers of actions per slot.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:07:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vlassis", "Nikos", ""], ["Gil", "Fernando Amat", ""], ["Chandrashekar", "Ashok", ""]]}, {"id": "2101.02776", "submitter": "Peyman Mohajerin Esfahani", "authors": "Armin Eftekhari and Peyman Mohajerin Esfahani", "title": "The Nonconvex Geometry of Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gauge function, closely related to the atomic norm, measures the\ncomplexity of a statistical model, and has found broad applications in machine\nlearning and statistical signal processing. In a high-dimensional learning\nproblem, the gauge function attempts to safeguard against overfitting by\npromoting a sparse (concise) representation within the learning alphabet.\n  In this work, within the context of linear inverse problems, we pinpoint the\nsource of its success, but also argue that the applicability of the gauge\nfunction is inherently limited by its convexity, and showcase several learning\nproblems where the classical gauge function theory fails. We then introduce a\nnew notion of statistical complexity, gauge$_p$ function, which overcomes the\nlimitations of the gauge function. The gauge$_p$ function is a simple\ngeneralization of the gauge function that can tightly control the sparsity of a\nstatistical model within the learning alphabet and, perhaps surprisingly, draws\nfurther inspiration from the Burer-Monteiro factorization in computational\nmathematics.\n  We also propose a new learning machine, with the building block of gauge$_p$\nfunction, and arm this machine with a number of statistical guarantees. The\npotential of the proposed gauge$_p$ function theory is then studied for two\nstylized applications. Finally, we discuss the computational aspects and, in\nparticular, suggest a tractable numerical algorithm for implementing the new\nlearning machine.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:55:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Eftekhari", "Armin", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "2101.02800", "submitter": "Kelly Ramsay", "authors": "Kelly Ramsay and Shoja'eddin Chenouri", "title": "Differentially private depth functions and their associated medians", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the differentially private estimation of data\ndepth functions and their associated medians. We introduce several methods for\nprivatizing depth values at a fixed point, and show that for some depth\nfunctions, when the depth is computed at an out of sample point, privacy can be\ngained for free when $n\\rightarrow \\infty$. We also present a method for\nprivately estimating the vector of sample point depth values. Additionally, we\nintroduce estimation methods for depth-based medians for both depth functions\nwith low global sensitivity and depth functions with only highly probable, low\nlocal sensitivity. We provide a general result (Lemma 1) which can be used to\nprove consistency of an estimator produced by the exponential mechanism,\nprovided the limiting cost function is sufficiently smooth at a unique\nminimizer. We also introduce a general algorithm to privately estimate a\nminimizer of a cost function which has, with high probability, low local\nsensitivity. This algorithm combines the propose-test-release algorithm with\nthe exponential mechanism. An application of this algorithm to generate\nconsistent estimates of the projection depth-based median is presented. Thus,\nfor these private depth-based medians, we show that it is possible for privacy\nto be obtained for free when $n\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:56:24 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 01:39:45 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 20:18:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ramsay", "Kelly", ""], ["Chenouri", "Shoja'eddin", ""]]}, {"id": "2101.02957", "submitter": "Oliver Mason", "authors": "Aisling Mc Glinchey and Oliver Mason", "title": "Observations on the Bias of Nonnegative Mechanisms for Differential\n  Privacy", "comments": null, "journal-ref": "AIMS Foundations of Data Science, December 2020", "doi": "10.3934/fods.2020020", "report-no": null, "categories": "cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two methods for differentially private analysis of bounded data and\nextend these to nonnegative queries. We first recall that for the Laplace\nmechanism, boundary inflated truncation (BIT) applied to nonnegative queries\nand truncation both lead to strictly positive bias. We then consider a\ngeneralization of BIT using translated ramp functions. We explicitly\ncharacterise the optimal function in this class for worst case bias. We show\nthat applying any square-integrable post-processing function to a Laplace\nmechanism leads to a strictly positive maximal absolute bias. A corresponding\nresult is also shown for a generalisation of truncation, which we refer to as\nrestriction. We also briefly consider an alternative approach based on\nmultiplicative mechanisms for positive data and show that, without additional\nrestrictions, these mechanisms can lead to infinite bias.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:58:25 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Glinchey", "Aisling Mc", ""], ["Mason", "Oliver", ""]]}, {"id": "2101.02983", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "Asymptotically optimal inference in sparse sequence models with a simple\n  data-dependent measure", "comments": "Comments welcome at https://researchers.one/articles/21.01.00001", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For high-dimensional inference problems, statisticians have a number of\ncompeting interests. On the one hand, procedures should provide accurate\nestimation, reliable structure learning, and valid uncertainty quantification.\nOn the other hand, procedures should be computationally efficient and able to\nscale to very high dimensions. In this note, I show that a very simple\ndata-dependent measure can achieve all of these desirable properties\nsimultaneously, along with some robustness to the error distribution, in sparse\nsequence models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 12:26:27 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "2101.03446", "submitter": "James Foster", "authors": "James Foster, Terry Lyons, Harald Oberhauser", "title": "The shifted ODE method for underdamped Langevin MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the underdamped Langevin diffusion (ULD) and\npropose a numerical approximation using its associated ordinary differential\nequation (ODE). When used as a Markov Chain Monte Carlo (MCMC) algorithm, we\nshow that the ODE approximation achieves a $2$-Wasserstein error of\n$\\varepsilon$ in\n$\\mathcal{O}\\big(d^{\\frac{1}{3}}/\\varepsilon^{\\frac{2}{3}}\\big)$ steps under\nthe standard smoothness and strong convexity assumptions on the target\ndistribution. This matches the complexity of the randomized midpoint method\nproposed by Shen and Lee [NeurIPS 2019] which was shown to be order optimal by\nCao, Lu and Wang. However, the main feature of the proposed numerical method is\nthat it can utilize additional smoothness of the target log-density $f$. More\nconcretely, we show that the ODE approximation achieves a $2$-Wasserstein error\nof $\\varepsilon$ in\n$\\mathcal{O}\\big(d^{\\frac{2}{5}}/\\varepsilon^{\\frac{2}{5}}\\big)$ and\n$\\mathcal{O}\\big(\\sqrt{d}/\\varepsilon^{\\frac{1}{3}}\\big)$ steps when Lipschitz\ncontinuity is assumed for the Hessian and third derivative of $f$. By\ndiscretizing this ODE using a third order Runge-Kutta method, we can obtain a\npractical MCMC method that uses just two additional gradient evaluations per\nstep. In our experiment, where the target comes from a logistic regression,\nthis method shows faster convergence compared to other unadjusted Langevin MCMC\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 00:05:01 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 12:30:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Foster", "James", ""], ["Lyons", "Terry", ""], ["Oberhauser", "Harald", ""]]}, {"id": "2101.03468", "submitter": "David Hong", "authors": "David Hong and Kyle Gilman and Laura Balzano and Jeffrey A. Fessler", "title": "HePPCAT: Probabilistic PCA for Data with Heteroscedastic Noise", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. 26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a classical and ubiquitous method for\nreducing data dimensionality, but it is suboptimal for heterogeneous data that\nare increasingly common in modern applications. PCA treats all samples\nuniformly so degrades when the noise is heteroscedastic across samples, as\noccurs, e.g., when samples come from sources of heterogeneous quality. This\npaper develops a probabilistic PCA variant that estimates and accounts for this\nheterogeneity by incorporating it in the statistical model. Unlike in the\nhomoscedastic setting, the resulting nonconvex optimization problem is not\nseemingly solved by singular value decomposition. This paper develops a\nheteroscedastic probabilistic PCA technique (HePPCAT) that uses efficient\nalternating maximization algorithms to jointly estimate both the underlying\nfactors and the unknown noise variances. Simulation experiments illustrate the\ncomparative speed of the algorithms, the benefit of accounting for\nheteroscedasticity, and the seemingly favorable optimization landscape of this\nproblem. Real data experiments on environmental air quality data show that\nHePPCAT can give a better PCA estimate than techniques that do not account for\nheteroscedasticity.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 03:52:56 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 03:12:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Hong", "David", ""], ["Gilman", "Kyle", ""], ["Balzano", "Laura", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "2101.03550", "submitter": "Nadji Rahmania", "authors": "Hamida Talhi (1), Hiba Aiachi (1) and Nadji Rahmania (2) ((1) Badji\n  Mokhtar University Annaba Algeria, (2) Lille University Villeneuve d Ascq\n  France)", "title": "Bayesian estimation of a competing risk model based on Weibull and\n  exponential distributions under right censored data", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we investigate the estimation of the unknown parameters of a\ncompeting risk model based on a Weibull distributed decreasing failure rate and\nan exponentially distributed constant failure rate, under right censored\ndata.likelihood estimators.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:38:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Talhi", "Hamida", ""], ["Aiachi", "Hiba", ""], ["Rahmania", "Nadji", ""]]}, {"id": "2101.03570", "submitter": "Anna-Laura Sattelberger", "authors": "Robin van der Veer and Anna-Laura Sattelberger", "title": "Maximum Likelihood Estimation from a Tropical and a Bernstein--Sato\n  Perspective", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate Maximum Likelihood Estimation with tools from\nTropical Geometry and Bernstein--Sato theory. We investigate the critical\npoints of very affine varieties and study their asymptotic behavior. We relate\nthese asymptotics to particular rays in the tropical variety as well as to\nBernstein--Sato ideals and give a connection to Maximum Likelihood Estimation\nin Statistics.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 15:43:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["van der Veer", "Robin", ""], ["Sattelberger", "Anna-Laura", ""]]}, {"id": "2101.03612", "submitter": "Sebastian Reich", "authors": "Yuming Ba, Jana de Wiljes, Dean S. Oliver, and Sebastian Reich", "title": "Randomised maximum likelihood based posterior sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimization of a stochastic cost function is commonly used for approximate\nsampling in high-dimensional Bayesian inverse problems with Gaussian prior\ndistributions and multimodal posterior distributions. The density of the\nsamples generated by minimization is not the desired target density, unless the\nobservation operator is linear, but the distribution of samples is useful as a\nproposal density for importance sampling or for Markov chain Monte Carlo\nmethods. In this paper, we focus on applications to sampling from multimodal\nposterior distributions in high dimensions. We first show that sampling from\nmultimodal distributions is improved by computing all critical points instead\nof only minimizers of the objective function. For applications to\nhigh-dimensional geoscience problems, we demonstrate an efficient approximate\nweighting that uses a low-rank Gauss-Newton approximation of the determinant of\nthe Jacobian. The method is applied to two toy problems with known posterior\ndistributions and a Darcy flow problem with multiple modes in the posterior.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:48:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ba", "Yuming", ""], ["de Wiljes", "Jana", ""], ["Oliver", "Dean S.", ""], ["Reich", "Sebastian", ""]]}, {"id": "2101.03801", "submitter": "Salem Said", "authors": "Salem Said, Nicolas Le Bihan, Jonathan H. Manton", "title": "Hidden Markov chains and fields with observations in Riemannian\n  manifolds", "comments": "accepted for publication at MTNS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov chain, or Markov field, models, with observations in a\nEuclidean space, play a major role across signal and image processing. The\npresent work provides a statistical framework which can be used to extend these\nmodels, along with related, popular algorithms (such as the Baum-Welch\nalgorithm), to the case where the observations lie in a Riemannian manifold. It\nis motivated by the potential use of hidden Markov chains and fields, with\nobservations in Riemannian manifolds, as models for complex signals and images.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:27:31 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:19:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Said", "Salem", ""], ["Bihan", "Nicolas Le", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "2101.03838", "submitter": "Kweku Abraham", "authors": "Kweku Abraham, Ismael Castillo, Elisabeth Gassiat", "title": "Multiple Testing in Nonparametric Hidden Markov Models: An Empirical\n  Bayes Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Given a nonparametric Hidden Markov Model (HMM) with two states, the question\nof constructing efficient multiple testing procedures is considered, treating\none of the states as an unknown null hypothesis. A procedure is introduced,\nbased on nonparametric empirical Bayes ideas, that controls the False Discovery\nRate (FDR) at a user--specified level. Guarantees on power are also provided,\nin the form of a control of the true positive rate. One of the key steps in the\nconstruction requires supremum--norm convergence of preliminary estimators of\nthe emission densities of the HMM. We provide the existence of such estimators,\nwith convergence at the optimal minimax rate, for the case of a HMM with $J\\ge\n2$ states, which is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:14:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Abraham", "Kweku", ""], ["Castillo", "Ismael", ""], ["Gassiat", "Elisabeth", ""]]}, {"id": "2101.04039", "submitter": "Sloan Nietert", "authors": "Sloan Nietert, Ziv Goldfeld, Kengo Kato", "title": "From Smooth Wasserstein Distance to Dual Sobolev Norm: Empirical\n  Approximation and Statistical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical distances, i.e., discrepancy measures between probability\ndistributions, are ubiquitous in probability theory, statistics and machine\nlearning. To combat the curse of dimensionality when estimating these distances\nfrom data, recent work has proposed smoothing out local irregularities in the\nmeasured distributions via convolution with a Gaussian kernel. Motivated by the\nscalability of the smooth framework to high dimensions, we conduct an in-depth\nstudy of the structural and statistical behavior of the Gaussian-smoothed\n$p$-Wasserstein distance $\\mathsf{W}_p^{(\\sigma)}$, for arbitrary $p\\geq 1$. We\nstart by showing that $\\mathsf{W}_p^{(\\sigma)}$ admits a metric structure that\nis topologically equivalent to classic $\\mathsf{W}_p$ and is stable with\nrespect to perturbations in $\\sigma$. Moving to statistical questions, we\nexplore the asymptotic properties of\n$\\mathsf{W}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$, where $\\hat{\\mu}_n$ is the\nempirical distribution of $n$ i.i.d. samples from $\\mu$. To that end, we prove\nthat $\\mathsf{W}_p^{(\\sigma)}$ is controlled by a $p$th order smooth dual\nSobolev norm $\\mathsf{d}_p^{(\\sigma)}$. Since\n$\\mathsf{d}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$ coincides with the supremum of an\nempirical process indexed by Gaussian-smoothed Sobolev functions, it lends\nitself well to analysis via empirical process theory. We derive the limit\ndistribution of $\\sqrt{n}\\mathsf{d}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$ in all\ndimensions $d$, when $\\mu$ is sub-Gaussian. Through the aforementioned bound,\nthis implies a parametric empirical convergence rate of $n^{-1/2}$ for\n$\\mathsf{W}_p^{(\\sigma)}$, contrasting the $n^{-1/d}$ rate for unsmoothed\n$\\mathsf{W}_p$ when $d \\geq 3$. As applications, we provide asymptotic\nguarantees for two-sample testing and minimum distance estimation. When $p=2$,\nwe further show that $\\mathsf{d}_2^{(\\sigma)}$ can be expressed as a maximum\nmean discrepancy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:23:24 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:39:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Nietert", "Sloan", ""], ["Goldfeld", "Ziv", ""], ["Kato", "Kengo", ""]]}, {"id": "2101.04084", "submitter": "Quan Zhou", "authors": "Quan Zhou, Hyunwoong Chang", "title": "Complexity analysis of Bayesian learning of high-dimensional DAG models\n  and their equivalence classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider MCMC methods for learning equivalence classes of sparse Gaussian\nDAG models when $p = e^{o(n)}$. The main contribution of this work is a rapid\nmixing result for a random walk Metropolis-Hastings algorithm, which we prove\nusing a canonical path method. It reveals that the complexity of Bayesian\nlearning of sparse equivalence classes grows only polynomially in $n$ and $p$,\nunder some common high-dimensional assumptions. Further, a series of\nhigh-dimensional consistency results is obtained by the path method, including\nthe strong selection consistency of an empirical Bayes model for structure\nlearning and the consistency of a greedy local search on the restricted search\nspace. Rapid mixing and slow mixing results for other structure-learning MCMC\nmethods are also derived. Our path method and mixing time results yield crucial\ninsights into the computational aspects of high-dimensional structure learning,\nwhich may be used to develop more efficient MCMC algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:27:59 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhou", "Quan", ""], ["Chang", "Hyunwoong", ""]]}, {"id": "2101.04105", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee", "title": "Some characterisation results on classical and free Poisson thinning", "comments": "19 pages, 1 figure. Added the free analogue of Craig's theorem in\n  this version and streamlined some proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poisson thinning is an elementary result in probability, which is of great\nimportance in the theory of Poisson point processes. In this article, we record\na couple of characterisation results on Poisson thinning. We also consider free\nPoisson thinning, the free probability analogue of Poisson thinning, which\narises naturally as a high-dimensional asymptotic analogue of Cochran's theorem\nfrom multivariate statistics on the \"Wishart-ness\" of quadratic functions of\nGaussian random matrices. The main difference between classical and free\nPoisson thinning is that, in the former, the involved Poisson random variable\ncan have an arbitrary mean, whereas, in the free version, the \"mean\" of the\nrelevant free Poisson variable must be 1. We prove similar characterisation\nresults for free Poisson thinning and note their implications in the context of\nCochran's theorem.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:55:01 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 18:08:52 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 18:54:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""]]}, {"id": "2101.04210", "submitter": "Kare Kamila", "authors": "Kare Kamila", "title": "General Hannan and Quinn Criterion for Common Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper aims to study data driven model selection criteria for a large\nclass of time series, which includes ARMA or AR($\\infty$) processes, as well as\nGARCH or ARCH($\\infty$), APARCH and many others processes. We tackled the\nchallenging issue of designing adaptive criteria which enjoys the strong\nconsistency property. When the observations are generated from one of the\naforementioned models, the new criteria, select the true model almost surely\nasymptotically. The proposed criteria are based on the minimization of a\npenalized contrast akin to the Hannan and Quinn's criterion and then involved a\nterm which is known for most classical time series models and for more complex\nmodels, this term can be data driven calibrated. Monte-Carlo experiments and an\nillustrative example on the CAC 40 index are performed to highlight the\nobtained results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:03:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kamila", "Kare", ""]]}, {"id": "2101.04235", "submitter": "Philip White", "authors": "Xavier Emery, Emilio Porcu, Philip White", "title": "Flexible Validity Conditions for the Multivariate Mat\\'ern Covariance in\n  any Spatial Dimension and for any Number of Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible multivariate covariance models for spatial data are on demand. This\npaper addresses the problem of parametric constraints for positive\nsemidefiniteness of the multivariate Mat{\\'e}rn model. Much attention has been\ngiven to the bivariate case, while highly multivariate cases have been explored\nto a limited extent only. The existing conditions often imply severe\nrestrictions on the upper bounds for the collocated correlation coefficients,\nwhich makes the multivariate Mat{\\'e}rn model appealing for the case of weak\nspatial cross-dependence only. We provide a collection of validity conditions\nfor the multivariate Mat{\\'e}rn covariance model that allows for more flexible\nparameterizations than those currently available. We also prove that, in\nseveral cases, we can attain much higher upper bounds for the collocated\ncorrelation coefficients in comparison with our competitors. We conclude with a\nsimple illustration on a trivariate geochemical dataset and show that our\nenlarged parametric space allows for better fitting performance with respect to\nour competitors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:24:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Emery", "Xavier", ""], ["Porcu", "Emilio", ""], ["White", "Philip", ""]]}, {"id": "2101.04250", "submitter": "Satoshi Hayakawa", "authors": "Satoshi Hayakawa, Terry Lyons, Harald Oberhauser", "title": "Estimating the probability that a given vector is in the convex hull of\n  a random sample", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a $d$-dimensional random vector $X$, let $p_{n, X}(\\theta)$ be the\nprobability that the convex hull of $n$ independent copies of $X$ contains a\ngiven point $\\theta$. We provide several sharp inequalities regarding $p_{n,\nX}(\\theta)$ and $N_X(\\theta)$ denoting the smallest $n$ for which $p_{n,\nX}(\\theta)\\ge1/2$. As a main result, we derive the totally general inequality\n$1/2 \\le \\alpha_X(\\theta)N_X(\\theta)\\le 3d + 1$, where $\\alpha_X(\\theta)$\n(a.k.a. the Tukey depth) is the minimum probability that $X$ is in a fixed\nclosed halfspace containing the point $\\theta$. We also show several\napplications of our general results: one is a moment-based bound on\n$N_X(\\mathbb{E}[X])$, which is an important quantity in randomized approaches\nto cubature construction or measure reduction problem. Another application is\nthe determination of the canonical convex body included in a random convex\npolytope given by independent copies of $X$, where our combinatorial approach\nallows us to generalize existing results in random matrix community\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:00:21 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 11:31:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hayakawa", "Satoshi", ""], ["Lyons", "Terry", ""], ["Oberhauser", "Harald", ""]]}, {"id": "2101.04276", "submitter": "Yao Zheng", "authors": "Di Wang, Yao Zheng and Guodong Li", "title": "High-Dimensional Low-Rank Tensor Autoregressive Time Series Modeling", "comments": "61 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern technological advances have enabled an unprecedented amount of\nstructured data with complex temporal dependence, urging the need for new\nmethods to efficiently model and forecast high-dimensional tensor-valued time\nseries. This paper provides the first practical tool to accomplish this task\nvia autoregression (AR). By considering a low-rank Tucker decomposition for the\ntransition tensor, the proposed tensor autoregression can flexibly capture the\nunderlying low-dimensional tensor dynamics, providing both substantial\ndimension reduction and meaningful dynamic factor interpretation. For this\nmodel, we introduce both low-dimensional rank-constrained estimator and\nhigh-dimensional regularized estimators, and derive their asymptotic and\nnon-asymptotic properties. In particular, by leveraging the special balanced\nstructure of the AR transition tensor, a novel convex regularization approach,\nbased on the sum of nuclear norms of square matricizations, is proposed to\nefficiently encourage low-rankness of the coefficient tensor. A truncation\nmethod is further introduced to consistently select the Tucker ranks.\nSimulation experiments and real data analysis demonstrate the advantages of the\nproposed approach over various competing ones.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:15:51 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Di", ""], ["Zheng", "Yao", ""], ["Li", "Guodong", ""]]}, {"id": "2101.04351", "submitter": "Jaeyong Lee", "authors": "Kyoungjae Lee, Seongil Jo, Jaeyong Lee", "title": "The Beta-Mixture Shrinkage Prior for Sparse Covariances with Posterior\n  Minimax Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical inference for sparse covariance matrices is crucial to reveal\ndependence structure of large multivariate data sets, but lacks scalable and\ntheoretically supported Bayesian methods. In this paper, we propose\nbeta-mixture shrinkage prior, computationally more efficient than the spike and\nslab prior, for sparse covariance matrices and establish its minimax optimality\nin high-dimensional settings. The proposed prior consists of beta-mixture\nshrinkage and gamma priors for off-diagonal and diagonal entries, respectively.\nTo ensure positive definiteness of the resulting covariance matrix, we further\nrestrict the support of the prior to a subspace of positive definite matrices.\nWe obtain the posterior convergence rate of the induced posterior under the\nFrobenius norm and establish a minimax lower bound for sparse covariance\nmatrices. The class of sparse covariance matrices for the minimax lower bound\nconsidered in this paper is controlled by the number of nonzero off-diagonal\nelements and has more intuitive appeal than those appeared in the literature.\nThe obtained posterior convergence rate coincides with the minimax lower bound\nunless the true covariance matrix is extremely sparse. In the simulation study,\nwe show that the proposed method is computationally more efficient than\ncompetitors, while achieving comparable performance. Advantages of the\nshrinkage prior are demonstrated based on two real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:40:59 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Jo", "Seongil", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2101.04491", "submitter": "Ismael Castillo", "authors": "Sayantan Banerjee, Isma\\\"el Castillo and Subhashis Ghosal", "title": "Bayesian inference in high-dimensional models", "comments": "Review chapter, 42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with dimension more than the available sample size are now commonly\nused in various applications. A sensible inference is possible using a\nlower-dimensional structure. In regression problems with a large number of\npredictors, the model is often assumed to be sparse, with only a few predictors\nactive. Interdependence between a large number of variables is succinctly\ndescribed by a graphical model, where variables are represented by nodes on a\ngraph and an edge between two nodes is used to indicate their conditional\ndependence given other variables. Many procedures for making inferences in the\nhigh-dimensional setting, typically using penalty functions to induce sparsity\nin the solution obtained by minimizing a loss function, were developed.\nBayesian methods have been proposed for such problems more recently, where the\nprior takes care of the sparsity structure. These methods have the natural\nability to also automatically quantify the uncertainty of the inference through\nthe posterior distribution. Theoretical studies of Bayesian procedures in\nhigh-dimension have been carried out recently. Questions that arise are,\nwhether the posterior distribution contracts near the true value of the\nparameter at the minimax optimal rate, whether the correct lower-dimensional\nstructure is discovered with high posterior probability, and whether a credible\nregion has adequate frequentist coverage. In this paper, we review these\nproperties of Bayesian and related methods for several high-dimensional models\nsuch as many normal means problem, linear regression, generalized linear\nmodels, Gaussian and non-Gaussian graphical models. Effective computational\napproaches are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:10:53 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Banerjee", "Sayantan", ""], ["Castillo", "Isma\u00ebl", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2101.04573", "submitter": "Martial Longla", "authors": "Martial Longla, Fidel Djongreba Ndikwa, Mathias Muia Nthiani, Patrice\n  Takam Soh", "title": "Perturbations of copulas and Mixing properties", "comments": "22 pages, 3 figures, journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the impact of perturbations of copulas on the dependence\nproperties of the Markov chains they generate. We consider Markov chains\ngenerated by perturbed copulas. Results are provided for the mixing\ncoefficients $\\beta_n$, $\\psi_n$ and $\\phi_n$. Several results are provided on\nmixing for the considered perturbations. New copula functions are provided in\nconnection with perturbations of variables that induce other types of\nperturbation of copulas not considered in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:06:18 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 14:10:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Longla", "Martial", ""], ["Ndikwa", "Fidel Djongreba", ""], ["Nthiani", "Mathias Muia", ""], ["Soh", "Patrice Takam", ""]]}, {"id": "2101.04584", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Zuofeng Shang", "title": "Sharp detection boundaries on testing dense subhypergraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of testing the existence of a dense subhypergraph. The\nnull hypothesis is an Erdos-Renyi uniform random hypergraph and the alternative\nhypothesis is a uniform random hypergraph that contains a dense subhypergraph.\nWe establish sharp detection boundaries in both scenarios: (1) the edge\nprobabilities are known; (2) the edge probabilities are unknown. In both\nscenarios, sharp detectable boundaries are characterized by the appropriate\nmodel parameters. Asymptotically powerful tests are provided when the model\nparameters fall in the detectable regions. Our results indicate that the\ndetectable regions for general hypergraph models are dramatically different\nfrom their graph counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:31:47 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2101.04661", "submitter": "Aniket Biswas", "authors": "Aniket Biswas and Subrata Chakraborty", "title": "A new method for constructing continuous distributions on the unit\n  interval", "comments": "15 Pages, 10 Figures; 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A novel approach towards construction of absolutely continuous distributions\nover the unit interval is proposed. Considering two absolutely continuous\nrandom variables with positive support, this method conditions on their\nconvolution to generate a new random variable in the unit interval. This\napproach is demonstrated using some popular choices of the positive random\nvariables such as the exponential, Lindley, gamma. Some existing distributions\nlike the uniform and the beta are formulated with this method. Several new\nstructures of density functions having potential for future application in real\nlife problems are also provided. One of the new distributions having one\nparameter is considered for parameter estimation and real life modelling\napplication and shown to provide better fit than the popular one parameter\nTopp-Leone model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:40:32 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Biswas", "Aniket", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "2101.04715", "submitter": "Yun Wei", "authors": "Alfred O. Hero, Bala Rajaratnam, Yun Wei", "title": "A unified framework for correlation mining in ultra-high dimension", "comments": "84 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important problem in large scale inference is the identification of\nvariables that have large correlations or partial correlations. Recent work has\nyielded breakthroughs in the ultra-high dimensional setting when the sample\nsize $n$ is fixed and the dimension $p \\rightarrow \\infty$ ([Hero, Rajaratnam\n2011, 2012]). Despite these advances, the correlation screening framework\nsuffers from some serious practical, methodological and theoretical\ndeficiencies. For instance, theoretical safeguards for partial correlation\nscreening requires that the population covariance matrix be block diagonal.\nThis block sparsity assumption is however highly restrictive in numerous\npractical applications. As a second example, results for correlation and\npartial correlation screening framework requires the estimation of dependence\nmeasures or functionals, which can be highly prohibitive computationally. In\nthis paper, we propose a unifying approach to correlation and partial\ncorrelation mining which specifically goes beyond the block diagonal\ncorrelation structure, thus yielding a methodology that is suitable for modern\napplications. By making connections to random geometric graphs, the number of\nhighly correlated or partial correlated variables are shown to have novel\ncompound Poisson finite-sample characterizations, which hold for both the\nfinite $p$ case and when $p \\rightarrow \\infty$. The unifying framework also\ndemonstrates an important duality between correlation and partial correlation\nscreening with important theoretical and practical consequences.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:36:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hero", "Alfred O.", ""], ["Rajaratnam", "Bala", ""], ["Wei", "Yun", ""]]}, {"id": "2101.04748", "submitter": "Fabian K\\\"achele", "authors": "Oliver Grothe, Fabian K\\\"achele, Friedrich Schmid", "title": "A multivariate extension of the Lorenz curve based on copulas and a\n  related multivariate Gini coefficient", "comments": "17 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the univariate Lorenz curve and of the Gini\ncoefficient to the multivariate case, i.e., to simultaneously measure\ninequality in more than one variable. Our extensions are based on copulas and\nmeasure inequality stemming from inequality in every single variable as well as\ninequality stemming from the dependence structure of the variables. We derive\nsimple nonparametric estimators for both instruments and apply them exemplary\nto data of individual income and wealth for various countries.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:50:50 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Grothe", "Oliver", ""], ["K\u00e4chele", "Fabian", ""], ["Schmid", "Friedrich", ""]]}, {"id": "2101.04783", "submitter": "Hailin Sang", "authors": "Janet Nakarmi, Hailin Sang and Lin Ge", "title": "Variable bandwidth kernel regression estimation", "comments": "accepted by ESAIM: PS. 36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose a variable bandwidth kernel regression estimator for\n$i.i.d.$ observations in $\\mathbb{R}^2$ to improve the classical\nNadaraya-Watson estimator. The bias is improved to the order of $O(h_n^4)$\nunder the condition that the fifth order derivative of the density function and\nthe sixth order derivative of the regression function are bounded and\ncontinuous. We also establish the central limit theorems for the proposed ideal\nand true variable kernel regression estimators. The simulation study confirms\nour results and demonstrates the advantage of the variable bandwidth kernel\nmethod over the classical kernel method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:34:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nakarmi", "Janet", ""], ["Sang", "Hailin", ""], ["Ge", "Lin", ""]]}, {"id": "2101.04805", "submitter": "Albert Vexler", "authors": "Ablert Vexler, Gregory Gurevich and Li Zou", "title": "Exact Multivariate Two-Sample Density-Based Empirical Likelihood Ratio\n  Tests Applicable to Retrospective and Group Sequential Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests for equality of multivariate distributions are frequently\ndesired in research. It is commonly required that test-procedures based on\nrelatively small samples of vectors accurately control the corresponding Type I\nError (TIE) rates. Often, in the multivariate testing, extensions of\nnull-distribution-free univariate methods, e.g., Kolmogorov-Smirnov and\nCramer-von Mises type schemes, are not exact, since their null distributions\ndepend on underlying data distributions. The present paper extends the\ndensity-based empirical likelihood technique in order to nonparametrically\napproximate the most powerful test for the multivariate two-sample (MTS)\nproblem, yielding an exact finite-sample test statistic. We rigorously\nestablish and apply one-to-one-mapping between the equality of vectors\ndistributions and the equality of distributions of relevant univariate linear\nprojections. In this framework, we prove an algorithm that simplifies the use\nof projection pursuit, employing only a few of the infinitely many linear\ncombinations of observed vectors components. The displayed distribution-free\nstrategy is employed in retrospective and group sequential manners. The\nasymptotic consistency of the proposed technique is shown. Monte Carlo studies\ndemonstrate that the proposed procedures exhibit extremely high and stable\npower characteristics across a variety of settings. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:54:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Vexler", "Ablert", ""], ["Gurevich", "Gregory", ""], ["Zou", "Li", ""]]}, {"id": "2101.04919", "submitter": "Hidemasa Oda", "authors": "Hidemasa Oda and Fumiyasu Komaki", "title": "Enriched standard conjugate priors and the right invariant prior for\n  Wishart distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Bayesian predictions for Wishart distributions by using the\nKullback-Leibler divergence. We compare between the Bayesian predictive\ndistributions based on a recently introduced class of prior distributions,\ncalled the family of enriched standard conjugate priors, which includes the\nJeffreys prior, the reference prior, and the right invariant prior. We\nexplicitly calculate the risks of Bayesian predictive distributions without\nusing asymptotic expansions and clarify the dependency on the sizes of current\nand future observations. We also construct a minimax predictive distribution\nwith a constant risk and prove this predictive distribution is not admissible.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:48:41 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Oda", "Hidemasa", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "2101.04968", "submitter": "Dominic Richards", "authors": "Dominic Richards, Mike Rabbat", "title": "Learning with Gradient Descent and Weakly Convex Losses", "comments": "Updated References", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the learning performance of gradient descent when the empirical risk\nis weakly convex, namely, the smallest negative eigenvalue of the empirical\nrisk's Hessian is bounded in magnitude. By showing that this eigenvalue can\ncontrol the stability of gradient descent, generalisation error bounds are\nproven that hold under a wider range of step sizes compared to previous work.\nOut of sample guarantees are then achieved by decomposing the test error into\ngeneralisation, optimisation and approximation errors, each of which can be\nbounded and traded off with respect to algorithmic parameters, sample size and\nmagnitude of this eigenvalue. In the case of a two layer neural network, we\ndemonstrate that the empirical risk can satisfy a notion of local weak\nconvexity, specifically, the Hessian's smallest eigenvalue during training can\nbe controlled by the normalisation of the layers, i.e., network scaling. This\nallows test error guarantees to then be achieved when the population risk\nminimiser satisfies a complexity assumption. By trading off the network\ncomplexity and scaling, insights are gained into the implicit bias of neural\nnetwork scaling, which are further supported by experimental findings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:58:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:43:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Richards", "Dominic", ""], ["Rabbat", "Mike", ""]]}, {"id": "2101.05099", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is and Beno\\^it Henry", "title": "Maximum likelihood estimation for spinal-structured trees", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate some aspects of the problem of the estimation of birth\ndistributions (BD) in multi-type Galton-Watson (MGW) trees with unobserved\ntypes. More precisely, we consider a two-type MGW called spinal-structured\ntrees. This kind of tree is characterized by a spine of special individuals\nwhose BD $\\nu$ is different from the other individuals in the tree (called\nnormal whose BD is denoted $\\mu$). In this work, we show that even in such a\nvery structured two-types population, our ability to distinguish the two types\nand estimate $\\mu$ and $\\nu$ is constrained by a trade off between the\ngrowth-rate of the population and the similarity of $\\mu$ and $\\nu$. Indeed, if\nthe growth-rate is too large, large deviations events are likely to be observed\nin the sampling of the normal individuals preventing us to distinguish them\nfrom special ones. Roughly speaking, our approach succeed if\n$r<\\mathfrak{D}(\\mu,\\nu)$ where $r$ is the exponential growth-rate of the\npopulation and $\\mathfrak{D}$ is a divergence measuring the dissimilarity\nbetween $\\mu$ and $\\nu$.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:33:58 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Aza\u00efs", "Romain", ""], ["Henry", "Beno\u00eet", ""]]}, {"id": "2101.05119", "submitter": "Stefano Vigogna", "authors": "Wenjing Liao, Mauro Maggioni and Stefano Vigogna", "title": "Multiscale regression on unknown manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regression problem of estimating functions on $\\mathbb{R}^D$\nbut supported on a $d$-dimensional manifold $ \\mathcal{M} \\subset \\mathbb{R}^D\n$ with $ d \\ll D $. Drawing ideas from multi-resolution analysis and nonlinear\napproximation, we construct low-dimensional coordinates on $\\mathcal{M}$ at\nmultiple scales, and perform multiscale regression by local polynomial fitting.\nWe propose a data-driven wavelet thresholding scheme that automatically adapts\nto the unknown regularity of the function, allowing for efficient estimation of\nfunctions exhibiting nonuniform regularity at different locations and scales.\nWe analyze the generalization error of our method by proving finite sample\nbounds in high probability on rich classes of priors. Our estimator attains\noptimal learning rates (up to logarithmic factors) as if the function was\ndefined on a known Euclidean domain of dimension $d$, instead of an unknown\nmanifold embedded in $\\mathbb{R}^D$. The implemented algorithm has quasilinear\ncomplexity in the sample size, with constants linear in $D$ and exponential in\n$d$. Our work therefore establishes a new framework for regression on\nlow-dimensional sets embedded in high dimensions, with fast implementation and\nstrong theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:14:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liao", "Wenjing", ""], ["Maggioni", "Mauro", ""], ["Vigogna", "Stefano", ""]]}, {"id": "2101.05197", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Vinayak A. Rao, Harsha Honnappa", "title": "PAC-Bayes Bounds on Variational Tempered Posteriors for Markov Models", "comments": "14 pages main, 24 pages appendix and citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets displaying temporal dependencies abound in science and engineering\napplications, with Markov models representing a simplified and popular view of\nthe temporal dependence structure. In this paper, we consider Bayesian settings\nthat place prior distributions over the parameters of the transition kernel of\na Markov model, and seeks to characterize the resulting, typically intractable,\nposterior distributions. We present a PAC-Bayesian analysis of variational\nBayes (VB) approximations to tempered Bayesian posterior distributions,\nbounding the model risk of the VB approximations. Tempered posteriors are known\nto be robust to model misspecification, and their variational approximations do\nnot suffer the usual problems of over confident approximations. Our results tie\nthe risk bounds to the mixing and ergodic properties of the Markov data\ngenerating model. We illustrate the PAC-Bayes bounds through a number of\nexample Markov models, and also consider the situation where the Markov model\nis misspecified.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:15:01 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Banerjee", "Imon", ""], ["Rao", "Vinayak A.", ""], ["Honnappa", "Harsha", ""]]}, {"id": "2101.05380", "submitter": "Fran\\c{c}ois-Xavier Vialard", "authors": "Adrien Vacher and Boris Muzellec and Alessandro Rudi and Francis Bach\n  and Francois-Xavier Vialard", "title": "A Dimension-free Computational Upper-bound for Smooth Optimal Transport\n  Estimation", "comments": "30 pages; Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that plug-in statistical estimation of optimal transport\nsuffers from the curse of dimensionality. Despite recent efforts to improve the\nrate of estimation with the smoothness of the problem, the computational\ncomplexities of these recently proposed methods still degrade exponentially\nwith the dimension. In this paper, thanks to an infinite-dimensional\nsum-of-squares representation, we derive a statistical estimator of smooth\noptimal transport which achieves a precision $\\varepsilon$ from\n$\\tilde{O}(\\varepsilon^{-2})$ independent and identically distributed samples\nfrom the distributions, for a computational cost of\n$\\tilde{O}(\\varepsilon^{-4})$ when the smoothness increases, hence yielding\ndimension-free statistical \\emph{and} computational rates, with potentially\nexponentially dimension-dependent constants.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 22:56:46 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 17:54:20 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 18:35:05 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Vacher", "Adrien", ""], ["Muzellec", "Boris", ""], ["Rudi", "Alessandro", ""], ["Bach", "Francis", ""], ["Vialard", "Francois-Xavier", ""]]}, {"id": "2101.05402", "submitter": "Anderson Ye Zhang", "authors": "Xin Chen, Anderson Y. Zhang", "title": "Optimal Clustering in Anisotropic Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the clustering task under anisotropic Gaussian Mixture Models where\nthe covariance matrices from different clusters are unknown and are not\nnecessarily the identical matrix. We characterize the dependence of\nsignal-to-noise ratios on the cluster centers and covariance matrices and\nobtain the minimax lower bound for the clustering problem. In addition, we\npropose a computationally feasible procedure and prove it achieves the optimal\nrate within a few iterations. The proposed procedure is a hard EM type\nalgorithm, and it can also be seen as a variant of the Lloyd's algorithm that\nis adjusted to the anisotropic covariance matrices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:31:52 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:24:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Xin", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.05477", "submitter": "Yi Yu", "authors": "Yi Yu, Oscar Hernan Madrid Padilla, Daren Wang and Alessandro Rinaldo", "title": "Optimal network online change point localisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online network change point detection. In this\nsetting, a collection of independent Bernoulli networks is collected\nsequentially, and the underlying distributions change when a change point\noccurs. The goal is to detect the change point as quickly as possible, if it\nexists, subject to a constraint on the number or probability of false alarms.\nIn this paper, on the detection delay, we establish a minimax lower bound and\ntwo upper bounds based on NP-hard algorithms and polynomial-time algorithms,\ni.e., \\[ \\mbox{detection delay} \\begin{cases} \\gtrsim \\log(1/\\alpha)\n\\frac{\\max\\{r^2/n, \\, 1\\}}{\\kappa_0^2 n \\rho},\\\\ \\lesssim \\log(\\Delta/\\alpha)\n\\frac{\\max\\{r^2/n, \\, \\log(r)\\}}{\\kappa_0^2 n \\rho}, & \\mbox{with NP-hard\nalgorithms},\\\\ \\lesssim \\log(\\Delta/\\alpha) \\frac{r}{\\kappa_0^2 n \\rho}, &\n\\mbox{with polynomial-time algorithms}, \\end{cases} \\] where $\\kappa_0, n,\n\\rho, r$ and $\\alpha$ are the normalised jump size, network size, entrywise\nsparsity, rank sparsity and the overall Type-I error upper bound. All the model\nparameters are allowed to vary as $\\Delta$, the location of the change point,\ndiverges. The polynomial-time algorithms are novel procedures that we propose\nin this paper, designed for quick detection under two different forms of Type-I\nerror control. The first is based on controlling the overall probability of a\nfalse alarm when there are no change points, and the second is based on\nspecifying a lower bound on the expected time of the first false alarm.\nExtensive experiments show that, under different scenarios and the\naforementioned forms of Type-I error control, our proposed approaches\noutperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:24:39 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yu", "Yi", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2101.05487", "submitter": "Sebastien da Veiga", "authors": "S\\'ebastien da Veiga", "title": "Kernel-based ANOVA decomposition and Shapley effects -- Application to\n  global sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis is the main quantitative technique for\nidentifying the most influential input variables in a numerical simulation\nmodel. In particular when the inputs are independent, Sobol' sensitivity\nindices attribute a portion of the output of interest variance to each input\nand all possible interactions in the model, thanks to a functional ANOVA\ndecomposition. On the other hand, moment-independent sensitivity indices focus\non the impact of input variables on the whole output distribution instead of\nthe variance only, thus providing complementary insight on the inputs / output\nrelationship. Unfortunately they do not enjoy the nice decomposition property\nof Sobol' indices and are consequently harder to analyze. In this paper, we\nintroduce two moment-independent indices based on kernel-embeddings of\nprobability distributions and show that the RKHS framework used for their\ndefinition makes it possible to exhibit a kernel-based ANOVA decomposition.\nThis is the first time such a desirable property is proved for sensitivity\nindices apart from Sobol' ones. When the inputs are dependent, we also use\nthese new sensitivity indices as building blocks to design kernel-embedding\nShapley effects which generalize the traditional variance-based ones used in\nsensitivity analysis. Several estimation procedures are discussed and\nillustrated on test cases with various output types such as categorical\nvariables and probability distributions. All these examples show their\npotential for enhancing traditional sensitivity analysis with a kernel point of\nview.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:47:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["da Veiga", "S\u00e9bastien", ""]]}, {"id": "2101.05654", "submitter": "Kirsten Schorning", "authors": "Kirsten Schorning and Holger Dette", "title": "Optimal designs for comparing regression curves -- dependence within and\n  between groups", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of designing experiments for the comparison of two\nregression curves describing the relation between a predictor and a response in\ntwo groups, where the data between and within the group may be dependent. In\norder to derive efficient designs we use results from stochastic analysis to\nidentify the best linear unbiased estimator (BLUE) in a corresponding\ncontinuous time model. It is demonstrated that in general simultaneous\nestimation using the data from both groups yields more precise results than\nestimation of the parameters separately in the two groups. Using the BLUE from\nsimultaneous estimation, we then construct an efficient linear estimator for\nfinite sample size by minimizing the mean squared error between the optimal\nsolution in the continuous time model and its discrete approximation with\nrespect to the weights (of the linear estimator). Finally, the optimal design\npoints are determined by minimizing the maximal width of a simultaneous\nconfidence band for the difference of the two regression functions. The\nadvantages of the new approach are illustrated by means of a simulation study,\nwhere it is shown that the use of the optimal designs yields substantially\nnarrower confidence bands than the application of uniform designs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:08:29 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Schorning", "Kirsten", ""], ["Dette", "Holger", ""]]}, {"id": "2101.05728", "submitter": "Gautier Appert", "authors": "Gautier Appert and Olivier Catoni", "title": "New bounds for $k$-means and information $k$-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we derive a new dimension-free non-asymptotic upper bound for\nthe quadratic $k$-means excess risk related to the quantization of an i.i.d\nsample in a separable Hilbert space. We improve the bound of order $\\mathcal{O}\n\\bigl( k / \\sqrt{n} \\bigr)$ of Biau, Devroye and Lugosi, recovering the rate\n$\\sqrt{k/n}$ that has already been proved by Fefferman, Mitter, and Narayanan\nand by Klochkov, Kroshnin and Zhivotovskiy but with worse log factors and\nconstants. More precisely, we bound the mean excess risk of an empirical\nminimizer by the explicit upper bound $16 B^2 \\log(n/k) \\sqrt{k \\log(k) / n}$,\nin the bounded case when $\\mathbb{P}( \\lVert X \\rVert \\leq B) = 1$. This is\nessentially optimal up to logarithmic factors since a lower bound of order\n$\\mathcal{O} \\bigl( \\sqrt{k^{1 - 4/d}/n} \\bigr)$ is known in dimension $d$. Our\ntechnique of proof is based on the linearization of the $k$-means criterion\nthrough a kernel trick and on PAC-Bayesian inequalities. To get a $1 /\n\\sqrt{n}$ speed, we introduce a new PAC-Bayesian chaining method replacing the\nconcept of $\\delta$-net with the perturbation of the parameter by an infinite\ndimensional Gaussian process.\n  In the meantime, we embed the usual $k$-means criterion into a broader family\nbuilt upon the Kullback divergence and its underlying properties. This results\nin a new algorithm that we named information $k$-means, well suited to the\nclustering of bags of words. Based on considerations from information theory,\nwe also introduce a new bounded $k$-means criterion that uses a scale parameter\nbut satisfies a generalization bound that does not require any boundedness or\neven integrability conditions on the sample. We describe the counterpart of\nLloyd's algorithm and prove generalization bounds for these new $k$-means\ncriteria.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 17:12:05 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 17:50:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Appert", "Gautier", ""], ["Catoni", "Olivier", ""]]}, {"id": "2101.05780", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny, Lucas Girard and Yannick Guyonvarch", "title": "Explicit non-asymptotic bounds for the distance to the first-order\n  Edgeworth expansion", "comments": "41 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study bounds on the uniform distance between the\ncumulative distribution function of a standardized sum of independent centered\nrandom variables with moments of order four and its first-order Edgeworth\nexpansion. Existing bounds are sharpened in two frameworks: when the variables\nare independent but not identically distributed and in the case of independent\nand identically distributed random variables. Improvements of these bounds are\nderived if the third moment of the distribution is zero. We also provide\nadapted versions of these bounds under additional regularity constraints on the\ntail behavior of the characteristic function. We finally present an application\nof our results to the lack of validity of one-sided tests based on the normal\napproximation of the mean for a fixed sample size.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:33:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Derumigny", "Alexis", ""], ["Girard", "Lucas", ""], ["Guyonvarch", "Yannick", ""]]}, {"id": "2101.05939", "submitter": "Nicholas Cavenagh", "authors": "Fahim Rahim and Nicholas Cavenagh", "title": "Row-column factorial designs with multiple levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An {\\em $m\\times n$ row-column factorial design} is an arrangement of the\nelements of a factorial design into a rectangular array. Such an array is used\nin experimental design, where the rows and columns can act as blocking factors.\nIf for each row/column and vector position, each element has the same\nregularity, then all main effects can be estimated without confounding by the\nrow and column blocking factors. Formally, for any integer $q$, let\n$[q]=\\{0,1,\\dots ,q-1\\}$. The $q^k$ (full) factorial design with replication\n$\\alpha$ is the multi-set consisting of $\\alpha$ occurrences of each element of\n$[q]^k$; we denote this by $\\alpha\\times [q]^k$. A {\\em regular $m\\times n$\nrow-column factorial design} is an arrangement of the the elements of $\\alpha\n\\times [q]^k$ into an $m\\times n$ array (which we say is of {\\em type}\n$I_k(m,n;q)$) such that for each row (column) and fixed vector position $i\\in\n[q]$, each element of $[q]$ occurs $n/q$ times (respectively, $m/q$ times). Let\n$m\\leq n$. We show that an array of type $I_k(m,n;q)$ exists if and only if (a)\n$q|m$ and $q|n$; (b) $q^k|mn$; (c) $(k,q,m,n)\\neq (2,6,6,6)$ and (d) if\n$(k,q,m)=(2,2,2)$ then $4$ divides $n$. This extends the work of Godolphin\n(2019), who showed the above is true for the case $q=2$ when $m$ and $n$ are\npowers of $2$. In the case $k=2$, the above implies necessary and sufficient\nconditions for the existence of a pair of mutually orthogonal frequency\nrectangles (or $F$-rectangles) whenever each symbol occurs the same number of\ntimes in a given row or column.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 02:21:39 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Rahim", "Fahim", ""], ["Cavenagh", "Nicholas", ""]]}, {"id": "2101.06061", "submitter": "Bogdan Georgiev", "authors": "Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee", "title": "Heating up decision boundaries: isocapacitory saturation, adversarial\n  scenarios and generalization bounds", "comments": "Accepted as conference paper at ICLR 2021. 36 pages, 16 figures,\n  comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.MG math.PR math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present work we study classifiers' decision boundaries via Brownian\nmotion processes in ambient data space and associated probabilistic techniques.\nIntuitively, our ideas correspond to placing a heat source at the decision\nboundary and observing how effectively the sample points warm up. We are\nlargely motivated by the search for a soft measure that sheds further light on\nthe decision boundary's geometry. En route, we bridge aspects of potential\ntheory and geometric analysis (Mazya, 2011, Grigoryan-Saloff-Coste, 2002) with\nactive fields of ML research such as adversarial examples and generalization\nbounds. First, we focus on the geometric behavior of decision boundaries in the\nlight of adversarial attack/defense mechanisms. Experimentally, we observe a\ncertain capacitory trend over different adversarial defense strategies:\ndecision boundaries locally become flatter as measured by isoperimetric\ninequalities (Ford et al, 2019); however, our more sensitive heat-diffusion\nmetrics extend this analysis and further reveal that some non-trivial geometry\ninvisible to plain distance-based methods is still preserved. Intuitively, we\nprovide evidence that the decision boundaries nevertheless retain many\npersistent \"wiggly and fuzzy\" regions on a finer scale. Second, we show how\nBrownian hitting probabilities translate to soft generalization bounds which\nare in turn connected to compression and noise stability (Arora et al, 2018),\nand these bounds are significantly stronger if the decision boundary has\ncontrolled geometric features.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:15:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Georgiev", "Bogdan", ""], ["Franken", "Lukas", ""], ["Mukherjee", "Mayukh", ""]]}, {"id": "2101.06130", "submitter": "Jack Noonan", "authors": "Jack Noonan and Anatoly Zhigljavsky", "title": "Random and quasi-random designs in group testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For large classes of group testing problems, we derive lower bounds for the\nprobability that all significant factors are uniquely identified using\nspecially constructed random designs. These bounds allow us to optimize\nparameters of the randomization schemes. We also suggest and numerically\njustify a procedure of construction of designs with better separability\nproperties than pure random designs. We illustrate theoretical consideration\nwith large simulation-based study. This study indicates, in particular, that in\nthe case of the common binary group testing, the suggested families of designs\nhave better separability than the popular designs constructed from the disjunct\nmatrices.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:21:36 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "2101.06290", "submitter": "Zeyi Wang", "authors": "Mark van der Laan, Zeyi Wang, and Lars van der Laan", "title": "Higher Order Targeted Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Asymptotic efficiency of targeted maximum likelihood estimators (TMLE) of\ntarget features of the data distribution relies on a a second order remainder\nbeing asymptotically negligible. In previous work we proposed a nonparametric\nMLE termed Highly Adaptive Lasso (HAL) which parametrizes the relevant\nfunctional of the data distribution in terms of a multivariate real valued\ncadlag function that is assumed to have finite variation norm. We showed that\nthe HAL-MLE converges in Kullback-Leibler dissimilarity at a rate n-1/3 up till\nlogn factors. Therefore, by using HAL as initial density estimator in the TMLE,\nthe resulting HAL-TMLE is an asymptotically efficient estimator only assuming\nthat the relevant nuisance functions of the data density are cadlag and have\nfinite variation norm. However, in finite samples, the second order remainder\ncan dominate the sampling distribution so that inference based on asymptotic\nnormality would be anti-conservative.\n  In this article we propose a new higher order TMLE, generalizing the regular\nfirst order TMLE. We prove that it satisfies an exact linear expansion, in\nterms of efficient influence functions of sequentially defined higher order\nfluctuations of the target parameter, with a remainder that is a k+1th order\nremainder. As a consequence, this k-th order TMLE allows statistical inference\nonly relying on the k+1th order remainder being negligible. We also provide a\nrationale for the higher order TMLE that it will be superior to the first order\nTMLE by (iteratively) locally minimizing the exact finite sample remainder of\nthe first order TMLE. The second order TMLE is demonstrated for nonparametric\nestimation of the integrated squared density and for the treatment specific\nmean outcome. We also provide an initial simulation study for the second order\nTMLE of the treatment specific mean confirming the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 20:13:00 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:08:32 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 20:14:10 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["van der Laan", "Mark", ""], ["Wang", "Zeyi", ""], ["van der Laan", "Lars", ""]]}, {"id": "2101.06309", "submitter": "Adel Javanmard", "authors": "Mohammad Mehrabi, Adel Javanmard, Ryan A. Rossi, Anup Rao and Tung Mai", "title": "Fundamental Tradeoffs in Distributionally Adversarial Training", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is among the most effective techniques to improve the\nrobustness of models against adversarial perturbations. However, the full\neffect of this approach on models is not well understood. For example, while\nadversarial training can reduce the adversarial risk (prediction error against\nan adversary), it sometimes increase standard risk (generalization error when\nthere is no adversary). Even more, such behavior is impacted by various\nelements of the learning problem, including the size and quality of training\ndata, specific forms of adversarial perturbations in the input, model\noverparameterization, and adversary's power, among others. In this paper, we\nfocus on \\emph{distribution perturbing} adversary framework wherein the\nadversary can change the test distribution within a neighborhood of the\ntraining data distribution. The neighborhood is defined via Wasserstein\ndistance between distributions and the radius of the neighborhood is a measure\nof adversary's manipulative power. We study the tradeoff between standard risk\nand adversarial risk and derive the Pareto-optimal tradeoff, achievable over\nspecific classes of models, in the infinite data limit with features dimension\nkept fixed. We consider three learning settings: 1) Regression with the class\nof linear models; 2) Binary classification under the Gaussian mixtures data\nmodel, with the class of linear classifiers; 3) Regression with the class of\nrandom features model (which can be equivalently represented as two-layer\nneural network with random first-layer weights). We show that a tradeoff\nbetween standard and adversarial risk is manifested in all three settings. We\nfurther characterize the Pareto-optimal tradeoff curves and discuss how a\nvariety of factors, such as features correlation, adversary's power or the\nwidth of two-layer neural network would affect this tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:59:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mehrabi", "Mohammad", ""], ["Javanmard", "Adel", ""], ["Rossi", "Ryan A.", ""], ["Rao", "Anup", ""], ["Mai", "Tung", ""]]}, {"id": "2101.06366", "submitter": "Andriy Olenko", "authors": "Nour Al Hayek, Illia Donhauzer, Rita Giuliano, Andriy Olenko, Andrei\n  Volodin", "title": "Asymptotics of running maxima for $\\varphi$-subgaussian random double\n  arrays", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article studies the running maxima $Y_{m,j}=\\max_{1 \\le k \\le m, 1 \\le n\n\\le j} X_{k,n} - a_{m,j}$ where $\\{X_{k,n}, k \\ge 1, n \\ge 1\\}$ is a double\narray of $\\varphi$-subgaussian random variables and $\\{a_{m,j}, m\\ge 1, j\\ge\n1\\}$ is a double array of constants. Asymptotics of the maxima of the double\narrays of positive and negative parts of $\\{Y_{m,j}, m \\ge 1, j \\ge 1\\}$ are\nstudied, when $\\{X_{k,n}, k \\ge 1, n \\ge 1\\}$ have suitable \"exponential-type\"\ntail distributions. The main results are specified for various important\nparticular scenarios and classes of $\\varphi$-subgaussian random variables.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 04:32:45 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hayek", "Nour Al", ""], ["Donhauzer", "Illia", ""], ["Giuliano", "Rita", ""], ["Olenko", "Andriy", ""], ["Volodin", "Andrei", ""]]}, {"id": "2101.06531", "submitter": "Sheng Jiang", "authors": "Sheng Jiang, Surya Tokdar", "title": "Consistent Bayesian Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic Block Models (SBMs) are a fundamental tool for community detection\nin network analysis. But little theoretical work exists on the statistical\nperformance of Bayesian SBMs, especially when the community count is unknown.\nThis paper studies a special class of SBMs whose community-wise connectivity\nprobability matrix is diagonally dominant, i.e., members of the same community\nare more likely to connect with one another than with members from other\ncommunities. The diagonal dominance constraint is embedded within an otherwise\nweak prior, and, under mild regularity conditions, the resulting posterior\ndistribution is shown to concentrate on the true community count and membership\nallocation as the network size grows to infinity. A reversible-jump Markov\nChain Monte Carlo posterior computation strategy is developed by adapting the\nallocation sampler of Mcdaid et al (2013). Finite sample properties are\nexamined via simulation studies in which the proposed method offers competitive\nestimation accuracy relative to existing methods under a variety of challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:08:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jiang", "Sheng", ""], ["Tokdar", "Surya", ""]]}, {"id": "2101.06638", "submitter": "Cecilia Dao", "authors": "Cecilia Dao, Jiming Jiang, Debashis Paul, Hongyu Zhao", "title": "Variance Estimation and Confidence Intervals from High-dimensional\n  Genome-wide Association Studies Through Misspecified Mixed Model Analysis", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variance estimation and associated confidence intervals for\nparameters characterizing genetic effects from genome-wide association studies\n(GWAS) misspecified mixed model analysis. Previous studies have shown that, in\nspite of the model misspecification, certain quantities of genetic interests\nare estimable, and consistent estimators of these quantities can be obtained\nusing the restricted maximum likelihood (REML) method under a misspecified\nlinear mixed model. However, the asymptotic variance of such a REML estimator\nis complicated and not ready to be implemented for practical use. In this\npaper, we develop practical and computationally convenient methods for\nestimating such asymptotic variances and constructing the associated confidence\nintervals. Performance of the proposed methods is evaluated empirically based\non Monte-Carlo simulations and real-data application.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:19:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dao", "Cecilia", ""], ["Jiang", "Jiming", ""], ["Paul", "Debashis", ""], ["Zhao", "Hongyu", ""]]}, {"id": "2101.06854", "submitter": "Xinyu Song", "authors": "Xinyu Song and Yazhen Wang and Shang Wu and Donggyu Kim", "title": "Statistical Analysis of Quantum Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum computers use quantum resources to carry out computational tasks and\nmay outperform classical computers in solving certain computational problems.\nSpecial-purpose quantum computers such as quantum annealers employ quantum\nadiabatic theorem to solve combinatorial optimization problems. In this paper,\nwe compare classical annealings such as simulated annealing and quantum\nannealings that are done by the D-Wave machines both theoretically and\nnumerically. We show that if the classical and quantum annealing are\ncharacterized by equivalent Ising models, then solving an optimization problem,\ni.e., finding the minimal energy of each Ising model, by the two annealing\nprocedures, are mathematically identical. For quantum annealing, we also derive\nthe probability lower-bound on successfully solving an optimization problem by\nmeasuring the system at the end of the annealing procedure. Moreover, we\npresent the Markov chain Monte Carlo (MCMC) method to realize quantum annealing\nby classical computers and investigate its statistical properties. In the\nnumerical section, we discuss the discrepancies between the MCMC based\nannealing approaches and the quantum annealing approach in solving optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:57:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Song", "Xinyu", ""], ["Wang", "Yazhen", ""], ["Wu", "Shang", ""], ["Kim", "Donggyu", ""]]}, {"id": "2101.06936", "submitter": "Adrian Riekert", "authors": "Adrian Riekert", "title": "Wasserstein Convergence Rate for Empirical Measures of Markov Chains", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a Markov chain on $\\mathbb{R}^d$ with invariant measure $\\mu$. We\nare interested in the rate of convergence of the empirical measures towards the\ninvariant measure with respect to the $1$-Wasserstein distance. The main result\nof this article is a new upper bound for the expected Wasserstein distance,\nwhich is proved by combining the Kantorovich dual formula with a Fourier\nexpansion. In addition, we show how concentration inequalities around the mean\ncan be obtained.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 08:50:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Riekert", "Adrian", ""]]}, {"id": "2101.06950", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Peter B\\\"uhlmann", "title": "Perturbations and Causality in Gaussian Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference is a challenging problem with observational data alone. The\ntask becomes easier when having access to data from perturbing the underlying\nsystem, even when happening in a non-randomized way: this is the setting we\nconsider, encompassing also latent confounding variables. To identify causal\nrelations among a collections of covariates and a response variable, existing\nprocedures rely on at least one of the following assumptions: i) the response\nvariable remains unperturbed, ii) the latent variables remain unperturbed, and\niii) the latent effects are dense. In this paper, we examine a perturbation\nmodel for interventional data, which can be viewed as a mixed-effects linear\nstructural causal model, over a collection of Gaussian variables that does not\nsatisfy any of these conditions. We propose a maximum-likelihood estimator --\ndubbed DirectLikelihood -- that exploits system-wide invariances to uniquely\nidentify the population causal structure from unspecific perturbation data, and\nour results carry over to linear structural causal models without requiring\nGaussianity. We illustrate the utility of our framework on synthetic data as\nwell as real data involving California reservoirs and protein expressions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:24:08 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 15:25:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Taeb", "Armeen", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.07108", "submitter": "Rajen Shah", "authors": "Anton Rask Lundborg, Rajen D. Shah, Jonas Peters", "title": "Conditional Independence Testing in Hilbert Spaces with Applications to\n  Functional Data Analysis", "comments": "69 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing the null hypothesis that X and Y are\nconditionally independent given Z, where each of X, Y and Z may be functional\nrandom variables. This generalises, for example, testing the significance of X\nin a scalar-on-function linear regression model of response Y on functional\nregressors X and Z. We show however that even in the idealised setting where\nadditionally (X, Y, Z) have a non-singular Gaussian distribution, the power of\nany test cannot exceed its size. Further modelling assumptions are needed to\nrestrict the null and we argue that a convenient way of specifying these is\nbased on choosing methods for regressing each of X and Y on Z. We thus propose\nas a test statistic, the Hilbert-Schmidt norm of the outer product of the\nresulting residuals, and prove that type I error control is guaranteed when the\nin-sample prediction errors are sufficiently small. We show this requirement is\nmet by ridge regression in functional linear model settings without requiring\nany eigen-spacing conditions or lower bounds on the eigenvalues of the\ncovariance of the functional regressor. We apply our test in constructing\nconfidence intervals for truncation points in truncated functional linear\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:10:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lundborg", "Anton Rask", ""], ["Shah", "Rajen D.", ""], ["Peters", "Jonas", ""]]}, {"id": "2101.07380", "submitter": "Aur\\'elien Bibaut", "authors": "Aurelien Bibaut, Maya Petersen, Nikos Vlassis, Maria Dimakopoulou,\n  Mark van der Laan", "title": "Sequential causal inference in a single world of connected units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive designs for a trial involving N individuals that we\nfollow along T time steps. We allow for the variables of one individual to\ndepend on its past and on the past of other individuals. Our goal is to learn a\nmean outcome, averaged across the N individuals, that we would observe, if we\nstarted from some given initial state, and we carried out a given sequence of\ncounterfactual interventions for $\\tau$ time steps.\n  We show how to identify a statistical parameter that equals this mean\ncounterfactual outcome, and how to perform inference for this parameter, while\nadaptively learning an oracle design defined as a parameter of the true data\ngenerating distribution. Oracle designs of interest include the design that\nmaximizes the efficiency for a statistical parameter of interest, or designs\nthat mix the optimal treatment rule with a certain exploration distribution. We\nalso show how to design adaptive stopping rules for sequential hypothesis\ntesting.\n  This setting presents unique technical challenges. Unlike in usual\nstatistical settings where the data consists of several independent\nobservations, here, due to network and temporal dependence, the data reduces to\none single observation with dependent components. In particular, this precludes\nthe use of sample splitting techniques. We therefore had to develop a new\nequicontinuity result and guarantees for estimators fitted on dependent data.\n  We were motivated to work on this problem by the following two questions. (1)\nIn the context of a sequential adaptive trial with K treatment arms, how to\ndesign a procedure to identify in as few rounds as possible the treatment arm\nwith best final outcome? (2) In the context of sequential randomized disease\ntesting at the scale of a city, how to estimate and infer the value of an\noptimal testing and isolation strategy?\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:56:57 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bibaut", "Aurelien", ""], ["Petersen", "Maya", ""], ["Vlassis", "Nikos", ""], ["Dimakopoulou", "Maria", ""], ["van der Laan", "Mark", ""]]}, {"id": "2101.07561", "submitter": "Paul Novello", "authors": "Paul Novello (CEA, X, Inria), Ga\\\"el Po\\\"ette (CEA), David Lugato\n  (CEA), Pietro Congedo (X, Inria)", "title": "Variance Based Samples Weighting for Supervised Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of supervised learning of a function by a Neural Network (NN),\nwe claim and empirically justify that a NN yields better results when the\ndistribution of the data set focuses on regions where the function to learn is\nsteeper. We first traduce this assumption in a mathematically workable way\nusing Taylor expansion. Then, theoretical derivations allow to construct a\nmethodology that we call Variance Based Samples Weighting (VBSW). VBSW uses\nlocal variance of the labels to weight the training points. This methodology is\ngeneral, scalable, cost effective, and significantly increases the performances\nof a large class of NNs for various classification and regression tasks on\nimage, text and multivariate data. We highlight its benefits with experiments\ninvolving NNs from shallow linear NN to Resnet or Bert.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:08:40 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:50:28 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Novello", "Paul", "", "CEA, X, Inria"], ["Po\u00ebtte", "Ga\u00ebl", "", "CEA"], ["Lugato", "David", "", "CEA"], ["Congedo", "Pietro", "", "X, Inria"]]}, {"id": "2101.07587", "submitter": "Svante Janson", "authors": "Svante Janson and Sofia Olhede", "title": "Can smooth graphons in several dimensions be represented by smooth\n  graphons on $[0,1]$?", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphon that is defined on $[0,1]^d$ and is H\\\"older$(\\alpha)$ continuous\nfor some $d\\ge2$ and $\\alpha\\in(0,1]$ can be represented by a graphon on\n$[0,1]$ that is H\\\"older$(\\alpha/d)$ continuous. We give examples that show\nthat this reduction in smoothness to $\\alpha/d$ is the best possible, for any\n$d$ and $\\alpha$; for $\\alpha=1$, the example is a dot product graphon and\nshows that the reduction is the best possible even for graphons that are\npolynomials.\n  A motivation for studying the smoothness of graphon functions is that this\nrepresents a key assumption in non-parametric statistical network analysis. Our\nexamples show that making a smoothness assumption in a particular dimension is\nnot equivalent to making it in any other latent dimension.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:17:01 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Janson", "Svante", ""], ["Olhede", "Sofia", ""]]}, {"id": "2101.07607", "submitter": "Pierpaolo De Blasi", "authors": "Pierpaolo De Blasi, Rams\\'es H. Mena, Igor Pr\\\"unster", "title": "Asymptotic behavior of the number of distinct values in a sample from\n  the geometric stick-breaking process", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discrete random probability measures are a key ingredient of Bayesian\nnonparametric inferential procedures. A sample generates ties with positive\nprobability and a fundamental object of both theoretical and applied interest\nis the corresponding random number of distinct values. The growth rate can be\ndetermined from the rate of decay of the small frequencies implying that, when\nthe decreasingly ordered frequencies admit a tractable form, the asymptotics of\nthe number of distinct values can be conveniently assessed. We focus on the\ngeometric stick-breaking process and we investigate the effect of the choice of\nthe distribution for the success probability on the asymptotic behavior of the\nnumber of distinct values. We show that a whole range of logarithmic behaviors\nare obtained by appropriately tuning the prior. We also derive a two-term\nexpansion and illustrate its use in a comparison with a larger family of\ndiscrete random probability measures having an additional parameter given by\nthe scale of the negative binomial distribution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:09:36 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["De Blasi", "Pierpaolo", ""], ["Mena", "Rams\u00e9s H.", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "2101.07693", "submitter": "Patrizia Semeraro", "authors": "Roberto Fontana and Patrizia Semeraro", "title": "Exchangeable Bernoulli distributions: high dimensional simulation,\n  estimate and testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the class of exchangeable Bernoulli distributions building on\ntheir geometrical structure. Exchangeable Bernoulli probability mass functions\nare points in a convex polytope and we have found analytical expressions for\ntheir extremal generators. The geometrical structure turns out to be crucial to\nsimulate high dimensional and negatively correlated binary data. Furthermore,\nfor a wide class of statistical indices and measures of a probability mass\nfunction we are able to find not only their sharp bounds in the class, but also\ntheir distribution across the class. Estimate and testing are also addressed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:41:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fontana", "Roberto", ""], ["Semeraro", "Patrizia", ""]]}, {"id": "2101.07781", "submitter": "Cong Ma", "authors": "Cong Ma, Banghua Zhu, Jiantao Jiao, Martin J. Wainwright", "title": "Minimax Off-Policy Evaluation for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation in the multi-armed bandit model\nwith bounded rewards, and develop minimax rate-optimal procedures under three\nsettings. First, when the behavior policy is known, we show that the Switch\nestimator, a method that alternates between the plug-in and importance sampling\nestimators, is minimax rate-optimal for all sample sizes. Second, when the\nbehavior policy is unknown, we analyze performance in terms of the competitive\nratio, thereby revealing a fundamental gap between the settings of known and\nunknown behavior policies. When the behavior policy is unknown, any estimator\nmust have mean-squared error larger -- relative to the oracle estimator\nequipped with the knowledge of the behavior policy -- by a multiplicative\nfactor proportional to the support size of the target policy. Moreover, we\ndemonstrate that the plug-in approach achieves this worst-case competitive\nratio up to a logarithmic factor. Third, we initiate the study of the partial\nknowledge setting in which it is assumed that the minimum probability taken by\nthe behavior policy is known. We show that the plug-in estimator is optimal for\nrelatively large values of the minimum probability, but is sub-optimal when the\nminimum probability is low. In order to remedy this gap, we propose a new\nestimator based on approximation by Chebyshev polynomials that provably\nachieves the optimal estimation error. Numerical experiments on both simulated\nand real data corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:55:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Cong", ""], ["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2101.07794", "submitter": "Daniel Bartl", "authors": "Daniel Bartl and Shahar Mendelson", "title": "On Monte-Carlo methods in convex stochastic optimization", "comments": "Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR q-fin.MF stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel procedure for estimating the optimizer of general convex\nstochastic optimization problems of the form $\\min_{x\\in\\mathcal{X}}\n\\mathbf{E}[F(x,\\xi)]$, when the given data is a finite independent sample\nselected according to $\\xi$. The procedure is based on a median-of-means\ntournament, and is the first procedure that exhibits the optimal statistical\nperformance in heavy tailed situations: we recover the asymptotic rates\ndictated by the central limit theorem in a non-asymptotic manner once the\nsample size exceeds some explicitly computable threshold. Additionally, our\nresults apply in the high-dimensional setup, as the threshold sample size\nexhibits the optimal dependence on the dimension (up to a logarithmic factor).\nThe general setting allows us to recover recent results on multivariate mean\nestimation and linear regression in heavy-tailed situations and to prove the\nfirst sharp, non-asymptotic results for the portfolio optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:53:30 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bartl", "Daniel", ""], ["Mendelson", "Shahar", ""]]}, {"id": "2101.07795", "submitter": "Leigh Roberts Dr", "authors": "Leigh A Roberts", "title": "On the derivation of the Khmaladze transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some 40 years ago Khmaladze introduced a transform which greatly facilitated\nthe distribution free goodness of fit testing of statistical hypotheses. In the\nlast decade, he has published a related transform, broadly offering an\nalternative means to the same end. The aim of this paper is to derive these\ntransforms using relatively elementary means, making some simplifications, but\nlosing little in the way of generality. In this way it is hoped to make these\ntransforms more accessible and more widely used in statistical practice. We\nalso propose a change of name of the second transform to the Khmaladze\nrotation, in order to better reflect its nature.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:13:24 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Roberts", "Leigh A", ""]]}, {"id": "2101.07860", "submitter": "Kristin Kirchner", "authors": "David Bolin and Kristin Kirchner", "title": "Equivalence of measures and asymptotically optimal linear prediction for\n  Gaussian random fields with fractional-order covariance operators", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Gaussian measures $\\mu, \\tilde{\\mu}$ on a separable Hilbert\nspace, with fractional-order covariance operators $A^{-2\\beta}$ resp.\n$\\tilde{A}^{-2\\tilde{\\beta}}$, and derive necessary and sufficient conditions\non $A, \\tilde{A}$ and $\\beta, \\tilde{\\beta} > 0$ for I. equivalence of the\nmeasures $\\mu$ and $\\tilde{\\mu}$, and II. uniform asymptotic optimality of\nlinear predictions for $\\mu$ based on the misspecified measure $\\tilde{\\mu}$.\nThese results hold, e.g., for Gaussian processes on compact metric spaces. As\nan important special case, we consider the class of generalized\nWhittle-Mat\\'ern Gaussian random fields, where $A$ and $\\tilde{A}$ are elliptic\nsecond-order differential operators, formulated on a bounded Euclidean domain\n$\\mathcal{D}\\subset\\mathbb{R}^d$ and augmented with homogeneous Dirichlet\nboundary conditions. Our outcomes explain why the predictive performances of\nstationary and non-stationary models in spatial statistics often are\ncomparable, and provide a crucial first step in deriving consistency results\nfor parameter estimation of generalized Whittle-Mat\\'ern fields.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:53:22 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bolin", "David", ""], ["Kirchner", "Kristin", ""]]}, {"id": "2101.07969", "submitter": "Zheng Liu", "authors": "Zheng Liu, Po-Ling Loh", "title": "Robust W-GAN-Based Estimation Under Wasserstein Contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is an important problem in statistics which aims at\nproviding a reasonable estimator when the data-generating distribution lies\nwithin an appropriately defined ball around an uncontaminated distribution.\nAlthough minimax rates of estimation have been established in recent years,\nmany existing robust estimators with provably optimal convergence rates are\nalso computationally intractable. In this paper, we study several estimation\nproblems under a Wasserstein contamination model and present computationally\ntractable estimators motivated by generative adversarial networks (GANs).\nSpecifically, we analyze properties of Wasserstein GAN-based estimators for\nlocation estimation, covariance matrix estimation, and linear regression and\nshow that our proposed estimators are minimax optimal in many scenarios.\nFinally, we present numerical results which demonstrate the effectiveness of\nour estimators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:15:16 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Zheng", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2101.07981", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Cody Freitag, Ziteng Sun,\n  Himanshu Tyagi", "title": "Inference under Information Constraints III: Local Privacy Constraints", "comments": "To appear in the Special Issue on Privacy and Security of Information\n  Systems of the IEEE Journal on Selected Areas in Information Theory (JSAIT),\n  2021. Journal version of the AISTATS'19 paper \"Test without Trust: Optimal\n  Locally Private Distribution Testing\" (arXiv:1808.02174), which it extends\n  and supersedes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study goodness-of-fit and independence testing of discrete distributions\nin a setting where samples are distributed across multiple users. The users\nwish to preserve the privacy of their data while enabling a central server to\nperform the tests. Under the notion of local differential privacy, we propose\nsimple, sample-optimal, and communication-efficient protocols for these two\nquestions in the noninteractive setting, where in addition users may or may not\nshare a common random seed. In particular, we show that the availability of\nshared (public) randomness greatly reduces the sample complexity. Underlying\nour public-coin protocols are privacy-preserving mappings which, when applied\nto the samples, minimally contract the distance between their respective\nprobability distributions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:07:49 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Freitag", "Cody", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2101.08083", "submitter": "Marouane Il Idrissi", "authors": "Marouane Il Idrissi, Vincent Chabridon, Bertrand Iooss", "title": "Developments and applications of Shapley effects to reliability-oriented\n  sensitivity analysis with correlated inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-oriented sensitivity analysis methods have been developed for\nunderstanding the influence of model inputs relative to events which\ncharacterize the failure of a system (e.g., a threshold exceedance of the model\noutput). In this field, the target sensitivity analysis focuses primarily on\ncapturing the influence of the inputs on the occurrence of such a critical\nevent. This paper proposes new target sensitivity indices, based on the Shapley\nvalues and called \"target Shapley effects\", allowing for interpretable\nsensitivity measures under dependent inputs. Two algorithms (one based on Monte\nCarlo sampling, and a given-data algorithm based on a nearest-neighbors\nprocedure) are proposed for the estimation of these target Shapley effects\nbased on the $\\ell^2$ norm. Additionally, the behavior of these target Shapley\neffects are theoretically and empirically studied through various toy-cases.\nFinally, the application of these new indices in two real-world use-cases (a\nriver flood model and a COVID-19 epidemiological model) is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:39:24 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:01:07 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:43:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Idrissi", "Marouane Il", ""], ["Chabridon", "Vincent", ""], ["Iooss", "Bertrand", ""]]}, {"id": "2101.08126", "submitter": "Vincent Divol", "authors": "Vincent Divol", "title": "A short proof on the rate of convergence of the empirical measure for\n  the Wasserstein distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a short proof that the Wasserstein distance between the empirical\nmeasure of a n-sample and the estimated measure is of order n^-(1/d), if the\nmeasure has a lower and upper bounded density on the d-dimensional flat torus.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:53:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Divol", "Vincent", ""]]}, {"id": "2101.08162", "submitter": "Steven Miller", "authors": "George Clark, Alex Gonye and Steven J Miller", "title": "Lessons from the German Tank Problem", "comments": "Version 2.1, 17 pages, 9 figures, to appear in the Mathematical\n  Intelligencer, fixed two typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During World War II the German army used tanks to devastating advantage. The\nAllies needed accurate estimates of their tank production and deployment. They\nused two approaches to find these values: spies, and statistics. This note\ndescribes the statistical approach. Assuming the tanks are labeled\nconsecutively starting at 1, if we observe $k$ serial numbers from an unknown\nnumber $N$ of tanks, with the maximum observed value $m$, then the best\nestimate for $N$ is $m(1 + 1/k) - 1$. This is now known as the German Tank\nProblem, and is a terrific example of the applicability of mathematics and\nstatistics in the real world. The first part of the paper reproduces known\nresults, specifically deriving this estimate and comparing its effectiveness to\nthat of the spies. The second part presents a result we have not found in print\nelsewhere, the generalization to the case where the smallest value is not\nnecessarily 1. We emphasize in detail why we are able to obtain such clean,\nclosed-form expressions for the estimates, and conclude with an appendix\nhighlighting how to use this problem to teach regression and how statistics can\nhelp us find functional relationships.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:52:02 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 11:50:37 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Clark", "George", ""], ["Gonye", "Alex", ""], ["Miller", "Steven J", ""]]}, {"id": "2101.08227", "submitter": "Artur O. Lopes", "authors": "Hermes H. Ferreira, Artur O. Lopes and Silvia R.C. Lopes", "title": "Decision Theory and Large Deviations for Dynamical Hypotheses Test:\n  Neyman-Pearson, Min-Max and Bayesian Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cond-mat.stat-mech math-ph math.DS math.MP math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze hypotheses tests via classical results on large deviations for the\ncase of two different Holder Gibbs probabilities. The main difference for the\nthe classical hypotheses tests in Decision Theory is that here the two\nconsidered measures are singular with respect to each other. We analyze the\nclassical Neyman-Pearson test showing its optimality. This test becomes\nexponentially better when compared to other alternative tests, with the sample\nsize going to infinity. We also consider both, the Min-Max and a certain type\nof Bayesian hypotheses tests. We shall consider these tests in the log\nlikelihood framework by using several tools of Thermodynamic Formalism.\nVersions of the Stein's Lemma and the Chernoff's information are also\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 17:46:07 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ferreira", "Hermes H.", ""], ["Lopes", "Artur O.", ""], ["Lopes", "Silvia R. C.", ""]]}, {"id": "2101.08421", "submitter": "Chao Gao", "authors": "Pinhan Chen, Chao Gao, Anderson Y. Zhang", "title": "Optimal Full Ranking from Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of ranking $n$ players from partial pairwise\ncomparison data under the Bradley-Terry-Luce model. For the first time in the\nliterature, the minimax rate of this ranking problem is derived with respect to\nthe Kendall's tau distance that measures the difference between two rank\nvectors by counting the number of inversions. The minimax rate of ranking\nexhibits a transition between an exponential rate and a polynomial rate\ndepending on the magnitude of the signal-to-noise ratio of the problem. To the\nbest of our knowledge, this phenomenon is unique to full ranking and has not\nbeen seen in any other statistical estimation problem. To achieve the minimax\nrate, we propose a divide-and-conquer ranking algorithm that first divides the\n$n$ players into groups of similar skills and then computes local MLE within\neach group. The optimality of the proposed algorithm is established by a\ncareful approximate independence argument between the two steps.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:34:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Pinhan", ""], ["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.08529", "submitter": "Riccardo Gatto", "authors": "Riccardo Gatto", "title": "Information theoretic results for stationary time series and the\n  Gaussian-generalized von Mises time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This chapter presents some novel information theoretic results for the\nanalysis of stationary time series in the frequency domain. In particular, the\nspectral distribution that corresponds to the most uncertain or unpredictable\ntime series with some values of the autocovariance function fixed, is the\ngeneralized von Mises spectral distribution. It is thus a maximum entropy\nspectral distribution and the corresponding stationary time series is called\nthe generalized von Mises time series. The generalized von Mises distribution\nis used in directional statistics for modelling planar directions that follow a\nmultimodal distribution. Furthermore, the Gaussian-generalized von Mises times\nseries is presented as the stationary time series that maximizes entropies in\nfrequency and time domains, respectively referred to as spectral and temporal\nentropies. Parameter estimation and some computational aspects with this time\nseries are briefly analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:27:05 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gatto", "Riccardo", ""]]}, {"id": "2101.08548", "submitter": "Chiara Amorino", "authors": "Chiara Amorino, Eulalia Nualart", "title": "Optimal convergence rates for the invariant density estimation of\n  jump-diffusion processes", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.23569.04962", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim at estimating the invariant density associated to a stochastic\ndifferential equation with jumps in low dimension, which is for $d=1$ and\n$d=2$. We consider a class of jump diffusion processes whose invariant density\nbelongs to some H\\\"older space. Firstly, in dimension one, we show that the\nkernel density estimator achieves the convergence rate $\\frac{1}{T}$, which is\nthe optimal rate in the absence of jumps. This improves the convergence rate\nobtained in [Amorino, Gloter (2021)], which depends on the Blumenthal-Getoor\nindex for $d=1$ and is equal to $\\frac{\\log T}{T}$ for $d=2$. Secondly, we show\nthat is not possible to find an estimator with faster rates of estimation.\nIndeed, we get some lower bounds with the same rates $\\{\\frac{1}{T},\\frac{\\log\nT}{T}\\}$ in the mono and bi-dimensional cases, respectively. Finally, we obtain\nthe asymptotic normality of the estimator in the one-dimensional case.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:02:07 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 09:32:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Amorino", "Chiara", ""], ["Nualart", "Eulalia", ""]]}, {"id": "2101.08597", "submitter": "Joseph Ngatchou-Wandji", "authors": "Joseph Ngatchou-Wandji, Marwa Ltaifa", "title": "On detecting weak changes in the mean of CHARN models", "comments": "45 pages, 3 figures, 5 tables, Latex; typos corrected, references\n  added, algorithms improved, tables added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a likelihood ratio test for detecting multiple {\\it weak} changes in\nthe mean of a class of CHARN models. The locally asymptotically normal (LAN)\nstructure of the family of likelihoods under study is established. It results\nthat the test is asymptotically optimal, and an explicit form of its asymptotic\nlocal power is given as a function of candidates change locations and changes\nmagnitudes. Strategies for weak change-points detection and their locations\nestimates are described. The estimates are obtained as the time indices\nmaximizing an estimate of the local power. A simulation study shows the good\nperformance of our methods compared to some existing approaches. Our results\nare also applied to three sets of real data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:26:47 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 14:30:55 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ngatchou-Wandji", "Joseph", ""], ["Ltaifa", "Marwa", ""]]}, {"id": "2101.08598", "submitter": "Dennis Schroers", "authors": "Fred Espen Benth, Giulia Di Nunno and Dennis Schroers", "title": "A Topological Proof of Sklar's Theorem in Arbitrary Dimensions", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove Sklar's theorem in infinite dimensions via a topological argument\nand the notion of inverse systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:37:00 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Benth", "Fred Espen", ""], ["Di Nunno", "Giulia", ""], ["Schroers", "Dennis", ""]]}, {"id": "2101.08628", "submitter": "Andreas Hamel H", "authors": "Andreas H Hamel and Daniel Kostner", "title": "Computation of quantile sets for bivariate data", "comments": "32 pages, 13 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are proposed for the computation of set-valued quantiles and the\nvalues of the lower cone distribution function for bivariate data sets. These\nnew objects make data analysis possible involving an order relation for the\ndata points in form of a vector order in two dimensions. The bivariate case\ndeserves special attention since two-dimensional vector orders are much simpler\nto handle than such orders in higher dimensions. Several examples illustrate\nhow the algorithms work and what kind of conclusions can be drawn with the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:19:45 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hamel", "Andreas H", ""], ["Kostner", "Daniel", ""]]}, {"id": "2101.08765", "submitter": "Shulei Wang", "authors": "Shulei Wang", "title": "Robust Differential Abundance Test in Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential abundance tests in compositional data are essential and\nfundamental tasks in various biomedical applications, such as single-cell, bulk\nRNA-seq, and microbiome data analysis. However, despite the recent developments\nin these fields, differential abundance analysis in compositional data remains\na complicated and unsolved statistical problem, because of the compositional\nconstraint and prevalent zero counts in the dataset. This study introduces a\nnew differential abundance test, the robust differential abundance (RDB) test,\nto address these challenges. Compared with existing methods, the RDB test 1) is\nsimple and computationally efficient, 2) is robust to prevalent zero counts in\ncompositional datasets, 3) can take the data's compositional nature into\naccount, and 4) has a theoretical guarantee of controlling false discoveries in\na general setting. Furthermore, in the presence of observed covariates, the RDB\ntest can work with the covariate balancing techniques to remove the potential\nconfounding effects and draw reliable conclusions. Finally, we apply the new\ntest to several numerical examples using simulated and real datasets to\ndemonstrate its practical merits.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:37:24 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:27:04 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Shulei", ""]]}, {"id": "2101.08861", "submitter": "Lu Zhang", "authors": "Lu Zhang, Wenpin Tang, Sudipto Banerjee", "title": "Fixed-Domain Asymptotics Under Vecchia's Approximation of Spatial\n  Process Likelihoods", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling for massive spatial data sets has generated a\nsubstantial literature on scalable spatial processes based upon a likelihood\napproximation proposed by Vecchia in 1988. Vecchia's approximation for Gaussian\nprocess models enables fast evaluation of the likelihood by restricting\ndependencies at a location to its neighbors. We establish inferential\nproperties of microergodic spatial covariance parameters within the paradigm of\nfixed-domain asymptotics when they are estimated using Vecchia's approximation.\nThe conditions required to formally establish these properties are explored,\ntheoretically and empirically, and the effectiveness of Vecchia's approximation\nis further corroborated from the standpoint of fixed-domain asymptotics. These\nexplorations suggest some practical diagnostics for evaluating the quality of\nthe approximation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:46:43 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 22:48:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Lu", ""], ["Tang", "Wenpin", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2101.08951", "submitter": "Alan Welsh", "authors": "Ziyang Lyu and A.H. Welsh (Australian National University)", "title": "Increasing Cluster Size Asymptotics for Nested Error Regression Models", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper establishes asymptotic results for the maximum likelihood and\nrestricted maximum likelihood (REML) estimators of the parameters in the nested\nerror regression model for clustered data when both of the number of\nindependent clusters and the cluster sizes (the number of observations in each\ncluster) go to infinity. Under very mild conditions, the estimators are shown\nto be asymptotically normal with an elegantly structured covariance matrix.\nThere are no restrictions on the rate at which the cluster size tends to\ninfinity but it turns out that we need to treat within cluster parameters (i.e.\ncoefficients of unit-level covariates that vary within clusters and the within\ncluster variance) differently from between cluster parameters (i.e.\ncoefficients of cluster-level covariates that are constant within clusters and\nthe between cluster variance) because they require different normalisations and\nare asymptotically independent.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 05:08:25 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lyu", "Ziyang", "", "Australian National University"], ["Welsh", "A. H.", "", "Australian National University"]]}, {"id": "2101.09054", "submitter": "Yuval Dagan", "authors": "Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, Eylon\n  Yogev", "title": "Adversarial Laws of Large Numbers and Optimal Regret in Online\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laws of large numbers guarantee that given a large enough sample from some\npopulation, the measure of any fixed sub-population is well-estimated by its\nfrequency in the sample. We study laws of large numbers in sampling processes\nthat can affect the environment they are acting upon and interact with it.\nSpecifically, we consider the sequential sampling model proposed by Ben-Eliezer\nand Yogev (2020), and characterize the classes which admit a uniform law of\nlarge numbers in this model: these are exactly the classes that are\n\\emph{online learnable}. Our characterization may be interpreted as an online\nanalogue to the equivalence between learnability and uniform convergence in\nstatistical (PAC) learning.\n  The sample-complexity bounds we obtain are tight for many parameter regimes,\nand as an application, we determine the optimal regret bounds in online\nlearning, stated in terms of \\emph{Littlestone's dimension}, thus resolving the\nmain open question from Ben-David, P\\'al, and Shalev-Shwartz (2009), which was\nalso posed by Rakhlin, Sridharan, and Tewari (2015).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:15:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""], ["Dagan", "Yuval", ""], ["Moran", "Shay", ""], ["Naor", "Moni", ""], ["Yogev", "Eylon", ""]]}, {"id": "2101.09117", "submitter": "Guillaume Lecu\\'e", "authors": "Jules Depersin and Guillaume Lecu\\'e", "title": "On the robustness to adversarial corruption and to heavy-tailed data of\n  the Stahel-Donoho median of means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider median of means (MOM) versions of the Stahel-Donoho outlyingness\n(SDO) [stahel 1981, donoho 1982] and of Median Absolute Deviation (MAD)\nfunctions to construct subgaussian estimators of a mean vector under\nadversarial contamination and heavy-tailed data. We develop a single analysis\nof the MOM version of the SDO which covers all cases ranging from the Gaussian\ncase to the L2 case. It is based on isomorphic and almost isometric properties\nof the MOM versions of SDO and MAD. This analysis also covers cases where the\nmean does not even exist but a location parameter does; in those cases we still\nrecover the same subgaussian rates and the same price for adversarial\ncontamination even though there is not even a first moment. These properties\nare achieved by the classical SDO median and are therefore the first\nnon-asymptotic statistical bounds on the Stahel-Donoho median complementing the\n$\\sqrt{n}$-consistency [maronna 1995] and asymptotic normality [Zuo, Cui, He,\n2004] of the Stahel-Donoho estimators. We also show that the MOM version of MAD\ncan be used to construct an estimator of the covariance matrix under only a\nL2-moment assumption or of a scale parameter if a second moment does not exist.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:11:27 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Depersin", "Jules", ""], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "2101.09183", "submitter": "Sancharee Basak", "authors": "Sancharee Basak and Ayanendranath Basu", "title": "The extended Bregman divergence and parametric estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimization of suitable statistical distances~(between the data and model\ndensities) has proved to be a very useful technique in the field of robust\ninference. Apart from the class of $\\phi$-divergences of \\cite{a} and \\cite{b},\nthe Bregman divergence (\\cite{c}) has been extensively used for this purpose.\nHowever, since the data density must have a linear presence in the cross\nproduct term of the Bregman divergence involving both the data and model\ndensities, several useful divergences cannot be captured by the usual Bregman\nform. In this respect, we provide an extension of the ordinary Bregman\ndivergence by considering an exponent of the density function as the argument\nrather than the density function itself. We demonstrate that many useful\ndivergence families, which are not ordinarily Bregman divergences, can be\naccommodated within this extended description. Using this formulation, one can\ndevelop many new families of divergences which may be useful in robust\ninference. In particular, through an application of this extension, we propose\nthe new class of the GSB divergence family. We explore the applicability of the\nminimum GSB divergence estimator in discrete parametric models. Simulation\nstudies as well as conforming real data examples are given to demonstrate the\nperformance of the estimator and to substantiate the theory developed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:11:09 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Basak", "Sancharee", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2101.09195", "submitter": "Xinran Li", "authors": "Devin Caughey, Allan Dafoe, Xinran Li, Luke Miratrix", "title": "Randomization Inference beyond the Sharp Null: Bounded Null Hypotheses\n  and Quantiles of Individual Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization (a.k.a. permutation) inference is typically interpreted as\ntesting Fisher's \"sharp\" null hypothesis that all effects are exactly zero.\nThis hypothesis is often criticized as uninteresting and implausible. We show,\nhowever, that many randomization tests are also valid for a \"bounded\" null\nhypothesis under which effects are all negative (or positive) for all units but\notherwise heterogeneous. The bounded null is closely related to important\nconcepts such as monotonicity and Pareto efficiency. Inverting tests of this\nhypothesis yields confidence intervals for the maximum (or minimum) individual\ntreatment effect. We then extend randomization tests to infer other quantiles\nof individual effects, which can be used to infer the proportion of units with\neffects larger (or smaller) than any threshold. The proposed confidence\nintervals for all quantiles of individual effects are simultaneously valid, in\nthe sense that no correction due to multiple analyses is needed. In sum, we\nprovide a broader justification for Fisher randomization tests, and develop\nexact nonparametric inference for quantiles of heterogeneous individual\neffects. We illustrate our methods with simulations and applications, where we\nfind that Stephenson rank statistics often provide the most informative\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:39:06 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Caughey", "Devin", ""], ["Dafoe", "Allan", ""], ["Li", "Xinran", ""], ["Miratrix", "Luke", ""]]}, {"id": "2101.09271", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "Representation of Context-Specific Causal Models with Observational and\n  Interventional Data", "comments": "26 pages, supplementary material 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of representing causal models that encode\ncontext-specific information for discrete data. To represent such models we use\na proper subclass of staged tree models which we call CStrees. We show that the\ncontext-specific information encoded by a CStree can be equivalently expressed\nvia a collection of DAGs. As not all staged tree models admit this property,\nCStrees are a subclass that provides a transparent, intuitive and compact\nrepresentation of context-specific causal information. Model equivalence for\nCStrees also takes a simpler form than for general staged trees: We provide a\ncharacterization of the complete set of asymmetric conditional independence\nrelations encoded by a CStree. As a consequence, we obtain a global Markov\nproperty for CStrees which leads to a graphical criterion of model equivalence\nfor CStrees generalizing that of Verma and Pearl for DAG models. In addition,\nwe provide a closed-form formula for the maximum likelihood estimator of a\nCStree and use it to show that the Bayesian information criterion is a locally\nconsistent score function for this model class. We also give an analogous\nglobal Markov property and characterization of model equivalence for general\ninterventions in CStrees. As examples, we apply these results to two real data\nsets, and examine how BIC-optimal CStrees for each provide a clear and concise\nrepresentation of the learned context-specific causal structure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:48:29 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:10:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2101.09382", "submitter": "Krzysztof Szajowski", "authors": "Krzysztof J. Szajowski and Kinga W{\\l}odarczyk", "title": "A measure of the importance of roads based on topography and traffic\n  intensity", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GR math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of street traffic allowing assessment of the importance\nof their individual segments for the functionality of the street system is\nconsidering. Based on methods of cooperative games and the reliability theory\nthe suitable measure is constructed. The main goal is to analyze methods for\nassessing the importance (rank) of road fragments, including their functions. A\nrelevance of these elements for effective accessibility for the entire system\nwill be considered.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:53:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Szajowski", "Krzysztof J.", ""], ["W\u0142odarczyk", "Kinga", ""]]}, {"id": "2101.09558", "submitter": "Alfredo Alegr\\'ia", "authors": "Xavier Emery and Alfredo Alegr\\'ia", "title": "The Gauss Hypergeometric Covariance Kernel for Modeling Second-Order\n  Stationary Random Fields in Euclidean Spaces: its Compact Support, Properties\n  and Spectral Representation", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a parametric family of compactly-supported positive\nsemidefinite kernels aimed to model the covariance structure of second-order\nstationary isotropic random fields defined in the $d$-dimensional Euclidean\nspace. Both the covariance and its spectral density have an analytic expression\ninvolving the hypergeometric functions ${}_2F_1$ and ${}_1F_2$, respectively,\nand four real-valued parameters related to the correlation range, smoothness\nand shape of the covariance. The presented hypergeometric kernel family\ncontains, as special cases, the spherical, cubic, penta, Askey, generalized\nWendland and truncated power covariances and, as asymptotic cases, the\nMat\\'ern, Laguerre, Tricomi, incomplete gamma and Gaussian covariances, among\nothers. The parameter space of the univariate hypergeometric kernel is\nidentified and its functional properties -- continuity, smoothness, transitive\nupscaling (mont\\'ee) and downscaling (descente) -- are examined. Several sets\nof sufficient conditions are also derived to obtain valid stationary bivariate\nand multivariate covariance kernels, characterized by four matrix-valued\nparameters. Such kernels turn out to be versatile, insofar as the direct and\ncross-covariances do not necessarily have the same shapes, correlation ranges\nor behaviors at short scale, thus associated with vector random fields whose\ncomponents are cross-correlated but have different spatial structures.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:55:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Emery", "Xavier", ""], ["Alegr\u00eda", "Alfredo", ""]]}, {"id": "2101.09605", "submitter": "Art Owen", "authors": "Dan M. Kluger and Art B. Owen", "title": "Local linear tie-breaker designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tie-breaker experimental designs are hybrids of Randomized Control Trials\n(RCTs) and Regression Discontinuity Designs (RDDs) in which subjects with\nmoderate scores are placed in an RCT while subjects with extreme scores are\ndeterministically assigned to the treatment or control group. The design\nmaintains the benefits of randomization for causal estimation while avoiding\nthe possibility of excluding the most deserving recipients from the treatment\ngroup. The causal effect estimator for a tie-breaker design can be estimated by\nfitting local linear regressions for both the treatment and control group, as\nis typically done for RDDs. We study the statistical efficiency of such local\nlinear regression-based causal estimators as a function of $\\Delta$, the radius\nof the interval in which treatment randomization occurs. In particular, we\ndetermine the efficiency of the estimator as a function of $\\Delta$ for a\nfixed, arbitrary bandwidth under the assumption of a uniform assignment\nvariable. To generalize beyond uniform assignment variables and asymptotic\nregimes, we also demonstrate on the Angrist and Lavy (1999) classroom size\ndataset that prior to conducting an experiment, an experimental designer can\nestimate the efficiency for various experimental radii choices by using Monte\nCarlo as long as they have access to the distribution of the assignment\nvariable. For both uniform and triangular kernels, we show that increasing the\nradius of randomized experiment interval will increase the efficiency until the\nradius is the size of the local-linear regression bandwidth, after which no\nadditional efficiency benefits are conferred.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:41:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kluger", "Dan M.", ""], ["Owen", "Art B.", ""]]}, {"id": "2101.09711", "submitter": "Joni Virta", "authors": "Joni Virta", "title": "Testing for subsphericity when $n$ and $p$ are of different asymptotic\n  order", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a classical test of subsphericity, based on the first two moments\nof the eigenvalues of the sample covariance matrix, to the high-dimensional\nregime where the signal eigenvalues of the covariance matrix diverge to\ninfinity and either $p/n \\rightarrow 0$ or $p/n \\rightarrow \\infty$. In the\nlatter case we further require that the divergence of the eigenvalues is\nsuitably fast in a specific sense. Our work can be seen to complement that of\nSchott (2006) who established equivalent results in the case $p/n \\rightarrow\n\\gamma \\in (0, \\infty)$. As our second main contribution, we use the test to\nderive a consistent estimator for the latent dimension of the model.\nSimulations and a real data example are used to demonstrate the results,\nproviding also evidence that the test might be further extendable to a wider\nasymptotic regime.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:51:44 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 07:47:54 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 11:52:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Virta", "Joni", ""]]}, {"id": "2101.09855", "submitter": "Kuang Xu", "authors": "Stefan Wager and Kuang Xu", "title": "Diffusion Asymptotics for Sequential Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new diffusion-asymptotic analysis for sequentially randomized\nexperiments, including those that arise in solving multi-armed bandit problems.\nIn an experiment with $ n $ time steps, we let the mean reward gaps between\nactions scale to the order $1/\\sqrt{n}$ so as to preserve the difficulty of the\nlearning task as $n$ grows. In this regime, we show that the behavior of a\nclass of sequentially randomized Markov experiments converges to a diffusion\nlimit, given as the solution of a stochastic differential equation. The\ndiffusion limit thus enables us to derive refined, instance-specific\ncharacterization of the stochastic dynamics of adaptive experiments. As an\napplication of this framework, we use the diffusion limit to obtain several new\ninsights on the regret and belief evolution of Thompson sampling. We show that\na version of Thompson sampling with an asymptotically uninformative prior\nvariance achieves nearly-optimal instance-specific regret scaling when the\nreward gaps are relatively large. We also demonstrate that, in this regime, the\nposterior beliefs underlying Thompson sampling are highly unstable over time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:20:20 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:50:40 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 22:02:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wager", "Stefan", ""], ["Xu", "Kuang", ""]]}, {"id": "2101.09875", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Nan Wu", "title": "Eigen-convergence of Gaussian kernelized graph Laplacian by manifold\n  heat interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work studies the spectral convergence of graph Laplacian to the\nLaplace-Beltrami operator when the graph affinity matrix is constructed from\n$N$ random samples on a $d$-dimensional manifold embedded in a possibly high\ndimensional space. By analyzing Dirichlet form convergence and constructing\ncandidate approximate eigenfunctions via convolution with manifold heat kernel,\nwe prove that, with Gaussian kernel, one can set the kernel bandwidth parameter\n$\\epsilon \\sim (\\log N/ N)^{1/(d/2+2)}$ such that the eigenvalue convergence\nrate is $N^{-1/(d/2+2)}$ and the eigenvector convergence in 2-norm has rate\n$N^{-1/(d+4)}$; When $\\epsilon \\sim N^{-1/(d/2+3)}$, both eigenvalue and\neigenvector rates are $N^{-1/(d/2+3)}$. These rates are up to a $\\log N$ factor\nand proved for finitely many low-lying eigenvalues. The result holds for\nun-normalized and random-walk graph Laplacians when data are uniformly sampled\non the manifold, as well as the density-corrected graph Laplacian (where the\naffinity matrix is normalized by the degree matrix from both sides) with\nnon-uniformly sampled data. As an intermediate result, we prove new point-wise\nand Dirichlet form convergence rates for the density-corrected graph Laplacian.\nNumerical results are provided to verify the theory.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:22:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Wu", "Nan", ""]]}, {"id": "2101.10058", "submitter": "Yikun Zhang", "authors": "Yikun Zhang, Yen-Chi Chen", "title": "The EM Perspective of Directional Mean Shift Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The directional mean shift (DMS) algorithm is a nonparametric method for\npursuing local modes of densities defined by kernel density estimators on the\nunit hypersphere. In this paper, we show that any DMS iteration can be viewed\nas a generalized Expectation-Maximization (EM) algorithm; in particular, when\nthe von Mises kernel is applied, it becomes an exact EM algorithm. Under the\n(generalized) EM framework, we provide a new proof for the ascending property\nof density estimates and demonstrate the global convergence of directional mean\nshift sequences. Finally, we give a new insight into the linear convergence of\nthe DMS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:17:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2101.10255", "submitter": "Abhimanyu Gupta", "authors": "Abhimanyu Gupta, Xi Qu", "title": "Consistent specification testing under spatial dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a series-based nonparametric specification test for a regression\nfunction when data are spatially dependent, the `space' being of a general\neconomic or social nature. Dependence can be parametric, parametric with\nincreasing dimension, semiparametric or any combination thereof, thus covering\na vast variety of settings. These include spatial error models of varying types\nand levels of complexity. Under a new smooth spatial dependence condition, our\ntest statistic is asymptotically standard normal. To prove the latter property,\nwe establish a central limit theorem for quadratic forms in linear processes in\nan increasing dimension setting. Finite sample performance is investigated in a\nsimulation study and empirical examples illustrate the test with real-world\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:44:51 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gupta", "Abhimanyu", ""], ["Qu", "Xi", ""]]}, {"id": "2101.10373", "submitter": "Yuqi Gu", "authors": "Yuqi Gu, David B. Dunson", "title": "Identifying Interpretable Discrete Latent Structures from Discrete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional categorical data are routinely collected in biomedical and\nsocial sciences. It is of great importance to build interpretable models that\nperform dimension reduction and uncover meaningful latent structures from such\ndiscrete data. Identifiability is a fundamental requirement for valid modeling\nand inference in such scenarios, yet is challenging to address when there are\ncomplex latent structures. In this article, we propose a class of interpretable\ndiscrete latent structure models for discrete data and develop a general\nidentifiability theory. Our theory is applicable to various types of latent\nstructures, ranging from a single latent variable to deep layers of latent\nvariables organized in a sparse graph (termed a Bayesian pyramid). The proposed\nidentifiability conditions can ensure Bayesian posterior consistency under\nsuitable priors. As an illustration, we consider the two-latent-layer model and\npropose a Bayesian shrinkage estimation approach. Simulation results for this\nmodel corroborate identifiability and estimability of the model parameters.\nApplications of the methodology to DNA nucleotide sequence data uncover\ndiscrete latent features that are both interpretable and highly predictive of\nsequence types. The proposed framework provides a recipe for interpretable\nunsupervised learning of discrete data, and can be a useful alternative to\npopular machine learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:43:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gu", "Yuqi", ""], ["Dunson", "David B.", ""]]}, {"id": "2101.10515", "submitter": "Rui Zhang", "authors": "Rui Zhang, Junting Chen, Yao Xie, Alexander Shapiro, Urbashi Mitra", "title": "Testing Rank of Incomplete Unimodal Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3070524", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistics-based detectors, based on unimodal matrix models, for\ndetermining the number of sources in a field are designed. A new variance ratio\nstatistic is proposed, and its asymptotic distribution is analyzed. The\nvariance ratio detector is shown to outperform the alternatives. It is shown\nthat further improvements are achievable via optimally selected rotations.\nNumerical experiments demonstrate the performance gains of our detection\nmethods over the baseline approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:52:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Rui", ""], ["Chen", "Junting", ""], ["Xie", "Yao", ""], ["Shapiro", "Alexander", ""], ["Mitra", "Urbashi", ""]]}, {"id": "2101.10588", "submitter": "Andrea Montanari", "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Generalization error of random features and kernel methods:\n  hypercontractivity and kernel matrix concentration", "comments": "77 pages; 1 pdf figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical supervised learning problem: we are given data\n$(y_i,{\\boldsymbol x}_i)$, $i\\le n$, with $y_i$ a response and ${\\boldsymbol\nx}_i\\in {\\mathcal X}$ a covariates vector, and try to learn a model\n$f:{\\mathcal X}\\to{\\mathbb R}$ to predict future responses. Random features\nmethods map the covariates vector ${\\boldsymbol x}_i$ to a point ${\\boldsymbol\n\\phi}({\\boldsymbol x}_i)$ in a higher dimensional space ${\\mathbb R}^N$, via a\nrandom featurization map ${\\boldsymbol \\phi}$. We study the use of random\nfeatures methods in conjunction with ridge regression in the feature space\n${\\mathbb R}^N$. This can be viewed as a finite-dimensional approximation of\nkernel ridge regression (KRR), or as a stylized model for neural networks in\nthe so called lazy training regime.\n  We define a class of problems satisfying certain spectral conditions on the\nunderlying kernels, and a hypercontractivity assumption on the associated\neigenfunctions. These conditions are verified by classical high-dimensional\nexamples. Under these conditions, we prove a sharp characterization of the\nerror of random features ridge regression. In particular, we address two\nfundamental questions: $(1)$~What is the generalization error of KRR? $(2)$~How\nbig $N$ should be for the random features approximation to achieve the same\nerror as KRR?\n  In this setting, we prove that KRR is well approximated by a projection onto\nthe top $\\ell$ eigenfunctions of the kernel, where $\\ell$ depends on the sample\nsize $n$. We show that the test error of random features ridge regression is\ndominated by its approximation error and is larger than the error of KRR as\nlong as $N\\le n^{1-\\delta}$ for some $\\delta>0$. We characterize this gap. For\n$N\\ge n^{1+\\delta}$, random features achieve the same error as the\ncorresponding KRR, and further increasing $N$ does not lead to a significant\nchange in test error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:46:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2101.10609", "submitter": "Olivier Besson", "authors": "Olivier Besson", "title": "On the distributions of some statistics related to adaptive filters\n  trained with $t$-distributed samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the behaviour of adaptive filters or detectors when\nthey are trained with $t$-distributed samples rather than Gaussian distributed\nsamples. More precisely we investigate the impact on the distribution of some\nrelevant statistics including the signal to noise ratio loss and the Gaussian\ngeneralized likelihood ratio test. Some properties of partitioned complex $F$\ndistributed matrices are derived which enable to obtain statistical\nrepresentations in terms of independent chi-square distributed random\nvariables. These representations are compared with their Gaussian counterparts\nand numerical simulations illustrate and quantify the induced degradation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:44:42 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 08:06:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Besson", "Olivier", ""]]}, {"id": "2101.10692", "submitter": "Francesco Ortelli", "authors": "Francesco Ortelli and Sara van de Geer", "title": "Tensor denoising with trend filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of trend filtering to tensors by considering the $k^{\\rm\nth}$-order Vitali variation, a discretized version of the integral of the\nabsolute value of the $k^{\\rm th}$-order total derivative. We prove adaptive\n$\\ell^0$-rates and not-so-slow $\\ell^1$-rates for tensor denoising with trend\nfiltering.\n  For $k=\\{1,2,3,4\\}$ we prove that the $d$-dimensional margin of a\n$d$-dimensional tensor can be estimated at the $\\ell^0$-rate $n^{-1}$, up to\nlogarithmic terms, if the underlying tensor is a product of $(k-1)^{\\rm\nth}$-order polynomials on a constant number of hyperrectangles. For general $k$\nwe prove the $\\ell^1$-rate of estimation $n^{- \\frac{H(d)+2k-1}{2H(d)+2k-1}}$,\nup to logarithmic terms, where $H(d)$ is the $d^{\\rm th}$ harmonic number.\n  Thanks to an ANOVA-type of decomposition we can apply these results to the\nlower dimensional margins of the tensor to prove bounds for denoising the whole\ntensor. Our tools are interpolating tensors to bound the effective sparsity for\n$\\ell^0$-rates, mesh grids for $\\ell^1$-rates and, in the background, the\nprojection arguments by Dalalyan et al.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:33:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ortelli", "Francesco", ""], ["van de Geer", "Sara", ""]]}, {"id": "2101.10695", "submitter": "Joseph Lehec", "authors": "Joseph Lehec", "title": "The Langevin Monte Carlo algorithm in the non-smooth log-concave case", "comments": "v2: Fixes a few typos and improves a bit Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove non asymptotic polynomial bounds on the convergence of the Langevin\nMonte Carlo algorithm in the case where the potential is a convex function\nwhich is globally Lipschitz on its domain, typically the maximum of a finite\nnumber of affine functions on an arbitrary convex set. In particular the\npotential is not assumed to be gradient Lipschitz, in contrast with most (if\nnot all) existing works on the topic.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:40:03 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:28:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lehec", "Joseph", ""]]}, {"id": "2101.10855", "submitter": "Salem Said", "authors": "Salem Said", "title": "Statistical models and probabilistic methods on Riemannian manifolds", "comments": "first version of habilitation thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This entry contains the core material of my habilitation thesis, soon to be\nofficially submitted. It provides a self-contained presentation of the original\nresults in this thesis, in addition to their detailed proofs. The motivation of\nthese results is the analysis of data which lie in Riemannian manifolds. Their\naim is to bring about general, meaningful, and applicable tools, which can be\nused to model, and to learn from such \"Riemannian data\", as well as to analyse\nthe various algorithms which may be required in this kind of pursuit (for\nsampling, optimisation, stochastic approximation, ...). The world of Riemannian\ndata and algorithms can be quite different from its Euclidean counterpart, and\nthis difference is the source of mathematical problems, addressed in my thesis.\nThe first chapter provides some taylor-made geometric constructions, to be used\nin the thesis, while subsequent chapters (there are four more of them), address\na series of issues, which arise from unresolved challenges, in the recent\nliterature. A one-page guide, on how to read the thesis, is to be found right\nafter the table of contents.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:18:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Said", "Salem", ""]]}, {"id": "2101.10880", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "USP: an independence test that improves on Pearson's chi-squared and the\n  $G$-test", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the $U$-Statistic Permutation (USP) test of independence in the\ncontext of discrete data displayed in a contingency table. Either Pearson's\nchi-squared test of independence, or the $G$-test, are typically used for this\ntask, but we argue that these tests have serious deficiencies, both in terms of\ntheir inability to control the size of the test, and their power properties. By\ncontrast, the USP test is guaranteed to control the size of the test at the\nnominal level for all sample sizes, has no issues with small (or zero) cell\ncounts, and is able to detect distributions that violate independence in only a\nminimal way. The test statistic is derived from a $U$-statistic estimator of a\nnatural population measure of dependence, and we prove that this is the unique\nminimum variance unbiased estimator of this population quantity. The practical\nutility of the USP test is demonstrated on both simulated data, where its power\ncan be dramatically greater than those of Pearson's test and the $G$-test, and\non real data. The USP test is implemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:42:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2101.10950", "submitter": "Ali Amiryousefi", "authors": "Ali Amiryousefi", "title": "Asymptotic Supervised Predictive Classifiers under Partition\n  Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The convergence of simultaneous and marginal predictive classifiers under\npartition exchangeability in supervised classification is obtained. The result\nshows the asymptotic convergence of these classifiers under infinite amount of\ntraining or test data, such that after observing umpteen amount of data, the\ndifferences between these classifiers would be negligible. This is an important\nresult from the practical perspective as under the presence of sufficiently\nlarge amount of data, one can replace the simpler marginal classifier with\ncomputationally more expensive simultaneous one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:17:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Amiryousefi", "Ali", ""]]}, {"id": "2101.10962", "submitter": "Song Wei", "authors": "Song Wei, Yao Xie, Dobromir Rahnev", "title": "Inferring serial correlation with dynamic backgrounds", "comments": "39 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential data with serial correlation and an unknown, unstructured, and\ndynamic background is ubiquitous in neuroscience, psychology, and econometrics.\nInferring serial correlation for such data is a fundamental challenge in\nstatistics. We propose a total variation constrained least square estimator\ncoupled with hypothesis tests to infer the serial correlation in the presence\nof unknown and unstructured dynamic background. The total variation constraint\non the dynamic background encourages a piece-wise constant structure, which can\napproximate a wide range of dynamic backgrounds. The tuning parameter is\nselected via the Ljung-Box test to control the bias-variance trade-off. We\nestablish a non-asymptotic upper bound for the estimation error through\nvariational inequalities. We also derive a lower error bound via Fano's method\nand show the proposed method is near-optimal. Numerical simulation and a real\nstudy in psychology demonstrate the excellent performance of our proposed\nmethod compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:40:51 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 22:22:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wei", "Song", ""], ["Xie", "Yao", ""], ["Rahnev", "Dobromir", ""]]}, {"id": "2101.11086", "submitter": "Qiyang Han", "authors": "Qiyang Han, Tiefeng Jiang, Yandi Shen", "title": "A general method for power analysis in testing high dimensional\n  covariance matrices", "comments": "80 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance matrix testing for high dimensional data is a fundamental problem.\nA large class of covariance test statistics based on certain averaged spectral\nstatistics of the sample covariance matrix are known to obey central limit\ntheorems under the null. However, precise understanding for the power behavior\nof the corresponding tests under general alternatives remains largely unknown.\nThis paper develops a general method for analyzing the power behavior of\ncovariance test statistics via accurate non-asymptotic power expansions. We\nspecialize our general method to two prototypical settings of testing identity\nand sphericity, and derive sharp power expansion for a number of widely used\ntests, including the likelihood ratio tests, Ledoit-Nagao-Wolf's test, Cai-Ma's\ntest and John's test. The power expansion for each of those tests holds\nuniformly over all possible alternatives under mild growth conditions on the\ndimension-to-sample ratio. Interestingly, although some of those tests are\npreviously known to share the same limiting power behavior under spiked\ncovariance alternatives with a fixed number of spikes, our new power\ncharacterizations indicate that such equivalence fails when many spikes exist.\nThe proofs of our results combine techniques from Poincar\\'e-type inequalities,\nrandom matrices and zonal polynomials.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:15:11 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Han", "Qiyang", ""], ["Jiang", "Tiefeng", ""], ["Shen", "Yandi", ""]]}, {"id": "2101.11257", "submitter": "Arnaud Guillin", "authors": "Patrick Cattiaux (IMT), Arnaud Guillin (LMBP)", "title": "Functional inequalities for perturbed measures with applications to\n  log-concave measures and to some Bayesian problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study functional inequalities (Poincar\\'e, Cheeger, log-Sobolev) for\nprobability measures obtained as perturbations. Several explicit results for\ngeneral measures as well as log-concave distributions are given.The initial\ngoal of this work was to obtain explicit bounds on the constants in view of\nstatistical applications for instance. These results are then applied to the\nLangevin Monte-Carlo method used in statistics in order to compute Bayesian\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:26:54 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cattiaux", "Patrick", "", "IMT"], ["Guillin", "Arnaud", "", "LMBP"]]}, {"id": "2101.11381", "submitter": "Sarah Ouadah", "authors": "Sarah Ouadah, Pierre Latouche, St\\'ephane Robin", "title": "Motif-based tests for bipartite networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks are a natural representation of the interactions between\nentities from two different types. The organization (or topology) of such\nnetworks gives insight to understand the systems they describe as a whole.\nHere, we rely on motifs which provide a meso-scale description of the topology.\nMoreover, we consider the bipartite expected degree distribution (B-EDD) model\nwhich accounts for both the density of the network and possible imbalances\nbetween the degrees of the nodes. Under the B-EDD model, we prove the\nasymptotic normality of the count of any given motif, considering sparsity\nconditions. We also provide close-form expressions for the mean and the\nvariance of this count. This allows to avoid computationally prohibitive\nresampling procedures. Based on these results, we define a goodness-of-fit test\nfor the B-EDD model and propose a family of tests for network comparisons. We\nassess the asymptotic normality of the test statistics and the power of the\nproposed tests on synthetic experiments and illustrate their use on ecological\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:27:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ouadah", "Sarah", ""], ["Latouche", "Pierre", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "2101.11460", "submitter": "Hamza M. Ruzayqat", "authors": "Dan Crisan, Pierre Del Moral, Ajay Jasra, Hamza Ruzayqat", "title": "Log-Normalization Constant Estimation using the Ensemble Kalman-Bucy\n  Filter with Application to High-Dimensional Models", "comments": "25 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we consider the estimation of the log-normalization constant\nassociated to a class of continuous-time filtering models. In particular, we\nconsider ensemble Kalman-Bucy filter based estimates based upon several\nnonlinear Kalman-Bucy diffusions. Based upon new conditional bias results for\nthe mean of the afore-mentioned methods, we analyze the empirical log-scale\nnormalization constants in terms of their $\\mathbb{L}_n-$errors and conditional\nbias. Depending on the type of nonlinear Kalman-Bucy diffusion, we show that\nthese are of order $(\\sqrt{t/N}) + t/N$ or $1/\\sqrt{N}$ ($\\mathbb{L}_n-$errors)\nand of order $[t+\\sqrt{t}]/N$ or $1/N$ (conditional bias), where $t$ is the\ntime horizon and $N$ is the ensemble size. Finally, we use these results for\nonline static parameter estimation for above filtering models and implement the\nmethodology for both linear and nonlinear models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:46:26 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Crisan", "Dan", ""], ["Del Moral", "Pierre", ""], ["Jasra", "Ajay", ""], ["Ruzayqat", "Hamza", ""]]}, {"id": "2101.11531", "submitter": "Ruriko Yoshida", "authors": "Ruriko Yoshida and Misaki Takamori and Hideyuki Matsumoto and Keiji\n  Miura", "title": "Tropical Support Vector Machines: Evaluations and Extension to Function\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Support Vector Machines (SVMs) are one of the most popular supervised\nlearning models to classify using a hyperplane in an Euclidean space. Similar\nto SVMs, tropical SVMs classify data points using a tropical hyperplane under\nthe tropical metric with the max-plus algebra. In this paper, first we show\ngeneralization error bounds of tropical SVMs over the tropical projective\nspace. While the generalization error bounds attained via VC dimensions in a\ndistribution-free manner still depend on the dimension, we also show\ntheoretically by extreme value statistics that the tropical SVMs for\nclassifying data points from two Gaussian distributions as well as empirical\ndata sets of different neuron types are fairly robust against the curse of\ndimensionality. Extreme value statistics also underlie the anomalous scaling\nbehaviors of the tropical distance between random vectors with additional noise\ndimensions. Finally, we define tropical SVMs over a function space with the\ntropical metric and discuss the Gaussian function space as an example.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:35:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yoshida", "Ruriko", ""], ["Takamori", "Misaki", ""], ["Matsumoto", "Hideyuki", ""], ["Miura", "Keiji", ""]]}, {"id": "2101.11783", "submitter": "Cheng Mao", "authors": "Cheng Mao, Mark Rudelson, and Konstantin Tikhomirov", "title": "Random Graph Matching with Improved Noise Robustness", "comments": "34 pages. Accepted for presentation at Conference on Learning Theory\n  (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching, also known as network alignment, refers to finding a\nbijection between the vertex sets of two given graphs so as to maximally align\ntheir edges. This fundamental computational problem arises frequently in\nmultiple fields such as computer vision and biology. Recently, there has been a\nplethora of work studying efficient algorithms for graph matching under\nprobabilistic models. In this work, we propose a new algorithm for graph\nmatching: Our algorithm associates each vertex with a signature vector using a\nmultistage procedure and then matches a pair of vertices from the two graphs if\ntheir signature vectors are close to each other. We show that, for two\nErd\\H{o}s--R\\'enyi graphs with edge correlation $1-\\alpha$, our algorithm\nrecovers the underlying matching exactly with high probability when $\\alpha \\le\n1 / (\\log \\log n)^C$, where $n$ is the number of vertices in each graph and $C$\ndenotes a positive universal constant. This improves the condition $\\alpha \\le\n1 / (\\log n)^C$ achieved in previous work.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:39:27 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 21:11:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mao", "Cheng", ""], ["Rudelson", "Mark", ""], ["Tikhomirov", "Konstantin", ""]]}, {"id": "2101.11815", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Benjamin Recht", "title": "Interpolating Classifiers Make Few Mistakes", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:51:24 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 01:40:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liang", "Tengyuan", ""], ["Recht", "Benjamin", ""]]}, {"id": "2101.11968", "submitter": "Holger Dette", "authors": "Holger Dette and Anatoly Zhigljavsky", "title": "Reproducing kernel Hilbert spaces, polynomials and the classical moment\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that polynomials do not belong to the reproducing kernel Hilbert\nspace of infinitely differentiable translation-invariant kernels whose spectral\nmeasures have moments corresponding to a determinate moment problem. Our proof\nis based on relating this question to the problem of best linear estimation in\ncontinuous time one-parameter regression models with a stationary error process\ndefined by the kernel. In particular, we show that the existence of a sequence\nof estimators with variances converging to $0$ implies that the regression\nfunction cannot be an element of the reproducing kernel Hilbert space. This\nquestion is then related to the determinacy of the Hamburger moment problem for\nthe spectral measure corresponding to the kernel.\n  In the literature it was observed that a non-vanishing constant function does\nnot belong to the reproducing kernel Hilbert space associated with the Gaussian\nkernel (see Corollary 4.44 in Steinwart and Christmann, 2008). Our results\nprovide a unifying view of this phenomenon and show that the mentioned result\ncan be extended for arbitrary polynomials and a broad class of\ntranslation-invariant kernels.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:46:56 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 12:51:24 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 11:33:23 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 14:26:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dette", "Holger", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "2101.12034", "submitter": "Adam Hall", "authors": "Adam Hall", "title": "Ellipse Combining with Unknown Cross Ellipse Correlations", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We discuss the combining of measurements where single measurement covariances\nare given but the joint measurement covariance is unknown. For this paper we\nassume the mapping of a single measurement to the solution space is the\nidentity matrix. We examine the solution when it is assumed all measurements\nare uncorrelated. We then present a way to parameter joint measurement\ncovariance based on pairwise correlation coefficients. Finally, we discuss how\nto use this parameterization to combine the measurements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:51:58 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hall", "Adam", ""]]}, {"id": "2101.12262", "submitter": "Takaaki Koike", "authors": "Takaaki Koike, Shogo Kato, Marius Hofert", "title": "Tail concordance measures: A fair assessment of tail dependence", "comments": "42 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of measures of bivariate tail dependence called tail concordance\nmeasures (TCMs) is proposed, which is defined as the limit of a measure of\nconcordance of the underlying copula restricted to the tail region of interest.\nTCMs captures the extremal relationship between random variables not only along\nthe diagonal but also along all angles weighted by a tail generating measure.\nAxioms of tail dependence measures are introduced, and TCMs are shown to\ncharacterize linear tail dependence measures. The infimum and supremum of TCMs\nover all generating measures are considered to investigate the issue of under-\nand overestimation of the degree of extreme co-movements. The infimum is shown\nto be attained by the classical tail dependence coefficient, and thus the\nclassical notion always underestimates the degree of tail dependence. A formula\nfor the supremum TCM is derived and shown to overestimate the degree of extreme\nco-movements. Estimators of the proposed measures are studied, and their\nperformance is demonstrated in numerical experiments. For a fair assessment of\ntail dependence and stability of the estimation under small sample sizes, TCMs\nweighted over all angles are suggested, with tail Spearman's rho and tail\nGini's gamma being interesting novel special cases of TCMs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 20:17:49 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 06:20:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Koike", "Takaaki", ""], ["Kato", "Shogo", ""], ["Hofert", "Marius", ""]]}, {"id": "2101.12282", "submitter": "Christoph Breunig", "authors": "Christoph Breunig, Xiaohong Chen", "title": "Adaptive Estimation of Quadratic Functionals in Nonparametric\n  Instrumental Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers adaptive estimation of quadratic functionals in the\nnonparametric instrumental variables (NPIV) models. Minimax estimation of a\nquadratic functional of a NPIV is an important problem in optimal estimation of\na nonlinear functional of an ill-posed inverse regression with an unknown\noperator using one random sample. We first show that a leave-one-out, sieve\nNPIV estimator of the quadratic functional proposed by \\cite{BC2020} attains a\nconvergence rate that coincides with the lower bound previously derived by\n\\cite{ChenChristensen2017}. The minimax rate is achieved by the optimal choice\nof a key tuning parameter (sieve dimension) that depends on unknown NPIV model\nfeatures. We next propose a data driven choice of the tuning parameter based on\nLepski's method. The adaptive estimator attains the minimax optimal rate in the\nseverely ill-posed case and in the regular, mildly ill-posed case, but up to a\nmultiplicative $\\sqrt{\\log n}$ in the irregular, mildly ill-posed case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:14:02 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Breunig", "Christoph", ""], ["Chen", "Xiaohong", ""]]}, {"id": "2101.12353", "submitter": "Yunfei Yang", "authors": "Yunfei Yang, Zhen Li, Yang Wang", "title": "On the capacity of deep generative networks for approximating\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficacy and efficiency of deep generative networks for\napproximating probability distributions. We prove that neural networks can\ntransform a low-dimensional source distribution to a distribution that is\narbitrarily close to a high-dimensional target distribution, when the closeness\nare measured by Wasserstein distances and maximum mean discrepancy. Upper\nbounds of the approximation error are obtained in terms of the width and depth\nof neural network. Furthermore, it is shown that the approximation error in\nWasserstein distance grows at most linearly on the ambient dimension and that\nthe approximation order only depends on the intrinsic dimension of the target\ndistribution. On the contrary, when $f$-divergences are used as metrics of\ndistributions, the approximation property is different. We show that in order\nto approximate the target distribution in $f$-divergences, the dimension of the\nsource distribution cannot be smaller than the intrinsic dimension of the\ntarget distribution.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:45:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 07:42:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Yunfei", ""], ["Li", "Zhen", ""], ["Wang", "Yang", ""]]}, {"id": "2101.12459", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Kazuki Okamura", "title": "On $f$-divergences between Cauchy distributions", "comments": "56 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the $f$-divergences between univariate Cauchy distributions are\nall symmetric, and can be expressed as strictly increasing scalar functions of\nthe symmetric chi-squared divergence. We report the corresponding scalar\nfunctions for the total variation distance, the Kullback-Leibler divergence,\n  the squared Hellinger divergence, and the Jensen-Shannon divergence among\nothers. Next, we give conditions to expand the $f$-divergences as converging\ninfinite series of higher-order power chi divergences, and illustrate the\ncriterion for converging Taylor series expressing the $f$-divergences between\nCauchy distributions. We then show that the symmetric property of\n$f$-divergences holds for multivariate location-scale families with prescribed\nmatrix scales provided that the standard density is even which includes the\ncases of the multivariate normal and Cauchy families. However, the\n$f$-divergences between multivariate Cauchy densities with different scale\nmatrices are shown asymmetric. Finally, we present several metrizations of\n$f$-divergences between univariate Cauchy distributions and further report\ngeometric embedding properties of these metrics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:10:35 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 03:57:51 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 03:38:24 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 08:07:42 GMT"}, {"version": "v5", "created": "Fri, 25 Jun 2021 00:46:58 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Nielsen", "Frank", ""], ["Okamura", "Kazuki", ""]]}, {"id": "2101.12525", "submitter": "Corinne Emmenegger", "authors": "Corinne Emmenegger and Peter B\\\"uhlmann", "title": "Regularizing Double Machine Learning in Partially Linear Endogenous\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the linear coefficient in a partially linear model with\nconfounding variables. We rely on double machine learning (DML) and extend it\nwith an additional regularization and selection scheme. We allow for more\ngeneral dependence structures among the model variables than what has been\ninvestigated previously, and we prove that this DML estimator remains\nasymptotically Gaussian and converges at the parametric rate. The DML estimator\nhas a two-stage least squares interpretation and may produce overly wide\nconfidence intervals. To address this issue, we propose the\nregularization-selection regsDML method that leads to narrower confidence\nintervals. It is fully data driven and optimizes an estimated asymptotic mean\nsquared error of the coefficient estimate. Empirical examples demonstrate our\nmethodological and theoretical developments. Software code for our regsDML\nmethod will be made available in the R-package dmlalg.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 11:24:18 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Emmenegger", "Corinne", ""], ["B\u00fchlmann", "Peter", ""]]}]