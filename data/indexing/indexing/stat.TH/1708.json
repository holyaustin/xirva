[{"id": "1708.00002", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Gautam Kamath, John Wright", "title": "Which Distribution Distances are Sublinearly Testable?", "comments": "To appear in SODA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples from an unknown distribution $p$ and a description of a\ndistribution $q$, are $p$ and $q$ close or far? This question of \"identity\ntesting\" has received significant attention in the case of testing whether $p$\nand $q$ are equal or far in total variation distance. However, in recent work,\nthe following questions have been been critical to solving problems at the\nfrontiers of distribution testing:\n  -Alternative Distances: Can we test whether $p$ and $q$ are far in other\ndistances, say Hellinger?\n  -Tolerance: Can we test when $p$ and $q$ are close, rather than equal? And if\nso, close in which distances?\n  Motivated by these questions, we characterize the complexity of distribution\ntesting under a variety of distances, including total variation, $\\ell_2$,\nHellinger, Kullback-Leibler, and $\\chi^2$. For each pair of distances $d_1$ and\n$d_2$, we study the complexity of testing if $p$ and $q$ are close in $d_1$\nversus far in $d_2$, with a focus on identifying which problems allow strongly\nsublinear testers (i.e., those with complexity $O(n^{1 - \\gamma})$ for some\n$\\gamma > 0$ where $n$ is the size of the support of the distributions $p$ and\n$q$). We provide matching upper and lower bounds for each case. We also study\nthese questions in the case where we only have samples from $q$ (equivalence\ntesting), showing qualitative differences from identity testing in terms of\nwhen tolerance can be achieved. Our algorithms fall into the classical paradigm\nof $\\chi^2$-statistics, but require crucial changes to handle the challenges\nintroduced by each distance we consider. Finally, we survey other recent\nresults in an attempt to serve as a reference for the complexity of various\ndistribution testing problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 18:00:00 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 02:44:14 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""], ["Wright", "John", ""]]}, {"id": "1708.00059", "submitter": "Min Ye", "authors": "Min Ye and Alexander Barg", "title": "Asymptotically optimal private estimation under mean square loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimax estimation problem of a discrete distribution with\nsupport size $k$ under locally differential privacy constraints. A\nprivatization scheme is applied to each raw sample independently, and we need\nto estimate the distribution of the raw samples from the privatized samples. A\npositive number $\\epsilon$ measures the privacy level of a privatization\nscheme.\n  In our previous work (arXiv:1702.00610), we proposed a family of new\nprivatization schemes and the corresponding estimator. We also proved that our\nscheme and estimator are order optimal in the regime $e^{\\epsilon} \\ll k$ under\nboth $\\ell_2^2$ and $\\ell_1$ loss. In other words, for a large number of\nsamples the worst-case estimation loss of our scheme was shown to differ from\nthe optimal value by at most a constant factor. In this paper, we eliminate\nthis gap by showing asymptotic optimality of the proposed scheme and estimator\nunder the $\\ell_2^2$ (mean square) loss. More precisely, we show that for any\n$k$ and $\\epsilon,$ the ratio between the worst-case estimation loss of our\nscheme and the optimal value approaches $1$ as the number of samples tends to\ninfinity.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 20:31:03 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Ye", "Min", ""], ["Barg", "Alexander", ""]]}, {"id": "1708.00078", "submitter": "Veronika Rockova", "authors": "Stephanie van der Pas and Veronika Rockova", "title": "Bayesian Dyadic Trees and Histograms for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tools for regression are based on recursive\npartitioning of the covariate space into smaller regions, where the regression\nfunction can be estimated locally. Among these, regression trees and their\nensembles have demonstrated impressive empirical performance. In this work, we\nshed light on the machinery behind Bayesian variants of these methods. In\nparticular, we study Bayesian regression histograms, such as Bayesian dyadic\ntrees, in the simple regression case with just one predictor. We focus on the\nreconstruction of regression surfaces that are piecewise constant, where the\nnumber of jumps is unknown. We show that with suitably designed priors,\nposterior distributions concentrate around the true step regression function at\na near-minimax rate. These results do not require the knowledge of the true\nnumber of steps, nor the width of the true partitioning cells. Thus, Bayesian\ndyadic regression trees are fully adaptive and can recover the true piecewise\nregression function nearly as well as if we knew the exact number and location\nof jumps. Our results constitute the first step towards understanding why\nBayesian trees and their ensembles have worked so well in practice. As an\naside, we discuss prior distributions on balanced interval partitions and how\nthey relate to an old problem in geometric probability. Namely, we relate the\nprobability of covering the circumference of a circle with random arcs whose\nendpoints are confined to a grid, a new variant of the original problem.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 21:51:23 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 22:47:19 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["van der Pas", "Stephanie", ""], ["Rockova", "Veronika", ""]]}, {"id": "1708.00100", "submitter": "Xin Wang", "authors": "Xin Wang, Vivekananda Roy", "title": "Analysis of the Polya-Gamma block Gibbs sampler for Bayesian logistic\n  linear mixed models", "comments": "10pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we construct a two-block Gibbs sampler using Polson et al.\n(2013) data augmentation technique with Polya-Gamma latent variables for\nBayesian logistic linear mixed models under proper priors. Furthermore, we\nprove the uniform ergodicity of this Gibbs sampler, which guarantees the\nexistence of the central limit theorems for MCMC based estimators.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:33:53 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 16:56:30 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Wang", "Xin", ""], ["Roy", "Vivekananda", ""]]}, {"id": "1708.00145", "submitter": "Rohit Patra", "authors": "Arun K. Kuchibhotla, Rohit K. Patra, and Bodhisattva Sen", "title": "Semiparametric Efficiency in Convexity Constrained Single Index Model", "comments": "Removed the density bounded away from zero assumption in assumption\n  (A5). Weakened assumption (B2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference in a single index regression model with\nan unknown convex link function. We introduce a convex and Lipschitz\nconstrained least squares estimator (CLSE) for both the parametric and the\nnonparametric components given independent and identically distributed\nobservations. We prove the consistency and find the rates of convergence of the\nCLSE when the errors are assumed to have only $q \\ge 2$ moments and are allowed\nto depend on the covariates. When $q\\ge 5$, we establish $n^{-1/2}$-rate of\nconvergence and asymptotic normality of the estimator of the parametric\ncomponent. Moreover, the CLSE is proved to be semiparametrically efficient if\nthe errors happen to be homoscedastic. {We develop and implement a numerically\nstable and computationally fast algorithm to compute our proposed estimator in\nthe R package~\\texttt{simest}}. We illustrate our methodology through extensive\nsimulations and data analysis. Finally, our proof of efficiency is geometric\nand provides a general framework that can be used to prove efficiency of\nestimators in a wide variety of semiparametric models even when they do not\nsatisfy the efficient score equation directly.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 03:21:11 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 20:14:16 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 19:15:13 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 19:39:24 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Patra", "Rohit K.", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1708.00205", "submitter": "Binyan Jiang", "authors": "Binyan Jiang, Ziqi Chen, Chenlei Leng", "title": "Dynamic Linear Discriminant Analysis in High Dimensional Space", "comments": "34 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data that evolve dynamically feature predominantly in the\nmodern data era. As a partial response to this, recent years have seen\nincreasing emphasis to address the dimensionality challenge. However, the\nnon-static nature of these datasets is largely ignored. This paper addresses\nboth challenges by proposing a novel yet simple dynamic linear programming\ndiscriminant (DLPD) rule for binary classification. Different from the usual\nstatic linear discriminant analysis, the new method is able to capture the\nchanging distributions of the underlying populations by modeling their means\nand covariances as smooth functions of covariates of interest. Under an\napproximate sparse condition, we show that the conditional misclassification\nrate of the DLPD rule converges to the Bayes risk in probability uniformly over\nthe range of the variables used for modeling the dynamics, when the\ndimensionality is allowed to grow exponentially with the sample size. The\nminimax lower bound of the estimation of the Bayes risk is also established,\nimplying that the misclassification rate of our proposed rule is minimax-rate\noptimal. The promising performance of the DLPD rule is illustrated via\nextensive simulation studies and the analysis of a breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 08:49:13 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 02:28:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 03:27:43 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Jiang", "Binyan", ""], ["Chen", "Ziqi", ""], ["Leng", "Chenlei", ""]]}, {"id": "1708.00294", "submitter": "Megan Owen", "authors": "Daniel G. Brown and Megan Owen", "title": "Mean and Variance of Phylogenetic Trees", "comments": "26 pages, 12 figures; revisions include new dataset, improved\n  exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the use of the Frechet mean and variance in the\nBillera-Holmes-Vogtmann (BHV) treespace to summarize and explore the diversity\nof a set of phylogenetic trees. We show that the Frechet mean is comparable to\nother summary methods, and, despite its stickiness property, is more likely to\nbe binary than the majority-rules consensus tree. We show that the Frechet\nvariance is faster and more precise than commonly used variance measures. The\nFrechet mean and variance are more theoretically justified, and more robust,\nthan previous estimates of this type, and can be estimated reasonably\nefficiently, providing a foundation for building more advanced statistical\nmethods and leading to applications such as mean hypothesis testing.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 12:59:02 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 20:28:04 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Brown", "Daniel G.", ""], ["Owen", "Megan", ""]]}, {"id": "1708.00430", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Breaking the curse of dimensionality in regression", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with many signals, high-dimensional models, often impose structures on\nthe signal strengths. The common assumption is that only a few signals are\nstrong and most of the signals are zero or close (collectively) to zero.\nHowever, such a requirement might not be valid in many real-life applications.\nIn this article, we are interested in conducting large-scale inference in\nmodels that might have signals of mixed strengths. The key challenge is that\nthe signals that are not under testing might be collectively non-negligible\n(although individually small) and cannot be accurately learned. This article\ndevelops a new class of tests that arise from a moment matching formulation. A\nvirtue of these moment-matching statistics is their ability to borrow strength\nacross features, adapt to the sparsity size and exert adjustment for testing\ngrowing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test\nharvests effective sparsity structures with hypothesis formulation for an\nefficient multiple testing procedure. Simulated data showcase that GRIPs error\ncontrol is far better than the alternative methods. We develop a minimax\ntheory, demonstrating optimality of GRIP for a broad range of models, including\nthose where the model is a mixture of a sparse and high-dimensional dense\nsignals.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 17:39:00 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1708.00502", "submitter": "Xiaohan Wei", "authors": "Stanislav Minsker, Xiaohan Wei", "title": "Estimation of the covariance structure of heavy-tailed distributions", "comments": "Fixed some minor typos and inconsistencies", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new estimator of the covariance matrix that admits\nstrong theoretical guarantees under weak assumptions on the underlying\ndistribution, such as existence of moments of only low order. While estimation\nof covariance matrices corresponding to sub-Gaussian distributions is\nwell-understood, much less in known in the case of heavy-tailed data. As K.\nBalasubramanian and M. Yuan write, \"data from real-world experiments oftentimes\ntend to be corrupted with outliers and/or exhibit heavy tails. In such cases,\nit is not clear that those covariance matrix estimators .. remain optimal\" and\n\"..what are the other possible strategies to deal with heavy tailed\ndistributions warrant further studies.\" We make a step towards answering this\nquestion and prove tight deviation inequalities for the proposed estimator that\ndepend only on the parameters controlling the \"intrinsic dimension\" associated\nto the covariance matrix (as opposed to the dimension of the ambient space); in\nparticular, our results are applicable in the case of high-dimensional\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 20:30:36 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 17:17:25 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 03:26:21 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Minsker", "Stanislav", ""], ["Wei", "Xiaohan", ""]]}, {"id": "1708.00689", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Dirichlet Bayesian Network Scores and the Maximum Relative Entropy\n  Principle", "comments": "20 pages, 4 figures; extended version submitted to Behaviormetrika", "journal-ref": "Journal of Machine Learning Research (73, Proceedings Track, AMBN\n  2017), 8-20; extended version in Behaviormetrika, 45(2), 337-362", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic approach for learning Bayesian networks from data is to identify a\nmaximum a posteriori (MAP) network structure. In the case of discrete Bayesian\nnetworks, MAP networks are selected by maximising one of several possible\nBayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet\nequivalent uniform (BDeu) score from Heckerman et al (1995). The key properties\nof BDeu arise from its uniform prior over the parameters of each local\ndistribution in the network, which makes structure learning computationally\nefficient; it does not require the elicitation of prior knowledge from experts;\nand it satisfies score equivalence.\n  In this paper we will review the derivation and the properties of BD scores,\nand of BDeu in particular, and we will link them to the corresponding entropy\nestimates to study them from an information theoretic perspective. To this end,\nwe will work in the context of the foundational work of Giffin and Caticha\n(2007), who showed that Bayesian inference can be framed as a particular case\nof the maximum relative entropy principle. We will use this connection to show\nthat BDeu should not be used for structure learning from sparse data, since it\nviolates the maximum relative entropy principle; and that it is also\nproblematic from a more classic Bayesian model selection perspective, because\nit produces Bayes factors that are sensitive to the value of its only\nhyperparameter. Using a large simulation study, we found in our previous work\n(Scutari, 2016) that the Bayesian Dirichlet sparse (BDs) score seems to provide\nbetter accuracy in structure learning; in this paper we further show that BDs\ndoes not suffer from the issues above, and we recommend to use it for sparse\ndata instead of BDeu. Finally, will show that these issues are in fact\ndifferent aspects of the same problem and a consequence of the distributional\nassumptions of the prior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 10:30:21 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 22:28:50 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 09:27:57 GMT"}, {"version": "v5", "created": "Mon, 5 Mar 2018 11:34:07 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1708.00711", "submitter": "Laura Turbatu", "authors": "Laura Turbatu", "title": "Accuracy and validity of posterior distributions using the Cressie-Read\n  empirical likelihoods", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of Cressie-Read empirical likelihoods are constructed with weights\nderived at a minimum distance from the empirical distribution in the\nCressie-Read family of divergences indexed by $\\gamma$ under the constraint of\nan unbiased set of $M$-estimating equations. At first order, they provide valid\nposterior probability statements for any given prior, but the bias in coverage\nof the resulting empirical quantile is inversely proportional to the asymptotic\nefficiency of the corresponding $M$-estimator. The Cressie-Read empirical\nlikelihoods based on the maximum likelihood estimating equations bring about\nquantiles covering with $O(n^{-1})$ accuracy at the underlying posterior\ndistribution. The choice of $\\gamma$ has an impact on the variance in small\nsamples of the posterior quantile function. Examples are given for the $M$-type\nestimating equations of location and for the quasi-likelihood functions in the\ngeneralized linear models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 11:49:55 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 09:50:28 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Turbatu", "Laura", ""]]}, {"id": "1708.00847", "submitter": "Piotr Zwiernik", "authors": "Piotr Zwiernik", "title": "Latent tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent tree models are graphical models defined on trees, in which only a\nsubset of variables is observed. They were first discussed by Judea Pearl as\ntree-decomposable distributions to generalise star-decomposable distributions\nsuch as the latent class model. Latent tree models, or their submodels, are\nwidely used in: phylogenetic analysis, network tomography, computer vision,\ncausal modeling, and data clustering. They also contain other well-known\nclasses of models like hidden Markov models, Brownian motion tree model, the\nIsing model on a tree, and many popular models used in phylogenetics. This\narticle offers a concise introduction to the theory of latent tree models. We\nemphasise the role of tree metrics in the structural description of this model\nclass, in designing learning algorithms, and in understanding fundamental\nlimits of what and when can be learned.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:43:29 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Zwiernik", "Piotr", ""]]}, {"id": "1708.00952", "submitter": "Alon Kipnis", "authors": "Alon Kipnis, John C. Duchi", "title": "Mean Estimation from Adaptive One-bit Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean of a normal distribution under\nthe following constraint: the estimator can access only a single bit from each\nsample from this distribution. We study the squared error risk in this\nestimation as a function of the number of samples and one-bit measurements $n$.\nWe consider an adaptive estimation setting where the single-bit sent at step\n$n$ is a function of both the new sample and the previous $n-1$ acquired bits.\nFor this setting, we show that no estimator can attain asymptotic mean squared\nerror smaller than $\\pi/(2n)+O(n^{-2})$ times the variance. In other words,\none-bit restriction increases the number of samples required for a prescribed\naccuracy of estimation by a factor of at least $\\pi/2$ compared to the\nunrestricted case. In addition, we provide an explicit estimator that attains\nthis asymptotic error, showing that, rather surprisingly, only $\\pi/2$ times\nmore samples are required in order to attain estimation performance equivalent\nto the unrestricted case.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 22:53:58 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 02:08:54 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Kipnis", "Alon", ""], ["Duchi", "John C.", ""]]}, {"id": "1708.01404", "submitter": "Xu He", "authors": "Xu He", "title": "Sliced rotated sphere packing designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling designs are popular choices for computer experiments. A sliced\ndesign is a design that can be partitioned into several subdesigns. We propose\na new type of sliced space-filling design called sliced rotated sphere packing\ndesigns. Their full designs and subdesigns are rotated sphere packing designs.\nThey are constructed by rescaling, rotating, translating and extracting the\npoints from a sliced lattice. We provide two fast algorithms to generate such\ndesigns. Furthermore, we propose a strategy to use sliced rotated sphere\npacking designs adaptively. Under this strategy, initial runs are uniformly\ndistributed in the design space, follow-up runs are added by incorporating\ninformation gained from initial runs, and the combined design is space-filling\nfor any local region. Examples are given to illustrate its potential\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 07:33:20 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["He", "Xu", ""]]}, {"id": "1708.01505", "submitter": "Kam Chung Wong", "authors": "Kam Chung Wong, Zifan Li and Ambuj Tewari", "title": "Lasso Guarantees for $ \\beta $-Mixing Heavy Tailed Time Series", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.04265", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theoretical results for the lasso require the samples to be iid. Recent\nwork has provided guarantees for the lasso assuming that the time series is\ngenerated by a sparse Vector Auto-Regressive (VAR) model with Gaussian\ninnovations. Proofs of these results rely critically on the fact that the true\ndata generating mechanism (DGM) is a finite-order Gaussian VAR. This assumption\nis quite brittle: linear transformations, including selecting a subset of\nvariables, can lead to the violation of this assumption. In order to break free\nfrom such assumptions, we derive non-asymptotic inequalities for estimation\nerror and prediction error of the lasso estimate of the best linear predictor\nwithout assuming any special parametric form of the DGM. Instead, we rely only\non (strict) stationarity and geometrically decaying \\b{eta}-mixing coefficients\nto establish error bounds for the lasso for subweibull random vectors. The\nclass of subweibull random variables that we introduce includes subgaussian and\nsubexponential random variables but also includes random variables with tails\nheavier than an exponential. We also show that, for Gaussian processes, the\n\\b{eta}-mixing condition can be relaxed to summability of the {\\alpha}-mixing\ncoefficients. Our work provides an alternative proof of the consistency of the\nlasso for sparse Gaussian VAR models. But the applicability of our results\nextends to non-Gaussian and non-linear times series models as the examples we\nprovide demonstrate.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:43:44 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 14:24:09 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Wong", "Kam Chung", ""], ["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1708.01582", "submitter": "Nick Whiteley Prof.", "authors": "Nick Whiteley", "title": "Dimension-free Wasserstein contraction of nonlinear filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class of partially observed diffusions, conditions are given for the\nmap from the initial condition of the signal to filtering distribution to be\ncontractive with respect to Wasserstein distances, with rate which does not\nnecessarily depend on the dimension of the state-space. The main assumptions\nare that the signal has affine drift and constant diffusion coefficient and\nthat the likelihood functions are log-concave. Ergodic and nonergodic signals\nare handled in a single framework. Examples include linear-Gaussian, stochastic\nvolatility, neural spike-train and dynamic generalized linear models. For these\nexamples filter stability can be established without any assumptions on the\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 16:46:08 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 10:50:18 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 21:56:50 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 09:57:29 GMT"}, {"version": "v5", "created": "Tue, 19 Jan 2021 15:04:13 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Whiteley", "Nick", ""]]}, {"id": "1708.01634", "submitter": "Jianhua Shi", "authors": "Huijuan Ma, Jianhua Shi, Yong Zhou", "title": "Proportional Mean Residual Life Model with Censored Survival Data under\n  Case-cohort Design", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional mean residual life model is studied for analysing survival data\nfrom the case-cohort design. To simultaneously estimate the regression\nparameters and the baseline mean residual life function, weighted estimating\nequations based on an inverse selection probability are proposed. The resulting\nregression coefficients estimates are shown to be consistent and asymptotic\nnormal with easily estimated variance-covariance. Simulation studies show that\nthe proposed estimators perform very well. An application to a real dataset\nfrom the South Welsh nickel refiners study is also given to illustrate the\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:11:52 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 12:52:43 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Ma", "Huijuan", ""], ["Shi", "Jianhua", ""], ["Zhou", "Yong", ""]]}, {"id": "1708.01686", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Joseph H. T. Kim", "title": "Exponentiated Generalized Pareto Distribution: Properties and\n  applications towards Extreme Value Theory", "comments": "24 pages, 10 figures, To appear in the proceedings of 2017 Joint\n  Statistical Meetings, Baltimore, Maryland", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generalized Pareto Distribution (GPD) plays a central role in modelling\nheavy tail phenomena in many applications. Applying the GPD to actual datasets\nhowever is a non-trivial task. One common way suggested in the literature to\ninvestigate the tail behaviour is to take logarithm to the original dataset in\norder to reduce the sample variability. Inspired by this, we propose and study\nthe Exponentiated Generalized Pareto Distribution (exGPD), which is created via\nlog-transform of the GPD variable. After introducing the exGPD we derive\nvarious distributional quantities, including the moment generating function,\ntail risk measures. As an application we also develop a plot as an alternative\nto the Hill plot to identify the tail index of heavy tailed datasets, based on\nthe moment matching for the exGPD. Various numerical analyses with both\nsimulated and actual datasets show that the proposed plot works well.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 23:43:00 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Lee", "Se Yoon", ""], ["Kim", "Joseph H. T.", ""]]}, {"id": "1708.01861", "submitter": "Kohei Miyaguchi", "authors": "Kohei Miyaguchi", "title": "Normalized Maximum Likelihood with Luckiness for Multivariate Normal\n  Distributions", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized maximum likelihood (NML) is one of the most important\ndistribution in coding theory and statistics. NML is the unique solution (if\nexists) to the pointwise minimax regret problem. However, NML is not defined\neven for simple family of distributions such as the normal distributions. Since\nthere does not exist any meaningful minimax-regret distribution for such case,\nit is pointed out that NML with luckiness (LNML) can be employed as an\nalternative to NML. In this paper, we develop the closed form of LNMLs for\nmultivariate normal distributions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 08:22:40 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 00:51:54 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 05:43:01 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Miyaguchi", "Kohei", ""]]}, {"id": "1708.01909", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo, Vincent Rivoirard, Judith Rousseau", "title": "Adaptive Supremum Norm Posterior Contraction: Wavelet Spike-and-Slab and\n  Anisotropic Besov Spaces", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supremum norm loss is intuitively more meaningful to quantify function\nestimation error in statistics. In the context of multivariate nonparametric\nregression with unknown error, we propose a Bayesian procedure based on\nspike-and-slab prior and wavelet projections to estimate the regression\nfunction and all its mixed partial derivatives. We show that their posterior\ndistributions contract to the truth optimally and adaptively under\nsupremum-norm loss. The master theorem through tests with exponential errors\nused in Bayesian nonparametrics was not adequate to deal with this problem, and\nwe developed a new idea such that posterior under the regression model is\nsystematically reduced to a posterior arising from some quasi-white noise\nmodel, where the latter model greatly simplifies our rate calculations. Hence,\nthis paper takes the first step in showing explicitly how one can translate\nresults from white noise to regression model in a Bayesian setting.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 16:59:36 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 20:38:58 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Yoo", "William Weimin", ""], ["Rivoirard", "Vincent", ""], ["Rousseau", "Judith", ""]]}, {"id": "1708.01974", "submitter": "David Frazier", "authors": "David T. Frazier, Christian P. Robert and Judith Rousseau", "title": "Model Misspecification in ABC: Consequences and Diagnostics", "comments": null, "journal-ref": null, "doi": "10.1111/369--7412/20/82421", "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the behavior of approximate Bayesian computation (ABC) when the\nmodel generating the simulated data differs from the actual data generating\nprocess; i.e., when the data simulator in ABC is misspecified. We demonstrate\nboth theoretically and in simple, but practically relevant, examples that when\nthe model is misspecified different versions of ABC can yield substantially\ndifferent results. Our theoretical results demonstrate that even though the\nmodel is misspecified, under regularity conditions, the accept/reject ABC\napproach concentrates posterior mass on an appropriately defined pseudo-true\nparameter value. However, under model misspecification the ABC posterior does\nnot yield credible sets with valid frequentist coverage and has non-standard\nasymptotic behavior. In addition, we examine the theoretical behavior of the\npopular local regression adjustment to ABC under model misspecification and\ndemonstrate that this approach concentrates posterior mass on a completely\ndifferent pseudo-true value than accept/reject ABC. Using our theoretical\nresults, we suggest two approaches to diagnose model misspecification in ABC.\nAll theoretical results and diagnostics are illustrated in a simple running\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 03:10:04 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 05:35:22 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 05:53:48 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 21:58:00 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Frazier", "David T.", ""], ["Robert", "Christian P.", ""], ["Rousseau", "Judith", ""]]}, {"id": "1708.02102", "submitter": "Kari Lock Morgan", "authors": "Kari Lock Morgan", "title": "Reallocating and Resampling: A Comparison for Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based inference plays a major role in modern statistics, and often\nemploys either reallocating (as in a randomization test) or resampling (as in\nbootstrapping). Reallocating mimics random allocation to treatment groups,\nwhile resampling mimics random sampling from a larger population; does it\nmatter whether the simulation method matches the data collection method?\nMoreover, do the results differ for testing versus estimation? Here we answer\nthese questions in a simple setting by exploring the distribution of a sample\ndifference in means under a basic two group design and four different\nscenarios: true random allocation, true random sampling, reallocating, and\nresampling. For testing a sharp null hypothesis, reallocating is superior in\nsmall samples, but reallocating and resampling are asymptotically equivalent.\nFor estimation, resampling is generally superior, unless the effect is truly\nadditive. Moreover, these results hold regardless of whether the data were\ncollected by random sampling or random allocation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:03:43 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Morgan", "Kari Lock", ""]]}, {"id": "1708.02107", "submitter": "De CASTRO Yohann", "authors": "Yohann De Castro, Claire Lacour and Thanh Mai Pham Ngoc", "title": "Adaptive Estimation of Nonparametric Geometric Graphs", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the recovery of graphons when they are convolution\nkernels on compact (symmetric) metric spaces. This case is of particular\ninterest since it covers the situation where the probability of an edge depends\nonly on some unknown nonparametric function of the distance between latent\npoints, referred to as Nonparametric Geometric Graphs (NGG). In this setting,\nadaptive estimation of NGG is possible using a spectral procedure combined with\na Goldenshluger-Lepski adaptation method. The latent spaces covered by our\nframework encompass (among others) compact symmetric spaces of rank one, namely\nreal spheres and projective spaces. For these latter, explicit computations of\nthe eigen-basis and of the model complexity can be achieved, leading to\nquantitative non-asymptotic results. The time complexity of our method scales\ncubicly in the size of the graph and exponentially in the regularity of the\ngraphon. Hence, this paper offers an algorithmically and theoretically\nefficient procedure to estimate smooth NGG. As a by product, this paper shows a\nnon-asymptotic concentration result on the spectrum of integral operators\ndefined by symmetric kernels (not necessarily positive).\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:17:23 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 16:51:27 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 09:03:43 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 01:15:59 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["De Castro", "Yohann", ""], ["Lacour", "Claire", ""], ["Ngoc", "Thanh Mai Pham", ""]]}, {"id": "1708.02140", "submitter": "Yotam Shem-Tov", "authors": "Jasjeet S. Sekhon, Yotam Shem-Tov", "title": "Inference on a New Class of Sample Average Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new variance formulas for inference on a general class of estimands\nof causal average treatment effects in a Randomized Control Trial (RCT). We\ngeneralize Robins (1988) and show that when the estimand of interest is the\nSample Average Treatment Effect of the Treated (SATT or SATC for controls), a\nconsistent variance estimator exists. Although these estimands are equal to the\nSample Average Treatment Effect (SATE) in expectation, potentially large\ndifferences in both accuracy and coverage can occur by the change of estimand,\neven asymptotically. Inference on the SATE, even using a conservative\nconfidence interval, provides incorrect coverage of the SATT or SATC. We derive\nthe variance and limiting distribution of a new and general class of\nestimands---any mixing between SATT and SATC---for which the SATE is a specific\ncase. We demonstrate the applicability of the new theoretical results using\nMonte-Carlo simulations and an empirical application with hundreds of online\nexperiments with an average sample size of approximately one hundred million\nobservations per experiment. An R package, estCI, that implements all the\nproposed estimation procedures is available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:48:09 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 06:17:26 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Sekhon", "Jasjeet S.", ""], ["Shem-Tov", "Yotam", ""]]}, {"id": "1708.02216", "submitter": "Nina Holden", "authors": "Lisa Hartung, Nina Holden, and Yuval Peres", "title": "Trace reconstruction with varying deletion probabilities", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the trace reconstruction problem an unknown string ${\\bf\nx}=(x_0,\\dots,x_{n-1})\\in\\{0,1,...,m-1\\}^n$ is observed through the deletion\nchannel, which deletes each $x_k$ with a certain probability, yielding a\ncontracted string $\\widetilde{\\bf X}$. Earlier works have proved that if each\n$x_k$ is deleted with the same probability $q\\in[0,1)$, then $\\exp(O(n^{1/3}))$\nindependent copies of the contracted string $\\widetilde{\\bf X}$ suffice to\nreconstruct $\\bf x$ with high probability. We extend this upper bound to the\nsetting where the deletion probabilities vary, assuming certain regularity\nconditions. First we consider the case where $x_k$ is deleted with some known\nprobability $q_k$. Then we consider the case where each letter $\\zeta\\in\n\\{0,1,...,m-1\\}$ is associated with some possibly unknown deletion probability\n$q_\\zeta$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:35:40 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Hartung", "Lisa", ""], ["Holden", "Nina", ""], ["Peres", "Yuval", ""]]}, {"id": "1708.02405", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Nonparametric Poisson regression from independent and weakly dependent\n  observations by model selection", "comments": "Major revision of previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-parametric Poisson regression problem where the integer\nvalued response $Y$ is the realization of a Poisson random variable with\nparameter $\\lambda(X)$. The aim is to estimate the functional parameter\n$\\lambda$ from independent or weakly dependent observations\n$(X_1,Y_1),\\ldots,(X_n,Y_n)$ in a random design framework.\n  First we determine upper risk bounds for projection estimators on finite\ndimensional subspaces under mild conditions. In the case of Sobolev ellipsoids\nthe obtained rates of convergence turn out to be optimal.\n  The main part of the paper is devoted to the construction of adaptive\nprojection estimators of $\\lambda$ via model selection. We proceed in two\nsteps: first, we assume that an upper bound for $\\Vert \\lambda \\Vert_\\infty$ is\nknown. Under this assumption, we construct an adaptive estimator whose\ndimension parameter is defined as the minimizer of a penalized contrast\ncriterion. Second, we replace the known upper bound on $\\Vert \\lambda\n\\Vert_\\infty$ by an appropriate plug-in estimator of $\\Vert \\lambda\n\\Vert_\\infty$. The resulting adaptive estimator is shown to attain the minimax\noptimal rate up to an additional logarithmic factor both in the independent and\nthe weakly dependent setup. Appropriate concentration inequalities for Poisson\npoint processes turn out to be an important ingredient of the proofs.\n  We illustrate our theoretical findings by a short simulation study and\nconclude by indicating directions of future research.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 08:33:03 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 11:31:42 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "1708.02426", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov and Thomas Jaki", "title": "An information-theoretic approach for selecting arms in clinical trials", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of selecting the \"best\" amongst different choices is a common\nproblem in statistics. In drug development, our motivating setting, the\nquestion becomes, for example: what is the dose that gives me a pre-specified\nrisk of toxicity or which treatment gives the best response rate. Motivated by\na recent development in the weighted information measures theory, we propose an\nexperimental design based on a simple and intuitive criterion which governs arm\nselection in the experiment with multinomial outcomes. The criterion leads to\naccurate arm selection without any parametric or monotonicity assumption. The\nasymptotic properties of the design are studied for different allocation rules\nand the small sample size behaviour is evaluated in simulations in the context\nof Phase I and Phase II clinical trials with binary endpoints. We compare the\nproposed design to currently used alternatives and discuss its practical\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:38:23 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 11:01:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""]]}, {"id": "1708.02516", "submitter": "Han Cheng Lie", "authors": "Han Cheng Lie and T. J. Sullivan", "title": "Equivalence of weak and strong modes of measures on topological vector\n  spaces", "comments": "22 pages, 3 figures", "journal-ref": "Inverse Problems (2018)", "doi": "10.1088/1361-6420/aadef2", "report-no": null, "categories": "math.FA math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strong mode of a probability measure on a normed space $X$ can be defined\nas a point $u$ such that the mass of the ball centred at $u$ uniformly\ndominates the mass of all other balls in the small-radius limit. Helin and\nBurger weakened this definition by considering only pairwise comparisons with\nballs whose centres differ by vectors in a dense, proper linear subspace $E$ of\n$X$, and posed the question of when these two types of modes coincide. We show\nthat, in a more general setting of metrisable vector spaces equipped with\nmeasures that are finite on bounded sets, the density of $E$ and a uniformity\ncondition suffice for the equivalence of these two types of modes. We\naccomplish this by introducing a new, intermediate type of mode. We also show\nthat these modes can be inequivalent if the uniformity condition fails. Our\nresults shed light on the relationships between among various notions of\nmaximum a posteriori estimator in non-parametric Bayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:16:29 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 12:31:58 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 15:44:17 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2018 13:07:53 GMT"}, {"version": "v5", "created": "Sat, 18 Aug 2018 12:11:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "Han Cheng", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1708.02564", "submitter": "Guang Cheng", "authors": "Ying Zhu, Zhuqing Yu, Guang Cheng", "title": "High Dimensional Inference in Partially Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two semiparametric versions of the debiased Lasso procedure for\nthe model $Y_i = X_i\\beta_0 + g_0(Z_i) + \\epsilon_i$, where $\\beta_0$ is high\ndimensional but sparse (exactly or approximately). Both versions are shown to\nhave the same asymptotic normal distribution and do not require the minimal\nsignal condition for statistical inference of any component in $\\beta_0$. Our\nmethod also works when $Z_i$ is high dimensional provided that the function\nclasses $E(X_{ij} |Z_i)$s and $E(Y_i|Z_i)$ belong to exhibit certain sparsity\nfeatures, e.g., a sparse additive decomposition structure. We further develop a\nsimultaneous hypothesis testing procedure based on multiplier bootstrap. Our\ntesting method automatically takes into account of the dependence structure\nwithin the debiased estimates, and allows the number of tested components to be\nexponentially high.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 17:13:57 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Zhu", "Ying", ""], ["Yu", "Zhuqing", ""], ["Cheng", "Guang", ""]]}, {"id": "1708.02691", "submitter": "Boris Hanin", "authors": "Boris Hanin", "title": "Universal Function Approximation by Deep Neural Nets with Bounded Width\n  and ReLU Activations", "comments": "v3. Theorem 3 removed. Comments Welcome. 9p", "journal-ref": "Mathematics 2019, 7(10), 992", "doi": "10.3390/math7100992", "report-no": null, "categories": "stat.ML cs.CG cs.LG math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns the expressive power of depth in neural nets with ReLU\nactivations and bounded width. We are particularly interested in the following\nquestions: what is the minimal width $w_{\\text{min}}(d)$ so that ReLU nets of\nwidth $w_{\\text{min}}(d)$ (and arbitrary depth) can approximate any continuous\nfunction on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this\nminimal width, what can one say about the depth necessary to approximate a\ngiven function? Our approach to this paper is based on the observation that,\ndue to the convexity of the ReLU activation, ReLU nets are particularly\nwell-suited for representing convex functions. In particular, we prove that\nReLU nets with width $d+1$ can approximate any continuous convex function of\n$d$ variables arbitrarily well. These results then give quantitative depth\nestimates for the rate of approximation of any continuous scalar function on\nthe $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 01:37:21 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 22:40:55 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 17:38:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hanin", "Boris", ""]]}, {"id": "1708.02705", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Kengo Kato", "title": "Jackknife multiplier bootstrap: finite sample approximations to the\n  $U$-process supremum with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with finite sample approximations to the supremum of\na non-degenerate $U$-process of a general order indexed by a function class. We\nare primarily interested in situations where the function class as well as the\nunderlying distribution change with the sample size, and the $U$-process itself\nis not weakly convergent as a process. Such situations arise in a variety of\nmodern statistical problems. We first consider Gaussian approximations, namely,\napproximate the $U$-process supremum by the supremum of a Gaussian process, and\nderive coupling and Kolmogorov distance bounds. Such Gaussian approximations\nare, however, not often directly applicable in statistical problems since the\ncovariance function of the approximating Gaussian process is unknown. This\nmotivates us to study bootstrap-type approximations to the $U$-process\nsupremum. We propose a novel jackknife multiplier bootstrap (JMB) tailored to\nthe $U$-process, and derive coupling and Kolmogorov distance bounds for the\nproposed JMB method. All these results are non-asymptotic, and established\nunder fairly general conditions on function classes and underlying\ndistributions. Key technical tools in the proofs are new local maximal\ninequalities for $U$-processes, which may be useful in other problems. We also\ndiscuss applications of the general approximation results to testing for\nqualitative features of nonparametric functions based on generalized local\n$U$-processes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 03:43:03 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:04:34 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 15:36:27 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2019 02:22:27 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1708.02708", "submitter": "Caroline Uhler", "authors": "Caroline Uhler and Donald Richards", "title": "Generalized Fr\\'echet Bounds for Cell Entries in Multidimensional\n  Contingency Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the lattice, $\\mathcal{L}$, of all subsets of a multidimensional\ncontingency table and establish the properties of monotonicity and\nsupermodularity for the marginalization function, $n(\\cdot)$, on $\\mathcal{L}$.\nWe derive from the supermodularity of $n(\\cdot)$ some generalized Fr\\'echet\ninequalities complementing and extending inequalities of Dobra and Fienberg.\nFurther, we construct new monotonic and supermodular functions from $n(\\cdot)$,\nand we remark on the connection between supermodularity and some correlation\ninequalities for probability distributions on lattices. We also apply an\ninequality of Ky Fan to derive a new approach to Fr\\'echet inequalities for\nmultidimensional contingency tables.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 04:02:00 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 05:49:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Uhler", "Caroline", ""], ["Richards", "Donald", ""]]}, {"id": "1708.02728", "submitter": "John Peebles", "authors": "Ilias Diakonikolas, Themis Gouleakis, John Peebles, Eric Price", "title": "Optimal Identity Testing with High Probability", "comments": null, "journal-ref": "ICALP 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing identity against a given distribution with a\nfocus on the high confidence regime. More precisely, given samples from an\nunknown distribution $p$ over $n$ elements, an explicitly given distribution\n$q$, and parameters $0< \\epsilon, \\delta < 1$, we wish to distinguish, {\\em\nwith probability at least $1-\\delta$}, whether the distributions are identical\nversus $\\varepsilon$-far in total variation distance. Most prior work focused\non the case that $\\delta = \\Omega(1)$, for which the sample complexity of\nidentity testing is known to be $\\Theta(\\sqrt{n}/\\epsilon^2)$. Given such an\nalgorithm, one can achieve arbitrarily small values of $\\delta$ via black-box\namplification, which multiplies the required number of samples by\n$\\Theta(\\log(1/\\delta))$.\n  We show that black-box amplification is suboptimal for any $\\delta = o(1)$,\nand give a new identity tester that achieves the optimal sample complexity. Our\nnew upper and lower bounds show that the optimal sample complexity of identity\ntesting is \\[\n  \\Theta\\left( \\frac{1}{\\epsilon^2}\\left(\\sqrt{n \\log(1/\\delta)} +\n\\log(1/\\delta) \\right)\\right) \\] for any $n, \\varepsilon$, and $\\delta$. For\nthe special case of uniformity testing, where the given distribution is the\nuniform distribution $U_n$ over the domain, our new tester is surprisingly\nsimple: to test whether $p = U_n$ versus $d_{\\mathrm TV}(p, U_n) \\geq\n\\varepsilon$, we simply threshold $d_{\\mathrm TV}(\\widehat{p}, U_n)$, where\n$\\widehat{p}$ is the empirical probability distribution. The fact that this\nsimple \"plug-in\" estimator is sample-optimal is surprising, even in the\nconstant $\\delta$ case. Indeed, it was believed that such a tester would not\nattain sublinear sample complexity even for constant values of $\\varepsilon$\nand $\\delta$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:17:30 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 23:23:35 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Peebles", "John", ""], ["Price", "Eric", ""]]}, {"id": "1708.02854", "submitter": "Martin Wahl", "authors": "Markus Rei\\ss, Martin Wahl", "title": "Functional estimation and hypothesis testing in nonparametric boundary\n  models", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Poisson point process with unknown support boundary curve $g$,\nwhich forms a prototype of an irregular statistical model. We address the\nproblem of estimating non-linear functionals of the form $\\int \\Phi(g(x))\\,dx$.\nFollowing a nonparametric maximum-likelihood approach, we construct an\nestimator which is UMVU over H\\\"older balls and achieves the (local) minimax\nrate of convergence. These results hold under weak assumptions on $\\Phi$ which\nare satisfied for $\\Phi(u)=|u|^p$, $p\\ge 1$. As an application, we consider the\nproblem of estimating the $L^p$-norm and derive the minimax separation rates in\nthe corresponding nonparametric hypothesis testing problem. Structural\ndifferences to results for regular nonparametric models are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 14:28:47 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 08:01:54 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Rei\u00df", "Markus", ""], ["Wahl", "Martin", ""]]}, {"id": "1708.02985", "submitter": "Soufiane Hayou", "authors": "Soufiane Hayou", "title": "Cleaning the correlation matrix with a denoising autoencoder", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use an adjusted autoencoder to estimate the true\neigenvalues of the population correlation matrix from the sample correlation\nmatrix when the number of samples is small. We show that the model outperforms\nthe Rotational Invariant Estimator (Bouchaud) which is the optimal estimator in\nthe sample eigenvectors basis when the dimension goes to infinity.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:54:47 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Hayou", "Soufiane", ""]]}, {"id": "1708.03046", "submitter": "Weijie J. Su", "authors": "Weijie J. Su", "title": "When Is the First Spurious Variable Selected by Sequential Regression\n  Procedures?", "comments": "Accepted by Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied statisticians use sequential regression procedures to produce a\nranking of explanatory variables and, in settings of low correlations between\nvariables and strong true effect sizes, expect that variables at the very top\nof this ranking are truly relevant to the response. In a regime of certain\nsparsity levels, however, three examples of sequential procedures--forward\nstepwise, the lasso, and least angle regression--are shown to include the first\nspurious variable unexpectedly early. We derive a rigorous, sharp prediction of\nthe rank of the first spurious variable for these three procedures,\ndemonstrating that the first spurious variable occurs earlier and earlier as\nthe regression coefficients become denser. This counterintuitive phenomenon\npersists for statistically independent Gaussian random designs and an\narbitrarily large magnitude of the true effects. We gain a better understanding\nof the phenomenon by identifying the underlying cause and then leverage the\ninsights to introduce a simple visualization tool termed the double-ranking\ndiagram to improve on sequential methods. As a byproduct of these findings, we\nobtain the first provable result certifying the exact equivalence between the\nlasso and least angle regression in the early stages of solution paths beyond\northogonal designs. This equivalence can seamlessly carry over many important\nmodel selection results concerning the lasso to least angle regression.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 01:56:16 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 16:02:59 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Su", "Weijie J.", ""]]}, {"id": "1708.03120", "submitter": "Judith Rousseau", "authors": "Fran\\c{c}ois Caron and Francesca Panero and Judith Rousseau", "title": "On sparsity, power-law and clustering properties of graphex processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates properties of the class of graphs based on\nexchangeable point processes. We provide asymptotic expressions for the number\nof edges, number of nodes and degree distributions, identifying four regimes:\n(i) a dense regime, (ii) a sparse almost dense regime, (iii) a sparse regime\nwith power-law behaviour, and (iv) an almost extremely sparse regime. We show\nthat under mild assumptions, both the global and local clustering coefficients\nconverge to constants which may or may not be the same. We also derive a\ncentral limit theorem for the number of nodes. Finally, we propose a class of\nmodels within this framework where one can separately control the latent\nstructure and the global sparsity/power-law properties of the graph.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 08:28:49 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 17:57:31 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 15:04:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Panero", "Francesca", ""], ["Rousseau", "Judith", ""]]}, {"id": "1708.03131", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Hypotheses testing on infinite random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing on some recent results that provide the formalism necessary to\ndefinite stationarity for infinite random graphs, this paper initiates the\nstudy of statistical and learning questions pertaining to these objects.\nSpecifically, a criterion for the existence of a consistent test for complex\nhypotheses is presented, generalizing the corresponding results on time series.\nAs an application, it is shown how one can test that a tree has the Markov\nproperty, or, more generally, to estimate its memory.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:11:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1708.03272", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Leonhard Held and H{\\aa}vard Rue", "title": "Fast and accurate Bayesian model criticism and conflict diagnostics\n  using R-INLA", "comments": null, "journal-ref": "Stat 6(1):331-344, 2017", "doi": "10.1002/sta4.163", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasingly popular for realistic modelling\nand analysis of complex data. This trend is accompanied by the need for\nflexible, general, and computationally efficient methods for model criticism\nand conflict detection. Usually, a Bayesian hierarchical model incorporates a\ngrouping of the individual data points, for example individuals in repeated\nmeasurement data. In such cases, the following question arises: Are any of the\ngroups \"outliers\", or in conflict with the remaining groups? Existing general\napproaches aiming to answer such questions tend to be extremely computationally\ndemanding when model fitting is based on MCMC. We show how group-level model\ncriticism and conflict detection can be done quickly and accurately through\nintegrated nested Laplace approximations (INLA). The new method is implemented\nas a part of the open source R-INLA package for Bayesian computing\n(http://r-inla.org).\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:49:25 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 16:50:55 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 20:26:35 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 17:15:55 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Held", "Leonhard", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1708.03288", "submitter": "Peter Radchenko", "authors": "Rahul Mazumder and Peter Radchenko and Antoine Dedieu", "title": "Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is\n  low", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:28:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 18:30:37 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 04:23:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""], ["Dedieu", "Antoine", ""]]}, {"id": "1708.03359", "submitter": "Gustavo Didier", "authors": "Patrice Abry and Gustavo Didier", "title": "Wavelet eigenvalue regression for $n$-variate operator fractional\n  Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we extend the methodology proposed in Abry and Didier\n(2017) to obtain the first joint estimator of the real parts of the Hurst\neigenvalues of $n$-variate OFBM. The procedure consists of a wavelet regression\non the log-eigenvalues of the sample wavelet spectrum. The estimator is shown\nto be consistent for any time reversible OFBM and, under stronger assumptions,\nalso asymptotically normal starting from either continuous or discrete time\nmeasurements. Simulation studies establish the finite sample effectiveness of\nthe methodology and illustrate its benefits compared to univariate-like\n(entrywise) analysis. As an application, we revisit the well-known self-similar\ncharacter of Internet traffic by applying the proposed methodology to 4-variate\ntime series of modern, high quality Internet traffic data. The analysis reveals\nthe presence of a rich multivariate self-similarity structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 19:13:00 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Abry", "Patrice", ""], ["Didier", "Gustavo", ""]]}, {"id": "1708.03372", "submitter": "Matthew Reimherr", "authors": "Matthew Reimherr, Bharath Sriperumbudur, Bahaeddine Taoufik", "title": "Optimal Prediction for Additive Function-on-Function Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As with classic statistics, functional regression models are invaluable in\nthe analysis of functional data. While there are now extensive tools with\naccompanying theory available for linear models, there is still a great deal of\nwork to be done concerning nonlinear models for functional data. In this work\nwe consider the Additive Function-on-Function Regression model, a type of\nnonlinear model that uses an additive relationship between the functional\noutcome and functional covariate. We present an estimation methodology built\nupon Reproducing Kernel Hilbert Spaces, and establish optimal rates of\nconvergence for our estimates in terms of prediction error. We also discuss\ncomputational challenges that arise with such complex models, developing a\nrepresenter theorem for our estimate as well as a more practical and\ncomputationally efficient approximation. Simulations and an application to\nCumulative Intraday Returns around the 2008 financial crisis are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 20:23:24 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 12:52:41 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Reimherr", "Matthew", ""], ["Sriperumbudur", "Bharath", ""], ["Taoufik", "Bahaeddine", ""]]}, {"id": "1708.03551", "submitter": "Soufiane Hayou", "authors": "Soufiane Hayou", "title": "On the overestimation of the largest eigenvalue of a covariance matrix", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use a new approach to prove that the largest eigenvalue of\nthe sample covariance matrix of a normally distributed vector is bigger than\nthe true largest eigenvalue with probability 1 when the dimension is infinite.\nWe prove a similar result for the smallest eigenvalue.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 14:18:42 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Hayou", "Soufiane", ""]]}, {"id": "1708.03751", "submitter": "Keisuke Yano", "authors": "Keisuke Yano, Fumiyasu Komaki", "title": "On $\\varepsilon$-Admissibility in High Dimension and Nonparametrics", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the use of $\\varepsilon$-admissibility for\nestimation in high-dimensional and nonparametric statistical models. The\nminimax rate of convergence is widely used to compare the performance of\nestimators in high-dimensional and nonparametric models. However, it often\nworks poorly as a criterion of comparison. In such cases, the addition of\ncomparison by $\\varepsilon$-admissibility provides a better outcome. We\ndemonstrate the usefulness of $\\varepsilon$-admissibility through\nhigh-dimensional Poisson model and Gaussian infinite sequence model, and\npresent noble results.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 07:57:35 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yano", "Keisuke", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1708.03799", "submitter": "Joonas Sova", "authors": "J\\\"uri Lember and Joonas Sova", "title": "Existence of infinite Viterbi path for pairwise Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hidden Markov models one of the most popular estimates of the hidden\nchain is the Viterbi path -- the path maximising the posterior probability. We\nconsider a more general setting, called the pairwise Markov model, where the\njoint process consisting of finite-state hidden regime and observation process\nis assumed to be a Markov chain. We prove that under some conditions it is\npossible to extend the Viterbi path to infinity for almost every observation\nsequence which in turn enables to define an infinite Viterbi decoding of the\nobservation process, called the Viterbi process. This is done by constructing a\nblock of observations, called a barrier, which ensures that the Viterbi path\ngoes trough a given state whenever this block occurs in the observation\nsequence.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 17:29:44 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Sova", "Joonas", ""]]}, {"id": "1708.03942", "submitter": "Housen Li", "authors": "Housen Li, Qinghai Guo, Axel Munk", "title": "Multiscale Change-point Segmentation: Beyond Step Functions", "comments": null, "journal-ref": "Electronic Journal of Statistics 2019, Vol. 13, No. 2, 3254-3296", "doi": "10.1214/19-EJS1608", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multiscale type segmentation methods are known to detect multiple\nchange-points with high statistical accuracy, while allowing for fast\ncomputation. Underpinning theory has been developed mainly for models that\nassume the signal as a piecewise constant function. In this paper this will be\nextended to certain function classes beyond such step functions in a\nnonparametric regression setting, revealing certain multiscale segmentation\nmethods as robust to deviation from such piecewise constant functions. Our main\nfinding is the adaptation over such function classes for a universal\nthresholding, which includes bounded variation functions, and (piecewise)\nH\\\"{o}lder functions of smoothness order $ 0 < \\alpha \\le1$ as special cases.\nFrom this we derive statistical guarantees on feature detection in terms of\njumps and modes. Another key finding is that these multiscale segmentation\nmethods perform nearly (up to a log-factor) as well as the oracle piecewise\nconstant segmentation estimator (with known jump locations), and the best\npiecewise constant approximants of the (unknown) true signal. Theoretical\nfindings are examined by various numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 17:43:41 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 08:29:18 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 22:18:13 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Li", "Housen", ""], ["Guo", "Qinghai", ""], ["Munk", "Axel", ""]]}, {"id": "1708.03964", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "Taras Bodnar, Holger Dette and Nestor Parolya", "title": "Testing for Independence of Large Dimensional Vectors", "comments": "30 pages main, 21 pages supplement, 14 Figures (accepted for\n  publication in Annals of Statistics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new tests for the independence of two high-dimensional vectors\nare investigated. We consider the case where the dimension of the vectors\nincreases with the sample size and propose multivariate analysis of\nvariance-type statistics for the hypothesis of a block diagonal covariance\nmatrix. The asymptotic properties of the new test statistics are investigated\nunder the null hypothesis and the alternative hypothesis using random matrix\ntheory. For this purpose we study the weak convergence of linear spectral\nstatistics of central and (conditionally) non-central Fisher matrices. In\nparticular, a central limit theorem for linear spectral statistics of large\ndimensional (conditionally) non-central Fisher matrices is derived which is\nthen used to analyse the power of the tests under the alternative.\n  The theoretical results are illustrated by means of a simulation study where\nwe also compare the new tests with several alternative, in particular with the\ncommonly used corrected likelihood ratio test. It is demonstrated that the\nlatter test does not keep its nominal level, if the dimension of one sub-vector\nis relatively small compared to the dimension of the other sub-vector. On the\nother hand the tests proposed in this paper provide a reasonable approximation\nof the nominal level in such situations. Moreover, we observe that one of the\nproposed tests is most powerful under a variety of correlation scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 21:00:24 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 11:09:27 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 19:41:58 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bodnar", "Taras", ""], ["Dette", "Holger", ""], ["Parolya", "Nestor", ""]]}, {"id": "1708.04482", "submitter": "Bin Zhu", "authors": "Bin Zhu", "title": "On Vector ARMA Models Consistent with a Finite Matrix Covariance\n  Sequence", "comments": "This is a topic that has been addressed before, and hence the novelty\n  of this work is very limited", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the so called \"VARMA covariance matching problem\" and\ndemonstrate the existence of a solution using the degree theory from\ndifferential topology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 13:06:24 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:02:58 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 06:20:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhu", "Bin", ""]]}, {"id": "1708.04527", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, and Rahul Mazumder", "title": "The Trimmed Lasso: Sparsity and Robustness", "comments": "32 pages (excluding appendix); 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:56:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1708.04658", "submitter": "David Kaplan", "authors": "Matt Goldman, David M. Kaplan", "title": "Comparing distributions by multiple testing across quantiles or CDF\n  values", "comments": "under review", "journal-ref": "Journal of Econometrics 206 (2018) 143-166", "doi": "10.1016/j.jeconom.2018.04.003", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When comparing two distributions, it is often helpful to learn at which\nquantiles or values there is a statistically significant difference. This\nprovides more information than the binary \"reject\" or \"do not reject\" decision\nof a global goodness-of-fit test. Framing our question as multiple testing\nacross the continuum of quantiles $\\tau\\in(0,1)$ or values $r\\in\\mathbb{R}$, we\nshow that the Kolmogorov--Smirnov test (interpreted as a multiple testing\nprocedure) achieves strong control of the familywise error rate. However, its\nwell-known flaw of low sensitivity in the tails remains. We provide an\nalternative method that retains such strong control of familywise error rate\nwhile also having even sensitivity, i.e., equal pointwise type I error rates at\neach of $n\\to\\infty$ order statistics across the distribution. Our one-sample\nmethod computes instantly, using our new formula that also instantly computes\ngoodness-of-fit $p$-values and uniform confidence bands. To improve power, we\nalso propose stepdown and pre-test procedures that maintain control of the\nasymptotic familywise error rate. One-sample and two-sample cases are\nconsidered, as well as extensions to regression discontinuity designs and\nconditional distributions. Simulations, empirical examples, and code are\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 19:30:50 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Goldman", "Matt", ""], ["Kaplan", "David M.", ""]]}, {"id": "1708.04696", "submitter": "Cl\\'ement Canonne", "authors": "Tu\\u{g}kan Batu and Cl\\'ement L. Canonne", "title": "Generalized Uniformity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we revisit the problem of uniformity testing of discrete\nprobability distributions. A fundamental problem in distribution testing,\ntesting uniformity over a known domain has been addressed over a significant\nline of works, and is by now fully understood.\n  The complexity of deciding whether an unknown distribution is uniform over\nits unknown (and arbitrary) support, however, is much less clear. Yet, this\ntask arises as soon as no prior knowledge on the domain is available, or\nwhenever the samples originate from an unknown and unstructured universe. In\nthis work, we introduce and study this generalized uniformity testing question,\nand establish nearly tight upper and lower bound showing that -- quite\nsurprisingly -- its sample complexity significantly differs from the\nknown-domain case. Moreover, our algorithm is intrinsically adaptive, in\ncontrast to the overwhelming majority of known distribution testing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 21:32:04 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Batu", "Tu\u011fkan", ""], ["Canonne", "Cl\u00e9ment L.", ""]]}, {"id": "1708.04738", "submitter": "Mengyang Gu", "authors": "Mengyang Gu, Xiaojing Wang and James O. Berger", "title": "Robust Gaussian Stochastic Process Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of the parameters of a Gaussian Stochastic Process\n(GaSP), in the context of emulation (approximation) of computer models for\nwhich the outcomes are real-valued scalars. The main focus is on estimation of\nthe GaSP parameters through various generalized maximum likelihood methods,\nmostly involving finding posterior modes; this is because full Bayesian\nanalysis in computer model emulation is typically prohibitively expensive. The\nposterior modes that are studied arise from objective priors, such as the\nreference prior. These priors have been studied in the literature for the\nsituation of an isotropic covariance function or under the assumption of\nseparability in the design of inputs for model runs used in the GaSP\nconstruction. In this paper, we consider more general designs (e.g., a Latin\nHypercube Design) with a class of commonly used anisotropic correlation\nfunctions, which can be written as a product of isotropic correlation\nfunctions, each having an unknown range parameter and a fixed roughness\nparameter. We discuss properties of the objective priors and marginal\nlikelihoods for the parameters of the GaSP and establish the posterior\npropriety of the GaSP parameters, but our main focus is to demonstrate that\ncertain parameterizations result in more robust estimation of the GaSP\nparameters than others, and that some parameterizations that are in common use\nshould clearly be avoided. These results are applicable to many frequently used\ncovariance functions, e.g., power exponential, Mat{\\'e}rn, rational quadratic\nand spherical covariance. We also generalize the results to the GaSP model with\na nugget parameter. Both theoretical and numerical evidence is presented\nconcerning the performance of the studied procedures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:30:50 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Gu", "Mengyang", ""], ["Wang", "Xiaojing", ""], ["Berger", "James O.", ""]]}, {"id": "1708.04753", "submitter": "Yun Yang", "authors": "Yun Yang, Anirban Bhattacharya, Debdeep Pati", "title": "Frequentist coverage and sup-norm convergence rate in Gaussian process\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) regression is a powerful interpolation technique due to\nits flexibility in capturing non-linearity. In this paper, we provide a general\nframework for understanding the frequentist coverage of point-wise and\nsimultaneous Bayesian credible sets in GP regression. As an intermediate\nresult, we develop a Bernstein von-Mises type result under supremum norm in\nrandom design GP regression. Identifying both the mean and covariance function\nof the posterior distribution of the Gaussian process as regularized\n$M$-estimators, we show that the sampling distribution of the posterior mean\nfunction and the centered posterior distribution can be respectively\napproximated by two population level GPs. By developing a comparison inequality\nbetween two GPs, we provide exact characterization of frequentist coverage\nprobabilities of Bayesian point-wise credible intervals and simultaneous\ncredible bands of the regression function. Our results show that inference\nbased on GP regression tends to be conservative; when the prior is\nunder-smoothed, the resulting credible intervals and bands have minimax-optimal\nsizes, with their frequentist coverage converging to a non-degenerate value\nbetween their nominal level and one. As a byproduct of our theory, we show that\nthe GP regression also yields minimax-optimal posterior contraction rate\nrelative to the supremum norm, which provides a positive evidence to the long\nstanding problem on optimal supremum norm contraction rate in GP regression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:07:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Yang", "Yun", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1708.04815", "submitter": "L\\'aszl\\'o N\\'emeth", "authors": "L\\'aszl\\'o N\\'emeth and Andr\\'as Zempl\\'eni", "title": "Regression estimator for the tail index", "comments": "15 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the tail index parameter is one of the primal objectives in\nextreme value theory. For heavy-tailed distributions the Hill estimator is the\nmost popular way to estimate the tail index parameter. Improving the Hill\nestimator was aimed by recent works with different methods, for example by\nusing bootstrap, or Kolmogorov-Smirnov metric. These methods are asymptotically\nconsistent, but for tail index $\\xi >1$ and smaller sample sizes the estimation\nfails to approach the theoretical value for realistic sample sizes. In this\npaper, we introduce a new empirical method, which can estimate high tail index\nparameters well and might also be useful for relatively small sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:56:48 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 07:16:35 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 08:45:24 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["N\u00e9meth", "L\u00e1szl\u00f3", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1708.04887", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Gerda Claeskens, Thomas Gueuning", "title": "Fixed effects testing in high-dimensional linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering challenges -- ranging from pharmacokinetic\ndrug dosage allocation and personalized medicine to marketing mix (4Ps)\nrecommendations -- require an understanding of the unobserved heterogeneity in\norder to develop the best decision making-processes. In this paper, we develop\na hypothesis test and the corresponding p-value for testing for the\nsignificance of the homogeneous structure in linear mixed models. A robust\nmatching moment construction is used for creating a test that adapts to the\nsize of the model sparsity. When unobserved heterogeneity at a cluster level is\nconstant, we show that our test is both consistent and unbiased even when the\ndimension of the model is extremely high. Our theoretical results rely on a new\nfamily of adaptive sparse estimators of the fixed effects that do not require\nconsistent estimation of the random effects. Moreover, our inference results do\nnot require consistent model selection. We showcase that moment matching can be\nextended to nonlinear mixed effects models and to generalized linear mixed\neffects models. In numerical and real data experiments, we find that the\ndeveloped method is extremely accurate, that it adapts to the size of the\nunderlying model and is decidedly powerful in the presence of irrelevant\ncovariates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 21:48:46 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bradic", "Jelena", ""], ["Claeskens", "Gerda", ""], ["Gueuning", "Thomas", ""]]}, {"id": "1708.04891", "submitter": "Karthik Bharath", "authors": "Karthik Bharath and Sebastian Kurtek", "title": "Distribution on Warp Maps for Alignment of Open and Closed Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment of curve data is an integral part of their statistical analysis,\nand can be achieved using model- or optimization-based approaches. The\nparameter space is usually the set of monotone, continuous warp maps of a\ndomain. Infinite-dimensional nature of the parameter space encourages sampling\nbased approaches, which require a distribution on the set of warp maps.\nMoreover, the distribution should also enable sampling in the presence of\nimportant landmark information on the curves which constrain the warp maps. For\nalignment of closed and open curves in $\\mathbb{R}^d, d=1,2,3$, possibly with\nlandmark information, we provide a constructive, point-process based definition\nof a distribution on the set of warp maps of $[0,1]$ and the unit circle\n$\\mathbb{S}^1$ that is (1) simple to sample from, and (2) possesses the\ndesiderata for decomposition of the alignment problem with landmark constraints\ninto multiple unconstrained ones. For warp maps on $[0,1]$, the distribution is\nrelated to the Dirichlet process. We demonstrate its utility by using it as a\nprior distribution on warp maps in a Bayesian model for alignment of two\nunivariate curves, and as a proposal distribution in a stochastic algorithm\nthat optimizes a suitable alignment functional for higher-dimensional curves.\nSeveral examples from simulated and real datasets are provided.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 13:41:03 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 16:12:18 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1708.04950", "submitter": "Val\\'erie Chavez-Demoulin", "authors": "Val\\'erie Chavez-Demoulin and Armelle Guillou", "title": "Risk measure estimation for $\\beta$-mixing time series and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the application of extreme value theory in the\ncontext of stationary $\\beta$-mixing sequences that belong to the Fr\\'echet\ndomain of attraction. In particular, we propose a methodology to construct\nbias-corrected tail estimators. Our approach is based on the combination of two\nestimators for the extreme value index to cancel the bias. The resulting\nestimator is used to estimate an extreme quantile. In a simulation study, we\noutline the performance of our proposals that we compare to alternative\nestimators recently introduced in the literature. Also, we compute the\nasymptotic variance in specific examples when possible. Our methodology is\napplied to two datasets on finance and environment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 15:50:26 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 19:05:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chavez-Demoulin", "Val\u00e9rie", ""], ["Guillou", "Armelle", ""]]}, {"id": "1708.04985", "submitter": "Mikhail  Ermakov s", "authors": "Mikhail Ermakov", "title": "On maxispaces of nonparametric tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problems of nonparametric hypothesis testing we introduce the notion\nof maxisets and maxispace. We point out the maxisets of $\\chi^2-$tests,\nCramer-von Mises tests, tests generated $\\mathbb{L}_2$- norms of kernel\nestimators and tests generated quadratic forms of estimators of Fourier\ncoefficients. For these tests we show that, if sequence of alternatives having\ngiven rates of convergence to hypothesis is consistent, then each altehrnative\ncan be broken down into the sum of two parts: a function belonging to maxiset\nand orthogonal function. Sequence of functions belonging to maxiset is\nconsistent sequence of alternatives.\n  We point out asymptotically minimax tests if sets of alternatives are maxiset\nwith deleted \"small\" $\\mathbb{L}_2$-balls.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:24:26 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 10:10:57 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 05:46:08 GMT"}, {"version": "v4", "created": "Mon, 13 Nov 2017 14:24:11 GMT"}, {"version": "v5", "created": "Mon, 11 Dec 2017 18:23:40 GMT"}, {"version": "v6", "created": "Thu, 15 Mar 2018 07:36:47 GMT"}, {"version": "v7", "created": "Wed, 28 Mar 2018 16:52:00 GMT"}, {"version": "v8", "created": "Thu, 17 May 2018 15:09:04 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ermakov", "Mikhail", ""]]}, {"id": "1708.04999", "submitter": "Sebastian Roch", "authors": "Sebastien Roch and Karl Rohe", "title": "Generalized least squares can overcome the critical threshold in\n  respondent-driven sampling", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to sample marginalized and/or hard-to-reach populations,\nrespondent-driven sampling (RDS) and similar techniques reach their\nparticipants via peer referral. Under a Markov model for RDS, previous research\nhas shown that if the typical participant refers too many contacts, then the\nvariance of common estimators does not decay like $O(n^{-1})$, where $n$ is the\nsample size. This implies that confidence intervals will be far wider than\nunder a typical sampling design. Here we show that generalized least squares\n(GLS) can effectively reduce the variance of RDS estimates. In particular, a\ntheoretical analysis indicates that the variance of the GLS estimator is\n$O(n^{-1})$. We then derive two classes of feasible GLS estimators. The first\nclass is based upon a Degree Corrected Stochastic Blockmodel for the underlying\nsocial network. The second class is based upon a rank-two model. It might be of\nindependent interest that in both model classes, the theoretical results show\nthat it is possible to estimate the spectral properties of the population\nnetwork from the sampled observations. Simulations on empirical social networks\nshow that the feasible GLS (fGLS) estimators can have drastically smaller error\nand rarely increase the error. A diagnostic plot helps to identify where fGLS\nwill aid estimation. The fGLS estimators continue to outperform standard\nestimators even when they are built from a misspecified model and when there is\npreferential recruitment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:54:34 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Roch", "Sebastien", ""], ["Rohe", "Karl", ""]]}, {"id": "1708.05026", "submitter": "Sungkyu Jung", "authors": "Sungkyu Jung", "title": "Adjusting systematic bias in high dimensional principal component scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis continues to be a powerful tool in dimension\nreduction of high dimensional data. We assume a variance-diverging model and\nuse the high-dimension, low-sample-size asymptotics to show that even though\nthe principal component directions are not consistent, the sample and\nprediction principal component scores can be useful in revealing the population\nstructure. We further show that these scores are biased, and the bias is\nasymptotically decomposed into rotation and scaling parts. We propose methods\nof bias-adjustment that are shown to be consistent and work well in the high\ndimensional situations with small sample sizes. The potential advantage of\nbias-adjustment is demonstrated in a classification setting.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:24:05 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 05:36:12 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Jung", "Sungkyu", ""]]}, {"id": "1708.05343", "submitter": "Wlodek Bryc", "authors": "Wlodzimierz Bryc and Raouf Fakhfakh and Wojciech Mlotkowski", "title": "Cauchy-Stieltjes families with polynomial variance functions and\n  generalized orthogonality", "comments": "Minor typos corrected", "journal-ref": "Probability and Mathematical Statistics 39 (2019) pp. 237-258", "doi": "10.19195/0208-4147.39.2.1", "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies variance functions of Cauchy-Stieltjes Kernel families\ngenerated by compactly supported centered probability measures. We describe\nseveral operations that allow us to construct additional variance functions\nfrom known ones. We construct a class of examples which exhausts all cubic\nvariance functions, and provide examples of polynomial variance functions of\narbitrary degree. We also relate Cauchy-Stieltjes Kernel families with\npolynomial variance functions to generalized orthogonality.\n  Our main results are stated solely in terms of classical probability; some\nproofs rely on analytic machinery of free probability.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 15:57:25 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 21:23:32 GMT"}, {"version": "v3", "created": "Sun, 28 Jan 2018 16:12:16 GMT"}, {"version": "v4", "created": "Sat, 12 Oct 2019 15:26:30 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Bryc", "Wlodzimierz", ""], ["Fakhfakh", "Raouf", ""], ["Mlotkowski", "Wojciech", ""]]}, {"id": "1708.05472", "submitter": "Braxton Osting", "authors": "Braxton Osting and Todd Harry Reeb", "title": "Consistency of Dirichlet Partitions", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Dirichlet $k$-partition of a domain $U \\subseteq \\mathbb{R}^d$ is a\ncollection of $k$ pairwise disjoint open subsets such that the sum of their\nfirst Laplace-Dirichlet eigenvalues is minimal. A discrete version of Dirichlet\npartitions has been posed on graphs with applications in data analysis. Both\nversions admit variational formulations: solutions are characterized by\nminimizers of the Dirichlet energy of mappings from $U$ into a singular space\n$\\Sigma_k \\subseteq \\mathbb{R}^k$. In this paper, we extend results of N.\\\nGarc\\'ia Trillos and D.\\ Slep\\v{c}ev to show that there exist solutions of the\ncontinuum problem arising as limits to solutions of a sequence of discrete\nproblems. Specifically, a sequence of points $\\{x_i\\}_{i \\in \\mathbb{N}}$ from\n$U$ is sampled i.i.d.\\ with respect to a given probability measure $\\nu$ on $U$\nand for all $n \\in \\mathbb{N}$, a geometric graph $G_n$ is constructed from the\nfirst $n$ points $x_1, x_2, \\ldots, x_n$ and the pairwise distances between the\npoints. With probability one with respect to the choice of points $\\{x_i\\}_{i\n\\in \\mathbb{N}}$, we show that as $n \\to \\infty$ the discrete Dirichlet\nenergies for functions $G_n \\to \\Sigma_k$ $\\Gamma$-converge to (a scalar\nmultiple of) the continuum Dirichlet energy for functions $U \\to \\Sigma_k$ with\nrespect to a metric coming from the theory of optimal transport. This, along\nwith a compactness property for the aforementioned energies that we prove,\nimplies the convergence of minimizers. When $\\nu$ is the uniform distribution,\nour results also imply the statistical consistency statement that Dirichlet\npartitions of geometric graphs converge to partitions of the sampled space in\nthe Hausdorff sense.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 00:19:17 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Osting", "Braxton", ""], ["Reeb", "Todd Harry", ""]]}, {"id": "1708.05499", "submitter": "David Gold", "authors": "David Gold, Johannes Lederer, Jing Tao", "title": "Inference for high-dimensional instrumental variables regression", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns statistical inference for the components of a\nhigh-dimensional regression parameter despite possible endogeneity of each\nregressor. Given a first-stage linear model for the endogenous regressors and a\nsecond-stage linear model for the dependent variable, we develop a novel\nadaptation of the parametric one-step update to a generic second-stage\nestimator. We provide conditions under which the scaled update is\nasymptotically normal. We then introduce a two-stage Lasso procedure and show\nthat the second-stage Lasso estimator satisfies the aforementioned conditions.\nUsing these results, we construct asymptotically valid confidence intervals for\nthe components of the second-stage regression coefficients. We complement our\nasymptotic theory with simulation studies, which demonstrate the performance of\nour method in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 03:36:03 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 15:04:10 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 21:10:51 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Gold", "David", ""], ["Lederer", "Johannes", ""], ["Tao", "Jing", ""]]}, {"id": "1708.05573", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Peter J. Bickel", "title": "Two provably consistent divide and conquer clustering algorithms for\n  large networks", "comments": "41 pages, comments are most welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advance divide-and-conquer strategies for solving the\ncommunity detection problem in networks. We propose two algorithms which\nperform clustering on a number of small subgraphs and finally patches the\nresults into a single clustering. The main advantage of these algorithms is\nthat they bring down significantly the computational cost of traditional\nalgorithms, including spectral clustering, semi-definite programs, modularity\nbased methods, likelihood based methods etc., without losing on accuracy and\neven improving accuracy at times. These algorithms are also, by nature,\nparallelizable. Thus, exploiting the facts that most traditional algorithms are\naccurate and the corresponding optimization problems are much simpler in small\nproblems, our divide-and-conquer methods provide an omnibus recipe for scaling\ntraditional algorithms up to large networks. We prove consistency of these\nalgorithms under various subgraph selection procedures and perform extensive\nsimulations and real-data analysis to understand the advantages of the\ndivide-and-conquer approach in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 12:09:10 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1708.05653", "submitter": "Luca Weihs", "authors": "Luca Weihs, Mathias Drton, Nicolai Meinshausen", "title": "Symmetric Rank Covariances: a Generalised Framework for Nonparametric\n  Measures of Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to test whether two random vectors are independent has spawned a\nlarge number of competing measures of dependence. We are interested in\nnonparametric measures that are invariant under strictly increasing\ntransformations, such as Kendall's tau, Hoeffding's D, and the more recently\ndiscovered Bergsma--Dassios sign covariance. Each of these measures exhibits\nsymmetries that are not readily apparent from their definitions. Making these\nsymmetries explicit, we define a new class of multivariate nonparametric\nmeasures of dependence that we refer to as Symmetric Rank Covariances. This new\nclass generalises all of the above measures and leads naturally to multivariate\nextensions of the Bergsma--Dassios sign covariance. Symmetric Rank Covariances\nmay be estimated unbiasedly using U-statistics for which we prove results on\ncomputational efficiency and large-sample behavior. The algorithms we develop\nfor their computation include, to the best of our knowledge, the first\nefficient algorithms for the well-known Hoeffding's D statistic in the\nmultivariate setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 15:40:02 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Weihs", "Luca", ""], ["Drton", "Mathias", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1708.05836", "submitter": "Monika Bhattacharjee", "authors": "Monika Bhattacharjee, Moulinath Banerjee and George Michailidis", "title": "Common change point estimation in panel data from the least squares and\n  maximum likelihood viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the convergence rates and asymptotic distributions of the common\nbreak change-point estimators, obtained by least squares and maximum likelihood\nin panel data models and compare their asymptotic variances. Our model\nassumptions accommodate a variety of commonly encountered probability\ndistributions and, in particular, models of particular interest in econometrics\nbeyond the commonly analyzed Gaussian model, including the zero-inflated\nPoisson model for count data, and the probit and tobit models. We also provide\nnovel results for time dependent data in the signal-plus-noise model, with\nemphasis on a wide array of noise processes, including Gaussian process,\nMA$(\\infty)$ and $m$-dependent processes. The obtained results show that\nmaximum likelihood estimation requires a stronger signal-to-noise model\nidentifiability condition compared to its least squares counterpart. Finally,\nsince there are three different asymptotic regimes that depend on the behavior\nof the norm difference of the model parameters before and after the change\npoint, which cannot be realistically assumed to be known, we develop a novel\ndata driven adaptive procedure that provides valid confidence intervals for the\ncommon break, without requiring a priori knowledge of the asymptotic regime the\nproblem falls in.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 12:09:59 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bhattacharjee", "Monika", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1708.05859", "submitter": "Renan Gross", "authors": "Ronen Eldan and Renan Gross", "title": "Decomposition of mean-field Gibbs distributions into product measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that under a low complexity condition on the gradient of a\nHamiltonian, Gibbs distributions on the Boolean hypercube are approximate\nmixtures of product measures whose probability vectors are critical points of\nan associated mean-field functional. This extends a previous work by the first\nauthor. As an application, we demonstrate how this framework helps characterize\nboth Ising models satisfying a mean-field condition and the conditional\ndistributions which arise in the emerging theory of nonlinear large deviations,\nboth in the dense case and in the polynomially-sparse case.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 15:05:19 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:58:54 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Eldan", "Ronen", ""], ["Gross", "Renan", ""]]}, {"id": "1708.06077", "submitter": "Waheed Bajwa", "authors": "Talal Ahmed and Waheed U. Bajwa", "title": "ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional\n  Linear Models", "comments": "30 pages; 3 figures and 1 table; preprint of a journal publication", "journal-ref": "EURASIP J. Signal Processing, vol. 159, pp. 33-48, Jun. 2019", "doi": "10.1016/j.sigpro.2019.01.018", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference can be computationally prohibitive in\nultrahigh-dimensional linear models. Correlation-based variable screening, in\nwhich one leverages marginal correlations for removal of irrelevant variables\nfrom the model prior to statistical inference, can be used to overcome this\nchallenge. Prior works on correlation-based variable screening either impose\nstatistical priors on the linear model or assume specific post-screening\ninference methods. This paper first extends the analysis of correlation-based\nvariable screening to arbitrary linear models and post-screening inference\ntechniques. In particular, (i) it shows that a condition---termed the screening\ncondition---is sufficient for successful correlation-based screening of linear\nmodels, and (ii) it provides insights into the dependence of marginal\ncorrelation-based screening on different problem parameters. Numerical\nexperiments confirm that these insights are not mere artifacts of analysis;\nrather, they are reflective of the challenges associated with marginal\ncorrelation-based variable screening. Second, the paper explicitly derives the\nscreening condition for arbitrary (random or deterministic) linear models and,\nin the process, it establishes that---under appropriate conditions---it is\npossible to reduce the dimension of an ultrahigh-dimensional, arbitrary linear\nmodel to almost the sample size even when the number of active variables scales\nalmost linearly with the sample size. Third, it specializes the screening\ncondition to sub-Gaussian linear models and contrasts the final results to\nthose existing in the literature. This specialization formally validates the\nclaim that the main result of this paper generalizes existing ones on\ncorrelation-based screening.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 03:41:14 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 18:48:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ahmed", "Talal", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1708.06332", "submitter": "Fran\\c{c}ois Monard", "authors": "Fran\\c{c}ois Monard and Richard Nickl and Gabriel P. Paternain", "title": "Efficient Nonparametric Bayesian Inference For X-Ray Transforms", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical inverse problem of recovering a function $f: M\n\\to \\mathbb R$, where $M$ is a smooth compact Riemannian manifold with\nboundary, from measurements of general $X$-ray transforms $I_a(f)$ of $f$,\ncorrupted by additive Gaussian noise. For $M$ equal to the unit disk with\n`flat' geometry and $a=0$ this reduces to the standard Radon transform, but our\ngeneral setting allows for anisotropic media $M$ and can further model local\n`attenuation' effects -- both highly relevant in practical imaging problems\nsuch as SPECT tomography. We propose a nonparametric Bayesian inference\napproach based on standard Gaussian process priors for $f$. The posterior\nreconstruction of $f$ corresponds to a Tikhonov regulariser with a reproducing\nkernel Hilbert space norm penalty that does not require the calculation of the\nsingular value decomposition of the forward operator $I_a$. We prove\nBernstein-von Mises theorems that entail that posterior-based inferences such\nas credible sets are valid and optimal from a frequentist point of view for a\nlarge family of semi-parametric aspects of $f$. In particular we derive the\nasymptotic distribution of smooth linear functionals of the Tikhonov\nregulariser, which is shown to attain the semi-parametric Cram\\'er-Rao\ninformation bound. The proofs rely on an invertibility result for the `Fisher\ninformation' operator $I_a^*I_a$ between suitable function spaces, a result of\nindependent interest that relies on techniques from microlocal analysis. We\nillustrate the performance of the proposed method via simulations in various\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 17:33:30 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 21:44:42 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Monard", "Fran\u00e7ois", ""], ["Nickl", "Richard", ""], ["Paternain", "Gabriel P.", ""]]}, {"id": "1708.06394", "submitter": "Mahdi Cheraghchi", "authors": "Mahdi Cheraghchi", "title": "Expressions for the Entropy of Binomial-Type Distributions", "comments": "A preliminary version of this work appears in Proceedings of the 2018\n  IEEE International Symposium on Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general method for computing logarithmic and log-gamma\nexpectations of distributions. As a result, we derive series expansions and\nintegral representations of the entropy for several fundamental distributions,\nincluding the Poisson, binomial, beta-binomial, negative binomial, and\nhypergeometric distributions. Our results also establish connections between\nthe entropy functions and to the Riemann zeta function and its generalizations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 19:58:52 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 22:28:17 GMT"}, {"version": "v3", "created": "Tue, 1 May 2018 00:28:03 GMT"}, {"version": "v4", "created": "Sat, 21 Jul 2018 14:20:42 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Cheraghchi", "Mahdi", ""]]}, {"id": "1708.06425", "submitter": "Mustafa Kocak", "authors": "Mustafa A. Kocak, David Ramirez, Elza Erkip, and Dennis E. Shasha", "title": "SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to\n  Guarantee Correctness", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, August 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SafePredict is a novel meta-algorithm that works with any base prediction\nalgorithm for online data to guarantee an arbitrarily chosen correctness rate,\n$1-\\epsilon$, by allowing refusals. Allowing refusals means that the\nmeta-algorithm may refuse to emit a prediction produced by the base algorithm\non occasion so that the error rate on non-refused predictions does not exceed\n$\\epsilon$. The SafePredict error bound does not rely on any assumptions on the\ndata distribution or the base predictor. When the base predictor happens not to\nexceed the target error rate $\\epsilon$, SafePredict refuses only a finite\nnumber of times. When the error rate of the base predictor changes through time\nSafePredict makes use of a weight-shifting heuristic that adapts to these\nchanges without knowing when the changes occur yet still maintains the\ncorrectness guarantee. Empirical results show that (i) SafePredict compares\nfavorably with state-of-the art confidence based refusal mechanisms which fail\nto offer robust error guarantees; and (ii) combining SafePredict with such\nrefusal mechanisms can in many cases further reduce the number of refusals. Our\nsoftware (currently in Python) is included in the supplementary material.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 21:23:42 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 03:35:00 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kocak", "Mustafa A.", ""], ["Ramirez", "David", ""], ["Erkip", "Elza", ""], ["Shasha", "Dennis E.", ""]]}, {"id": "1708.06436", "submitter": "Jann Spiess", "authors": "Jann Spiess", "title": "Unbiased Shrinkage Estimation", "comments": "Updated title and abstract, substance unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimation usually reduces variance at the cost of bias. But when\nwe care only about some parameters of a model, I show that we can reduce\nvariance without incurring bias if we have additional information about the\ndistribution of covariates. In a linear regression model with homoscedastic\nNormal noise, I consider shrinkage estimation of the nuisance parameters\nassociated with control variables. For at least three control variables and\nexogenous treatment, I establish that the standard least-squares estimator is\ndominated with respect to squared-error loss in the treatment effect even among\nunbiased estimators and even when the target parameter is low-dimensional. I\nconstruct the dominating estimator by a variant of James-Stein shrinkage in a\nhigh-dimensional Normal-means problem. It can be interpreted as an invariant\ngeneralized Bayes estimator with an uninformative (improper) Jeffreys prior in\nthe target parameter.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:20:31 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:44:08 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Spiess", "Jann", ""]]}, {"id": "1708.06443", "submitter": "Jann Spiess", "authors": "Jann Spiess", "title": "Bias Reduction in Instrumental Variable Estimation through First-Stage\n  Shrinkage", "comments": "Updated title and abstract, substance unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-stage least-squares (2SLS) estimator is known to be biased when its\nfirst-stage fit is poor. I show that better first-stage prediction can\nalleviate this bias. In a two-stage linear regression model with Normal noise,\nI consider shrinkage in the estimation of the first-stage instrumental variable\ncoefficients. For at least four instrumental variables and a single endogenous\nregressor, I establish that the standard 2SLS estimator is dominated with\nrespect to bias. The dominating IV estimator applies James-Stein type shrinkage\nin a first-stage high-dimensional Normal-means problem followed by a\ncontrol-function approach in the second stage. It preserves invariances of the\nstructural instrumental variable equations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:38:21 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:55:40 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Spiess", "Jann", ""]]}, {"id": "1708.06633", "submitter": "Johannes Schmidt-Hieber", "authors": "Johannes Schmidt-Hieber", "title": "Nonparametric regression using deep neural networks with ReLU activation\n  function", "comments": "article, rejoinder and supplementary material", "journal-ref": "Article: Annals of Statistics, Volume 48, Number 4, 1875-1897,\n  2020, Rejoinder: Annals of Statistics, Volume 48, Number 4, 1916-1921, 2020", "doi": "10.1214/19-AOS1875", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the multivariate nonparametric regression model. It is shown that\nestimators based on sparsely connected deep neural networks with ReLU\nactivation function and properly chosen network architecture achieve the\nminimax rates of convergence (up to $\\log n$-factors) under a general\ncomposition assumption on the regression function. The framework includes many\nwell-studied structural constraints such as (generalized) additive models.\nWhile there is a lot of flexibility in the network architecture, the tuning\nparameter is the sparsity of the network. Specifically, we consider large\nnetworks with number of potential network parameters exceeding the sample size.\nThe analysis gives some insights into why multilayer feedforward neural\nnetworks perform well in practice. Interestingly, for ReLU activation function\nthe depth (number of layers) of the neural network architectures plays an\nimportant role and our theory suggests that for nonparametric regression,\nscaling the network depth with the sample size is natural. It is also shown\nthat under the composition assumption wavelet estimators can only achieve\nsuboptimal rates.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:25:55 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 08:08:07 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 14:57:26 GMT"}, {"version": "v4", "created": "Sat, 16 Mar 2019 01:45:13 GMT"}, {"version": "v5", "created": "Sun, 13 Sep 2020 10:18:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1708.06716", "submitter": "Larissa Albantakis", "authors": "Larissa Albantakis, William Marshall, Erik Hoel, Giulio Tononi", "title": "What caused what? A quantitative account of actual causation using\n  dynamical causal networks", "comments": "43 pages, 16 figures, supplementary discussion, supplementary\n  methods, supplementary proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actual causation is concerned with the question \"what caused what?\" Consider\na transition between two states within a system of interacting elements, such\nas an artificial neural network, or a biological brain circuit. Which\ncombination of synapses caused the neuron to fire? Which image features caused\nthe classifier to misinterpret the picture? Even detailed knowledge of the\nsystem's causal network, its elements, their states, connectivity, and dynamics\ndoes not automatically provide a straightforward answer to the \"what caused\nwhat?\" question. Counterfactual accounts of actual causation based on graphical\nmodels, paired with system interventions, have demonstrated initial success in\naddressing specific problem cases in line with intuitive causal judgments.\nHere, we start from a set of basic requirements for causation (realization,\ncomposition, information, integration, and exclusion) and develop a rigorous,\nquantitative account of actual causation that is generally applicable to\ndiscrete dynamical systems. We present a formal framework to evaluate these\ncausal requirements that is based on system interventions and partitions, and\nconsiders all counterfactuals of a state transition. This framework is used to\nprovide a complete causal account of the transition by identifying and\nquantifying the strength of all actual causes and effects linking the two\nconsecutive system states. Finally, we examine several exemplary cases and\nparadoxes of causation and show that they can be illuminated by the proposed\nframework for quantifying actual causation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:51:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 20:53:07 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Albantakis", "Larissa", ""], ["Marshall", "William", ""], ["Hoel", "Erik", ""], ["Tononi", "Giulio", ""]]}, {"id": "1708.06990", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Sebastiano Stramaglia, Daniele Marinazzo", "title": "On the interpretability and computational reliability of\n  frequency-domain Granger causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment to the paper 'A study of problems encountered in Granger\ncausality analysis from a neuroscience perspective'. We agree that\ninterpretation issues of Granger Causality in Neuroscience exist (partially due\nto the historical unfortunate use of the name 'causality', as nicely described\nin previous literature). On the other hand we think that the paper uses a\nformulation of Granger causality which is outdated (albeit still used), and in\ndoing so it dismisses the measure based on a suboptimal use of it. Furthermore,\nsince data from simulated systems are used, the pitfalls that are found with\nthe used formulation are intended to be general, and not limited to\nneuroscience. It would be a pity if this paper, even written in good faith,\nbecame a wildcard against all possible applications of Granger Causality,\nregardless of the hard work of colleagues aiming to seriously address the\nmethodological and interpretation pitfalls. In order to provide a balanced\nview, we replicated their simulations used the updated State Space\nimplementation, proposed already some years ago, in which the pitfalls are\nmitigated or directly solved.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:32:24 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Faes", "Luca", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1708.07070", "submitter": "Ahmed Kebaier", "authors": "Mohamed Ben Alaya, Ahmed Kebaier and Ngoc Khue Tran", "title": "Local asymptotic properties for Cox-Ingersoll-Ross process with discrete\n  observations", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a one-dimensional Cox-Ingersoll-Ross (CIR) process\nwhose drift coefficient depends on unknown parameters. Considering the process\ndiscretely observed at high frequency, we prove the local asymptotic normality\nproperty in the subcritical case, the local asymptotic quadraticity in the\ncritical case, and the local asymptotic mixed normality property in the\nsupercritical case. To obtain these results, we use the Malliavin calculus\ntechniques developed recently for CIR process by Al\\`os et {\\it al.}\n\\cite{AE08} and Altmayer et {\\it al.} \\cite{AN14} together with the $L^p$-norm\nestimation for positive and negative moments of the CIR process obtained by\nBossy et {\\it al.} \\cite{BD07} and Ben Alaya et {\\it al.} \\cite{BK12,BK13}. In\nthis study, we require the same conditions of high frequency\n$\\Delta_n\\rightarrow 0$ and infinite horizon $n\\Delta_n\\rightarrow\\infty$ as in\nthe case of ergodic diffusions with globally Lipschitz coefficients studied\nearlier by Gobet \\cite{G02}. However, in the non-ergodic cases, additional\nassumptions on the decreasing rate of $\\Delta_n$ are required due to the fact\nthat the square root diffusion coefficient of the CIR process is not regular\nenough. Indeed, we assume $n\\Delta_n^{3}\\to 0$ for the critical case and\n$\\Delta_n^{2}e^{-b_0n\\Delta_n}\\to 0$ for the supercritical case.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:59:29 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:48:50 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 11:00:12 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 09:59:49 GMT"}, {"version": "v5", "created": "Thu, 25 Jun 2020 07:47:54 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Alaya", "Mohamed Ben", ""], ["Kebaier", "Ahmed", ""], ["Tran", "Ngoc Khue", ""]]}, {"id": "1708.07259", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Lexin Li", "title": "Dynamic Tensor Clustering", "comments": "Accepted at Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic tensor data are becoming prevalent in numerous applications. Existing\ntensor clustering methods either fail to account for the dynamic nature of the\ndata, or are inapplicable to a general-order tensor. Also there is often a gap\nbetween statistical guarantee and computational efficiency for existing tensor\nclustering solutions. In this article, we aim to bridge this gap by proposing a\nnew dynamic tensor clustering method, which takes into account both sparsity\nand fusion structures, and enjoys strong statistical guarantees as well as high\ncomputational efficiency. Our proposal is based upon a new structured tensor\nfactorization that encourages both sparsity and smoothness in parameters along\nthe specified tensor modes. Computationally, we develop a highly efficient\noptimization algorithm that benefits from substantial dimension reduction. In\ntheory, we first establish a non-asymptotic error bound for the estimator from\nthe structured tensor factorization. Built upon this error bound, we then\nderive the rate of convergence of the estimated cluster centers, and show that\nthe estimated clusters recover the true cluster structures with a high\nprobability. Moreover, our proposed method can be naturally extended to\nco-clustering of multiple modes of the tensor data. The efficacy of our\napproach is illustrated via simulations and a brain dynamic functional\nconnectivity analysis from an Autism spectrum disorder study.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 02:50:15 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 15:41:54 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1708.07367", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Aryeh Kontorovich, David A. Levin, Yuval Peres, Csaba\n  Szepesv\\'ari", "title": "Mixing time estimation in reversible Markov chains from a single sample\n  path", "comments": "34 pages, merges results of arXiv:1506.02903 and arXiv:1612.05330", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral gap $\\gamma$ of a finite, ergodic, and reversible Markov chain\nis an important parameter measuring the asymptotic rate of convergence. In\napplications, the transition matrix $P$ may be unknown, yet one sample of the\nchain up to a fixed time $n$ may be observed. We consider here the problem of\nestimating $\\gamma$ from this data. Let $\\pi$ be the stationary distribution of\n$P$, and $\\pi_\\star = \\min_x \\pi(x)$. We show that if $n =\n\\tilde{O}\\bigl(\\frac{1}{\\gamma \\pi_\\star}\\bigr)$, then $\\gamma$ can be\nestimated to within multiplicative constants with high probability. When $\\pi$\nis uniform on $d$ states, this matches (up to logarithmic correction) a lower\nbound of $\\tilde{\\Omega}\\bigl(\\frac{d}{\\gamma}\\bigr)$ steps required for\nprecise estimation of $\\gamma$. Moreover, we provide the first procedure for\ncomputing a fully data-dependent interval, from a single finite-length\ntrajectory of the chain, that traps the mixing time $t_{\\text{mix}}$ of the\nchain at a prescribed confidence level. The interval does not require the\nknowledge of any parameters of the chain. This stands in contrast to previous\napproaches, which either only provide point estimates, or require a reset\nmechanism, or additional prior knowledge. The interval is constructed around\nthe relaxation time $t_{\\text{relax}} = 1/\\gamma$, which is strongly related to\nthe mixing time, and the width of the interval converges to zero roughly at a\n$1/\\sqrt{n}$ rate, where $n$ is the length of the sample path.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 12:05:11 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Hsu", "Daniel", ""], ["Kontorovich", "Aryeh", ""], ["Levin", "David A.", ""], ["Peres", "Yuval", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1708.07446", "submitter": "Marko Voutilainen", "authors": "Marko Voutilainen, Lauri Viitasaari, Pauliina Ilmonen", "title": "On model fitting and estimation of strictly stationary processes", "comments": "Published at https://doi.org/10.15559/17-VMSTA91 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2017, Vol. 4, No. 4,\n  381-406", "doi": "10.15559/17-VMSTA91", "report-no": "VTeX-VMSTA-VMSTA91", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary processes have been extensively studied in the literature. Their\napplications include modeling and forecasting numerous real life phenomena such\nas natural disasters, sales and market movements. When stationary processes are\nconsidered, modeling is traditionally based on fitting an autoregressive moving\naverage (ARMA) process. However, we challenge this conventional approach.\nInstead of fitting an ARMA model, we apply an AR(1) characterization in\nmodeling any strictly stationary processes. Moreover, we derive consistent and\nasymptotically normal estimators of the corresponding model parameter.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:01:04 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 13:33:32 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Voutilainen", "Marko", ""], ["Viitasaari", "Lauri", ""], ["Ilmonen", "Pauliina", ""]]}, {"id": "1708.07459", "submitter": "Philip Chodrow", "authors": "Philip Chodrow", "title": "Divergence, Entropy, Information: An Opinionated Introduction to\n  Information Theory", "comments": "18 pages, typo fixes and other style revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory is a mathematical theory of learning with deep connections\nwith topics as diverse as artificial intelligence, statistical physics, and\nbiological evolution. Many primers on information theory paint a broad picture\nwith relatively little mathematical sophistication, while many others develop\nspecific application areas in detail. In contrast, these informal notes aim to\noutline some elements of the information-theoretic \"way of thinking,\" by\ncutting a rapid and interesting path through some of the theory's foundational\nconcepts and results. They are aimed at practicing systems scientists who are\ninterested in exploring potential connections between information theory and\ntheir own fields. The main mathematical prerequisite for the notes is comfort\nwith elementary probability, including sample spaces, conditioning, and\nexpectations. We take the Kullback-Leibler divergence as our most basic\nconcept, and then proceed to develop the entropy and mutual information. We\ndiscuss some of the main results, including the Chernoff bounds as a\ncharacterization of the divergence; Gibbs' Theorem; and the Data Processing\nInequality. A recurring theme is that the definitions of information theory\nsupport natural theorems that sound ``obvious'' when translated into English.\nMore pithily, ``information theory makes common sense precise.'' Since the\nfocus of the notes is not primarily on technical details, proofs are provided\nonly where the relevant techniques are illustrative of broader themes.\nOtherwise, proofs and intriguing tangents are referenced in liberally-sprinkled\nfootnotes. The notes close with a highly nonexhaustive list of references to\nresources and other perspectives on the field.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:37:57 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 18:34:02 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chodrow", "Philip", ""]]}, {"id": "1708.07485", "submitter": "Angshuman Roy", "authors": "Angshuman Roy, Alok Goswami, C. A. Murthy", "title": "Multivariate Dependency Measure based on Copula and Gaussian Kernel", "comments": "This work is postponed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multivariate dependency measure. It is obtained by\nconsidering a Gaussian kernel based distance between the copula transform of\nthe given d-dimensional distribution and the uniform copula and then\nappropriately normalizing it. The resulting measure is shown to satisfy a\nnumber of desirable properties. A nonparametric estimate is proposed for this\ndependency measure and its properties (finite sample as well as asymptotic) are\nderived. Some comparative studies of the proposed dependency measure estimate\nwith some widely used dependency measure estimates on artificial datasets are\nincluded. A non-parametric test of independence between two or more random\nvariables based on this measure is proposed. A comparison of the proposed test\nwith some existing nonparametric multivariate test for independence is\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:43:01 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 07:59:58 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 15:11:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Roy", "Angshuman", ""], ["Goswami", "Alok", ""], ["Murthy", "C. A.", ""]]}, {"id": "1708.07642", "submitter": "Matthias L\\\"offler", "authors": "Vladimir Koltchinskii, Matthias L\\\"offler, Richard Nickl", "title": "Efficient Estimation of Linear Functionals of Principal Components", "comments": "48 pages, to appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study principal component analysis (PCA) for mean zero i.i.d. Gaussian\nobservations $X_1,\\dots, X_n$ in a separable Hilbert space $\\mathbb{H}$ with\nunknown covariance operator $\\Sigma.$ The complexity of the problem is\ncharacterized by its effective rank ${\\bf r}(\\Sigma):= \\frac{{\\rm\ntr}(\\Sigma)}{\\|\\Sigma\\|},$ where ${\\rm tr}(\\Sigma)$ denotes the trace of\n$\\Sigma$ and $\\|\\Sigma\\|$ denotes its operator norm. We develop a method of\nbias reduction in the problem of estimation of linear functionals of\neigenvectors of $\\Sigma.$ Under the assumption that ${\\bf r}(\\Sigma)=o(n),$ we\nestablish the asymptotic normality and asymptotic properties of the risk of the\nresulting estimators and prove matching minimax lower bounds, showing their\nsemi-parametric optimality.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 08:08:39 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 19:42:59 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 15:33:05 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 20:58:23 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["L\u00f6ffler", "Matthias", ""], ["Nickl", "Richard", ""]]}, {"id": "1708.07964", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky and Shelemyahu Zacks", "title": "Proportional Closeness Estimation of Probability of Contamination Under\n  Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is focused on the problem of estimating the probability $p$ of\nindividual contaminated sample, under group testing. The precision of the\nestimator is given by the probability of proportional closeness, a concept\ndefined in the Introduction. Two-stage and sequential sampling procedures are\ncharacterized. An adaptive procedure is examined.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 12:59:23 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 21:27:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Zacks", "Shelemyahu", ""]]}, {"id": "1708.07986", "submitter": "Sara van de Geer", "authors": "Sara van de Geer", "title": "On the efficiency of the de-biased Lasso", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high-dimensional linear regression model $Y = X \\beta^0 +\n\\epsilon$ with Gaussian noise $\\epsilon$ and Gaussian random design $X$. We\nassume that $\\Sigma:= E X^T X / n$ is non-singular and write its inverse as\n$\\Theta := \\Sigma^{-1}$. The parameter of interest is the first component\n$\\beta_1^0$ of $\\beta^0$. We show that in the high-dimensional case the\nasymptotic variance of a debiased Lasso estimator can be smaller than\n$\\Theta_{1,1}$. For some special such cases we establish asymptotic efficiency.\nThe conditions include $\\beta^0$ being sparse and the first column $\\Theta_1$\nof $\\Theta$ being not sparse. These conditions depend on whether $\\Sigma$ is\nknown or not.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 15:30:57 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 12:41:44 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["van de Geer", "Sara", ""]]}, {"id": "1708.08278", "submitter": "Rianne de Heide", "authors": "Rianne de Heide, Peter D. Gr\\\"unwald", "title": "Why optional stopping can be a problem for Bayesians", "comments": "Replacement of Figures 7a-7d in the appendix. There was a mistake in\n  the sampling plan. Thanks to Jorge Tendeiro for pointing this out. Replaced\n  the main text with the final (published) version. Psychonomic Bulletin &\n  Review 2020 Advance Publication", "journal-ref": null, "doi": "10.3758/s13423-020-01803-x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, optional stopping has been a subject of debate in the Bayesian\npsychology community. Rouder (2014) argues that optional stopping is no problem\nfor Bayesians, and even recommends the use of optional stopping in practice, as\ndo Wagenmakers et al. (2012). This article addresses the question whether\noptional stopping is problematic for Bayesian methods, and specifies under\nwhich circumstances and in which sense it is and is not. By slightly varying\nand extending Rouder's (2014) experiments, we illustrate that, as soon as the\nparameters of interest are equipped with default or pragmatic priors - which\nmeans, in most practical applications of Bayes factor hypothesis testing -\nresilience to optional stopping can break down. We distinguish between three\ntypes of default priors, each having their own specific issues with optional\nstopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type\nII priors).\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 11:46:00 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 14:54:12 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 15:39:35 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 09:57:32 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 18:12:30 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["de Heide", "Rianne", ""], ["Gr\u00fcnwald", "Peter D.", ""]]}, {"id": "1708.08321", "submitter": "Gery Geenens", "authors": "Carlos Aya Moreno, Gery Geenens and Spiridon Penev", "title": "Shape-preserving wavelet-based multivariate density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet estimators for a probability density f enjoy many good properties,\nhowever they are not \"shape-preserving\" in the sense that the final estimate\nmay not be non-negative or integrate to unity. A solution to negativity issues\nmay be to estimate first the square-root of f and then square this estimate up.\nThis paper proposes and investigates such an estimation scheme, generalising to\nhigher dimensions some previous constructions which are valid only in one\ndimension. The estimation is mainly based on nearest-neighbour-balls. The\ntheoretical properties of the proposed estimator are obtained, and it is shown\nto reach the optimal rate of convergence uniformly over large classes of\ndensities under mild conditions. Simulations show that the new estimator\nperforms as well in general as the classical wavelet estimator, while\nautomatically producing estimates which are bona fide densities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:13:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Moreno", "Carlos Aya", ""], ["Geenens", "Gery", ""], ["Penev", "Spiridon", ""]]}, {"id": "1708.08432", "submitter": "Ansgar Steland", "authors": "Annabel Prause and Ansgar Steland", "title": "Estimation of the asymptotic variance of univariate and multivariate\n  random fields and statistical inference", "comments": null, "journal-ref": "Electronic Journal of Statistics, 12(1), 890-940, 2018", "doi": "10.1214/18-EJS1398", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated random fields are a common way to model dependence struc- tures in\nhigh-dimensional data, especially for data collected in imaging. One important\nparameter characterizing the degree of dependence is the asymp- totic variance\nwhich adds up all autocovariances in the temporal and spatial domain.\nEspecially, it arises in the standardization of test statistics based on\npartial sums of random fields and thus the construction of tests requires its\nestimation. In this paper we propose consistent estimators for this parameter\nfor strictly stationary {\\phi}-mixing random fields with arbitrary dimension of\nthe domain and taking values in a Euclidean space of arbitrary dimension, thus\nallowing for multivariate random fields. We establish consistency, provide cen-\ntral limit theorems and show that distributional approximations of related test\nstatistics based on sample autocovariances of random fields can be obtained by\nthe subsampling approach. As in applications the spatial-temporal correlations\nare often quite local, such that a large number of autocovariances vanish or\nare negligible, we also investigate a thresholding approach where sample\nautocovariances of small magnitude are omitted. Extensive simulation studies\nshow that the proposed estimators work well in practice and, when used to\nstandardize image test statistics, can provide highly accurate image testing\nprocedures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 17:35:34 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 16:22:06 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 17:22:51 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Prause", "Annabel", ""], ["Steland", "Ansgar", ""]]}, {"id": "1708.08479", "submitter": "Steven Gratton", "authors": "Steven Gratton", "title": "GLASS: A General Likelihood Approximate Solution Scheme", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for constructing suitable posterior probability\ndistributions in situations for which the sampling distribution of the data is\nnot known. This is very useful for modern scientific data analysis in the era\nof \"big data\", for which exact likelihoods are commonly either unknown,\ncomputationally prohibitively expensive or inapplicable because of systematic\neffects in the data. The scheme involves implicitly computing the changes in an\napproximate sampling distribution as model parameters are changed via\nexplicitly-computed moments of statistics constructed from the data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 18:39:16 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Gratton", "Steven", ""]]}, {"id": "1708.08505", "submitter": "Johannes Krebs", "authors": "Johannes T. N. Krebs", "title": "A Note on Exponential Inequalities in Hilbert Spaces for Spatial\n  Processes with Applications to the Functional Kernel Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we present exponential inequalities for spatial lattice\nprocesses which take values in a separable Hilbert space and satisfy certain\ndependence conditions. We consider two types of dependence: spatial data under\n$\\alpha$-mixing conditions and spatial data which satisfies a weak dependence\ncondition introduced by Dedecker and Prieur [2005]. We demonstrate their\nusefulness in the functional kernel regression model of Ferraty and Vieu [2004]\nwhere we study uniform consistency properties of the estimated regression\noperator on increasing subsets of the underlying function space.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 19:55:47 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 06:37:39 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Krebs", "Johannes T. N.", ""]]}, {"id": "1708.08522", "submitter": "Edward Kao", "authors": "Edward K. Kao", "title": "Causal Inference Under Network Interference: A Framework for Experiments\n  on Social Networks", "comments": "PhD Thesis at Harvard Department of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No man is an island, as individuals interact and influence one another daily\nin our society. When social influence takes place in experiments on a\npopulation of interconnected individuals, the treatment on a unit may affect\nthe outcomes of other units, a phenomenon known as interference. This thesis\ndevelops a causal framework and inference methodology for experiments where\ninterference takes place on a network of influence (i.e. network interference).\nIn this framework, the network potential outcomes serve as the key quantity and\nflexible building blocks for causal estimands that represent a variety of\nprimary, peer, and total treatment effects. These causal estimands are\nestimated via principled Bayesian imputation of missing outcomes. The theory on\nthe unconfoundedness assumptions leading to simplified imputation highlights\nthe importance of including relevant network covariates in the potential\noutcome model. Additionally, experimental designs that result in balanced\ncovariates and sizes across treatment exposure groups further improve the\ncausal estimate, especially by mitigating potential outcome model\nmis-specification. The true potential outcome model is not typically known in\nreal-world experiments, so the best practice is to account for interference and\nconfounding network covariates through both balanced designs and model-based\nimputation. A full factorial simulated experiment is formulated to demonstrate\nthis principle by comparing performance across different randomization schemes\nduring the design phase and estimators during the analysis phase, under varying\nnetwork topology and true potential outcome models. Overall, this thesis\nasserts that interference is not just a nuisance for analysis but rather an\nopportunity for quantifying and leveraging peer effects in real-world\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 20:50:35 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Kao", "Edward K.", ""]]}, {"id": "1708.08587", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos, and Jian Ma", "title": "On the Reconstruction Risk of Convolutional Sparse Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse dictionary learning (SDL) has become a popular method for adaptively\nidentifying parsimonious representations of a dataset, a fundamental problem in\nmachine learning and signal processing. While most work on SDL assumes a\ntraining dataset of independent and identically distributed samples, a variant\nknown as convolutional sparse dictionary learning (CSDL) relaxes this\nassumption, allowing more general sequential data sources, such as time series\nor other dependent data. Although recent work has explored the statistical\nproperties of classical SDL, the statistical properties of CSDL remain\nunstudied. This paper begins to study this by identifying the minimax\nconvergence rate of CSDL in terms of reconstruction risk, by both upper\nbounding the risk of an established CSDL estimator and proving a matching\ninformation-theoretic lower bound. Our results indicate that consistency in\nreconstruction risk is possible precisely in the `ultra-sparse' setting, in\nwhich the sparsity (i.e., the number of feature occurrences) is in $o(N)$ in\nterms of the length N of the training sequence. Notably, our results make very\nweak assumptions, allowing arbitrary dictionaries and dependent measurement\nnoise. Finally, we verify our theoretical results with numerical experiments on\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 04:15:39 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 23:49:15 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Ma", "Jian", ""]]}, {"id": "1708.08604", "submitter": "Mohammad Arashi", "authors": "M. Kazemi, D. Shahsavani, M. Arashi", "title": "A sure independence screening procedure for ultra-high dimensional\n  partially linear additive models", "comments": "22 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two-step procedure, in the context of ultra-high dimensional\nadditive models, which aims to reduce the size of covariates vector and\ndistinguish linear and nonlinear effects among nonzero components. Our proposed\nscreening procedure, in the first step, is constructed based on the concept of\ncumulative distribution function and conditional expectation of response in the\nframework of marginal correlation. B-splines and empirical distribution\nfunctions are used to estimate the two above measures. The sure property of\nthis procedure is also established. In the second step, a double penalization\nbased procedure is applied to identify nonzero and linear components,\nsimultaneously. The performance of the designed method is examined by several\ntest functions to show its capabilities against competitor methods when errors\ndistribution are varied. Simulation studies imply that the proposed screening\nprocedure can be applied to the ultra-high dimensional data and well detect the\nin uential covariates. It is also demonstrate the superiority in comparison\nwith the existing methods. This method is also applied to identify most in\nuential genes for overexpression of a G protein-coupled receptor in mice.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 06:45:42 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Kazemi", "M.", ""], ["Shahsavani", "D.", ""], ["Arashi", "M.", ""]]}, {"id": "1708.08688", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Further Results on Size and Power of Heteroskedasticity and\n  Autocorrelation Robust Tests, with an Application to Trend Testing", "comments": "Revised version. Some restructuring, some errors corrected, new\n  results added", "journal-ref": "Electronic Journal of Statistics 13 (2019), 3893-3942", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We complement the theory developed in Preinerstorfer and P\\\"otscher (2016)\nwith further finite sample results on size and power of heteroskedasticity and\nautocorrelation robust tests. These allows us, in particular, to show that the\nsufficient conditions for the existence of size-controlling critical values\nrecently obtained in P\\\"otscher and Preinerstorfer (2018) are often also\nnecessary. We furthermore apply the results obtained to tests for hypotheses on\ndeterministic trends in stationary time series regressions, and find that many\ntests currently used are strongly size-distorted.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:44:22 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 10:04:22 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "1708.08734", "submitter": "Veronika Rockova", "authors": "Veronika Rockova and Stephanie van der Pas", "title": "Posterior Concentration for Bayesian Regression Trees and Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their inception in the 1980's, regression trees have been one of the\nmore widely used non-parametric prediction methods. Tree-structured methods\nyield a histogram reconstruction of the regression surface, where the bins\ncorrespond to terminal nodes of recursive partitioning. Trees are powerful, yet\nsusceptible to over-fitting. Strategies against overfitting have traditionally\nrelied on pruning greedily grown trees. The Bayesian framework offers an\nalternative remedy against overfitting through priors. Roughly speaking, a good\nprior charges smaller trees where overfitting does not occur. While the\nconsistency of random histograms, trees and their ensembles has been studied\nquite extensively, the theoretical understanding of the Bayesian counterparts\nhas been missing. In this paper, we take a step towards understanding why/when\ndo Bayesian trees and their ensembles not overfit. To address this question, we\nstudy the speed at which the posterior concentrates around the true smooth\nregression function. We propose a spike-and-tree variant of the popular\nBayesian CART prior and establish new theoretical results showing that\nregression trees (and their ensembles) (a) are capable of recovering smooth\nregression surfaces, achieving optimal rates up to a log factor, (b) can adapt\nto the unknown level of smoothness and (c) can perform effective dimension\nreduction when p>n. These results provide a piece of missing theoretical\nevidence explaining why Bayesian trees (and additive variants thereof) have\nworked so well in practice.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:16:18 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 13:58:40 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 13:44:20 GMT"}, {"version": "v4", "created": "Tue, 24 Jul 2018 19:18:24 GMT"}, {"version": "v5", "created": "Sat, 19 Jan 2019 15:48:32 GMT"}, {"version": "v6", "created": "Thu, 13 Jun 2019 20:55:09 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Rockova", "Veronika", ""], ["van der Pas", "Stephanie", ""]]}, {"id": "1708.09004", "submitter": "Yasin Asar", "authors": "Yasin Asar, Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Mohammad Arashi, Jibo Wu", "title": "Preliminary testing derivatives of a linear unified estimator in the\n  logistic regression model", "comments": "16 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the well known Liu estimator (Liu, 1993) is attracted researcher's\nattention in regression parameter estimation for an ill conditioned linear\nmodel. It is also argued that imposing sub-space hypothesis restriction on\nparameters improves estimation by shrinking toward non-sample information.\nChang (2015) proposed the almost unbiased Liu estimator (AULE) in the binary\nlogistic regression. In this article, some improved unbiased Liu type\nestimators, namely, restricted AULE, preliminary test AULE, Stein-type\nshrinkage AULE and its positive part for estimating the regression parameters\nin the binary logistic regression model are proposed based on the work Chang\n(2015). The performances of the newly defined estimators are analysed through\nsome numerical results. A real data example is also provided to support the\nfindings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:10:37 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Asar", "Yasin", ""], ["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Arashi", "Mohammad", ""], ["Wu", "Jibo", ""]]}, {"id": "1708.09211", "submitter": "Norbert Henze", "authors": "Norbert Henze", "title": "On the consistency of the spacings test for multivariate uniformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple conceptual proof of the consistency of a test for\nmultivariate uniformity in a bounded set $K \\subset \\mathbb{R}^d$ that is based\non the maximal spacing generated by i.i.d. points $X_1, \\ldots,X_n$ in $K$,\ni.e., the volume of the largest convex set of a given shape that is contained\nin $K$ and avoids each of these points. Since asymptotic results for the case\n$d > 1$ are only availabe under uniformity, a key element of the proof is a\nsuitable coupling.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 10:55:54 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Henze", "Norbert", ""]]}, {"id": "1708.09305", "submitter": "Anthony Hou", "authors": "Jiajie Chen, Anthony Hou, Thomas Y. Hou", "title": "A Pseudo Knockoff Filter for Correlated Features", "comments": "25 pages, 5 figures", "journal-ref": "Information and Inference: A Journal of the IMA, Volume 8, Issue\n  2, June 2019, Pages 313-341", "doi": "10.1093/imaiai/iay012", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, Barber and Candes introduced a new variable selection procedure\ncalled the knockoff filter to control the false discovery rate (FDR) and prove\nthat this method achieves exact FDR control. Inspired by the work of Barber and\nCandes (2015), we propose and analyze a pseudo-knockoff filter that inherits\nsome advantages of the original knockoff filter and has more flexibility in\nconstructing its knockoff matrix. Moreover, we perform a number of numerical\nexperiments that seem to suggest that the pseudo knockoff filter with the half\nLasso statistic has FDR control and offers more power than the original\nknockoff filter with the Lasso Path or the half Lasso Statistic for the\nnumerical examples that we consider in this paper. Although we cannot establish\nrigorous FDR control for the pseudo knockoff filter, we provide some partial\nanalysis of the pseudo knockoff filter with the half Lasso statistic and\nestablish a uniform FDP bound and an expectation inequality.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 14:44:01 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 17:00:08 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 19:47:45 GMT"}, {"version": "v4", "created": "Mon, 22 Jul 2019 04:26:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Jiajie", ""], ["Hou", "Anthony", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "1708.09468", "submitter": "Richard Samworth", "authors": "Qiyang Han, Tengyao Wang, Sabyasachi Chatterjee and Richard J.\n  Samworth", "title": "Isotonic regression in general dimensions", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the least squares regression function estimator over the class of\nreal-valued functions on $[0,1]^d$ that are increasing in each coordinate. For\nuniformly bounded signals and with a fixed, cubic lattice design, we establish\nthat the estimator achieves the minimax rate of order\n$n^{-\\min\\{2/(d+2),1/d\\}}$ in the empirical $L_2$ loss, up to poly-logarithmic\nfactors. Further, we prove a sharp oracle inequality, which reveals in\nparticular that when the true regression function is piecewise constant on $k$\nhyperrectangles, the least squares estimator enjoys a faster, adaptive rate of\nconvergence of $(k/n)^{\\min(1,2/d)}$, again up to poly-logarithmic factors.\nPrevious results are confined to the case $d \\leq 2$. Finally, we establish\ncorresponding bounds (which are new even in the case $d=2$) in the more\nchallenging random design setting. There are two surprising features of these\nresults: first, they demonstrate that it is possible for a global empirical\nrisk minimisation procedure to be rate optimal up to poly-logarithmic factors\neven when the corresponding entropy integral for the function class diverges\nrapidly; second, they indicate that the adaptation rate for shape-constrained\nestimators can be strictly worse than the parametric rate.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 20:53:32 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Han", "Qiyang", ""], ["Wang", "Tengyao", ""], ["Chatterjee", "Sabyasachi", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1708.09502", "submitter": "Mark Van Der Laan J.", "authors": "Mark van der Laan", "title": "Finite Sample Inference for Targeted Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Highly-Adaptive-Lasso(HAL)-TMLE is an efficient estimator of a pathwise\ndifferentiable parameter in a statistical model that at minimal (and possibly\nonly) assumes that the sectional variation norm of the true nuisance parameters\nare finite. It relies on an initial estimator (HAL-MLE) of the nuisance\nparameters by minimizing the empirical risk over the parameter space under the\nconstraint that sectional variation norm is bounded by a constant, where this\nconstant can be selected with cross-validation. In the formulation of the\nHALMLE this sectional variation norm corresponds with the sum of absolute value\nof coefficients for an indicator basis. Due to its reliance on machine\nlearning, statistical inference for the TMLE has been based on its normal limit\ndistribution, thereby potentially ignoring a large second order remainder in\nfinite samples.\n  In this article, we present four methods for construction of a finite sample\n0.95-confidence interval that use the nonparametric bootstrap to estimate the\nfinite sample distribution of the HAL-TMLE or a conservative distribution\ndominating the true finite sample distribution. We prove that it consistently\nestimates the optimal normal limit distribution, while its approximation error\nis driven by the performance of the bootstrap for a well behaved empirical\nprocess. We demonstrate our general inferential methods for 1) nonparametric\nestimation of the average treatment effect based on observing on each unit a\ncovariate vector, binary treatment, and outcome, and for 2) nonparametric\nestimation of the integral of the square of the multivariate density of the\ndata distribution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 23:32:37 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["van der Laan", "Mark", ""]]}, {"id": "1708.09555", "submitter": "Giovanni Pistone", "authors": "Giovanni Pistone", "title": "Translations in the exponential Orlicz space with Gaussian weight", "comments": "Submitted to GSI 2017, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the continuity of space translations on non-parametric exponential\nfamilies based on the exponential Orlicz space with Gaussian reference density.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 04:18:57 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Pistone", "Giovanni", ""]]}, {"id": "1708.09608", "submitter": "Ulrike Schneider", "authors": "Karl Ewald, Ulrike Schneider", "title": "On the Distribution, Model Selection Properties and Uniqueness of the\n  Lasso Estimator in Low and High Dimensions", "comments": "restructured proofs, added examples", "journal-ref": "Electron. J. Statist. 14 (2020), 944-969", "doi": "10.1214/20-EJS1687", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive expressions for the finite-sample distribution of the Lasso\nestimator in the context of a linear regression model in low as well as in high\ndimensions by exploiting the structure of the optimization problem defining the\nestimator. In low dimensions, we assume full rank of the regressor matrix and\npresent expressions for the cumulative distribution function as well as the\ndensities of the absolutely continuous parts of the estimator. Our results are\npresented for the case of normally distributed errors, but do not hinge on this\nassumption and can easily be generalized. Additionally, we establish an\nexplicit formula for the correspondence between the Lasso and the least-squares\nestimator. We derive analogous results for the distribution in less explicit\nform in high dimensions where we make no assumptions on the regressor matrix at\nall. In this setting, we also investigate the model selection properties of the\nLasso and show that possibly only a subset of models might be selected by the\nestimator, completely independently of the observed response vector. Finally,\nwe present a condition for uniqueness of the estimator that is necessary as\nwell as sufficient.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 08:16:18 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 21:58:15 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 16:35:30 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ewald", "Karl", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1708.09657", "submitter": "Niels Richard Hansen", "authors": "Niels Richard Hansen", "title": "A comment on Stein's unbiased risk estimate for reduced rank estimators", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of matrix valued observables with low rank means, Stein's\nunbiased risk estimate (SURE) can be useful for risk estimation and for tuning\nthe amount of shrinkage towards low rank matrices. This was demonstrated by\nCand\\`es et al. (2013) for singular value soft thresholding, which is a\nLipschitz continuous estimator. SURE provides an unbiased risk estimate for an\nestimator whenever the differentiability requirements for Stein's lemma are\nsatisfied. Lipschitz continuity of the estimator is sufficient, but it is\nemphasized that differentiability Lebesgue almost everywhere isn't. The reduced\nrank estimator, which gives the best approximation of the observation with a\nfixed rank, is an example of a discontinuous estimator for which Stein's lemma\nactually applies. This was observed by Mukherjee et al. (2015), but the proof\nwas incomplete. This brief note gives a sufficient condition for Stein's lemma\nto hold for estimators with discontinuities, which is then shown to be\nfulfilled for a class of spectral function estimators including the reduced\nrank estimator. Singular value hard thresholding does, however, not satisfy the\ncondition, and Stein's lemma does not apply to this estimator.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 10:37:27 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Hansen", "Niels Richard", ""]]}, {"id": "1708.09692", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Tuhin Majumder, Ayanendranath Basu", "title": "General Robust Bayes Pseudo-Posterior: Exponential Convergence results\n  with Applications", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Bayesian inference is an immensely popular paradigm among a large\nsegment of scientists including statisticians, most applications consider\nobjective priors and need critical investigations (Efron, 2013, Science). While\nit has several optimal properties, a major drawback of Bayesian inference is\nthe lack of robustness against data contamination and model misspecification,\nwhich becomes pernicious in the use of objective priors. This paper presents\nthe general formulation of a Bayes pseudo-posterior distribution yielding\nrobust inference. Exponential convergence results related to the new\npseudo-posterior and the corresponding Bayes estimators are established under\nthe general parametric set-up and illustrations are provided for the\nindependent stationary as well as non-homogeneous models. Several additional\ndetails and properties of the procedure are described, including the estimation\nunder fixed-design regression models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:58:11 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 10:02:54 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ghosh", "Abhik", ""], ["Majumder", "Tuhin", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1708.09708", "submitter": "Harald Oberhauser", "authors": "Terry Lyons, Harald Oberhauser", "title": "Sketching the order of events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce features for massive data streams. These stream features can be\nthought of as \"ordered moments\" and generalize stream sketches from \"moments of\norder one\" to \"ordered moments of arbitrary order\". In analogy to classic\nmoments, they have theoretical guarantees such as universality that are\nimportant for learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:51:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Lyons", "Terry", ""], ["Oberhauser", "Harald", ""]]}]