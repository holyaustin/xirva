[{"id": "1311.0035", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Arian Maleki, Richard G. Baraniuk", "title": "Parameterless Optimal Approximate Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative thresholding algorithms are well-suited for high-dimensional\nproblems in sparse recovery and compressive sensing. The performance of this\nclass of algorithms depends heavily on the tuning of certain threshold\nparameters. In particular, both the final reconstruction error and the\nconvergence rate of the algorithm crucially rely on how the threshold parameter\nis set at each step of the algorithm. In this paper, we propose a\nparameter-free approximate message passing (AMP) algorithm that sets the\nthreshold parameter at each iteration in a fully automatic way without either\nhaving an information about the signal to be reconstructed or needing any\ntuning from the user. We show that the proposed method attains both the minimum\nreconstruction error and the highest convergence rate. Our method is based on\napplying the Stein unbiased risk estimate (SURE) along with a modified gradient\ndescent to find the optimal threshold in each iteration. Motivated by the\nconnections between AMP and LASSO, it could be employed to find the solution of\nthe LASSO for the optimal regularization parameter. To the best of our\nknowledge, this is the first work concerning parameter tuning that obtains the\nfastest convergence rate with theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 21:03:00 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Mousavi", "Ali", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1311.0072", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini and XuanLong Nguyen", "title": "Bayesian inference as iterated random functions with applications to\n  sequential inference in graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general formalism of iterated random functions with semigroup\nproperty, under which exact and approximate Bayesian posterior updates can be\nviewed as specific instances. A convergence theory for iterated random\nfunctions is presented. As an application of the general theory we analyze\nconvergence behaviors of exact and approximate message-passing algorithms that\narise in a sequential change point detection problem formulated via a latent\nvariable directed graphical model. The sequential inference algorithm and its\nsupporting theory are illustrated by simulated examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 02:17:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Amini", "Arash A.", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1311.0274", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional\n  Regression", "comments": "21 pages, short version appears in Annual Allerton Conference on\n  Communication, Control and Computing, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting the parameters of a high-dimensional\nlinear regression model. In the regime where the number of parameters $p$ is\ncomparable to or exceeds the sample size $n$, a successful approach uses an\n$\\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,\nunlike for linear estimators (e.g., ordinary least squares), no\nwell-established method exists to compute confidence intervals or p-values on\nthe basis of the Lasso estimator. Very recently, a line of work\n\\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this\nproblem by constructing a debiased version of the Lasso estimator. In this\npaper, we study this approach for random design model, under the assumption\nthat a good estimator exists for the precision matrix of the design. Our\nanalysis improves over the state of the art in that it establishes nearly\noptimal \\emph{average} testing power if the sample size $n$ asymptotically\ndominates $s_0 (\\log p)^2$, with $s_0$ being the sparsity level (number of\nnon-zero coefficients). Earlier work obtains provable guarantees only for much\nlarger sample size, namely it requires $n$ to asymptotically dominate $(s_0\n\\log p)^2$.\n  In particular, for random designs with a sparse precision matrix we show that\nan estimator thereof having the required properties can be computed\nefficiently. Finally, we evaluate this approach on synthetic data and compare\nit with earlier proposals.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:41:42 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1311.0412", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen and Timothy Christensen", "title": "Optimal Uniform Convergence Rates for Sieve Nonparametric Instrumental\n  Variables Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric regression when the regressor is\nendogenous, which is an important nonparametric instrumental variables (NPIV)\nregression in econometrics and a difficult ill-posed inverse problem with\nunknown operator in statistics. We first establish a general upper bound on the\nsup-norm (uniform) convergence rate of a sieve estimator, allowing for\nendogenous regressors and weakly dependent data. This result leads to the\noptimal sup-norm convergence rates for spline and wavelet least squares\nregression estimators under weakly dependent data and heavy-tailed error terms.\nThis upper bound also yields the sup-norm convergence rates for sieve NPIV\nestimators under i.i.d. data: the rates coincide with the known optimal\n$L^2$-norm rates for severely ill-posed problems, and are power of $\\log(n)$\nslower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then\nestablish the minimax risk lower bound in sup-norm loss, which coincides with\nour upper bounds on sup-norm rates for the spline and wavelet sieve NPIV\nestimators. This sup-norm rate optimality provides another justification for\nthe wide application of sieve NPIV estimators. Useful results on\nweakly-dependent random matrices are also provided.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 21:25:13 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""]]}, {"id": "1311.0540", "submitter": "Philippe Barbe", "authors": "Ph. Barbe (CNRS), Miriam Isabel Seifert (Helmut Schmidt Universitat)", "title": "A conditional limit theorem for a bivariate representation of a\n  univariate random variable and conditional extreme values", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a real random variable X represented through a random pair of\nreal random variables (R,T) and a deterministic function u as X=Ru(T). Under\nsome additional assumptions, we prove a limit theorem for (R,T) given X>x, as x\ntends to infinity. As a consequence, we derive conditional limit theorems for\nrandom pairs (X,Y)=(Ru(T),Rv(T)) given that X is large. These results imply\nearlier ones which were obtained in the literature under stronger assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 22:59:35 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Barbe", "Ph.", "", "CNRS"], ["Seifert", "Miriam Isabel", "", "Helmut Schmidt Universitat"]]}, {"id": "1311.0562", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen, Subhadeep Mukhopadhyay", "title": "LP Mixed Data Science : Outline of Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the theoretical foundation of a new frontier of\nresearch-`LP Mixed Data Science'-that simultaneously extends and integrates the\npractice of traditional and novel statistical methods for nonparametric\nexploratory data modeling, and is applicable to the teaching and training of\nstatistics.\n  Statistics journals have great difficulty accepting papers unlike those\npreviously published. For statisticians with new big ideas a practical strategy\nis to publish them in many small applied studies which enables one to provide\nreferences to work of others. This essay outlines the many concepts, new\ntheory, and important algorithms of our new culture of statistical science\ncalled LP MIXED DATA SCIENCE. It provides comprehensive solutions to problems\nof data analysis and nonparametric modeling of many variables that are\ncontinuous or discrete, which does not yet have a large literature. It develops\na new modeling approach to nonparametric estimation of the multivariate copula\ndensity. We discuss the theory which we believe is very elegant (and can\nprovide a framework for United Statistical Algorithms, for traditional Small\nData methods and Big Data methods).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 01:56:04 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2013 13:44:49 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Parzen", "Emanuel", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1311.0587", "submitter": "Gerard Biau", "authors": "G\\'erard Biau (LSTA, LPMA, DMA, INRIA Paris - Rocquencourt), David\n  D.M. Mason", "title": "High-dimensional $p$-norms", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\bX=(X_1, \\hdots, X_d)$ be a $\\mathbb R^d$-valued random vector with\ni.i.d. components, and let $\\Vert\\bX\\Vert_p= (\\sum_{j=1}^d|X_j|^p)^{1/p}$ be\nits $p$-norm, for $p>0$. The impact of letting $d$ go to infinity on\n$\\Vert\\bX\\Vert_p$ has surprising consequences, which may dramatically affect\nhigh-dimensional data processing. This effect is usually referred to as the\n{\\it distance concentration phenomenon} in the computational learning\nliterature. Despite a growing interest in this important question, previous\nwork has essentially characterized the problem in terms of numerical\nexperiments and incomplete mathematical statements. In the present paper, we\nsolidify some of the arguments which previously appeared in the literature and\noffer new insights into the phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 05:49:37 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, LPMA, DMA, INRIA Paris - Rocquencourt"], ["Mason", "David D. M.", ""]]}, {"id": "1311.0594", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik and Ami Wiesel", "title": "Covariance Estimation in Elliptical Models with Convex Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address structured covariance estimation in Elliptical distribution. We\nassume it is a priori known that the covariance belongs to a given convex set,\ne.g., the set of Toeplitz or banded matrices. We consider the General Method of\nMoments (GMM) optimization subject to these convex constraints. Unfortunately,\nGMM is still non-convex due to objective. Instead, we propose COCA - a convex\nrelaxation which can be efficiently solved. We prove that the relaxation is\ntight in the unconstrained case for a finite number of samples, and in the\nconstrained case asymptotically. We then illustrate the advantages of COCA in\nsynthetic simulations with structured Compound Gaussian distributions. In these\nexamples, COCA outperforms competing methods as Tyler's estimate and its\nprojection onto a convex set.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 06:51:51 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Wiesel", "Ami", ""]]}, {"id": "1311.0662", "submitter": "Peter Forbes", "authors": "Peter G. M. Forbes and Steffen Lauritzen", "title": "Linear Estimating Equations for Exponential Families with Application to\n  Gaussian Linear Concentration Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.laa.2014.08.015", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many families of distributions, maximum likelihood estimation is\nintractable because the normalization constant for the density which enters\ninto the likelihood function is not easily available. The score matching\nestimator of Hyv\\\"arinen (2005) provides an alternative where this\nnormalization constant is not required. The corresponding estimating equations\nbecome linear for an exponential family. The score matching estimator is shown\nto be consistent and asymptotically normally distributed for such models,\nalthough not necessarily efficient. Gaussian linear concentration models are\nexamples of such families. For linear concentration models that are also linear\nin the covariance we show that the score matching estimator is identical to the\nmaximum likelihood estimator, hence in such cases it is also efficient.\nGaussian graphical models and graphical models with symmetries form\nparticularly interesting subclasses of linear concentration models and we\ninvestigate the potential use of the score matching estimator for this case.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 12:05:46 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 13:11:35 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Forbes", "Peter G. M.", ""], ["Lauritzen", "Steffen", ""]]}, {"id": "1311.0669", "submitter": "Xiaoping Yuan", "authors": "Wencai Liu and Xiaoping Yuan", "title": "H\\\"{o}lder Continuity of the Spectral Measures for One-Dimensional\n  Schr\\\"{o}dinger Operator in Exponential Regime", "comments": null, "journal-ref": "J. Math. Phys. 56 (2015), no. 1, 012701, 21 pp", "doi": "10.1063/1.4904835", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Avila and Jitomirskaya prove that the spectral measure $\\mu_{\\lambda v,\n\\alpha,x}^f$ of quasi-periodic Schr\\\"{o}dinger operator is $1/2$-H\\\"{o}lder\ncontinuous with appropriate initial vector $f$, if $\\alpha $ satisfies\nDiophantine condition and $\\lambda$ is small. In the present paper, the\nconclusion is extended to that for all $\\alpha$ with $\\beta(\\alpha)<\\infty$,\nthe spectral measure $\\mu_{\\lambda v, \\alpha,x}^f$ is $1/2$-H\\\"{o}lder\ncontinuous with small $\\lambda$, if $v$ is real analytic in a neighbor of\n$\\{|\\Im x|\\leq C\\beta\\}$, where $C$ is a large absolute constant. In\nparticular, the spectral measure $\\mu_{\\lambda, \\alpha,x}^f$ of almost Mathieu\noperator is $1/2$-H\\\"{o}lder continuous if $|\\lambda|<e^{-C\\beta}$ with $C$ a\nlarge absolute constant.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 12:18:16 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Liu", "Wencai", ""], ["Yuan", "Xiaoping", ""]]}, {"id": "1311.0737", "submitter": "Daniel Romero", "authors": "Daniel Romero, Roberto Lopez-Valcarce and Geert Leus", "title": "Compression Limits for Random Vectors with Linearly Parameterized\n  Second-Order Statistics", "comments": "15 pages, 2 figures, 1 table, submitted to IEEE Transactions on\n  Information Theory on Nov. 4, 2013", "journal-ref": null, "doi": "10.1109/TIT.2015.2394784", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of complex random vectors whose covariance matrix is linearly\nparameterized by a basis of Hermitian Toeplitz (HT) matrices is considered, and\nthe maximum compression ratios that preserve all second-order information are\nderived --- the statistics of the uncompressed vector must be recoverable from\na set of linearly compressed observations. This kind of vectors arises\nnaturally when sampling wide-sense stationary random processes and features a\nnumber of applications in signal and array processing.\n  Explicit guidelines to design optimal and nearly optimal schemes operating\nboth in a periodic and non-periodic fashion are provided by considering two of\nthe most common linear compression schemes, which we classify as dense or\nsparse. It is seen that the maximum compression ratios depend on the structure\nof the HT subspace containing the covariance matrix of the uncompressed\nobservations. Compression patterns attaining these maximum ratios are found for\nthe case without structure as well as for the cases with circulant or banded\nstructure. Universal samplers are also proposed to compress unknown HT\nsubspaces.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 15:44:30 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 09:49:04 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2013 15:33:15 GMT"}, {"version": "v4", "created": "Fri, 30 Jan 2015 16:23:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Romero", "Daniel", ""], ["Lopez-Valcarce", "Roberto", ""], ["Leus", "Geert", ""]]}, {"id": "1311.0811", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and Laurent A.F. Callot", "title": "Oracle Inequalities for High Dimensional Vector Autoregressions", "comments": "39 pages. This is a revised version of an article not previously\n  circulated on arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes non-asymptotic oracle inequalities for the prediction\nerror and estimation accuracy of the LASSO in stationary vector autoregressive\nmodels. These inequalities are used to establish consistency of the LASSO even\nwhen the number of parameters is of a much larger order of magnitude than the\nsample size. We also give conditions under which no relevant variables are\nexcluded.\n  Next, non-asymptotic probabilities are given for the Adaptive LASSO to select\nthe correct sparsity pattern. We then give conditions under which the Adaptive\nLASSO reveals the correct sparsity pattern asymptotically. We establish that\nthe estimates of the non-zero coefficients are asymptotically equivalent to the\noracle assisted least squares estimator. This is used to show that the rate of\nconvergence of the estimates of the non-zero coefficients is identical to the\none of least squares only including the relevant covariates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 18:47:25 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 09:41:21 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Callot", "Laurent A. F.", ""]]}, {"id": "1311.0851", "submitter": "Matan Gavish", "authors": "David L. Donoho, Matan Gavish and Iain M. Johnstone", "title": "Optimal Shrinkage of Eigenvalues in the Spiked Covariance Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that in a common high-dimensional covariance model, the choice of\nloss function has a profound effect on optimal estimation. In an asymptotic\nframework based on the Spiked Covariance model and use of orthogonally\ninvariant estimators, we show that optimal estimation of the population\ncovariance matrix boils down to design of an optimal shrinker $\\eta$ that acts\nelementwise on the sample eigenvalues. Indeed, to each loss function there\ncorresponds a unique admissible eigenvalue shrinker $\\eta^*$ dominating all\nother shrinkers. The shape of the optimal shrinker is determined by the choice\nof loss function and, crucially, by inconsistency of both eigenvalues and\neigenvectors of the sample covariance matrix. Details of these phenomena and\nclosed form formulas for the optimal eigenvalue shrinkers are worked out for a\nmenagerie of 26 loss functions for covariance estimation found in the\nliterature, including the Stein, Entropy, Divergence, Frechet,\nBhattacharya/Matusita, Frobenius Norm, Operator Norm, Nuclear Norm and\nCondition Number losses.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:55:31 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 06:37:49 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 20:46:43 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Donoho", "David L.", ""], ["Gavish", "Matan", ""], ["Johnstone", "Iain M.", ""]]}, {"id": "1311.1027", "submitter": "Laurent Decreusefond", "authors": "Laurent Decreusefond (LTCI), Ian Flint (LTCI), Kah Choon Low (LTCI)", "title": "Perfect Simulation of Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPP) serve as a practicable modeling for many\napplications of repulsive point processes. A known approach for simulation was\nproposed in \\cite{Hough(2006)}, which generate the desired distribution point\nwise through rejection sampling. Unfortunately, the size of rejection could be\nvery large. In this paper, we investigate the application of perfect simulation\nvia coupling from the past (CFTP) on DPP. We give a general framework for\nperfect simulation on DPP model. It is shown that the limiting sequence of the\ntime-to-coalescence of the coupling is bounded by $K|\\Lambda|\\log K|\\Lambda|$.\nAn application is given to the stationary models in DPP.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 12:21:20 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Decreusefond", "Laurent", "", "LTCI"], ["Flint", "Ian", "", "LTCI"], ["Low", "Kah Choon", "", "LTCI"]]}, {"id": "1311.1067", "submitter": "Christele Bioche", "authors": "Christele Bioche, Pierre Druilhet", "title": "Approximation of improper priors", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ708 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 3, 1709-1728", "doi": "10.3150/15-BEJ708", "report-no": "IMS-BEJ-BEJ708", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convergence mode for positive Radon measures which allows a\nsequence of probability measures to have an improper limiting measure. We\ndefine a sequence of vague priors as a sequence of probability measures that\nconverges to an improper prior. We consider some cases where vague priors have\nnecessarily large variances and other cases where they have not. We study the\nconsequences of the convergence of prior distributions on the posterior\nanalysis. Then we give some constructions of vague priors that approximate the\nHaar measures or the Jeffreys priors. We also revisit the Jeffreys-Lindley\nparadox.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 14:31:51 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 11:14:22 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Bioche", "Christele", ""], ["Druilhet", "Pierre", ""]]}, {"id": "1311.1131", "submitter": "Peter Forbes", "authors": "Peter G. M. Forbes", "title": "Compatible Weighted Proper Scoring Rules", "comments": null, "journal-ref": "Biometrika (2012) 99 (4): 989-994", "doi": "10.1093/biomet/ass046", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many proper scoring rules such as the Brier and log scoring rules implicitly\nreward a probability forecaster relative to a uniform baseline distribution.\nRecent work has motivated weighted proper scoring rules, which have an\nadditional baseline parameter. To date two families of weighted proper scoring\nrules have been introduced, the weighted power and pseudospherical scoring\nfamilies. These families are compatible with the log scoring rule: when the\nbaseline maximizes the log scoring rule over some set of distributions, the\nbaseline also maximizes the weighted power and pseudospherical scoring rules\nover the same set. We characterize all weighted proper scoring families and\nprove a general property: every proper scoring rule is compatible with some\nweighted scoring family, and every weighted scoring family is compatible with\nsome proper scoring rule.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 17:23:29 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Forbes", "Peter G. M.", ""]]}, {"id": "1311.1138", "submitter": "Sergios Agapiou", "authors": "Sergios Agapiou, Johnathan M. Bardsley, Omiros Papaspiliopoulos,\n  Andrew M. Stuart", "title": "Analysis of the Gibbs sampler for hierarchical inverse problems", "comments": "to appear, SIAM/ASA Journal on Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inverse problems arising in applications come from continuum models\nwhere the unknown parameter is a field. In practice the unknown field is\ndiscretized resulting in a problem in $\\mathbb{R}^N$, with an understanding\nthat refining the discretization, that is increasing $N$, will often be\ndesirable. In the context of Bayesian inversion this situation suggests the\nimportance of two issues: (i) defining hyper-parameters in such a way that they\nare interpretable in the continuum limit $N \\to \\infty$ and so that their\nvalues may be compared between different discretization levels; (ii)\nunderstanding the efficiency of algorithms for probing the posterior\ndistribution, as a function of large $N.$ Here we address these two issues in\nthe context of linear inverse problems subject to additive Gaussian noise\nwithin a hierarchical modelling framework based on a Gaussian prior for the\nunknown field and an inverse-gamma prior for a hyper-parameter, namely the\namplitude of the prior variance. The structure of the model is such that the\nGibbs sampler can be easily implemented for probing the posterior distribution.\nSubscribing to the dogma that one should think infinite-dimensionally before\nimplementing in finite dimensions, we present function space intuition and\nprovide rigorous theory showing that as $N$ increases, the component of the\nGibbs sampler for sampling the amplitude of the prior variance becomes\nincreasingly slower. We discuss a reparametrization of the prior variance that\nis robust with respect to the increase in dimension; we give numerical\nexperiments which exhibit that our reparametrization prevents the slowing down.\nOur intuition on the behaviour of the prior hyper-parameter, with and without\nreparametrization, is sufficiently general to include a broad class of\nnonlinear inverse problems as well as other families of hyper-priors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 17:48:49 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 09:38:30 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 11:45:15 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Agapiou", "Sergios", ""], ["Bardsley", "Johnathan M.", ""], ["Papaspiliopoulos", "Omiros", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1311.1154", "submitter": "Song-Yon Kim Dr", "authors": "Kim Song Yon, Kim Mun Chol", "title": "Modeling of Volatility with Non-linear Time Series Model", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": "KISU-MATH-2011-E-C-022", "categories": "q-fin.ST math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, non-linear time series models are used to describe volatility\nin financial time series data. To describe volatility, two of the non-linear\ntime series are combined into form TAR (Threshold Auto-Regressive Model) with\nAARCH (Asymmetric Auto-Regressive Conditional Heteroskedasticity) error term\nand its parameter estimation is studied.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 10:19:18 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 10:12:00 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Yon", "Kim Song", ""], ["Chol", "Kim Mun", ""]]}, {"id": "1311.1283", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie, Masoud Asgharian and David A. Stephens", "title": "Variable Selection in Causal Inference Using Penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the causal adjustment setting, variable selection techniques based on\neither the outcome or treatment allocation model can result in the omission of\nconfounders or the inclusion of spurious variables in the propensity score. We\npropose a variable selection method based on a penalized likelihood which\nconsiders the response and treatment assignment models simultaneously. The\nproposed method facilitates confounder selection in high-dimensional settings.\nWe show that under some conditions our method attains the oracle property. The\nselected variables are used to form a double robust regression estimator of the\ntreatment effect. Simulation results are presented and economic growth data are\nanalyzed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 03:33:52 GMT"}, {"version": "v2", "created": "Thu, 5 Jun 2014 01:58:15 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Asgharian", "Masoud", ""], ["Stephens", "David A.", ""]]}, {"id": "1311.1400", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie, Masoud Asgharian and David Stephens", "title": "The Propensity Score Estimation in the Presence of Length-biased\n  Sampling: A Nonparametric Adjustment Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive use of prevalent cohort studies on disease duration,\nincreasingly calls for appropriate methodologies to account for the biases that\ninvariably accompany samples formed by such data. It is well-known, for\nexample, that subjects with shorter lifetime are less likely to be present in\nsuch studies. Moreover, certain covariate values could be preferentially\nselected into the sample, being linked to the long-term survivors. The existing\nmethodology for estimation of the propensity score using data collected on\nprevalent cases requires the correct conditional survival/hazard function given\nthe treatment and covariates. This requirement can be alleviated if the disease\nunder study has stationary incidence, the so-called stationarity assumption. We\npropose a nonparametric adjustment technique based on a weighted estimating\nequation for estimating the propensity score which does not require modeling\nthe conditional survival/hazard function when the stationarity assumption\nholds. Large sample properties of the estimator is established and its small\nsample behavior is studied via simulation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 14:09:18 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Asgharian", "Masoud", ""], ["Stephens", "David", ""]]}, {"id": "1311.1454", "submitter": "Mark Steel", "authors": "Catalina A. Vallejos and Mark F.J. Steel", "title": "On posterior propriety for the Student-$t$ linear regression model under\n  Jeffreys priors", "comments": "minor editorial changes in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with fat-tailed error terms are an increasingly popular\nchoice to obtain more robust inference to the presence of outlying\nobservations. This article focuses on Bayesian inference for the Student-$t$\nlinear regression model under objective priors that are based on the Jeffreys\nrule. Posterior propriety results presented in Fonseca et al. (2008) are\nrevisited and corrected. In particular, it is shown that the standard\nJeffreys-rule prior precludes the existence of a proper posterior distribution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 17:34:08 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 11:07:02 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Vallejos", "Catalina A.", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1311.1595", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Kyungchul Song, Yoon-Jae Whang", "title": "Testing for a General Class of Functional Inequalities", "comments": "128 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general method for testing inequality\nrestrictions on nonparametric functions. Our framework includes many\nnonparametric testing problems in a unified framework, with a number of\npossible applications in auction models, game theoretic models, wage\ninequality, and revealed preferences. Our test involves a one-sided version of\n$L_{p}$ functionals of kernel-type estimators $(1\\leq p <\\infty )$ and is easy\nto implement in general, mainly due to its recourse to the bootstrap method.\nThe bootstrap procedure is based on nonparametric bootstrap applied to\nkernel-based test statistics, with estimated \"contact sets.\" We provide\nregularity conditions under which the bootstrap test is asymptotically valid\nuniformly over a large class of distributions, including the cases that the\nlimiting distribution of the test statistic is degenerate. Our bootstrap test\nis shown to exhibit good power properties in Monte Carlo experiments, and we\nprovide a general form of the local power function. As an illustration, we\nconsider testing implications from auction theory, provide primitive conditions\nfor our test, and demonstrate its usefulness by applying our test to real data.\nWe supplement this example with the second empirical illustration in the\ncontext of wage inequality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 08:16:52 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 02:09:53 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2014 04:15:43 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 13:59:14 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Lee", "Sokbae", ""], ["Song", "Kyungchul", ""], ["Whang", "Yoon-Jae", ""]]}, {"id": "1311.1687", "submitter": "Collet", "authors": "Collet J\\'er\\^ome", "title": "Estimate dependence in medium dimensions, using ranks and sub-sampling", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that non-parametric methods suffer from the \"curse of\ndimensionality\". We propose here a new estimation method for a multivariate\ndistribution, using sub-sampling and ranks, which seems not to suffer from this\n\"curse\". We prove that in case of independence, the uncertainty of the\nestimated distribution increases almost linearly w.r.t. the dimension, for\ndimensions around 6. Otherwise, a simulation study shows that if we use this\nestimation to build an independence test, the number of observations needed to\nobtain a given power increases linearly with the dimension. Finally, we give\nexamples of a regression using this estimation: with 3000 observations, in\ndimension 5, with a markedly complicated dependence, the estimated distribution\nis graphically very similar to the real one.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 14:17:57 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["J\u00e9r\u00f4me", "Collet", ""]]}, {"id": "1311.1738", "submitter": "Mei Yin", "authors": "Mei Yin, Alessandro Rinaldo, and Sukhada Fadnavis", "title": "Asymptotic quantization of exponential random graphs", "comments": "38 pages, 7 figures", "journal-ref": "Ann. Appl. Probab. 26: 3251-3285 (2016)", "doi": "10.1214/16-AAP1175", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the asymptotic properties of the edge-triangle exponential random\ngraph model as the natural parameters diverge along straight lines. We show\nthat as we continuously vary the slopes of these lines, a typical graph drawn\nfrom this model exhibits quantized behavior, jumping from one complete\nmultipartite graph to another, and the jumps happen precisely at the normal\nlines of a polyhedral set with infinitely many facets. As a result, we provide\na complete description of all asymptotic extremal behaviors of the model.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 16:40:59 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 17:36:10 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 20:56:29 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Yin", "Mei", ""], ["Rinaldo", "Alessandro", ""], ["Fadnavis", "Sukhada", ""]]}, {"id": "1311.1797", "submitter": "Alexandre Janon", "authors": "Fabrice Gamboa (UMR CNRS 5219), Alexandre Janon (LM-Orsay, -\n  M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques),\n  Thierry Klein (IMT), Agn\\`es Lagnoux (IMT)", "title": "Sensitivity analysis for multidimensional and functional outputs", "comments": "Fixed missing references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X:=(X_1, \\ldots, X_p)$ be random objects (the inputs), defined on some\nprobability space $(\\Omega,{\\mathcal{F}}, \\mathbb P)$ and valued in some\nmeasurable space $E=E_1\\times\\ldots \\times E_p$. Further, let $Y:=Y = f(X_1,\n\\ldots, X_p)$ be the output. Here, $f$ is a measurable function from $E$ to\nsome Hilbert space $\\mathbb{H}$ ($\\mathbb{H}$ could be either of finite or\ninfinite dimension). In this work, we give a natural generalization of the\nSobol indices (that are classically defined when $Y\\in\\mathbb R$ ), when the\noutput belongs to $\\mathbb{H}$. These indices have very nice properties. First,\nthey are invariant. under isometry and scaling. Further they can be, as in\ndimension $1$, easily estimated by using the so-called Pick and Freeze method.\nWe investigate the asymptotic behaviour of such estimation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 20:09:54 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 09:42:36 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Gamboa", "Fabrice", "", "UMR CNRS 5219"], ["Janon", "Alexandre", "", "LM-Orsay, -\n  M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux", "Agn\u00e8s", "", "IMT"]]}, {"id": "1311.1835", "submitter": "Demetris Christopoulos", "authors": "Demetris T. Christopoulos", "title": "Linear Regression without computing pseudo-inverse matrix", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting a method of linear regression based on Gram-Schmidt\northogonal projection that does not compute a pseudo-inverse matrix. This is\nuseful when we want to make several regressions with random data vectors for\nsimulation purposes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 09:41:27 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Christopoulos", "Demetris T.", ""]]}, {"id": "1311.1890", "submitter": "Daniel Rudolf", "authors": "Josef Dick, Daniel Rudolf", "title": "Discrepancy estimates for variance bounding Markov chain quasi-Monte\n  Carlo", "comments": "24 pages", "journal-ref": "Electron. J. Probab. 19 (2014), no. 105, 1-24", "doi": "10.1214/EJP.v19-3132", "report-no": null, "categories": "stat.CO math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) simulations are modeled as driven by true\nrandom numbers. We consider variance bounding Markov chains driven by a\ndeterministic sequence of numbers. The star-discrepancy provides a measure of\nefficiency of such Markov chain quasi-Monte Carlo methods. We define a\npull-back discrepancy of the driver sequence and state a close relation to the\nstar-discrepancy of the Markov chain-quasi Monte Carlo samples. We prove that\nthere exists a deterministic driver sequence such that the discrepancies\ndecrease almost with the Monte Carlo rate $n^{1/2}$. As for MCMC simulations, a\nburn-in period can also be taken into account for Markov chain quasi-Monte\nCarlo to reduce the influence of the initial state. In particular, our\ndiscrepancy bound leads to an estimate of the error for the computation of\nexpectations. To illustrate our theory we provide an example for the Metropolis\nalgorithm based on a ball walk. Furthermore, under additional assumptions we\nprove the existence of a driver sequence such that the discrepancy of the\ncorresponding deterministic Markov chain sample decreases with order\n$n^{-1+\\delta}$ for every $\\delta>0$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 07:47:13 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 16:30:34 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Dick", "Josef", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1311.1893", "submitter": "Vladimir Ostrovski Dr", "authors": "Vladimir Ostrovski", "title": "Semiparametric testing of statistical functionals revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along the lines of Janssen's and Pfanzagl's work the testing theory for\nstatistical functionals is further developed for non-parametric one-sample\nproblems. Efficient tests for the one-sided and two-sided problems are derived\nfor nonparametric statistical functionals. The asymptotic power function is\ncalculated under implicit alternatives and hypotheses, which are given by the\nfunctional itself, for the one- sided and two-sided cases. Under mild\nregularity assumptions it is shown that these tests are asymptotic most\npowerful. The combination of the modern theory of Le Cam and approximation in\nlimit experiments provide a deep insight into the upper bounds for asymptotic\npower functions of tests for the one-sided and two-sided problems. As example\ntests based on the von Mises functional are treated in nonparametric context.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 07:57:32 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Ostrovski", "Vladimir", ""]]}, {"id": "1311.1894", "submitter": "Junya Honda", "authors": "Junya Honda and Akimichi Takemura", "title": "Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic bandit problems, a Bayesian policy called Thompson sampling\n(TS) has recently attracted much attention for its excellent empirical\nperformance. However, the theoretical analysis of this policy is difficult and\nits asymptotic optimality is only proved for one-parameter models. In this\npaper we discuss the optimality of TS for the model of normal distributions\nwith unknown means and variances as one of the most fundamental example of\nmultiparameter models. First we prove that the expected regret of TS with the\nuniform prior achieves the theoretical bound, which is the first result to show\nthat the asymptotic bound is achievable for the normal distribution model. Next\nwe prove that TS with Jeffreys prior and reference prior cannot achieve the\ntheoretical bound. Therefore the choice of priors is important for TS and\nnon-informative priors are sometimes risky in cases of multiparameter models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 08:04:28 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Honda", "Junya", ""], ["Takemura", "Akimichi", ""]]}, {"id": "1311.1899", "submitter": "Daniel Rudolf", "authors": "Erich Novak, Daniel Rudolf", "title": "Computation of expectations by Markov chain Monte Carlo methods", "comments": "14 pages. In: \"Extraction of quantifiable information from complex\n  systems\", S. Dahlke et al. (eds.), Springer, 2014", "journal-ref": "Extraction of Quantifiable Information from Complex Systems,\n  Lecture Notes in Computational Science and Engineering, Volume 102, 2014, pp\n  397-411", "doi": "10.1007/978-3-319-08159-5_20", "report-no": null, "categories": "math.ST math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are a very versatile and widely used\ntool to compute integrals and expectations. In this short survey we focus on\nerror bounds, rules for choosing the burn in, high dimensional problems and\ntractability versus curse of dimension.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 08:26:34 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 18:47:13 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Novak", "Erich", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1311.1959", "submitter": "Min Tsao", "authors": "Min Tsao, Fan Wu", "title": "Empirical likelihood on the full parameter space", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1143 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 2176-2196", "doi": "10.1214/13-AOS1143", "report-no": "IMS-AOS-AOS1143", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the empirical likelihood of Owen [Ann. Statist. 18 (1990) 90-120]\nby partitioning its domain into the collection of its contours and mapping the\ncontours through a continuous sequence of similarity transformations onto the\nfull parameter space. The resulting extended empirical likelihood is a natural\ngeneralization of the original empirical likelihood to the full parameter\nspace; it has the same asymptotic properties and identically shaped contours as\nthe original empirical likelihood. It can also attain the second order accuracy\nof the Bartlett corrected empirical likelihood of DiCiccio, Hall and Romano\n[Ann. Statist. 19 (1991) 1053-1061]. A simple first order extended empirical\nlikelihood is found to be substantially more accurate than the original\nempirical likelihood. It is also more accurate than available second order\nempirical likelihood methods in most small sample situations and competitive in\naccuracy in large sample situations. Importantly, in many one-dimensional\napplications this first order extended empirical likelihood is accurate for\nsample sizes as small as ten, making it a practical and reliable choice for\nsmall sample empirical likelihood inference.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 13:16:46 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Tsao", "Min", ""], ["Wu", "Fan", ""]]}, {"id": "1311.1981", "submitter": "Gyorgy Terdik DR", "authors": "T. Subba Rao and Gy. Terdik", "title": "A space-time covariance function for spatio-temporal random processes\n  and spatio-temporal prediction (kriging)", "comments": "29 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary spatio-temporal random process and assume that we\nhave a sample. By defining a sequence of discrete Fourier transforms at\ncanonical frequencies at each location, and using these complex valued random\nvarables as observed sample, we obtain expressions for the spatio-temporal\ncovariance functions and the spectral density functions of the spatio-temporal\nrandom processes. These spectra correspond to non separable class of random\nprocesses. The spatio-temporal covariance functions, obtained here are\nfunctions of the spatial distance and the temporal frequency and are similar to\nMatern class. These are in terms of modified Bessel functions of the second\nkind. and the parameters are in terms of the second order spectral density\nfunctions of the random proces and the spatial distances. We consider the\nestimation of the parameters of the covariance function and also briefly\nmention their asymptotic properties. The estimation of the entire data at a\nknown location, and also the estimation of a value given the above sample is\nalso considered. The predictors are obtained using the vectors of Discrete\nFourier Transforms. We also describe a statistical test for testing the\nindependence of the m spatial time series (testing for spatial independence)\nusing the Finite Fourier Transforms and it is based on the likelihood ratio\ntest of complex valued random variables The methods are illustrated with real\ndata.\n  Keywords: Discrete Fourier Transforms, Covariance functions, Spectral density\nfunctions, Space-Time Processses, Prediction(kriging) Laplacian operators,\nFrequency Variogram, Tests for independence, Whittle likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 14:23:44 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2015 20:20:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Rao", "T. Subba", ""], ["Terdik", "Gy.", ""]]}, {"id": "1311.2038", "submitter": "Jochen Voss", "authors": "Stuart Barber, Jochen Voss and Mark Webster", "title": "The Rate of Convergence for Approximate Bayesian Computation", "comments": "25 pages, 3 figures; address the distinction between fixed number of\n  proposals and fixed number of accepted samples more explicitly", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a popular computational method for\nlikelihood-free Bayesian inference. The term \"likelihood-free\" refers to\nproblems where the likelihood is intractable to compute or estimate directly,\nbut where it is possible to generate simulated data $X$ relatively easily given\na candidate set of parameters $\\theta$ simulated from a prior distribution.\nParameters which generate simulated data within some tolerance $\\delta$ of the\nobserved data $x^*$ are regarded as plausible, and a collection of such\n$\\theta$ is used to estimate the posterior distribution $\\theta\\,|\\,X\\!=\\!x^*$.\nSuitable choice of $\\delta$ is vital for ABC methods to return good\napproximations to $\\theta$ in reasonable computational time.\n  While ABC methods are widely used in practice, particularly in population\ngenetics, study of the mathematical properties of ABC estimators is still in\nits infancy. We prove that ABC estimates converge to the exact solution under\nvery weak assumptions and, under slightly stronger assumptions, quantify the\nrate of this convergence. Our results can be used to guide the choice of the\ntolerance parameter $\\delta$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 19:09:00 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 17:00:38 GMT"}, {"version": "v3", "created": "Fri, 18 Jul 2014 12:18:37 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Barber", "Stuart", ""], ["Voss", "Jochen", ""], ["Webster", "Mark", ""]]}, {"id": "1311.2234", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff\n  Schneider, Fang-Cheng Yeh, Wen-Yih Tseng", "title": "FuSSO: Functional Shrinkage and Selection Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the FuSSO, a functional analogue to the LASSO, that efficiently\nfinds a sparse set of functional input covariates to regress a real-valued\nresponse against. The FuSSO does so in a semi-parametric fashion, making no\nparametric assumptions about the nature of input functional covariates and\nassuming a linear form to the mapping of functional covariates to the response.\nWe provide a statistical backing for use of the FuSSO via proof of asymptotic\nsparsistency under various conditions. Furthermore, we observe good results on\nboth synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 00:44:01 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 02:30:26 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Oliva", "Junier B.", ""], ["Poczos", "Barnabas", ""], ["Verstynen", "Timothy", ""], ["Singh", "Aarti", ""], ["Schneider", "Jeff", ""], ["Yeh", "Fang-Cheng", ""], ["Tseng", "Wen-Yih", ""]]}, {"id": "1311.2236", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider,\n  Eric Xing", "title": "Fast Distribution To Real Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distribution to real-value regression, where one aims\nto regress a mapping $f$ that takes in a distribution input covariate $P\\in\n\\mathcal{I}$ (for a non-parametric family of distributions $\\mathcal{I}$) and\noutputs a real-valued response $Y=f(P) + \\epsilon$. This setting was recently\nstudied, and a \"Kernel-Kernel\" estimator was introduced and shown to have a\npolynomial rate of convergence. However, evaluating a new prediction with the\nKernel-Kernel estimator scales as $\\Omega(N)$. This causes the difficult\nsituation where a large amount of data may be necessary for a low estimation\nrisk, but the computation cost of estimation becomes infeasible when the\ndata-set is too large. To this end, we propose the Double-Basis estimator,\nwhich looks to alleviate this big data problem in two ways: first, the\nDouble-Basis estimator is shown to have a computation complexity that is\nindependent of the number of of instances $N$ when evaluating new predictions\nafter training; secondly, the Double-Basis estimator is shown to have a fast\nrate of convergence for a general class of mappings $f\\in\\mathcal{F}$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 01:17:19 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 03:41:35 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Oliva", "Junier B.", ""], ["Neiswanger", "Willie", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Xing", "Eric", ""]]}, {"id": "1311.2336", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris and Alexander Tartakovsky", "title": "Unstructured sequential testing in sensor networks", "comments": "6 two-column pages, To appear in the Proceedings 2013 IEEE Conference\n  on Decision and Control, Firenze, Italy, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickly detecting a signal in a sensor network\nwhen the subset of sensors in which signal may be present is completely\nunknown. We formulate this problem as a sequential hypothesis testing problem\nwith a simple null (signal is absent everywhere) and a composite alternative\n(signal is present somewhere). We introduce a novel class of scalable\nsequential tests which, for any subset of affected sensors, minimize the\nexpected sample size for a decision asymptotically, that is as the error\nprobabilities go to 0. Moreover, we propose sequential tests that require\nminimal transmission activity from the sensors to the fusion center, while\npreserving this asymptotic optimality property.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 02:38:58 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Fellouris", "Georgios", ""], ["Tartakovsky", "Alexander", ""]]}, {"id": "1311.2341", "submitter": "Leonid Sirota", "authors": "E.Ostrovsky, L.Sirota, A.Zeldin", "title": "Characterization of quasy-Gaussian distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new characterization of the multivariate so-called \"quasi-Gaussian\ndistribution\" (the authors dared to coin a new term) by means of independence\ntheir Cartesian and polar coordinates proposed.\n  The authors try to show that these distributions may essentially differ from\nthe classical Gaussian distribution. Some properties of these distributions are\nstudied: calculating of moments and of bilateral tail behavior. Some potential\napplications of these distributions and mixtures of the distributions in\ndemography, philology are discussed in the final section.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 03:04:08 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Ostrovsky", "E.", ""], ["Sirota", "L.", ""], ["Zeldin", "A.", ""]]}, {"id": "1311.2445", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui", "title": "Asymptotic behavior of unregularized and ridge-regularized\n  high-dimensional robust regression estimators : rigorous results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of high-dimensional robust regression estimators in the\nasymptotic regime where $p/n$ tends to a finite non-zero limit. More\nspecifically, we study ridge-regularized estimators, i.e\n$\\widehat{\\beta}=\\text{argmin}_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n}\\sum_{i=1}^n\n\\rho(\\varepsilon_i-X_i' \\beta)+\\frac{\\tau}{2}\\lVert\\beta\\rVert^2$. In a\nrecently published paper, we had developed with collaborators probabilistic\nheuristics to understand the asymptotic behavior of $\\widehat{\\beta}$. We give\nhere a rigorous proof, properly justifying all the arguments we had given in\nthat paper. Our proof is based on the probabilistic heuristics we had\ndeveloped, and hence ideas from random matrix theory, measure concentration and\nconvex analysis. While most the work is done for $\\tau>0$, we show that under\nsome extra assumptions on $\\rho$, it is possible to recover the case $\\tau=0$\nas a limiting case. We require that the $X_i$'s be i.i.d with independent\nentries, but our proof handles the case where these entries are not Gaussian. A\n2-week old paper of Donoho and Montanari [arXiv:1310.7320] studied a similar\nproblem by a different method and with a different point of view. At this\npoint, their interesting approach requires Gaussianity of the design matrix.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 14:04:06 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Karoui", "Noureddine El", ""]]}, {"id": "1311.2483", "submitter": "Sebastien Da Veiga", "authors": "S\\'ebastien Da Veiga (IFPEN, - M\\'ethodes d'Analyse Stochastique des\n  Codes et Traitements Num\\'eriques)", "title": "Global Sensitivity Analysis with Dependence Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis with variance-based measures suffers from several\ntheoretical and practical limitations, since they focus only on the variance of\nthe output and handle multivariate variables in a limited way. In this paper,\nwe introduce a new class of sensitivity indices based on dependence measures\nwhich overcomes these insufficiencies. Our approach originates from the idea to\ncompare the output distribution with its conditional counterpart when one of\nthe input variables is fixed. We establish that this comparison yields\npreviously proposed indices when it is performed with Csiszar f-divergences, as\nwell as sensitivity indices which are well-known dependence measures between\nrandom variables. This leads us to investigate completely new sensitivity\nindices based on recent state-of-the-art dependence measures, such as distance\ncorrelation and the Hilbert-Schmidt independence criterion. We also emphasize\nthe potential of feature selection techniques relying on such dependence\nmeasures as alternatives to screening in high dimension.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 16:30:06 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Da Veiga", "S\u00e9bastien", "", "IFPEN, - M\u00e9thodes d'Analyse Stochastique des\n  Codes et Traitements Num\u00e9riques"]]}, {"id": "1311.2628", "submitter": "Guang Cheng", "authors": "Guang Cheng, Zuofeng Shang", "title": "Joint asymptotics for semi-nonparametric regression models with\n  partially linear structure", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1313 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1351-1390", "doi": "10.1214/15-AOS1313", "report-no": "IMS-AOS-AOS1313", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a joint asymptotic framework for studying semi-nonparametric\nregression models where (finite-dimensional) Euclidean parameters and\n(infinite-dimensional) functional parameters are both of interest. The class of\nmodels in consideration share a partially linear structure and are estimated in\ntwo general contexts: (i) quasi-likelihood and (ii) true likelihood. We first\nshow that the Euclidean estimator and (pointwise) functional estimator, which\nare re-scaled at different rates, jointly converge to a zero-mean Gaussian\nvector. This weak convergence result reveals a surprising joint asymptotics\nphenomenon: these two estimators are asymptotically independent. A major goal\nof this paper is to gain first-hand insights into the above phenomenon.\nMoreover, a likelihood ratio testing is proposed for a set of joint local\nhypotheses, where a new version of the Wilks phenomenon [Ann. Math. Stat. 9\n(1938) 60-62; Ann. Statist. 1 (2001) 153-193] is unveiled. A novel technical\ntool, called a joint Bahadur representation, is developed for studying these\njoint asymptotics results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 22:05:36 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 17:20:25 GMT"}, {"version": "v3", "created": "Mon, 19 Jan 2015 22:55:02 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 09:03:13 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Cheng", "Guang", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1311.2645", "submitter": "Ivan Fernandez-Val", "authors": "Alexandre Belloni and Victor Chernozhukov and Ivan Fern\\'andez-Val and\n  Christian Hansen", "title": "Program Evaluation and Causal Inference with High-Dimensional Data", "comments": "118 pages, 3 tables, 11 figures, includes supplementary appendix.\n  This version corrects some typos in Example 2 of the published version", "journal-ref": "Econometrica, Vol. 85, No. 1 (January, 2017), 233-298", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide efficient estimators and honest confidence bands\nfor a variety of treatment effects including local average (LATE) and local\nquantile treatment effects (LQTE) in data-rich environments. We can handle very\nmany control variables, endogenous receipt of treatment, heterogeneous\ntreatment effects, and function-valued outcomes. Our framework covers the\nspecial case of exogenous receipt of treatment, either conditional on controls\nor unconditionally as in randomized control trials. In the latter case, our\napproach produces efficient estimators and honest bands for (functional)\naverage treatment effects (ATE) and quantile treatment effects (QTE). To make\ninformative inference possible, we assume that key reduced form predictive\nrelationships are approximately sparse. This assumption allows the use of\nregularization and selection methods to estimate those relations, and we\nprovide methods for post-regularization and post-selection inference that are\nuniformly valid (honest) across a wide-range of models. We show that a key\ningredient enabling honest inference is the use of orthogonal or doubly robust\nmoment conditions in estimating certain reduced form functional parameters. We\nillustrate the use of the proposed methods with an application to estimating\nthe effect of 401(k) eligibility and participation on accumulated assets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 23:36:44 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 19:24:31 GMT"}, {"version": "v3", "created": "Thu, 5 Jun 2014 20:21:24 GMT"}, {"version": "v4", "created": "Sat, 9 Aug 2014 17:53:36 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2015 02:26:55 GMT"}, {"version": "v6", "created": "Fri, 18 Mar 2016 02:33:33 GMT"}, {"version": "v7", "created": "Wed, 21 Dec 2016 18:57:37 GMT"}, {"version": "v8", "created": "Fri, 5 Jan 2018 07:13:54 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Ivan", ""], ["Hansen", "Christian", ""]]}, {"id": "1311.2657", "submitter": "Sean O'Rourke", "authors": "Sean O'Rourke, Van Vu, Ke Wang", "title": "Random perturbation of low rank matrices: Improving classical bounds", "comments": "28 pages, 1 figure. Updated introduction and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix perturbation inequalities, such as Weyl's theorem (concerning the\nsingular values) and the Davis-Kahan theorem (concerning the singular vectors),\nplay essential roles in quantitative science; in particular, these bounds have\nfound application in data analysis as well as related areas of engineering and\ncomputer science.\n  In many situations, the perturbation is assumed to be random, and the\noriginal matrix has certain structural properties (such as having low rank). We\nshow that, in this scenario, classical perturbation results, such as Weyl and\nDavis-Kahan, can be improved significantly. We believe many of our new bounds\nare close to optimal and also discuss some applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 02:14:46 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 15:25:27 GMT"}, {"version": "v3", "created": "Fri, 5 Sep 2014 14:35:42 GMT"}, {"version": "v4", "created": "Fri, 13 May 2016 20:53:13 GMT"}, {"version": "v5", "created": "Thu, 16 Nov 2017 20:27:48 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["O'Rourke", "Sean", ""], ["Vu", "Van", ""], ["Wang", "Ke", ""]]}, {"id": "1311.2669", "submitter": "John Duchi", "authors": "John C. Duchi and Martin J. Wainwright", "title": "Distance-based and continuum Fano inequalities with applications to\n  statistical estimation", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical note, we give two extensions of the classical Fano\ninequality in information theory. The first extends Fano's inequality to the\nsetting of estimation, providing lower bounds on the probability that an\nestimator of a discrete quantity is within some distance $t$ of the quantity.\nThe second inequality extends our bound to a continuum setting and provides a\nvolume-based bound. We illustrate how these inequalities lead to direct and\nsimple proofs of several statistical minimax lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 04:38:18 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 02:48:25 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Duchi", "John C.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1311.2694", "submitter": "Purnamrita Sarkar", "authors": "Peter J. Bickel, Purnamrita Sarkar", "title": "Hypothesis Testing for Automated Community Detection in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in networks is a key exploratory tool with applications\nin a diverse set of areas, ranging from finding communities in social and\nbiological networks to identifying link farms in the World Wide Web. The\nproblem of finding communities or clusters in a network has received much\nattention from statistics, physics and computer science. However, most\nclustering algorithms assume knowledge of the number of clusters k. In this\npaper we propose to automatically determine k in a graph generated from a\nStochastic Blockmodel. Our main contribution is twofold; first, we\ntheoretically establish the limiting distribution of the principal eigenvalue\nof the suitably centered and scaled adjacency matrix, and use that distribution\nfor our hypothesis test. Secondly, we use this test to design a recursive\nbipartitioning algorithm. Using quantifiable classification tasks on real world\nnetworks with ground truth, we show that our algorithm outperforms existing\nprobabilistic models for learning overlapping clusters, and on unlabeled\nnetworks, we show that we uncover nested community structure.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 07:00:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 05:40:00 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Bickel", "Peter J.", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1311.2725", "submitter": "Dai Taguchi", "authors": "Hoang-Long Ngo and Dai Taguchi", "title": "Strong Rate of Convergence for the Euler-Maruyama Approximation of\n  Stochastic Differential Equations with Irregular Coefficients", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Euler-Maruyama approximation for multi-dimensional stochastic\ndifferential equations with irregular coefficients. We provide the rate of\nstrong convergence where the possibly discontinuous drift coefficient satisfies\na one-sided Lipschitz condition and the diffusion coefficient is H\\\"older\ncontinuous.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 09:50:49 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 10:12:58 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Ngo", "Hoang-Long", ""], ["Taguchi", "Dai", ""]]}, {"id": "1311.2740", "submitter": "Wei Zheng", "authors": "Wei Zheng", "title": "Optimal crossover designs for the proportional model", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1148 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 2218-2235", "doi": "10.1214/13-AOS1148", "report-no": "IMS-AOS-AOS1148", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crossover design experiments, the proportional model, where the carryover\neffects are proportional to their direct treatment effects, has draw attentions\nin recent years. We discover that the universally optimal design under the\ntraditional model is E-optimal design under the proportional model. Moreover,\nwe establish equivalence theorems of Kiefer-Wolfowitz's type for four popular\noptimality criteria, namely A, D, E and T (trace).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 11:26:01 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Zheng", "Wei", ""]]}, {"id": "1311.2742", "submitter": "Jinchi Lv", "authors": "Jinchi Lv", "title": "Impacts of high dimensionality in finite samples", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1149 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 2236-2262", "doi": "10.1214/13-AOS1149", "report-no": "IMS-AOS-AOS1149", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data sets are commonly collected in many contemporary\napplications arising in various fields of scientific research. We present two\nviews of finite samples in high dimensions: a probabilistic one and a\nnonprobabilistic one. With the probabilistic view, we establish the\nconcentration property and robust spark bound for large random design matrix\ngenerated from elliptical distributions, with the former related to the sure\nscreening property and the latter related to sparse model identifiability. An\ninteresting concentration phenomenon in high dimensions is revealed. With the\nnonprobabilistic view, we derive general bounds on dimensionality with some\ndistance constraint on sparse models. These results provide new insights into\nthe impacts of high dimensionality in finite samples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 11:47:48 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Lv", "Jinchi", ""]]}, {"id": "1311.2791", "submitter": "Shachar Kaufman", "authors": "Shachar Kaufman and Saharon Rosset", "title": "When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient\n  Conditions and Counter Examples from Lasso and Ridge Regression", "comments": "Main text: 15 pages, 2 figures; Supplementary material is included at\n  the end of the main text: 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization aims to improve prediction performance of a given statistical\nmodeling approach by moving to a second approach which achieves worse training\nerror but is expected to have fewer degrees of freedom, i.e., better agreement\nbetween training and prediction error. We show here, however, that this\nexpected behavior does not hold in general. In fact, counter examples are given\nthat show regularization can increase the degrees of freedom in simple\nsituations, including lasso and ridge regression, which are the most common\nregularization approaches in use. In such situations, the regularization\nincreases both training error and degrees of freedom, and is thus inherently\nwithout merit. On the other hand, two important regularization scenarios are\ndescribed where the expected reduction in degrees of freedom is indeed\nguaranteed: (a) all symmetric linear smoothers, and (b) linear regression\nversus convex constrained linear regression (as in the constrained variant of\nridge regression and lasso).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 14:29:32 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Kaufman", "Shachar", ""], ["Rosset", "Saharon", ""]]}, {"id": "1311.2799", "submitter": "Philippe Rigollet", "authors": "Dong Dai, Philippe Rigollet, Lucy Xia and Tong Zhang", "title": "Aggregation of Affine Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of aggregating a general collection of affine\nestimators for fixed design regression. Relevant examples include some commonly\nused statistical estimators such as least squares, ridge and robust least\nsquares estimators. Dalalyan and Salmon (2012) have established that, for this\nproblem, exponentially weighted (EW) model selection aggregation leads to sharp\noracle inequalities in expectation, but similar bounds in deviation were not\npreviously known. While results indicate that the same aggregation scheme may\nnot satisfy sharp oracle inequalities with high probability, we prove that a\nweaker notion of oracle inequality for EW that holds with high probability.\nMoreover, using a generalization of the newly introduced $Q$-aggregation scheme\nwe also prove sharp oracle inequalities that hold with high probability.\nFinally, we apply our results to universal aggregation and show that our\nproposed estimator leads simultaneously to all the best known bounds for\naggregation, including $\\ell_q$-aggregation, $q \\in (0,1)$, with high\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 14:53:51 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Dai", "Dong", ""], ["Rigollet", "Philippe", ""], ["Xia", "Lucy", ""], ["Zhang", "Tong", ""]]}, {"id": "1311.2902", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel (CREST)", "title": "A universal deviation inequality for random polytopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the convex hull of a finite sample of i.i.d. points uniformly\ndistributed in a convex body in $\\R^d$, $d\\geq 2$. We prove an exponential\ndeviation inequality, which leads to rate optimal upper bounds on all the\nmoments of the missing volume of the convex hull, uniformly over all convex\nbodies of $\\R^d$, with no restriction on their volume, location in the space\nand smoothness of the boundary.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 20:02:37 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Brunel", "Victor-Emmanuel", "", "CREST"]]}, {"id": "1311.3060", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher and Johan Segers", "title": "Extreme value copula estimation based on block maxima of a multivariate\n  stationary time series", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core of the classical block maxima method consists of fitting an extreme\nvalue distribution to a sample of maxima over blocks extracted from an\nunderlying series. In asymptotic theory, it is usually postulated that the\nblock maxima are an independent random sample of an extreme value distribution.\nIn practice however, block sizes are finite, so that the extreme value\npostulate will only hold approximately. A more accurate asymptotic framework is\nthat of a triangular array of block maxima, the block size depending on the\nsize of the underlying sample in such a way that both the block size and the\nnumber of blocks within that sample tend to infinity. The copula of the vector\nof componentwise maxima in a block is assumed to converge to a limit, which,\nunder mild conditions, is then necessarily an extreme value copula. Under this\nsetting and for absolutely regular stationary sequences, the empirical copula\nof the sample of vectors of block maxima is shown to be a consistent and\nasymptotically normal estimator for the limiting extreme value copula.\nMoreover, the empirical copula serves as a basis for rank-based, nonparametric\nestimation of the Pickands dependence function of the extreme value copula. The\nresults are illustrated by theoretical examples and a Monte Carlo simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 09:39:26 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 06:28:23 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Segers", "Johan", ""]]}, {"id": "1311.3092", "submitter": "Elodie Vernet", "authors": "Elodie Vernet", "title": "Posterior consistency for nonparametric hidden Markov models with finite\n  state space", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study posterior consistency for different topologies on the\nparameters for hidden Markov models with finite state space. We first obtain\nweak and strong posterior consistency for the marginal density function of\nfinitely many consecutive observations. We deduce posterior consistency for the\ndifferent components of the parameter. We also obtain posterior consistency for\nmarginal smoothing distributions in the discrete case. We finally apply our\nresults to independent emission probabilities, translated emission\nprobabilities and discrete HMMs, under various types of priors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 11:41:27 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 11:20:38 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Vernet", "Elodie", ""]]}, {"id": "1311.3118", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine, Thomas Verdebout", "title": "On high-dimensional sign tests", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ710 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 3, 1745-1769", "doi": "10.3150/15-BEJ710", "report-no": "IMS-BEJ-BEJ710", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign tests are among the most successful procedures in multivariate\nnonparametric statistics. In this paper, we consider several testing problems\nin multivariate analysis, directional statistics and multivariate time series\nanalysis, and we show that, under appropriate symmetry assumptions, the\nfixed-$p$ multivariate sign tests remain valid in the high-dimensional case.\nRemarkably, our asymptotic results are universal, in the sense that, unlike in\nmost previous works in high-dimensional statistics, $p$ may go to infinity in\nan arbitrary way as $n$ does. We conduct simulations that (i) confirm our\nasymptotic results, (ii) reveal that, even for relatively large $p$, chi-square\ncritical values are to be favoured over the (asymptotically equivalent)\nGaussian ones and (iii) show that, for testing i.i.d.-ness against serial\ndependence in the high-dimensional case, Portmanteau sign tests outperform\ntheir competitors in terms of validity-robustness.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 13:15:25 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 11:19:47 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Paindaveine", "Davy", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1311.3190", "submitter": "Amit Moscovich-Eiger", "authors": "Amit Moscovich, Boaz Nadler, Clifford Spiegelman", "title": "On the exact Berk-Jones statistics and their p-value calculation", "comments": "29 pages, 3 figures, pdflatex; Minor revision", "journal-ref": "Electronic Journal of Statistics 10 (2016) 2329-2354", "doi": "10.1214/16-EJS1172", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous goodness-of-fit testing is a classical problem in statistics.\nDespite having low power for detecting deviations at the tail of a\ndistribution, the most popular test is based on the Kolmogorov-Smirnov\nstatistic. While similar variance-weighted statistics, such as Anderson-Darling\nand the Higher Criticism statistic give more weight to tail deviations, as\nshown in various works, they still mishandle the extreme tails.\n  As a viable alternative, in this paper we study some of the statistical\nproperties of the exact $M_n$ statistics of Berk and Jones. We derive the\nasymptotic null distributions of $M_n, M_n^+, M_n^-$, and further prove their\nconsistency as well as asymptotic optimality for a wide range of rare-weak\nmixture models. Additionally, we present a new computationally efficient method\nto calculate $p$-values for any supremum-based one-sided statistic, including\nthe one-sided $M_n^+,M_n^-$ and $R_n^+,R_n^-$ statistics of Berk and Jones and\nthe Higher Criticism statistic. We illustrate our theoretical analysis with\nseveral finite-sample simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:19:58 GMT"}, {"version": "v2", "created": "Sun, 18 May 2014 17:34:05 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 16:49:58 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2015 11:26:37 GMT"}, {"version": "v5", "created": "Thu, 24 Mar 2016 15:20:16 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Moscovich", "Amit", ""], ["Nadler", "Boaz", ""], ["Spiegelman", "Clifford", ""]]}, {"id": "1311.3350", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Jinlin Song", "title": "Sequential Tests of Multiple Hypotheses Controlling False Discovery and\n  Nondiscovery Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general and flexible procedure for testing multiple hypotheses\nabout sequential (or streaming) data that simultaneously controls both the\nfalse discovery rate (FDR) and false nondiscovery rate (FNR) under minimal\nassumptions about the data streams which may differ in distribution, dimension,\nand be dependent. All that is needed is a test statistic for each data stream\nthat controls the conventional type I and II error probabilities, and no\ninformation or assumptions are required about the joint distribution of the\nstatistics or data streams. The procedure can be used with sequential, group\nsequential, truncated, or other sampling schemes. The procedure is a natural\nextension of Benjamini and Hochberg's (1995) widely-used fixed sample size\nprocedure to the domain of sequential data, with the added benefit of\nsimultaneous FDR and FNR control that sequential sampling affords. We prove the\nprocedure's error control and give some tips for implementation in commonly\nencountered testing situations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 00:22:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 17:33:03 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Bartroff", "Jay", ""], ["Song", "Jinlin", ""]]}, {"id": "1311.3492", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh and Peter B\\\"uhlmann", "title": "High-dimensional learning of linear causal networks via inverse\n  covariance estimation", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a new framework for statistical estimation of directed acyclic\ngraphs (DAGs) when data are generated from a linear, possibly non-Gaussian\nstructural equation model. Our framework consists of two parts: (1) inferring\nthe moralized graph from the support of the inverse covariance matrix; and (2)\nselecting the best-scoring graph amongst DAGs that are consistent with the\nmoralized graph. We show that when the error variances are known or estimated\nto close enough precision, the true DAG is the unique minimizer of the score\ncomputed using the reweighted squared l_2-loss. Our population-level results\nhave implications for the identifiability of linear SEMs when the error\ncovariances are specified up to a constant multiple. On the statistical side,\nwe establish rigorous conditions for high-dimensional consistency of our\ntwo-part algorithm, defined in terms of a \"gap\" between the true DAG and the\nnext best candidate. Finally, we demonstrate that dynamic programming may be\nused to select the optimal DAG in linear time when the treewidth of the\nmoralized graph is bounded.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 13:08:51 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Loh", "Po-Ling", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1311.3753", "submitter": "Yoshio Komori", "authors": "Yoshio Komori and Hideo Hirose", "title": "Easy estimation by a new parameterization for the three-parameter\n  lognormal distribution", "comments": "13 pages, 3 figures", "journal-ref": "J. Stat. Comput. Simul. 74 (2004), 63-74", "doi": "10.1080/0094965031000104341", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new parameterization and algorithm are proposed for seeking the primary\nrelative maximum of the likelihood function in the three-parameter lognormal\ndistribution. The parameterization yields the dimension reduction of the\nthree-parameter estimation problem to a two-parameter estimation problem on the\nbasis of an extended lognormal distribution. The algorithm provides the way of\nseeking the profile of an object function in the two-parameter estimation\nproblem. It is simple and numerically stable because it is constructed on the\nbasis of the bisection method. The profile clearly and easily shows whether a\nprimary relative maximum exists or not, and also gives a primary relative\nmaximum certainly if it exists.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 07:41:10 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Komori", "Yoshio", ""], ["Hirose", "Hideo", ""]]}, {"id": "1311.3755", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "Deterministic Bayesian Information Fusion and the Analysis of its\n  Performance", "comments": null, "journal-ref": "Information and Inference 3(4):345-366 (2014)", "doi": "10.1093/imaiai/iau009", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a mathematical and computational framework for analyzing\nthe expected performance of Bayesian data fusion, or joint statistical\ninference, within a sensor network. We use variational techniques to obtain the\nposterior expectation as the optimal fusion rule under a deterministic\nconstraint and a quadratic cost, and study the smoothness and other properties\nof its classification performance. For a certain class of fusion problems, we\nprove that this fusion rule is also optimal in a much wider sense and satisfies\nstrong asymptotic convergence results. We show how these results apply to a\nvariety of examples with Gaussian, exponential and other statistics, and\ndiscuss computational methods for determining the fusion system's performance\nin more general, large-scale problems. These results are motivated by studying\nthe performance of fusing multi-modal radar and acoustic sensors for detecting\nexplosive substances, but have broad applicability to other Bayesian decision\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 07:46:54 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 01:26:16 GMT"}, {"version": "v3", "created": "Sat, 4 Oct 2014 17:30:11 GMT"}, {"version": "v4", "created": "Sun, 2 Nov 2014 13:51:59 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1311.3765", "submitter": "Sabyasachi Chatterjee", "authors": "Sabyasachi Chatterjee, Adityanand Guntuboyina, Bodhisattva Sen", "title": "On risk bounds in isotonic and other shape restricted regression\n  problems", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1324 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1774-1800", "doi": "10.1214/15-AOS1324", "report-no": "IMS-AOS-AOS1324", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an unknown $\\theta\\in {\\mathbb{R}}^n$\nfrom noisy observations under the constraint that $\\theta$ belongs to certain\nconvex polyhedral cones in ${\\mathbb{R}}^n$. Under this setting, we prove\nbounds for the risk of the least squares estimator (LSE). The obtained risk\nbound behaves differently depending on the true sequence $\\theta$ which\nhighlights the adaptive behavior of $\\theta$. As special cases of our general\nresult, we derive risk bounds for the LSE in univariate isotonic and convex\nregression. We study the risk bound in isotonic regression in greater detail:\nwe show that the isotonic LSE converges at a whole range of rates from $\\log\nn/n$ (when $\\theta$ is constant) to $n^{-2/3}$ (when $\\theta$ is uniformly\nincreasing in a certain sense). We argue that the bound presents a benchmark\nfor the risk of any estimator in isotonic regression by proving nonasymptotic\nlocal minimax lower bounds. We prove an analogue of our bound for model\nmisspecification where the true $\\theta$ is not necessarily nondecreasing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 08:26:58 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 04:03:50 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 13:17:20 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Chatterjee", "Sabyasachi", ""], ["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1311.3768", "submitter": "Jacques Froment", "authors": "Simon Postec (LMBA), Jacques Froment (LMBA), B\\'eatrice Vedel (LMBA)", "title": "Non-Local means est un algorithme de d\\'ebruitage local (Non-Local means\n  is a local image denoising algorithm)", "comments": "XXIVe Colloque GRETSI - Traitement du Signal et des Images, Brest :\n  France (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Non-Local Means (NLM) image denoising algorithm pushed the limits of\ndenoising. But it introduced a new paradigm, according to which one could\ncapture the similarity of images with the NLM weights. We show that, contrary\nto the prevailing opinion, the NLM weights do not allow to get a reliable\nmeasure of the similarity in a noisy image, unless one add a locality\nconstraint. As an image denoising method, the Non-Local Means prove to be\nlocal. Some works had already pointed out that to get the best denoising\nperformances with the NLM algorithm, one should run it locally. But no general\nconclusion has been yet proposed and the only explanation that was proposed to\njustify the experimental results is not sufficient. Our study based on\nexperimental evidence proves that, on average on natural images, the bias of\nthe NLM estimator is an increasing function of the radius of the similarity\nsearching zone. The reason for this phenomenon is that noise disrupts the order\nof similarity between patches. Hence the mean squared error between the\noriginal image and the NLM estimation, which is the sum of the bias, the\nvariance and the covariance of the estimator, has an absolute minimum for a\ndisk of radius 3 to 4 pixels.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 08:36:54 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Postec", "Simon", "", "LMBA"], ["Froment", "Jacques", "", "LMBA"], ["Vedel", "B\u00e9atrice", "", "LMBA"]]}, {"id": "1311.3969", "submitter": "Andrew  Rukhin", "authors": "Andrew L. Rukhin", "title": "Restricted likelihood representation and decision-theoretic aspects of\n  meta-analysis", "comments": "Published in at http://dx.doi.org/10.3150/13-BEJ547 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2014, Vol. 20, No. 4, 1979-1998", "doi": "10.3150/13-BEJ547", "report-no": "IMS-BEJ-BEJ547", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the random-effects model of meta-analysis a canonical representation of\nthe restricted likelihood function is obtained. This representation relates the\nmean effect and the heterogeneity variance estimation problems. An explicit\nform of the variance of weighted means statistics determined by means of a\nquadratic form is found. The behavior of the mean squared error for large\nheterogeneity variance is elucidated. It is noted that the sample mean is not\nadmissible nor minimax under a natural risk function for the number of studies\nexceeding three.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 20:37:34 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 13:28:56 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Rukhin", "Andrew L.", ""]]}, {"id": "1311.3998", "submitter": "Karthik Bharath", "authors": "Karthik Bharath", "title": "On a Clustering Criterion for Dependent Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A univariate clustering criterion for stationary processes satisfying a\n$\\beta$-mixing condition is proposed extending the work of \\cite{KB2} to the\ndependent setup. The approach is characterized by an alternative sample\ncriterion function based on truncated partial sums which renders the framework\namenable to various interesting extensions for which limit results for partial\nsums are available. Techniques from empirical process theory for mixing\nsequences play a vital role in the arguments employed in the proofs of the\nlimit theorems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 23:12:55 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Bharath", "Karthik", ""]]}, {"id": "1311.4030", "submitter": "Etienne Roquain", "authors": "Sylvain Delattre (LPMA), Etienne Roquain (LPMA)", "title": "New procedures controlling the false discovery proportion via\n  Romano-Wolf's heuristic", "comments": null, "journal-ref": "Annals of Statistics, Institute of Mathematical Statistics (IMS),\n  2015, 43 (3), pp.1141-1177", "doi": "10.1214/14-AOS1302", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The false discovery proportion (FDP) is a convenient way to account for false\npositives when a large number $m$ of tests are performed simultaneously. Romano\nand Wolf [Ann. Statist. 35 (2007) 1378-1408] have proposed a general principle\nthat builds FDP controlling procedures from $k$-family-wise error rate\ncontrolling procedures while incorporating dependencies in an appropriate\nmanner; see Korn et al. [J. Statist. Plann. Inference 124 (2004) 379-398];\nRomano and Wolf (2007). However, the theoretical validity of the latter is\nstill largely unknown. This paper provides a careful study of this heuristic:\nfirst, we extend this approach by using a notion of \"bounding device\" that\nallows us to cover a wide range of critical values, including those that adapt\nto $m\\_0$, the number of true null hypotheses. Second, the theoretical validity\nof the latter is investigated both nonasymptotically and asymptotically. Third,\nwe introduce suitable modifications of this heuristic that provide new methods,\novercoming the existing procedures with a proven FDP control.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 07:35:31 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 14:15:26 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 09:24:57 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Delattre", "Sylvain", "", "LPMA"], ["Roquain", "Etienne", "", "LPMA"]]}, {"id": "1311.4043", "submitter": "arXiv Admin", "authors": "Gong Zi Jiang Nan", "title": "High Dimensional Tests Based on U-Statistics for Generalized Linear\n  Models", "comments": "This submission has been withdrawn by arXiv administrators due to\n  disputed authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose two U-statistics to test coefficients in generalized linear models.\nOne of them is used to deal with global hypothesis and the other one to test\nwith the nuisance parameter. Both the statistics proposed are within\nhigh-dimensional setting which means the number of coefficients is much larger\nthan the sample size. The statistics are based on quasi-likelihood function so\nthat they have wilder applications. I theoretically analyze the asymptotic\ndistribution of the statistics under the null hypothesis and the power\nfunctions under the local and fixed alternatives. To serve as a comparison, the\npower functions of the test proposed by Goeman et al. (2011) are also derived.\nSome simulation studies are carried out and I apply my methods to an empirical\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 10:07:59 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 21:15:23 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Nan", "Gong Zi Jiang", ""]]}, {"id": "1311.4163", "submitter": "Earnest Akofor", "authors": "Earnest Akofor, Biao Chen", "title": "Interactive Distributed Detection: Architecture and Performance Analysis", "comments": "32 pages, 7 figures, Final version, Published in IEEE Transactions on\n  Information Theory", "journal-ref": "Interactive Distributed Detection: Architecture and Performance\n  Analysis, IEEE Trans. Info. Theory, Vol. 60, Issue 10, pgs. 6456 - 6473, 2014", "doi": "10.1109/TIT.2014.2346497", "report-no": null, "categories": "cs.IT math.IT math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the impact of interactive fusion on detection performance\nin tandem fusion networks with conditionally independent observations. Within\nthe Neyman-Pearson framework, two distinct regimes are considered: the fixed\nsample size test and the large sample test. For the former, it is established\nthat interactive distributed detection may strictly outperform the one-way\ntandem fusion structure. However, for the large sample regime, it is shown that\ninteractive fusion has no improvement on the asymptotic performance\ncharacterized by the Kullback-Leibler (KL) distance compared with the simple\none-way tandem fusion. The results are then extended to interactive fusion\nsystems where the fusion center and the sensor may undergo multiple steps of\nmemoryless interactions or that involve multiple peripheral sensors, as well as\nto interactive fusion with soft sensor outputs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 14:27:15 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2013 01:24:53 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 02:45:12 GMT"}, {"version": "v4", "created": "Mon, 10 Mar 2014 19:35:54 GMT"}, {"version": "v5", "created": "Mon, 28 Jul 2014 17:07:47 GMT"}, {"version": "v6", "created": "Thu, 18 Sep 2014 01:31:45 GMT"}, {"version": "v7", "created": "Fri, 19 Sep 2014 00:49:22 GMT"}, {"version": "v8", "created": "Mon, 29 Sep 2014 17:41:52 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Akofor", "Earnest", ""], ["Chen", "Biao", ""]]}, {"id": "1311.4175", "submitter": "Sumanta Basu", "authors": "Sumanta Basu, George Michailidis", "title": "Regularized estimation in sparse high-dimensional time series models", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1315 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1535-1567", "doi": "10.1214/15-AOS1315", "report-no": "IMS-AOS-AOS1315", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and economic problems involve the analysis of\nhigh-dimensional time series datasets. However, theoretical studies in\nhigh-dimensional statistics to date rely primarily on the assumption of\nindependent and identically distributed (i.i.d.) samples. In this work, we\nfocus on stable Gaussian processes and investigate the theoretical properties\nof $\\ell _1$-regularized estimates in two important statistical problems in the\ncontext of high-dimensional time series: (a) stochastic regression with\nserially correlated errors and (b) transition matrix estimation in vector\nautoregressive (VAR) models. We derive nonasymptotic upper bounds on the\nestimation errors of the regularized estimates and establish that consistent\nestimation under high-dimensional scaling is possible via\n$\\ell_1$-regularization for a large class of stable processes under sparsity\nconstraints. A key technical contribution of the work is to introduce a measure\nof stability for stationary processes using their spectral properties that\nprovides insight into the effect of dependence on the accuracy of the\nregularized estimates. With this proposed stability measure, we establish some\nuseful deviation bounds for dependent data, which can be used to study several\nimportant regularized estimates in a time series setting.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 16:30:13 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 09:19:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Basu", "Sumanta", ""], ["Michailidis", "George", ""]]}, {"id": "1311.4500", "submitter": "Andres Sanchez-Perez", "authors": "Andres Sanchez-Perez (LTCI)", "title": "Time series prediction via aggregation : an oracle bound including\n  numerical cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of forecasting a time series meeting the Causal\nBernoulli Shift model, using a parametric set of predictors. The aggregation\ntechnique provides a predictor with well established and quite satisfying\ntheoretical properties expressed by an oracle inequality for the prediction\nrisk. The numerical computation of the aggregated predictor usually relies on a\nMarkov chain Monte Carlo method whose convergence should be evaluated. In\nparticular, it is crucial to bound the number of simulations needed to achieve\na numerical precision of the same order as the prediction risk. In this\ndirection we present a fairly general result which can be seen as an oracle\ninequality including the numerical cost of the predictor computation. The\nnumerical cost appears by letting the oracle inequality depend on the number of\nsimulations required in the Monte Carlo approximation. Some numerical\nexperiments are then carried out to support our findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 19:26:01 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 07:11:53 GMT"}, {"version": "v3", "created": "Sun, 27 Apr 2014 15:28:27 GMT"}, {"version": "v4", "created": "Mon, 26 May 2014 18:46:28 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Sanchez-Perez", "Andres", "", "LTCI"]]}, {"id": "1311.4548", "submitter": "Simon DeDeo", "authors": "David H. Wolpert and Simon DeDeo", "title": "Estimating Functions of Distributions Defined over Spaces of Unknown\n  Size", "comments": "33 pages, 3 figures. Matches published version", "journal-ref": "Entropy 2013, 15(11), 4668-4699", "doi": "10.3390/e15114668", "report-no": null, "categories": "math.ST physics.data-an q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian estimation of information-theoretic quantities from\ndata, using a Dirichlet prior. Acknowledging the uncertainty of the event space\nsize $m$ and the Dirichlet prior's concentration parameter $c$, we treat both\nas random variables set by a hyperprior. We show that the associated\nhyperprior, $P(c, m)$, obeys a simple \"Irrelevance of Unseen Variables\" (IUV)\ndesideratum iff $P(c, m) = P(c) P(m)$. Thus, requiring IUV greatly reduces the\nnumber of degrees of freedom of the hyperprior. Some information-theoretic\nquantities can be expressed multiple ways, in terms of different event spaces,\ne.g., mutual information. With all hyperpriors (implicitly) used in earlier\nwork, different choices of this event space lead to different posterior\nexpected values of these information-theoretic quantities. We show that there\nis no such dependence on the choice of event space for a hyperprior that obeys\nIUV. We also derive a result that allows us to exploit IUV to greatly simplify\ncalculations, like the posterior expected mutual information or posterior\nexpected multi-information. We also use computer experiments to favorably\ncompare an IUV-based estimator of entropy to three alternative methods in\ncommon use. We end by discussing how seemingly innocuous changes to the\nformalization of an estimation problem can substantially affect the resultant\nestimates of posterior expectations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:00:10 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Wolpert", "David H.", ""], ["DeDeo", "Simon", ""]]}, {"id": "1311.4976", "submitter": "Yazhen Wang", "authors": "Yazhen Wang", "title": "Asymptotic equivalence of quantum state tomography and noisy matrix\n  completion", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1156 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2462-2504", "doi": "10.1214/13-AOS1156", "report-no": "IMS-AOS-AOS1156", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion and quantum tomography are two unrelated research areas\nwith great current interest in many modern scientific studies. This paper\ninvestigates the statistical relationship between trace regression in matrix\ncompletion and quantum state tomography in quantum physics and quantum\ninformation science. As quantum state tomography and trace regression share the\ncommon goal of recovering an unknown matrix, it is nature to put them in the Le\nCam paradigm for statistical comparison. Regarding the two types of matrix\ninference problems as two statistical experiments, we establish their\nasymptotic equivalence in terms of deficiency distance. The equivalence study\nmotivates us to introduce a new trace regression model. The asymptotic\nequivalence provides a sound statistical foundation for applying matrix\ncompletion methods to quantum state tomography. We investigate the asymptotic\nequivalence for sparse density matrices and low rank density matrices and\ndemonstrate that sparsity and low rank are not necessarily helpful for\nachieving the asymptotic equivalence of quantum state tomography and trace\nregression. In particular, we show that popular Pauli measurements are bad for\nestablishing the asymptotic equivalence for sparse density matrices and low\nrank density matrices.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 07:57:20 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Wang", "Yazhen", ""]]}, {"id": "1311.4981", "submitter": "Lan Wang", "authors": "Lan Wang, Yongdai Kim, Runze Li", "title": "Calibrating nonconvex penalized regression in ultra-high dimension", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1159 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2505-2536", "doi": "10.1214/13-AOS1159", "report-no": "IMS-AOS-AOS1159", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate high-dimensional nonconvex penalized regression, where the\nnumber of covariates may grow at an exponential rate. Although recent\nasymptotic theory established that there exists a local minimum possessing the\noracle property under general conditions, it is still largely an open problem\nhow to identify the oracle estimator among potentially multiple local minima.\nThere are two main obstacles: (1) due to the presence of multiple minima, the\nsolution path is nonunique and is not guaranteed to contain the oracle\nestimator; (2) even if a solution path is known to contain the oracle\nestimator, the optimal tuning parameter depends on many unknown factors and is\nhard to estimate. To address these two challenging issues, we first prove that\nan easy-to-calculate calibrated CCCP algorithm produces a consistent solution\npath which contains the oracle estimator with probability approaching one.\nFurthermore, we propose a high-dimensional BIC criterion and show that it can\nbe applied to the solution path to select the optimal tuning parameter which\nasymptotically identifies the oracle estimator. The theory for a general class\nof nonconvex penalties in the ultra-high dimensional setup is established when\nthe random errors follow the sub-Gaussian distribution. Monte Carlo studies\nconfirm that the calibrated CCCP algorithm combined with the proposed\nhigh-dimensional BIC has desirable performance in identifying the underlying\nsparsity pattern for high-dimensional data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 08:34:55 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Wang", "Lan", ""], ["Kim", "Yongdai", ""], ["Li", "Runze", ""]]}, {"id": "1311.5000", "submitter": "Ningning Xia", "authors": "Ningning Xia, Yingli Qin, Zhidong Bai", "title": "Convergence rates of eigenvector empirical spectral distribution of\n  large dimensional sample covariance matrix", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1154 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2572-2607", "doi": "10.1214/13-AOS1154", "report-no": "IMS-AOS-AOS1154", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eigenvector Empirical Spectral Distribution (VESD) is adopted to\ninvestigate the limiting behavior of eigenvectors and eigenvalues of covariance\nmatrices. In this paper, we shall show that the Kolmogorov distance between the\nexpected VESD of sample covariance matrix and the Mar\\v{c}enko-Pastur\ndistribution function is of order $O(N^{-1/2})$. Given that data dimension $n$\nto sample size $N$ ratio is bounded between 0 and 1, this convergence rate is\nestablished under finite 10th moment condition of the underlying distribution.\nIt is also shown that, for any fixed $\\eta>0$, the convergence rates of VESD\nare $O(N^{-1/4})$ in probability and $O(N^{-1/4+\\eta})$ almost surely,\nrequiring finite 8th moment of the underlying distribution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 10:28:53 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 12:54:02 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Xia", "Ningning", ""], ["Qin", "Yingli", ""], ["Bai", "Zhidong", ""]]}, {"id": "1311.5024", "submitter": "Guillaume Lecu\\'e", "authors": "Guillaume Lecu\\'e and Shahar Mendelson", "title": "Minimax rate of convergence and the performance of ERM in phase recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We study the performance of Empirical Risk Minimization in noisy phase\nretrieval problems, indexed by subsets of $\\R^n$ and relative to subgaussian\nsampling; that is, when the given data is $y_i=\\inr{a_i,x_0}^2+w_i$ for a\nsubgaussian random vector $a$, independent noise $w$ and a fixed but unknown\n$x_0$ that belongs to a given subset of $\\R^n$.\n  We show that ERM produces $\\hat{x}$ whose Euclidean distance to either $x_0$\nor $-x_0$ depends on the gaussian mean-width of the indexing set and on the\nsignal-to-noise ratio of the problem. The bound coincides with the one for\nlinear regression when $\\|x_0\\|_2$ is of the order of a constant. In addition,\nwe obtain a minimax lower bound for the problem and identify sets for which ERM\nis a minimax procedure. As examples, we study the class of $d$-sparse vectors\nin $\\R^n$ and the unit ball in $\\ell_1^n$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 11:53:42 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Lecu\u00e9", "Guillaume", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1311.5066", "submitter": "David Edwards", "authors": "David Edwards and Smitha Ankinakatte", "title": "Some context-specific graphical models for discrete longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ron et al (1998) introduced a rich family of models for discrete longitudinal\ndata, called acyclic probabilistic finite automata. These may be described as\ncontext-specific graphical models, since they are represented as directed\nmultigraphs that embody context-specific conditional independence relations.\nHere we develop the methodology from a statistical modelling perspective. We\nshow how likelihood ratio tests may be constructed using standard contingency\ntable methods, and indicate how these may be used in model selection. We also\nshow that the models generalize certain subclasses of conventional undirected\nand directed graphical models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 14:30:12 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 09:26:31 GMT"}, {"version": "v3", "created": "Wed, 13 Aug 2014 11:47:46 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Edwards", "David", ""], ["Ankinakatte", "Smitha", ""]]}, {"id": "1311.5179", "submitter": "Yash Deshpande", "authors": "Yash Deshpande and Andrea Montanari", "title": "Sparse PCA via Covariance Thresholding", "comments": "40 pages, 3 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In sparse principal component analysis we are given noisy observations of a\nlow-rank matrix of dimension $n\\times p$ and seek to reconstruct it under\nadditional sparsity assumptions. In particular, we assume here each of the\nprincipal components $\\mathbf{v}_1,\\dots,\\mathbf{v}_r$ has at most $s_0$\nnon-zero entries. We are particularly interested in the high dimensional regime\nwherein $p$ is comparable to, or even much larger than $n$. In an influential\npaper, \\cite{johnstone2004sparse} introduced a simple algorithm that estimates\nthe support of the principal vectors $\\mathbf{v}_1,\\dots,\\mathbf{v}_r$ by the\nlargest entries in the diagonal of the empirical covariance. This method can be\nshown to identify the correct support with high probability if $s_0\\le\nK_1\\sqrt{n/\\log p}$, and to fail with high probability if $s_0\\ge K_2\n\\sqrt{n/\\log p}$ for two constants $0<K_1,K_2<\\infty$. Despite a considerable\namount of work over the last ten years, no practical algorithm exists with\nprovably better support recovery guarantees.\n  Here we analyze a covariance thresholding algorithm that was recently\nproposed by \\cite{KrauthgamerSPCA}. On the basis of numerical simulations (for\nthe rank-one case), these authors conjectured that covariance thresholding\ncorrectly recover the support with high probability for $s_0\\le K\\sqrt{n}$\n(assuming $n$ of the same order as $p$). We prove this conjecture, and in fact\nestablish a more general guarantee including higher-rank as well as $n$ much\nsmaller than $p$. Recent lower bounds \\cite{berthet2013computational,\nma2015sum} suggest that no polynomial time algorithm can do significantly\nbetter. The key technical component of our analysis develops new bounds on the\nnorm of kernel random matrices, in regimes that were not considered before.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 19:21:02 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 00:10:09 GMT"}, {"version": "v3", "created": "Tue, 28 Oct 2014 05:39:21 GMT"}, {"version": "v4", "created": "Tue, 4 Nov 2014 02:43:38 GMT"}, {"version": "v5", "created": "Mon, 25 Apr 2016 22:43:29 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""]]}, {"id": "1311.5187", "submitter": "Victor Yohai", "authors": "Ricardo Maronna and V\\'ictor Yohai", "title": "High finite-sample efficiency and robustness based on\n  distance-constrained maximum likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good robust estimators can be tuned to combine a high breakdown point and a\nspecified asymptotic efficiency at a central model. This happens in regression\nwith MM- and tau-estimators among others. However, the finite-sample efficiency\nof these estimators can be much lower than the asymptotic one. To overcome this\ndrawback, an approach is proposed for parametric models, which is based on a\ndistance between parameters. Given a robust estimator, the proposed one is\nobtained by maximizing the likelihood under the constraint that the distance is\nless than a given threshold. For the linear model with normal errors and using\nthe MM estimator and the distance induced by the Kullback-Leibler divergence,\nsimulations show that the proposed estimator attains a finite-sample efficiency\nclose to one, while its maximum mean squared error under pointwise outlier\ncontamination is smaller than that of the MM estimator. The same approach also\nshows good results in the estimation of multivariate location and scatter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 19:43:42 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Maronna", "Ricardo", ""], ["Yohai", "V\u00edctor", ""]]}, {"id": "1311.5301", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Hironori Fujisawa", "title": "Robust Estimation under Heavy Contamination using Enlarged Models", "comments": "32 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data analysis, contamination caused by outliers is inevitable, and robust\nstatistical methods are strongly demanded. In this paper, our concern is to\ndevelop a new approach for robust data analysis based on scoring rules. The\nscoring rule is a discrepancy measure to assess the quality of probabilistic\nforecasts. We propose a simple way of estimating not only the parameter in the\nstatistical model but also the contamination ratio of outliers. Estimating the\ncontamination ratio is important, since one can detect outliers out of the\ntraining samples based on the estimated contamination ratio. For this purpose,\nwe use scoring rules with an extended statistical models, that is called the\nenlarged models. Also, the regression problems are considered. We study a\ncomplex heterogeneous contamination, in which the contamination ratio of\noutliers in the dependent variable may depend on the independent variable. We\npropose a simple method to obtain a robust regression estimator under\nheterogeneous contamination. In addition, we show that our method provides also\nan estimator of the expected contamination ratio that is available to detect\nthe outliers out of training samples. Numerical experiments demonstrate the\neffectiveness of our methods compared to the conventional estimators.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 03:27:49 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1311.5354", "submitter": "Jonathan Roseblatt", "authors": "Jonathan Rosenblatt, Yoav Benjamini", "title": "Another Argument in Favour of Wilcoxon's Signed Rank Test", "comments": null, "journal-ref": null, "doi": "10.1080/00031305.2017.1360795", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wilcoxon Signed Rank test is typically called upon when testing whether a\nsymmetric distribution has a specified centre and the Gaussianity is in\nquestion. As with all insurance policies it comes with a cost, even if small,\nin terms of power versus a t-test, when the distribution is indeed Gaussian. In\nthis note we further show that even when the distribution tested is Gaussian\nthere need not be power loss at all, if the alternative is of a mixture type\nrather than a shift. The signed rank test may turn out to be more powerful than\nthe t-test, and the supposedly conservative strategy, might actually be the\nmore powerful one. Drug testing and functional magnetic imaging are two such\nscenarios.\n  Wilcoxon' signed rank test will typically be called upon by a researcher when\ntesting for the location of a single population, using a small sample and\nGaussianity is dubious. As all insurance policies, it will come with a cost--\npower. It is well known, that under a Gaussian setup, the signed rank test is\nless powerful than, say, a t-test. The works of Pitman and others have\nreassured us that this power loss is surprisingly small. In this note we argue\nthat the power loss might actually be smaller than typically assumed. In\nparticular, if the deviation from the null Gaussian distribution is of a\nmixture type and not a shift type, the signed rank test is no longer dominated\nby the t-test and can actually be more powerful.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 10:34:59 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Rosenblatt", "Jonathan", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1311.5366", "submitter": "Pierre-Andr\\'e Savalle", "authors": "Rui M. Castro, Gabor Lugosi, Pierre-Andr\\'e Savalle", "title": "Detection of Correlations with Adaptive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting correlations from samples of a high-dimensional\nGaussian vector has recently received a lot of attention. In most existing\nwork, detection procedures are provided with a full sample. However, following\ncommon wisdom in experimental design, the experimenter may have the capacity to\nmake targeted measurements in an on-line and adaptive manner. In this work, we\ninvestigate such adaptive sensing procedures for detecting positive\ncorrelations. It it shown that, using the same number of measurements, adaptive\nprocedures are able to detect significantly weaker correlations than their\nnon-adaptive counterparts. We also establish minimax lower bounds that show the\nlimitations of any procedure.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 11:03:10 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 14:53:50 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Castro", "Rui M.", ""], ["Lugosi", "Gabor", ""], ["Savalle", "Pierre-Andr\u00e9", ""]]}, {"id": "1311.5552", "submitter": "Steven Smith", "authors": "Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein,\n  and Scott Philips", "title": "Bayesian Discovery of Threat Networks", "comments": "IEEE Trans. Signal Process., major revision of\n  arxiv.org/abs/1303.5613. arXiv admin note: substantial text overlap with\n  arXiv:1303.5613", "journal-ref": "IEEE Trans. Signal Process., vol. 62, no. 20, pp. 5324-5338,\n  October 2014", "doi": "10.1109/TSP.2014.2336613", "report-no": null, "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel unified Bayesian framework for network detection is developed, under\nwhich a detection algorithm is derived based on random walks on graphs. The\nalgorithm detects threat networks using partial observations of their activity,\nand is proved to be optimum in the Neyman-Pearson sense. The algorithm is\ndefined by a graph, at least one observation, and a diffusion model for threat.\nA link to well-known spectral detection methods is provided, and the\nequivalence of the random walk and harmonic solutions to the Bayesian\nformulation is proven. A general diffusion model is introduced that utilizes\nspatio-temporal relationships between vertices, and is used for a specific\nspace-time formulation that leads to significant performance improvements on\ncoordinated covert networks. This performance is demonstrated using a new\nhybrid mixed-membership blockmodel introduced to simulate random covert\nnetworks with realistic properties.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 20:43:44 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 20:07:08 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 17:14:10 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Smith", "Steven T.", ""], ["Kao", "Edward K.", ""], ["Senne", "Kenneth D.", ""], ["Bernstein", "Garrett", ""], ["Philips", "Scott", ""]]}, {"id": "1311.5768", "submitter": "Alexander Jung", "authors": "Alexander Jung", "title": "An RKHS Approach to Estimation with Sparsity Constraints", "comments": "PHD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The investigation of the effects of sparsity or sparsity constraints in\nsignal processing problems has received considerable attention recently.\nSparsity constraints refer to the a priori information that the object or\nsignal of interest can be represented by using only few elements of a\npredefined dictionary. Within this thesis, sparsity refers to the fact that a\nvector to be estimated has only few nonzero entries. One specific field\nconcerned with sparsity constraints has become popular under the name\nCompressed Sensing (CS). Within CS, the sparsity is exploited in order to\nperform (nearly) lossless compression. Moreover, this compression is carried\nout jointly or simultaneously with the process of sensing a physical quantity.\nIn contrast to CS, one can alternatively use sparsity to enhance signal\nprocessing methods. Obviously, sparsity constraints can only improve the\nobtainable estimation performance since the constraints can be interpreted as\nan additional prior information about the unknown parameter vector which is to\nbe estimated. Our main focus will be on this aspect of sparsity, i.e., we\nanalyze how much we can gain in estimation performance due to the sparsity\nconstraints.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 14:50:29 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Jung", "Alexander", ""]]}, {"id": "1311.5774", "submitter": "Shanti Venetiaan", "authors": "Shanti Venetiaan", "title": "When does third order efficiency imply fourth order efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article third and fourth order efficiency are studied in the\nframework of translation equivariant location estimators. We assume\n$X_1,...,X_n$ i.i.d. $f(\\cdot -\\theta)$. By recognizing that equality in a\nspecial form of the Cauchy-Schwarz inequality leads to a certain dependence of\nthe cumulants of the maximum likelihood estimator (MLE) for $\\theta$, it is\nshown that this MLE is fourth order efficient if the underlying distribution is\nGumbel. Contrary to similar results which were previously published this result\nis not based on symmetry.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 14:56:46 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Venetiaan", "Shanti", ""]]}, {"id": "1311.5832", "submitter": "Michael Harder", "authors": "Michael Harder and Ulrich Stadtm\\\"uller", "title": "Maximal Non-Exchangeability in Dimension d", "comments": "12 pages, no figures", "journal-ref": "Journal of Multivariate Analysis 124C (2014), pp. 31-41", "doi": "10.1016/j.jmva.2013.10.003", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the maximal distance between a copula and itself when the argument is\npermuted for arbitrary dimension, generalizing a result for dimension two by\nNelsen (2007), Klement and Mesiar (2006). Furthermore, we establish a subset of\n$[0,1]^d$ in which this bound might be attained. For each point in this subset\nwe present a copula and a permutation, for which the distance in this point is\nmaximal. In the process, we see that this subset depends on the dimension being\neven or odd.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 17:48:34 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Harder", "Michael", ""], ["Stadtm\u00fcller", "Ulrich", ""]]}, {"id": "1311.6018", "submitter": "Rahul Mukerjee", "authors": "Rahul Mukerjee and S.H. Ong", "title": "Variance and Covariance Inequalities for Truncated Joint Normal\n  Distribution via Monotone Likelihood Ratio and Log-concavity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X Nv(0, {\\Lambda}) be a normal vector in v dimensions, where {\\Lambda} is\ndiagonal. With reference to the truncated distribution of X on the interior of\na v-dimensional Euclidean ball, we completely prove a variance inequality and a\ncovariance inequality that were recently discussed by F. Palombi and S. Toti\n[J. Multivariate Anal. 122 (2013) 355-376]. These inequalities ensure the\nconvergence of an algorithm for the reconstruction of {\\Lambda} only on the\nbasis of the covariance matrix of X truncated to the Euclidean ball. The\nconcept of monotone likelihood ratio is useful in our proofs. Moreover, we also\nprove and utilize the fact that the cumulative distribution function of any\npositive linear combination of independent chi-square variates is log-concave,\neven though the same may not be true for the corresponding density function.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 16:32:25 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Mukerjee", "Rahul", ""], ["Ong", "S. H.", ""]]}, {"id": "1311.6186", "submitter": "Zhao Ren", "authors": "Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou", "title": "Sparse CCA via Precision Adjusted Iterative Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Canonical Correlation Analysis (CCA) has received considerable\nattention in high-dimensional data analysis to study the relationship between\ntwo sets of random variables. However, there has been remarkably little\ntheoretical statistical foundation on sparse CCA in high-dimensional settings\ndespite active methodological and applied research activities. In this paper,\nwe introduce an elementary sufficient and necessary characterization such that\nthe solution of CCA is indeed sparse, propose a computationally efficient\nprocedure, called CAPIT, to estimate the canonical directions, and show that\nthe procedure is rate-optimal under various assumptions on nuisance parameters.\nThe procedure is applied to a breast cancer dataset from The Cancer Genome\nAtlas project. We identify methylation probes that are associated with genes,\nwhich have been previously characterized as prognosis signatures of the\nmetastasis of breast cancer.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 23:50:20 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Ren", "Zhao", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1311.6238", "submitter": "Jason D. Lee", "authors": "Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor", "title": "Exact post-selection inference, with application to the lasso", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1371 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 3, 907-927", "doi": "10.1214/15-AOS1371", "report-no": "IMS-AOS-AOS1371", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general approach to valid inference after model selection. At\nthe core of our framework is a result that characterizes the distribution of a\npost-selection estimator conditioned on the selection event. We specialize the\napproach to model selection by the lasso to form valid confidence intervals for\nthe selected coefficients and test whether all relevant variables have been\nincluded in the model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 09:01:08 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 20:15:48 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 00:56:16 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 03:06:38 GMT"}, {"version": "v5", "created": "Wed, 14 Jan 2015 02:52:11 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2015 23:57:55 GMT"}, {"version": "v7", "created": "Tue, 16 Feb 2016 09:02:58 GMT"}, {"version": "v8", "created": "Tue, 3 May 2016 07:48:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Dennis L.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1311.6293", "submitter": "Daniele Ramazzotti", "authors": "Loes Olde Loohuis, Giulio Caravagna, Alex Graudenzi, Daniele\n  Ramazzotti, Giancarlo Mauri, Marco Antoniotti, Bud Mishra", "title": "Inferring tree causal models of cancer progression with probability\n  raising", "comments": null, "journal-ref": "PLoS ONE 9(10): e108358 (2014)", "doi": "10.1371/journal.pone.0108358", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to reconstruct tree models of progression for\naccumulative processes, such as cancer, seek to estimate causation by combining\ncorrelation and a frequentist notion of temporal priority. In this paper, we\ndefine a novel theoretical framework called CAPRESE (CAncer PRogression\nExtraction with Single Edges) to reconstruct such models based on the notion of\nprobabilistic causation defined by Suppes. We consider a general reconstruction\nsetting complicated by the presence of noise in the data due to biological\nvariation, as well as experimental or measurement errors. To improve tolerance\nto noise we define and use a shrinkage-like estimator. We prove the correctness\nof our algorithm by showing asymptotic convergence to the correct tree under\nmild constraints on the level of noise. Moreover, on synthetic data, we show\nthat our approach outperforms the state-of-the-art, that it is efficient even\nwith a relatively small number of samples and that its performance quickly\nconverges to its asymptote as the number of samples increases. For real cancer\ndatasets obtained with different technologies, we highlight biologically\nsignificant differences in the progressions inferred with respect to other\ncompeting techniques and we also show how to validate conjectured biological\nrelations with progression models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 13:28:53 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 12:43:30 GMT"}, {"version": "v3", "created": "Mon, 25 Aug 2014 18:28:24 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Loohuis", "Loes Olde", ""], ["Caravagna", "Giulio", ""], ["Graudenzi", "Alex", ""], ["Ramazzotti", "Daniele", ""], ["Mauri", "Giancarlo", ""], ["Antoniotti", "Marco", ""], ["Mishra", "Bud", ""]]}, {"id": "1311.6327", "submitter": "Marie-Colette van Lieshout", "authors": "O. Cronie, M.N.M. van Lieshout", "title": "A J-function for inhomogeneous spatio-temporal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new summary statistic for inhomogeneous intensity-reweighted\nmoment stationary spatio-temporal point processes. The statistic is defined\nthrough the n-point correlation functions of the point process and it\ngeneralises the J-function when stationarity is assumed. We show that our\nstatistic can be represented in terms of the generating functional and that it\nis related to the inhomogeneous K-function. We further discuss its explicit\nform under some specific model assumptions and derive a ratio-unbiased\nestimator. We finally illustrate the use of our statistic on simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 15:10:52 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Cronie", "O.", ""], ["van Lieshout", "M. N. M.", ""]]}, {"id": "1311.6435", "submitter": "Emeline Schmisser", "authors": "Emeline Schmisser (LPP)", "title": "Non parametric estimation of the diffusion coefficents of a diffusion\n  with jumps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a jump diffusion process (X_t), with drift\nfunction b, diffusion coefficient sigma and jump coefficient xi^{2}. This\nprocess is observed at discrete times t=0,Delta,...,nDelta. The sampling\ninterval Delta tends to 0 and nDelta tends to infinity. We assume that (X_t) is\nergodic, strictly stationary and exponentially beta-mixing. We use a penalized\nleast-square approach to compute adaptive estimators of the functions\nsigma^2+xi^2 and sigma^2. We provide bounds for the risks of the two\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 20:23:13 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 13:56:52 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Schmisser", "Emeline", "", "LPP"]]}, {"id": "1311.6636", "submitter": "Junlong Zhao", "authors": "Junlong Zhao, Chenlei Leng, Lexin Li, Hansheng Wang", "title": "High-dimensional influence measure", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1165 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2639-2667", "doi": "10.1214/13-AOS1165", "report-no": "IMS-AOS-AOS1165", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagnosis is important since presence of influential observations\ncould lead to distorted analysis and misleading interpretations. For\nhigh-dimensional data, it is particularly so, as the increased dimensionality\nand complexity may amplify both the chance of an observation being influential,\nand its potential impact on the analysis. In this article, we propose a novel\nhigh-dimensional influence measure for regressions with the number of\npredictors far exceeding the sample size. Our proposal can be viewed as a\nhigh-dimensional counterpart to the classical Cook's distance. However, whereas\nthe Cook's distance quantifies the individual observation's influence on the\nleast squares regression coefficient estimate, our new diagnosis measure\ncaptures the influence on the marginal correlations, which in turn exerts\nserious influence on downstream analysis including coefficient estimation,\nvariable selection and screening. Moreover, we establish the asymptotic\ndistribution of the proposed influence measure by letting the predictor\ndimension go to infinity. Availability of this asymptotic distribution leads to\na principled rule to determine the critical value for influential observation\ndetection. Both simulations and real data analysis demonstrate usefulness of\nthe new influence diagnosis measure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 12:15:50 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Zhao", "Junlong", ""], ["Leng", "Chenlei", ""], ["Li", "Lexin", ""], ["Wang", "Hansheng", ""]]}, {"id": "1311.6765", "submitter": "Arkadi Nemirovski", "authors": "A. Goldenshluger, A. Juditski, A. Nemirovski", "title": "Hypothesis testing by convex optimization", "comments": null, "journal-ref": "Electronic Journal of Statistics 9(2):1645-1712, 2015", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general approach to handling \"multiple hypotheses\" testing in\nthe case when a particular hypothesis states that the vector of parameters\nidentifying the distribution of observations belongs to a convex compact set\nassociated with the hypothesis. With our approach, this problem reduces to\ntesting the hypotheses pairwise. Our central result is a test for a pair of\nhypotheses of the outlined type which, under appropriate assumptions, is\nprovably nearly optimal. The test is yielded by a solution to a convex\nprogramming problem, so that our construction admits computationally efficient\nimplementation. We further demonstrate that our assumptions are satisfied in\nseveral important and interesting applications. Finally, we show how our\napproach can be applied to a rather general detection problem encompassing\nseveral classical statistical settings such as detection of abrupt signal\nchanges, cusp detection and multi-sensor detection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 18:33:18 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 18:27:12 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 14:51:11 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 18:34:08 GMT"}, {"version": "v5", "created": "Wed, 15 Oct 2014 16:46:38 GMT"}, {"version": "v6", "created": "Sun, 30 Nov 2014 12:07:24 GMT"}, {"version": "v7", "created": "Tue, 23 Feb 2016 00:13:20 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Goldenshluger", "A.", ""], ["Juditski", "A.", ""], ["Nemirovski", "A.", ""]]}, {"id": "1311.7039", "submitter": "Adrien Richou", "authors": "Bernard Bercu, Adrien Richou", "title": "Large deviations for the Ornstein-Uhlenbeck process with shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the large deviation properties of the maximum likelihood\nestimators for the Ornstein-Uhlenbeck process with shift. We estimate\nsimultaneously the drift and shift parameters. On the one hand, we establish a\nlarge deviation principle for the maximum likelihood estimates of the drift and\nshift parameters. Surprisingly, we find that the drift estimator shares the\nsame large deviation principle as the one previously established for the\nOrnstein-Uhlenbeck process without shift. Sharp large deviation principles are\nalso provided. On the other hand, we show that the maximum likelihood estimator\nof the shift parameter satisfies a large deviation principle with a very\nunusual implicit rate function.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 16:58:56 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 14:37:30 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Bercu", "Bernard", ""], ["Richou", "Adrien", ""]]}, {"id": "1311.7049", "submitter": "Viacheslav Saenko", "authors": "Olga Yanushkevichiene, Viacheslav Saenko", "title": "Estimation of the characteristic exponent of stable laws", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s10986-017-9360-5", "report-no": "Lithuanian Mathematical Journal, V. 57, No 2, pp. 266-281", "categories": "math.ST physics.plasm-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical algorithm for estimating the characteristic parameter $\\alpha$\nof the stable law is presented and the estimate of its quadratic deviation is\nobtained in the paper. This algorithm is applied in the description of the\nfluctuation induced flux in the edge region of the plasma cord.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 17:48:50 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Yanushkevichiene", "Olga", ""], ["Saenko", "Viacheslav", ""]]}, {"id": "1311.7118", "submitter": "Rui Castro", "authors": "Ervin T\\'anczos and Rui M. Castro", "title": "Adaptive Sensing for Estimation of Structured Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical settings one can sequentially and adaptively guide the\ncollection of future data, based on information extracted from data collected\npreviously. These sequential data collection procedures are known by different\nnames, such as sequential experimental design, active learning or adaptive\nsensing/sampling. The intricate relation between data analysis and acquisition\nin adaptive sensing paradigms can be extremely powerful, and often allows for\nreliable signal estimation and detection in situations where non-adaptive\nsensing would fail dramatically.\n  In this work we investigate the problem of estimating the support of a\nstructured sparse signal from coordinate-wise observations under the adaptive\nsensing paradigm. We present a general procedure for support set estimation\nthat is optimal in a variety of cases and shows that through the use of\nadaptive sensing one can: (i) mitigate the effect of observation noise when\ncompared to non-adaptive sensing and, (ii) capitalize on structural information\nto a much larger extent than possible with non-adaptive sensing. In addition to\na general procedure to perform adaptive sensing in structured settings we\npresent both performance upper bounds, and corresponding lower bounds for both\nsensing paradigms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 20:41:28 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["T\u00e1nczos", "Ervin", ""], ["Castro", "Rui M.", ""]]}, {"id": "1311.7217", "submitter": "James Sharpnack", "authors": "James Sharpnack, Alessandro Rinaldo, Aarti Singh", "title": "Detecting Anomalous Activity on Networks with the Graph Fourier Scan\n  Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deciding, based on a single noisy measurement at\neach vertex of a given graph, whether the underlying unknown signal is constant\nover the graph or there exists a cluster of vertices with anomalous activation.\nThis problem is relevant to several applications such as surveillance, disease\noutbreak detection, biomedical imaging, environmental monitoring, etc. Since\nthe activations in these problems often tend to be localized to small groups of\nvertices in the graphs, we model such activity by a class of signals that are\nsupported over a (possibly disconnected) cluster with low cut size relative to\nits size. We analyze the corresponding generalized likelihood ratio (GLR)\nstatistics and relate it to the problem of finding a sparsest cut in the graph.\nWe develop a tractable relaxation of the GLR statistic based on the\ncombinatorial Laplacian of the graph, which we call the graph Fourier scan\nstatistic, and analyze its properties. We show how its performance as a testing\nprocedure depends directly on the spectrum of the graph, and use this result to\nexplicitly derive its asymptotic properties on a few significant graph\ntopologies. Finally, we demonstrate theoretically and with simulations that the\ngraph Fourier scan statistic can outperform naive testing procedures based on\nglobal averaging and vertex-wise thresholding. We also demonstrate the\nusefulness of the GFSS by analyzing groundwater Arsenic concentrations from a\nU.S. Geological Survey dataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 06:26:43 GMT"}, {"version": "v2", "created": "Sun, 21 Sep 2014 22:46:01 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Sharpnack", "James", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""]]}, {"id": "1311.7455", "submitter": "Jian Huang", "authors": "Jian Huang, Shuangge Ma, Cun-Hui Zhang and Yong Zhou", "title": "Semi-Penalized Inference with Direct False Discovery Rate Control in\n  High-Dimensions", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method, semi-penalized inference with direct false discovery\nrate control (SPIDR), for variable selection and confidence interval\nconstruction in high-dimensional linear regression. SPIDR first uses a\nsemi-penalized approach to constructing estimators of the regression\ncoefficients. We show that the SPIDR estimator is ideal in the sense that it\nequals an ideal least squares estimator with high probability under a sparsity\nand other suitable conditions. Consequently, the SPIDR estimator is\nasymptotically normal. Based on this distributional result, SPIDR determines\nthe selection rule by directly controlling false discovery rate. This provides\nan explicit assessment of the selection error. This also naturally leads to\nconfidence intervals for the selected coefficients with a proper confidence\nstatement. We conduct simulation studies to evaluate its finite sample\nperformance and demonstrate its application on a breast cancer gene expression\ndata set. Our simulation studies and data example suggest that SPIDR is a\nuseful method for high-dimensional statistical inference in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 01:28:52 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Huang", "Jian", ""], ["Ma", "Shuangge", ""], ["Zhang", "Cun-Hui", ""], ["Zhou", "Yong", ""]]}, {"id": "1311.7474", "submitter": "Botond Szabo", "authors": "Botond Szabo, Aad van der Vaart and Harry van Zanten", "title": "Honest Bayesian confidence sets for the L2-norm", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of constructing Bayesian credible sets that are\nhonest and adaptive for the L2-loss over a scale of Sobolev classes with\nregularity ranging between [D; 2D], for some given D in the context of the\nsignal-in-white-noise model. We consider a scale of prior distributions indexed\nby a regularity hyper-parameter and choose the hyper-parameter both by marginal\nlikelihood empirical Bayes and by hierarchical Bayes method, respectively. Next\nwe consider a ball centered around the corresponding posterior mean with\nprescribed posterior probability. We show by theory and examples that both the\nempirical Bayes and the hierarchical Bayes credible sets give misleading,\noverconfident uncertainty quantification for certain oddly behaving truth. Then\nwe construct a new empirical Bayes method based on risk estimation, which\nprovides the correct uncertainty quantification and optimal size.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 06:50:40 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 12:11:56 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Szabo", "Botond", ""], ["van der Vaart", "Aad", ""], ["van Zanten", "Harry", ""]]}, {"id": "1311.7513", "submitter": "Philip Dawid", "authors": "Philip Dawid, Monica Musio, Stephen E. Fienberg", "title": "From Statistical Evidence to Evidence of Causality", "comments": "27 pages, 1 table, 9 figures. This is a fairly substantial revision\n  of version 1", "journal-ref": "Bayesian Analysis, Volume 11, Number 3 (2016), 725-752", "doi": "10.1214/15-BA968", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statisticians and quantitative social scientists typically study the\n\"effects of causes\" (EoC), Lawyers and the Courts are more concerned with\nunderstanding the \"causes of effects\" (CoE). EoC can be addressed using\nexperimental design and statistical analysis, but it is less clear how to\nincorporate statistical or epidemiological evidence into CoE reasoning, as\nmight be required for a case at Law. Some form of counterfactual reasoning,\nsuch as the \"potential outcomes\" approach championed by Rubin, appears\nunavoidable, but this typically yields \"answers\" that are sensitive to\narbitrary and untestable assumptions. We must therefore recognise that a CoE\nquestion simply might not have a well-determined answer. It is nevertheless\npossible to use statistical data to set bounds within which any answer must\nlie. With less than perfect data these bounds will themselves be uncertain,\nleading to a compounding of different kinds of uncertainty. Still further care\nis required in the presence of possible confounding factors. In addition, even\nidentifying the relevant \"counterfactual contrast\" may be a matter of Policy as\nmuch as of Science. Defining the question is as non-trivial a task as finding a\nroute towards an answer. This paper develops some technical elaborations of\nthese philosophical points, and illustrates them with an analysis of a case\nstudy in child protection.\n  Keywords: benfluorex, causes of effects, counterfactual, child protection,\neffects of causes, Fre'chet bound, potential outcome, probability of causation\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 10:17:54 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2014 20:39:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "Philip", ""], ["Musio", "Monica", ""], ["Fienberg", "Stephen E.", ""]]}]