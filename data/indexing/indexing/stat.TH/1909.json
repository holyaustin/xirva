[{"id": "1909.00002", "submitter": "Steffen Betsch", "authors": "Steffen Betsch, Bruno Ebner, Bernhard Klar", "title": "Minimum $L^q$-distance estimators for non-normalized parametric models", "comments": "27 pages, 8 tables", "journal-ref": "The Canadian Journal of Statistics, Volume 49, Issue 2, pages\n  514-548, (2021)", "doi": "10.1002/cjs.11574", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and investigate a new estimation method for the parameters of\nmodels consisting of smooth density functions on the positive half axis. The\nprocedure is based on a recently introduced characterization result for the\nrespective probability distributions, and is to be classified as a minimum\ndistance estimator, incorporating as a distance function the $L^q$-norm.\nThroughout, we deal rigorously with issues of existence and measurability of\nthese implicitly defined estimators. Moreover, we provide consistency results\nin a common asymptotic setting, and compare our new method with classical\nestimators for the exponential-, the Rayleigh-, and the Burr Type XII\ndistribution in Monte Carlo simulation studies. We also assess the performance\nof different estimators for non-normalized models in the context of an\nexponential-polynomial family.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:19:28 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 10:22:14 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""], ["Klar", "Bernhard", ""]]}, {"id": "1909.00116", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan", "title": "Statistical Inferences of Linear Forms for Noisy Matrix Completion", "comments": "Minor typos are corrected; real data examples are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible framework for making inferences about general linear\nforms of a large matrix based on noisy observations of a subset of its entries.\nIn particular, under mild regularity conditions, we develop a universal\nprocedure to construct asymptotically normal estimators of its linear forms\nthrough double-sample debiasing and low-rank projection whenever an entry-wise\nconsistent estimator of the matrix is available. These estimators allow us to\nsubsequently construct confidence intervals for and test hypotheses about the\nlinear forms. Our proposal was motivated by a careful perturbation analysis of\nthe empirical singular spaces under the noisy matrix completion model which\nmight be of independent interest. The practical merits of our proposed\ninference procedure are demonstrated on both simulated and real-world data\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 03:30:07 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 08:49:01 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""]]}, {"id": "1909.00232", "submitter": "Aretha Teckentrup", "authors": "Aretha L Teckentrup", "title": "Convergence of Gaussian Process Regression with Estimated\n  Hyper-parameters and Applications in Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with the convergence of Gaussian process regression. A\nparticular focus is on hierarchical Gaussian process regression, where\nhyper-parameters appearing in the mean and covariance structure of the Gaussian\nprocess emulator are a-priori unknown, and are learnt from the data, along with\nthe posterior mean and covariance. We work in the framework of empirical Bayes,\nwhere a point estimate of the hyper-parameters is computed, using the data, and\nthen used within the standard Gaussian process prior to posterior update. We\nprovide a convergence analysis that (i) holds for any continuous function $f$\nto be emulated; and (ii) shows that convergence of Gaussian process regression\nis unaffected by the additional learning of hyper-parameters from data, and is\nguaranteed in a wide range of scenarios. As the primary motivation for the work\nis the use of Gaussian process regression to approximate the data likelihood in\nBayesian inverse problems, we provide a bound on the error introduced in the\nBayesian posterior distribution in this context.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 15:52:17 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 09:45:02 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 10:26:49 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Teckentrup", "Aretha L", ""]]}, {"id": "1909.00250", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao", "title": "A Note on New Bernstein-type Inequalities for the Log-likelihood\n  Function of Bernoulli Variables", "comments": "Accepted by Statistics & Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a new Bernstein-type inequality for the log-likelihood function of\nBernoulli variables. In contrast to classical Bernstein's inequality and\nHoeffding's inequality when applied to the log-likelihood, the new bound is\nindependent of the parameters of the Bernoulli variables and therefore does not\nblow up as the parameters approach 0 or 1. The new inequality strengthens\ncertain theoretical results on likelihood-based methods for community detection\nin networks and can be applied to other likelihood-based methods for binary\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 17:47:34 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 09:06:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhao", "Yunpeng", ""]]}, {"id": "1909.00320", "submitter": "Hwiyoung Lee", "authors": "Hwiyoung Lee, Vic Patrangenaru", "title": "Anti-MANOVA on Compact Manifolds with Applications to 3D Projective\n  Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of hypotheses testing for equality of extrinsic antimeans on compact\nmanifolds are unveiled in this paper. The two and multiple sample problem for\nantimeans on compact manifolds is addressed for large samples via asymptotic\ndistributions, as well as for small samples using nonparametric bootstrap. An\nexample of face differentiation using 3D VW antimean projective shape analysis\nfor data extracted from digital camera images is also given.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 04:14:50 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lee", "Hwiyoung", ""], ["Patrangenaru", "Vic", ""]]}, {"id": "1909.00410", "submitter": "Fernando Galaz-Garcia", "authors": "Benjamin Eltzner, Fernando Galaz-Garcia, Stephan F. Huckemann,\n  Wilderich Tuschmann", "title": "Stability of the Cut Locus and a Central Limit Theorem for Fr\\'echet\n  Means of Riemannian Manifolds", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a Central Limit Theorem for closed Riemannian manifolds, clarifying\nalong the way the geometric meaning of some of the hypotheses in Bhattacharya\nand Lin's Omnibus Central Limit Theorem for Fr\\'echet means. We obtain our CLT\nassuming certain stability hypothesis for the cut locus, which always holds\nwhen the manifold is compact but may not be satisfied in the non-compact case.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 14:41:56 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 07:47:42 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Eltzner", "Benjamin", ""], ["Galaz-Garcia", "Fernando", ""], ["Huckemann", "Stephan F.", ""], ["Tuschmann", "Wilderich", ""]]}, {"id": "1909.00474", "submitter": "Randolf Altmeyer", "authors": "Randolf Altmeyer", "title": "Central limit theorems for discretized occupation time functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of integral type functionals is studied for discrete\nobservations of a continuous It\\^o semimartingale. Based on novel\napproximations in the Fourier domain, central limit theorems are proved for\n$L^2$-Sobolev functions with fractional smoothness. An explicit $L^2$-lower\nbound shows that already lower order quadrature rules, such as the trapezoidal\nrule and the classical Riemann estimator, are rate optimal, but only the\ntrapezoidal rule is efficient, achieving the minimal asymptotic variance.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 21:19:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Altmeyer", "Randolf", ""]]}, {"id": "1909.00559", "submitter": "Yassine El Maazouz", "authors": "Yassine El Maazouz, Ngoc Mai Tran", "title": "Statistics of Gaussians on local fields and their tropicalizations", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multivariate Gaussian distributions on local fields such as the\nfield of p-adic numbers. We introduce the Bruhat-Tits building as a parameter\nspace for Gaussian distributions and study some classic statistical problems in\nthis setting. Finally we study geometric and probabilistic structures of the\ntropicalization of such distributions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 06:24:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Maazouz", "Yassine El", ""], ["Tran", "Ngoc Mai", ""]]}, {"id": "1909.00579", "submitter": "Tino Werner", "authors": "Tino Werner", "title": "Asymptotic linear expansion of regularized M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric high-dimensional regression analysis requires the usage of\nregularization terms to get interpretable models. The respective estimators can\nbe regarded as regularized M-functionals which are naturally highly nonlinear.\nWe study under which conditions these M-functionals are compactly\ndifferentiable, so that the corresponding estimators admit an asymptotically\nlinear expansion. In a one-step construction, for a suitably consistent\nstarting estimator, this linearization replaces solving optimization problems\nby evaluating the corresponding influence curves at the given data points. We\nshow under which conditions the asymptotic linear expansion is valid and\nprovide concrete examples of machine learning algorithms that fit into this\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 07:32:12 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Werner", "Tino", ""]]}, {"id": "1909.00625", "submitter": "Lixue Pang", "authors": "Geurt Jongbloed, Frank van der Meulen and Lixue Pang", "title": "Nonparametric Bayesian estimation of a concave distribution function\n  with mixed interval censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume we observe a finite number of inspection times together with\ninformation on whether a specific event has occurred before each of these\ntimes. Suppose replicated measurements are available on multiple event times.\nThe set of inspection times, including the number of inspections, may be\ndifferent for each event. This is known as mixed case interval censored data.\nWe consider Bayesian estimation of the distribution function of the event time\nwhile assuming it is concave. We provide sufficient conditions on the prior\nsuch that the resulting procedure is consistent from the Bayesian point of\nview. We also provide computational methods for drawing from the posterior and\nillustrate the performance of the Bayesian method in both a simulation study\nand two real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:33:18 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 09:30:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jongbloed", "Geurt", ""], ["van der Meulen", "Frank", ""], ["Pang", "Lixue", ""]]}, {"id": "1909.00747", "submitter": "Toby Kenney", "authors": "Toby Kenney", "title": "Consistency of Ranking Estimators", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ranking problem is to order a collection of units by some unobserved\nparameter, based on observations from the associated distribution. This problem\narises naturally in a number of contexts, such as business, where we may want\nto rank potential projects by profitability; or science, where we may want to\nrank predictors potentially associated with some trait by the strength of the\nassociation. This approach provides a valuable alternative to the sparsity\nframework often used with big data. Most approaches to this problem are\nempirical Bayesian, where we use the data to estimate the hyperparameters of\nthe prior distribution, then use that distribution to estimate the unobserved\nparameter values. There are a number of different approaches to this problem,\nbased on different loss functions for mis-ranking units. Despite the number of\npapers developing methods for this problem, there is no work on the consistency\nof these methods. In this paper, we develop a general framework for consistency\nof empirical Bayesian ranking methods, which includes nearly all commonly used\nmethods. We then determine conditions under which consistency holds. Given that\nlittle work has been done on selection of prior distribution, and that the loss\nfunctions developed are not strongly motivated, we consider the case where both\nof these are misspecified. We show that provided the loss function is\nreasonable; the prior distribution is not too light-tailed; and the error in\nmeasuring each unit converges to zero at a fast enough rate compared with the\nnumber of units (which is assumed to increase to infinity); all ranking methods\nare consistent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:50:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kenney", "Toby", ""]]}, {"id": "1909.00966", "submitter": "Nhat Ho", "authors": "Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter Bartlett, Michael I.\n  Jordan", "title": "A Diffusion Process Perspective on Posterior Contraction Rates for\n  Parameters", "comments": "36 pages. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that diffusion processes can be exploited to study the posterior\ncontraction rates of parameters in Bayesian models. By treating the posterior\ndistribution as a stationary distribution of a stochastic differential equation\n(SDE), posterior convergence rates can be established via control of the\nmoments of the corresponding SDE. Our results depend on the structure of the\npopulation log-likelihood function, obtained in the limit of an infinite sample\nsample size, and stochastic perturbation bounds between the population and\nsample log-likelihood functions. When the population log-likelihood is strongly\nconcave, we establish posterior convergence of a $d$-dimensional parameter at\nthe optimal rate $(d/n)^{1/ 2}$. In the weakly concave setting, we show that\nthe convergence rate is determined by the unique solution of a non-linear\nequation that arises from the interplay between the degree of weak concavity\nand the stochastic perturbation bounds. We illustrate this general theory by\nderiving posterior convergence rates for three concrete examples: Bayesian\nlogistic regression models, Bayesian single index models, and over-specified\nBayesian mixture models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 06:15:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mou", "Wenlong", ""], ["Ho", "Nhat", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1909.01103", "submitter": "Mahendra Saha", "authors": "Mahendra Saha, Abhimanyu Singh Yadav, Arvind Pandey, Shivanshi Shukla,\n  Sudhansu S Maiti", "title": "The extended xgamma distribution", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article aims to introduced a new distribution named as extended xgamma\n(EXg) distribution. This generalization is derived from xgamma distribution\n(Xg), a special finite mixture of exponential and gamma distributions [see, Sen\net al. ($2016$)]. Some important statistical properties, viz., survival\ncharacteristics, moments, mean deviation and random number generation have been\nderived. Further, maximum likelihood estimation for the estimation of the\nunknown parameters have also been discussed for the complete sample. The\napplication of the proposed model has been illustrated through a real data set\nand observed that the proposed model might be taken as an better alternative to\nsome well known lifetime distributions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:59:57 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Saha", "Mahendra", ""], ["Yadav", "Abhimanyu Singh", ""], ["Pandey", "Arvind", ""], ["Shukla", "Shivanshi", ""], ["Maiti", "Sudhansu S", ""]]}, {"id": "1909.01175", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Controlled Loosening-up (CLuP) -- achieving exact MIMO ML in polynomial\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we attack one of the most fundamental signal\nprocessing/informaton theory problems, widely known as the MIMO ML-detection.\nWe introduce a powerful Random Duality Theory (RDT) mechanism that we refer to\nas the Controlled Loosening-up (CLuP) as a way of achieving the exact\nML-performance in MIMO systems in polynomial time. We first outline the general\nstrategy and then discuss the rationale behind the entire concept. A solid\ncollection of results obtained through numerical experiments is presented as\nwell and found to be in an excellent agreement with what the theory predicts.\nAs this is the introductory paper of a massively general concept that we have\ndeveloped, we mainly focus on keeping things as simple as possible and put the\nemphasis on the most fundamental ideas. In our several companion papers we\npresent various other complementary results that relate to both, theoretical\nand practical aspects and their connections to a large collection of other\nproblems and results that we have achieved over the years in Random Duality.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:43:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1909.01190", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Complexity analysis of the Controlled Loosening-up (CLuP) algorithm", "comments": "A few typos and numerical values polished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our companion paper \\cite{Stojnicclupint19} we introduced a powerful\nmechanism that we referred to as the Controlled Loosening-up (CLuP) for\nhandling MIMO ML-detection problems. It turned out that the algorithm has many\nremarkable features and one of them, the \\emph{computational complexity}, we\ndiscuss in more details in this paper. As was explained in\n\\cite{Stojnicclupint19}, the CLuP is an iterative procedure where each\niteration amounts to solving a simple quadratic program. This clearly implies\nthat the key contributing factor to its overall computational complexity is the\nnumber of iterations needed to achieve a required precision. As was also hinted\nin \\cite{Stojnicclupint19}, that number seems to be fairly low and in some of\nthe most interesting scenarios often not even larger than $10$. Here we provide\na Random Duality Theory based careful analysis that indeed indicates that a\nvery small number of iterations is sufficient to achieve an excellent\nperformance. A solid set of results obtained through numerical experiments is\npresented as well and shown to be in a nice agreement with what the theoretical\nanalysis predicts. Also, as was the case in \\cite{Stojnicclupint19}, we again\nfocus only on the core CLuP algorithm but do mention on several occasions that\nthe concepts that we introduce here are as remarkably general as those that we\nintroduced in \\cite{Stojnicclupint19} and can be utilized in the analysis of a\nlarge number of classes of algorithms applicable in the most diverse of\nscientific fields. Many results in these directions we will present in several\nof our companion papers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:59:06 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 11:14:52 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1909.01201", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Starting CLuP with polytope relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Controlled Loosening-up (CLuP) mechanism that we recently introduced in\n\\cite{Stojnicclupint19} is a generic concept that can be utilized to solve a\nlarge class of problems in polynomial time. Since it relies in its core on an\niterative procedure, the key to its excellent performance lies in a typically\nvery small number of iterations needed to execute the entire algorithm. In a\nseparate paper \\cite{Stojnicclupcmpl19}, we presented a detailed complexity\nanalysis that indeed confirms the relatively small number of iterations. Since\nboth papers, \\cite{Stojnicclupint19} and \\cite{Stojnicclupcmpl19} are the\nintroductory papers on the topic we made sure to limit the initial discussion\njust to the core of the algorithm and consequently focused only on the\nalgorithm's most basic version. On numerous occasions though, we emphasized\nthat various improvements and further upgrades are possible. In this paper we\npresent a first step in this direction and discuss a very simple upgrade that\ncan be introduced on top of the basic CLuP mechanism. It relates to the\nstarting of the CLuP and suggests the well-known so-called polytope-relaxation\nheuristic (see, e.g. \\cite{StojnicBBSD05,StojnicBBSD08}) as the starting point.\nWe refer to this variant of CLuP as the CLuP-plt and proceed with the\npresentation of its complexity analysis. As in \\cite{Stojnicclupcmpl19}, a\nparticular \\textbf{\\emph{complexity analysis per iteration level}} type of\ncomplexity analysis is chosen and presented through the algorithm's application\non the well-known MIMO ML detection problem. As expected, the analysis confirms\nthat CLuP-plt performs even better than the original CLuP. In some of the most\ninteresting regimes it often achieves within the \\textbf{\\emph{first three\niterations}} an excellent performance. We also complement the theoretical\nfindings with a solid set of numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:13:29 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "1909.01211", "submitter": "Kou Fujimori", "authors": "Kou Fujimori, Sota Sakamoto and Yasutaka Shimizu", "title": "Moment convergence of the generalized maximum composite likelihood\n  estimators for determinantal point processes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum composite likelihood estimator for parametric models of\ndeterminantal point processes (DPPs) is discussed. Since the joint intensities\nof these point processes are given by determinant of positive definite kernels,\nwe have the explicit form of the joint intensities for every order. This fact\nenables us to consider the generalized maximum composite likelihood estimator\nfor any order. This paper introduces the two step generalized composite\nlikelihood estimator and shows the moment convergence of the estimator under a\nstationarity. Moreover, our results can yield information criteria for\nstatistical model selection within DPPs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:27:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Fujimori", "Kou", ""], ["Sakamoto", "Sota", ""], ["Shimizu", "Yasutaka", ""]]}, {"id": "1909.01464", "submitter": "Guang Cheng", "authors": "Xingye Qiao, Jiexin Duan, Guang Cheng", "title": "Rates of Convergence for Large-scale Nearest Neighbor Classification", "comments": "Camera ready version for NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor is a popular class of classification methods with many\ndesirable properties. For a large data set which cannot be loaded into the\nmemory of a single machine due to computation, communication, privacy, or\nownership limitations, we consider the divide and conquer scheme: the entire\ndata set is divided into small subsamples, on which nearest neighbor\npredictions are made, and then a final decision is reached by aggregating the\npredictions on subsamples by majority voting. We name this method the big\nNearest Neighbor (bigNN) classifier, and provide its rates of convergence under\nminimal assumptions, in terms of both the excess risk and the classification\ninstability, which are proven to be the same rates as the oracle nearest\nneighbor classifier and cannot be improved. To significantly reduce the\nprediction time that is required for achieving the optimal rate, we also\nconsider the pre-training acceleration technique applied to the bigNN method,\nwith proven convergence rate. We find that in the distributed setting, the\noptimal choice of the neighbor $k$ should scale with both the total sample size\nand the number of partitions, and there is a theoretical upper limit for the\nlatter. Numerical studies have verified the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:36:41 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 02:10:29 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Qiao", "Xingye", ""], ["Duan", "Jiexin", ""], ["Cheng", "Guang", ""]]}, {"id": "1909.01675", "submitter": "Tatiana Komarova", "authors": "Tatiana Komarova and Javier Hidalgo", "title": "Testing nonparametric shape restrictions", "comments": "62 pages, 6 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and examine a test for a general class of shape constraints, such\nas constraints on the signs of derivatives, U-(S-)shape, symmetry,\nquasi-convexity, log-convexity, $r$-convexity, among others, in a nonparametric\nframework using partial sums empirical processes. We show that, after a\nsuitable transformation, its asymptotic distribution is a functional of the\nstandard Brownian motion, so that critical values are available. However, due\nto the possible poor approximation of the asymptotic critical values to the\nfinite sample ones, we also describe a valid bootstrap algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:13:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:21:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Komarova", "Tatiana", ""], ["Hidalgo", "Javier", ""]]}, {"id": "1909.01691", "submitter": "Alexander Fisch", "authors": "Alexander T M Fisch, Idris A Eckley, Paul Fearnhead", "title": "Subset Multivariate Collective And Point Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in identifying anomalous\nstructure within multivariate data streams. We consider the problem of\ndetecting collective anomalies, corresponding to intervals where one or more of\nthe data streams behaves anomalously. We first develop a test for a single\ncollective anomaly that has power to simultaneously detect anomalies that are\neither rare, that is affecting few data streams, or common. We then show how to\ndetect multiple anomalies in a way that is computationally efficient but avoids\nthe approximations inherent in binary segmentation-like approaches. This\napproach, which we call MVCAPA, is shown to consistently estimate the number\nand location of the collective anomalies, a property that has not previously\nbeen shown for competing methods. MVCAPA can be made robust to point anomalies\nand can allow for the anomalies to be imperfectly aligned. We show the\npractical usefulness of allowing for imperfect alignments through a resulting\nincrease in power to detect regions of copy number variation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:58:46 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fisch", "Alexander T M", ""], ["Eckley", "Idris A", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1909.01723", "submitter": "Lucas Rooney", "authors": "Lucas Rooney", "title": "Random Graph Models and Matchings", "comments": "Written as reference paper on topic during undergraduate research\n  project", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will provide an introductory understanding of random graph\nmodels, and matchings in the case of Erdos-Renyi random graphs. We will provide\na synthesis of background theory to this end. We will further examine pertinent\nrecent results and provide a basis of further exploration.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:49:32 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 01:52:24 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Rooney", "Lucas", ""]]}, {"id": "1909.01812", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi", "title": "Learning Distributions Generated by One-Layer ReLU Networks", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of a $d$-dimensional\nrectified Gaussian distribution from i.i.d. samples. A rectified Gaussian\ndistribution is defined by passing a standard Gaussian distribution through a\none-layer ReLU neural network. We give a simple algorithm to estimate the\nparameters (i.e., the weight matrix and bias vector of the ReLU neural network)\nup to an error $\\epsilon||W||_F$ using $\\tilde{O}(1/\\epsilon^2)$ samples and\n$\\tilde{O}(d^2/\\epsilon^2)$ time (log factors are ignored for simplicity). This\nimplies that we can estimate the distribution up to $\\epsilon$ in total\nvariation distance using $\\tilde{O}(\\kappa^2d^2/\\epsilon^2)$ samples, where\n$\\kappa$ is the condition number of the covariance matrix. Our only assumption\nis that the bias vector is non-negative. Without this non-negativity\nassumption, we show that estimating the bias vector within any error requires\nthe number of samples at least exponential in the infinity norm of the bias\nvector. Our algorithm is based on the key observation that vector norms and\npairwise angles can be estimated separately. We use a recent result on learning\nfrom truncated samples. We also prove two sample complexity lower bounds:\n$\\Omega(1/\\epsilon^2)$ samples are required to estimate the parameters up to\nerror $\\epsilon$, while $\\Omega(d/\\epsilon^2)$ samples are necessary to\nestimate the distribution up to $\\epsilon$ in total variation distance. The\nfirst lower bound implies that our algorithm is optimal for parameter\nestimation. Finally, we show an interesting connection between learning a\ntwo-layer generative model and non-negative matrix factorization. Experimental\nresults are provided to support our analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:04:46 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 16:04:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Wu", "Shanshan", ""], ["Dimakis", "Alexandros G.", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1909.01848", "submitter": "Daniel Malinsky", "authors": "Daniel Malinsky, Ilya Shpitser, Eric J Tchetgen Tchetgen", "title": "Semiparametric Inference for Non-monotone Missing-Not-at-Random Data:\n  the No Self-Censoring Model", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the identification and estimation of statistical functionals of\nmultivariate data missing non-monotonically and not-at-random, taking a\nsemiparametric approach. Specifically, we assume that the missingness mechanism\nsatisfies what has been previously called \"no self-censoring\" or \"itemwise\nconditionally independent nonresponse,\" which roughly corresponds to the\nassumption that no partially-observed variable directly determines its own\nmissingness status. We show that this assumption, combined with an odds ratio\nparameterization of the joint density, enables identification of functionals of\ninterest, and we establish the semiparametric efficiency bound for the\nnonparametric model satisfying this assumption. We propose a practical\naugmented inverse probability weighted estimator, and in the setting with a\n(possibly high-dimensional) always-observed subset of covariates, our proposed\nestimator enjoys a certain double-robustness property. We explore the\nperformance of our estimator with simulation experiments and on a\npreviously-studied data set of HIV-positive mothers in Botswana.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:47:28 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 17:48:42 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 23:08:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric J Tchetgen", ""]]}, {"id": "1909.01978", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Bryon Aragam and Qing Zhou", "title": "On perfectness in Gaussian graphical models", "comments": "This note is based on a result that first appeared in\n  arXiv:1711.00991v1. The original article has now been split into two parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing when a graphical model is perfect to a distribution is essential in\norder to relate separation in the graph to conditional independence in the\ndistribution, and this is particularly important when performing inference from\ndata. When the model is perfect, there is a one-to-one correspondence between\nconditional independence statements in the distribution and separation\nstatements in the graph. Previous work has shown that almost all models based\non linear directed acyclic graphs as well as Gaussian chain graphs are perfect,\nthe latter of which subsumes Gaussian graphical models (i.e., the undirected\nGaussian models) as a special case. However, the complexity of chain graph\nmodels leads to a proof of this result which is indirect and mired by the\ncomplications of parameterizing this general class. In this paper, we directly\napproach the problem of perfectness for the Gaussian graphical models, and\nprovide a new proof, via a more transparent parametrization, that almost all\nsuch models are perfect. Our approach is based on, and substantially extends, a\nconstruction of Ln\\v{e}ni\\v{c}ka and Mat\\'u\\v{s} showing the existence of a\nperfect Gaussian distribution for any graph.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 19:17:23 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Amini", "Arash A.", ""], ["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1909.02088", "submitter": "Rohit Patra", "authors": "Arun K. Kuchibhotla and Rohit K. Patra", "title": "On Least Squares Estimation under Heteroscedastic and Heavy-Tailed\n  Errors", "comments": "49 pages, 2 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider least squares estimation in a general nonparametric regression\nmodel. The rate of convergence of the least squares estimator (LSE) for the\nunknown regression function is well studied when the errors are sub-Gaussian.\nWe find upper bounds on the rates of convergence of the LSE when the errors\nhave uniformly bounded conditional variance and have only finitely many\nmoments. We show that the interplay between the moment assumptions on the\nerror, the metric entropy of the class of functions involved, and the \"local\"\nstructure of the function class around the truth drives the rate of convergence\nof the LSE. We find sufficient conditions on the errors under which the rate of\nthe LSE matches the rate of the LSE under sub-Gaussian error. Our results are\nfinite sample and allow for heteroscedastic and heavy-tailed errors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:21:02 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 16:58:51 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 22:02:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Patra", "Rohit K.", ""]]}, {"id": "1909.02139", "submitter": "Hyo Young Choi", "authors": "Hyo Young Choi and J. S. Marron", "title": "Theory of high-dimensional outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study concerns the issue of high dimensional outliers which are\nchallenging to distinguish from inliers due to the special structure of high\ndimensional space. We introduce a new notion of high dimensional outliers that\nembraces various types and provides deep insights into understanding the\nbehavior of these outliers based on several asymptotic regimes. Our study of\ngeometrical properties of high dimensional outliers reveals an interesting\ntransition phenomenon of outliers from near the surface of a high dimensional\nsphere to being distant from the sphere. Also, we study the PCA subspace\nconsistency when data contain a limited number of outliers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 22:31:45 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Choi", "Hyo Young", ""], ["Marron", "J. S.", ""]]}, {"id": "1909.02229", "submitter": "Shouri Hu", "authors": "Hock Peng Chan and Shouri Hu", "title": "Optimal UCB Adjustments for Large Arm Sizes", "comments": "First Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regret lower bound of Lai and Robbins (1985), the gold standard for\nchecking optimality of bandit algorithms, considers arm size fixed as sample\nsize goes to infinity. We show that when arm size increases polynomially with\nsample size, a surprisingly smaller lower bound is achievable. This is because\nthe larger experimentation costs when there are more arms permit regret savings\nby exploiting the best performer more often. In particular we are able to\nconstruct a UCB-Large algorithm that adaptively exploits more when there are\nmore arms. It achieves the smaller lower bound and is thus optimal. Numerical\nexperiments show that UCB-Large performs better than classical UCB that does\nnot correct for arm size, and better than Thompson sampling.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 06:30:17 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Chan", "Hock Peng", ""], ["Hu", "Shouri", ""]]}, {"id": "1909.02243", "submitter": "Wenquan Cui", "authors": "Wenquan Cui, Jianjun Xu and Yuehua Wu", "title": "A new reproducing kernel based nonlinear dimension reduction method for\n  survival data", "comments": "51 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the theories of sliced inverse regression (SIR) and reproducing\nkernel Hilbert space (RKHS), a new approach RDSIR (RKHS-based Double SIR) to\nnonlinear dimension reduction for survival data is proposed and discussed. An\nisometrically isomorphism is constructed based on RKHS property, then the\nnonlinear function in the RKHS can be represented by the inner product of two\nelements which reside in the isomorphic feature space. Due to the censorship of\nsurvival data, double slicing is used to estimate weight function or\nconditional survival function to adjust for the censoring bias. The sufficient\ndimension reduction (SDR) subspace is estimated by a generalized\neigen-decomposition problem. Our method is computationally efficient with fast\ncalculation speed and small computational burden. The asymptotic property and\nthe convergence rate of the estimator are also discussed based on the\nperturbation theory. Finally, we illustrate the performance of RDSIR on\nsimulated and real data to confirm that RDSIR is comparable with linear SDR\nmethod. The most important is that RDSIR can also extract nonlinearity in\nsurvival data effectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 07:29:21 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Cui", "Wenquan", ""], ["Xu", "Jianjun", ""], ["Wu", "Yuehua", ""]]}, {"id": "1909.02303", "submitter": "Aniket Biswas", "authors": "Aniket Biswas, Subrata Chakraborty and Meghna Mukherjee", "title": "Further study on inferential aspects of log-Lindley distribution with an\n  application of stress-strength reliability in insurance", "comments": "19 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-Lindley distribution was recently introduced in the literature as a\nviable alternative to the Beta distribution. This distribution has a simple\nstructure and possesses useful theoretical properties relevant in insurance.\nClassical estimation methods have been well studied. We introduce estimation of\nparameters from Bayesian point of view for this distribution. Explicit\nstructure of stress-strength reliability and its inference under both classical\nand Bayesian set-up is addressed. Extensive simulation studies show marked\nimprovement with Bayesian approach over classical given reasonable prior\ninformation. An application of a useful metric of discrepancy derived from\nstress-strength reliability is considered and computed for two categories of\nfirm with respect to a certain financial indicator.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:24:57 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Biswas", "Aniket", ""], ["Chakraborty", "Subrata", ""], ["Mukherjee", "Meghna", ""]]}, {"id": "1909.02376", "submitter": "Marko Voutilainen", "authors": "Marko Voutilainen, Lauri Viitasaari, Pauliina Ilmonen, Soledad Torres,\n  Ciprian Tudor", "title": "Vector-valued Generalised Ornstein-Uhlenbeck Processes", "comments": "Alignment of equations has been changed so that they fit inside the\n  margins. Updated reference list", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalisations of the Ornstein-Uhlenbeck process defined through Langevin\nequation $dU_t = - \\Theta U_t dt + dG_t,$ such as fractional Ornstein-Uhlenbeck\nprocesses, have recently received a lot of attention in the literature. In\nparticular, estimation of the unknown parameter $\\Theta$ is widely studied\nunder Gaussian stationary increment noise $G$. Langevin equation is well-known\nfor its connections to physics. In addition to that, motivation for studying\nLangevin equation with a general noise $G$ stems from the fact that the\nequation characterises all univariate stationary processes. Most of the\nliterature on the topic focuses on the one-dimensional case with Gaussian noise\n$G$. In this article, we consider estimation of the unknown model parameter in\nthe multidimensional version of the Langevin equation, where the parameter\n$\\Theta$ is a matrix and $G$ is a general, not necessarily Gaussian,\nvector-valued process with stationary increments. Based on algebraic Riccati\nequations, we construct an estimator for the matrix $\\Theta$. Moreover, we\nprove the consistency of the estimator and derive its limiting distribution\nunder natural assumptions. In addition, to motivate our work, we prove that the\nLangevin equation characterises all stationary processes in a multidimensional\nsetting as well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 13:00:17 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 17:53:16 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Voutilainen", "Marko", ""], ["Viitasaari", "Lauri", ""], ["Ilmonen", "Pauliina", ""], ["Torres", "Soledad", ""], ["Tudor", "Ciprian", ""]]}, {"id": "1909.02546", "submitter": "Philip Ernst", "authors": "Philip A. Ernst, L.C.G. Rogers, and Quan Zhou", "title": "The distribution of Yule's \"nonsense correlation\"", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2017, the authors of~\\citet{ernst2017yule} explicitly computed the second\nmoment of Yule's \"nonsense correlation,\" offering the first mathematical\nexplanation of Yule's 1926 empirical finding of nonsense\ncorrelation.~\\citep{yule1926}. The present work closes the final longstanding\nopen question on the distribution of Yule's nonsense correlation \\beqn \\rho:=\n\\frac{\\int_0^1W_1(t)W_2(t) dt - \\int_0^1W_1(t) dt \\int_0^1 W_2(t)\ndt}{\\sqrt{\\int_0^1 W^2_1(t) dt - \\parens{\\int_0^1W_1(t) dt}^2} \\sqrt{\\int_0^1\nW^2_2(t) dt - \\parens{\\int_0^1W_2(t) dt}^2}} \\eeqn by explicitly calculating\nall moments of $\\rho$ (up to order 16) for two {\\em independent} Wiener\nprocesses, $W_1, W_2$. These lead to an approximation to the density of Yule's\nnonsense correlation, apparently for the first time. We proceed to explicitly\ncompute higher moments of Yule's nonsense correlation when the two independent\nWiener processes are replaced by two \\textit{correlated} Wiener processes, two\nindependent Ornstein-Uhlenbeck processes, and two independent Brownian bridges.\nWe conclude by extending the definition of $\\rho$ to the time interval $[0, T]$\nfor any $T > 0$ and prove a Central Limit Theorem for the case of two\nindependent Ornstein-Uhlenbeck processes processes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:38:46 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 17:22:51 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 02:21:02 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Ernst", "Philip A.", ""], ["Rogers", "L. C. G.", ""], ["Zhou", "Quan", ""]]}, {"id": "1909.02553", "submitter": "Nathan Kallus", "authors": "Yichun Hu, Nathan Kallus, Xiaojie Mao", "title": "Smooth Contextual Bandits: Bridging the Parametric and\n  Non-differentiable Regret Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a nonparametric contextual bandit problem where the expected reward\nfunctions belong to a H\\\"older class with smoothness parameter $\\beta$. We show\nhow this interpolates between two extremes that were previously studied in\nisolation: non-differentiable bandits ($\\beta\\leq1$), where rate-optimal regret\nis achieved by running separate non-contextual bandits in different context\nregions, and parametric-response bandits (satisfying $\\beta=\\infty$), where\nrate-optimal regret can be achieved with minimal or no exploration due to\ninfinite extrapolatability. We develop a novel algorithm that carefully adjusts\nto all smoothness settings and we prove its regret is rate-optimal by\nestablishing matching upper and lower bounds, recovering the existing results\nat the two extremes. In this sense, our work bridges the gap between the\nexisting literature on parametric and non-differentiable contextual bandit\nproblems and between bandit algorithms that exclusively use global or local\ninformation, shedding light on the crucial interplay of complexity and regret\nin contextual bandits.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:51:14 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 13:12:08 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 14:36:21 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 12:55:27 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Hu", "Yichun", ""], ["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""]]}, {"id": "1909.02556", "submitter": "Steven Finch", "authors": "Steven Finch", "title": "Number of Sign Changes: Segment of AR(1)", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{t}$ denote a stationary first-order autoregressive process. Consider\n$n$ contiguous observations (in time $t$) of the series (e.g., $X_{1}, ...,\nX_{n}$). Let its mean be zero and its lag-one serial correlation be $\\rho$,\nwhich satisfies $|\\rho| < 1$. Rice (1945) proved that $(n-1) \\arccos(\\rho)/\\pi$\nis the expected number of sign changes. A corresponding formula for\nhigher-order moments was proposed by Nyberg, Lizana & Ambj\\\"ornsson (2018),\nbased on an independent interval approximation. We focus on the variance only,\nfor small $n$, and see a promising fit between theory and model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:53:31 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Finch", "Steven", ""]]}, {"id": "1909.02662", "submitter": "Todd Kuffner", "authors": "Todd A. Kuffner, Stephen M.-S. Lee and G. Alastair Young", "title": "Block bootstrap optimality for density estimation with dependent data", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate approximation of the sampling distribution of nonparametric kernel\ndensity estimators is crucial for many statistical inference problems. Since\nthese estimators have complex asymptotic distributions, bootstrap methods are\noften used for this purpose. With i.i.d. observations, a large literature\nexists concerning optimal bootstrap methods which achieve the fastest possible\nconvergence rate of the bootstrap estimator of the sampling distribution of the\nkernel density estimator. With dependent data, such an optimality theory is an\nimportant open problem. We establish a general theory of optimality of the\nblock bootstrap for kernel density estimation under weak dependence assumptions\nwhich are satisfied by many important time series models. We propose a unified\nframework for a theoretical study of a rich class of bootstrap methods which\ninclude as special cases subsampling, Kunsch's moving block bootstrap, Hall's\nunder-smoothing (UNS) as well as approaches incorporating no (NBC) or explicit\nbias correction (EBC). Moreover, we consider their accuracy under a broad\nspectrum of choices of the bandwidth $h$, which include as an important special\ncase the MSE-optimal choice, as well as other under-smoothed choices. Under\neach choice of $h$, we derive the optimal tuning parameters and compare optimal\nperformances between the main subclasses (EBC, NBC, UNS) of the bootstrap\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 22:49:31 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kuffner", "Todd A.", ""], ["Lee", "Stephen M. -S.", ""], ["Young", "G. Alastair", ""]]}, {"id": "1909.02686", "submitter": "Yu-Chih Huang", "authors": "Yu-Chih Huang, Yu-Jui Huang, Shih-Chun Lin", "title": "Asymptotic Optimality in Byzantine Distributed Quickest Change Detection", "comments": "47pages, 3 figures. Part of the results have been presented at the\n  IEEE ISIT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Byzantine distributed quickest change detection (BDQCD) is studied, where\na fusion center monitors the occurrence of an abrupt event through a bunch of\ndistributed sensors that may be compromised. We first consider the binary\nhypothesis case where there is only one post-change hypothesis and prove a\nnovel converse to the first-order asymptotic detection delay in the large mean\ntime to a false alarm regime. This converse is tight in that it coincides with\nthe currently best achievability shown by Fellouris et al.; hence, the optimal\nasymptotic performance of binary BDQCD is characterized. An important\nimplication of this result is that, even with compromised sensors, a 1-bit link\nbetween each sensor and the fusion center suffices to achieve asymptotic\noptimality. To accommodate multiple post-change hypotheses, we then formulate\nthe multi-hypothesis BDQCD problem and again investigate the optimal\nfirst-order performance under different bandwidth constraints. A converse is\nfirst obtained by extending our converse from binary to multi-hypothesis BDQCD.\nTwo families of stopping rules, namely the simultaneous $d$-th alarm and the\nmulti-shot $d$-th alarm, are then proposed. Under sufficient link bandwidth,\nthe simultaneous $d$-th alarm, with $d$ being set to the number of honest\nsensors, can achieve the asymptotic performance that coincides with the derived\nconverse bound; hence, the asymptotically optimal performance of\nmulti-hypothesis BDQCD is again characterized. Moreover, although being shown\nto be asymptotically optimal only for some special cases, the multi-shot $d$-th\nalarm is much more bandwidth-efficient and energy-efficient than the\nsimultaneous $d$-th alarm. Built upon the above success in characterizing the\nasymptotic optimality of the BDQCD, a corresponding leader-follower Stackelberg\ngame is formulated and its solution is found.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 01:22:34 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Huang", "Yu-Chih", ""], ["Huang", "Yu-Jui", ""], ["Lin", "Shih-Chun", ""]]}, {"id": "1909.02739", "submitter": "Giacomo Francisci", "authors": "Giacomo Francisci, Alicia Nieto-Reyes and Claudio Agostinelli", "title": "Generalization of the simplicial depth: no vanishment outside the convex\n  hull of the distribution support", "comments": "24+31 pages, 7+5 figures, 0+11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplicial depth, like other relevant multivariate statistical data depth\nfunctions, vanishes right outside the convex hull of the support of the\ndistribution with respect to which the depth is computed. This is problematic\nwhen it is required to differentiate among points outside the convex hull of\nthe distribution support, with respect to which the depth is computed, based on\ntheir depth values. We provide the first two proposals to overcome this issue\nas well as several corresponding estimators, which do not vanish right outside\nthe convex hull of the data. The properties of the proposals and of the\ncorresponding estimators are studied, theoretically and by means of Monte Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:06:34 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 10:21:19 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Francisci", "Giacomo", ""], ["Nieto-Reyes", "Alicia", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "1909.02876", "submitter": "Nabil Kahale", "authors": "Nabil Kahale", "title": "Optimal unbiased estimators via convex hulls", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Necessary and sufficient conditions for the square-integrability of recently\nproposed unbiased estimators are established. A geometric characterization of a\ndistribution that optimizes the performance of these estimators is given. An\nalgorithm based on convex hulls that finds the optimal distribution truncated\nto its first m terms in time linear in m is described. The algorithm exploits a\nconnection with a recent randomized dimension reduction method and is\nillustrated via a numerical example.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:53:35 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kahale", "Nabil", ""]]}, {"id": "1909.02900", "submitter": "Yann Issartel", "authors": "Yann Issartel", "title": "On the Estimation of Network Complexity: Dimension of Graphons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network complexity has been studied for over half a century and has found a\nwide range of applications. Many methods have been developed to characterize\nand estimate the complexity of networks. However, there has been little\nresearch with statistical guarantees. In this paper, we develop a statistical\ntheory of graph complexity in a general model of random graphs, the so-called\ngraphon model.\n  Given a graphon, we endow the latent space of the nodes with the neighborhood\ndistance that measures the propensity of two nodes to be connected with similar\nnodes. Our complexity index is then based on the covering number and the\nMinkowski dimension of (a purified version of) this metric space. Although the\nlatent space is not identifiable, these indices turn out to be identifiable.\nThis notion of complexity has simple interpretations on popular examples of\nrandom graphs: it matches the number of communities in stochastic block models;\nthe dimension of the Euclidean space in random geometric graphs; the regularity\nof the link function in H\\\"older graphon models.\n  From a single observation of the graph, we construct an estimator of the\nneighborhood-distance and show universal non-asymptotic bounds for its risk,\nmatching minimax lower bounds. Based on this estimated distance, we compute the\ncorresponding covering number and Minkowski dimension and we provide optimal\nnon-asymptotic error bounds for these two plug-in estimators.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:35:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 00:52:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Issartel", "Yann", ""]]}, {"id": "1909.02929", "submitter": "Paolo Gorgi", "authors": "Paolo Gorgi", "title": "BNB autoregressions for modeling integer-valued time series with extreme\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a general class of heavy-tailed autoregressions for\nmodeling integer-valued time series with outliers. The proposed specification\nis based on a heavy-tailed mixture of negative binomial distributions that\nfeatures an observation-driven dynamic equation for the conditional\nexpectation. The existence of a unique stationary and ergodic solution for the\nclass of autoregressive processes is shown under a general contraction\ncondition. The estimation of the model can be easily performed by Maximum\nLikelihood given the closed form of the likelihood function. The strong\nconsistency and the asymptotic normality of the estimator are formally derived.\nTwo examples of specifications illustrate the flexibility of the approach and\nthe relevance of the theoretical results. In particular, a linear dynamic\nequation and a score-driven equation for the conditional expectation are\nconsidered. The score-driven specification is shown to be particularly\nappealing as it delivers a robust filtering method that attenuates the impact\nof outliers. An empirical application to the time series of narcotics\ntrafficking reports in Sydney illustrates the effectiveness of the method in\nhandling extreme observations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:27:51 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gorgi", "Paolo", ""]]}, {"id": "1909.02998", "submitter": "Tino Werner", "authors": "Tino Werner", "title": "A review on ranking problems in statistical learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking problems, also known as preference learning problems, define a widely\nspread class of statistical learning problems with many applications, including\nfraud detection, document ranking, medicine, credit risk screening, image\nranking or media memorability. In this article, we systematically review\ndifferent types of instance ranking problems, i.e., ranking problems that\nrequire the prediction of an order of the response variables, and the\ncorresponding loss functions resp. goodness criteria. We discuss the\ndifficulties when trying to optimize those criteria. As for a detailed and\ncomprehensive overview of existing machine learning techniques to solve such\nranking problems, we systemize existing techniques and recapitulate the\ncorresponding optimization problems in a unified notation. We also discuss to\nwhich of the ranking problems the respective algorithms are tailored and\nidentify their strengths and limitations. Computational aspects and open\nresearch problems are also considered.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:24:23 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 13:30:41 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 14:11:07 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Werner", "Tino", ""]]}, {"id": "1909.03214", "submitter": "Xuan Xie", "authors": "Xuan Xie, Junhao Yu, Hui Feng, Bo Hu", "title": "Bayesian Design of Sampling Set for Bandlimited Graph Signals", "comments": "Accepted by GloalSIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of sampling set (DoS) for bandlimited graph signals (GS) has been\nextensively studied in recent years, but few of them exploit the benefits of\nthe stochastic prior of GS. In this work, we introduce the optimization\nframework for Bayesian DoS of bandlimited GS. We also illustrate how the choice\nof different sampling sets affects the estimation error and how the prior\nknowledge influences the result of DoS compared with the non-Bayesian DoS by\nthe aid of analyzing Gershgorin discs of error metric matrix. Finally, based on\nour analysis, we propose a heuristic algorithm for DoS to avoid solving the\noptimization problem directly.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 08:24:08 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Xie", "Xuan", ""], ["Yu", "Junhao", ""], ["Feng", "Hui", ""], ["Hu", "Bo", ""]]}, {"id": "1909.03217", "submitter": "Kay Bogerd", "authors": "Kay Bogerd, Rui M. Castro, Remco van der Hofstad and Nicolas Verzelen", "title": "Detecting a planted community in an inhomogeneous random graph", "comments": "44 pages", "journal-ref": null, "doi": "10.3150/20-BEJ1269", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting whether an inhomogeneous random graph\ncontains a planted community. Specifically, we observe a single realization of\na graph. Under the null hypothesis, this graph is a sample from an\ninhomogeneous random graph, whereas under the alternative, there exists a small\nsubgraph where the edge probabilities are increased by a multiplicative scaling\nfactor. We present a scan test that is able to detect the presence of such a\nplanted community, even when this community is very small and the underlying\ngraph is inhomogeneous. We also derive an information theoretic lower bound for\nthis problem which shows that in some regimes the scan test is almost\nasymptotically optimal. We illustrate our results through examples and\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 08:33:34 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 16:31:32 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bogerd", "Kay", ""], ["Castro", "Rui M.", ""], ["van der Hofstad", "Remco", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1909.03302", "submitter": "Tong Li", "authors": "Tong Li and Ming Yuan", "title": "On the Optimality of Gaussian Kernel Based Nonparametric Tests against\n  Smooth Alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests via kernel embedding of distributions have witnessed a\ngreat deal of practical successes in recent years. However, statistical\nproperties of these tests are largely unknown beyond consistency against a\nfixed alternative. To fill in this void, we study here the asymptotic\nproperties of goodness-of-fit, homogeneity and independence tests using\nGaussian kernels, arguably the most popular and successful among such tests.\nOur results provide theoretical justifications for this common practice by\nshowing that tests using Gaussian kernel with an appropriately chosen scaling\nparameter are minimax optimal against smooth alternatives in all three\nsettings. In addition, our analysis also pinpoints the importance of choosing a\ndiverging scaling parameter when using Gaussian kernels and suggests a\ndata-driven choice of the scaling parameter that yields tests optimal, up to an\niterated logarithmic factor, over a wide range of smooth alternatives.\nNumerical experiments are also presented to further demonstrate the practical\nmerits of the methodology.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 16:43:20 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Tong", ""], ["Yuan", "Ming", ""]]}, {"id": "1909.03347", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini and Zahra S. Razaee", "title": "Concentration of kernel matrices with application to kernel spectral\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the concentration of random kernel matrices around their mean. We\nderive nonasymptotic exponential concentration inequalities for Lipschitz\nkernels assuming that the data points are independent draws from a class of\nmultivariate distributions on $\\mathbb R^d$, including the strongly log-concave\ndistributions under affine transformations. A feature of our result is that the\ndata points need not have identical distributions or zero mean, which is key in\ncertain applications such as clustering. Our bound for the Lipschitz kernels is\ndimension-free and sharp up to constants. For comparison, we also derive the\ncompanion result for the Euclidean (inner product) kernel for a class of\nsub-Gaussian distributions. A notable difference between the two cases is that,\nin contrast to the Euclidean kernel, in the Lipschitz case, the concentration\ninequality does not depend on the mean of the underlying vectors. As an\napplication of these inequalities, we derive a bound on the misclassification\nrate of a kernel spectral clustering (KSC) algorithm, under a perturbed\nnonparametric mixture model. We show an example where this bound establishes\nthe high-dimensional consistency (as $d \\to \\infty$) of the KSC, when applied\nwith a Gaussian kernel, to a noisy model of nested nonlinear manifolds.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 22:56:55 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 08:25:48 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Amini", "Arash A.", ""], ["Razaee", "Zahra S.", ""]]}, {"id": "1909.03488", "submitter": "Adam Brown", "authors": "Adam Brown, Omer Bobrowski, Elizabeth Munch, Bei Wang", "title": "Probabilistic Convergence and Stability of Random Mapper Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the probabilistic convergence between the mapper graph and the Reeb\ngraph of a topological space $\\mathbb{X}$ equipped with a continuous function\n$f: \\mathbb{X} \\rightarrow \\mathbb{R}$. We first give a categorification of the\nmapper graph and the Reeb graph by interpreting them in terms of cosheaves and\nstratified covers of the real line $\\mathbb{R}$. We then introduce a variant of\nthe classic mapper graph of Singh et al.~(2007), referred to as the enhanced\nmapper graph, and demonstrate that such a construction approximates the Reeb\ngraph of $(\\mathbb{X}, f)$ when it is applied to points randomly sampled from a\nprobability density function concentrated on $(\\mathbb{X}, f)$.\n  Our techniques are based on the interleaving distance of constructible\ncosheaves and topological estimation via kernel density estimates. Following\nMunch and Wang (2018), we first show that the mapper graph of $(\\mathbb{X},\nf)$, a constructible $\\mathbb{R}$-space (with a fixed open cover), approximates\nthe Reeb graph of the same space. We then construct an isomorphism between the\nmapper of $(\\mathbb{X},f)$ to the mapper of a super-level set of a probability\ndensity function concentrated on $(\\mathbb{X}, f)$. Finally, building on the\napproach of Bobrowski et al.~(2017), we show that, with high probability, we\ncan recover the mapper of the super-level set given a sufficiently large\nsample. Our work is the first to consider the mapper construction using the\ntheory of cosheaves in a probabilistic setting. It is part of an ongoing effort\nto combine sheaf theory, probability, and statistics, to support topological\ndata analysis with random data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 16:02:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 21:33:58 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 15:13:28 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Brown", "Adam", ""], ["Bobrowski", "Omer", ""], ["Munch", "Elizabeth", ""], ["Wang", "Bei", ""]]}, {"id": "1909.03530", "submitter": "Quan Zhou", "authors": "Shige Peng, Quan Zhou", "title": "A hypothesis-testing perspective on the G-normal distribution theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The G-normal distribution was introduced by Peng [2007] as the limiting\ndistribution in the central limit theorem for sublinear expectation spaces.\nEquivalently, it can be interpreted as the solution to a stochastic control\nproblem where we have a sequence of random variables, whose variances can be\nchosen based on all past information. In this note we study the tail behavior\nof the G-normal distribution through analyzing a nonlinear heat equation.\nAsymptotic results are provided so that the tail \"probabilities\" can be easily\nevaluated with high accuracy. This study also has a significant impact on the\nhypothesis testing theory for heteroscedastic data; we show that even if the\ndata are generated under the null hypothesis, it is possible to cheat and\nattain statistical significance by sequentially manipulating the error\nvariances of the observations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 19:03:46 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Peng", "Shige", ""], ["Zhou", "Quan", ""]]}, {"id": "1909.03540", "submitter": "Hamid Eftekhari", "authors": "Hamid Eftekhari, Moulinath Banerjee, Ya'acov Ritov", "title": "Inference In High-dimensional Single-Index Models Under Symmetric\n  Designs", "comments": "The current version was published in the Journal of Machine Learning\n  Research, Volume 22. It is available online at\n  https://jmlr.org/papers/v22/19-744.html", "journal-ref": "Journal of Machine Learning Research, 22(27), 1-63 (2021)", "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of statistical inference for regression coefficients in a\nhigh-dimensional single-index model is considered. Under elliptical symmetry,\nthe single index model can be reformulated as a proxy linear model whose\nregression parameter is identifiable. We construct estimates of the regression\ncoefficients of interest that are similar to the debiased lasso estimates in\nthe standard linear model and exhibit similar properties: root-n-consistency\nand asymptotic normality. The procedure completely bypasses the estimation of\nthe unknown link function, which can be extremely challenging depending on the\nunderlying structure of the problem. Furthermore, under Gaussianity, we propose\nmore efficient estimates of the coefficients by expanding the link function in\nthe Hermite polynomial basis. Finally, we illustrate our approach via carefully\ndesigned simulation experiments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:22:08 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 20:44:08 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 18:39:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Eftekhari", "Hamid", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1909.03651", "submitter": "Sricharan Shah", "authors": "Sricharan Shah and Partha Jyoti Hazarika", "title": "The Alpha-Beta-Skew-Logistic Distribution And Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, an alpha-beta-skew-logistic distribution is proposed following\nthe same methodology as those of alpha-beta-skew-normal of Shafiei et al.\n(2016) and investigated some of its related distributional properties. Finally,\nthe validity of our proposed distribution has tested by considering three real\nlife applications and comparing the values of Akaike Information Criterion\n(AIC) and Bayesian Information Criterion (BIC) with the values of some other\nrelated distributions. Likelihood ratio test is used for discriminating between\nlogistic and the proposed distributions.\n  Keywords: Skew Distributions, Alpha-Skew Distributions, Bimodal Distributions\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:32:23 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shah", "Sricharan", ""], ["Hazarika", "Partha Jyoti", ""]]}, {"id": "1909.03725", "submitter": "Alexander Henzi", "authors": "Alexander Henzi and Johanna F. Ziegel and Tilmann Gneiting", "title": "Isotonic Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic distributional regression (IDR) is a powerful nonparametric\ntechnique for the estimation of conditional distributions under order\nrestrictions. In a nutshell, IDR learns conditional distributions that are\ncalibrated, and simultaneously optimal relative to comprehensive classes of\nrelevant loss functions, subject to isotonicity constraints in terms of a\npartial order on the covariate space. Nonparametric isotonic quantile\nregression and nonparametric isotonic binary regression emerge as special\ncases. For prediction, we propose an interpolation method that generalizes\nextant specifications under the pool adjacent violators algorithm. We recommend\nthe use of IDR as a generic benchmark technique in probabilistic forecast\nproblems, as it does not involve any parameter tuning nor implementation\nchoices, except for the selection of a partial order on the covariate space.\nThe method can be combined with subsample aggregation, with the benefits of\nsmoother regression functions and gains in computational efficiency. In a\nsimulation study, we compare methods for distributional regression in terms of\nthe continuous ranked probability score (CRPS) and $L_2$ estimation error,\nwhich are closely linked. In a case study on raw and postprocessed quantitative\nprecipitation forecasts from a leading numerical weather prediction system, IDR\nis competitive with state of the art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:43:07 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 10:34:50 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Henzi", "Alexander", ""], ["Ziegel", "Johanna F.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1909.03763", "submitter": "Fritjof Freise", "authors": "Fritjof Freise and Norbert Gaffke and Rainer Schwabe", "title": "Convergence of least squares estimators in the adaptive Wynn algorithm\n  for a class of nonlinear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper continues the authors' work on the adaptive Wynn algorithm in a\nnonlinear regression model. In the present paper it is shown that if the mean\nresponse function satisfies a condition of `saturated identifiability', which\nwas introduced by Pronzato \\cite{Pronzato}, then the adaptive least squares\nestimators are strongly consistent. The condition states that the regression\nparameter is identifiable under any saturated design, i.e., the values of the\nmean response function at any $p$ distinct design points determine the\nparameter point uniquely where, typically, $p$ is the dimension of the\nregression parameter vector. Further essential assumptions are compactness of\nthe experimental region and of the parameter space together with some natural\ncontinuity assumptions. If the true parameter point is an interior point of the\nparameter space then under some smoothness assumptions and asymptotic\nhomoscedasticity of random errors the asymptotic normality of adaptive least\nsquares estimators is obtained.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 11:12:49 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Freise", "Fritjof", ""], ["Gaffke", "Norbert", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1909.04325", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno and Claudio Agostinelli", "title": "Robust Multivariate Estimation Based On Statistical Depth Filters", "comments": "25 pages, 11 figures. TEST (2021)", "journal-ref": null, "doi": "10.1007/s11749-021-00757-z", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical contamination models, such as the gross-error (Huber and\nTukey contamination model or Case-wise Contamination), observations are\nconsidered as the units to be identified as outliers or not. This model is very\nuseful when the number of considered variables is moderately small. Alqallaf et\nal. [2009] shows the limits of this approach for a larger number of variables\nand introduced the Independent contamination model (Cell-wise Contamination)\nwhere now the cells are the units to be identified as outliers or not. One\napproach to deal, at the same time, with both type of contamination is filter\nout the contaminated cells from the data set and then apply a robust procedure\nable to handle case-wise outliers and missing values. Here we develop a general\nframework to build filters in any dimension based on statistical data depth\nfunctions. We show that previous approaches, e.g. Agostinelli et al. [2015a]\nand Leung et al. [2017], are special cases. We illustrate our method by using\nthe half-space depth.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 07:11:50 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 19:04:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "1909.04357", "submitter": "Kammoun Abla", "authors": "Zhedong Liu and Abla Kammoun and Mohamed Slim Alouini", "title": "On Robust Spectrum Sensing Using M-estimators of Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the spectrum sensing in cognitive radio networks\nwhen the impulsive noise appears. We propose a class of blind and robust\ndetectors using M-estimators in eigenvalue based spectrum sensing method. The\nconventional eigenvalue based method uses statistics derived from the\neigenvalues of sample covariance matrix(SCM) as testing statistics, which are\ninefficient and unstable in the impulsive noise environment. Instead of SCM, we\ncan use M-estimators, which have good performance under both impulsive and\nnon-impulsive noise. Among those M-estimators, We recommend the Tyler's\nM-estimator instead, which requires no knowledge of noise distribution and have\nthe same probability of false alarm under different complex elliptically\nsymmetric distributions. In addition, it performs better than the detector\nusing sample covariance matrix when the noise is highly impulsive. It should be\nemphasized that this detector does not require knowledge of noise power which\nis required by the energy detection based methods. Simulations show that it\nperforms better than conventional detector using sample covariance matrix in a\nhighly impulsive noise environment.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 08:58:32 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Liu", "Zhedong", ""], ["Kammoun", "Abla", ""], ["Alouini", "Mohamed Slim", ""]]}, {"id": "1909.04661", "submitter": "Jean-Michel Zakoian", "authors": "Christian Francq and Jean-Michel Zakoian", "title": "Virtual Historical Simulation for estimating the conditional VaR of\n  large portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to estimate the conditional risk of a portfolio's return, two\nstrategies can be advocated. A multivariate strategy requires estimating a\ndynamic model for the vector of risk factors, which is often challenging, when\nat all possible, for large portfolios. A univariate approach based on a dynamic\nmodel for the portfolio's return seems more attractive. However, when the\ncombination of the individual returns is time varying, the portfolio's return\nseries is typically non stationary which may invalidate statistical inference.\nAn alternative approach consists in reconstituting a \"virtual portfolio\", whose\nreturns are built using the current composition of the portfolio and for which\na stationary dynamic model can be estimated.\n  This paper establishes the asymptotic properties of this method, that we call\nVirtual Historical Simulation. Numerical illustrations on simulated and real\ndata are provided.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:01:19 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Francq", "Christian", ""], ["Zakoian", "Jean-Michel", ""]]}, {"id": "1909.04767", "submitter": "Tommaso Lando", "authors": "Tommaso Lando, Lucio Bertoli-Barsotti", "title": "Distorted stochastic dominance: a generalized family of stochastic\n  orders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalized family of stochastic orders, semiparametrized by a\ndistortion function H, namely H-distorted stochastic dominance, which may\ndetermine a continuum of dominance relations from the first- to the\nsecond-order stochastic dominance (and beyond). Such a family is especially\nsuitable for representing a decision maker's preferences in terms of risk\naversion and may be used in those situations in which a strong order does not\nhave enough discriminative power, whilst a weaker one is poorly representative\nof some classes of decision makers. In particular, we focus on the class of\npower distortion functions, yielding power-distorted stochastic dominance,\nwhich seems to be particularly appealing owing to its computational simplicity\nand some interesting statistical interpretations. Finally, we characterize\ndistorted stochastic dominance in terms of distortion functions yielding\nisotonic classes of distorted expectations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:39:01 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Lando", "Tommaso", ""], ["Bertoli-Barsotti", "Lucio", ""]]}, {"id": "1909.04798", "submitter": "Lihua Lei", "authors": "Lihua Lei", "title": "Unified $\\ell_{2\\rightarrow\\infty}$ Eigenspace Perturbation Theory for\n  Symmetric Random Matrices", "comments": "100 pages; typos corrected; Remark 2.3 added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications in statistics, computer science and network science have\nseen tremendous values of finer matrix spectral perturbation theory. In this\npaper, we derive a generic $\\ell_{2\\rightarrow\\infty}$ eigenspace perturbation\nbound for symmetric random matrices, with independent or dependent entries and\nfairly flexible entry distributions. In particular, we apply our generic bound\nto binary random matrices with independent entries or with certain dependency\nstructures, including the unnormalized Laplacian of inhomogenous random graphs\nand $m$-dependent matrices. Through a detailed comparison, we found that for\nbinary random matrices with independent entries, our\n$\\ell_{2\\rightarrow\\infty}$ bound is tighter than all existing bounds that we\nare aware of, while our condition is weaker than all but one of them in a less\ncommon regime. We apply our perturbation bounds in three problems and improve\nthe state of the art: concentration of the spectral norm of sparse random\ngraphs, exact recovery of communities in stochastic block models and partial\nconsistency of divisive hierarchical clustering. Finally we discuss the\nextensions of our theory to random matrices with more complex dependency\nstructures and non-binary entries, asymmetric rectangular matrices and induced\nperturbation theory in other metrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 00:21:49 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:26:44 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lei", "Lihua", ""]]}, {"id": "1909.04853", "submitter": "Jose Figueroa-Lopez", "authors": "Qi Wang, Jos\\'e E. Figueroa-L\\'opez, and Todd Kuffner", "title": "Bayesian Inference on Volatility in the Presence of Infinite Jump\n  Activity and Microstructure Noise", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility estimation based on high-frequency data is key to accurately\nmeasure and control the risk of financial assets. A L\\'{e}vy process with\ninfinite jump activity and microstructure noise is considered one of the\nsimplest, yet accurate enough, models for financial data at high-frequency.\nUtilizing this model, we propose a \"purposely misspecified\" posterior of the\nvolatility obtained by ignoring the jump-component of the process. The\nmisspecified posterior is further corrected by a simple estimate of the\nlocation shift and re-scaling of the log likelihood. Our main result\nestablishes a Bernstein-von Mises (BvM) theorem, which states that the proposed\nadjusted posterior is asymptotically Gaussian, centered at a consistent\nestimator, and with variance equal to the inverse of the Fisher information. In\nthe absence of microstructure noise, our approach can be extended to inferences\nof the integrated variance of a general It\\^o semimartingale. Simulations are\nprovided to demonstrate the accuracy of the resulting credible intervals, and\nthe frequentist properties of the approximate Bayesian inference based on the\nadjusted posterior.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:06:47 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Wang", "Qi", ""], ["Figueroa-L\u00f3pez", "Jos\u00e9 E.", ""], ["Kuffner", "Todd", ""]]}, {"id": "1909.04890", "submitter": "Guillaume Maillard", "authors": "Guillaume Maillard (LMO), Sylvain Arlot (LM-Orsay), Matthieu Lerasle\n  (LM-Orsay)", "title": "Aggregated Hold-Out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated hold-out (Agghoo) is a method which averages learning rules\nselected by hold-out (that is, cross-validation with a single split). We\nprovide the first theoretical guarantees on Agghoo, ensuring that it can be\nused safely: Agghoo performs at worst like the hold-out when the risk is\nconvex. The same holds true in classification with the 0-1 risk, with an\nadditional constant factor. For the hold-out, oracle inequalities are known for\nbounded losses, as in binary classification. We show that similar results can\nbe proved, under appropriate assumptions, for other risk-minimization problems.\nIn particular, we obtain an oracle inequality for regularized kernel regression\nwith a Lip-schitz loss, without requiring that the Y variable or the regressors\nbe bounded. Numerical experiments show that aggregation brings a significant\nimprovement over the hold-out and that Agghoo is competitive with\ncross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 07:46:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Maillard", "Guillaume", "", "LMO"], ["Arlot", "Sylvain", "", "LM-Orsay"], ["Lerasle", "Matthieu", "", "LM-Orsay"]]}, {"id": "1909.05007", "submitter": "Daron Anderson", "authors": "Daron Anderson, Douglas Leith", "title": "Optimality of the Subgradient Algorithm in the Stochastic Setting", "comments": "6 figures, Corrected off-by-one errors coming from proof in Appendix\n  A. Replaced with newer Version April 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG cs.SY eess.SY math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Subgradient algorithm is universal for online learning on\nthe simplex in the sense that it simultaneously achieves $O(\\sqrt N)$ regret\nfor adversarial costs and $O(1)$ pseudo-regret for i.i.d costs. To the best of\nour knowledge this is the first demonstration of a universal algorithm on the\nsimplex that is not a variant of Hedge. Since Subgradient is a popular and\nwidely used algorithm our results have immediate broad application.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 12:44:30 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 14:33:20 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:22:22 GMT"}, {"version": "v4", "created": "Wed, 30 Oct 2019 12:02:47 GMT"}, {"version": "v5", "created": "Thu, 31 Oct 2019 12:50:45 GMT"}, {"version": "v6", "created": "Fri, 3 Apr 2020 17:04:11 GMT"}, {"version": "v7", "created": "Fri, 27 Nov 2020 14:31:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anderson", "Daron", ""], ["Leith", "Douglas", ""]]}, {"id": "1909.05117", "submitter": "Minerva Mukhopadhyay", "authors": "Minerva Mukhopadhyay and David B. Dunson", "title": "Targeted Random Projection for Prediction from High-Dimensional Features", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.02445", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computationally-efficient prediction with high\ndimensional and highly correlated predictors when accurate variable selection\nis effectively impossible. Direct application of penalization or Bayesian\nmethods implemented with Markov chain Monte Carlo can be computationally\ndaunting and unstable. A common solution is first stage dimension reduction\nthrough screening or projecting the design matrix to a lower dimensional\nhyper-plane. Screening is highly sensitive to threshold choice, while\nprojections often have poor performance in very high-dimensions. We propose\nTArgeted Random Projection (TARP) to combine positive aspects of both\nstrategies. TARP uses screening to order the inclusion probabilities of the\nfeatures in the projection matrix used for dimension reduction, leading to\ndata-informed sparsity. We provide theoretical support for a Bayesian\npredictive algorithm based on TARP, including statistical and computational\ncomplexity guarantees. Examples for simulated and real data applications\nillustrate gains relative to a variety of competitors.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:45:39 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Dunson", "David B.", ""]]}, {"id": "1909.05229", "submitter": "Rui Zhang", "authors": "Alexander Shapiro, Yao Xie, Rui Zhang", "title": "Goodness-of-fit tests on manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general theory for the goodness-of-fit test to non-linear\nmodels. In particular, we assume that the observations are noisy samples of a\nsubmanifold defined by a \\yao{sufficiently smooth non-linear map}. The\nobservation noise is additive Gaussian. Our main result shows that the\n\"residual\" of the model fit, by solving a non-linear least-square problem,\nfollows a (possibly noncentral) $\\chi^2$ distribution. The parameters of the\n$\\chi^2$ distribution are related to the model order and dimension of the\nproblem. We further present a method to select the model orders sequentially.\nWe demonstrate the broad application of the general theory in machine learning\nand signal processing, including determining the rank of low-rank (possibly\ncomplex-valued) matrices and tensors from noisy, partial, or indirect\nobservations, determining the number of sources in signal demixing, and\npotential applications in determining the number of hidden nodes in neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:38:25 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 21:32:24 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Shapiro", "Alexander", ""], ["Xie", "Yao", ""], ["Zhang", "Rui", ""]]}, {"id": "1909.05244", "submitter": "Liyang Sun", "authors": "Rahul Singh and Liyang Sun", "title": "De-biased Machine Learning in Instrumental Variable Models for Treatment\n  Effects", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a de-biased machine learning (DML) approach to estimating\ncomplier parameters with high-dimensional data. Complier parameters include\nlocal average treatment effect, average complier characteristics, and complier\ncounterfactual outcome distributions. In our approach, the de-biasing is itself\nperformed by machine learning, a variant called automatic de-biased machine\nlearning (Auto-DML). By regularizing the balancing weights, it does not require\nad hoc trimming or censoring. We prove our estimator is consistent,\nasymptotically normal, and semi-parametrically efficient. We use the new\napproach to estimate the effect of 401(k) participation on the distribution of\nnet financial assets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:08:54 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:41:14 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 05:06:29 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Singh", "Rahul", ""], ["Sun", "Liyang", ""]]}, {"id": "1909.05418", "submitter": "Eric Strobl", "authors": "Eric V. Strobl", "title": "The Global Markov Property for a Mixture of DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real causal processes may contain feedback loops and change over time. In\nthis paper, we model cycles and non-stationary distributions using a mixture of\ndirected acyclic graphs (DAGs). We then study the conditional independence (CI)\nrelations induced by a density that factorizes according to a mixture of DAGs\nin two steps. First, we generalize d-separation for a single DAG to mixture\nd-separation for a mixture of DAGs. We then utilize the mixture d-separation\ncriterion to derive a global Markov property that allows us to read off the CI\nrelations induced by a mixture of DAGs using a particular summary graph. This\nresult has potentially far reaching applications in algorithm design for causal\ndiscovery.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 00:45:51 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 01:47:21 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Strobl", "Eric V.", ""]]}, {"id": "1909.05420", "submitter": "Niushan Gao", "authors": "Niushan Gao, Alexandra Kirillova, Zihao Tong", "title": "A refined determinantal inequality for correlation matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Olkin [3] obtained a neat upper bound for the determinant of a correlation\nmatrix. In this note, we present an extension and improvement of his result.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 00:59:28 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gao", "Niushan", ""], ["Kirillova", "Alexandra", ""], ["Tong", "Zihao", ""]]}, {"id": "1909.05433", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Emmanuel J. Cand\\`es", "title": "A comparison of some conformal quantile regression methods", "comments": "20 pages, 9 figures, 3 tables", "journal-ref": "Stat. 2020; 9:e261", "doi": "10.1002/sta4.261", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two recently proposed methods that combine ideas from conformal\ninference and quantile regression to produce locally adaptive and marginally\nvalid prediction intervals under sample exchangeability (Romano et al., 2019;\nKivaranovic et al., 2019). First, we prove that these two approaches are\nasymptotically efficient in large samples, under some additional assumptions.\nThen we compare them empirically on simulated and real data. Our results\ndemonstrate that the method in Romano et al. (2019) typically yields tighter\nprediction intervals in finite samples. Finally, we discuss how to tune these\nprocedures by fixing the relative proportions of observations used for training\nand conformalization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 01:48:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sesia", "Matteo", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1909.05481", "submitter": "Aurelie Muller-Gueudin", "authors": "B\\'erang\\`ere Bastien, Taha Boukhobza (CRAN), H\\'el\\`ene Dumond\n  (CRAN), Anne G\\'egout-Petit (BIGS, IECL), Aur\\'elie Muller-Gueudin (BIGS,\n  IECL), Charl\\`ene Thi\\'ebaut (CRAN)", "title": "A statistical methodology to select covariates in high-dimensional data\n  under dependence. Application to the classification of genetic profiles in\n  oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology for selecting and ranking covariates associated\nwith a variable of interest in a context of high-dimensional data under\ndependence but few observations. The methodology successively intertwines the\nclustering of covariates, decorrelation of covariates using Factor Latent\nAnalysis, selection using aggregation of adapted methods and finally ranking.\nSimulations study shows the interest of the decorrelation inside the different\nclusters of covariates. We first apply our method to transcriptomic data of 37\npatients with advanced non-small-cell lung cancer who have received\nchemotherapy, to select the transcriptomic covariates that explain the survival\noutcome of the treatment. Secondly, we apply our method to 79 breast tumor\nsamples to define patient profiles for a new metastatic biomarker and\nassociated gene network in order to personalize the treatments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:45:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bastien", "B\u00e9rang\u00e8re", "", "CRAN"], ["Boukhobza", "Taha", "", "CRAN"], ["Dumond", "H\u00e9l\u00e8ne", "", "CRAN"], ["G\u00e9gout-Petit", "Anne", "", "BIGS, IECL"], ["Muller-Gueudin", "Aur\u00e9lie", "", "BIGS,\n  IECL"], ["Thi\u00e9baut", "Charl\u00e8ne", "", "CRAN"]]}, {"id": "1909.05495", "submitter": "Mona Azadkia", "authors": "Mona Azadkia", "title": "Optimal choice of $k$ for $k$-nearest neighbor regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-nearest neighbor algorithm ($k$-NN) is a widely used non-parametric\nmethod for classification and regression. We study the mean squared error of\nthe $k$-NN estimator when $k$ is chosen by leave-one-out cross-validation\n(LOOCV). Although it was known that this choice of $k$ is asymptotically\nconsistent, it was not known previously that it is an optimal $k$. We show,\nwith high probability, the mean squared error of this estimator is close to the\nminimum mean squared error using the $k$-NN estimate, where the minimum is over\nall choices of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:01:20 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 17:19:16 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 15:15:11 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 04:08:22 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Azadkia", "Mona", ""]]}, {"id": "1909.05570", "submitter": "Marguerite Zani", "authors": "Thi Truong (IDP), Marguerite Zani (IDP)", "title": "Sharp Large Deviations for empirical correlation coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Sharp Large Deviations for Pearson's empirical correlation\ncoefficients in the Spherical and Gaussian cases\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:19:04 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Truong", "Thi", "", "IDP"], ["Zani", "Marguerite", "", "IDP"]]}, {"id": "1909.05582", "submitter": "Michael Brand", "authors": "Michael Brand, Thomas Hendrey", "title": "A taxonomy of estimator consistency on discrete estimation problems", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a four-level hierarchy mapping both all discrete estimation\nproblems and all estimators on these problems, such that the hierarchy\ndescribes each estimator's consistency guarantees on each problem class. We\nshow that no estimator is consistent for all estimation problems, but that some\nestimators, such as Maximum A Posteriori, are consistent for the widest\npossible class of discrete estimation problems. For Maximum Likelihood and\nApproximate Maximum Likelihood estimators we show that they do not provide\nconsistency on as wide a class, but define a sub-class of problems\ncharacterised by their consistency. Lastly, we show that some popular\nestimators, specifically Strict Minimum Message Length, do not provide\nconsistency guarantees even within the sub-class.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:49:43 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Brand", "Michael", ""], ["Hendrey", "Thomas", ""]]}, {"id": "1909.05892", "submitter": "Sen Na", "authors": "Sen Na, Mladen Kolar, Oluwasanmi Koyejo", "title": "Estimating Differential Latent Variable Graphical Models with\n  Applications to Brain Connectivity", "comments": "60 pages", "journal-ref": "Biometrika 2020", "doi": "10.1093/biomet/asaa066", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential graphical models are designed to represent the difference\nbetween the conditional dependence structures of two groups, thus are of\nparticular interest for scientific investigation. Motivated by modern\napplications, this manuscript considers an extended setting where each group is\ngenerated by a latent variable Gaussian graphical model. Due to the existence\nof latent factors, the differential network is decomposed into sparse and\nlow-rank components, both of which are symmetric indefinite matrices. We\nestimate these two components simultaneously using a two-stage procedure: (i)\nan initialization stage, which computes a simple, consistent estimator, and\n(ii) a convergence stage, implemented using a projected alternating gradient\ndescent algorithm applied to a nonconvex objective, initialized using the\noutput of the first stage. We prove that given the initialization, the\nestimator converges linearly with a nontrivial, minimax optimal statistical\nerror. Experiments on synthetic and real data illustrate that the proposed\nnonconvex procedure outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:12:46 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:58:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Na", "Sen", ""], ["Kolar", "Mladen", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1909.05903", "submitter": "Xiaoou Li", "authors": "Yunxiao Chen, Xiaoou Li", "title": "Compound Sequential Change-point Detection in Parallel Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential change-point detection in parallel data streams, where\neach stream has its own change point. Once a change is detected in a data\nstream, this stream is deactivated permanently. The goal is to maximize the\nnormal operation of the pre-change streams, while controlling the proportion of\npost-change streams among the active streams at all time points. Taking a\nBayesian formulation, we develop a compound decision framework for this\nproblem. A procedure is proposed that is uniformly optimal among all sequential\nprocedures which control the expected proportion of postchange streams at all\ntime points. We also investigate the asymptotic behavior of the proposed method\nwhen the number of data streams grows large. Numerical examples are provided to\nillustrate the use and performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:48:21 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:08:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "1909.06083", "submitter": "Israel Mart\\'inez Hern\\'andez", "authors": "Israel Mart\\'inez-Hern\\'andez and Marc G. Genton", "title": "Generalized Records for Functional Time Series with Application to Unit\n  Root Tests", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generalization of the definition of records to functional data is proposed.\nThe definition is based on ranking curves using a notion of functional depth.\nThis approach allows us to study the curves of the number of records over time.\nWe focus on functional time series and apply ideas from univariate time series\nto demonstrate the asymptotic distribution describing the number of records. A\nunit root test is proposed as an application of functional record theory.\nThrough a Monte Carlo study, different scenarios of functional processes are\nsimulated to evaluate the performance of the unit root test. The generalized\nrecord definition is applied on two different datasets: Annual mortality rates\nin France and daily curves of wind speed at Yanbu, Saudi Arabia. The record\ncurves are identified and the underlying functional process is studied based on\nthe number of record curves observed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:31:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Mart\u00ednez-Hern\u00e1ndez", "Israel", ""], ["Genton", "Marc G.", ""]]}, {"id": "1909.06098", "submitter": "Holger Dette", "authors": "Alexander Aue, Holger Dette, Gregory Rice", "title": "Two-sample tests for relevant differences in the eigenfunctions of\n  covariance operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with two-sample tests for functional time series data, which\nhave become widely available in conjunction with the advent of modern complex\nobservation systems. Here, particular interest is in evaluating whether two\nsets of functional time series observations share the shape of their primary\nmodes of variation as encoded by the eigenfunctions of the respective\ncovariance operators. To this end, a novel testing approach is introduced that\nconnects with, and extends, existing literature in two main ways. First, tests\nare set up in the relevant testing framework, where interest is not in testing\nan exact null hypothesis but rather in detecting deviations deemed sufficiently\nrelevant, with relevance determined by the practitioner and perhaps guided by\ndomain experts. Second, the proposed test statistics rely on a\nself-normalization principle that helps to avoid the notoriously difficult task\nof estimating the long-run covariance structure of the underlying functional\ntime series. The main theoretical result of this paper is the derivation of the\nlarge-sample behavior of the proposed test statistics. Empirical evidence,\nindicating that the proposed procedures work well in finite samples and compare\nfavorably with competing methods, is provided through a simulation study, and\nan application to annual temperature data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:17:07 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Aue", "Alexander", ""], ["Dette", "Holger", ""], ["Rice", "Gregory", ""]]}, {"id": "1909.06120", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, N. Benjamin Erichson, Michael W. Mahoney", "title": "Bootstrapping the Operator Norm in High Dimensions: Error Estimation for\n  Covariance Matrices and Sketching", "comments": "52 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the operator (spectral) norm is one of the most widely used metrics\nfor covariance estimation, comparatively little is known about the fluctuations\nof error in this norm. To be specific, let $\\hat\\Sigma$ denote the sample\ncovariance matrix of $n$ observations in $\\mathbb{R}^p$ that arise from a\npopulation matrix $\\Sigma$, and let\n$T_n=\\sqrt{n}\\|\\hat\\Sigma-\\Sigma\\|_{\\text{op}}$. In the setting where the\neigenvalues of $\\Sigma$ have a decay profile of the form\n$\\lambda_j(\\Sigma)\\asymp j^{-2\\beta}$, we analyze how well the bootstrap can\napproximate the distribution of $T_n$. Our main result shows that up to factors\nof $\\log(n)$, the bootstrap can approximate the distribution of $T_n$ at the\ndimension-free rate of $n^{-\\frac{\\beta-1/2}{6\\beta+4}}$, with respect to the\nKolmogorov metric. Perhaps surprisingly, a result of this type appears to be\nnew even in settings where $p< n$. More generally, we discuss the consequences\nof this result beyond covariance matrices and show how the bootstrap can be\nused to estimate the errors of sketching algorithms in randomized numerical\nlinear algebra (RandNLA). An illustration of these ideas is also provided with\na climate data example.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:02:08 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Lopes", "Miles E.", ""], ["Erichson", "N. Benjamin", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1909.06155", "submitter": "Khalifa Es-Sebaiy", "authors": "Khalifa Es-Sebaiy and Mohammed Es.Sebaiy", "title": "Estimating drift parameters in a non-ergodic Gaussian Vasicek-type model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of parameter estimation for a non-ergodic Gaussian\nVasicek-type model defined as $dX_t=(\\mu+\\theta X_t)dt+dG_t,\\ t\\geq0$ with\nunknown parameters $\\theta>0$ and $\\mu\\in\\mathbb{R}$, where $G$ is a Gaussian\nprocess. We provide least square-type estimators $\\widetilde{\\theta}_T$ and\n$\\widetilde{\\mu}_T$ respectively for the drift parameters $\\theta$ and $\\mu$\nbased on continuous-time observations $\\{X_t,\\ t\\in[0,T]\\}$ as\n$T\\rightarrow\\infty$.\n  Our aim is to derive some sufficient conditions on the driving Gaussian\nprocess $G$ in order to ensure that $\\widetilde{\\theta}_T$ and\n$\\widetilde{\\mu}_T$ are strongly consistent, the limit distribution of\n$\\widetilde{\\theta}_T$ is a Cauchy-type distribution and $\\widetilde{\\mu}_T$ is\nasymptotically normal. We apply our result to fractional Vasicek, subfractional\nVasicek and bifractional Vasicek processes. In addition, this work extends the\nresult of \\cite{EEO} studied in the case where $\\mu=0$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 11:40:35 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 14:05:09 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 03:06:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Es-Sebaiy", "Khalifa", ""], ["Sebaiy", "Mohammed Es.", ""]]}, {"id": "1909.06164", "submitter": "Segrey Malov Ph. D.", "authors": "Sergey V. Malov", "title": "Uniform convergence rate of nonparametric maximum likelihood estimator\n  for the current status data with competing risks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the uniform convergence rate of the nonparametric maximum likelihood\nestimator (MLE) for the sub-distribution functions in the current status data\nwith competing risks model. It is known that the MLE have $L^2$-norm\nconvergence rate $O_P(n^{-1/3})$ in the absolutely continuous case, but there\nis no arguments for the same rate of uniform convergence. We specify conditions\nfor the uniform convergence rate $O_P(n^{-1/3}\\log^{1/3} n)$ of the MLE for the\nsub-distribution functions of competing risks on finite intervals. The obtained\nresult refines known uniform convergence rate in the particular case of current\nstatus data. The main result is applied in order to get the uniform convergence\nrate of the MLE for the survival function of failure time in the current status\nright-censored data model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:21:29 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Malov", "Sergey V.", ""]]}, {"id": "1909.06205", "submitter": "Paavo Sattler", "authors": "Paavo Sattler, Arne C. Bathke and Markus Pauly", "title": "Testing Hypotheses about Covariance Matrices in General MANOVA Designs", "comments": "Submitted to Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a unified approach to testing a variety of rather general null\nhypotheses that can be formulated in terms of covariances matrices. These\ninclude as special cases, for example, testing for equal variances, equal\ntraces, or for elements of the covariance matrix taking certain values. The\nproposed method only requires very few assumptions and thus promises to be of\nbroad practical use. Two test statistics are defined, and their asymptotic or\napproximate sampling distributions are derived. In order to improve\nparticularly the small-sample behavior of the resulting tests, two\nbootstrap-based methods are developed and theoretically justified. Several\nsimulations shed light on the performance of the proposed tests. The analysis\nof a real data set illustrates the application of the procedures.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:08:35 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:45:11 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 10:17:53 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Sattler", "Paavo", ""], ["Bathke", "Arne C.", ""], ["Pauly", "Markus", ""]]}, {"id": "1909.06307", "submitter": "Weichi Wu", "authors": "Weichi Wu, Zhou Zhou", "title": "Multiscale Jump Testing and Estimation Under Complex Temporal Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting jumps in an otherwise smoothly evolving\ntrend whilst the covariance and higher-order structures of the system can\nexperience both smooth and abrupt changes over time. The number of jump points\nis allowed to diverge to infinity with the jump sizes possibly shrinking to\nzero. The method is based on a multiscale application of an optimal jump-pass\nfilter to the time series, where the scales are dense between admissible lower\nand upper bounds. For a wide class of non-stationary time series models and\nassociated trend functions, the proposed method is shown to be able to detect\nall jump points within a nearly optimal range with a prescribed probability\nasymptotically. For a time series of length $n$, the computational complexity\nof the proposed method is $O(n)$ for each scale and $O(n\\log^{1+\\epsilon} n)$\noverall, where $\\epsilon$ is an arbitrarily small positive constant.\nSimulations and data analysis show that the proposed jump testing and\nestimation method performs robustly and accurately under complex temporal\ndynamics.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:58:20 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 15:36:20 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wu", "Weichi", ""], ["Zhou", "Zhou", ""]]}, {"id": "1909.06359", "submitter": "Yi Yu", "authors": "Daren Wang and Yi Yu and Alessandro Rinaldo and Rebecca Willett", "title": "Localizing Changes in High-Dimensional Vector Autoregressive Processes", "comments": "53 pages; 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models capture stochastic processes in which past realizations\ndetermine the generative distribution of new data; they arise naturally in a\nvariety of industrial, biomedical, and financial settings. A key challenge when\nworking with such data is to determine when the underlying generative model has\nchanged, as this can offer insights into distinct operating regimes of the\nunderlying system. This paper describes a novel dynamic programming approach to\nlocalizing changes in high-dimensional autoregressive processes and associated\nerror rates that improve upon the prior state of the art. When the model\nparameters are piecewise constant over time and the corresponding process is\npiecewise stable, the proposed dynamic programming algorithm consistently\nlocalizes change points even as the dimensionality, the sparsity of the\ncoefficient matrices, the temporal spacing between two consecutive change\npoints, and the magnitude of the difference of two consecutive coefficient\nmatrices are allowed to vary with the sample size. Furthermore, the accuracy of\ninitial, coarse change point localization estimates can be boosted via a\ncomputationally-efficient refinement algorithm that provably improves the\nlocalization error rate. Finally, a comprehensive simulation experiments and a\nreal data analysis are provided to show the numerical superiority of our\nproposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:07:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:45:38 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""], ["Willett", "Rebecca", ""]]}, {"id": "1909.06406", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Order statistics on the spacings between order statistics for the\n  uniform distribution", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed-form expressions for the distributions of the order statistics on the\nspacings between order statistics for the uniform distribution are obtained.\nThis generalizes a result by Fisher concerning tests of significance in the\nharmonic analysis of a series.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 18:56:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1909.06476", "submitter": "Dhaker Hamza", "authors": "Youssou Ciss, El hadji Deme and Hamza Dhaker", "title": "Some improvement on non-parametric estimation of income distribution and\n  poverty index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose an estimator of Foster, Greer and Thorbecke class\nof measures $\\displaystyle P(z,\\alpha) =\n\\int_0^{z}\\Big(\\frac{z-x}{z}\\Big)^{\\alpha}f(x)\\, dx$, where $z>0$ is the\npoverty line, $f$ is the probabily density function of the income distribution\nand $\\alpha$ is the so-called poverty aversion. The estimator is constructed\nwith a bias reduced kernel estimator. Uniform almost sure consistency and\nuniform mean square consistenty are established. A simulation study indicates\nthat our new estimator performs well.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 22:27:01 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:59:53 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Ciss", "Youssou", ""], ["Deme", "El hadji", ""], ["Dhaker", "Hamza", ""]]}, {"id": "1909.06503", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke, Feng Shi, Dong Xia", "title": "Community Detection for Hypergraph Networks via Regularized Tensor Power\n  Iteration", "comments": "53 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, social network analysis has been largely focused on pairwise\ninteractions. The study of higher-order interactions, via a hypergraph network,\nbrings in new insights. We study community detection in a hypergraph network. A\npopular approach is to project the hypergraph to a graph and then apply\ncommunity detection methods for graph networks, but we show that this approach\nmay cause unwanted information loss. We propose a new method for community\ndetection that operates directly on the hypergraph. At the heart of our method\nis a regularized higher-order orthogonal iteration (reg-HOOI) algorithm that\ncomputes an approximate low-rank decomposition of the network adjacency tensor.\nCompared with existing tensor decomposition methods such as HOSVD and vanilla\nHOOI, reg-HOOI yields better performance, especially when the hypergraph is\nsparse. Given the output of tensor decomposition, we then generalize the\ncommunity detection method SCORE (Jin, 2015) from graph networks to hypergraph\nnetworks. We call our new method Tensor-SCORE.\n  In theory, we introduce a degree-corrected block model for hypergraphs\n(hDCBM), and show that Tensor-SCORE yields consistent community detection for a\nwide range of network sparsity and degree heterogeneity. As a byproduct, we\nderive the rates of convergence on estimating the principal subspace by\nreg-HOOI, with different initializations, including the two new initialization\nmethods we propose, a diagonal-removed HOSVD and a randomized graph projection.\n  We apply our method to several real hypergraph networks which yields\nencouraging results. It suggests that exploring higher-order interactions\nprovides additional information not seen in graph representations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 01:50:19 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 17:53:29 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Shi", "Feng", ""], ["Xia", "Dong", ""]]}, {"id": "1909.06583", "submitter": "Stephan Huckemann", "authors": "Fabian J.E. Telschow, Michael R. Pierrynowski and Stephan F. Huckemann", "title": "Confidence Tubes for Curves on SO(3) and Identification of\n  Subject-Specific Gait Change after Kneeling", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify changes of gait patterns, e.g. due to prolonged\noccupational kneeling, which is believed to be major risk factor, among others,\nfor the development of knee osteoarthritis, we develop confidence tubes for\ncurves following a Gaussian perturbation model on SO(3). These are based on an\napplication of the Gaussian kinematic formula to a process of Hotelling\nstatistics and we approximate them by a computible version, for which we show\nconvergence. Simulations endorse our method, which in application to gait\ncurves from eight volunteers undergoing kneeling tasks, identifies phases of\nthe gait cycle that have changed due to kneeling tasks. We find that after\nkneeling, deviation from normal gait is stronger, in particular for older aged\nmale volunteers. Notably our method adjusts for different walking speeds and\nmarker replacement at different visits.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 11:47:33 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Telschow", "Fabian J. E.", ""], ["Pierrynowski", "Michael R.", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "1909.06593", "submitter": "Daniel Irving Bernstein", "authors": "Daniel Irving Bernstein, Grigoriy Blekherman, Kisun Lee", "title": "Typical ranks in symmetric matrix completion", "comments": "Version to appear in Journal of Pure and Applied Algebra", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of low-rank matrix completion for symmetric matrices.\nThe minimum rank of a completion of a generic partially specified symmetric\nmatrix depends only on the location of the specified entries, and not their\nvalues, if complex entries are allowed. When the entries are required to be\nreal, this is no longer the case and the possible minimum ranks are called\ntypical ranks. We give a combinatorial description of the patterns of specified\nentires of $n\\times n$ symmetric matrices that have $n$ as a typical rank.\nMoreover, we describe exactly when such a generic partial matrix is minimally\ncompletable to rank $n$. We also characterize the typical ranks for patterns of\nentries with low maximal typical rank.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 13:08:51 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 18:23:11 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bernstein", "Daniel Irving", ""], ["Blekherman", "Grigoriy", ""], ["Lee", "Kisun", ""]]}, {"id": "1909.06597", "submitter": "Victor Bakhtin", "authors": "V. I. Bakhtin, A. V. Lebedev", "title": "Sup-sums principles for F-divergence, Kullback--Leibler divergence, and\n  new definition for t-entropy", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents new sup-sums principles for integral F-divergence for\narbitrary convex function F and arbitrary (not necessarily positive and\nabsolutely continuous) measures. As applications of these results we derive the\ncorresponding sup-sums principle for Kullback--Leibler divergence and work out\nnew `integral' definition for t-entropy explicitly establishing its relation to\nKullback--Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 13:14:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Bakhtin", "V. I.", ""], ["Lebedev", "A. V.", ""]]}, {"id": "1909.06648", "submitter": "Toma\\v{z} Ko\\v{s}ir", "authors": "Damjana Kokol Bukov\\v{s}ek, Toma\\v{z} Ko\\v{s}ir, Bla\\v{z}\n  Moj\\v{s}kerc, and Matja\\v{z} Omladi\\v{c}", "title": "Relation between non-exchangeability and measures of concordance of\n  copulas", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An investigation is presented of how a comprehensive choice of five most\nimportant measures of concordance (namely Spearman's rho, Kendall's tau, Gini's\ngamma, Blomqvist's beta, and their weaker counterpart Spearman's footrule)\nrelate to non-exchangeability, i.e., asymmetry on copulas. Besides these\nresults, the method proposed also seems to be new and may serve as a raw model\nfor exploration of the relationship between a specific property of a copula and\nsome of its measures of dependence structure, or perhaps the relationship\nbetween various measures of dependence structure themselves.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 18:08:46 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 12:37:07 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Bukov\u0161ek", "Damjana Kokol", ""], ["Ko\u0161ir", "Toma\u017e", ""], ["Moj\u0161kerc", "Bla\u017e", ""], ["Omladi\u010d", "Matja\u017e", ""]]}, {"id": "1909.06649", "submitter": "Debraj Das", "authors": "Debraj Das, Arindam Chatterjee and S. N. Lahiri", "title": "Higher Order Refinements by Bootstrap in Lasso and other Penalized\n  Regression Methods", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection of important covariates and to drop the unimportant ones from a\nhigh-dimensional regression model is a long standing problem and hence have\nreceived lots of attention in the last two decades. After selecting the correct\nmodel, it is also important to properly estimate the existing parameters\ncorresponding to important covariates. In this spirit, Fan and Li (2001)\nproposed Oracle property as a desired feature of a variable selection method.\nOracle property has two parts; one is the variable selection consistency (VSC)\nand the other one is the asymptotic normality. Keeping VSC fixed and making the\nother part stronger, Fan and Lv (2008) introduced the strong oracle property.\nIn this paper, we consider different penalized regression techniques which are\nVSC and classify those based on oracle and strong oracle property. We show that\nboth the residual and the perturbation bootstrap methods are second order\ncorrect for any penalized estimator irrespective of its class. Most interesting\nof all is the Lasso, introduced by Tibshirani (1996). Although Lasso is VSC, it\nis not asymptotically normal and hence fails to satisfy the oracle property.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 18:13:58 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Das", "Debraj", ""], ["Chatterjee", "Arindam", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1909.06984", "submitter": "Bahman Moraffah", "authors": "Bahman Moraffah", "title": "Inference for multiple object tracking: A Bayesian nonparametric\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi object tracking (MOT) problem has drawn attention to\nit and has been studied in various research areas. However, some of the\nchallenging problems including time dependent cardinality, unordered\nmeasurement set, and object labeling remain unclear. In this paper, we propose\nrobust nonparametric methods to model the state prior for MOT problem. These\nmodels are shown to be more flexible and robust compared to existing methods.\nIn particular, the overall approach estimates time dependent object\ncardinality, provides object labeling, and identifies object associated\nmeasurements. Moreover, our proposed framework dynamically contends with the\nbirth/death and survival of the objects through dependent nonparametric\nprocesses. We present Inference algorithms that demonstrate the utility of the\ndependent nonparametric models for tracking. We employ Monte Carlo sampling\nmethods to demonstrate the proposed algorithms efficiently learn the trajectory\nof objects from noisy measurements. The computational results display the\nperformance of the proposed algorithms and comparison not only between one\nanother, but also between proposed algorithms and labeled multi Bernoulli\ntracker.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:42:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Moraffah", "Bahman", ""]]}, {"id": "1909.07026", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Olivier Marchal, Bernardo Nipoti", "title": "On the Hurwitz zeta function with an application to the beta-exponential\n  distribution", "comments": "8 pages, 1 figure", "journal-ref": "Journal of Inequalities and Applications, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a monotonicity property of the Hurwitz zeta function which, in turn,\ntranslates into a chain of inequalities for polygamma functions of different\norders. We provide a probabilistic interpretation of our result by exploiting a\nconnection between Hurwitz zeta function and the cumulants of the\nbeta-exponential distribution.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:13:05 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:05:28 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Arbel", "Julyan", ""], ["Marchal", "Olivier", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1909.07178", "submitter": "Leonie Selk", "authors": "Maria Mohr, Leonie Selk", "title": "Estimating change points in nonparametric time series regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a regression model that allows for time series\ncovariates as well as heteroscedasticity with a regression function that is\nmodelled nonparametrically. We assume that the regression function changes at\nsome unknown time $\\lfloor ns_0\\rfloor$, $s_0\\in(0,1)$, and our aim is to\nestimate the (rescaled) change point $s_0$. The considered estimator is based\non a Kolmogorov-Smirnov functional of the marked empirical process of\nresiduals. We show consistency of the estimator and prove a rate of convergence\nof $O_P(n^{-1})$ which in this case is clearly optimal as there are only $n$\npoints in the sequence. Additionally we investigate the case of lagged\ndependent covariates, that is, autoregression models with a change in the\nnonparametric (auto-) regression function and give a consistency result. The\nmethod of proof also allows for different kinds of functionals such that\nCram\\'er-von Mises type estimators can be considered similarly. The approach\nextends existing literature by allowing nonparametric models, time series data\nas well as heteroscedasticity. Finite sample simulations indicate the good\nperformance of our estimator in regression as well as autoregression models and\na real data example shows its applicability in practise.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:16:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mohr", "Maria", ""], ["Selk", "Leonie", ""]]}, {"id": "1909.07232", "submitter": "Evgeny Pchelintsev", "authors": "Evgeny A. Pchelintsev, Serguei M. Pergamenshchikov, Maria A. Povzun", "title": "Improved estimation via model selection method for semimartingale\n  regressions based on discrete data", "comments": "38 pages, 6 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the robust adaptive nonparametric estimation problem for a\nperiodic function observed in the framework of a continuous time regression\nmodel with semimartingale noises.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:23:22 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 10:21:34 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Pchelintsev", "Evgeny A.", ""], ["Pergamenshchikov", "Serguei M.", ""], ["Povzun", "Maria A.", ""]]}, {"id": "1909.07324", "submitter": "Kai Qi", "authors": "Kai Qi, Yang Chen, Wei Wu", "title": "Dirichlet Depths for Point Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical depths have been well studied for multivariate and functional\ndata over the past few decades, but remain under-explored for point processes.\nA first attempt on the notion of point process depth was conducted recently\nwhere the depth was defined as a weighted product of two terms: (1) the\nprobability of the number of events in each process and (2) the depth of the\nevent times conditioned on the number of events by using a Mahalanobis depth.\nWe point out that multivariate depths such as the Mahalanobis depth cannot be\ndirectly used because they often neglect the important ordered property in the\npoint process events. To deal with this problem, we propose a model-based\napproach for point processes systematically. In particular, we develop a\nDirichlet-distribution-based framework on the conditional depth term, where the\nnew methods are referred to as Dirichlet depths. We examine the mathematical\nproperties of the new depths and conduct the asymptotic analysis. In addition,\nwe illustrate the new methods using various simulated and real experiment data.\nIt is found that the proposed framework provides a proper center-outward rank\nand the new methods have superior decoding performance to previous methods in\ntwo neural spike train datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:36:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Qi", "Kai", ""], ["Chen", "Yang", ""], ["Wu", "Wei", ""]]}, {"id": "1909.07513", "submitter": "Jonathan Niles-Weed", "authors": "Jonathan Niles-Weed and Philippe Rigollet", "title": "Estimation of Wasserstein distances in the Spiked Transport Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical model, the spiked transport model, which\nformalizes the assumption that two probability distributions differ only on a\nlow-dimensional subspace. We study the minimax rate of estimation for the\nWasserstein distance under this model and show that this low-dimensional\nstructure can be exploited to avoid the curse of dimensionality. As a byproduct\nof our minimax analysis, we establish a lower bound showing that, in the\nabsence of such structure, the plug-in estimator is nearly rate-optimal for\nestimating the Wasserstein distance in high dimension. We also give evidence\nfor a statistical-computational gap and conjecture that any computationally\nefficient estimator is bound to suffer from the curse of dimensionality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:51:38 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Niles-Weed", "Jonathan", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1909.07527", "submitter": "Arno Berger", "authors": "Arno Berger and Theodore P. Hill", "title": "The Mathematics of Benford's Law -- A Primer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.HO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a concise overview of the main mathematical theory of\nBenford's law in a form accessible to scientists and students who have had\nfirst courses in calculus and probability. In particular, one of the main\nobjectives here is to aid researchers who are interested in applying Benford's\nlaw, and need to understand general principles clarifying when to expect the\nappearance of Benford's law in real-life data and when not to expect it. A\nsecond main target audience is students of statistics or mathematics, at all\nlevels, who are curious about the mathematics underlying this surprising and\nrobust phenomenon, and may wish to delve more deeply into the subject. This\nsurvey of the fundamental principles behind Benford's law includes many basic\nexamples and theorems, but does not include the proofs or the most general\nstatements of the theorems; rather it provides precise references where both\nmay be found.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 00:16:53 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 23:35:54 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 23:09:39 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Berger", "Arno", ""], ["Hill", "Theodore P.", ""]]}, {"id": "1909.07719", "submitter": "Alain Desgagn\\'e", "authors": "Alain Desgagn\\'e", "title": "Efficient and Robust Estimation of Linear Regression with Normal Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression with normally distributed errors - including particular\ncases such as ANOVA, Student's t-test or location-scale inference - is a widely\nused statistical procedure. In this case the ordinary least squares estimator\npossesses remarkable properties but is very sensitive to outliers. Several\nrobust alternatives have been proposed, but there is still significant room for\nimprovement. This paper thus proposes an original method of estimation that\noffers the best efficiency simultaneously in the absence and the presence of\noutliers, both for the estimation of the regression coefficients and the scale\nparameter. The approach first consists in broadening the normal assumption of\nthe errors to a mixture of the normal and the filtered-log-Pareto (FLP), an\noriginal distribution designed to represent the outliers. The\nexpectation-maximization (EM) algorithm is then adapted and we obtain the N-FLP\nestimators of the regression coefficients, the scale parameter and the\nproportion of outliers, along with probabilities of each observation being an\noutlier. The performance of the N-FLP estimators is compared with the best\nalternatives in an extensive Monte Carlo simulation. The paper demonstrates\nthat this method of estimation can also be used for a complete robust\ninference, including confidence intervals, hypothesis testing and model\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:18:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Desgagn\u00e9", "Alain", ""]]}, {"id": "1909.07836", "submitter": "Haiyan Cai", "authors": "Haiyan Cai, Bryan Goggin, Qingtang Jiang", "title": "Two-Sample Test Based on Classification Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust classification algorithms have been developed in recent years with\ngreat success. We take advantage of this development and recast the classical\ntwo-sample test problem in the framework of classification. Based on the\nestimates of classification probabilities from a classifier trained from the\nsamples, a test statistic is proposed. We explain why such a test can be a\npowerful test and compare its performance in terms of the power and efficiency\nwith those of some other recently proposed tests with simulation and real-life\ndata. The test proposed is nonparametric and can be applied to complex and high\ndimensional data wherever there is a classifier that provides consistent\nestimate of the classification probability for such data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:18:39 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Cai", "Haiyan", ""], ["Goggin", "Bryan", ""], ["Jiang", "Qingtang", ""]]}, {"id": "1909.07862", "submitter": "Tudor Manole", "authors": "Tudor Manole, Sivaraman Balakrishnan, Larry Wasserman", "title": "Minimax Confidence Intervals for the Sliced Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the growing popularity of variants of the Wasserstein distance\nin statistics and machine learning, we study statistical inference for the\nSliced Wasserstein distance--an easily computable variant of the Wasserstein\ndistance. Specifically, we construct confidence intervals for the Sliced\nWasserstein distance which have finite-sample validity under no assumptions or\nunder mild moment assumptions. These intervals are adaptive in length to the\nregularity of the underlying distributions. We also bound the minimax risk of\nestimating the Sliced Wasserstein distance, and as a consequence establish that\nthe lengths of our proposed confidence intervals are minimax optimal over\nappropriate distribution classes. To motivate the choice of these classes, we\nalso study minimax rates of estimating a distribution under the Sliced\nWasserstein distance. These theoretical findings are complemented with a\nsimulation study demonstrating the deficiencies of the classical bootstrap, and\nthe advantages of our proposed methods. We also show strong correspondences\nbetween our theoretical predictions and the adaptivity of our confidence\ninterval lengths in simulations. We conclude by demonstrating the use of our\nconfidence intervals in the setting of simulator-based likelihood-free\ninference. In this setting, contrasting popular approximate Bayesian\ncomputation methods, we develop uncertainty quantification methods with\nrigorous frequentist coverage guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 14:51:17 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 17:20:09 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Manole", "Tudor", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1909.07974", "submitter": "William Leeb", "authors": "William Leeb", "title": "Properties of Laplacian Pyramids for Extension and Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the Laplacian pyramids algorithm of Rabin and Coifman for\nextending and denoising a function sampled on a discrete set of points. We\nprovide mild conditions under which the algorithm converges, and prove\nstability bounds on the extended function. We also consider the iterative\napplication of truncated Laplacian pyramids kernels for denoising signals by\nnon-local means.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 20:28:24 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "1909.08022", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters", "title": "Rotational Uniqueness Conditions Under Oblique Factor Correlation Metric", "comments": "Postprint, 5 pages", "journal-ref": "Psychometrika, 77 (2012): 288-292", "doi": "10.1007/s11336-012-9259-3", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an addendum to his seminal 1969 article J\\\"{o}reskog stated two sets of\nconditions for rotational identification of the oblique factor solution under\nutilization of fixed zero elements in the factor loadings matrix. These\ncondition sets, formulated under factor correlation and factor covariance\nmetrics, respectively, were claimed to be equivalent and to lead to global\nrotational uniqueness of the factor solution. It is shown here that the\nconditions for the oblique factor correlation structure need to be amended for\nglobal rotational uniqueness, and hence, that the condition sets are not\nequivalent in terms of unicity of the solution.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:46:39 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Peeters", "Carel F. W.", ""]]}, {"id": "1909.08101", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Venkata K Jandhyala, and Stergios B Fotopoulos", "title": "Inference on the change point with the jump size near the boundary of\n  the region of detectability in high dimensional time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a projected least squares estimator for the change point parameter\nin a high dimensional time series model with a potential change point.\nImportantly we work under the setup where the jump size may be near the\nboundary of the region of detectability. The proposed methodology yields an\noptimal rate of convergence despite high dimensionality of the assumed model\nand a potentially diminishing jump size. The limiting distribution of this\nestimate is derived, thereby allowing construction of a confidence interval for\nthe location of the change point. A secondary near optimal estimate is proposed\nwhich is required for the implementation of the optimal projected least squares\nestimate. The prestep estimation procedure is designed to also agnostically\ndetect the case where no change point exists, thereby removing the need to\npretest for the existence of a change point for the implementation of the\ninference methodology. Our results are presented under a general positive\ndefinite spatial dependence setup, assuming no special structure on this\ndependence. The proposed methodology is designed to be highly scalable, and\napplicable to very large data. Theoretical results regarding detection and\nestimation consistency and the limiting distribution are numerically supported\nvia monte carlo simulations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 21:08:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kaul", "Abhishek", ""], ["Jandhyala", "Venkata K", ""], ["Fotopoulos", "Stergios B", ""]]}, {"id": "1909.08436", "submitter": "Mikael Escobar-Bach", "authors": "Mikael Escobar-Bach, Ingrid Van Keilegom", "title": "Nonparametric estimation of conditional cure models for heavy-tailed\n  distributions and under insufficient follow-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing time-to-event data, it often happens that some subjects do not\nexperience the event of interest. Survival models that take this feature into\naccount (called `cure models') have been developed in the presence of\ncovariates. However, the current literature on nonparametric cure models with\ncovariates cannot be applied when the follow-up is insufficient, i.e., when the\nright endpoint of the support of the censoring time is strictly smaller than\nthat of the survival time of the susceptible subjects. In this paper we attempt\nto fill this gap in the literature by proposing new estimators of the\nconditional cure rate and the conditional survival function using extrapolation\ntechniques coming from extreme value theory. We establish the asymptotic\nnormality of the proposed estimators, and show how the estimators work for\nsmall samples by means of a simulation study. We also illustrate their\npractical applicability through the analysis of data on the survival of colon\ncancer patients.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:26:58 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Escobar-Bach", "Mikael", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1909.08447", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh, N.Balakrishnan", "title": "On compatibility/incompatibility of two discrete probability\n  distributions in the presence of incomplete specification", "comments": "19 pages article. arXiv admin note: substantial text overlap with\n  arXiv:1711.00608", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional specification of distributions is a developing area with many\napplications. In the finite discrete case, a variety of compatible conditions\ncan be derived. In this paper, we propose an alternative approach to study the\ncompatibility of two conditional probability distributions under the finite\ndiscrete set up. A technique based on rank-based criterion is shown to be\nparticularly convenient for identifying compatible distributions corresponding\nto complete conditional specification, including the case with zeros. The\nproposed methods are finally illustrated with several examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:41:20 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ghosh", "Indranil", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1909.08733", "submitter": "Nabarun Deb", "authors": "Nabarun Deb and Bodhisattva Sen", "title": "Multivariate Rank-based Distribution-free Nonparametric Testing using\n  Measure Transportation", "comments": "77 pages, 5 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework for distribution-free\nnonparametric testing in multi-dimensions, based on a notion of multivariate\nranks defined using the theory of measure transportation. Unlike other existing\nproposals in the literature, these multivariate ranks share a number of useful\nproperties with the usual one-dimensional ranks; most importantly, these ranks\nare distribution-free. This crucial observation allows us to design\nnonparametric tests that are exactly distribution-free under the null\nhypothesis. We demonstrate the applicability of this approach by constructing\nexact distribution-free tests for two classical nonparametric problems: (i)\ntesting for mutual independence between random vectors, and (ii) testing for\nthe equality of multivariate distributions. In particular, we propose\n(multivariate) rank versions of distance covariance (Sz\\'ekely et al., 2007)\nand energy statistic (Sz\\'ekely and Rizzo, 2013) for testing scenarios (i) and\n(ii) respectively. In both these problems, we derive the asymptotic null\ndistribution of the proposed test statistics. We further show that our tests\nare consistent against all fixed alternatives. Moreover, the proposed tests are\ntuning-free, computationally feasible and are well-defined under minimal\nassumptions on the underlying distributions (e.g., they do not need any moment\nassumptions). We also demonstrate the efficacy of these procedures via\nextensive simulations. In the process of analyzing the theoretical properties\nof our procedures, we end up proving some new results in the theory of measure\ntransportation and in the limit theory of permutation statistics using Stein's\nmethod for exchangeable pairs, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:19:35 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 20:00:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Deb", "Nabarun", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1909.08749", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Martin J. Wainwright", "title": "Instance-dependent $\\ell_\\infty$-bounds for policy evaluation in tabular\n  reinforcement learning", "comments": "Version v2 is consistent with manuscript to appear in IEEE\n  Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov reward processes (MRPs) are used to model stochastic phenomena arising\nin operations research, control engineering, robotics, and artificial\nintelligence, as well as communication and transportation networks. In many of\nthese cases, such as in the policy evaluation problem encountered in\nreinforcement learning, the goal is to estimate the long-term value function of\nsuch a process without access to the underlying population transition and\nreward functions. Working with samples generated under the synchronous model,\nwe study the problem of estimating the value function of an infinite-horizon,\ndiscounted MRP on finitely many states in the $\\ell_\\infty$-norm. We analyze\nboth the standard plug-in approach to this problem and a more robust variant,\nand establish non-asymptotic bounds that depend on the (unknown) problem\ninstance, as well as data-dependent bounds that can be evaluated based on the\nobservations of state-transitions and rewards. We show that these approaches\nare minimax-optimal up to constant factors over natural sub-classes of MRPs.\nOur analysis makes use of a leave-one-out decoupling argument tailored to the\npolicy evaluation problem, one which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:38:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:20:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1909.08755", "submitter": "Banghua Zhu", "authors": "Banghua Zhu, Jiantao Jiao and Jacob Steinhardt", "title": "Generalized Resilience and Robust Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust statistics traditionally focuses on outliers, or perturbations in\ntotal variation distance. However, a dataset could be corrupted in many other\nways, such as systematic measurement errors and missing covariates. We\ngeneralize the robust statistics approach to consider perturbations under any\nWasserstein distance, and show that robust estimation is possible whenever a\ndistribution's population statistics are robust under a certain family of\nfriendly perturbations. This generalizes a property called resilience\npreviously employed in the special case of mean estimation with outliers. We\njustify the generalized resilience property by showing that it holds under\nmoment or hypercontractive conditions. Even in the total variation case, these\nsubsume conditions in the literature for mean estimation, regression, and\ncovariance estimation; the resulting analysis simplifies and sometimes improves\nthese known results in both population limit and finite-sample rate. Our robust\nestimators are based on minimum distance (MD) functionals (Donoho and Liu,\n1988), which project onto a set of distributions under a discrepancy related to\nthe perturbation. We present two approaches for designing MD estimators with\ngood finite-sample rates: weakening the discrepancy and expanding the set of\ndistributions. We also present connections to Gao et al. (2019)'s recent\nanalysis of generative adversarial networks for robust estimation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:08:06 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 06:36:12 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 07:50:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "1909.08988", "submitter": "Gr\\'egoire Clart\\'e", "authors": "Gr\\'egoire Clart\\'e, Antoine Diez, Jean Feydy", "title": "Collective Proposal Distributions for Nonlinear MCMC samplers:\n  Mean-Field Theory and Fast Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, various \"non-linear\" MCMC methods have arised. While\nappealing for their convergence speed and efficiency, their practical\nimplementation and theoretical study remain challenging. In this paper, we\nintroduce a large class of non-linear samplers that can be studied and\nsimulated as the mean-field limit of a system of interacting particles. The\npractical implementation we propose leverages the computational power of modern\nhardware (GPU).\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:53:13 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 10:33:41 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 11:36:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Clart\u00e9", "Gr\u00e9goire", ""], ["Diez", "Antoine", ""], ["Feydy", "Jean", ""]]}, {"id": "1909.09261", "submitter": "Tianjian Zhou", "authors": "Tong Li, Tianjian Zhou, Kam-Wah Tsui, Lin Wei, Yuan Ji", "title": "Posterior Contraction Rate of Sparse Latent Feature Models with\n  Application to Proteomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Indian buffet process (IBP) and phylogenetic Indian buffet process (pIBP)\ncan be used as prior models to infer latent features in a data set. The\ntheoretical properties of these models are under-explored, however, especially\nin high dimensional settings. In this paper, we show that under mild sparsity\ncondition, the posterior distribution of the latent feature matrix, generated\nvia IBP or pIBP priors, converges to the true latent feature matrix\nasymptotically. We derive the posterior convergence rate, referred to as the\ncontraction rate. We show that the convergence holds even when the\ndimensionality of the latent feature matrix increases with the sample size,\ntherefore making the posterior inference valid in high dimensional setting. We\ndemonstrate the theoretical results using computer simulation, in which the\nparallel-tempering Markov chain Monte Carlo method is applied to overcome\ncomputational hurdles. The practical utility of the derived properties is\ndemonstrated by inferring the latent features in a reverse phase protein arrays\n(RPPA) dataset under the IBP prior model. Software and dataset reported in the\nmanuscript are provided at http://www.compgenome.org/IBP.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 23:44:27 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Tong", ""], ["Zhou", "Tianjian", ""], ["Tsui", "Kam-Wah", ""], ["Wei", "Lin", ""], ["Ji", "Yuan", ""]]}, {"id": "1909.09302", "submitter": "Guan'ao Yan", "authors": "Jun Zhao, Guan'ao Yan and Yi Zhang", "title": "Robust Estimation and Shrinkage in Ultrahigh Dimensional Expectile\n  Regression with Heavy Tails and Variance Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data subject to heavy-tailed phenomena and heterogeneity are\ncommonly encountered in various scientific fields and bring new challenges to\nthe classical statistical methods. In this paper, we combine the asymmetric\nsquare loss and huber-type robust technique to develop the robust expectile\nregression for ultrahigh dimensional heavy-tailed heterogeneous data. Different\nfrom the classical huber method, we introduce two different tuning parameters\non both sides to account for possibly asymmetry and allow them to diverge to\nreduce bias induced by the robust approximation. In the regularized framework,\nwe adopt the generally folded concave penalty function like the SCAD or MCP\npenalty for the seek of bias reduction. We investigate the finite sample\nproperty of the corresponding estimator and figure out how our method plays its\nrole to trades off the estimation accuracy against the heavy-tailed\ndistribution. Also, noting that the robust asymmetric loss function is\neverywhere differentiable, based on our theoretical study, we propose an\nefficient first-order optimization algorithm after locally linear approximation\nof the non-convex problem. Simulation studies under various distributions\ndemonstrates the satisfactory performances of our method in coefficient\nestimation, model selection and heterogeneity detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 03:16:33 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 13:19:05 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhao", "Jun", ""], ["Yan", "Guan'ao", ""], ["Zhang", "Yi", ""]]}, {"id": "1909.09336", "submitter": "Eitan Greenshtein", "authors": "Eitan Greenshtein and Ya'acov Ritov", "title": "Generalized Maximum Likelihood Estimators and their applications to\n  stratified sampling and post-stratification with many unobserved strata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating a weighted average of the means of $n$\nstrata, based on a random sample with realized $K_i$ observations from stratum\n$i, \\; i=1,...,n$.\n  This task is non-trivial in cases where for a significant portion of the\nstrata the corresponding $K_i=0$. Such a situation may happen in\npost-stratification, when it is desired to have a very fine sftratification. A\nfine stratification could be desired in order that assumptions, or,\napproximations, like Missing At Random conditional on strata, will be\nappealing. A fine stratification could also be desired in observational\nstudies, when it is desired to estimate average treatment effect, by averaging\nthe effects in small and homogenous strata.\n  Our approach is based on applying Generalized Maximum Likelihood Estimators\n(GMLE), and ideas that are related to Non-Parametric Empirical Bayes, in order\nto estimate the means of strata $i$ with corresponding $K_i=0$. There are no\nassumptions about a relation between the means of the unobserved strata (i.e.,\nwith $K_i=0$) and those of the observed strata.\n  The performance of our approach is demonstrated both in simulations and on a\nreal data set. Some consistency and asymptotic results are also presented. In\naddition, related basic results about GMLE estimation of the mean of mixtures\nof exponential families are provided.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 06:05:48 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:58:23 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Greenshtein", "Eitan", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1909.09345", "submitter": "Shuaiwen Wang", "authors": "Shuaiwen Wang, Haolei Weng, Arian Maleki", "title": "Does SLOPE outperform bridge regression?", "comments": "51 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently proposed SLOPE estimator (arXiv:1407.3824) has been shown to\nadaptively achieve the minimax $\\ell_2$ estimation rate under high-dimensional\nsparse linear regression models (arXiv:1503.08393). Such minimax optimality\nholds in the regime where the sparsity level $k$, sample size $n$, and\ndimension $p$ satisfy $k/p \\rightarrow 0$, $k\\log p/n \\rightarrow 0$. In this\npaper, we characterize the estimation error of SLOPE under the complementary\nregime where both $k$ and $n$ scale linearly with $p$, and provide new insights\ninto the performance of SLOPE estimators. We first derive a concentration\ninequality for the finite sample mean square error (MSE) of SLOPE. The quantity\nthat MSE concentrates around takes a complicated and implicit form. With\ndelicate analysis of the quantity, we prove that among all SLOPE estimators,\nLASSO is optimal for estimating $k$-sparse parameter vectors that do not have\ntied non-zero components in the low noise scenario. On the other hand, in the\nlarge noise scenario, the family of SLOPE estimators are sub-optimal compared\nwith bridge regression such as the Ridge estimator.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:01:51 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 06:50:30 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Wang", "Shuaiwen", ""], ["Weng", "Haolei", ""], ["Maleki", "Arian", ""]]}, {"id": "1909.09438", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai and Matthias Scherer", "title": "On the structure of exchangeable extreme-value copulas", "comments": null, "journal-ref": "Journal of Multivariate Analysis 180, 104670 [11 pages] (2020)", "doi": "10.1016/j.jmva.2020.104670", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the set of $d$-variate symmetric stable tail dependence\nfunctions, uniquely associated with exchangeable $d$-dimensional extreme-value\ncopulas, is a simplex and determine its extremal boundary. The subset of\nelements which arises as $d$-margins of the set of $(d+k)$-variate symmetric\nstable tail dependence functions is shown to be proper for arbitrary $k \\geq\n1$. Finally, we derive an intuitive and useful necessary condition for a\nbivariate extreme-value copula to arise as bi-margin of an exchangeable\nextreme-value copula of arbitrarily large dimension, and thus to be\nconditionally iid.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:53:25 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mai", "Jan-Frederik", ""], ["Scherer", "Matthias", ""]]}, {"id": "1909.09517", "submitter": "Christophe Pouet", "authors": "Yuri Golubev (IITP), Christophe Pouet (I2M)", "title": "Multi-level Bayes and MAP monotonicity testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop Bayes and maximum a posteriori probability (MAP)\napproaches to monotonicity testing. In order to simplify this problem, we\nconsider a simple white Gaussian noise model and with the help of the Haar\ntransform we reduce it to the equivalent problem of testing positivity of the\nHaar coefficients. This approach permits, in particular, to understand links\nbetween monotonicity testing and sparse vectors detection, to construct new\ntests, and to prove their optimality without supplementary assumptions. The\nmain idea in our construction of multi-level tests is based on some invariance\nproperties of specific probability distributions. Along with Bayes and MAP\ntests, we construct also adaptive multi-level tests that are free from the\nprior information about the sizes of non-monotonicity segments of the function.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:06:00 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Golubev", "Yuri", "", "IITP"], ["Pouet", "Christophe", "", "I2M"]]}, {"id": "1909.09528", "submitter": "S\\\"oren Christensen", "authors": "S\\\"oren Christensen and Claudia Strauch", "title": "Nonparametric learning for impulse control problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental assumptions in stochastic control of continuous time\nprocesses is that the dynamics of the underlying (diffusion) process is known.\nThis is, however, usually obviously not fulfilled in practice. On the other\nhand, over the last decades, a rich theory for nonparametric estimation of the\ndrift (and volatility) for continuous time processes has been developed. The\naim of this paper is bringing together techniques from stochastic control with\nmethods from statistics for stochastic processes to find a way to both learn\nthe dynamics of the underlying process and control in a reasonable way at the\nsame time. More precisely, we study a long-term average impulse control\nproblem, a stochastic version of the classical Faustmann timber harvesting\nproblem. One of the problems that immediately arises is an\nexploration-exploitation dilemma as is well known for problems in machine\nlearning. We propose a way to deal with this issue by combining exploration and\nexploitation periods in a suitable way. Our main finding is that this\nconstruction can be based on the rates of convergence of estimators for the\ninvariant density. Using this, we obtain that the average cumulated regret is\nof uniform order $O({T^{-1/3}})$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:32:08 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:15:44 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Christensen", "S\u00f6ren", ""], ["Strauch", "Claudia", ""]]}, {"id": "1909.09846", "submitter": "Yuliy Baryshnikov", "authors": "Yuliy Baryshnikov", "title": "Time Series, Persistent Homology and Chirality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the point process of persistent diagram for Brownian motions\nwith drift, obtaining some of its basic characteristics. Further we introduce\nand study the refinement of the persistent homology, assigning to each bar its\nchirality.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 15:33:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Baryshnikov", "Yuliy", ""]]}, {"id": "1909.09851", "submitter": "Anru Zhang", "authors": "T. Tony Cai, Anru Zhang, Yuchen Zhou", "title": "Sparse Group Lasso: Optimal Sample Complexity, Convergence Rate, and\n  Statistical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we study sparse group Lasso for high-dimensional double sparse\nlinear regression, where the parameter of interest is simultaneously\nelement-wise and group-wise sparse. This problem is an important instance of\nthe simultaneously structured model -- an actively studied topic in statistics\nand machine learning. In the noiseless case, we provide matching upper and\nlower bounds on sample complexity for the exact recovery of sparse vectors and\nfor stable estimation of approximately sparse vectors, respectively. In the\nnoisy case, we develop upper and matching minimax lower bounds for estimation\nerror. We also consider the debiased sparse group Lasso and investigate its\nasymptotic property for the purpose of statistical inference. Finally,\nnumerical studies are provided to support the theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 16:17:04 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""], ["Zhou", "Yuchen", ""]]}, {"id": "1909.09985", "submitter": "Roman F\\\"oll", "authors": "Roman F\\\"oll, Ingo Steinwart", "title": "PAC-Bayesian Bounds for Deep Gaussian Processes", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approximation techniques and inference for stochastic models in\nmachine learning has gained much attention the last years. Especially in the\ncase of Gaussian Processes (GP) and their deep versions, Deep Gaussian\nProcesses (DGPs), these viewpoints improved state of the art work. In this\npaper we introduce Probably Approximately Correct (PAC)-Bayesian risk bounds\nfor DGPs making use of variational approximations. We show that the\nminimization of PAC-Bayesian generalization risk bounds maximizes the\nvariational lower bounds belonging to the specific DGP model. We generalize the\nloss function property of the log likelihood loss function in the context of\nPAC-Bayesian risk bounds to the quadratic-form-Gaussian case. Consistency\nresults are given and an oracle-type inequality gives insights in the\nconvergence between the raw model (predictor without variational approximation)\nand our variational models (predictor for the variational approximation).\nFurthermore, we give extensions of our main theorems for specific assumptions\nand parameter cases. Moreover, we show experimentally the evolution of the\nconsistency results for two Deep Recurrent Gaussian Processes (DRGP) modeling\ntime-series, namely the recurrent Gaussian Process (RGP) and the DRGP with\nVariational Sparse Spectrum approximation, namely DRGP-(V)SS.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 11:29:54 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["F\u00f6ll", "Roman", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1909.09996", "submitter": "Guy Martial Nkiet", "authors": "M\\`etolidji Moquilas Raymond Affossogbe, Guy Martial Nkiet, and Carlos\n  Ogouyandjou", "title": "Dimension reduction in spatial regression with kernel SAVE method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the smoothed version of sliced average variance estimation (SAVE)\ndimension reduction method for dealing with spatially dependent data that are\nobservations of a strongly mixing random field. We propose kernel estimators\nfor the interest matrix and the effective dimension reduction (EDR) space, and\nshow their consistency.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 12:59:19 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Affossogbe", "M\u00e8tolidji Moquilas Raymond", ""], ["Nkiet", "Guy Martial", ""], ["Ogouyandjou", "Carlos", ""]]}, {"id": "1909.10024", "submitter": "Fang Han", "authors": "Hongjian Shi, Mathias Drton, and Fang Han", "title": "Distribution-free consistent independence tests via center-outward ranks\n  and signs", "comments": "to appear in JASA T&M", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of testing independence of two random\nvectors of general dimensions. For this, we give for the first time a\ndistribution-free consistent test. Our approach combines distance covariance\nwith the center-outward ranks and signs developed in Hallin (2017). In\ntechnical terms, the proposed test is consistent and distribution-free in the\nfamily of multivariate distributions with nonvanishing (Lebesgue) probability\ndensities. Exploiting the (degenerate) U-statistic structure of the distance\ncovariance and the combinatorial nature of Hallin's center-outward ranks and\nsigns, we are able to derive the limiting null distribution of our test\nstatistic. The resulting asymptotic approximation is accurate already for\nmoderate sample sizes and makes the test implementable without requiring\npermutation. The limiting distribution is derived via a more general result\nthat gives a new type of combinatorial non-central limit theorem for double-\nand multiple-indexed permutation statistics.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 15:12:17 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 17:32:09 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 19:10:49 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 17:41:13 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Shi", "Hongjian", ""], ["Drton", "Mathias", ""], ["Han", "Fang", ""]]}, {"id": "1909.10047", "submitter": "James Griffin", "authors": "James T. Griffin", "title": "Probabilistic Fitting of Topological Structure to Data", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a class of probability distributions that we call simplicial\nmixture models, inspired by simplicial complexes from algebraic topology. The\nparameters of these distributions represent their topology and we show that it\nis possible and feasible to fit topological structure to data using a\nmaximum-likelihood approach. We prove under reasonable assumptions that with a\nfixed number of vertices a distribution can be approximated arbitrarily closely\nby a simplicial mixture model when using enough simplices. Even if the topology\nis not of primary interest, when using a model that takes the topology of the\ndata into account the vertex positions are good candidates for\narchetype/endmember vectors in unmixing problems.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 17:08:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Griffin", "James T.", ""]]}, {"id": "1909.10110", "submitter": "Indrabati Bhattacharya", "authors": "Indrabati Bhattacharya, Subhashis Ghosal", "title": "Bayesian Inference on Multivariate Medians and Quantiles", "comments": null, "journal-ref": "Statistica Sinica, 2020", "doi": "10.5705/ss.202020.0108", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian inference on a class of multivariate\nmedian and the multivariate quantile functionals of a joint distribution using\na Dirichlet process prior. Since, unlike univariate quantiles, the exact\nposterior distribution of multivariate median and multivariate quantiles are\nnot obtainable explicitly, we study these distributions asymptotically. We\nderive a Bernstein-von Mises theorem for the multivariate $\\ell_1$-median with\nrespect to general $\\ell_p$-norm, which in particular shows that its posterior\nconcentrates around its true value at $n^{-1/2}$-rate and its credible sets\nhave asymptotically correct frequentist coverage. In particular, asymptotic\nnormality results for the empirical multivariate median with general\n$\\ell_p$-norm is also derived in the course of the proof which extends the\nresults from the case $p=2$ in the literature to a general $p$. The technique\ninvolves approximating the posterior Dirichlet process by a Bayesian bootstrap\nprocess and deriving a conditional Donsker theorem. We also obtain analogous\nresults for an affine equivariant version of the multivariate $\\ell_1$-median\nbased on an adaptive transformation and re-transformation technique. The\nresults are extended to a joint distribution of multivariate quantiles. The\naccuracy of the asymptotic result is confirmed by a simulation study. We also\nuse the results to obtain Bayesian credible regions for multivariate medians\nfor Fisher's iris data, which consists of four features measured for each of\nthree plant species.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 01:11:00 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1909.10140", "submitter": "Sourav Chatterjee", "authors": "Sourav Chatterjee", "title": "A new coefficient of correlation", "comments": "39 pages, 9 figures, 2 tables. To appear in J. Amer. Statist. Assoc.\n  R package available at https://CRAN.R-project.org/package=XICOR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to define a coefficient of correlation which is (a) as simple\nas the classical coefficients like Pearson's correlation or Spearman's\ncorrelation, and yet (b) consistently estimates some simple and interpretable\nmeasure of the degree of dependence between the variables, which is 0 if and\nonly if the variables are independent and 1 if and only if one is a measurable\nfunction of the other, and (c) has a simple asymptotic theory under the\nhypothesis of independence, like the classical coefficients? This article\nanswers this question in the affirmative, by producing such a coefficient. No\nassumptions are needed on the distributions of the variables. There are several\ncoefficients in the literature that converge to 0 if and only if the variables\nare independent, but none that satisfy any of the other properties mentioned\nabove.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 03:31:42 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 02:22:00 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 06:29:23 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 21:55:04 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chatterjee", "Sourav", ""]]}, {"id": "1909.10143", "submitter": "Haolei Weng", "authors": "Rahul Mazumder and Haolei Weng", "title": "Computing the degrees of freedom of rank-regularized estimators and\n  cousins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a low rank matrix from its linear measurements is a problem of\ncentral importance in contemporary statistical analysis. The choice of tuning\nparameters for estimators remains an important challenge from a theoretical and\npractical perspective. To this end, Stein's Unbiased Risk Estimate (SURE)\nframework provides a well-grounded statistical framework for degrees of freedom\nestimation. In this paper, we use the SURE framework to obtain degrees of\nfreedom estimates for a general class of spectral regularized matrix\nestimators, generalizing beyond the class of estimators that have been studied\nthus far. To this end, we use a result due to Shapiro (2002) pertaining to the\ndifferentiability of symmetric matrix valued functions, developed in the\ncontext of semidefinite optimization algorithms. We rigorously verify the\napplicability of Stein's lemma towards the derivation of degrees of freedom\nestimates; and also present new techniques based on Gaussian convolution to\nestimate the degrees of freedom of a class of spectral estimators to which\nStein's lemma is not directly applicable.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 03:44:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mazumder", "Rahul", ""], ["Weng", "Haolei", ""]]}, {"id": "1909.10197", "submitter": "Edoardo Mainini", "authors": "Emanuele Dolera and Edoardo Mainini", "title": "On uniform continuity of posterior distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of dominated statistical models, we provide conditions\nyielding strong continuity of the posterior distribution with respect to the\nobserved data. We show some applications, with special focus on exponential\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 07:49:25 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Dolera", "Emanuele", ""], ["Mainini", "Edoardo", ""]]}, {"id": "1909.10271", "submitter": "Matus Maciak", "authors": "Mat\\'u\\v{s} Maciak", "title": "Quantile LASSO with changepoints in panel data models applied to option\n  pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panel data are modern statistical tools which are commonly used in all kinds\nof econometric problems under various regularity assumptions. The panel data\nmodels with changepoints are introduced together with atomic pursuit methods\nand they are applied to estimate the underlying option price function. Robust\nestimates and complex insight into the data are both achieved by adopting the\nquantile LASSO approach. The final model is produced in a fully data-driven\nmanner in just one single modeling step. In addition, the arbitrage-free\nscenarios are obtained by introducing a set of well defined linear constraints.\nThe final estimate is, under some reasonable assumptions, consistent with\nrespect to the model estimation and the changepoint detection performance. The\nfinite sample properties are investigated in a simulation study and proposed\nmethodology is applied for the Apple call option pricing problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 10:42:06 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Maciak", "Mat\u00fa\u0161", ""]]}, {"id": "1909.10279", "submitter": "Alec Koppel", "authors": "Alec Koppel, Amrit Singh Bedi, Brian M. Sadler, and Victor Elvira", "title": "Nearly Consistent Finite Particle Estimates in Streaming Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian inference, we seek to compute information about random variables\nsuch as moments or quantiles on the basis of {available data} and prior\ninformation. When the distribution of random variables is {intractable}, Monte\nCarlo (MC) sampling is usually required. {Importance sampling is a standard MC\ntool that approximates this unavailable distribution with a set of weighted\nsamples.} This procedure is asymptotically consistent as the number of MC\nsamples (particles) go to infinity. However, retaining infinitely many\nparticles is intractable. Thus, we propose a way to only keep a \\emph{finite\nrepresentative subset} of particles and their augmented importance weights that\nis \\emph{nearly consistent}. To do so in {an online manner}, we (1) embed the\nposterior density estimate in a reproducing kernel Hilbert space (RKHS) through\nits kernel mean embedding; and (2) sequentially project this RKHS element onto\na lower-dimensional subspace in RKHS using the maximum mean discrepancy, an\nintegral probability metric. Theoretically, we establish that this scheme\nresults in a bias determined by a compression parameter, which yields a tunable\ntradeoff between consistency and memory. In experiments, we observe the\ncompressed estimates achieve comparable performance to the dense ones with\nsubstantial reductions in representational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:06:15 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 16:51:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Koppel", "Alec", ""], ["Bedi", "Amrit Singh", ""], ["Sadler", "Brian M.", ""], ["Elvira", "Victor", ""]]}, {"id": "1909.10457", "submitter": "Nikolai Leonenko", "authors": "A.V. Ivanov, N.N. Leonenko, I.V. Orlovskyi", "title": "On the Whittle estimator for linear random noise spectral density\n  parameter in continuous-time nonlinear regression models", "comments": "38 pages", "journal-ref": "Statistical Inference for Stochastic Processes, 2019", "doi": "10.1007/s11203-019-09206-z", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A continuous-time nonlinear regression model with L\\'evy-driven linear noise\nprocess is considered. Sufficient conditions of consistency and asymptotic\nnormality of the Whittle estimator for the parameter of the noise spectral\ndensity are obtained in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 16:19:30 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ivanov", "A. V.", ""], ["Leonenko", "N. N.", ""], ["Orlovskyi", "I. V.", ""]]}, {"id": "1909.10673", "submitter": "Rajat Talak", "authors": "Rajat Talak, Sertac Karaman, and Eytan Modiano", "title": "A Theory of Uncertainty Variables for State Estimation and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new framework of uncertainty variables to model uncertainty. An\nuncertainty variable is characterized by an uncertainty set, in which its\nrealization is bound to lie, while the conditional uncertainty is characterized\nby a set map, from a given realization of a variable to a set of possible\nrealizations of another variable. We prove Bayes' law and the law of total\nprobability equivalents for uncertainty variables. We define a notion of\nindependence, conditional independence, and pairwise independence for a\ncollection of uncertainty variables, and show that this new notion of\nindependence preserves the properties of independence defined over random\nvariables. We then develop a graphical model, namely Bayesian uncertainty\nnetwork, a Bayesian network equivalent defined over a collection of uncertainty\nvariables, and show that all the natural conditional independence properties,\nexpected out of a Bayesian network, hold for the Bayesian uncertainty network.\nWe also define the notion of point estimate, and show its relation with the\nmaximum a posteriori estimate. Probability theory starts with a distribution\nfunction (equivalently a probability measure) as a primitive and builds all\nother useful concepts, such as law of total probability, Bayes' law,\nindependence, graphical models, point estimate, on it. Our work shows that it\nis perfectly possible to start with a set, instead of a distribution function,\nand retain all the useful ideas needed for state estimation and inference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:31:32 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 18:46:08 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Talak", "Rajat", ""], ["Karaman", "Sertac", ""], ["Modiano", "Eytan", ""]]}, {"id": "1909.10734", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar, Prashant Jha and Prabrisha Rakhshit", "title": "The Trimmed Mean in Non-parametric Regression Function Estimation", "comments": "37 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies a trimmed version of the Nadaraya-Watson estimator to\nestimate the unknown non-parametric regression function. The characterization\nof the estimator through minimization problem is established, and its pointwise\nasymptotic distribution is also derived. The robustness property of the\nproposed estimator is also studied through breakdown point. Besides, the\nasymptotic efficiency study along with an extensive simulation study shows that\nthis estimator performs well for various cases. The practicability of the\nestimator is shown for three benchmark real data as well.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 06:55:10 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 10:55:56 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Rakhshit", "Prabrisha", ""]]}, {"id": "1909.10739", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar, Prashant Jha and Aranyak Acharyya", "title": "On Variable Screening in Multiple Nonparametric Regression Model", "comments": "There were several modifications needed in Sections 3, 4, and\n  Appendix, due to errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the problem of variable screening in multiple\nnonparametric regression model. The proposed methodology is based on the fact\nthat the partial derivative of the regression function with respect to the\nirrelevant variable should be negligible. The Statistical property of the\nproposed methodology is investigated under both cases : (i) when the variance\nof the error term is known, and (ii) when the variance of the error term is\nunknown. Moreover, we establish the practicality of our proposed methodology\nfor various simulated and real data related to interdisciplinary sciences such\nas Economics, Finance and other sciences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 07:23:09 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 14:06:02 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 15:12:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Acharyya", "Aranyak", ""]]}, {"id": "1909.10787", "submitter": "Cassandra Milbradt", "authors": "Cassandra Milbradt, Martin Wahl", "title": "High-probability bounds for the reconstruction error of PCA", "comments": "10 pages, to appear in Statistics & Probability Letters", "journal-ref": null, "doi": "10.1016/j.spl.2020.108741", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive high-probability bounds for the reconstruction error of PCA in\ninfinite dimensions. We apply our bounds in the case that the eigenvalues of\nthe covariance operator satisfy polynomial or exponential upper bounds.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:10:35 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 16:04:10 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Milbradt", "Cassandra", ""], ["Wahl", "Martin", ""]]}, {"id": "1909.10828", "submitter": "Rajen Shah", "authors": "Rajen D. Shah and Peter B\\\"uhlmann", "title": "Double-estimation-friendly inference for high-dimensional misspecified\n  models", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All models may be wrong---but that is not necessarily a problem for\ninference. Consider the standard $t$-test for the significance of a variable\n$X$ for predicting response $Y$ whilst controlling for $p$ other covariates $Z$\nin a random design linear model. This yields correct asymptotic type~I error\ncontrol for the null hypothesis that $X$ is conditionally independent of $Y$\ngiven $Z$ under an \\emph{arbitrary} regression model of $Y$ on $(X, Z)$,\nprovided that a linear regression model for $X$ on $Z$ holds. An analogous\nrobustness to misspecification, which we term the \"double-estimation-friendly\"\n(DEF) property, also holds for Wald tests in generalised linear models, with\nsome small modifications.\n  In this expository paper we explore this phenomenon, and propose methodology\nfor high-dimensional regression settings that respects the DEF property. We\nadvocate specifying (sparse) generalised linear regression models for both $Y$\nand the covariate of interest $X$; our framework gives valid inference for the\nconditional independence null if either of these hold. In the special case\nwhere both specifications are linear, our proposal amounts to a small\nmodification of the popular debiased Lasso test. We also investigate\nconstructing confidence intervals for the regression coefficient of $X$ via\ninverting our tests; these have coverage guarantees even in partially linear\nmodels where the contribution of $Z$ to $Y$ can be arbitrary. Numerical\nexperiments demonstrate the effectiveness of the methodology.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:55:37 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Shah", "Rajen D.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1909.10960", "submitter": "Tino Werner", "authors": "Tino Werner, Peter Ruckdeschel", "title": "The column measure and Gradient-Free Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse model selection by structural risk minimization leads to a set of a\nfew predictors, ideally a subset of the true predictors. This selection clearly\ndepends on the underlying loss function $\\tilde L$. For linear regression with\nsquare loss, the particular (functional) Gradient Boosting variant\n$L_2-$Boosting excels for its computational efficiency even for very large\npredictor sets, while still providing suitable estimation consistency. For more\ngeneral loss functions, functional gradients are not always easily accessible\nor, like in the case of continuous ranking, need not even exist. To close this\ngap, starting from column selection frequencies obtained from $L_2-$Boosting,\nwe introduce a loss-dependent ''column measure'' $\\nu^{(\\tilde L)}$ which\nmathematically describes variable selection. The fact that certain variables\nrelevant for a particular loss $\\tilde L$ never get selected by $L_2-$Boosting\nis reflected by a respective singular part of $\\nu^{(\\tilde L)}$ w.r.t.\n$\\nu^{(L_2)}$. With this concept at hand, it amounts to a suitable change of\nmeasure (accounting for singular parts) to make $L_2-$Boosting select variables\naccording to a different loss $\\tilde L$. As a consequence, this opens the\nbridge to applications of simulational techniques such as various resampling\ntechniques, or rejection sampling, to achieve this change of measure in an\nalgorithmic way.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:42:45 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Werner", "Tino", ""], ["Ruckdeschel", "Peter", ""]]}, {"id": "1909.11062", "submitter": "Anna Little", "authors": "Matthew Hirn, Anna Little", "title": "Wavelet invariants for statistically robust multi-reference alignment", "comments": "59 pages, 8 figures. v3 replaces v2 and is an extensive revision.\n  Revisions include additional background and motivation, additional context\n  relating the approach to other methods, a discussion of stability, and\n  improved presentation. Code reproducing all numerical results is available at\n  https://bitbucket.org/annavlittle/code_wavelet_invariants/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonlinear, wavelet based signal representation that is\ntranslation invariant and robust to both additive noise and random dilations.\nMotivated by the multi-reference alignment problem and generalizations thereof,\nwe analyze the statistical properties of this representation given a large\nnumber of independent corruptions of a target signal. We prove the nonlinear\nwavelet based representation uniquely defines the power spectrum but allows for\nan unbiasing procedure that cannot be directly applied to the power spectrum.\nAfter unbiasing the representation to remove the effects of the additive noise\nand random dilations, we recover an approximation of the power spectrum by\nsolving a convex optimization problem, and thus reduce to a phase retrieval\nproblem. Extensive numerical experiments demonstrate the statistical robustness\nof this approximation procedure.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:21:31 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 19:03:58 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 12:43:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hirn", "Matthew", ""], ["Little", "Anna", ""]]}, {"id": "1909.11298", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Alexander Cloninger", "title": "Classification Logit Two-sample Testing by Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of generative adversarial networks and variational\nlearning suggests training a classifier network may work well in addressing the\nclassical two-sample problem. Network-based tests have the computational\nadvantage that the algorithm scales to large samples. This paper proposes a\ntwo-sample statistic which is the difference of the logit function, provided by\na trained classification neural network, evaluated on the testing set split of\nthe two datasets. Theoretically, we prove the testing power to differentiate\ntwo sub-exponential densities given that the network is sufficiently\nparametrized. When the two densities lie on or near to low-dimensional\nmanifolds embedded in possibly high-dimensional space, the needed network\ncomplexity is reduced to only scale with the intrinsic dimensionality. Both the\napproximation and estimation error analysis are based on a new result of\nnear-manifold integral approximation. In experiments, the proposed method\ndemonstrates better performance than previous network-based tests using\nclassification accuracy as the two-sample statistic, and compares favorably to\ncertain kernel maximum mean discrepancy tests on synthetic datasets and\nhand-written digit datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 05:55:28 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 20:16:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Cloninger", "Alexander", ""]]}, {"id": "1909.11392", "submitter": "Lionel Truquet", "authors": "Zinsou Max Debaly and Lionel Truquet", "title": "Stationarity and Moment Properties of some Multivariate Count\n  Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stationarity and moments properties of some count time series models\nfrom contraction and stability properties of iterated random maps. Both\nunivariate and multivariate processes are considered, including the recent\nmultivariate count time series models introduced recently by Doukhan et al.\n(2017). We improve many existing results by providing optimal stationarity\nconditions or conditions ensuring existence of some exponential moments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 10:36:30 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Debaly", "Zinsou Max", ""], ["Truquet", "Lionel", ""]]}, {"id": "1909.11564", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Analytical confidence intervals for the number of different objects in\n  data streams", "comments": "accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops a new mathematical-statistical approach to analyze a\nclass of Flajolet-Martin algorithms (FMa), and provides analytical confidence\nintervals for the number F0 of distinct elements in a stream, based on Chernoff\nbounds. The class of FMa has reached a significant popularity in bigdata stream\nlearning, and the attention of the literature has mainly been based on\nalgorithmic aspects, basically complexity optimality, while the statistical\nanalysis of these class of algorithms has been often faced heuristically. The\nanalysis provided here shows deep connections with mathematical special\nfunctions and with extreme value theory. The latter connection may help in\nexplaining heuristic considerations, while the first opens many numerical\nissues, faced at the end of the present paper. Finally, the algorithms are\ntested on an anonymized real data stream and MonteCarlo simulations are\nprovided to support our analytical choice in this context.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:46:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 10:37:18 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 16:50:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1909.11716", "submitter": "Asgar Jamneshan", "authors": "T. \\\"O. \\c{C}elik, A. Jamneshan, G. Mont\\'ufar, B. Sturmfels, L.\n  Venturello", "title": "Optimal Transport to a Variety", "comments": "17 pages, 5 figures and 2 tables; v2: minor modifications following\n  referee reports, corrected minor computational mistakes, improved\n  presentation and added further explanations", "journal-ref": "MACIS 2019", "doi": null, "report-no": null, "categories": "math.OC math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the Wasserstein distance between a\nprobability distribution and an algebraic variety. We consider the setting of\nfinite state spaces and describe the solution depending on the choice of the\nground metric and the given distribution. The Wasserstein distance between the\ndistribution and the variety is the minimum of a linear functional over a union\nof transportation polytopes. We obtain a description in terms of the solutions\nof a finite number of systems of polynomial equations. The case analysis is\nbased on the ground metric. A detailed analysis is given for the two bit\nindependence model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:59:10 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 16:35:37 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["\u00c7elik", "T. \u00d6.", ""], ["Jamneshan", "A.", ""], ["Mont\u00fafar", "G.", ""], ["Sturmfels", "B.", ""], ["Venturello", "L.", ""]]}, {"id": "1909.11734", "submitter": "Hau-tieng Wu", "authors": "Xiucai Ding and Hau-Tieng Wu", "title": "On the spectral property of kernel-based sensor fusion algorithms of\n  high dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply local laws of random matrices and free probability theory to study\nthe spectral properties of two kernel-based sensor fusion algorithms,\nnonparametric canonical correlation analysis (NCCA) and alternating diffusion\n(AD), for two simultaneously recorded high dimensional datasets under the null\nhypothesis. The matrix of interest is the product of the kernel matrices\nassociated with the databsets, which may not be diagonalizable in general. We\nprove that in the regime where dimensions of both random vectors are comparable\nto the sample size, if NCCA and AD are conducted using a smooth kernel\nfunction, then the first few nontrivial eigenvalues will converge to real\ndeterministic values provided the datasets are independent Gaussian random\nvectors. Toward the claimed result, we also provide a convergence rate of\neigenvalues of a kernel affinity matrix.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 19:53:46 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 18:53:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ding", "Xiucai", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1909.11773", "submitter": "Dana Yang", "authors": "David Pollard and Dana Yang", "title": "Rapid mixing of a Markov chain for an exponentially weighted aggregation\n  estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-Hastings method is often used to construct a Markov chain with\na given $\\pi$ as its stationary distribution. The method works even if $\\pi$ is\nknown only up to an intractable constant of proportionality. Polynomial time\nconvergence results for such chains (rapid mixing) are hard to obtain for high\ndimensional probability models where the size of the state space potentially\ngrows exponentially with the model dimension. In a Bayesian context, Yang,\nWainwright, and Jordan (2016) (=YWJ) used the path method to prove rapid mixing\nfor high dimensional linear models. This paper proposes a modification of the\nYWJ approach that simplifies the theoretical argument and improves the rate of\nconvergence. The new approach is illustrated by an application to an\nexponentially weighted aggregation estimator.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 21:07:05 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pollard", "David", ""], ["Yang", "Dana", ""]]}, {"id": "1909.12112", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Compound vectors of subordinators and their associated positive L\\'evy\n  copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L\\'evy copulas are an important tool which can be used to build dependent\nL\\'evy processes. In a classical setting, they have been used to model\nfinancial applications. In a Bayesian framework they have been employed to\nintroduce dependent nonparametric priors which allow to model heterogeneous\ndata. This paper focuses on introducing a new class of L\\'evy copulas based on\na class of subordinators recently appeared in the literature, called\n\\textit{Compound Random Measures}. The well-known Clayton L\\'evy copula is a\nspecial case of this new class. Furthermore, we provide some novel results\nabout the underlying vector of subordinators such as a series representation\nand relevant moments. The article concludes with an application to a Danish\nfire dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:00:53 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 09:57:54 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 06:55:30 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1909.12218", "submitter": "Timo Klock", "authors": "Zeljko Kereta, Timo Klock", "title": "Estimating covariance and precision matrices along subspaces", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": "10.1214/20-EJS1782", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the accuracy of estimating the covariance and the precision matrix\nof a $D$-variate sub-Gaussian distribution along a prescribed subspace or\ndirection using the finite sample covariance. Our results show that the\nestimation accuracy depends almost exclusively on the components of the\ndistribution that correspond to desired subspaces or directions. This is\nrelevant and important for problems where the behavior of data along a\nlower-dimensional space is of specific interest, such as dimension reduction or\nstructured regression problems. We also show that estimation of precision\nmatrices is almost independent of the condition number of the covariance\nmatrix. The presented applications include direction-sensitive eigenspace\nperturbation bounds, relative bounds for the smallest eigenvalue, and the\nestimation of the single-index model. For the latter, a new estimator, derived\nfrom the analysis, with strong theoretical guarantees and superior numerical\nperformance is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:15:49 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 11:55:04 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 15:34:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kereta", "Zeljko", ""], ["Klock", "Timo", ""]]}, {"id": "1909.12237", "submitter": "Ruobin Gong", "authors": "Ruobin Gong", "title": "Exact Inference with Approximate Computation for Differentially Private\n  Data via Perturbations", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy protects individuals' confidential information by\nsubjecting data summaries to probabilistic perturbation mechanisms, carefully\ndesigned to minimize undue sacrifice of statistical efficiency. When properly\naccounted for, differentially private data are conducive to exact inference\nwhen approximate computation techniques are employed. This paper shows that\napproximate Bayesian computation, a practical suite of methods to simulate from\napproximate posterior distributions of complex Bayesian models, produces exact\nposterior samples when applied to differentially private perturbation data. An\nimportance sampling implementation of Monte Carlo expectation-maximization for\nlikelihood inference is also discussed. The results illustrate a duality\nbetween approximate computation on exact data, and exact computation on\napproximate data. A cleverly designed inferential procedure exploits the\nalignment between the statistical tradeoff of privacy versus efficiency, and\nthe computational tradeoff of approximation versus exactness, so that paying\nthe cost of one gains the benefit of both.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:34:18 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 18:18:36 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gong", "Ruobin", ""]]}, {"id": "1909.12239", "submitter": "Kam\\'elia Daudel", "authors": "Kam\\'elia Daudel, Randal Douc, Fran\\c{c}ois Portier, Fran\\c{c}ois\n  Roueff", "title": "The $f$-Divergence Expectation Iteration Scheme", "comments": "This content ended up being split into the papers arXiv:2005.10618\n  and arXiv:2103.05684, which correspond to two separate and more in-depth\n  approaches", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the $f$-EI$(\\phi)$ algorithm, a novel iterative\nalgorithm which operates on measures and performs $f$-divergence minimisation\nin a Bayesian framework. We prove that for a rich family of values of\n$(f,\\phi)$ this algorithm leads at each step to a systematic decrease in the\n$f$-divergence and show that we achieve an optimum. In the particular case\nwhere we consider a weighted sum of Dirac measures and the $\\alpha$-divergence,\nwe obtain that the calculations involved in the $f$-EI$(\\phi)$ algorithm\nsimplify to gradient-based computations. Empirical results support the claim\nthat the $f$-EI$(\\phi)$ algorithm serves as a powerful tool to assist\nVariational methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:34:54 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:09:32 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Daudel", "Kam\u00e9lia", ""], ["Douc", "Randal", ""], ["Portier", "Fran\u00e7ois", ""], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "1909.12378", "submitter": "Thiago do R\\^ego Sousa", "authors": "Thiago do R\\^ego Sousa and Robert Stelzer", "title": "Moment based estimation for the multivariate COGARCH(1,1) process", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For the multivariate COGARCH process, we obtain explicit expressions for the\nsecond-order structure of the \"squared returns\" process observed on an\nequidistant grid. Based on this, we present a generalized method of moments\nestimator for its parameters. Under appropriate moment and strong mixing\nconditions, we show that the resulting estimator is consistent and\nasymptotically normal. Sufficient conditions for strong mixing, stationarity\nand identifiability of the model parameters are discussed in detail. We\ninvestigate the finite sample behavior of the estimator in a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:39:24 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 11:38:41 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Sousa", "Thiago do R\u00eago", ""], ["Stelzer", "Robert", ""]]}, {"id": "1909.12624", "submitter": "Bruno Ebner", "authors": "Philip D\\\"orr, Bruno Ebner, Norbert Henze", "title": "Testing multivariate normality by zeros of the harmonic oscillator in\n  characteristic function spaces", "comments": "29 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel class of affine invariant and consistent tests for normality\nin any dimension. The tests are based on a characterization of the standard\n$d$-variate normal distribution as the unique solution of an initial value\nproblem of a partial differential equation motivated by the harmonic\noscillator, which is a special case of a Schr\\\"odinger operator. We derive the\nasymptotic distribution of the test statistics under the hypothesis of\nnormality as well as under fixed and contiguous alternatives. The tests are\nconsistent against general alternatives, exhibit strong power performance for\nfinite samples, and they are applied to a classical data set due to R.A.\nFisher. The results can also be used for a neighborhood-of-model validation\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 11:20:52 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["D\u00f6rr", "Philip", ""], ["Ebner", "Bruno", ""], ["Henze", "Norbert", ""]]}, {"id": "1909.12710", "submitter": "Jan van Waaij PhD", "authors": "Jan van Waaij", "title": "Adaptive posterior contraction rates for empirical Bayesian drift\n  estimation of a diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to their conjugate posteriors, Gaussian process priors are attractive for\nestimating the drift of stochastic differential equations with continuous time\nobservations. However, their performance strongly depends on the choice of the\nhyper-parameters. We employ the marginal maximum likelihood estimator to\nestimate the scaling and/or smoothness parameter(s) of the prior and show that\nthe corresponding posterior has optimal rates of convergence. General theorems\ndo not apply directly to this model as the usual test functions are with\nrespect to a random Hellinger-type metric. We allow for continuous and\ndiscrete, one- and two-dimensional sets of hyper-parameters, where optimising\nover the two-dimensional set of smoothness and scaling hyper-parameters is\nshown to be beneficial in terms of the adaptive range.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 14:31:38 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 15:07:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["van Waaij", "Jan", ""]]}, {"id": "1909.13031", "submitter": "Sarath Yasodharan", "authors": "Sarath Yasodharan, Patrick Loiseau", "title": "Nonzero-sum Adversarial Hypothesis Testing Games", "comments": "23 pages, 14 figures. Accepted for publication in the 33rd Conference\n  on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonzero-sum hypothesis testing games that arise in the context of\nadversarial classification, in both the Bayesian as well as the Neyman-Pearson\nframeworks. We first show that these games admit mixed strategy Nash\nequilibria, and then we examine some interesting concentration phenomena of\nthese equilibria. Our main results are on the exponential rates of convergence\nof classification errors at equilibrium, which are analogous to the well-known\nChernoff-Stein lemma and Chernoff information that describe the error exponents\nin the classical binary hypothesis testing problem, but with parameters derived\nfrom the adversarial model. The results are validated through numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 05:46:48 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yasodharan", "Sarath", ""], ["Loiseau", "Patrick", ""]]}, {"id": "1909.13286", "submitter": "Mohd Arshad", "authors": "Qazi Azhad Jamal, Mohd. Arshad, Nancy Khandelwal", "title": "Multicomponent stress strength reliability estimation for Pareto\n  distribution based on upper record values", "comments": "31 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, inferences about the multicomponent stress strength\nreliability are drawn under the assumption that strength and stress follow\nindependent Pareto distribution with different shapes $(\\alpha_1,\\alpha_2)$ and\ncommon scale parameter $\\theta$. The maximum likelihood estimator, Bayes\nestimator under squared error and Linear exponential loss function, of\nmulticomponent stress-strength reliability are constructed with corresponding\nhighest posterior density interval for unknown $\\theta.$ For known $\\theta,$\nuniformly minimum variance unbiased estimator and asymptotic distribution of\nmulticomponent stress-strength reliability with asymptotic confidence interval\nis discussed. Also, various Bootstrap confidence intervals are constructed. A\nsimulation study is conducted to numerically compare the performances of\nvarious estimators of multicomponent stress-strength reliability. Finally, a\nreal life example is presented to show the applications of derived results in\nreal life scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 14:00:57 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Jamal", "Qazi Azhad", ""], ["Arshad", "Mohd.", ""], ["Khandelwal", "Nancy", ""]]}, {"id": "1909.13339", "submitter": "Badr-Eddine Ch\\'erief-Abdellatif", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier", "title": "MMD-Bayes: Robust Bayesian Estimation via Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some misspecified settings, the posterior distribution in Bayesian\nstatistics may lead to inconsistent estimates. To fix this issue, it has been\nsuggested to replace the likelihood by a pseudo-likelihood, that is the\nexponential of a loss function enjoying suitable robustness properties. In this\npaper, we build a pseudo-likelihood based on the Maximum Mean Discrepancy,\ndefined via an embedding of probability distributions into a reproducing kernel\nHilbert space. We show that this MMD-Bayes posterior is consistent and robust\nto model misspecification. As the posterior obtained in this way might be\nintractable, we also prove that reasonable variational approximations of this\nposterior enjoy the same properties. We provide details on a stochastic\ngradient algorithm to compute these variational approximations. Numerical\nsimulations indeed suggest that our estimator is more robust to\nmisspecification than the ones based on the likelihood.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 18:49:05 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 17:51:01 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1909.13477", "submitter": "Dali Liu", "authors": "Dali Liu, Zheng Li, Hanchao Wang, Zengjing Chen", "title": "Non-uniform Berry-Esseen Bound by Unbounded Exchangeable Pair Approach", "comments": "34pages, we reviewed the paper and corrected some typos. All comments\n  are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new technique is introduced to obtain non-uniform\nBerry-Esseen bounds of normal and nonnormal approximation for unbounded\nexchangeable pairs. This technique does not rely on the concentration\ninequalities developed by Chen and Shao \\cite{cls1, cls2} and can be applied to\nthe quadratic forms, general Curie-Weiss model and an independence test. In\nparticular, our non-uniform result about the independence test is under 6th\nmoment condition, while the uniform bound in Chen and Shao \\cite{cs2} requires\n24th moment condition.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:44:28 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 10:55:02 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 02:31:45 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 11:53:41 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Liu", "Dali", ""], ["Li", "Zheng", ""], ["Wang", "Hanchao", ""], ["Chen", "Zengjing", ""]]}, {"id": "1909.13479", "submitter": "Luke Prendergast", "authors": "Luke A. Prendergast and Jodie A. Smith", "title": "Influence functions for Linear Discriminant Analysis: Sensitivity\n  analysis and efficient influence diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst influence functions for linear discriminant analysis (LDA) have been\nfound for a single discriminant when dealing with two groups, until now these\nhave not been derived in the setting of a general number of groups. In this\npaper we explore the relationship between Sliced Inverse Regression (SIR) and\nLDA, and exploit this relationship to develop influence functions for LDA from\nthose already derived for SIR. These influence functions can be used to\nunderstand robustness properties of LDA and also to detect influential\nobservations in practice. We illustrate the usefulness of these via their\napplication to a real data set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:53:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Smith", "Jodie A.", ""]]}, {"id": "1909.13499", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LMO, CELESTE)", "title": "Rejoinder on: Minimal penalties and the slope heuristics: a survey", "comments": null, "journal-ref": "Journal de la Societe Fran{\\c c}aise de Statistique, Societe\n  Fran{\\c c}aise de Statistique et Societe Mathematique de France, Vol 106,\n  No.3, 158-168. 2019", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is the rejoinder following the discussion of a survey paper about\nminimal penalties and the slope heuristics (Arlot, 2019. Minimal penalties and\nthe slope heuristics: a survey. Journal de la SFDS). While commenting on the\nremarks made by the discussants, it provides two new results about the slope\nheuristics for model selection among a collection of projection estimators in\nleast-squares fixed-design regression. First, we prove that the slope\nheuristics works even when all models are significantly biased. Second, when\nthe noise is Gaussian with a general dependence structure, we compute\nexpectations of key quantities, showing that the slope heuristics certainly is\nvalid in this setting also.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:04:46 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Arlot", "Sylvain", "", "LMO, CELESTE"]]}, {"id": "1909.13557", "submitter": "Yusuke Kaino", "authors": "Yusuke Kaino and Masayuki Uchida", "title": "Parametric estimation for a parabolic linear SPDE model based on sampled\n  data", "comments": "38 pages, 73 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric estimation for a parabolic linear second order\nstochastic partial differential equation (SPDE) from high frequency data which\nare observed in time and space. By using thinned data obtained from the high\nfrequency data, adaptive estimators of the coefficient parameters including the\nvolatility parameter of a parabolic linear SPDE model are proposed. Moreover,\nwe give some examples and simulation results of the adaptive estimators of the\nSPDE model based on the high frequency data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 09:44:58 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kaino", "Yusuke", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1909.13602", "submitter": "Arnaud Guyader", "authors": "Qiming Du and Arnaud Guyader", "title": "Variance Estimation in Adaptive Sequential Monte Carlo", "comments": "42 pages v2: some minor changes and two appendices added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods represent a classical set of techniques\nto simulate a sequence of probability measures through a simple\nselection/mutation mechanism. However, the associated selection functions and\nmutation kernels usually depend on tuning parameters that are of first\nimportance for the efficiency of the algorithm. A standard way to address this\nproblem is to apply Adaptive Sequential Monte Carlo (ASMC) methods, which\nconsist in exploiting the information given by the history of the sample to\ntune the parameters. This article is concerned with variance estimation in such\nASMC methods. Specifically, we focus on the case where the asymptotic variance\ncoincides with the one of the \"limiting\" Sequential Monte Carlo algorithm as\ndefined by Beskos et al. (2016). We prove that, under natural assumptions, the\nestimator introduced by Lee and Whiteley (2018) in the nonadaptive case (i.e.,\nSMC) is also a consistent estimator of the asymptotic variance for ASMC\nmethods. To do this, we introduce a new estimator that is expressed in terms of\ncoalescent tree-based measures, and explain its connection with the previous\none. Our estimator is constructed by tracing the genealogy of the associated\nInteracting Particle System. The tools we use connect the study of Particle\nMarkov Chain Monte Carlo methods and the variance estimation problem in SMC\nmethods. As such, they may give some new insights when dealing with complex\ngenealogy-involved problems of Interacting Particle Systems in more general\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:40:21 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 12:34:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Du", "Qiming", ""], ["Guyader", "Arnaud", ""]]}, {"id": "1909.13702", "submitter": "Bernhard Stankewitz", "authors": "Bernhard Stankewitz", "title": "Smoothed residual stopping for statistical inverse problems via\n  truncated SVD estimation", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines under what circumstances adaptivity for truncated SVD\nestimation can be achieved by an early stopping rule based on the smoothed\nresiduals $ \\| ( A A^{\\top} )^{\\alpha / 2} ( Y - A \\hat{\\mu}^{( m )}) \\|^{2} $.\nLower and upper bounds for the risk are derived, which show that moderate\nsmoothing of the residuals can be used to adapt over classes of signals with\nvarying smoothness, while oversmoothing yields suboptimal convergence rates.\nThe theoretical results are illustrated by Monte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:52:51 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 23:55:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Stankewitz", "Bernhard", ""]]}, {"id": "1909.13727", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and Arnold Janssen", "title": "Dependence correction of multiple tests with applications to sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper establishes new multiple procedures for simultaneous\ntesting of a large number of hypotheses under dependence. Special attention is\ndevoted to experiments with rare false hypotheses. This sparsity assumption is\ntypically for various genome studies when a portion of remarkable genes should\nbe detected. The aim is to derive tests which control the false discovery rate\n(FDR) always at finite sample size. The procedures are compared for the set up\nof dependent and independent $p$-values. It turns out that the FDR bounds\ndiffer by a dependency factor which can be used as a correction quantity. We\noffer sparsity modifications and improved dependence tests which generalize the\nBenjamini-Yekutieli test and adaptive tests in the sense of Storey. As a\nbyproduct, an early stopped test is presented in order to bound the number of\nrejections. The new procedures perform well for real genome data examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:14:37 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Janssen", "Arnold", ""]]}]