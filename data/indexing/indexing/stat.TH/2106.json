[{"id": "2106.00097", "submitter": "Helton Saulo", "authors": "Roberto Vila, Helton Saulo and Jamer Roldan", "title": "On some properties of the bimodal normal distribution and its bivariate\n  version", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we derive some novel properties of the bimodal normal\ndistribution. Some of its mathematical properties are examined. We provide a\nformal proof for the bimodality and assess identifiability. We then discuss the\nmaximum likelihood estimates as well as the existence of these estimates, and\nalso some asymptotic properties of the estimator of the parameter that controls\nthe bimodality. A bivariate version of the BN distribution is derived and some\ncharacteristics such as covariance and correlation are analyzed. We study\nstationarity and ergodicity and a triangular array central limit theorem.\nFinally, a Monte Carlo study is carried out for evaluating the performance of\nthe maximum likelihood estimates.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 20:52:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Vila", "Roberto", ""], ["Saulo", "Helton", ""], ["Roldan", "Jamer", ""]]}, {"id": "2106.00164", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla", "title": "Median bias of M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we derive bounds on the median bias of univariate M-estimators\nunder mild regularity conditions. These requirements are not sufficient to\nimply convergence in distribution of the M-estimators. We also discuss median\nbias of some multivariate M-estimators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:15:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2106.00418", "submitter": "Aur\\'elien Bibaut", "authors": "Aur\\'elien Bibaut and Antoine Chambaz and Maria Dimakopoulou and\n  Nathan Kallus and Mark van der Laan", "title": "Post-Contextual-Bandit Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms are increasingly replacing non-adaptive A/B\ntests in e-commerce, healthcare, and policymaking because they can both improve\noutcomes for study participants and increase the chance of identifying good or\neven best policies. To support credible inference on novel interventions at the\nend of the study, nonetheless, we still want to construct valid confidence\nintervals on average treatment effects, subgroup effects, or value of new\npolicies. The adaptive nature of the data collected by contextual bandit\nalgorithms, however, makes this difficult: standard estimators are no longer\nasymptotically normally distributed and classic confidence intervals fail to\nprovide correct coverage. While this has been addressed in non-contextual\nsettings by using stabilized estimators, the contextual setting poses unique\nchallenges that we tackle for the first time in this paper. We propose the\nContextual Adaptive Doubly Robust (CADR) estimator, the first estimator for\npolicy value that is asymptotically normal under contextual adaptive data\ncollection. The main technical challenge in constructing CADR is designing\nadaptive and consistent conditional standard deviation estimators for\nstabilization. Extensive numerical experiments using 57 OpenML datasets\ndemonstrate that confidence intervals based on CADR uniquely provide correct\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:01:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bibaut", "Aur\u00e9lien", ""], ["Chambaz", "Antoine", ""], ["Dimakopoulou", "Maria", ""], ["Kallus", "Nathan", ""], ["van der Laan", "Mark", ""]]}, {"id": "2106.00453", "submitter": "Mehmet Siddik Cadirci", "authors": "Mehmet Siddik Cadirci, Dafydd Evans, Nikolai Leonenko, Oleg Seleznjev", "title": "Statistical tests based on R\\'{e}nyi entropy estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entropy and its various generalizations are important in many fields,\nincluding mathematical statistics, communication theory, physics and computer\nscience, for characterizing the amount of information associated with a\nprobability distribution. In this paper we propose goodness-of-fit statistics\nfor the multivariate Student and multivariate Pearson type II distributions,\nbased on the maximum entropy principle and a class of estimators for R\\'{e}nyi\nentropy based on nearest neighbour distances. We prove the L^2-consistency of\nthese statistics using results on the subadditivity of Euclidean functionals on\nnearest neighbour graphs, and investigate their rate of convergence and\nasymptotic distribution using Monte Carlo methods. In addition we present a\nnovel iterative method for estimating the shape parameter of the multivariate\nStudent and multivariate Pearson type II distributions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:04:02 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cadirci", "Mehmet Siddik", ""], ["Evans", "Dafydd", ""], ["Leonenko", "Nikolai", ""], ["Seleznjev", "Oleg", ""]]}, {"id": "2106.00560", "submitter": "Vladimir Pastukhov", "authors": "Vladimir Pastukhov", "title": "Stacked Grenander and rearrangement estimators of a discrete\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the stacking of isotonic regression and the method\nof rearrangement with the empirical estimator to estimate a discrete\ndistribution with an infinite support. The estimators are proved to be strongly\nconsistent with $\\sqrt{n}$-rate of convergence. We obtain the asymptotic\ndistributions of the estimators and construct the asymptotically correct\nconservative global confidence bands. We show that stacked Grenander estimator\noutperforms the stacked rearrangement estimator. The new estimators behave well\neven for small sized data sets and provide a trade-off between goodness-of-fit\nand shape constraints.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:19:03 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:10:16 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pastukhov", "Vladimir", ""]]}, {"id": "2106.00616", "submitter": "Stanislav Nagy", "authors": "Petra Laketa and Stanislav Nagy", "title": "Halfspace depth for general measures: The ray basis theorem and its\n  consequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The halfspace depth is a prominent tool of nonparametric multivariate\nanalysis. The upper level sets of the depth, termed the trimmed regions of a\nmeasure, serve as a natural generalization of the quantiles and inter-quantile\nregions to higher-dimensional spaces. The smallest non-empty trimmed region,\ncoined the halfspace median of a measure, generalizes the median. We focus on\nthe (inverse) ray basis theorem for the halfspace depth, a crucial theoretical\nresult that characterizes the halfspace median by a covering property. First, a\nnovel elementary proof of that statement is provided, under minimal assumptions\non the underlying measure. The proof applies not only to the median, but also\nto other trimmed regions. Motivated by the technical development of the amended\nray basis theorem, we specify connections between the trimmed regions, floating\nbodies, and additional equi-affine convex sets related to the depth. As a\nconsequence, minimal conditions for the strict monotonicity of the depth are\nobtained. Applications to the computation of the depth and robust estimation\nare outlined.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:49:09 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Laketa", "Petra", ""], ["Nagy", "Stanislav", ""]]}, {"id": "2106.00896", "submitter": "Jiachun Pan", "authors": "Jiachun Pan, Yonglong Li, Vincent Y. F. Tan", "title": "Asymptotics of Sequential Composite Hypothesis Testing under\n  Probabilistic Constraints", "comments": "The paper was presented in part at the 2021 International Symposium\n  on Information Theory (ISIT). It was submitted to Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential composite binary hypothesis testing problem in\nwhich one of the hypotheses is governed by a single distribution while the\nother is governed by a family of distributions whose parameters belong to a\nknown set $\\Gamma$. We would like to design a test to decide which hypothesis\nis in effect. Under the constraints that the probabilities that the length of\nthe test, a stopping time, exceeds $n$ are bounded by a certain threshold\n$\\epsilon$, we obtain certain fundamental limits on the asymptotic behavior of\nthe sequential test as $n$ tends to infinity. Assuming that $\\Gamma$ is a\nconvex and compact set, we obtain the set of all first-order error exponents\nfor the problem. We also prove a strong converse. Additionally, we obtain the\nset of second-order error exponents under the assumption that $\\mathcal{X}$ is\na finite alphabet. In the proof of second-order asymptotics, a main technical\ncontribution is the derivation of a central limit-type result for a maximum of\nan uncountable set of log-likelihood ratios under suitable conditions. This\nresult may be of independent interest. We also show that some important\nstatistical models satisfy the conditions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:21:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Pan", "Jiachun", ""], ["Li", "Yonglong", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2106.01004", "submitter": "Abdelhakim Necir Necir", "authors": "Saida Mancer, Abdelhakim Necir, Souad Benchaira", "title": "Semiparametric tail-index estimation for randomly right-truncated\n  heavy-tailed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was shown that when one disposes of a parametric information of the\ntruncation distribution, the semiparametric estimator of the distribution\nfunction for truncated data (Wang, 1989) is more efficient than the\nnonparametric one. On the basis of this estimation method, we derive an\nestimator for the tail index of Pareto-type distributions that are randomly\nright-truncated and establish its consistency and asymptotic normality. The\nfinite sample behavior of the proposed estimator is carried out by simulation\nstudy. We point out that, in terms of both bias and root of the mean squared\nerror, our estimator performs better than those based on nonparametric\nestimation methods. An application to a real dataset of induction times of AIDS\ndiseases is given as well.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:57:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mancer", "Saida", ""], ["Necir", "Abdelhakim", ""], ["Benchaira", "Souad", ""]]}, {"id": "2106.01070", "submitter": "Viet Anh Nguyen", "authors": "Nian Si and Karthyek Murthy and Jose Blanchet and Viet Anh Nguyen", "title": "Testing Group Fairness via Optimal Transport Projections", "comments": null, "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a statistical testing framework to detect if a given machine\nlearning classifier fails to satisfy a wide range of group fairness notions.\nThe proposed test is a flexible, interpretable, and statistically rigorous tool\nfor auditing whether exhibited biases are intrinsic to the algorithm or due to\nthe randomness in the data. The statistical challenges, which may arise from\nmultiple impact criteria that define group fairness and which are discontinuous\non model parameters, are conveniently tackled by projecting the empirical\nmeasure onto the set of group-fair probability models using optimal transport.\nThis statistic is efficiently computed using linear programming and its\nasymptotic distribution is explicitly obtained. The proposed framework can also\nbe used to test for testing composite fairness hypotheses and fairness with\nmultiple sensitive attributes. The optimal transport testing formulation\nimproves interpretability by characterizing the minimal covariate perturbations\nthat eliminate the bias observed in the audit.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:51:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Si", "Nian", ""], ["Murthy", "Karthyek", ""], ["Blanchet", "Jose", ""], ["Nguyen", "Viet Anh", ""]]}, {"id": "2106.01092", "submitter": "Henry WJ Reeve", "authors": "Henry W. J. Reeve, Ata Kaban", "title": "Statistical optimality conditions for compressive ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a framework for the theoretical analysis of ensembles of\nlow-complexity empirical risk minimisers trained on independent random\ncompressions of high-dimensional data. First we introduce a general\ndistribution-dependent upper-bound on the excess risk, framed in terms of a\nnatural notion of compressibility. This bound is independent of the dimension\nof the original data representation, and explains the in-built regularisation\neffect of the compressive approach. We then instantiate this general bound to\nclassification and regression tasks, considering Johnson-Lindenstrauss mappings\nas the compression scheme. For each of these tasks, our strategy is to develop\na tight upper bound on the compressibility function, and by doing so we\ndiscover distributional conditions of geometric nature under which the\ncompressive algorithm attains minimax-optimal rates up to at most\npoly-logarithmic factors. In the case of compressive classification, this is\nachieved with a mild geometric margin condition along with a flexible moment\ncondition that is significantly more general than the assumption of bounded\ndomain. In the case of regression with strongly convex smooth loss functions we\nfind that compressive regression is capable of exploiting spectral decay with\nnear-optimal guarantees. In addition, a key ingredient for our central upper\nbound is a high probability uniform upper bound on the integrated deviation of\ndependent empirical processes, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:52:31 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Reeve", "Henry W. J.", ""], ["Kaban", "Ata", ""]]}, {"id": "2106.01121", "submitter": "Motonobu Kanagawa", "authors": "Veit Wild, Motonobu Kanagawa, Dino Sejdinovic", "title": "Connections and Equivalences between the Nystr\\\"om Method and Sparse\n  Variational Gaussian Processes", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the connections between sparse approximation methods for\nmaking kernel methods and Gaussian processes (GPs) scalable to massive data,\nfocusing on the Nystr\\\"om method and the Sparse Variational Gaussian Processes\n(SVGP). While sparse approximation methods for GPs and kernel methods share\nsome algebraic similarities, the literature lacks a deep understanding of how\nand why they are related. This is a possible obstacle for the communications\nbetween the GP and kernel communities, making it difficult to transfer results\nfrom one side to the other. Our motivation is to remove this possible obstacle,\nby clarifying the connections between the sparse approximations for GPs and\nkernel methods. In this work, we study the two popular approaches, the\nNystr\\\"om and SVGP approximations, in the context of a regression problem, and\nestablish various connections and equivalences between them. In particular, we\nprovide an RKHS interpretation of the SVGP approximation, and show that the\nEvidence Lower Bound of the SVGP contains the objective function of the\nNystr\\\"om approximation, revealing the origin of the algebraic equivalence\nbetween the two approaches. We also study recently established convergence\nresults for the SVGP and how they are related to the approximation quality of\nthe Nystr\\\"om method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:45:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wild", "Veit", ""], ["Kanagawa", "Motonobu", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2106.01257", "submitter": "Alexey Naumov", "authors": "Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin\n  Scaman, Hoi-To Wai", "title": "Tight High Probability Bounds for Linear Stochastic Approximation with\n  Fixed Stepsize", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a non-asymptotic analysis of linear stochastic\napproximation (LSA) algorithms with fixed stepsize. This family of methods\narises in many machine learning tasks and is used to obtain approximate\nsolutions of a linear system $\\bar{A}\\theta = \\bar{b}$ for which $\\bar{A}$ and\n$\\bar{b}$ can only be accessed through random estimates $\\{({\\bf A}_n, {\\bf\nb}_n): n \\in \\mathbb{N}^*\\}$. Our analysis is based on new results regarding\nmoments and high probability bounds for products of matrices which are shown to\nbe tight. We derive high probability bounds on the performance of LSA under\nweaker conditions on the sequence $\\{({\\bf A}_n, {\\bf b}_n): n \\in\n\\mathbb{N}^*\\}$ than previous works. However, in contrast, we establish\npolynomial concentration bounds with order depending on the stepsize. We show\nthat our conclusions cannot be improved without additional assumptions on the\nsequence of random matrices $\\{{\\bf A}_n: n \\in \\mathbb{N}^*\\}$, and in\nparticular that no Gaussian or exponential high probability bounds can hold.\nFinally, we pay a particular attention to establishing bounds with sharp order\nwith respect to the number of iterations and the stepsize and whose leading\nterms contain the covariance matrices appearing in the central limit theorems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:10:37 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Naumov", "Alexey", ""], ["Samsonov", "Sergey", ""], ["Scaman", "Kevin", ""], ["Wai", "Hoi-To", ""]]}, {"id": "2106.01529", "submitter": "Alden Green", "authors": "Alden Green, Sivaraman Balakrishnan, Ryan J. Tibshirani", "title": "Minimax Optimal Regression over Sobolev Spaces via Laplacian\n  Regularization on Neighborhood Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study the statistical properties of Laplacian smoothing, a\ngraph-based approach to nonparametric regression. Under standard regularity\nconditions, we establish upper bounds on the error of the Laplacian smoothing\nestimator $\\widehat{f}$, and a goodness-of-fit test also based on\n$\\widehat{f}$. These upper bounds match the minimax optimal estimation and\ntesting rates of convergence over the first-order Sobolev class\n$H^1(\\mathcal{X})$, for $\\mathcal{X}\\subseteq \\mathbb{R}^d$ and $1 \\leq d < 4$;\nin the estimation problem, for $d = 4$, they are optimal modulo a $\\log n$\nfactor. Additionally, we prove that Laplacian smoothing is manifold-adaptive:\nif $\\mathcal{X} \\subseteq \\mathbb{R}^d$ is an $m$-dimensional manifold with $m\n< d$, then the error rate of Laplacian smoothing (in either estimation or\ntesting) depends only on $m$, in the same way it would if $\\mathcal{X}$ were a\nfull-dimensional set in $\\mathbb{R}^d$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:20:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Green", "Alden", ""], ["Balakrishnan", "Sivaraman", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "2106.01564", "submitter": "Nathan Ross", "authors": "A. D. Barbour, Nathan Ross, Guangqu Zheng", "title": "Stein's method, smoothing and functional approximation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method for Gaussian process approximation can be used to bound the\ndifferences between the expectations of smooth functionals $h$ of a c\\`adl\\`ag\nrandom process $X$ of interest and the expectations of the same functionals of\na well understood target random process $Z$ with continuous paths.\nUnfortunately, the class of smooth functionals for which this is easily\npossible is very restricted. Here, we prove an infinite dimensional Gaussian\nsmoothing inequality, which enables the class of functionals to be greatly\nexpanded -- examples are Lipschitz functionals with respect to the uniform\nmetric, and indicators of arbitrary events -- in exchange for a loss of\nprecision in the bounds. Our inequalities are expressed in terms of the smooth\ntest function bound, an expectation of a functional of $X$ that is closely\nrelated to classical tightness criteria, a similar expectation for $Z$, and,\nfor the indicator of a set $K$, the probability $\\mathbb{P}(Z \\in K^\\theta\n\\setminus K^{-\\theta})$ that the target process is close to the boundary of\n$K$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:13:32 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Barbour", "A. D.", ""], ["Ross", "Nathan", ""], ["Zheng", "Guangqu", ""]]}, {"id": "2106.01660", "submitter": "Botao Hao", "authors": "Tor Lattimore, Botao Hao", "title": "Bandit Phase Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a bandit version of phase retrieval where the learner chooses\nactions $(A_t)_{t=1}^n$ in the $d$-dimensional unit ball and the expected\nreward is $\\langle A_t, \\theta_\\star\\rangle^2$ where $\\theta_\\star \\in \\mathbb\nR^d$ is an unknown parameter vector. We prove that the minimax cumulative\nregret in this problem is $\\smash{\\tilde \\Theta(d \\sqrt{n})}$, which improves\non the best known bounds by a factor of $\\smash{\\sqrt{d}}$. We also show that\nthe minimax simple regret is $\\smash{\\tilde \\Theta(d / \\sqrt{n})}$ and that\nthis is only achievable by an adaptive algorithm. Our analysis shows that an\napparently convincing heuristic for guessing lower bounds can be misleading and\nthat uniform bounds on the information ratio for information-directed sampling\nare not sufficient for optimal regret.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:04:33 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 15:52:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Lattimore", "Tor", ""], ["Hao", "Botao", ""]]}, {"id": "2106.01723", "submitter": "Aur\\'elien Bibaut", "authors": "Aur\\'elien Bibaut and Antoine Chambaz and Maria Dimakopoulou and\n  Nathan Kallus and Mark van der Laan", "title": "Risk Minimization from Adaptively Collected Data: Guarantees for\n  Supervised and Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization (ERM) is the workhorse of machine learning,\nwhether for classification and regression or for off-policy policy learning,\nbut its model-agnostic guarantees can fail when we use adaptively collected\ndata, such as the result of running a contextual bandit algorithm. We study a\ngeneric importance sampling weighted ERM algorithm for using adaptively\ncollected data to minimize the average of a loss function over a hypothesis\nclass and provide first-of-their-kind generalization guarantees and fast\nconvergence rates. Our results are based on a new maximal inequality that\ncarefully leverages the importance sampling structure to obtain rates with the\nright dependence on the exploration rate in the data. For regression, we\nprovide fast rates that leverage the strong convexity of squared-error loss.\nFor policy learning, we provide rate-optimal regret guarantees that close an\nopen gap in the existing literature whenever exploration decays to zero, as is\nthe case for bandit-collected data. An empirical investigation validates our\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:50:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bibaut", "Aur\u00e9lien", ""], ["Chambaz", "Antoine", ""], ["Dimakopoulou", "Maria", ""], ["Kallus", "Nathan", ""], ["van der Laan", "Mark", ""]]}, {"id": "2106.02031", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Change-Point Analysis of Time Series with Evolutionary Spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops change-point methods for the spectrum of a locally\nstationary time series. We focus on series with a bounded spectral density that\nchange smoothly under the null hypothesis but exhibits change-points or becomes\nless smooth under the alternative. We address two local problems. The first is\nthe detection of discontinuities (or breaks) in the spectrum at unknown dates\nand frequencies. The second involves abrupt yet continuous changes in the\nspectrum over a short time period at an unknown frequency without signifying a\nbreak. Both problems can be cast into changes in the degree of smoothness of\nthe spectral density over time. We consider estimation and minimax-optimal\ntesting. We determine the optimal rate for the minimax distinguishable\nboundary, i.e., the minimum break magnitude such that we are able to uniformly\ncontrol type I and type II errors. We propose a novel procedure for the\nestimation of the change-points based on a wild sequential top-down algorithm\nand show its consistency under shrinking shifts and possibly growing number of\nchange-points. Our method can be used across many fields and a companion\nprogram is made available in popular software packages.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:56:24 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 15:14:19 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "2106.02035", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, and Manuel Hernandez", "title": "Home range estimation under a restricted sampling scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of animal movement has gained attention recently, and new\ncontinuous-time models and statistical methods have been developed. All of them\nare based on the assumption that this movement can be recorded over a long\nperiod of time, which is sometimes infeasible, for instance when the battery\nlife of the GPS is short. We prove that the estimation of its home range\nimproves if periods when the GPS is on are alternated with periods when the GPS\nis turned off. This is illustrated through a simulation study, and real life\ndata. We also provide estimators of the stationary distribution, level sets\n(which provides estimators of the core area) and the drift function.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:57:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Hernandez", "Manuel", ""]]}, {"id": "2106.02111", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui", "title": "Efficient $\\mathbb{Z}_2$ synchronization on $\\mathbb{Z}^d$ under\n  symmetry-preserving side information", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider $\\mathbb{Z}_2$-synchronization on the Euclidean lattice. Every\nvertex of $\\mathbb{Z}^d$ is assigned an independent symmetric random sign\n$\\theta_u$, and for every edge $(u,v)$ of the lattice, one observes the product\n$\\theta_u\\theta_v$ flipped independently with probability $p$. The task is to\nreconstruct products $\\theta_u\\theta_v$ for pairs of vertices $u$ and $v$ which\nare arbitrarily far apart. Abb\\'e, Massouli\\'e, Montanari, Sly and Srivastava\n(2018) showed that synchronization is possible if and only if $p$ is below a\ncritical threshold $\\tilde{p}_c(d)$, and efficiently so for $p$ small enough.\nWe augment this synchronization setting with a model of side information\npreserving the sign symmetry of $\\theta$, and propose an \\emph{efficient}\nalgorithm which synchronizes a randomly chosen pair of far away vertices on\naverage, up to a differently defined critical threshold $p_c(d)$. We conjecture\nthat $ p_c(d)=\\tilde{p}_c(d)$ for all $d \\ge 2$. Our strategy is to\n\\emph{renormalize} the synchronization model in order to reduce the effective\nnoise parameter, and then apply a variant of the multiscale algorithm of AMMSS.\nThe success of the renormalization procedure is conditional on a plausible but\nunproved assumption about the regularity of the free energy of an Ising spin\nglass model on $\\mathbb{Z}^d$.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:07:31 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Alaoui", "Ahmed El", ""]]}, {"id": "2106.02131", "submitter": "Nestor Parolya Dr.", "authors": "Taras Bodnar, Nestor Parolya and Erik Thorsen", "title": "Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance\n  Portfolio", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST q-fin.PM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, new results in random matrix theory are derived which allow us\nto construct a shrinkage estimator of the global minimum variance (GMV)\nportfolio when the shrinkage target is a random object. More specifically, the\nshrinkage target is determined as the holding portfolio estimated from previous\ndata. The theoretical findings are applied to develop theory for dynamic\nestimation of the GMV portfolio, where the new estimator of its weights is\nshrunk to the holding portfolio at each time of reconstruction. Both cases with\nand without overlapping samples are considered in the paper. The\nnon-overlapping samples corresponds to the case when different data of the\nasset returns are used to construct the traditional estimator of the GMV\nportfolio weights and to determine the target portfolio, while the overlapping\ncase allows intersections between the samples. The theoretical results are\nderived under weak assumptions imposed on the data-generating process. No\nspecific distribution is assumed for the asset returns except from the\nassumption of finite $4+\\varepsilon$, $\\varepsilon>0$, moments. Also, the\npopulation covariance matrix with unbounded spectrum can be considered. The\nperformance of new trading strategies is investigated via an extensive\nsimulation. Finally, the theoretical findings are implemented in an empirical\nillustration based on the returns on stocks included in the S\\&P 500 index.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 21:08:08 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bodnar", "Taras", ""], ["Parolya", "Nestor", ""], ["Thorsen", "Erik", ""]]}, {"id": "2106.02237", "submitter": "Lei Liu", "authors": "Lei Liu, Shunqi Huang, Brian M. Kurkoski", "title": "Memory Approximate Message Passing", "comments": "6 pages, 5 figures, accepted by IEEE ISIT 2021. arXiv admin note:\n  substantial text overlap with arXiv:2012.10861", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI eess.SP math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate message passing (AMP) is a low-cost iterative\nparameter-estimation technique for certain high-dimensional linear systems with\nnon-Gaussian distributions. However, AMP only applies to independent\nidentically distributed (IID) transform matrices, but may become unreliable for\nother matrix ensembles, especially for ill-conditioned ones. To handle this\ndifficulty, orthogonal/vector AMP (OAMP/VAMP) was proposed for general\nright-unitarily-invariant matrices. However, the Bayes-optimal OAMP/VAMP\nrequires high-complexity linear minimum mean square error estimator. To solve\nthe disadvantages of AMP and OAMP/VAMP, this paper proposes a memory AMP\n(MAMP), in which a long-memory matched filter is proposed for interference\nsuppression. The complexity of MAMP is comparable to AMP. The asymptotic\nGaussianity of estimation errors in MAMP is guaranteed by the orthogonality\nprinciple. A state evolution is derived to asymptotically characterize the\nperformance of MAMP. Based on the state evolution, the relaxation parameters\nand damping vector in MAMP are optimized. For all right-unitarily-invariant\nmatrices, the optimized MAMP converges to OAMP/VAMP, and thus is Bayes-optimal\nif it has a unique fixed point. Finally, simulations are provided to verify the\nvalidity and accuracy of the theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 03:37:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Lei", ""], ["Huang", "Shunqi", ""], ["Kurkoski", "Brian M.", ""]]}, {"id": "2106.02290", "submitter": "Sourav Chatterjee", "authors": "Sohom Bhattacharya, Sourav Chatterjee", "title": "Matrix completion with data-dependent missingness probabilities", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of completing a large matrix with lots of missing entries has\nreceived widespread attention in the last couple of decades. Two popular\napproaches to the matrix completion problem are based on singular value\nthresholding and nuclear norm minimization. Most of the past works on this\nsubject assume that there is a single number $p$ such that each entry of the\nmatrix is available independently with probability $p$ and missing otherwise.\nThis assumption may not be realistic for many applications. In this work, we\nreplace it with the assumption that the probability that an entry is available\nis an unknown function $f$ of the entry itself. For example, if the entry is\nthe rating given to a movie by a viewer, then it seems plausible that high\nvalue entries have greater probability of being available than low value\nentries. We propose two new estimators, based on singular value thresholding\nand nuclear norm minimization, to recover the matrix under this assumption. The\nestimators are shown to be consistent under a low rank assumption. We also\nprovide a consistent estimator of the unknown function $f$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:07:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bhattacharya", "Sohom", ""], ["Chatterjee", "Sourav", ""]]}, {"id": "2106.02356", "submitter": "Marco Mondelli", "authors": "Marco Mondelli and Ramji Venkataramanan", "title": "PCA Initialization for Approximate Message Passing in Rotationally\n  Invariant Models", "comments": "70 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a rank-$1$ signal in the presence of\nrotationally invariant noise-a class of perturbations more general than\nGaussian noise. Principal Component Analysis (PCA) provides a natural\nestimator, and sharp results on its performance have been obtained in the\nhigh-dimensional regime. Recently, an Approximate Message Passing (AMP)\nalgorithm has been proposed as an alternative estimator with the potential to\nimprove the accuracy of PCA. However, the existing analysis of AMP requires an\ninitialization that is both correlated with the signal and independent of the\nnoise, which is often unrealistic in practice. In this work, we combine the two\nmethods, and propose to initialize AMP with PCA. Our main result is a rigorous\nasymptotic characterization of the performance of this estimator. Both the AMP\nalgorithm and its analysis differ from those previously derived in the Gaussian\nsetting: at every iteration, our AMP algorithm requires a specific term to\naccount for PCA initialization, while in the Gaussian case, PCA initialization\naffects only the first iteration of AMP. The proof is based on a two-phase\nartificial AMP that first approximates the PCA estimator and then mimics the\ntrue AMP. Our numerical simulations show an excellent agreement between AMP\nresults and theoretical predictions, and suggest an interesting open direction\non achieving Bayes-optimal performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:13:51 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Mondelli", "Marco", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "2106.02389", "submitter": "Lev A. Sakhnovich", "authors": "Lev Sakhnovich", "title": "The sine kernel, two corresponding operator identities, and random\n  matrices", "comments": "This paper is a prolongation (and an important development of some\n  results) of our paper arXiv:2104.12694", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.FA math.PR math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we consider the integral operator, which acts in\nHilbert space and has sine kernel. This operator generates two operator\nidentities and two corresponding canonical differential systems. We find the\nasymptotics of the corresponding resolvent and Hamiltonians. We use both the\nmethod of operator identities and the theory of random matrices.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:03:30 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sakhnovich", "Lev", ""]]}, {"id": "2106.02589", "submitter": "Maya Ramchandran", "authors": "Maya Ramchandran and Rajarshi Mukherjee", "title": "On Ensembling vs Merging: Least Squares and Random Forests under\n  Covariate Shift", "comments": "9 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been postulated and observed in practice that for prediction problems\nin which covariate data can be naturally partitioned into clusters, ensembling\nalgorithms based on suitably aggregating models trained on individual clusters\noften perform substantially better than methods that ignore the clustering\nstructure in the data. In this paper, we provide theoretical support to these\nempirical observations by asymptotically analyzing linear least squares and\nrandom forest regressions under a linear model. Our main results demonstrate\nthat the benefit of ensembling compared to training a single model on the\nentire data, often termed 'merging', might depend on the underlying bias and\nvariance interplay of the individual predictors to be aggregated. In\nparticular, under both fixed and high dimensional linear models, we show that\nmerging is asymptotically superior to optimal ensembling techniques for linear\nleast squares regression due to the unbiased nature of least squares\nprediction. In contrast, for random forest regression under fixed dimensional\nlinear models, our bounds imply a strict benefit of ensembling over merging.\nFinally, we also present numerical experiments to verify the validity of our\nasymptotic results across different situations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:35:00 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ramchandran", "Maya", ""], ["Mukherjee", "Rajarshi", ""]]}, {"id": "2106.02590", "submitter": "J\\'er\\^ome-Alexis Chevalier", "authors": "J\\'er\\^ome-Alexis Chevalier, Tuan-Binh Nguyen, Bertrand Thirion,\n  Joseph Salmon", "title": "Spatially relaxed inference on high-dimensional linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the inference problem for high-dimensional linear models, when\ncovariates have an underlying spatial organization reflected in their\ncorrelation. A typical example of such a setting is high-resolution imaging, in\nwhich neighboring pixels are usually very similar. Accurate point and\nconfidence intervals estimation is not possible in this context with many more\ncovariates than samples, furthermore with high correlation between covariates.\nThis calls for a reformulation of the statistical inference problem, that takes\ninto account the underlying spatial structure: if covariates are locally\ncorrelated, it is acceptable to detect them up to a given spatial uncertainty.\nWe thus propose to rely on the $\\delta$-FWER, that is the probability of making\na false discovery at a distance greater than $\\delta$ from any true positive.\nWith this target measure in mind, we study the properties of ensembled\nclustered inference algorithms which combine three techniques: spatially\nconstrained clustering, statistical inference, and ensembling to aggregate\nseveral clustered inference solutions. We show that ensembled clustered\ninference algorithms control the $\\delta$-FWER under standard assumptions for\n$\\delta$ equal to the largest cluster diameter. We complement the theoretical\nanalysis with empirical results, demonstrating accurate $\\delta$-FWER control\nand decent power achieved by such inference algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:37:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Nguyen", "Tuan-Binh", ""], ["Thirion", "Bertrand", ""], ["Salmon", "Joseph", ""]]}, {"id": "2106.02600", "submitter": "Song Wei", "authors": "Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran", "title": "Inferring Granger Causality from Irregularly Sampled Time Series", "comments": "33 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous, automated surveillance systems that incorporate machine learning\nmodels are becoming increasingly more common in healthcare environments. These\nmodels can capture temporally dependent changes across multiple patient\nvariables and can enhance a clinician's situational awareness by providing an\nearly warning alarm of an impending adverse event such as sepsis. However, most\ncommonly used methods, e.g., XGBoost, fail to provide an interpretable\nmechanism for understanding why a model produced a sepsis alarm at a given\ntime. The black-box nature of many models is a severe limitation as it prevents\nclinicians from independently corroborating those physiologic features that\nhave contributed to the sepsis alarm. To overcome this limitation, we propose a\ngeneralized linear model (GLM) approach to fit a Granger causal graph based on\nthe physiology of several major sepsis-associated derangements (SADs). We adopt\na recently developed stochastic monotone variational inequality-based estimator\ncoupled with forwarding feature selection to learn the graph structure from\nboth continuous and discrete-valued as well as regularly and irregularly\nsampled time series. Most importantly, we develop a non-asymptotic upper bound\non the estimation error for any monotone link function in the GLM. We conduct\nreal-data experiments and demonstrate that our proposed method can achieve\ncomparable performance to popular and powerful prediction methods such as\nXGBoost while simultaneously maintaining a high level of interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:59:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wei", "Song", ""], ["Xie", "Yao", ""], ["Josef", "Christopher S.", ""], ["Kamaleswaran", "Rishikesan", ""]]}, {"id": "2106.02693", "submitter": "Peter Gr\\\"unwald", "authors": "Rosanne Turner, Alexander Ly, Peter Gr\\\"unwald", "title": "Safe Tests and Always-Valid Confidence Intervals for contingency tables\n  and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop E variables for testing whether two data streams come from the\nsame source or not, and more generally, whether the difference between the\nsources is larger than some minimal effect size. These E variables lead to\ntests that remain safe, i.e. keep their Type-I error guarantees, under flexible\nsampling scenarios such as optional stopping and continuation. We also develop\nthe corresponding always-valid confidence intervals. In special cases our E\nvariables also have an optimal `growth' property under the alternative. We\nillustrate the generic construction through the special case of 2x2 contingency\ntables, where we also allow for the incorporation of different restrictions on\na composite alternative. Comparison to p-value analysis in simulations and a\nreal-world example show that E variables, through their flexibility, often\nallow for early stopping of data collection, thereby retaining similar power as\nclassical methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:12:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Turner", "Rosanne", ""], ["Ly", "Alexander", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "2106.02727", "submitter": "Fabian Mies", "authors": "Stefan Bedbur and Fabian Mies", "title": "Confidence bands for exponential distribution functions under\n  progressive type-II censoring", "comments": "AOM. This article has been accepted for publication in Journal of\n  Statistical Computation and Simulation, published by Taylor & Francis", "journal-ref": null, "doi": "10.1080/00949655.2021.1931211", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a progressively type-II censored sample from the exponential\ndistribution with unknown location and scale parameter, confidence bands are\nproposed for the underlying distribution function by using confidence regions\nfor the parameters and Kolmogorov-Smirnov type statistics. Simple explicit\nrepresentations for the boundaries and for the coverage probabilities of the\nconfidence bands are analytically derived, and the performance of the bands is\ncompared in terms of band width and area by means of a data example. As a\nby-product, a novel confidence region for the location-scale parameter is\nobtained. Extensions of the results to related models for ordered data, such as\nsequential order statistics, as well as to other underlying location-scale\nfamilies of distributions are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:25:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bedbur", "Stefan", ""], ["Mies", "Fabian", ""]]}, {"id": "2106.02735", "submitter": "Sui Tang", "authors": "Jinchao Feng, Yunxiang Ren, Sui Tang", "title": "Data-driven discovery of interacting particle systems using Gaussian\n  processes", "comments": "10 pages; Appendix 19 pages;", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interacting particle or agent systems that display a rich variety of\ncollection motions are ubiquitous in science and engineering. A fundamental and\nchallenging goal is to understand the link between individual interaction rules\nand collective behaviors. In this paper, we study the data-driven discovery of\ndistance-based interaction laws in second-order interacting particle systems.\nWe propose a learning approach that models the latent interaction kernel\nfunctions as Gaussian processes, which can simultaneously fulfill two inference\ngoals: one is the nonparametric inference of interaction kernel function with\nthe pointwise uncertainty quantification, and the other one is the inference of\nunknown parameters in the non-collective forces of the system. We formulate\nlearning interaction kernel functions as a statistical inverse problem and\nprovide a detailed analysis of recoverability conditions, establishing that a\ncoercivity condition is sufficient for recoverability. We provide a\nfinite-sample analysis, showing that our posterior mean estimator converges at\nan optimal rate equal to the one in the classical 1-dimensional Kernel Ridge\nregression. Numerical results on systems that exhibit different collective\nbehaviors demonstrate efficient learning of our approach from scarce noisy\ntrajectory data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:00:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Feng", "Jinchao", ""], ["Ren", "Yunxiang", ""], ["Tang", "Sui", ""]]}, {"id": "2106.02741", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li and Changbao Wu", "title": "Semiparametric inference on Gini indices of two semicontinuous\n  populations under density ratio models", "comments": "49 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gini index is a popular inequality measure with many applications in\nsocial and economic studies. This paper studies semiparametric inference on the\nGini indices of two semicontinuous populations. We characterize the\ndistribution of each semicontinuous population by a mixture of a discrete point\nmass at zero and a continuous skewed positive component. A semiparametric\ndensity ratio model is then employed to link the positive components of the two\ndistributions. We propose the maximum empirical likelihood estimators of the\ntwo Gini indices and their difference, and further investigate the asymptotic\nproperties of the proposed estimators. The asymptotic results enable us to\nconstruct confidence intervals and perform hypothesis tests for the two Gini\nindices and their difference. We show that the proposed estimators are more\nefficient than the existing fully nonparametric estimators. The proposed\nestimators and the asymptotic results are also applicable to cases without\nexcessive zero values. Simulation studies show the superiority of our proposed\nmethod over existing methods. Two real-data applications are presented using\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:17:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2106.02774", "submitter": "Allen Liu", "authors": "Jerry Li, Allen Liu, Ankur Moitra", "title": "Sparsification for Sums of Exponentials and its Algorithmic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in signal processing and learning theory operate under the\nassumption that the underlying model is simple, e.g. that a signal is\napproximately $k$-Fourier-sparse or that a distribution can be approximated by\na mixture model that has at most $k$ components. However the problem of fitting\nthe parameters of such a model becomes more challenging when the\nfrequencies/components are too close together.\n  In this work we introduce new methods for sparsifying sums of exponentials\nand give various algorithmic applications. First we study Fourier-sparse\ninterpolation without a frequency gap, where Chen et al. gave an algorithm for\nfinding an $\\epsilon$-approximate solution which uses $k' = \\mbox{poly}(k, \\log\n1/\\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in\none dimension without a separation condition. Kernel density estimators give an\n$\\epsilon$-approximation that uses $k' = O(k/\\epsilon^2)$ components. These\nmethods both output models that are much more complex than what we started out\nwith. We show how to post-process to reduce the number of\nfrequencies/components down to $k' = \\widetilde{O}(k)$, which is optimal up to\nlogarithmic factors. Moreover we give applications to model selection. In\nparticular, we give the first algorithms for approximately (and robustly)\ndetermining the number of components in a Gaussian mixture model that work\nwithout a separation condition.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:58:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Jerry", ""], ["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2106.02803", "submitter": "Tianxi Li", "authors": "Tianxi Li, Can M. Le", "title": "Network Estimation by Mixing: Adaptivity and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks analysis has been commonly used to study the interactions between\nunits of complex systems. One problem of particular interest is learning the\nnetwork's underlying connection pattern given a single and noisy instantiation.\nWhile many methods have been proposed to address this problem in recent years,\nthey usually assume that the true model belongs to a known class, which is not\nverifiable in most real-world applications. Consequently, network modeling\nbased on these methods either suffers from model misspecification or relies on\nadditional model selection procedures that are not well understood in theory\nand can potentially be unstable in practice. To address this difficulty, we\npropose a mixing strategy that leverages available arbitrary models to improve\ntheir individual performances. The proposed method is computationally efficient\nand almost tuning-free; thus, it can be used as an off-the-shelf method for\nnetwork modeling. We show that the proposed method performs equally well as the\noracle estimate when the true model is included as individual candidates. More\nimportantly, the method remains robust and outperforms all current estimates\neven when the models are misspecified. Extensive simulation examples are used\nto verify the advantage of the proposed mixing method. Evaluation of link\nprediction performance on 385 real-world networks from six domains also\ndemonstrates the universal competitiveness of the mixing method across multiple\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 05:17:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Tianxi", ""], ["Le", "Can M.", ""]]}, {"id": "2106.02897", "submitter": "Robert Gaunt", "authors": "Robert E. Gaunt", "title": "The basic distributional theory for the product of zero mean correlated\n  normal random variables", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The product of two zero mean correlated normal random variables has received\nmuch attention in the statistics literature and appears in many application\nareas. However, many important distributional properties are yet to tabulated.\nThis paper fills this gap by providing the basic distributional theory for the\nproduct of two zero mean correlated normal random variables.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 13:45:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gaunt", "Robert E.", ""]]}, {"id": "2106.03156", "submitter": "Youngki Shin", "authors": "Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin", "title": "Fast and Robust Online Inference with Stochastic Gradient Descent via\n  Random Scaling", "comments": "16 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We develop a new method of online inference for a vector of parameters\nestimated by the Polyak-Ruppert averaging procedure of stochastic gradient\ndescent (SGD) algorithms. We leverage insights from time series regression in\neconometrics and construct asymptotically pivotal statistics via random\nscaling. Our approach is fully operational with online data and is rigorously\nunderpinned by a functional central limit theorem. Our proposed inference\nmethod has a couple of key advantages over the existing methods. First, the\ntest statistic is computed in an online fashion with only SGD iterates and the\ncritical values can be obtained without any resampling methods, thereby\nallowing for efficient implementation suitable for massive online data. Second,\nthere is no need to estimate the asymptotic variance and our inference method\nis shown to be robust to changes in the tuning parameters for SGD algorithms in\nsimulation experiments with synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:38:37 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:52:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee", "Sokbae", ""], ["Liao", "Yuan", ""], ["Seo", "Myung Hwan", ""], ["Shin", "Youngki", ""]]}, {"id": "2106.03163", "submitter": "My Phan", "authors": "My Phan, Philip S. Thomas and Erik Learned-Miller", "title": "Towards Practical Mean Bounds for Small Samples", "comments": "This is an extended work of our ICML 2021 paper \"Towards Practical\n  Mean Bounds for Small Samples\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, to bound the mean for small sample sizes, practitioners have\nhad to choose between using methods with unrealistic assumptions about the\nunknown distribution (e.g., Gaussianity) and methods like Hoeffding's\ninequality that use weaker assumptions but produce much looser (wider)\nintervals. In 1969, Anderson (1969) proposed a mean confidence interval\nstrictly better than or equal to Hoeffding's whose only assumption is that the\ndistribution's support is contained in an interval $[a,b]$. For the first time\nsince then, we present a new family of bounds that compares favorably to\nAnderson's. We prove that each bound in the family has {\\em guaranteed\ncoverage}, i.e., it holds with probability at least $1-\\alpha$ for all\ndistributions on an interval $[a,b]$. Furthermore, one of the bounds is tighter\nthan or equal to Anderson's for all samples. In simulations, we show that for\nmany distributions, the gain over Anderson's bound is substantial.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 16:07:20 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 14:38:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Phan", "My", ""], ["Thomas", "Philip S.", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "2106.03187", "submitter": "Arun Kumar", "authors": "Niharika Bhootna and Arun Kumar", "title": "Tempered Stable Autoregressive Models", "comments": "19 pages, 8 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce and study a one sided tempered stable first\norder autoregressive model called TAR(1). Under the assumption of stationarity\nof the model, the marginal probability density function of the error term is\nfound. It is shown that the distribution of the error term is infinitely\ndivisible. Parameter estimation of the introduced TAR(1) process is done by\nadopting the conditional least square and method of moments based approach and\nthe performance of the proposed methods are evaluated on simulated data. Also\nwe study an autoregressive model of order one with tempered stable innovations.\nUsing appropriate test statistic it is shown that the model fit very well on\nreal and simulated data. Our models generalize the inverse Gaussian and\none-sided stable autoregressive models existing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:32:01 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 05:14:48 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Bhootna", "Niharika", ""], ["Kumar", "Arun", ""]]}, {"id": "2106.03227", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Yao Xie", "title": "Neural Tangent Kernel Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel neural network Maximum Mean Discrepancy (MMD) statistic by\nidentifying a connection between neural tangent kernel (NTK) and MMD statistic.\nThis connection enables us to develop a computationally efficient and\nmemory-efficient approach to compute the MMD statistic and perform neural\nnetwork based two-sample tests towards addressing the long-standing challenge\nof memory and computational complexity of the MMD statistic, which is essential\nfor online implementation to assimilate new samples. Theoretically, such a\nconnection allows us to understand the properties of the new test statistic,\nsuch as Type-I error and testing power for performing the two-sample test, by\nleveraging analysis tools for kernel MMD. Numerical experiments on synthetic\nand real-world datasets validate the theory and demonstrate the effectiveness\nof the proposed NTK-MMD statistic.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:00:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Xie", "Yao", ""]]}, {"id": "2106.03285", "submitter": "Qiuping Wang", "authors": "Qiuping Wang", "title": "A sparse $p_0$ model with covariates for directed networks", "comments": "19 pages,2 figures,3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1609.04558 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned here with unrestricted maximum likelihood estimation in a\nsparse $p_0$ model with covariates for directed networks. The model has a\ndensity parameter $\\nu$, a $2n$-dimensional node parameter $\\bs{\\eta}$ and a\nfixed dimensional regression coefficient $\\bs{\\gamma}$ of covariates. Previous\nstudies focus on the restricted likelihood inference. When the number of nodes\n$n$ goes to infinity, we derive the $\\ell_\\infty$-error between the maximum\nlikelihood estimator (MLE) $(\\widehat{\\bs{\\eta}}, \\widehat{\\bs{\\gamma}})$ and\nits true value $(\\bs{\\eta}, \\bs{\\gamma})$. They are $O_p( (\\log n/n)^{1/2} )$\nfor $\\widehat{\\bs{\\eta}}$ and $O_p( \\log n/n)$ for $\\widehat{\\bs{\\gamma}}$, up\nto an additional factor. This explains the asymptotic bias phenomenon in the\nasymptotic normality of $\\widehat{\\bs{\\gamma}}$ in\n\\cite{Yan-Jiang-Fienberg-Leng2018}. Further, we derive the asymptotic normality\nof the MLE. Numerical studies and a data analysis demonstrate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 00:48:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Qiuping", ""]]}, {"id": "2106.03344", "submitter": "Fei Xue", "authors": "Fei Xue, Rong Ma, Hongzhe Li", "title": "Semi-Supervised Statistical Inference for High-Dimensional Linear\n  Regression with Blockwise Missing Data", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockwise missing data occurs frequently when we integrate multisource or\nmultimodality data where different sources or modalities contain complementary\ninformation. In this paper, we consider a high-dimensional linear regression\nmodel with blockwise missing covariates and a partially observed response\nvariable. Under this semi-supervised framework, we propose a computationally\nefficient estimator for the regression coefficient vector based on carefully\nconstructed unbiased estimating equations and a multiple blockwise imputation\nprocedure, and obtain its rates of convergence. Furthermore, building upon an\ninnovative semi-supervised projected estimating equation technique that\nintrinsically achieves bias-correction of the initial estimator, we propose\nnearly unbiased estimators for the individual regression coefficients that are\nasymptotically normally distributed under mild conditions. By carefully\nanalyzing these debiased estimators, asymptotically valid confidence intervals\nand statistical tests about each regression coefficient are constructed.\nNumerical studies and application analysis of the Alzheimer's Disease\nNeuroimaging Initiative data show that the proposed method performs better and\nbenefits more from unsupervised samples than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:12:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xue", "Fei", ""], ["Ma", "Rong", ""], ["Li", "Hongzhe", ""]]}, {"id": "2106.03350", "submitter": "B.L.S. Prakasa Rao", "authors": "B.L.S. Prakasa Rao", "title": "Maximum likelihood estimation for sub-fractional Vasicek model", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.06102", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the asymptotic properties of maximum likelihood estimators of\nthe drift parameter for fractional vasicek model driven by a sub-fractional\nBrownian motion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:38:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rao", "B. L. S. Prakasa", ""]]}, {"id": "2106.03431", "submitter": "Mathias H{\\o}jgaard Jensen", "authors": "Mathias H{\\o}jgaard Jensen and Sarang Joshi and Stefan Sommer", "title": "Bridge Simulation and Metric Estimation on Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simulation scheme for simulating Brownian bridges on complete\nand connected Lie groups. We show how this simulation scheme leads to absolute\ncontinuity of the Brownian bridge measure with respect to the guided process\nmeasure. This result generalizes the Euclidean result of Delyon and Hu to Lie\ngroups. We present numerical results of the guided process in the Lie group\n$\\SO(3)$. In particular, we apply importance sampling to estimate the metric on\n$\\SO(3)$ using an iterative maximum likelihood method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:59:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jensen", "Mathias H\u00f8jgaard", ""], ["Joshi", "Sarang", ""], ["Sommer", "Stefan", ""]]}, {"id": "2106.03542", "submitter": "Andrew Y. K. Foong", "authors": "Andrew Y. K. Foong, Wessel P. Bruinsma, David R. Burt, Richard E.\n  Turner", "title": "How Tight Can PAC-Bayes be in the Small Data Regime?", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the question: Given a small number of\ndatapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be\nmade? For such small datasets, test set bounds adversely affect generalisation\nperformance by discarding data. In this setting, PAC-Bayes bounds are\nespecially attractive, due to their ability to use all the data to\nsimultaneously learn a posterior and bound its generalisation risk. We focus on\nthe case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes\ntheorem of Germain et al. (2009) and Begin et al. (2016). While their theorem\nis known to recover many existing PAC-Bayes bounds, it is unclear what the\ntightest bound derivable from their framework is. Surprisingly, we show that\nfor a fixed learning algorithm and dataset, the tightest bound of this form\ncoincides with the tightest bound of the more restrictive family of bounds\nconsidered in Catoni (2007). In contrast, in the more natural case of\ndistributions over datasets, we give examples (both analytic and numerical)\nshowing that the family of bounds in Catoni (2007) can be suboptimal. Within\nthe proof framework of Germain et al. (2009) and Begin et al. (2016), we\nestablish a lower bound on the best bound achievable in expectation, which\nrecovers the Chernoff test set bound in the case when the posterior is equal to\nthe prior. Finally, to illustrate how tight these bounds can potentially be, we\nstudy a synthetic one-dimensional classification task in which it is feasible\nto meta-learn both the prior and the form of the bound to obtain the tightest\nPAC-Bayes and test set bounds possible. We find that in this simple, controlled\nscenario, PAC-Bayes bounds are surprisingly competitive with comparable,\ncommonly used Chernoff test set bounds. However, the sharpest test set bounds\nstill lead to better guarantees on the generalisation error than the PAC-Bayes\nbounds we consider.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 12:11:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Foong", "Andrew Y. K.", ""], ["Bruinsma", "Wessel P.", ""], ["Burt", "David R.", ""], ["Turner", "Richard E.", ""]]}, {"id": "2106.03700", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and David Preinerstorfer", "title": "Superconsistency of tests in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To assess whether there is some signal in a big database, aggregate tests for\nthe global null hypothesis of no effect are routinely applied in practice\nbefore more specialized analysis is carried out. Although a plethora of\naggregate tests is available, each test has its strengths but also its blind\nspots. In a Gaussian sequence model, we study whether it is possible to obtain\na test with substantially better consistency properties than the likelihood\nratio (i.e., Euclidean norm based) test. We establish an impossibility result,\nshowing that in the high-dimensional framework we consider, the set of\nalternatives for which a test may improve upon the likelihood ratio test --\nthat is, its superconsistency points -- is always asymptotically negligible in\na relative volume sense.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:10:48 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2106.03811", "submitter": "Antonio Forcina", "authors": "Antonio Forcina and Francesco Bartolucci", "title": "Estimating the size of a closed population by modeling latent and\n  observed heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper describe a new class of capture-recapture models for closed\npopulations when individual covariates are available. The novelty consists in\ncombining a latent class model for capture probabilities where the marginal\nweights and the conditional distributions given the latent may depend on\ncovariates, with a model for the marginal distribution of the available\ncovariates. In addition, a general formulation for the conditional\ndistributions given the latent and covariates which allows serial dependence is\nprovided. A Fisher scoring algorithm for maximum likelihood estimation is\npresented, asymptotic results are derived, and a procedure for constructing\nlikelihood based confidence intervals for the population total is presented.\nTwo examples with real data are used to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:19:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 19:18:28 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 16:05:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Forcina", "Antonio", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "2106.03969", "submitter": "Enric Boix-Adser\\`a", "authors": "Enric Boix-Adsera, Guy Bresler, Frederic Koehler", "title": "Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models", "comments": "49 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a tree-structured Ising model from data,\nsuch that subsequent predictions computed using the model are accurate.\nConcretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small\nsets of variables $S$ are accurate. Since its introduction more than 50 years\nago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood\ntree, has been the benchmark algorithm for learning tree-structured graphical\nmodels. A bound on the sample complexity of the Chow-Liu algorithm with respect\nto the prediction-centric local total variation loss was shown in [BK19]. While\nthose results demonstrated that it is possible to learn a useful model even\nwhen recovering the true underlying graph is impossible, their bound depends on\nthe maximum strength of interactions and thus does not achieve the\ninformation-theoretic optimum. In this paper, we introduce a new algorithm that\ncarefully combines elements of the Chow-Liu algorithm with tree metric\nreconstruction methods to efficiently and optimally learn tree Ising models\nunder a prediction-centric loss. Our algorithm is robust to model\nmisspecification and adversarial corruptions. In contrast, we show that the\ncelebrated Chow-Liu algorithm can be arbitrarily suboptimal.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 21:09:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 13:18:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Boix-Adsera", "Enric", ""], ["Bresler", "Guy", ""], ["Koehler", "Frederic", ""]]}, {"id": "2106.04082", "submitter": "Satoru Odake", "authors": "Satoru Odake and Ryu Sasaki", "title": "Markov Chains Generated by Convolutions of Orthogonality Measures", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": "DPSU-21-1", "categories": "math.PR math-ph math.CA math.MP math.ST nlin.SI stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  About two dozens of exactly solvable Markov chains on one-dimensional finite\nand semi-infinite integer lattices are constructed in terms of convolutions of\northogonality measures of the Krawtchouk, Hahn, Meixner, Charlier, $q$-Hahn and\n$q$-Meixner polynomials. By construction, the stationary probability\ndistributions, the complete sets of eigenvalues and eigenvectors are provided\nby the polynomials and the orthogonality measures. An interesting property\npossessed by these stationary probability distributions, called `convolutional\nself-similarity,' is demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 03:39:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Odake", "Satoru", ""], ["Sasaki", "Ryu", ""]]}, {"id": "2106.04118", "submitter": "Matteo Sesia", "authors": "Shuangning Li, Matteo Sesia, Yaniv Romano, Emmanuel Cand\\`es, Chiara\n  Sabatti", "title": "Searching for consistent associations with a multi-environment knockoff\n  filter", "comments": "41 pages, 21 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a method based on model-X knockoffs to find conditional\nassociations that are consistent across diverse environments, controlling the\nfalse discovery rate. The motivation for this problem is that large data sets\nmay contain numerous associations that are statistically significant and yet\nmisleading, as they are induced by confounders or sampling imperfections.\nHowever, associations consistently replicated under different conditions may be\nmore interesting. In fact, consistency sometimes provably leads to valid causal\ninferences even if conditional associations do not. While the proposed method\nis flexible and can be deployed in a wide range of applications, this paper\nhighlights its relevance to genome-wide association studies, in which\nconsistency across populations with diverse ancestries mitigates confounding\ndue to unmeasured variants. The effectiveness of this approach is demonstrated\nby simulations and applications to the UK Biobank data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:04:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Shuangning", ""], ["Sesia", "Matteo", ""], ["Romano", "Yaniv", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "2106.04333", "submitter": "Ronan Le Guevel", "authors": "Magalie Fromont (IRMAR, UR2), Fabrice Grela (IRMAR, UR2), Ronan Le\n  Gu\\'evel (IRMAR, UR2)", "title": "Minimax and adaptive tests for detecting abrupt and possibly transitory\n  changes in a Poisson process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in cybersecurity and epidemiology, we consider the\nproblem of detecting an abrupt change in the intensity of a Poisson process,\ncharacterised by a jump (non transitory change) or a bump (transitory change)\nfrom constant. We propose a complete study from the nonasymptotic minimax\ntesting point of view, when the constant baseline intensity is known or\nunknown. The question of minimax adaptation with respect to each parameter\n(height, location, length) of the change is tackled, leading to a comprehensive\noverview of the various minimax separation rate regimes. We exhibit three such\nregimes and identify the factors of the two phase transitions, by giving the\ncost of adaptation to each parameter. For each alternative hypothesis,\ndepending on the knowledge or not of each change parameter, we propose minimax\nor minimax adaptive tests based on linear statistics, close to CUSUM\nstatistics, or quadratic statistics more adapted to the L 2-distance considered\nin our minimax criteria and typically more powerful in practice, as our\nsimulation study shows. When the change location or length is unknown, our\nadaptive tests are constructed from a scan aggregation principle combined with\nBonferroni or min-p level correction, and a conditioning trick when the\nbaseline intensity is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:41:09 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Fromont", "Magalie", "", "IRMAR, UR2"], ["Grela", "Fabrice", "", "IRMAR, UR2"], ["Gu\u00e9vel", "Ronan Le", "", "IRMAR, UR2"]]}, {"id": "2106.04373", "submitter": "Jana Jure\\v{c}kov\\'a", "authors": "Jana Jure\\v{c}kov\\'a", "title": "Process of the slope components of $\\alpha$-regression quantile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression model along with the process of its\n$\\alpha$-regression quantile, $0<\\alpha<1$. We are interested mainly in the\nslope components of $\\alpha$-regression quantile and in their dependence on the\nchoice of $\\alpha.$ While they are invariant to the location, and only the\nintercept part of the $\\alpha$-regression quantile estimates the quantile\n$F^{-1}(\\alpha)$ of the model errors, their dispersion depends on $\\alpha$ and\nis infinitely increasing as $\\alpha\\rightarrow 0,1$, in the same rate as for\nthe ordinary quantiles. We study the process of $R$-estimators of the slope\nparameters over $\\alpha\\in[0,1]$, generated by the H\\'{a}jek rank scores. We\nshow that this process, standardized by $f(F ^{-1}(\\alpha))$ under\nexponentially tailed $F$, converges to the vector of independent Brownian\nbridges. The same course is true for the process of the slope components of\n$\\alpha$-regression quantile.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:08:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Jure\u010dkov\u00e1", "Jana", ""]]}, {"id": "2106.04795", "submitter": "Huiyuan Wang", "authors": "Huiyuan Wang and Wei Lin", "title": "Harmless Overparametrization in Two-layer Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Overparametrized neural networks, where the number of active parameters is\nlarger than the sample size, prove remarkably effective in modern deep learning\npractice. From the classical perspective, however, much fewer parameters are\nsufficient for optimal estimation and prediction, whereas overparametrization\ncan be harmful even in the presence of explicit regularization. To reconcile\nthis conflict, we present a generalization theory for overparametrized ReLU\nnetworks by incorporating an explicit regularizer based on the scaled variation\nnorm. Interestingly, this regularizer is equivalent to the ridge from the angle\nof gradient-based optimization, but is similar to the group lasso in terms of\ncontrolling model complexity. By exploiting this ridge-lasso duality, we show\nthat overparametrization is generally harmless to two-layer ReLU networks. In\nparticular, the overparametrized estimators are minimax optimal up to a\nlogarithmic factor. By contrast, we show that overparametrized random feature\nmodels suffer from the curse of dimensionality and thus are suboptimal.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:52:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wang", "Huiyuan", ""], ["Lin", "Wei", ""]]}, {"id": "2106.04869", "submitter": "Xin Liu", "authors": "Xin Liu, Liwen Zhang, Zhen Zhang", "title": "Ultra High Dimensional Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural breaks have been commonly seen in applications. Specifically for\ndetection of change points in time, research gap still remains on the setting\nin ultra high dimension, where the covariates may bear spurious correlations.\nIn this paper, we propose a two-stage approach to detect change points in ultra\nhigh dimension, by firstly proposing the dynamic titled current correlation\nscreening method to reduce the input dimension, and then detecting possible\nchange points in the framework of group variable selection. Not only the\nspurious correlation between ultra-high dimensional covariates is taken into\nconsideration in variable screening, but non-convex penalties are studied in\nchange point detection in the ultra high dimension. Asymptotic properties are\nderived to guarantee the asymptotic consistency of the selection procedure, and\nthe numerical investigations show the promising performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:50:23 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Xin", ""], ["Zhang", "Liwen", ""], ["Zhang", "Zhen", ""]]}, {"id": "2106.05114", "submitter": "Kam\\'elia Daudel", "authors": "Kam\\'elia Daudel and Randal Douc", "title": "Mixture weights optimisation for Alpha-Divergence Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on $\\alpha$-divergence minimisation methods for\nVariational Inference. More precisely, we are interested in algorithms\noptimising the mixture weights of any given mixture model, without any\ninformation on the underlying distribution of its mixture components\nparameters. The Power Descent, defined for all $\\alpha \\neq 1$, is one such\nalgorithm and we establish in our work the full proof of its convergence\ntowards the optimal mixture weights when $\\alpha <1$. Since the\n$\\alpha$-divergence recovers the widely-used forward Kullback-Leibler when\n$\\alpha \\to 1$, we then extend the Power Descent to the case $\\alpha = 1$ and\nshow that we obtain an Entropic Mirror Descent. This leads us to investigate\nthe link between Power Descent and Entropic Mirror Descent: first-order\napproximations allow us to introduce the Renyi Descent, a novel algorithm for\nwhich we prove an $O(1/N)$ convergence rate. Lastly, we compare numerically the\nbehavior of the unbiased Power Descent and of the biased Renyi Descent and we\ndiscuss the potential advantages of one algorithm over the other.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:47:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Daudel", "Kam\u00e9lia", ""], ["Douc", "Randal", ""]]}, {"id": "2106.05117", "submitter": "Gloria Buritica", "authors": "Gloria Buritic\\'a (LPSM (UMR\\_8001)), Meyer Nicolas (KU), Thomas\n  Mikosch (KU), Olivier Wintenberger (LPSM (UMR\\_8001))", "title": "Some variations on the extremal index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-consider Leadbetter's extremal index for stationary sequences. It has\ninterpretation as reciprocal of the expected size of an extremal cluster above\nhigh thresholds. We focus on heavy-tailed time series, in particular on\nregularly varying stationary sequences, and discuss recent research in extreme\nvalue theory for these models. A regularly varying time series has multivariate\nregularly varying finite-dimensional distributions. Thanks to results by Basrak\nand Segers we have explicit representations of the limiting cluster structure\nof extremes, leading to explicit expressions of the limiting point process of\nexceedances and the extremal index as a summary measure of extremal clustering.\nThe extremal index appears in various situations which do not seem to be\ndirectly related, like the convergence of maxima and point processes. We\nconsider different representations of the extremal index which arise from the\nconsidered context. We discuss the theory and apply it to a regularly varying\nAR(1) process and the solution to an affine stochastic recurrence equation\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:51:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Buritic\u00e1", "Gloria", "", "LPSM"], ["Nicolas", "Meyer", "", "KU"], ["Mikosch", "Thomas", "", "KU"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2106.05183", "submitter": "Panagiotis Lolas", "authors": "Panagiotis Lolas, Lexing Ying", "title": "Shrinkage Estimation of Functions of Large Noisy Symmetric Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating functions of a large symmetric matrix\n$A_n$ when we only have\n  access to a noisy estimate $\\hat{A}_n=A_n+\\sigma Z_n/\\sqrt{n}.$ We are\ninterested\n  in the case that $Z_n$ is a Wigner ensemble and suggest an algorithm based on\nnonlinear shrinkage of\n  the eigenvalues of $\\hat{A}_n.$ As an intermediate step we explain how\nrecovery of the spectrum of\n  $A_n$ is possible using only the spectrum of $\\hat{A}_n$. Our algorithm has\nimportant applications,\n  for example, in solving high-dimensional noisy systems of equations or\nsymmetric matrix\n  denoising. Throughout our analysis we rely on tools from random matrix\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:22:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lolas", "Panagiotis", ""], ["Ying", "Lexing", ""]]}, {"id": "2106.05201", "submitter": "Francois Roueff", "authors": "Tepmony Sim (ITC), Randal Douc (TIPIC-SAMOVAR, CNRS), Fran\\c{c}ois\n  Roueff (LTCI)", "title": "General-order observation-driven models: ergodicity and consistency of\n  the maximum likelihood estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of observation-driven models (ODMs) includes many models of\nnon-linear time series which, in a fashion similar to, yet different from,\nhidden Markov models (HMMs), involve hidden variables. Interestingly, in\ncontrast to most HMMs, ODMs enjoy likelihoods that can be computed exactly with\ncomputational complexity of the same order as the number of observations,\nmaking maximum likelihood estimation the privileged approach for statistical\ninference for these models. A celebrated example of general order ODMs is the\nGARCH$(p,q)$ model, for which ergodicity and inference has been studied\nextensively. However little is known on more general models, in particular\ninteger-valued ones, such as the log-linear Poisson GARCH or the NBIN-GARCH of\norder $(p,q)$ about which most of the existing results seem restricted to the\ncase $p=q=1$. Here we fill this gap and derive ergodicity conditions for\ngeneral ODMs. The consistency and the asymptotic normality of the maximum\nlikelihood estimator (MLE) can then be derived using the method already\ndeveloped for first order ODMs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:09:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sim", "Tepmony", "", "ITC"], ["Douc", "Randal", "", "TIPIC-SAMOVAR, CNRS"], ["Roueff", "Fran\u00e7ois", "", "LTCI"]]}, {"id": "2106.05454", "submitter": "Wencan Zhu", "authors": "Wencan Zhu, Eric Adjakossa, C\\'eline L\\'evy-Leduc and Nils Tern\\`es", "title": "Sign Consistency of the Generalized Elastic Net Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable selection approach in the\nframework of high-dimensional linear models where the columns of the design\nmatrix are highly correlated. It consists in rewriting the initial\nhigh-dimensional linear model to remove the correlation between the columns of\nthe design matrix and in applying a generalized Elastic Net criterion since it\ncan be seen as an extension of the generalized Lasso. The properties of our\napproach called gEN (generalized Elastic Net) are investigated both from a\ntheoretical and a numerical point of view. More precisely, we provide a new\ncondition called GIC (Generalized Irrepresentable Condition) which generalizes\nthe EIC (Elastic Net Irrepresentable Condition) of Jia and Yu (2010) under\nwhich we prove that our estimator can recover the positions of the null and non\nnull entries of the coefficients when the sample size tends to infinity. We\nalso assess the performance of our methodology using synthetic data and compare\nit with alternative approaches. Our numerical experiments show that our\napproach improves the variable selection performance in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 01:52:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Wencan", ""], ["Adjakossa", "Eric", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Tern\u00e8s", "Nils", ""]]}, {"id": "2106.05480", "submitter": "Kevin Tian", "authors": "Yin Tat Lee, Ruoqi Shen, Kevin Tian", "title": "Lower Bounds on Metropolized Sampling Methods for Well-Conditioned\n  Distributions", "comments": "47 pages, 1 figure, comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give lower bounds on the performance of two of the most popular sampling\nmethods in practice, the Metropolis-adjusted Langevin algorithm (MALA) and\nmulti-step Hamiltonian Monte Carlo (HMC) with a leapfrog integrator, when\napplied to well-conditioned distributions. Our main result is a nearly-tight\nlower bound of $\\widetilde{\\Omega}(\\kappa d)$ on the mixing time of MALA from\nan exponentially warm start, matching a line of algorithmic results up to\nlogarithmic factors and answering an open question of Chewi et. al. We also\nshow that a polynomial dependence on dimension is necessary for the relaxation\ntime of HMC under any number of leapfrog steps, and bound the gains achievable\nby changing the step count. Our HMC analysis draws upon a novel connection\nbetween leapfrog integration and Chebyshev polynomials, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:47:39 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lee", "Yin Tat", ""], ["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""]]}, {"id": "2106.05515", "submitter": "Yu Bai", "authors": "Yu Bai, Song Mei, Huan Wang, Caiming Xiong", "title": "Understanding the Under-Coverage Bias in Uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the data uncertainty in regression tasks is often done by learning\na quantile function or a prediction interval of the true label conditioned on\nthe input. It is frequently observed that quantile regression -- a vanilla\nalgorithm for learning quantiles with asymptotic guarantees -- tends to\n\\emph{under-cover} than the desired coverage level in reality. While various\nfixes have been proposed, a more fundamental understanding of why this\nunder-coverage bias happens in the first place remains elusive.\n  In this paper, we present a rigorous theoretical study on the coverage of\nuncertainty estimation algorithms in learning quantiles. We prove that quantile\nregression suffers from an inherent under-coverage bias, in a vanilla setting\nwhere we learn a realizable linear quantile function and there is more data\nthan parameters. More quantitatively, for $\\alpha>0.5$ and small $d/n$, the\n$\\alpha$-quantile learned by quantile regression roughly achieves coverage\n$\\alpha - (\\alpha-1/2)\\cdot d/n$ regardless of the noise distribution, where\n$d$ is the input dimension and $n$ is the number of training data. Our theory\nreveals that this under-coverage bias stems from a certain high-dimensional\nparameter estimation error that is not implied by existing theories on quantile\nregression. Experiments on simulated and real data verify our theory and\nfurther illustrate the effect of various factors such as sample size and model\ncapacity on the under-coverage bias in more practical setups.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:11:55 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bai", "Yu", ""], ["Mei", "Song", ""], ["Wang", "Huan", ""], ["Xiong", "Caiming", ""]]}, {"id": "2106.05669", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer and Shun Watanabe", "title": "Information Geometry of Reversible Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the information geometric structure of time reversibility for\nparametric families of irreducible transition kernels of Markov chains. We\ndefine and characterize reversible exponential families of Markov kernels, and\nshow that irreducible and reversible Markov kernels form both a mixture family\nand, perhaps surprisingly, an exponential family in the set of all stochastic\nkernels. We propose a parametrization of the entire manifold of reversible\nkernels, and inspect reversible geodesics. We define information projections\nonto the reversible manifold, and derive closed-form expressions for the\ne-projection and m-projection, along with Pythagorean identities with respect\nto information divergence, leading to some new notion of reversiblization of\nMarkov kernels. We show the family of edge measures pertaining to irreducible\nand reversible kernels also forms an exponential family among distributions\nover pairs. We further explore geometric properties of the reversible family,\nby comparing them with other remarkable families of stochastic matrices.\nFinally, we show that reversible kernels are, in a sense we define, the minimal\nexponential family generated by the m-family of symmetric kernels, and the\nsmallest mixture family that comprises the e-family of memoryless kernels.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:38:00 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wolfer", "Geoffrey", ""], ["Watanabe", "Shun", ""]]}, {"id": "2106.05694", "submitter": "David Strieder", "authors": "David Strieder, Tobias Freidling, Stefan Haffner and Mathias Drton", "title": "Confidence in Causal Discovery with Linear Causal Models", "comments": "Accepted for the 37th conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural causal models postulate noisy functional relations among a set of\ninteracting variables. The causal structure underlying each such model is\nnaturally represented by a directed graph whose edges indicate for each\nvariable which other variables it causally depends upon. Under a number of\ndifferent model assumptions, it has been shown that this causal graph and, thus\nalso, causal effects are identifiable from mere observational data. For these\nmodels, practical algorithms have been devised to learn the graph. Moreover,\nwhen the graph is known, standard techniques may be used to give estimates and\nconfidence intervals for causal effects. We argue, however, that a two-step\nmethod that first learns a graph and then treats the graph as known yields\nconfidence intervals that are overly optimistic and can drastically fail to\naccount for the uncertain causal structure. To address this issue we lay out a\nframework based on test inversion that allows us to give confidence regions for\ntotal causal effects that capture both sources of uncertainty: causal structure\nand numerical size of nonzero effects. Our ideas are developed in the context\nof bivariate linear causal models with homoscedastic errors, but as we\nexemplify they are generalizable to larger systems as well as other settings\nsuch as, in particular, linear non-Gaussian models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:27:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Strieder", "David", ""], ["Freidling", "Tobias", ""], ["Haffner", "Stefan", ""], ["Drton", "Mathias", ""]]}, {"id": "2106.05739", "submitter": "Carles Domingo-Enrich", "authors": "Carles Domingo-Enrich, Youssef Mroueh", "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability\n  Metrics", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:41:33 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 19:47:34 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 18:49:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Domingo-Enrich", "Carles", ""], ["Mroueh", "Youssef", ""]]}, {"id": "2106.05766", "submitter": "Martial Longla", "authors": "Martial Longla, Mathias Muia Nthiani, Fidel Djongreba Ndikwa", "title": "Dependence and mixing for perturbations of copula-based Markov chains", "comments": "13 pages 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the impact of perturbations of copulas on dependence\nproperties of the Markov chains they generate. We use an observation that is\nvalid for convex combinations of copulas to establish sufficient conditions for\nthe mixing coefficients $\\rho_n$, $\\alpha_n$ and some other measures of\nassociation. New copula families are derived based on perturbations of copulas\nand their multivariate analogs for $n$-copulas are provided in general. Several\nfamilies of copulas can be constructed from the provided framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:19:31 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Longla", "Martial", ""], ["Nthiani", "Mathias Muia", ""], ["Ndikwa", "Fidel Djongreba", ""]]}, {"id": "2106.05828", "submitter": "Housen Li", "authors": "Markus Haltmeier and Housen Li and Axel Munk", "title": "A Variational View on Statistical Multiscale Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying view on various statistical estimation techniques\nincluding penalization, variational and thresholding methods. These estimators\nwill be analyzed in the context of statistical linear inverse problems\nincluding nonparametric and change point regression, and high dimensional\nlinear models as examples. Our approach reveals many seemingly unrelated\nestimation schemes as special instances of a general class of variational\nmultiscale estimators, named MIND (MultIscale Nemirovskii--Dantzig). These\nestimators result from minimizing certain regularization functionals under\nconvex constraints that can be seen as multiple statistical tests for local\nhypotheses.\n  For computational purposes, we recast MIND in terms of simpler unconstraint\noptimization problems via Lagrangian penalization as well as Fenchel duality.\nPerformance of several MINDs is demonstrated on numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:47:16 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Haltmeier", "Markus", ""], ["Li", "Housen", ""], ["Munk", "Axel", ""]]}, {"id": "2106.05838", "submitter": "Jingyi Zhang", "authors": "Cheng Meng, Yuan Ke, Jingyi Zhang, Mengrui Zhang, Wenxuan Zhong, Ping\n  Ma", "title": "Large-scale optimal transport map estimation using projection pursuit", "comments": null, "journal-ref": "Meng, C. \"Large-scale optimal transport map estimation using\n  projection pursuit.\" NeurIPS 2019 (2019); Ke, Y. \"Large-scale optimal\n  transport map estimation using projection pursuit.\" NeurIPS 2019 (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of large-scale optimal transport maps\n(OTM), which is a well-known challenging problem owing to the curse of\ndimensionality. Existing literature approximates the large-scale OTM by a\nseries of one-dimensional OTM problems through iterative random projection.\nSuch methods, however, suffer from slow or none convergence in practice due to\nthe nature of randomly selected projection directions. Instead, we propose an\nestimation method of large-scale OTM by combining the idea of projection\npursuit regression and sufficient dimension reduction. The proposed method,\nnamed projection pursuit Monge map (PPMM), adaptively selects the most\n``informative'' projection direction in each iteration. We theoretically show\nthe proposed dimension reduction method can consistently estimate the most\n``informative'' projection direction in each iteration. Furthermore, the PPMM\nalgorithm weakly convergences to the target large-scale OTM in a reasonable\nnumber of steps. Empirically, PPMM is computationally easy and converges fast.\nWe assess its finite sample performance through the applications of Wasserstein\ndistance estimation and generative models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:53:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Meng", "Cheng", ""], ["Ke", "Yuan", ""], ["Zhang", "Jingyi", ""], ["Zhang", "Mengrui", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2106.05840", "submitter": "Mian Adnan", "authors": "Mian Arif Shams Adnan, H. M. Miraz Mahmud", "title": "A Bagging and Boosting Based Convexly Combined Optimum Mixture\n  Probabilistic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unlike previous studies on mixture distributions, a bagging and boosting\nbased convexly combined mixture probabilistic model has been suggested. This\nmodel is a result of iteratively searching for obtaining the optimum\nprobabilistic model that provides the maximum p value.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:20:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Adnan", "Mian Arif Shams", ""], ["Mahmud", "H. M. Miraz", ""]]}, {"id": "2106.05850", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Raymond K. W. Wong, Xiaojun Mao, Kwun Chuen Gary Chan", "title": "Matrix Completion with Model-free Weighting", "comments": "Proceedings of the 38th International Conference on Machine Learning,\n  PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for matrix completion under general\nnon-uniform missing structures. By controlling an upper bound of a novel\nbalancing error, we construct weights that can actively adjust for the\nnon-uniformity in the empirical risk without explicitly modeling the\nobservation probabilities, and can be computed efficiently via convex\noptimization. The recovered matrix based on the proposed weighted empirical\nrisk enjoys appealing theoretical guarantees. In particular, the proposed\nmethod achieves a stronger guarantee than existing work in terms of the scaling\nwith respect to the observation probabilities, under asymptotically\nheterogeneous missing settings (where entry-wise observation probabilities can\nbe of different orders). These settings can be regarded as a better theoretical\nmodel of missing patterns with highly varying probabilities. We also provide a\nnew minimax lower bound under a class of heterogeneous settings. Numerical\nexperiments are also provided to demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:28:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Jiayi", ""], ["Wong", "Raymond K. W.", ""], ["Mao", "Xiaojun", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "2106.05890", "submitter": "Nazar Buzun", "authors": "Nazar Buzun, Nikolay Shvetsov, Dmitry V. Dylov", "title": "Strong Gaussian Approximation for the Sum of Random Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper derives a new strong Gaussian approximation bound for the sum of\nindependent random vectors. The approach relies on the optimal transport theory\nand yields explicit dependence on the dimension size $p$ and the sample size\n$n$. This dependence establishes a new fundamental limit for all practical\napplications of statistical learning theory. Particularly, based on this bound,\nwe prove approximation by distribution for the maximum norm in a\nhigh-dimensional setting ($p >n$).\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:17:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Buzun", "Nazar", ""], ["Shvetsov", "Nikolay", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2106.05918", "submitter": "Wei Peng", "authors": "Wei Peng, Lucas Mentch, Leonard Stefanski", "title": "Bias, Consistency, and Alternative Perspectives of the Infinitesimal\n  Jackknife", "comments": "57 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though introduced nearly 50 years ago, the infinitesimal jackknife (IJ)\nremains a popular modern tool for quantifying predictive uncertainty in complex\nestimation settings. In particular, when supervised learning ensembles are\nconstructed via bootstrap samples, recent work demonstrated that the IJ\nestimate of variance is particularly convenient and useful. However, despite\nthe algebraic simplicity of its final form, its derivation is rather complex.\nAs a result, studies clarifying the intuition behind the estimator or\nrigorously investigating its properties have been severely lacking. This work\naims to take a step forward on both fronts. We demonstrate that surprisingly,\nthe exact form of the IJ estimator can be obtained via a straightforward linear\nregression of the individual bootstrap estimates on their respective weights or\nvia the classical jackknife. The latter realization is particularly useful as\nit allows us to formally investigate the bias of the IJ variance estimator and\nbetter characterize the settings in which its use is appropriate. Finally, we\nextend these results to the case of U-statistics where base models are\nconstructed via subsampling rather than bootstrapping and provide a consistent\nestimate of the resulting variance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:03:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Peng", "Wei", ""], ["Mentch", "Lucas", ""], ["Stefanski", "Leonard", ""]]}, {"id": "2106.05925", "submitter": "Jian Huang", "authors": "Ruijian Han, Lan Luo, Yuanyuan Lin, Jian Huang", "title": "Online Debiased Lasso", "comments": "Ruijian Han and Lan Luo contributed equally to this work.\n  Co-corresponding authors: Yuanyuan Lin (Email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (Email: jian-huang@uiowa.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an online debiased lasso (ODL) method for statistical inference in\nhigh-dimensional linear models with streaming data. The proposed ODL consists\nof an efficient computational algorithm for streaming data and approximately\nnormal estimators for the regression coefficients. Its implementation only\nrequires the availability of the current data batch in the data stream and\nsufficient statistics of the historical data at each stage of the analysis. A\nnew dynamic procedure is developed to select and update the tuning parameters\nupon the arrival of each new data batch so that we can adjust the amount of\nregularization adaptively along the data stream. The asymptotic normality of\nthe ODL estimator is established under the conditions similar to those in an\noffline setting and mild conditions on the size of data batches in the stream,\nwhich provides theoretical justification for the proposed online statistical\ninference procedure. We conduct extensive numerical experiments to evaluate the\nperformance of ODL. These experiments demonstrate the effectiveness of our\nalgorithm and support the theoretical results. An air quality dataset is\nanalyzed to illustrate the application of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:17:29 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Han", "Ruijian", ""], ["Luo", "Lan", ""], ["Lin", "Yuanyuan", ""], ["Huang", "Jian", ""]]}, {"id": "2106.05955", "submitter": "Zuzanna Szyma\\'nska Ph.D.", "authors": "Zuzanna Szyma\\'nska and Jakub Skrzeczkowski and B{\\l}a\\.zej Miasojedow\n  and Piotr Gwiazda", "title": "Bayesian inference of a non-local proliferation model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a systems biology perspective the majority of cancer models, although\ninteresting and providing a qualitative explanation of some problems, have a\nmajor disadvantage in that they usually miss a genuine connection with\nexperimental data. Having this in mind, in this paper, we aim at contributing\nto the improvement of many cancer models which contain a proliferation term. To\nthis end, we propose a new non-local model of cell proliferation. We select\ndata which are suitable to perform a Bayesian inference for unknown parameters\nand we provide a discussion on the range of applicability of the model.\nFurthermore, we provide proof of the stability of a posteriori distributions in\ntotal variation norm which exploits the theory of spaces of measures equipped\nwith the weighted flat norm. In a companion paper, we provide a detailed proof\nof the well-posedness of the problem and we investigate the convergence of the\nEBT algorithm applied to solve the equation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:50:20 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 09:41:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Szyma\u0144ska", "Zuzanna", ""], ["Skrzeczkowski", "Jakub", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Gwiazda", "Piotr", ""]]}, {"id": "2106.06190", "submitter": "Johannes Maly", "authors": "Johannes Maly, Tianyu Yang, Sjoerd Dirksen, Holger Rauhut, Giuseppe\n  Caire", "title": "New challenges in covariance estimation: multiple structures and coarse\n  quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this self-contained chapter, we revisit a fundamental problem of\nmultivariate statistics: estimating covariance matrices from finitely many\nindependent samples. Based on massive Multiple-Input Multiple-Output (MIMO)\nsystems we illustrate the necessity of leveraging structure and considering\nquantization of samples when estimating covariance matrices in practice. We\nthen provide a selective survey of theoretical advances of the last decade\nfocusing on the estimation of structured covariance matrices. This review is\nspiced up by some yet unpublished insights on how to benefit from combined\nstructural constraints. Finally, we summarize the findings of our recently\npublished preprint \"Covariance estimation under one-bit quantization\" to show\nhow guaranteed covariance estimation is possible even under coarse quantization\nof the samples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 06:39:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Maly", "Johannes", ""], ["Yang", "Tianyu", ""], ["Dirksen", "Sjoerd", ""], ["Rauhut", "Holger", ""], ["Caire", "Giuseppe", ""]]}, {"id": "2106.06225", "submitter": "Qixian Zhong", "authors": "Qixian Zhong and Jane-Ling Wang", "title": "Neural Networks for Partially Linear Quantile Regression", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has enjoyed tremendous success in a variety of applications but\nits application to quantile regressions remains scarce. A major advantage of\nthe deep learning approach is its flexibility to model complex data in a more\nparsimonious way than nonparametric smoothing methods. However, while deep\nlearning brought breakthroughs in prediction, it often lacks interpretability\ndue to the black-box nature of multilayer structure with millions of\nparameters, hence it is not well suited for statistical inference. In this\npaper, we leverage the advantages of deep learning to apply it to quantile\nregression where the goal to produce interpretable results and perform\nstatistical inference. We achieve this by adopting a semiparametric approach\nbased on the partially linear quantile regression model, where covariates of\nprimary interest for statistical inference are modelled linearly and all other\ncovariates are modelled nonparametrically by means of a deep neural network. In\naddition to the new methodology, we provide theoretical justification for the\nproposed model by establishing the root-$n$ consistency and asymptotically\nnormality of the parametric coefficient estimator and the minimax optimal\nconvergence rate of the neural nonparametric function estimator. Across several\nsimulated and real data examples, our proposed model empirically produces\nsuperior estimates and more accurate predictions than various alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:07:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhong", "Qixian", ""], ["Wang", "Jane-Ling", ""]]}, {"id": "2106.06266", "submitter": "Sebastian Engelke", "authors": "Corina Birghila, Maximilian Aigner, Sebastian Engelke", "title": "Distributionally robust tail bounds based on Wasserstein distance and\n  $f$-divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide robust bounds on the tail probabilities and the tail\nindex of heavy-tailed distributions in the context of model misspecification.\nThey are defined as the optimal value when computing the worst-case tail\nbehavior over all models within some neighborhood of the reference model. The\nchoice of the discrepancy between the models used to build this neighborhood\nplays a crucial role in assessing the size of the asymptotic bounds. We\nevaluate the robust tail behavior in ambiguity sets based on the Wasserstein\ndistance and Csisz\\'ar $f$-divergence and obtain explicit expressions for the\ncorresponding asymptotic bounds. In an application to Danish fire insurance\nclaims we compare the difference between these bounds and show the importance\nof the choice of discrepancy measure.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:33:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Birghila", "Corina", ""], ["Aigner", "Maximilian", ""], ["Engelke", "Sebastian", ""]]}, {"id": "2106.06308", "submitter": "Davin Choo", "authors": "Davin Choo, Tommaso d'Orsi", "title": "The Complexity of Sparse Tensor PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sparse tensor principal component analysis: given a\ntensor $\\pmb Y = \\pmb W + \\lambda x^{\\otimes p}$ with $\\pmb W \\in\n\\otimes^p\\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover\nthe $k$-sparse unit vector $x \\in \\mathbb{R}^n$. The model captures both sparse\nPCA (in its Wigner form) and tensor PCA.\n  For the highly sparse regime of $k \\leq \\sqrt{n}$, we present a family of\nalgorithms that smoothly interpolates between a simple polynomial-time\nalgorithm and the exponential-time exhaustive search algorithm. For any $1 \\leq\nt \\leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio\n$\\lambda \\geq \\tilde{\\mathcal{O}} (\\sqrt{t} \\cdot (k/t)^{p/2})$ in time\n$\\tilde{\\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for\nthe matrix settings (in both the polynomial-time and sub-exponential time\nregimes).\n  Our results naturally extend to the case of $r$ distinct $k$-sparse signals\nwith disjoint supports, with guarantees that are independent of the number of\nspikes. Even in the restricted case of sparse PCA, known algorithms only\nrecover the sparse vectors for $\\lambda \\geq \\tilde{\\mathcal{O}}(k \\cdot r)$\nwhile our algorithms require $\\lambda \\geq \\tilde{\\mathcal{O}}(k)$.\n  Finally, by analyzing the low-degree likelihood ratio, we complement these\nalgorithmic results with rigorous evidence illustrating the trade-offs between\nsignal-to-noise ratio and running time. This lower bound captures the known\nlower bounds for both sparse PCA and tensor PCA. In this general model, we\nobserve a more intricate three-way trade-off between the number of samples $n$,\nthe sparsity $k$, and the tensor power $p$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:57:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Choo", "Davin", ""], ["d'Orsi", "Tommaso", ""]]}, {"id": "2106.06513", "submitter": "Luca Ratti", "authors": "Giovanni S. Alberti, Ernesto De Vito, Matti Lassas, Luca Ratti, Matteo\n  Santacesaria", "title": "Learning the optimal regularizer for inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the linear inverse problem $y=Ax+\\epsilon$, where\n$A\\colon X\\to Y$ is a known linear operator between the separable Hilbert\nspaces $X$ and $Y$, $x$ is a random variable in $X$ and $\\epsilon$ is a\nzero-mean random process in $Y$. This setting covers several inverse problems\nin imaging including denoising, deblurring, and X-ray tomography. Within the\nclassical framework of regularization, we focus on the case where the\nregularization functional is not given a priori but learned from data. Our\nfirst result is a characterization of the optimal generalized Tikhonov\nregularizer, with respect to the mean squared error. We find that it is\ncompletely independent of the forward operator $A$ and depends only on the mean\nand covariance of $x$. Then, we consider the problem of learning the\nregularizer from a finite training set in two different frameworks: one\nsupervised, based on samples of both $x$ and $y$, and one unsupervised, based\nonly on samples of $x$. In both cases, we prove generalization bounds, under\nsome weak assumptions on the distribution of $x$ and $\\epsilon$, including the\ncase of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces,\nthereby showing that finer and finer discretizations do not make this learning\nproblem harder. The results are validated through numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:14:27 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Alberti", "Giovanni S.", ""], ["De Vito", "Ernesto", ""], ["Lassas", "Matti", ""], ["Ratti", "Luca", ""], ["Santacesaria", "Matteo", ""]]}, {"id": "2106.06597", "submitter": "Stephen Walker", "authors": "Stephen G Walker", "title": "On an Asymptotic Distribution for the MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The paper presents a novel asymptotic distribution for a mle when the\nlog--likelihood is strictly concave in the parameter for all data points; for\nexample, the exponential family. The new asymptotic distribution can be seen as\na refinement of the usual normal asymptotic distribution and is comparable to\nan Edgeworth expansion. However, it is obtained with weaker conditions than\neven those for asymptotic normality. The same technique is then used to find\nthe exact distribution of the weighted likelihood bootstrap sampler.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:08:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Walker", "Stephen G", ""]]}, {"id": "2106.06779", "submitter": "Nomvelo Sibisi", "authors": "Nomvelo Sibisi", "title": "A Cluster Model for Growth of Random Trees", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We first consider the growth of trees by probabilistic attachment of new\nvertices to leaves. This leads to a growth model based on vertex clusters and\nprobabilities assigned to clusters. This model turns out to be readily\napplicable to attachment at any depth of the tree, hence the paper evolves to a\ngeneral study of tree growth by cluster-based attachment. Drawing inspiration\nfrom the concept of intrinsic vertex fitness due to Bianconi and Barab\\'asi, we\nintroduce vertex mass as an additive intrinsic vertex attribute. Unlike\nBianconi and Barab\\'asi who used fitness as a vertex degree multiplier in the\ncontext of growth by preferential attachment, we treat vertex mass as a\nfundamental probabilistic construct whose additivity plays a primary role.\nNotably, independent mass distributions induce a distribution on the sum of\nsuch masses through Laplace convolution. In this way, clusters of vertices\ninherit their mass distributions from vertices within the cluster.\n  Our main contribution is a novel theorem for the joint distribution of\ncluster masses, conditioned on their respective distributions. As described by\nFerguson and Kingman in the context of distributions on general measures, the\nchoice of gamma conditioning distributions leads to the Dirichlet distribution.\nBeyond gamma conditioning distributions, our theorem allows other choices, such\nas the fat-tailed stable distributions with infinite mean. We discuss L\\'evy\nconditioning distributions as a gamma alternative, the L\\'evy distribution\nbeing a notable instance of the stable family. We conclude with a theorem\ngiving the analytic marginals of the normalised distribution conditioned on the\nL\\'evy distribution.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 13:48:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sibisi", "Nomvelo", ""]]}, {"id": "2106.06894", "submitter": "Langxuan Su", "authors": "Langxuan Su, Sayan Mukherjee", "title": "A Large Deviation Approach to Posterior Consistency in Dynamical Systems", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide asymptotic results concerning (generalized)\nBayesian inference for certain dynamical systems based on a large deviation\napproach. Given a sequence of observations $y$, a class of model processes\nparameterized by $\\theta \\in \\Theta$ which can be characterized as a stochastic\nprocess $X^\\theta$ or a measure $\\mu_\\theta$, and a loss function $L$ which\nmeasures the error between $y$ and a realization of $X^\\theta$, we specify the\ngeneralized posterior distribution $\\pi_t(\\theta \\mid y)$. The goal of this\npaper is to study the asymptotic behavior of $\\pi_t(\\theta \\mid y)$ as $t \\to\n\\infty.$ In particular, we state conditions on the model family\n$\\{\\mu_\\theta\\}_{\\theta \\in \\Theta}$ and the loss function $L$ such that the\nposterior distribution converges. The two conditions we require are: (1) a\nconditional large deviation behavior for a single $X^\\theta$, and (2) an\nexponential continuity condition over the model family for the map from the\nparameter $\\theta$ to the loss incurred between $X^\\theta$ and the observation\nsequence $y$. The proposed framework is quite general, we apply it to two very\ndifferent classes of dynamical systems: continuous time hypermixing processes\nand Gibbs processes on shifts of finite type. We also show that the generalized\nposterior distribution concentrates asymptotically on those parameters that\nminimize the expected loss and a divergence term, hence proving posterior\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 01:47:22 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Su", "Langxuan", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "2106.07024", "submitter": "Sebastian Espinosa", "authors": "Sebastian Espinosa, Jorge F. Silva and Pablo Piantanida", "title": "Finite-Length Bounds on Hypothesis Testing Subject to Vanishing Type I\n  Error Restrictions", "comments": null, "journal-ref": "Vol. 28, 2021, 229 - 233", "doi": "10.1109/LSP.2021.3050381", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in Binary Hypothesis Testing (BHT) is to determine the\noptimal tradeoff between the Type I error (referred to as false alarm) and Type\nII (referred to as miss) error. In this context, the exponential rate of\nconvergence of the optimal miss error probability -- as the sample size tends\nto infinity -- given some (positive) restrictions on the false alarm\nprobabilities is a fundamental question to address in theory. Considering the\nmore realistic context of a BHT with a finite number of observations, this\npaper presents a new non-asymptotic result for the scenario with monotonic\n(sub-exponential decreasing) restriction on the Type I error probability, which\nextends the result presented by Strassen in 2009. Building on the use of\nconcentration inequalities, we offer new upper and lower bounds to the optimal\nType II error probability for the case of finite observations. Finally, the\nderived bounds are evaluated and interpreted numerically (as a function of the\nnumber samples) for some vanishing Type I error restrictions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:30:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Espinosa", "Sebastian", ""], ["Silva", "Jorge F.", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2106.07044", "submitter": "Arnak Dalalyan S.", "authors": "Tigran Galstyan, Arshak Minasyan, Arnak Dalalyan", "title": "Optimal detection of the feature matching map in presence of noise and\n  outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the matching map between two sets of $d$\ndimensional vectors from noisy observations, where the second set contains\noutliers. The matching map is then an injection, which can be consistently\nestimated only if the vectors of the second set are well separated. The main\nresult shows that, in the high-dimensional setting, a detection region of\nunknown injection can be characterized by the sets of vectors for which the\ninlier-inlier distance is of order at least $d^{1/4}$ and the inlier-outlier\ndistance is of order at least $d^{1/2}$. These rates are achieved using the\nestimated matching minimizing the sum of logarithms of distances between\nmatched pairs of points. We also prove lower bounds establishing optimality of\nthese rates. Finally, we report results of numerical experiments on both\nsynthetic and real world data that illustrate our theoretical results and\nprovide further insight into the properties of the estimators studied in this\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:08:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Galstyan", "Tigran", ""], ["Minasyan", "Arshak", ""], ["Dalalyan", "Arnak", ""]]}, {"id": "2106.07053", "submitter": "Qingyun Sun", "authors": "Qingyun Sun and David Donoho", "title": "Convex Sparse Blind Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.SY eess.SY math.IT math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the blind deconvolution problem, we observe the convolution of an unknown\nfilter and unknown signal and attempt to reconstruct the filter and signal. The\nproblem seems impossible in general, since there are seemingly many more\nunknowns than knowns . Nevertheless, this problem arises in many application\nfields; and empirically, some of these fields have had success using heuristic\nmethods -- even economically very important ones, in wireless communications\nand oil exploration. Today's fashionable heuristic formulations pose non-convex\noptimization problems which are then attacked heuristically as well. The fact\nthat blind deconvolution can be solved under some repeatable and\nnaturally-occurring circumstances poses a theoretical puzzle.\n  To bridge the gulf between reported successes and theory's limited\nunderstanding, we exhibit a convex optimization problem that -- assuming signal\nsparsity -- can convert a crude approximation to the true filter into a\nhigh-accuracy recovery of the true filter. Our proposed formulation is based on\nL1 minimization of inverse filter outputs. We give sharp guarantees on\nperformance of the minimizer assuming sparsity of signal, showing that our\nproposal precisely recovers the true inverse filter, up to shift and rescaling.\nThere is a sparsity/initial accuracy tradeoff: the less accurate the initial\napproximation, the greater we rely on sparsity to enable exact recovery. To our\nknowledge this is the first reported tradeoff of this kind. We consider it\nsurprising that this tradeoff is independent of dimension.\n  We also develop finite-$N$ guarantees, for highly accurate reconstruction\nunder $N\\geq O(k \\log(k) )$ with high probability. We further show stable\napproximation when the true inverse filter is infinitely long and extend our\nguarantees to the case where the observations are contaminated by stochastic or\nadversarial noise.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:39:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sun", "Qingyun", ""], ["Donoho", "David", ""]]}, {"id": "2106.07054", "submitter": "Azadeh Khaleghi", "authors": "Azadeh Khaleghi and G\\'abor Lugosi", "title": "Inferring the mixing properties of an ergodic process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose strongly consistent estimators of the $\\ell_1$ norm of the\nsequence of $\\alpha$-mixing (respectively $\\beta$-mixing) coefficients of a\nstationary ergodic process. We further provide strongly consistent estimators\nof individual $\\alpha$-mixing (respectively $\\beta$-mixing) coefficients for a\nsubclass of stationary $\\alpha$-mixing (respectively $\\beta$-mixing) processes\nwith summable sequences of mixing coefficients. The estimators are in turn used\nto develop strongly consistent goodness-of-fit hypothesis tests. In particular,\nwe develop hypothesis tests to determine whether, under the same summability\nassumption, the $\\alpha$-mixing (respectively $\\beta$-mixing) coefficients of a\nprocess are upper bounded by a given rate function. Moreover, given a sample\ngenerated by a (not necessarily mixing) stationary ergodic process, we provide\na consistent test to discern the null hypothesis that the $\\ell_1$ norm of the\nsequence $\\boldsymbol{\\alpha}$ of $\\alpha$-mixing coefficients of the process\nis bounded by a given threshold $\\gamma \\in [0,\\infty)$ from the alternative\nhypothesis that $\\left\\lVert \\boldsymbol{\\alpha} \\right\\rVert> \\gamma$. An\nanalogous goodness-of-fit test is proposed for the $\\ell_1$ norm of the\nsequence of $\\beta$-mixing coefficients of a stationary ergodic process.\nMoreover, the procedure gives rise to an asymptotically consistent test for\nindependence.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:39:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Khaleghi", "Azadeh", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "2106.07138", "submitter": "Shulei Wang", "authors": "Shulei Wang", "title": "Self-Supervised Metric Learning in Multi-View Data: A Downstream Task\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised metric learning has been a successful approach for learning a\ndistance from an unlabeled dataset. The resulting distance is broadly useful\nfor improving various distance-based downstream tasks, even when no information\nfrom downstream tasks is utilized in the metric learning stage. To gain\ninsights into this approach, we develop a statistical framework to\ntheoretically study how self-supervised metric learning can benefit downstream\ntasks in the context of multi-view data. Under this framework, we show that the\ntarget distance of metric learning satisfies several desired properties for the\ndownstream tasks. On the other hand, our investigation suggests the target\ndistance can be further improved by moderating each direction's weights. In\naddition, our analysis precisely characterizes the improvement by\nself-supervised metric learning on four commonly used downstream tasks: sample\nidentification, two-sample testing, $k$-means clustering, and $k$-nearest\nneighbor classification. As a by-product, we propose a simple spectral method\nfor self-supervised metric learning, which is computationally efficient and\nminimax optimal for estimating target distance. Finally, numerical experiments\nare presented to support the theoretical results in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:34:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Shulei", ""]]}, {"id": "2106.07191", "submitter": "Zhengqing Zhou", "authors": "Zhengqing Zhou, Jose Blanchet, Peter W. Glynn", "title": "Distributionally Robust Martingale Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST q-fin.CP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of bounding path-dependent expectations (within any\nfinite time horizon $d$) over the class of discrete-time martingales whose\nmarginal distributions lie within a prescribed tolerance of a given collection\nof benchmark marginal distributions. This problem is a relaxation of the\nmartingale optimal transport (MOT) problem and is motivated by applications to\nsuper-hedging in financial markets. We show that the empirical version of our\nrelaxed MOT problem can be approximated within $O\\left( n^{-1/2}\\right)$ error\nwhere $n$ is the number of samples of each of the individual marginal\ndistributions (generated independently) and using a suitably constructed\nfinite-dimensional linear programming problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 06:38:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhou", "Zhengqing", ""], ["Blanchet", "Jose", ""], ["Glynn", "Peter W.", ""]]}, {"id": "2106.07222", "submitter": "Messan Martial Amovin-Assagba", "authors": "Martial Amovin-Assagba (ERIC, AMK), Ir\\`ene Gannaz, Julien Jacques\n  (ERIC)", "title": "Outlier detection in multivariate functional data through a contaminated\n  mixture model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by an application in an industrial context, where the\nactivity of sensors is recorded at a high frequency. The objective is to\nautomatically detect abnormal measurement behaviour. Considering the sensor\nmeasures as functional data, we are formally interested in detecting outliers\nin a multivariate functional data set. Due to the heterogeneity of this data\nset, the proposed contaminated mixture model both clusters the multivariate\nfunctional data into homogeneous groups and detects outliers. The main\nadvantage of this procedure over its competitors is that it does not require us\nto specify the proportion of outliers. Model inference is performed through an\nExpectation-Conditional Maximization algorithm, and the BIC criterion is used\nto select the number of clusters. Numerical experiments on simulated data\ndemonstrate the high performance achieved by the inference algorithm. In\nparticular, the proposed model outperforms competitors. Its application on the\nreal data which motivated this study allows us to correctly detect abnormal\nbehaviours.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:17:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Amovin-Assagba", "Martial", "", "ERIC, AMK"], ["Gannaz", "Ir\u00e8ne", "", "ERIC"], ["Jacques", "Julien", "", "ERIC"]]}, {"id": "2106.07401", "submitter": "Dylan Spicker", "authors": "Dylan Spicker, Michael P Wallace, Grace Y Yi", "title": "Generalizations to Corrections for the Effects of Measurement Error in\n  Approximately Consistent Methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measurement error is a pervasive issue which renders the results of an\nanalysis unreliable. The measurement error literature contains numerous\ncorrection techniques, which can be broadly divided into those which aim to\nproduce exactly consistent estimators, and those which are only approximately\nconsistent. While consistency is a desirable property, it is typically attained\nonly under specific model assumptions. Two approximately consistent techniques,\nregression calibration and simulation extrapolation, are used frequently in a\nwide variety of parametric and semiparametric settings. We generalize these\ncorrections, relaxing assumptions placed on replicate measurements. Under\nregularity conditions, the estimators are shown to be asymptotically normal,\nwith a sandwich estimator for the asymptotic variance. Through simulation, we\ndemonstrate the improved performance of our estimators, over the standard\ntechniques, when these assumptions are violated. We motivate these corrections\nusing the Framingham Heart Study, and apply our generalized techniques to an\nanalysis of these data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:02:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Spicker", "Dylan", ""], ["Wallace", "Michael P", ""], ["Yi", "Grace Y", ""]]}, {"id": "2106.07523", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Dependency in DAG models with Hidden Variables", "comments": "In Proceedings of the 37th Conference on Artificial Intelligence; 12\n  pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Directed acyclic graph models with hidden variables have been much studied,\nparticularly in view of their computational efficiency and connection with\ncausal methods. In this paper we provide the circumstances under which it is\npossible for two variables to be identically equal, while all other observed\nvariables stay jointly independent of them and mutually of each other. We find\nthat this is possible if and only if the two variables are `densely connected';\nin other words, if applications of identifiable causal interventions on the\ngraph cannot (non-trivially) separate them. As a consequence of this, we can\nalso allow such pairs of random variables have any bivariate joint distribution\nthat we choose. This has implications for model search, since it suggests that\nwe can reduce to only consider graphs in which densely connected vertices are\nalways joined by an edge.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:47:07 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "2106.07529", "submitter": "Giovanna Nappo", "authors": "Emilio De Santis, Antonio Galves, Giovanna Nappo, and Mauro Piccioni", "title": "Estimating the interaction graph of stochastic neuronal dynamics by\n  observing only pairs of neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": "Roma01.Math", "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the questions of identifying pairs of interacting neurons from the\nobservation of their spiking activity. The neuronal network is modeled by a\nsystem of interacting point processes with memory of variable length. The\ninfluence of a neuron on another can be either excitatory or inhibitory. To\nidentify the existence and the nature of an interaction we propose an algorithm\nbased only on the observation of joint activity of the two neurons in\nsuccessive time slots. This reduces the amount of computation and storage\nrequired to run the algorithm, thereby making the algorithm suitable for the\nanalysis of real neuronal data sets. We obtain computable upper bounds for the\nprobabilities of false positive and false negative detection. As a corollary we\nprove the consistency of the identification algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:56:25 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 18:42:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["De Santis", "Emilio", ""], ["Galves", "Antonio", ""], ["Nappo", "Giovanna", ""], ["Piccioni", "Mauro", ""]]}, {"id": "2106.07587", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Quasi-Maximum Likelihood based Model Selection Procedures for Binary\n  Outcomes", "comments": "Keywords: Causal inference, Unmeasured covariates, Quasi-maximum\n  lilkelihood, Two-stage residual inclusion, Model selection, Consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I propose two model selection procedures based on a\nquasi-maximum likelihood estimator when there exist unmeasured covariates. I\nprove that a proposed BIC-type model selection procedure has model selection\nconsistency, and confirm these property through simulation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:45:15 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 05:33:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2106.07725", "submitter": "Yandi Shen", "authors": "Qiyang Han and Yandi Shen", "title": "Generalized kernel distance covariance in high dimensions: non-null CLTs\n  and power universality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Distance covariance is a popular dependence measure for two random vectors\n$X$ and $Y$ of possibly different dimensions and types. Recent years have\nwitnessed concentrated efforts in the literature to understand the\ndistributional properties of the sample distance covariance in a\nhigh-dimensional setting, with an exclusive emphasis on the null case that $X$\nand $Y$ are independent. This paper derives the first non-null central limit\ntheorem for the sample distance covariance, and the more general sample\n(Hilbert-Schmidt) kernel distance covariance in high dimensions, primarily in\nthe Gaussian case. The new non-null central limit theorem yields an\nasymptotically exact first-order power formula for the widely used generalized\nkernel distance correlation test of independence between $X$ and $Y$. The power\nformula in particular unveils an interesting universality phenomenon: the power\nof the generalized kernel distance correlation test is completely determined by\n$n\\cdot \\text{dcor}^2(X,Y)/\\sqrt{2}$ in the high dimensional limit, regardless\nof a wide range of choices of the kernels and bandwidth parameters.\nFurthermore, this separation rate is also shown to be optimal in a minimax\nsense. The key step in the proof of the non-null central limit theorem is a\nprecise expansion of the mean and variance of the sample distance covariance in\nhigh dimensions, which shows, among other things, that the non-null Gaussian\napproximation of the sample distance covariance involves a rather subtle\ninterplay between the dimension-to-sample ratio and the dependence between $X$\nand $Y$.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:42:43 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Han", "Qiyang", ""], ["Shen", "Yandi", ""]]}, {"id": "2106.08297", "submitter": "Giovanna Nappo", "authors": "Rachele Foschi, Giovanna Nappo, Fabio L. Spizzichino", "title": "Diagonal sections of copulas, multivariate conditional hazard rates and\n  distributions of order statistics for minimally stable lifetimes", "comments": null, "journal-ref": null, "doi": null, "report-no": "Roma01.Math", "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a motivating problem, we aim to study some special aspects of the marginal\ndistributions of the order statistics for exchangeable and (more generally) for\nminimally stable non-negative random variables $T_{1},...,T_{r}$. In any case,\nwe assume that $T_{1},...,T_{r}$ are identically distributed, with a common\nsurvival function $\\overline{G}$ and their survival copula is denoted by $K$.\nThe diagonal's and subdiagonals' sections of $K$, along with $\\overline{G}$,\nare possible tools to describe the information needed to recover the laws of\norder statistics.\n  When attention is restricted to the absolutely continuous case, such a joint\ndistribution can be described in terms of the associated multivariate\nconditional hazard rate (m.c.h.r.) functions. We then study the distributions\nof the order statistics of $T_{1},...,T_{r}$ also in terms of the system of the\nm.c.h.r. functions. We compare and, in a sense, we combine the two different\napproaches in order to obtain different detailed formulas and to analyze some\nprobabilistic aspects for the distributions of interest. This study also leads\nus to compare the two cases of exchangeable and minimally stable variables both\nin terms of copulas and of m.c.h.r. functions. The paper concludes with the\nanalysis of two remarkable special cases of stochastic dependence, namely\nArchimedean copulas and load sharing models. This analysis will allow us to\nprovide some illustrative examples, and some discussion about peculiar aspects\nof our results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:18:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Foschi", "Rachele", ""], ["Nappo", "Giovanna", ""], ["Spizzichino", "Fabio L.", ""]]}, {"id": "2106.08305", "submitter": "Carlos Am\\'endola", "authors": "Carlos Am\\'endola, Ben Hollering, Seth Sullivant, Ngoc Tran", "title": "Markov Equivalence of Max-Linear Bayesian Networks", "comments": "19 pages, 5 figures, accepted for the 37th conference on Uncertainty\n  in Artificial Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-linear Bayesian networks have emerged as highly applicable models for\ncausal inference via extreme value data. However, conditional independence (CI)\nfor max-linear Bayesian networks behaves differently than for classical\nGaussian Bayesian networks. We establish the parallel between the two theories\nvia tropicalization, and establish the surprising result that the Markov\nequivalence classes for max-linear Bayesian networks coincide with the ones\nobtained by regular CI. Our paper opens up many problems at the intersection of\nextreme value statistics, causal inference and tropical geometry.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:27:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Hollering", "Ben", ""], ["Sullivant", "Seth", ""], ["Tran", "Ngoc", ""]]}, {"id": "2106.08396", "submitter": "Shyam Narayanan", "authors": "Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep\n  Silwal, Tal Wagner", "title": "Learning-based Support Estimation in Sublinear Time", "comments": "17 pages. Published as a conference paper in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the number of distinct elements in a\nlarge data set (or, equivalently, the support size of the distribution induced\nby the data set) from a random sample of its elements. The problem occurs in\nmany applications, including biology, genomics, computer systems and\nlinguistics. A line of research spanning the last decade resulted in algorithms\nthat estimate the support up to $ \\pm \\varepsilon n$ from a sample of size\n$O(\\log^2(1/\\varepsilon) \\cdot n/\\log n)$, where $n$ is the data set size.\nUnfortunately, this bound is known to be tight, limiting further improvements\nto the complexity of this problem. In this paper we consider estimation\nalgorithms augmented with a machine-learning-based predictor that, given any\nelement, returns an estimation of its frequency. We show that if the predictor\nis correct up to a constant approximation factor, then the sample complexity\ncan be reduced significantly, to \\[ \\ \\log (1/\\varepsilon) \\cdot\nn^{1-\\Theta(1/\\log(1/\\varepsilon))}. \\] We evaluate the proposed algorithms on\na collection of data sets, using the neural-network based estimators from {Hsu\net al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to\n3x) improvements in the estimation accuracy compared to the state of the art\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 19:53:12 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Eden", "Talya", ""], ["Indyk", "Piotr", ""], ["Narayanan", "Shyam", ""], ["Rubinfeld", "Ronitt", ""], ["Silwal", "Sandeep", ""], ["Wagner", "Tal", ""]]}, {"id": "2106.08460", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "Localized Conformal Prediction: A Generalized Inference Framework for\n  Conformal Prediction", "comments": "This paper is based on the results on localized conformal prediction\n  under the i.i.d settings from arXiv:1908.08558, with strengthened theoretical\n  results, new and more efficient algorithms, and additional empirical studies.\n  50 pages; 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new inference framework called localized conformal prediction.\nIt generalizes the framework of conformal prediction and offers a\nsingle-test-sample adaptive construction by emphasizing a local region around\nit. Although there have been methods constructing heterogeneous prediction\nintervals for $Y$ by designing better conformal score functions, to our\nknowledge, this is the first work that introduces an adaptive nature to the\ninference framework itself. We prove that our proposal leads to an\nassumption-free and finite sample marginal coverage guarantee, as well as an\napproximate conditional coverage guarantee. Our proposal achieves asymptotic\nconditional coverage under suitable assumptions.\n  The localized conformal prediction can be combined with many existing works\nin conformal prediction, including different types of conformal score\nconstructions. We will demonstrate how to change from conformal prediction to\nlocalized conformal prediction in these related works and a potential gain via\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:10:43 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "2106.08472", "submitter": "Bikramjit Das", "authors": "Bikramjit Das, Tiandong Wang and Gengling Dai", "title": "Asymptotic Behavior of Common Connections in Sparse Random Networks", "comments": "21 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random network models generated using sparse exchangeable graphs have\nprovided a mechanism to study a wide variety of complex real-life networks. In\nparticular, these models help with investigating power-law properties of degree\ndistributions, number of edges, and other relevant network metrics which\nsupport the scale-free structure of networks. Previous work on such graphs\nimposes a marginal assumption of univariate regular variation (e.g., power-law\ntail) on the bivariate generating graphex function. In this paper, we study\nsparse exchangeable graphs generated by graphex functions which are\nmultivariate regularly varying. We also focus on a different metric for our\nstudy: the distribution of the number of common vertices (connections) shared\nby a pair of vertices. The number being high for a fixed pair is an indicator\nof the original pair of vertices being connected. We find that the distribution\nof number of common connections are regularly varying as well, where the tail\nindices of regular variation are governed by the type of graphex function used.\nOur results are verified on simulated graphs by estimating the relevant tail\nindex parameters.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:43:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Das", "Bikramjit", ""], ["Wang", "Tiandong", ""], ["Dai", "Gengling", ""]]}, {"id": "2106.08626", "submitter": "Sim\\'eon Val\\`ere Bitseki Penda", "authors": "S. Val\\`ere Bitseki Penda and Jean-Fran\\c{c}ois Delmas", "title": "Central limit theorem for kernel estimator of invariant density in\n  bifurcating Markov chains models", "comments": "40 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:2012.04741; text overlap with arXiv:2106.07711", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bifurcating Markov chains (BMC) are Markov chains indexed by a full binary\ntree representing the evolution of a trait along a population where each\nindividual has two children. Motivated by the functional estimation of the\ndensity of the invariant probability measure which appears as the asymptotic\ndistribution of the trait, we prove the consistence and the Gaussian\nfluctuations for a kernel estimator of this density based on late generations.\nIn this setting, it is interesting to note that the distinction of the three\nregimes on the ergodic rate identified in a previous work (for fluctuations of\naverage over large generations) disappears. This result is a first step to go\nbeyond the threshold condition on the ergodic rate given in previous\nstatistical papers on functional estimation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:33:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Penda", "S. Val\u00e8re Bitseki", ""], ["Delmas", "Jean-Fran\u00e7ois", ""]]}, {"id": "2106.09071", "submitter": "Xu Han", "authors": "Xu Han, Ethan X Fang, Cheng Yong Tang", "title": "Pre-processing with Orthogonal Decompositions for High-dimensional\n  Explanatory Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Strong correlations between explanatory variables are problematic for\nhigh-dimensional regularized regression methods. Due to the violation of the\nIrrepresentable Condition, the popular LASSO method may suffer from false\ninclusions of inactive variables. In this paper, we propose pre-processing with\northogonal decompositions (PROD) for the explanatory variables in\nhigh-dimensional regressions. The PROD procedure is constructed based upon a\ngeneric orthogonal decomposition of the design matrix. We demonstrate by two\nconcrete cases that the PROD approach can be effectively constructed for\nimproving the performance of high-dimensional penalized regression. Our\ntheoretical analysis reveals their properties and benefits for high-dimensional\npenalized linear regression with LASSO. Extensive numerical studies with\nsimulations and data analysis show the promising performance of the PROD.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:22:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Han", "Xu", ""], ["Fang", "Ethan X", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "2106.09115", "submitter": "Marcio Valk Valk .M", "authors": "Debora Zava Bello, Marcio Valk and Gabriela Bettella Cybis", "title": "Clustering inference in multiple groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference in clustering is paramount to uncovering inherent group structure\nin data. Clustering methods which assess statistical significance have recently\ndrawn attention owing to their importance for the identification of patterns in\nhigh dimensional data with applications in many scientific fields. We present\nhere a U-statistics based approach, specially tailored for high-dimensional\ndata, that clusters the data into three groups while assessing the significance\nof such partitions. Because our approach stands on the U-statistics based\nclustering framework of the methods in R package uclust, it inherits its\ncharacteristics being a non-parametric method relying on very few assumptions\nabout the data, and thus can be applied to a wide range of dataset. Furthermore\nour method aims to be a more powerful tool to find the best partitions of the\ndata into three groups when that particular structure is present. In order to\ndo so, we first propose an extension of the test U-statistic and develop its\nasymptotic theory. Additionally we propose a ternary non-nested significance\nclustering method. Our approach is tested through multiple simulations and\nfound to have more statistical power than competing alternatives in all\nscenarios considered. Applications to peripheral blood mononuclear cells and to\nimage recognition shows the versatility of our proposal, presenting a superior\nperformance when compared with other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:38:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Bello", "Debora Zava", ""], ["Valk", "Marcio", ""], ["Cybis", "Gabriela Bettella", ""]]}, {"id": "2106.09136", "submitter": "Yonghoon Lee", "authors": "Yonghoon Lee and Rina Foygel Barber", "title": "Binary classification with corrupted labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a binary classification problem where the goal is to fit an accurate\npredictor, the presence of corrupted labels in the training data set may create\nan additional challenge. However, in settings where likelihood maximization is\npoorly behaved-for example, if positive and negative labels are perfectly\nseparable-then a small fraction of corrupted labels can improve performance by\nensuring robustness. In this work, we establish that in such settings,\ncorruption acts as a form of regularization, and we compute precise upper\nbounds on estimation error in the presence of corruptions. Our results suggest\nthat the presence of corrupted data points is beneficial only up to a small\nfraction of the total sample, scaling with the square root of the sample size.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:23:48 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lee", "Yonghoon", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "2106.09207", "submitter": "Dhruv Rohatgi", "authors": "Jonathan Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi", "title": "On the Power of Preconditioning in Sparse Linear Regression", "comments": "73 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression is a fundamental problem in high-dimensional\nstatistics, but strikingly little is known about how to efficiently solve it\nwithout restrictive conditions on the design matrix. We consider the\n(correlated) random design setting, where the covariates are independently\ndrawn from a multivariate Gaussian $N(0,\\Sigma)$ with $\\Sigma : n \\times n$,\nand seek estimators $\\hat{w}$ minimizing $(\\hat{w}-w^*)^T\\Sigma(\\hat{w}-w^*)$,\nwhere $w^*$ is the $k$-sparse ground truth. Information theoretically, one can\nachieve strong error bounds with $O(k \\log n)$ samples for arbitrary $\\Sigma$\nand $w^*$; however, no efficient algorithms are known to match these guarantees\neven with $o(n)$ samples, without further assumptions on $\\Sigma$ or $w^*$. As\nfar as hardness, computational lower bounds are only known with worst-case\ndesign matrices. Random-design instances are known which are hard for the\nLasso, but these instances can generally be solved by Lasso after a simple\nchange-of-basis (i.e. preconditioning).\n  In this work, we give upper and lower bounds clarifying the power of\npreconditioning in sparse linear regression. First, we show that the\npreconditioned Lasso can solve a large class of sparse linear regression\nproblems nearly optimally: it succeeds whenever the dependency structure of the\ncovariates, in the sense of the Markov property, has low treewidth -- even if\n$\\Sigma$ is highly ill-conditioned. Second, we construct (for the first time)\nrandom-design instances which are provably hard for an optimally preconditioned\nLasso. In fact, we complete our treewidth classification by proving that for\nany treewidth-$t$ graph, there exists a Gaussian Markov Random Field on this\ngraph such that the preconditioned Lasso, with any choice of preconditioner,\nrequires $\\Omega(t^{1/20})$ samples to recover $O(\\log n)$-sparse signals when\ncovariates are drawn from this model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:12:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kelner", "Jonathan", ""], ["Koehler", "Frederic", ""], ["Meka", "Raghu", ""], ["Rohatgi", "Dhruv", ""]]}, {"id": "2106.09276", "submitter": "Danica J. Sutherland", "authors": "Frederic Koehler and Lijia Zhou and Danica J. Sutherland and Nathan\n  Srebro", "title": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and\n  Benign Overfitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interpolation learning in high-dimensional linear regression with\nGaussian data, and prove a generic uniform convergence guarantee on the\ngeneralization error of interpolators in an arbitrary hypothesis class in terms\nof the class's Gaussian width. Applying the generic bound to Euclidean norm\nballs recovers the consistency result of Bartlett et al. (2020) for\nminimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for\nnear-minimal-norm interpolators in the special case of Gaussian data. We\ndemonstrate the generality of the bound by applying it to the simplex,\nobtaining a novel consistency result for minimum l1-norm interpolators (basis\npursuit). Our results show how norm-based generalization bounds can explain and\nbe used to analyze benign overfitting, at least in some settings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 06:58:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Koehler", "Frederic", ""], ["Zhou", "Lijia", ""], ["Sutherland", "Danica J.", ""], ["Srebro", "Nathan", ""]]}, {"id": "2106.09327", "submitter": "Guillaume Dalle", "authors": "Guillaume Dalle (CERMICS), Yohann de Castro (ICJ, ECL)", "title": "Minimax Estimation of Partially-Observed Vector AutoRegressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the behavior of large dynamical systems like transportation\nnetworks, one must often rely on measurements transmitted by a set of sensors,\nfor instance individual vehicles. Such measurements are likely to be incomplete\nand imprecise, which makes it hard to recover the underlying signal of\ninterest.Hoping to quantify this phenomenon, we study the properties of a\npartially-observed state-space model. In our setting, the latent state $X$\nfollows a high-dimensional Vector AutoRegressive process $X_t = \\theta X_{t-1}\n+ \\varepsilon_t$. Meanwhile, the observations $Y$ are given by a\nnoise-corrupted random sample from the state $Y_t = \\Pi_t X_t + \\eta_t$.\nSeveral random sampling mechanisms are studied, allowing us to investigate the\neffect of spatial and temporal correlations in the distribution of the sampling\nmatrices $\\Pi_t$.We first prove a lower bound on the minimax estimation error\nfor the transition matrix $\\theta$. We then describe a sparse estimator based\non the Dantzig selector and upper bound its non-asymptotic error, showing that\nit achieves the optimal convergence rate for most of our sampling mechanisms.\nNumerical experiments on simulated time series validate our theoretical\nfindings, while an application to open railway data highlights the relevance of\nthis model for public transport traffic analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:46:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dalle", "Guillaume", "", "CERMICS"], ["de Castro", "Yohann", "", "ICJ, ECL"]]}, {"id": "2106.09387", "submitter": "Feng Ruan", "authors": "Feng Ruan, Keli Liu, Michael I. Jordan", "title": "Taming Nonconvexity in Kernel Feature Selection---Favorable Properties\n  of the Laplace Kernel", "comments": "28 pages main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based feature selection is an important tool in nonparametric\nstatistics. Despite many practical applications of kernel-based feature\nselection, there is little statistical theory available to support the method.\nA core challenge is the objective function of the optimization problems used to\ndefine kernel-based feature selection are nonconvex. The literature has only\nstudied the statistical properties of the \\emph{global optima}, which is a\nmismatch, given that the gradient-based algorithms available for nonconvex\noptimization are only able to guarantee convergence to local minima. Studying\nthe full landscape associated with kernel-based methods, we show that feature\nselection objectives using the Laplace kernel (and other $\\ell_1$ kernels) come\nwith statistical guarantees that other kernels, including the ubiquitous\nGaussian kernel (or other $\\ell_2$ kernels) do not possess. Based on a sharp\ncharacterization of the gradient of the objective function, we show that\n$\\ell_1$ kernels eliminate unfavorable stationary points that appear when using\nan $\\ell_2$ kernel. Armed with this insight, we establish statistical\nguarantees for $\\ell_1$ kernel-based feature selection which do not require\nreaching the global minima. In particular, we establish model-selection\nconsistency of $\\ell_1$-kernel-based feature selection in recovering main\neffects and hierarchical interactions in the nonparametric setting with $n \\sim\n\\log p$ samples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:05:48 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 05:15:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ruan", "Feng", ""], ["Liu", "Keli", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2106.09689", "submitter": "Ankit Pensia", "authors": "Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia, Thanasis Pittas,\n  Alistair Stewart", "title": "Statistical Query Lower Bounds for List-Decodable Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of list-decodable linear regression, where an adversary\ncan corrupt a majority of the examples. Specifically, we are given a set $T$ of\nlabeled examples $(x, y) \\in \\mathbb{R}^d \\times \\mathbb{R}$ and a parameter\n$0< \\alpha <1/2$ such that an $\\alpha$-fraction of the points in $T$ are i.i.d.\nsamples from a linear regression model with Gaussian covariates, and the\nremaining $(1-\\alpha)$-fraction of the points are drawn from an arbitrary noise\ndistribution. The goal is to output a small list of hypothesis vectors such\nthat at least one of them is close to the target regression vector. Our main\nresult is a Statistical Query (SQ) lower bound of $d^{\\mathrm{poly}(1/\\alpha)}$\nfor this problem. Our SQ lower bound qualitatively matches the performance of\npreviously developed algorithms, providing evidence that current upper bounds\nfor this task are nearly best possible.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:45:21 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pensia", "Ankit", ""], ["Pittas", "Thanasis", ""], ["Stewart", "Alistair", ""]]}, {"id": "2106.09769", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch, Na\\^amane La\\\"ib", "title": "Generalized regression operator estimation for continuous time\n  functional data processes with missing at random response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are interested in nonparametric kernel estimation of a\ngeneralized regression function, including conditional cumulative distribution\nand conditional quantile functions, based on an incomplete sample $(X_t, Y_t,\n\\zeta_t)_{t\\in \\mathbb{ R}^+}$ copies of a continuous-time stationary ergodic\nprocess $(X, Y, \\zeta)$. The predictor $X$ is valued in some\ninfinite-dimensional space, whereas the real-valued process $Y$ is observed\nwhen $\\zeta= 1$ and missing whenever $\\zeta = 0$. Pointwise and uniform\nconsistency (with rates) of these estimators as well as a central limit theorem\nare established. Conditional bias and asymptotic quadratic error are also\nprovided. Asymptotic and bootstrap-based confidence intervals for the\ngeneralized regression function are also discussed. A first simulation study is\nperformed to compare the discrete-time to the continuous-time estimations. A\nsecond simulation is also conducted to discuss the selection of the optimal\nsampling mesh in the continuous-time case. Finally, it is worth noting that our\nresults are stated under ergodic assumption without assuming any classical\nmixing conditions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:56:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chaouch", "Mohamed", ""], ["La\u00efb", "Na\u00e2mane", ""]]}, {"id": "2106.09840", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie", "title": "Entrywise limit theorems of eigenvectors and their one-step refinement\n  for sparse random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish finite-sample Berry-Esseen theorems for the entrywise limits of\nthe eigenvectors and their one-step refinement for sparse random graphs. For\nthe entrywise limits of the eigenvectors, the average expected degree is\nallowed to grow at the rate $\\Omega(\\log n)$, where $n$ is the number of\nvertices, and for the entrywise limits of the one-step refinement of the\neigenvectors, we require the expected degree to grow at the rate $\\omega(\\log\nn)$. The one-step refinement is shown to have a smaller entrywise covariance\nthan the eigenvectors in spectra. The key technical contribution towards the\ndevelopment of these limit theorems is a sharp finite-sample entrywise\neigenvector perturbation bound. In particular, the existed error bounds on the\ntwo-to-infinity norms of the higher-order remainders are not sufficient when\nthe graph average expected degree is proportional to $\\log n$. Our proof relies\non a decoupling strategy using a ``leave-one-out'' construction of auxiliary\nmatrices.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 22:32:56 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Xie", "Fangzheng", ""]]}, {"id": "2106.09962", "submitter": "Guillaume Maillard", "authors": "Guillaume Maillard (CELESTE, LM-Orsay)", "title": "Local asymptotics of cross-validation in least-squares density\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model selection, several types of cross-validation are commonly used and\nmany variants have been introduced. While consistency of some of these methods\nhas been proven, their rate of convergence to the oracle is generally still\nunknown. Until now, an asymptotic analysis of crossvalidation able to answer\nthis question has been lacking. Existing results focus on the ''pointwise''\nestimation of the risk of a single estimator, whereas analysing model selection\nrequires understanding how the CV risk varies with the model. In this article,\nwe investigate the asymptotics of the CV risk in the neighbourhood of the\noptimal model, for trigonometric series estimators in density estimation.\nAsymptotically, simple validation and ''incomplete'' V --fold CV behave like\nthe sum of a convex function fn and a symmetrized Brownian changed in time W\ngn/V. We argue that this is the right asymptotic framework for studying model\nselection.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:34:39 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Maillard", "Guillaume", "", "CELESTE, LM-Orsay"]]}, {"id": "2106.10037", "submitter": "Ola H\\\"ossjer", "authors": "Ola H\\\"ossjer and Arvid Sj\\\"olander", "title": "Sharp Lower and Upper Bounds for the Covariance of Bounded Random\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive sharp lower and upper bounds for the covariance of\ntwo bounded random variables when knowledge about their expected values,\nvariances or both is available. When only the expected values are known, our\nresult can be viewed as an extension of the Bhatia-Davis Inequality for\nvariances. We also provide a number of different ways to standardize\ncovariance. For a binary pair random variables, one of these standardized\nmeasures of covariation agrees with a frequently used measure of dependence\nbetween genetic variants.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:20:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["H\u00f6ssjer", "Ola", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2106.10135", "submitter": "Liu Zhijun", "authors": "Liu Zhijun, Bai Zhidong, Hu Jiang, Song Haiyan", "title": "CLT for LSS of sample covariance matrices with unbounded dispersions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under the high-dimensional setting that data dimension and sample size tend\nto infinity proportionally, we derive the central limit theorem (CLT) for\nlinear spectral statistics (LSS) of large-dimensional sample covariance matrix.\nDifferent from existing literature, our results do not require the assumption\nthat the population covariance matrices are bounded. Moreover, many common\nkernel functions in the real data such as logarithmic functions and polynomial\nfunctions are allowed in this paper. In our model, the number of spiked\neigenvalues can be fixed or tend to infinity. One salient feature of the\nasymptotic mean and covariance in our proposed central limit theorem is that it\nis related to the divergence order of the population spectral norm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:56:32 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhijun", "Liu", ""], ["Zhidong", "Bai", ""], ["Jiang", "Hu", ""], ["Haiyan", "Song", ""]]}, {"id": "2106.10312", "submitter": "Suchandan Kayal", "authors": "Suchandan Kayal", "title": "Weighted Fractional Generalized Cumulative Past Entropy", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce weighted fractional generalized cumulative past\nentropy of a nonnegative absolutely continuous random variable with bounded\nsupport. Various properties of the proposed weighted fractional measure are\nstudied. Bounds and stochastic orderings are derived. A connection between the\nproposed measure and the left-sided Riemann-Liouville fractional integral is\nestablished. Further, the proposed measure is studied for the proportional\nreversed hazard rate models. Next, a nonparametric estimator of the weighted\nfractional generalized cumulative past entropy is suggested based on the\nempirical distribution function. Various examples with a real life data set are\nconsidered for the illustration purposes. Finally, large sample properties of\nthe proposed empirical estimator are studied.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 18:52:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kayal", "Suchandan", ""]]}, {"id": "2106.10387", "submitter": "Ning Ning", "authors": "Ning Ning and Edward L. Ionides", "title": "Systemic Infinitesimal Over-dispersion on General Stochastic Graphical\n  Models", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models of interacting populations have crucial roles in scientific\nfields such as epidemiology and ecology, yet the standard approach to extending\nan ordinary differential equation model to a Markov chain does not have\nsufficient flexibility in the mean-variance relationship to match data (e.g.\n\\cite{bjornstad2001noisy}). A previous theory on time-homogeneous dynamics over\na single arrow by \\cite{breto2011compound} showed how gamma white noise could\nbe used to construct certain over-dispersed Markov chains, leading to widely\nused models (e.g. \\cite{breto2009time,he2010plug}). In this paper, we define\nsystemic infinitesimal over-dispersion, developing theory and methodology for\ngeneral time-inhomogeneous stochastic graphical models. Our approach, based on\nDirichlet noise, leads to a new class of Markov models over general direct\ngraphs. It is compatible with modern likelihood-based inference methodologies\n(e.g. \\cite{ionides2006inference,ionides2015inference,king2008inapparent}) and\ntherefore we can assess how well the new models fit data. We demonstrate our\nmethodology on a widely analyzed measles dataset, adding Dirichlet noise to a\nclassical SEIR (Susceptible-Exposed-Infected-Recovered) model. We find that the\nproposed methodology has higher log-likelihood than the gamma white noise\napproach, and the resulting parameter estimations provide new insights into the\nover-dispersion of this biological system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 23:29:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ning", "Ning", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2106.10398", "submitter": "Roberto Vila Gabriel", "authors": "Cira E. G. Otiniano, Roberto Vila, Pedro C. Brom, and Marcelo\n  Bourguignon", "title": "On the bimodal Gumbel model with application to environmental data", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Gumbel model is a very popular statistical model due to its wide\napplicability for instance in the course of certain survival, environmental,\nfinancial or reliability studies. In this work, we have introduced a bimodal\ngeneralization of the Gumbel distribution that can be an alternative to model\nbimodal data. We derive the analytical shapes of the corresponding probability\ndensity function and the hazard rate function and provide graphical\nillustrations. Furthermore, We have discussed the properties of this density\nsuch as mode, bimodality, moment generating function and moments. Our results\nwere verified using the Markov chain Monte Carlo simulation method. The maximum\nlikelihood method is used for parameters estimation. Finally, we also carry out\nan application to real data that demonstrates the usefulness of the proposed\ndistribution.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 00:30:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Otiniano", "Cira E. G.", ""], ["Vila", "Roberto", ""], ["Brom", "Pedro C.", ""], ["Bourguignon", "Marcelo", ""]]}, {"id": "2106.10496", "submitter": "Anthony Davison C.", "authors": "Anthony C. Davison and Nancy Reid", "title": "The Tangent Exponential Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The likelihood function is central to both frequentist and Bayesian\nformulations of parametric statistical inference, and large-sample\napproximations to the sampling distributions of estimators and test statistics,\nand to posterior densities, are widely used in practice. Improved\napproximations have been widely studied and can provide highly accurate\ninferences when samples are small or there are many nuisance parameters. This\narticle reviews improved approximations based on the tangent exponential model\ndeveloped in a series of articles by D.~A.~S.~Fraser and co-workers, attempting\nto explain the theoretical basis of this model and to provide a guide to the\nassociated literature, including a partially-annotated bibliography.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 13:27:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Davison", "Anthony C.", ""], ["Reid", "Nancy", ""]]}, {"id": "2106.10726", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Bingqing Yi", "title": "Some smooth sequential empirical copula processes and their multiplier\n  bootstraps under strong mixing", "comments": "48 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad class of smooth empirical copulas that contains the empirical beta\ncopula proposed by Segers, Sibuya and Tsukahara is studied. Conditions under\nwhich the corresponding sequential empirical copula processes converge weakly\nare provided. Specific members of this general class of smooth estimators that\ndepend on a scalar parameter determining the amount of marginal smoothing and a\nfunctional parameter controlling the shape of the smoothing region are\nproposed. The empirical investigation of the influence of these parameters\nsuggests to focus on a subclass of data-adaptive smooth nonparametric copulas.\nTo allow the use of the proposed class of smooth estimators in inference\nprocedures on an unknown copula, including in change-point analysis, natural\nsmooth extensions of the sequential dependent multiplier bootstrap are\nasymptotically validated and their finite-sample performance is studied through\nMonte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 17:00:59 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:06:34 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Yi", "Bingqing", ""]]}, {"id": "2106.10744", "submitter": "Min Jae Song", "authors": "Min Jae Song, Ilias Zadik, Joan Bruna", "title": "On the Cryptographic Hardness of Learning Single Periodic Neurons", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a simple reduction which demonstrates the cryptographic hardness of\nlearning a single periodic neuron over isotropic Gaussian distributions in the\npresence of noise. More precisely, our reduction shows that any polynomial-time\nalgorithm (not necessarily gradient-based) for learning such functions under\nsmall noise implies a polynomial-time quantum algorithm for solving worst-case\nlattice problems, whose hardness form the foundation of lattice-based\ncryptography. Our core hard family of functions, which are well-approximated by\none-layer neural networks, take the general form of a univariate periodic\nfunction applied to an affine projection of the data. These functions have\nappeared in previous seminal works which demonstrate their hardness against\ngradient-based (Shamir'18), and Statistical Query (SQ) algorithms (Song et\nal.'17). We show that if (polynomially) small noise is added to the labels, the\nintractability of learning these functions applies to all polynomial-time\nalgorithms under the aforementioned cryptographic assumptions.\n  Moreover, we demonstrate the necessity of noise in the hardness result by\ndesigning a polynomial-time algorithm for learning certain families of such\nfunctions under exponentially small adversarial noise. Our proposed algorithm\nis not a gradient-based or an SQ algorithm, but is rather based on the\ncelebrated Lenstra-Lenstra-Lov\\'asz (LLL) lattice basis reduction algorithm.\nFurthermore, in the absence of noise, this algorithm can be directly applied to\nsolve CLWE detection (Bruna et al.'21) and phase retrieval with an optimal\nsample complexity of $d+1$ samples. In the former case, this improves upon the\nquadratic-in-$d$ sample complexity required in (Bruna et al.'21). In the latter\ncase, this improves upon the state-of-the-art AMP-based algorithm, which\nrequires approximately $1.128d$ samples (Barbier et al.'19).\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 20:03:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Song", "Min Jae", ""], ["Zadik", "Ilias", ""], ["Bruna", "Joan", ""]]}, {"id": "2106.11104", "submitter": "Timo Dimitriadis", "authors": "Yannick Hoga, Timo Dimitriadis", "title": "On Testing Equal Conditional Predictive Ability Under Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Loss functions are widely used to compare several competing forecasts.\nHowever, forecast comparisons are often based on mismeasured proxy variables\nfor the true target. We introduce the concept of exact robustness to\nmeasurement error for loss functions and fully characterize this class of loss\nfunctions as the Bregman class. For such exactly robust loss functions,\nforecast loss differences are on average unaffected by the use of proxy\nvariables and, thus, inference on conditional predictive ability can be carried\nout as usual. Moreover, we show that more precise proxies give predictive\nability tests higher power in discriminating between competing forecasts.\nSimulations illustrate the different behavior of exactly robust and non-robust\nloss functions. An empirical application to US GDP growth rates demonstrates\nthat it is easier to discriminate between forecasts issued at different\nhorizons if a better proxy for GDP growth is used.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:39:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hoga", "Yannick", ""], ["Dimitriadis", "Timo", ""]]}, {"id": "2106.11213", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana, Fabio Rapallo, Henry P. Wynn", "title": "Circuits for robust designs", "comments": "21 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the application of circuit theory to experimental design\nstarted by the first two authors. The theory gives a very special and detailed\nrepresentation of the kernel of the design model matrix. This representation\nturns out to be an appropriate way to study the optimality criteria referred to\nas robustness: the sensitivity of the design to the removal of design points.\nMany examples are given, from classical combinatorial designs to two-level\nfactorial design including interactions. The complexity of the circuit\nrepresentations are useful because the large range of options they offer, but\nconversely require the use of dedicated software. Suggestions for speed\nimprovement are made.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:56:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""], ["Wynn", "Henry P.", ""]]}, {"id": "2106.11428", "submitter": "Song Mei", "authors": "Michael Celentano, Zhou Fan, Song Mei", "title": "Local convexity of the TAP free energy and AMP convergence for\n  Z2-synchronization", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study mean-field variational Bayesian inference using the TAP approach,\nfor Z2-synchronization as a prototypical example of a high-dimensional Bayesian\nmodel. We show that for any signal strength $\\lambda > 1$ (the weak-recovery\nthreshold), there exists a unique local minimizer of the TAP free energy\nfunctional near the mean of the Bayes posterior law. Furthermore, the TAP free\nenergy in a local neighborhood of this minimizer is strongly convex.\nConsequently, a natural-gradient/mirror-descent algorithm achieves linear\nconvergence to this minimizer from a local initialization, which may be\nobtained by a finite number of iterates of Approximate Message Passing (AMP).\nThis provides a rigorous foundation for variational inference in high\ndimensions via minimization of the TAP free energy.\n  We also analyze the finite-sample convergence of AMP, showing that AMP is\nasymptotically stable at the TAP minimizer for any $\\lambda > 1$, and is\nlinearly convergent to this minimizer from a spectral initialization for\nsufficiently large $\\lambda$. Such a guarantee is stronger than results\nobtainable by state evolution analyses, which only describe a fixed number of\nAMP iterations in the infinite-sample limit.\n  Our proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian\ncomparison inequality to analyze the complexity of critical points that satisfy\nstrong convexity and stability conditions within their local neighborhoods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 22:08:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Celentano", "Michael", ""], ["Fan", "Zhou", ""], ["Mei", "Song", ""]]}, {"id": "2106.11767", "submitter": "Zhiqi Bu", "authors": "Matteo Sordello, Zhiqi Bu, Jinshuo Dong", "title": "Privacy Amplification via Iteration for Shuffled and Online PNSGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the framework of privacy amplification via\niteration, which is originally proposed by Feldman et al. and subsequently\nsimplified by Asoodeh et al. in their analysis via the contraction coefficient.\nThis line of work focuses on the study of the privacy guarantees obtained by\nthe projected noisy stochastic gradient descent (PNSGD) algorithm with hidden\nintermediate updates. A limitation in the existing literature is that only the\nearly stopped PNSGD has been studied, while no result has been proved on the\nmore widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has\nbeen yet proposed regarding how to decrease the injected noise when new data\nare received in an online fashion. In this work, we first prove a privacy\nguarantee for shuffled PNSGD, which is investigated asymptotically when the\nnoise is fixed for each sample size $n$ but reduced at a predetermined rate\nwhen $n$ increases, in order to achieve the convergence of privacy loss. We\nthen analyze the online setting and provide a faster decaying scheme for the\nmagnitude of the injected noise that also guarantees the convergence of privacy\nloss.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 17:48:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sordello", "Matteo", ""], ["Bu", "Zhiqi", ""], ["Dong", "Jinshuo", ""]]}, {"id": "2106.11862", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Jonathan Niles-Weed and Austin J. Stromme", "title": "Asymptotics for semi-discrete entropic optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute exact second-order asymptotics for the cost of an optimal solution\nto the entropic optimal transport problem in the continuous-to-discrete, or\nsemi-discrete, setting. In contrast to the discrete-discrete or\ncontinuous-continuous case, we show that the first-order term in this expansion\nvanishes but the second-order term does not, so that in the semi-discrete\nsetting the difference in cost between the unregularized and regularized\nsolution is quadratic in the inverse regularization parameter, with a leading\nconstant that depends explicitly on the value of the density at the points of\ndiscontinuity of the optimal unregularized map between the measures. We develop\nthese results by proving new pointwise convergence rates of the solutions to\nthe dual problem, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:31:30 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Niles-Weed", "Jonathan", ""], ["Stromme", "Austin J.", ""]]}, {"id": "2106.12001", "submitter": "Nancy Reid", "authors": "Heather S. Battey and Nancy Reid", "title": "Inference in High-dimensional Linear Regression", "comments": "27 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an approach to inference in a linear regression model when the\nnumber of potential explanatory variables is larger than the sample size. Our\napproach treats each regression coefficient in turn as the interest parameter,\nthe remaining coefficients being nuisance parameters, and seeks an optimal\ninterest-respecting transformation. The role of this transformation is to allow\na marginal least squares analysis for each variable, as in a factorial\nexperiment. One parameterization of the problem is found to be particularly\nconvenient, both computationally and mathematically. In particular, it permits\nan analytic solution to the optimal transformation problem, facilitating\ncomparison to other work. In contrast to regularized regression such as the\nlasso (Tibshirani, 1996) and its extensions, neither adjustment for selection,\nnor rescaling of the explanatory variables is needed, ensuring the physical\ninterpretation of regression coefficients is retained. We discuss the use of\nsuch confidence intervals as part of a broader set of inferential statements,\nso as to reflect uncertainty over the model as well as over the parameters. The\nconsiderations involved in extending the work to other regression models are\nbriefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:19:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Battey", "Heather S.", ""], ["Reid", "Nancy", ""]]}, {"id": "2106.12199", "submitter": "Prateek Jaiswal", "authors": "Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao", "title": "Bayesian Joint Chance Constrained Optimization: Approximations and\n  Statistical Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers data-driven chance-constrained stochastic optimization\nproblems in a Bayesian framework. Bayesian posteriors afford a principled\nmechanism to incorporate data and prior knowledge into stochastic optimization\nproblems. However, the computation of Bayesian posteriors is typically an\nintractable problem, and has spawned a large literature on approximate Bayesian\ncomputation. Here, in the context of chance-constrained optimization, we focus\non the question of statistical consistency (in an appropriate sense) of the\noptimal value, computed using an approximate posterior distribution. To this\nend, we rigorously prove a frequentist consistency result demonstrating the\nconvergence of the optimal value to the optimal value of a fixed, parameterized\nconstrained optimization problem. We augment this by also establishing a\nprobabilistic rate of convergence of the optimal value. We also prove the\nconvex feasibility of the approximate Bayesian stochastic optimization problem.\nFinally, we demonstrate the utility of our approach on an optimal staffing\nproblem for an M/M/c queueing model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:11:39 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 17:01:49 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jaiswal", "Prateek", ""], ["Honnappa", "Harsha", ""], ["Rao", "Vinayak A.", ""]]}, {"id": "2106.12292", "submitter": "Francesco Buono", "authors": "Francesco Buono, Camilla Cal\\`i and Maria Longobardi", "title": "Dispersion indexes based on bivariate measures of uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.PM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of varentropy has been recently introduced as a dispersion index\nof the reliability of measure of information. In this paper, we introduce new\nmeasures of variability for two bivariate measures of uncertainty, the Kerridge\ninaccuracy measure and the Kullback-Leibler divergence. These new definitions\nand related properties, bounds and examples are presented. Finally we show an\napplication of Kullback-Leibler divergence and its dispersion index using the\nmean-variance rule introduced in portfolio theory.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:20:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Buono", "Francesco", ""], ["Cal\u00ec", "Camilla", ""], ["Longobardi", "Maria", ""]]}, {"id": "2106.12311", "submitter": "Khalifa Es-Sebaiy", "authors": "Khalifa Es-Sebaiy", "title": "Gaussian and Hermite Ornstein-Uhlenbeck processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the present paper we study the asymptotic behavior of the auto-covariance\nfunction for Ornstein-Uhlenbeck (OU) processes driven by Gaussian noises with\nstationary and non-stationary increments and for Hermite OU processes. Our\nresults are generalizations of the corresponding results of Cheridito et al.\n\\cite{CKM} and Kaarakka and Salminen \\cite{KS}.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:14:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Es-Sebaiy", "Khalifa", ""]]}, {"id": "2106.12367", "submitter": "Jean-David Fermanian", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "Identifiability and estimation of meta-elliptical copula generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-elliptical copulas are often proposed to model dependence between the\ncomponents of a random vector. They are specified by a correlation matrix and a\nmap $g$, called a density generator. When the latter correlation matrix can\neasily be estimated from pseudo-samples of observations, this is not the case\nfor the density generator when it does not belong to a parametric family. We\nstate sufficient conditions to non-parametrically identify this generator.\nSeveral nonparametric estimators of $g$ are then proposed, by M-estimation,\nsimulation-based inference or by an iterative procedure available in a R\npackage. Some simulations illustrate the relevance of the latter method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:03:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "2106.12677", "submitter": "Judith Lok", "authors": "Judith J. Lok, Department of Mathematics and Statistics, Boston\n  University", "title": "Optimal estimation of coarse structural nested mean models with\n  application to initiating ART in HIV infected patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coarse structural nested mean models are used to estimate treatment effects\nfrom longitudinal observational data. Coarse structural nested mean models lead\nto a large class of estimators. It turns out that estimates and standard errors\nmay differ considerably within this class. We prove that, under additional\nassumptions, there exists an explicit solution for the optimal estimator within\nthe class of coarse structural nested mean models. Moreover, we show that even\nif the additional assumptions do not hold, this optimal estimator is\ndoubly-robust: it is consistent and asymptotically normal not only if the model\nfor treatment initiation is correct, but also if a certain outcome-regression\nmodel is correct.\n  We compare the optimal estimator to some naive choices within the class of\ncoarse structural nested mean models in a simulation study. Furthermore, we\napply the optimal and naive estimators to study how the CD4 count increase due\nto one year of antiretroviral treatment (ART) depends on the time between HIV\ninfection and ART initiation in recently infected HIV infected patients. Both\nin the simulation study and in the application, the use of optimal estimators\nleads to substantial increases in precision.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:29:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lok", "Judith J.", ""], ["Mathematics", "Department of", ""], ["Statistics", "", ""], ["University", "Boston", ""]]}, {"id": "2106.12796", "submitter": "Quentin Duchemin", "authors": "Quentin Duchemin, Yohann De Castro and Claire Lacour", "title": "Three rates of convergence or separation via U-statistics in a dependent\n  framework", "comments": "This submission completes the submission arXiv:2011.11435v1 which has\n  been split into two pieces: concentration inequalities arXiv:2011.11435v3 and\n  further versions, and this submission about three applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the ubiquity of U-statistics in modern Probability and Statistics,\ntheir non-asymptotic analysis in a dependent framework may have been\noverlooked. In a recent work, a new concentration inequality for U-statistics\nof order two for uniformly ergodic Markov chains has been proved. In this\npaper, we put this theoretical breakthrough into action by pushing further the\ncurrent state of knowledge in three different active fields of research. First,\nwe establish a new exponential inequality for the estimation of spectra of\ntrace class integral operators with MCMC methods. The novelty is that this\nresult holds for kernels with positive and negative eigenvalues, which is new\nas far as we know. In addition, we investigate generalization performance of\nonline algorithms working with pairwise loss functions and Markov chain\nsamples. We provide an online-to-batch conversion result by showing how we can\nextract a low risk hypothesis from the sequence of hypotheses generated by any\nonline learner. We finally give a non-asymptotic analysis of a goodness-of-fit\ntest on the density of the invariant measure of a Markov chain. We identify\nsome classes of alternatives over which our test based on the $L_2$ distance\nhas a prescribed power.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:10:36 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Duchemin", "Quentin", ""], ["De Castro", "Yohann", ""], ["Lacour", "Claire", ""]]}, {"id": "2106.12815", "submitter": "Alexis Rosuel", "authors": "Alexis Rosuel (LIGM), Philippe Loubaton (LIGM), Pascal Vallet (IMS),\n  Xavier Mestre (CTTC)", "title": "On the detection of low-rank signal in the presence of spatially\n  uncorrelated noise: a frequency domain approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the detection of a M-dimensional useful signal modeled as\nthe output of a M xK MIMO filter driven by a K-dimensional white Gaussian\nnoise, and corrupted by a M-dimensional Gaussian noise with mutually\nuncorrelated components. The study is focused on frequency domain test\nstatistics based on the eigenvalues of an estimate of the spectral coherence\nmatrix (SCM), obtained as a renormalization of the frequency-smoothed\nperiodogram of the observed signal. If N denotes the sample size and B the\nsmoothing span, it is proved that in the high-dimensional regime where M, B, N\nconverge to infinity while K remains fixed, the SCM behaves as a certain\ncorrelated Wishart matrix. Exploiting well-known results on the behaviour of\nthe eigenvalues of such matrices, it is deduced that the standard tests based\non linear spectral statistics of the SCM fail to detect the presence of the\nuseful signal in the high-dimensional regime. A new test based on the SCM,\nwhich is proved to be consistent, is also proposed, and its statistical\nperformance is evaluated through numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:12:45 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Rosuel", "Alexis", "", "LIGM"], ["Loubaton", "Philippe", "", "LIGM"], ["Vallet", "Pascal", "", "IMS"], ["Mestre", "Xavier", "", "CTTC"]]}, {"id": "2106.12886", "submitter": "Shosei Sakaguchi", "authors": "Toru Kitagawa, Shosei Sakaguchi, and Aleksey Tetenov", "title": "Constrained Classification and Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern machine learning approaches to classification, including AdaBoost,\nsupport vector machines, and deep neural networks, utilize surrogate loss\ntechniques to circumvent the computational complexity of minimizing empirical\nclassification risk. These techniques are also useful for causal policy\nlearning problems, since estimation of individualized treatment rules can be\ncast as a weighted (cost-sensitive) classification problem. Consistency of the\nsurrogate loss approaches studied in Zhang (2004) and Bartlett et al. (2006)\ncrucially relies on the assumption of correct specification, meaning that the\nspecified set of classifiers is rich enough to contain a first-best classifier.\nThis assumption is, however, less credible when the set of classifiers is\nconstrained by interpretability or fairness, leaving the applicability of\nsurrogate loss based algorithms unknown in such second-best scenarios. This\npaper studies consistency of surrogate loss procedures under a constrained set\nof classifiers without assuming correct specification. We show that in the\nsetting where the constraint restricts the classifier's prediction set only,\nhinge losses (i.e., $\\ell_1$-support vector machines) are the only surrogate\nlosses that preserve consistency in second-best scenarios. If the constraint\nadditionally restricts the functional form of the classifier, consistency of a\nsurrogate loss approach is not guaranteed even with hinge loss. We therefore\ncharacterize conditions for the constrained set of classifiers that can\nguarantee consistency of hinge risk minimizing classifiers. Exploiting our\ntheoretical results, we develop robust and computationally attractive hinge\nloss based procedures for a monotone classification problem.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:43:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kitagawa", "Toru", ""], ["Sakaguchi", "Shosei", ""], ["Tetenov", "Aleksey", ""]]}, {"id": "2106.12936", "submitter": "Kweku Abraham", "authors": "Kweku Abraham, Zacharie Naulet, Elisabeth Gassiat", "title": "Fundamental limits for learning hidden Markov model parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the frontier between learnable and unlearnable hidden Markov models\n(HMMs). HMMs are flexible tools for clustering dependent data coming from\nunknown populations. The model parameters are known to be identifiable as soon\nas the clusters are distinct and the hidden chain is ergodic with a full rank\ntransition matrix. In the limit as any one of these conditions fails, it\nbecomes impossible to identify parameters. For a chain with two hidden states\nwe prove nonasymptotic minimax upper and lower bounds, matching up to\nconstants, which exhibit thresholds at which the parameters become learnable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:02:33 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Abraham", "Kweku", ""], ["Naulet", "Zacharie", ""], ["Gassiat", "Elisabeth", ""]]}, {"id": "2106.12996", "submitter": "Subhro Ghosh", "authors": "Subhro Ghosh and Philippe Rigollet", "title": "Multi-Reference Alignment for sparse signals, Uniform Uncertainty\n  Principles and the Beltway Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by cutting-edge applications like cryo-electron microscopy\n(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an\nunknown signal from repeated measurements of its images under the latent action\nof a group of isometries and additive noise of magnitude $\\sigma$. Despite\nsignificant interest, a clear picture for understanding rates of estimation in\nthis model has emerged only recently, particularly in the high-noise regime\n$\\sigma \\gg 1$ that is highly relevant in applications. Recent investigations\nhave revealed a remarkable asymptotic sample complexity of order $\\sigma^6$ for\ncertain signals whose Fourier transforms have full support, in stark contrast\nto the traditional $\\sigma^2$ that arise in regular models. Often prohibitively\nlarge in practice, these results have prompted the investigation of variations\naround the MRA model where better sample complexity may be achieved. In this\npaper, we show that \\emph{sparse} signals exhibit an intermediate $\\sigma^4$\nsample complexity even in the classical MRA model. Our results explore and\nexploit connections of the MRA estimation problem with two classical topics in\napplied mathematics: the \\textit{beltway problem} from combinatorial\noptimization, and \\textit{uniform uncertainty principles} from harmonic\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:13:10 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ghosh", "Subhro", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2106.13073", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Lena Eid and Bernhard Klar", "title": "Cauchy or not Cauchy? New goodness-of-fit tests for the Cauchy\n  distribution", "comments": "21 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new characterization of the Cauchy distribution and propose a\nclass of goodness-of-fit tests to the Cauchy family. The limit distribution is\nderived in a Hilbert space framework under the null hypothesis and under fixed\nalternatives. The new tests are consistent against a large class of\nalternatives. A comparative Monte Carlo simulation study shows that the test is\ncompetitive to the state of the art procedures, and we apply the tests to\nlog-returns of cryptocurrencies.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:59:25 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ebner", "Bruno", ""], ["Eid", "Lena", ""], ["Klar", "Bernhard", ""]]}, {"id": "2106.13104", "submitter": "Alessio Borz\\`i", "authors": "Alessio Borz\\'i, Xiangying Chen, Harshit J. Motwani, Lorenzo\n  Venturello, Martin Vodi\\v{c}ka", "title": "The leading coefficient of Lascoux polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.CO math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lascoux polynomials have been recently introduced to prove polynomiality of\nthe maximum-likelihood degree of linear concentration models. We find the\nleading coefficient of the Lascoux polynomials (type C) and their\ngeneralizations to the case of general matrices (type A) and skew symmetric\nmatrices (type D). In particular, we determine the degrees of such polynomials.\nAs an application, we find the degree of the polynomial $\\delta(m,n,n-s)$ of\nthe algebraic degree of semidefinite programming, and when $s=1$ we find its\nleading coefficient for types C, A and D.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:39:49 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 13:13:37 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Borz\u00ed", "Alessio", ""], ["Chen", "Xiangying", ""], ["Motwani", "Harshit J.", ""], ["Venturello", "Lorenzo", ""], ["Vodi\u010dka", "Martin", ""]]}, {"id": "2106.13181", "submitter": "Tudor Manole", "authors": "Tudor Manole, Jonathan Niles-Weed", "title": "Sharp Convergence Rates for Empirical Optimal Transport with Smooth\n  Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the question of characterizing the convergence rate of plug-in\nestimators of optimal transport costs. It is well known that an empirical\nmeasure comprising independent samples from an absolutely continuous\ndistribution on $\\mathbb{R}^d$ converges to that distribution at the rate\n$n^{-1/d}$ in Wasserstein distance, which can be used to prove that plug-in\nestimators of many optimal transport costs converge at this same rate. However,\nwe show that when the cost is smooth, this analysis is loose: plug-in\nestimators based on empirical measures converge quadratically faster, at the\nrate $n^{-2/d}$. As a corollary, we show that the Wasserstein distance between\ntwo distributions is significantly easier to estimate when the measures are far\napart. We also prove lower bounds, showing not only that our analysis of the\nplug-in estimator is tight, but also that no other estimator can enjoy\nsignificantly faster rates of convergence uniformly over all pairs of measures.\nOur proofs rely on empirical process theory arguments based on tight control of\n$L^2$ covering numbers for locally Lipschitz and semi-concave functions. As a\nbyproduct of our proofs, we derive $L^\\infty$ estimates on the displacement\ninduced by the optimal coupling between any two measures satisfying suitable\nmoment conditions, for a wide range of cost functions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:57:35 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Manole", "Tudor", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "2106.13293", "submitter": "Nicolas Marie", "authors": "Fabienne Comte and Nicolas Marie", "title": "On a Projection Estimator of the Regression Function Derivative", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation of the derivative of a regression\nfunction in a standard univariate regression model. The estimators are defined\neither by derivating nonparametric least-squares estimators of the regression\nfunction or by estimating the projection of the derivative. We prove two simple\nrisk bounds allowing to compare our estimators. More elaborate bounds under a\nstability assumption are then provided. Bases and spaces on which we can\nillustrate our assumptions and first results are both of compact or non compact\ntype, and we discuss the rates reached by our estimators. They turn out to be\noptimal in the compact case. Lastly, we propose a model selection procedure and\nprove the associated risk bound. To consider bases with a non compact support\nmakes the problem difficult.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 19:51:46 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Comte", "Fabienne", ""], ["Marie", "Nicolas", ""]]}, {"id": "2106.13414", "submitter": "Gautam Kamath", "authors": "Cl\\'ement L. Canonne, Ayush Jain, Gautam Kamath, Jerry Li", "title": "The Price of Tolerance in Distribution Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of tolerant distribution testing. That is, given\nsamples from an unknown distribution $p$ over $\\{1, \\dots, n\\}$, is it\n$\\varepsilon_1$-close to or $\\varepsilon_2$-far from a reference distribution\n$q$ (in total variation distance)? Despite significant interest over the past\ndecade, this problem is well understood only in the extreme cases. In the\nnoiseless setting (i.e., $\\varepsilon_1 = 0$) the sample complexity is\n$\\Theta(\\sqrt{n})$, strongly sublinear in the domain size. At the other end of\nthe spectrum, when $\\varepsilon_1 = \\varepsilon_2/2$, the sample complexity\njumps to the barely sublinear $\\Theta(n/\\log n)$. However, very little is known\nabout the intermediate regime. We fully characterize the price of tolerance in\ndistribution testing as a function of $n$, $\\varepsilon_1$, $\\varepsilon_2$, up\nto a single $\\log n$ factor. Specifically, we show the sample complexity to be\n\\[\\tilde \\Theta\\left(\\frac{\\sqrt{n}}{\\varepsilon_2^{2}} + \\frac{n}{\\log n}\n\\cdot \\max\n\\left\\{\\frac{\\varepsilon_1}{\\varepsilon_2^2},\\left(\\frac{\\varepsilon_1}{\\varepsilon_2^2}\\right)^{\\!\\!2}\\right\\}\\right),\\]\nproviding a smooth tradeoff between the two previously known cases. We also\nprovide a similar characterization for the problem of tolerant equivalence\ntesting, where both $p$ and $q$ are unknown. Surprisingly, in both cases, the\nmain quantity dictating the sample complexity is the ratio\n$\\varepsilon_1/\\varepsilon_2^2$, and not the more intuitive\n$\\varepsilon_1/\\varepsilon_2$. Of particular technical interest is our lower\nbound framework, which involves novel approximation-theoretic tools required to\nhandle the asymmetry between $\\varepsilon_1$ and $\\varepsilon_2$, a challenge\nabsent from previous works.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 03:59:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Jain", "Ayush", ""], ["Kamath", "Gautam", ""], ["Li", "Jerry", ""]]}, {"id": "2106.13501", "submitter": "Etienne Roquain", "authors": "David Mary and Etienne Roquain", "title": "Semi-supervised multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important limitation of standard multiple testing procedures is that the\nnull distribution should be known. Here, we consider a null distribution-free\napproach for multiple testing in the following semi-supervised setting: the\nuser does not know the null distribution, but has at hand a single sample drawn\nfrom this null distribution. In practical situations, this null training sample\n(NTS) can come from previous experiments, from a part of the data under test,\nfrom specific simulations, or from a sampling process. In this work, we present\ntheoretical results that handle such a framework, with a focus on the false\ndiscovery rate (FDR) control and the Benjamini-Hochberg (BH) procedure. First,\nwe introduce a procedure providing strong FDR control. Second, we also give a\npower analysis for that procedure suggesting that the price to pay for ignoring\nthe null distribution is low when the NTS sample size $n$ is sufficiently large\nin front of the number of test $m$; namely $n\\gtrsim m/(\\max(1,k))$, where $k$\ndenotes the number of \"detectable\" alternatives. Third, to complete the\npicture, we also present a negative result that evidences an intrinsic\ntransition phase to the general semi-supervised multiple testing problem {and\nshows that the proposed method is optimal in the sense that its performance\nboundary follows this transition phase}. Our theoretical properties are\nsupported by numerical experiments, which also show that the delineated\nboundary is of correct order without further tuning any constant. Finally, we\ndemonstrate that our approach provides a theoretical ground for standard\npractice in astronomical data analysis, and in particular for the procedure\nproposed in \\cite{Origin2020} for galaxy detection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:41:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mary", "David", ""], ["Roquain", "Etienne", ""]]}, {"id": "2106.13694", "submitter": "Keisuke Yano", "authors": "Yukito Iba and Keisuke Yano", "title": "Posterior Covariance Information Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an information criterion, PCIC, for predictive evaluation based\non quasi-posterior distributions. It is regarded as a natural generalisation of\nthe widely applicable information criterion (WAIC) and can be computed via a\nsingle Markov chain Monte Carlo run. PCIC is useful in a variety of predictive\nsettings that are not well dealt with in WAIC, including weighted likelihood\ninference and quasi-Bayesian prediction\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:28:36 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 05:22:53 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 05:32:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Iba", "Yukito", ""], ["Yano", "Keisuke", ""]]}, {"id": "2106.13751", "submitter": "Louis Sharrock", "authors": "Louis Sharrock, Nikolas Kantas, Panos Parpas, Grigorios A. Pavliotis", "title": "Parameter Estimation for the McKean-Vlasov Stochastic Differential\n  Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of parameter estimation for a\nstochastic McKean-Vlasov equation, and the associated system of weakly\ninteracting particles. We first establish consistency and asymptotic normality\nof the offline maximum likelihood estimator for the interacting particle system\nin the limit as the number of particles $N\\rightarrow\\infty$. We then propose\nan online estimator for the parameters of the McKean-Vlasov SDE, which evolves\naccording to a continuous-time stochastic gradient descent algorithm on the\nasymptotic log-likelihood of the interacting particle system. We prove that\nthis estimator converges in $\\mathbb{L}^1$ to the stationary points of the\nasymptotic log-likelihood of the McKean-Vlasov SDE in the joint limit as\n$N\\rightarrow\\infty$ and $t\\rightarrow\\infty$, under suitable assumptions which\nguarantee ergodicity and uniform-in-time propagation of chaos. We then\ndemonstrate, under the additional assumption of global strong concavity, that\nour estimator converges in $\\mathbb{L}^2$ to the unique maximiser of this\nasymptotic log-likelihood function, and establish an $\\mathbb{L}^2$ convergence\nrate. We also obtain analogous results under the assumption that, rather than\nobserving multiple trajectories of the interacting particle system, we instead\nobserve multiple independent replicates of the McKean-Vlasov SDE itself or,\nless realistically, a single sample path of the McKean-Vlasov SDE and its law.\nOur theoretical results are demonstrated via two numerical examples, a linear\nmean field model and a stochastic opinion dynamics model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:40:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sharrock", "Louis", ""], ["Kantas", "Nikolas", ""], ["Parpas", "Panos", ""], ["Pavliotis", "Grigorios A.", ""]]}, {"id": "2106.13947", "submitter": "Soham Jana", "authors": "Yanjun Han, Soham Jana, Yihong Wu", "title": "Optimal prediction of Markov chains with and without spectral gap", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following learning problem with dependent data: Observing a\ntrajectory of length $n$ from a stationary Markov chain with $k$ states, the\ngoal is to predict the next state. For $3 \\leq k \\leq O(\\sqrt{n})$, using\ntechniques from universal compression, the optimal prediction risk in\nKullback-Leibler divergence is shown to be $\\Theta(\\frac{k^2}{n}\\log\n\\frac{n}{k^2})$, in contrast to the optimal rate of $\\Theta(\\frac{\\log \\log\nn}{n})$ for $k=2$ previously shown in Falahatgar et al., 2016. These rates,\nslower than the parametric rate of $O(\\frac{k^2}{n})$, can be attributed to the\nmemory in the data, as the spectral gap of the Markov chain can be arbitrarily\nsmall. To quantify the memory effect, we study irreducible reversible chains\nwith a prescribed spectral gap. In addition to characterizing the optimal\nprediction risk for two states, we show that, as long as the spectral gap is\nnot excessively small, the prediction risk in the Markov model is\n$O(\\frac{k^2}{n})$, which coincides with that of an iid model with the same\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 05:18:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Han", "Yanjun", ""], ["Jana", "Soham", ""], ["Wu", "Yihong", ""]]}, {"id": "2106.13962", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Norbert Henze", "title": "Bahadur efficiencies of the Epps--Pulley test for normality", "comments": "13 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test for normality suggested by Epps and Pulley (1983) is a serious\ncompetitor to tests based on the empirical distribution function. In contrast\nto the latter procedures, it has been generalized to obtain a genuine affine\ninvariant and universally consistent test for normality in any dimension. We\nobtain approximate Bahadur efficiencies for the test of Epps and Pulley, thus\ncomplementing recent results of Milo\\v{s}evi\\'c et al. (2021). For certain\nvalues of a tuning parameter that is inherent in the Epps--Pulley test, this\ntest outperforms each of its competitors considered in Milo\\v{s}evi\\'c et al.\n(2021), over the whole range of six close alternatives to normality.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:02:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""]]}, {"id": "2106.14077", "submitter": "Masahiro Kato", "authors": "Masahiro Kato and Kaito Ariu", "title": "The Role of Contextual Information in Best Arm Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the best-arm identification problem with fixed confidence when\ncontextual (covariate) information is available in stochastic bandits. Although\nwe can use contextual information in each round, we are interested in the\nmarginalized mean reward over the contextual distribution. Our goal is to\nidentify the best arm with a minimal number of samplings under a given value of\nthe error rate. We show the instance-specific sample complexity lower bounds\nfor the problem. Then, we propose a context-aware version of the\n\"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the\nset of optimal allocations and prove that the expected number of arm draws\nmatches the lower bound asymptotically. We demonstrate that contextual\ninformation can be used to improve the efficiency of the identification of the\nbest marginalized mean reward compared with the results of Garivier & Kaufmann\n(2016). We experimentally confirm that context information contributes to\nfaster best-arm identification.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:39:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kato", "Masahiro", ""], ["Ariu", "Kaito", ""]]}, {"id": "2106.14240", "submitter": "Ahmed Sani", "authors": "Mohamed El Maazouz and Ahmed Sani", "title": "New copulas and their applications to symmetrizations of bivariate\n  copulas", "comments": "There are some figires which illusrate the asymmery of couplas\n  constructed and will appear in the publishd version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New copulas, based on perturbation theory, are introduced to clarify a\n\\emph{symmetrization} procedure for asymmetric copulas. We give also some\nproperties of the \\emph{symmetrized} copula. Finally, we examine families of\ncopulas with a prescribed symmetrized one. By the way, we study topologically,\nthe set of all symmetric copulas and give some of its classical and new\nproperties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 14:12:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Maazouz", "Mohamed El", ""], ["Sani", "Ahmed", ""]]}, {"id": "2106.14353", "submitter": "Tarik Faouzi Nadim", "authors": "Tarik Faouzi, Emilio Porcu, Igor Kondrashuk and Anatoliy Malyarenko", "title": "A deep look into the Dagum family of isotropic covariance functions", "comments": "15 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dagum family of isotropic covariance functions has two parameters that\nallow for decoupling of the fractal dimension and Hurst effect for Gaussian\nrandom fields that are stationary and isotropic over Euclidean spaces.\nSufficient conditions that allow for positive definiteness in Rd of the Dagum\nfamily have been proposed on the basis of the fact that the Dagum family allows\nfor complete monotonicity under some parameter restrictions. The spectral\nproperties of the Dagum family have been inspected to a very limited extent\nonly, and this paper gives insight into this direction. Specifically, we study\nfinite and asymptotic properties of the isotropic spectral density (intended as\nthe Hankel transform) of the Dagum model. Also, we establish some closed forms\nexpressions for the Dagum spectral density in terms of the Fox{Wright\nfunctions. Finally, we provide asymptotic properties for such a class of\nspectral densities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:42:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Faouzi", "Tarik", ""], ["Porcu", "Emilio", ""], ["Kondrashuk", "Igor", ""], ["Malyarenko", "Anatoliy", ""]]}, {"id": "2106.14470", "submitter": "Olga Klopp", "authors": "Farida Enikeeva, Olga Klopp (CREST)", "title": "Change-Point Detection in Dynamic Networks with Missing Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural changes occur in dynamic networks quite frequently and its\ndetection is an important question in many situations such as fraud detection\nor cybersecurity. Real-life networks are often incompletely observed due to\nindividual non-response or network size. In the present paper we consider the\nproblem of change-point detection at a temporal sequence of partially observed\nnetworks. The goal is to test whether there is a change in the network\nparameters. Our approach is based on the Matrix CUSUM test statistic and allows\ngrowing size of networks. We show that the proposed test is minimax optimal and\nrobust to missing links. We also demonstrate the good behavior of our approach\nin practice through simulation study and a real-data application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:35:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Enikeeva", "Farida", "", "CREST"], ["Klopp", "Olga", "", "CREST"]]}, {"id": "2106.14633", "submitter": "Irene Gannaz", "authors": "Sophie Achard, Ir\\`ene Gannaz (PSPM, ICJ)", "title": "Whittle estimation with (quasi-)analytic wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of long-memory is considered in the case of multivariate time\nseries, not necessarily Gaussian nor stationary. The long-memory\ncharacteristics are defined by the long-memory parameters describing the\nautocorrelation structure of each process and the long-run covariance measuring\nthe coupling between time series. A phase term is present in the model to widen\nthe classes of models. We introduce a representation of the time series by\nquasi-analytic wavelets for inference in this setting. We first show that the\ncovariance of the wavelet coefficients provides an adequate estimator of the\ncovariance structure of the processes, including the phase term. Consistent\nestimators are then proposed which is based on a Whittle approximation.\nSimulations highlight a satisfactory behavior of the estimation on finite\nsamples on some linear time series and on multivariate fractional Brownian\nmotions. An application on a real dataset in neuroscience is displayed, where\nlong-memory and brain connectivity are inferred.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:35:53 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Achard", "Sophie", "", "PSPM, ICJ"], ["Gannaz", "Ir\u00e8ne", "", "PSPM, ICJ"]]}, {"id": "2106.14634", "submitter": "Daniel Trejo Medina", "authors": "Daniel Trejo Medina, Karla Sarai Jimenez", "title": "Algebraic Topology for Data Analysis", "comments": "22 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research addresses a new tool for data analysis known as Topological\nData Analysis TDA It underlies an area of Mathematics known as Combinatorial\nAlgebra or more recently Algebraic Topology which through making strong use of\nComputation Statistics Probability and Topology among other concepts extracts\nmathematical characteristics from a set of data that allow us associate create\nand infer general and quality information about them\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:21:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Medina", "Daniel Trejo", ""], ["Jimenez", "Karla Sarai", ""]]}, {"id": "2106.14635", "submitter": "Arni S.R. Srinivasa Rao", "authors": "Arni S.R. Srinivasa Rao and Steven G. Krantz", "title": "Rao distances and Conformal Mapping", "comments": "17 pages, 4 figures", "journal-ref": "Information Geometry (2021), Volume 45: Handbook of Statistics,\n  Elsevier/North-Holland, Amsterdam", "doi": null, "report-no": null, "categories": "math.ST math.DG math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, we have described the Rao distance (due to C.R. Rao) and\nideas of conformal mappings on 3D objects with angle preservations. Three\npropositions help us to construct distances between the points within the 3D\nobjects in \\mathbb{R}^{3} and line integrals within complex planes. We\nhighlight the application of these concepts to virtual tourism.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 22:04:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rao", "Arni S. R. Srinivasa", ""], ["Krantz", "Steven G.", ""]]}, {"id": "2106.14669", "submitter": "Vincent Rivoirard", "authors": "Minh-Lien Jeanne Nguyen, Claire Lacour (LAMA), Vincent Rivoirard\n  (CEREMADE)", "title": "Adaptive greedy algorithm for moderately large dimensions in kernel\n  conditional density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of the conditional density f (x, $\\times$)\nof Y i given X i = x, from the observation of an i.i.d. sample (X i , Y i)\n$\\in$ R d , i = 1,. .. , n. We assume that f depends only on r unknown\ncomponents with typically r d. We provide an adaptive fully-nonparametric\nstrategy based on kernel rules to estimate f. To select the bandwidth of our\nkernel rule, we propose a new fast iterative algorithm inspired by the Rodeo\nalgorithm (Wasserman and Lafferty (2006)) to detect the sparsity structure of\nf. More precisely, in the minimax setting, our pointwise estimator, which is\nadaptive to both the regularity and the sparsity, achieves the quasi-optimal\nrate of convergence. Its computational complexity is only O(dn log n).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:50:41 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nguyen", "Minh-Lien Jeanne", "", "LAMA"], ["Lacour", "Claire", "", "LAMA"], ["Rivoirard", "Vincent", "", "CEREMADE"]]}, {"id": "2106.14825", "submitter": "Jun Tao Duan", "authors": "JunTao Duan, Ionel Popescu, Fan Zhou", "title": "Central Limit Theorem for product of dependent random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given $\\{X_k\\}$ is a martingale difference sequence. And given another\n$\\{Y_k\\}$ which has dependency within the sequence. Assume $\\{X_k\\}$ is\nindependent with $\\{Y_k\\}$, we study the properties of the sums of product of\ntwo sequences $\\sum_{k=1}^{n} X_k Y_k$. We obtain product-CLT, a modification\nof classical central limit theorem, which can be useful in the study of random\nprojections. We also obtain the rate of convergence which is similar to the\nBerry-Essen theorem in the classical CLT.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:02:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Duan", "JunTao", ""], ["Popescu", "Ionel", ""], ["Zhou", "Fan", ""]]}, {"id": "2106.14857", "submitter": "Robert Lunde", "authors": "Robert Lunde, Purnamrita Sarkar, Rachel Ward", "title": "Bootstrapping the error of Oja's Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of quantifying uncertainty for the estimation error\nof the leading eigenvector from Oja's algorithm for streaming principal\ncomponent analysis, where the data are generated IID from some unknown\ndistribution. By combining classical tools from the U-statistics literature\nwith recent results on high-dimensional central limit theorems for quadratic\nforms of random vectors and concentration of matrix products, we establish a\n$\\chi^2$ approximation result for the $\\sin^2$ error between the population\neigenvector and the output of Oja's algorithm. Since estimating the covariance\nmatrix associated with the approximating distribution requires knowledge of\nunknown model parameters, we propose a multiplier bootstrap algorithm that may\nbe updated in an online manner. We establish conditions under which the\nbootstrap distribution is close to the corresponding sampling distribution with\nhigh probability, thereby establishing the bootstrap as a consistent\ninferential method in an appropriate asymptotic regime.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:27:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""], ["Ward", "Rachel", ""]]}, {"id": "2106.14958", "submitter": "Aaron Hendrickson", "authors": "Aaron Hendrickson", "title": "A novel approach to photon transfer conversion gain estimation", "comments": "122 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP math.CA stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonuniformities in the imaging characteristics of modern image sensors are a\nprimary factor in the push to develop a pixel-level generalization of the\nphoton transfer characterization method. In this paper, we seek to develop a\nbody of theoretical results leading toward a comprehensive approach for\ntackling the biggest obstacle in the way of this goal: a means of pixel-level\nconversion gain estimation. This is accomplished by developing an estimator for\nthe reciprocal-difference of normal variances and then using this to construct\na novel estimator of the conversion gain. The first two moments of this\nestimator are derived and used to construct exact and approximate confidence\nintervals for its absolute relative bias and absolute coefficient of variation,\nrespectively. A means of approximating and computing optimal sample sizes are\nalso discussed and used to demonstrate the process of pixel-level conversion\ngain estimation for a real image sensor.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 19:50:18 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hendrickson", "Aaron", ""]]}, {"id": "2106.14997", "submitter": "Jonathan Siegel", "authors": "Jonathan W. Siegel, Jinchao Xu", "title": "Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:2101.12365", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximation rates of shallow neural networks with respect\nto the variation norm. Upper bounds on these rates have been established for\nsigmoidal and ReLU activation functions, but it has remained an important open\nproblem whether these rates are sharp. In this article, we provide a solution\nto this problem by proving sharp lower bounds on the approximation rates for\nshallow neural networks, which are obtained by lower bounding the $L^2$-metric\nentropy of the convex hull of the neural network basis functions. In addition,\nour methods also give sharp lower bounds on the Kolmogorov $n$-widths of this\nconvex hull, which show that the variation spaces corresponding to shallow\nneural networks cannot be efficiently approximated by linear methods. These\nlower bounds apply to both sigmoidal activation functions with bounded\nvariation and to activation functions which are a power of the ReLU. Our\nresults also quantify how much stronger the Barron spectral norm is than the\nvariation norm and, combined with previous results, give the asymptotics of the\n$L^\\infty$-metric entropy up to logarithmic factors in the case of the ReLU\nactivation function.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:01:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Siegel", "Jonathan W.", ""], ["Xu", "Jinchao", ""]]}, {"id": "2106.15000", "submitter": "Jonathan Siegel", "authors": "Jonathan W. Siegel, Jinchao Xu", "title": "Improved Convergence Rates for the Orthogonal Greedy Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the orthogonal greedy algorithm when applied to dictionaries\n$\\mathbb{D}$ whose convex hull has small entropy. We show that if the metric\nentropy of the convex hull of $\\mathbb{D}$ decays at a rate of\n$O(n^{-\\frac{1}{2}-\\alpha})$ for $\\alpha > 0$, then the orthogonal greedy\nalgorithm converges at the same rate. This improves upon the well-known\n$O(n^{-\\frac{1}{2}})$ convergence rate of the orthogonal greedy algorithm in\nmany cases, most notably for dictionaries corresponding to shallow neural\nnetworks. Finally, we show that these improved rates are sharp under the given\nentropy decay assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:07:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Siegel", "Jonathan W.", ""], ["Xu", "Jinchao", ""]]}, {"id": "2106.15003", "submitter": "Guy Tchuente", "authors": "Guy Tchuente", "title": "A Note on the Topology of the First Stage of 2SLS with Many Instruments", "comments": "21", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The finite sample properties of estimators are usually understood or\napproximated using asymptotic theories. Two main asymptotic constructions have\nbeen used to characterize the presence of many instruments. The first assumes\nthat the number of instruments increases with the sample size. I demonstrate\nthat in this case, one of the key assumptions used in the asymptotic\nconstruction may imply that the number of ``effective\" instruments should be\nfinite, resulting in an internal contradiction. The second asymptotic\nrepresentation considers that the number of instrumental variables (IVs) may be\nfinite, infinite, or even a continuum. The number does not change with the\nsample size. In this scenario, the regularized estimator obtained depends on\nthe topology imposed on the set of instruments as well as on a regularization\nparameter. These restrictions may induce a bias or restrict the set of\nadmissible instruments. However, the assumptions are internally coherent. The\nlimitations of many IVs asymptotic assumptions provide support for finite\nsample distributional studies to better understand the behavior of many IV\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:22:52 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tchuente", "Guy", ""]]}, {"id": "2106.15013", "submitter": "Dominik St\\\"oger", "authors": "Dominik St\\\"oger and Mahdi Soltanolkotabi", "title": "Small random initialization is akin to spectral learning: Optimization\n  and generalization guarantees for overparameterized low-rank matrix\n  reconstruction", "comments": "80 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant theoretical progress on understanding the\nconvergence and generalization of gradient-based methods on nonconvex losses\nwith overparameterized models. Nevertheless, many aspects of optimization and\ngeneralization and in particular the critical role of small random\ninitialization are not fully understood. In this paper, we take a step towards\ndemystifying this role by proving that small random initialization followed by\na few iterations of gradient descent behaves akin to popular spectral methods.\nWe also show that this implicit spectral bias from small random initialization,\nwhich is provably more prominent for overparameterized models, also puts the\ngradient descent iterations on a particular trajectory towards solutions that\nare not only globally optimal but also generalize well. Concretely, we focus on\nthe problem of reconstructing a low-rank matrix from a few measurements via a\nnatural nonconvex formulation. In this setting, we show that the trajectory of\nthe gradient descent iterations from small random initialization can be\napproximately decomposed into three phases: (I) a spectral or alignment phase\nwhere we show that that the iterates have an implicit spectral bias akin to\nspectral initialization allowing us to show that at the end of this phase the\ncolumn space of the iterates and the underlying low-rank matrix are\nsufficiently aligned, (II) a saddle avoidance/refinement phase where we show\nthat the trajectory of the gradient iterates moves away from certain degenerate\nsaddle points, and (III) a local refinement phase where we show that after\navoiding the saddles the iterates converge quickly to the underlying low-rank\nmatrix. Underlying our analysis are insights for the analysis of\noverparameterized nonconvex optimization schemes that may have implications for\ncomputational problems beyond low-rank reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:52:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["St\u00f6ger", "Dominik", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2106.15074", "submitter": "Ye Wang", "authors": "Ye Wang", "title": "Causal Inference under Temporal and Spatial Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social events and policies generate spillover effects in both time and\nspace. Their occurrence influences not only the outcomes of interest in the\nfuture, but also these outcomes in nearby areas. In this paper, we propose a\ndesign-based approach to estimate the direct and indirect/spillover treatment\neffects of any event or policy under the assumption of sequential ignorability,\nwhen both temporal and spatial interference are allowed to present. The\nproposed estimators are shown to be consistent and asymptotically Normal if the\ndegree of interference dependence does not grow too fast relative to the sample\nsize. The conventional difference-in-differences (DID) or two-way fixed effects\nmodel, nevertheless, leads to biased estimates in this scenario. We apply the\nmethod to examine the impact of Hong Kong's Umbrella Movement on the result of\nthe ensuing election and how an institutional reform affects real estate\nassessment in New York State.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:47:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Ye", ""]]}, {"id": "2106.15369", "submitter": "Johanna F. Ziegel", "authors": "Anja M\\\"uhlemann and Johanna F. Ziegel", "title": "Isotonic regression for functionals of elicitation complexity greater\n  than one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the non-parametric isotonic regression problem for bivariate\nelicitable functionals that are given as an elicitable univariate functional\nand its Bayes risk. Prominent examples for functionals of this type are (mean,\nvariance) and (Value-at-Risk, Expected Shortfall), where the latter pair\nconsists of important risk measures in finance. We present our results for\ntotally ordered covariates but extenstions to partial orders are given in the\nappendix.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:55:47 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["M\u00fchlemann", "Anja", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "2106.15521", "submitter": "Fadlalla Elfadaly PhD", "authors": "Paul H. Garthwaite, Maha W. Moustafa and Fadlalla G. Elfadaly", "title": "Locally correct confidence intervals for a binomial proportion: A new\n  criteria for an interval estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Well-recommended methods of forming `confidence intervals' for a binomial\nproportion give interval estimates that do not actually meet the definition of\na confidence interval, in that their coverages are sometimes lower than the\nnominal confidence level. The methods are favoured because their intervals have\na shorter average length than the Clopper-Pearson (gold-standard) method, whose\nintervals really are confidence intervals. Comparison of such methods is tricky\n-- the best method should perhaps be the one that gives the shortest intervals\n(on average), but when is the coverage of a method so poor that it should not\nbe classed as a means of forming confidence intervals?\n  As the definition of a confidence interval is not being adhered to, another\ncriterion for forming interval estimates for a binomial proportion is needed.\nIn this paper we suggest a new criterion; methods which meet the criterion are\nsaid to yield $\\textit{locally correct confidence intervals}$. We propose a\nmethod that yields such intervals and prove that its intervals have a shorter\naverage length than those of any other method that meets the criterion.\nCompared with the Clopper-Pearson method, the proposed method gives intervals\nwith an appreciably smaller average length. The mid-$p$ method also satisfies\nthe new criterion and has its own optimality property.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:49:49 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Garthwaite", "Paul H.", ""], ["Moustafa", "Maha W.", ""], ["Elfadaly", "Fadlalla G.", ""]]}, {"id": "2106.15603", "submitter": "Viktor Skorniakov", "authors": "Ugn\\.e \\v{C}i\\v{z}ikovien\\.e and Viktor Skorniakov", "title": "On the Optimal Configuration of a Square Array Group Testing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Up to date, only lower and upper bounds for the optimal configuration of a\nSquare Array (A2) Group Testing (GT) algorithm are known. We establish exact\nanalytical formulae and provide a couple of applications of our result. First,\nwe compare the A2 GT scheme to several other classical GT schemes in terms of\nthe gain per specimen attained at optimal configuration. Second, operating\nunder objective Bayesian framework with the loss designed to attain minimum at\noptimal GT configuration, we suggest the preferred choice of the group size\nunder natural minimal assumptions: the prior information regarding the\nprevalence suggests that grouping and application of A2 is better than\nindividual testing. The same suggestion is provided for the Minimax strategy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:45:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["\u010ci\u017eikovien\u0117", "Ugn\u0117", ""], ["Skorniakov", "Viktor", ""]]}, {"id": "2106.15675", "submitter": "Julia Lindberg", "authors": "Julia Lindberg, Carlos Am\\'endola, Jose Israel Rodriguez", "title": "Estimating Gaussian mixtures using sparse polynomial moment systems", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of moments is a statistical technique for density estimation that\nsolves a system of moment equations to estimate the parameters of an unknown\ndistribution. A fundamental question critical to understanding identifiability\nasks how many moment equations are needed to get finitely many solutions and\nhow many solutions there are. We answer this question for classes of Gaussian\nmixture models using the tools of polyhedral geometry. Using these results, we\npresent an algorithm that performs parameter recovery, and therefore density\nestimation, for high dimensional Gaussian mixture models that scales linearly\nin the dimension.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 18:43:36 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lindberg", "Julia", ""], ["Am\u00e9ndola", "Carlos", ""], ["Rodriguez", "Jose Israel", ""]]}, {"id": "2106.15682", "submitter": "Bo Luan", "authors": "Bo Luan, Yoonkyung Lee, Yunzhang Zhu", "title": "Predictive Model Degrees of Freedom in Linear Regression", "comments": "47 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overparametrized interpolating models have drawn increasing attention from\nmachine learning. Some recent studies suggest that regularized interpolating\nmodels can generalize well. This phenomenon seemingly contradicts the\nconventional wisdom that interpolation tends to overfit the data and performs\npoorly on test data. Further, it appears to defy the bias-variance trade-off.\nAs one of the shortcomings of the existing theory, the classical notion of\nmodel degrees of freedom fails to explain the intrinsic difference among the\ninterpolating models since it focuses on estimation of in-sample prediction\nerror. This motivates an alternative measure of model complexity which can\ndifferentiate those interpolating models and take different test points into\naccount. In particular, we propose a measure with a proper adjustment based on\nthe squared covariance between the predictions and observations. Our analysis\nwith least squares method reveals some interesting properties of the measure,\nwhich can reconcile the \"double descent\" phenomenon with the classical theory.\nThis opens doors to an extended definition of model degrees of freedom in\nmodern predictive settings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:12:12 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Luan", "Bo", ""], ["Lee", "Yoonkyung", ""], ["Zhu", "Yunzhang", ""]]}, {"id": "2106.15743", "submitter": "Chiao-Yu Yang", "authors": "Chiao-Yu Yang, Lihua Lei, Nhat Ho, and Will Fithian", "title": "BONuS: Multiple multivariate testing with a data-adaptivetest statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new adaptive empirical Bayes framework, the\nBag-Of-Null-Statistics (BONuS) procedure, for multiple testing where each\nhypothesis testing problem is itself multivariate or nonparametric. BONuS is an\nadaptive and interactive knockoff-type method that helps improve the testing\npower while controlling the false discovery rate (FDR), and is closely\nconnected to the \"counting knockoffs\" procedure analyzed in Weinstein et al.\n(2017). Contrary to procedures that start with a $p$-value for each hypothesis,\nour method analyzes the entire data set to adaptively estimate an optimal\n$p$-value transform based on an empirical Bayes model. Despite the extra\nadaptivity, our method controls FDR in finite samples even if the empirical\nBayes model is incorrect or the estimation is poor. An extension, the Double\nBONuS procedure, validates the empirical Bayes model to guard against power\nloss due to model misspecification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 22:08:26 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 18:28:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Chiao-Yu", ""], ["Lei", "Lihua", ""], ["Ho", "Nhat", ""], ["Fithian", "Will", ""]]}, {"id": "2106.15908", "submitter": "Vasilis Kontonis", "authors": "Constantinos Daskalakis, Vasilis Kontonis, Christos Tzamos, Manolis\n  Zampetakis", "title": "A Statistical Taylor Theorem and Extrapolation of Truncated Densities", "comments": "Appeared at COLT2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a statistical version of Taylor's theorem and apply this result to\nnon-parametric density estimation from truncated samples, which is a classical\nchallenge in Statistics \\cite{woodroofe1985estimating, stute1993almost}. The\nsingle-dimensional version of our theorem has the following implication: \"For\nany distribution $P$ on $[0, 1]$ with a smooth log-density function, given\nsamples from the conditional distribution of $P$ on $[a, a + \\varepsilon]\n\\subset [0, 1]$, we can efficiently identify an approximation to $P$ over the\n\\emph{whole} interval $[0, 1]$, with quality of approximation that improves\nwith the smoothness of $P$.\"\n  To the best of knowledge, our result is the first in the area of\nnon-parametric density estimation from truncated samples, which works under the\nhard truncation model, where the samples outside some survival set $S$ are\nnever observed, and applies to multiple dimensions. In contrast, previous works\nassume single dimensional data where each sample has a different survival set\n$S$ so that samples from the whole support will ultimately be collected.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:53:43 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2106.16042", "submitter": "Dong Xia", "authors": "Zhongyuan Lyu and Dong Xia and Yuan Zhang", "title": "Latent Space Model for Higher-order Networks and Generalized Tensor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a unified framework, formulated as general latent space models,\nto study complex higher-order network interactions among multiple entities. Our\nframework covers several popular models in recent network analysis literature,\nincluding mixture multi-layer latent space model and hypergraph latent space\nmodel. We formulate the relationship between the latent positions and the\nobserved data via a generalized multilinear kernel as the link function. While\nour model enjoys decent generality, its maximum likelihood parameter estimation\nis also convenient via a generalized tensor decomposition procedure.We propose\na novel algorithm using projected gradient descent on Grassmannians. We also\ndevelop original theoretical guarantees for our algorithm. First, we show its\nlinear convergence under mild conditions. Second, we establish finite-sample\nstatistical error rates of latent position estimation, determined by the signal\nstrength, degrees of freedom and the smoothness of link function, for both\ngeneral and specific latent space models. We demonstrate the effectiveness of\nour method on synthetic data. We also showcase the merit of our method on two\nreal-world datasets that are conventionally described by different specific\nmodels in producing meaningful and interpretable parameter estimations and\naccurate link prediction. We demonstrate the effectiveness of our method on\nsynthetic data. We also showcase the merit of our method on two real-world\ndatasets that are conventionally described by different specific models in\nproducing meaningful and interpretable parameter estimations and accurate link\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:11:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lyu", "Zhongyuan", ""], ["Xia", "Dong", ""], ["Zhang", "Yuan", ""]]}, {"id": "2106.16085", "submitter": "Jinwook Lee Ph.D.", "authors": "Matthew J. Schneider and Jinwook Lee", "title": "Protecting Time Series Data with Minimal Forecast Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting could be negatively impacted due to anonymization requirements in\ndata protection legislation. To measure the potential severity of this problem,\nwe derive theoretical bounds for the loss to forecasts from additive\nexponential smoothing models using protected data. Following the guidelines of\nanonymization from the General Data Protection Regulation (GDPR) and California\nConsumer Privacy Act (CCPA), we develop the $k$-nearest Time Series ($k$-nTS)\nSwapping and $k$-means Time Series ($k$-mTS) Shuffling methods to create\nprotected time series data that minimizes the loss to forecasts while\npreventing a data intruder from detecting privacy issues. For efficient and\neffective decision making, we formally model an integer programming problem for\na perfect matching for simultaneous data swapping in each cluster. We call it a\ntwo-party data privacy framework since our optimization model includes the\nutilities of a data provider and data intruder. We apply our data protection\nmethods to thousands of time series and find that it maintains the forecasts\nand patterns (level, trend, and seasonality) of time series well compared to\nstandard data protection methods suggested in legislation. Substantively, our\npaper addresses the challenge of protecting time series data when used for\nforecasting. Our findings suggest the managerial importance of incorporating\nthe concerns of forecasters into the data protection itself.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:20:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Schneider", "Matthew J.", ""], ["Lee", "Jinwook", ""]]}, {"id": "2106.16116", "submitter": "Carlo Ciliberto", "authors": "Alessandro Rudi and Carlo Ciliberto", "title": "PSD Representations for Effective Probability Models", "comments": "52 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a good way to model probability densities is key to probabilistic\ninference. An ideal model should be able to concisely approximate any\nprobability, while being also compatible with two main operations:\nmultiplications of two models (product rule) and marginalization with respect\nto a subset of the random variables (sum rule). In this work, we show that a\nrecently proposed class of positive semi-definite (PSD) models for non-negative\nfunctions is particularly suited to this end. In particular, we characterize\nboth approximation and generalization capabilities of PSD models, showing that\nthey enjoy strong theoretical guarantees. Moreover, we show that we can perform\nefficiently both sum and product rule in closed form via matrix operations,\nenjoying the same versatility of mixture models. Our results open the way to\napplications of PSD models to density estimation, decision theory and\ninference. Preliminary empirical evaluation supports our findings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:13:39 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 13:41:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Rudi", "Alessandro", ""], ["Ciliberto", "Carlo", ""]]}, {"id": "2106.16149", "submitter": "Thomas Delerue", "authors": "Carsten Chong and Thomas Delerue and Guoying Li", "title": "Mixed semimartingales: Volatility estimation in the presence of\n  fractional noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating volatility for high-frequency data when\nthe observed process is the sum of a continuous It\\^o semimartingale and a\nnoise process that locally behaves like fractional Brownian motion with Hurst\nparameter H. The resulting class of processes, which we call mixed\nsemimartingales, generalizes the mixed fractional Brownian motion introduced by\nCheridito [Bernoulli 7 (2001) 913-934] to time-dependent and stochastic\nvolatility. Based on central limit theorems for variation functionals, we\nderive consistent estimators and asymptotic confidence intervals for H and the\nintegrated volatilities of both the semimartingale and the noise part, in all\ncases where these quantities are identifiable. When applied to recent stock\nprice data, we find strong empirical evidence for the presence of fractional\nnoise, with Hurst parameters H that vary considerably over time and between\nassets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:50:35 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chong", "Carsten", ""], ["Delerue", "Thomas", ""], ["Li", "Guoying", ""]]}, {"id": "2106.16225", "submitter": "Sidak Pal Singh", "authors": "Sidak Pal Singh, Gregor Bachmann, Thomas Hofmann", "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:29:58 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:57:50 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Singh", "Sidak Pal", ""], ["Bachmann", "Gregor", ""], ["Hofmann", "Thomas", ""]]}]