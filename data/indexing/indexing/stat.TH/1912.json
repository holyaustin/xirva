[{"id": "1912.00150", "submitter": "Amadou Diadie Ba", "authors": "Ba Amadou Diadie", "title": "Non parametric estimation of residual-past entropy, mean residual-past\n  lifetime, residual-past inaccuracy measure and asymptotic limits", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we provide the asymptotic behavior of the residual-past\nentropy, of the mean residual-past lifetime distribution and of the\nresidual-past inaccuracy measure. We are interested in these measures of\nuncertainty in the discrete case. Almost sure rates of convergence and\nasymptotic normality results are established. Our theoretical results are\nvalidated by simulations\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:30:00 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Diadie", "Ba Amadou", ""]]}, {"id": "1912.00306", "submitter": "Ezequiel Smucler", "authors": "Andrea Rotnitzky and Ezequiel Smucler", "title": "Efficient adjustment sets for population average treatment effect\n  estimation in non-parametric causal graphical models", "comments": "Fixed a typo in Example 1, an arrow was missing from L1 to Y in the\n  DAG and L1 was missing in the second adjustment set", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of covariate adjustment is often used for estimation of population\naverage treatment effects in observational studies. Graphical rules for\ndetermining all valid covariate adjustment sets from an assumed causal\ngraphical model are well known. Restricting attention to causal linear models,\na recent article derived two novel graphical criteria: one to compare the\nasymptotic variance of linear regression treatment effect estimators that\ncontrol for certain distinct adjustment sets and another to identify the\noptimal adjustment set that yields the least squares treatment effect estimator\nwith the smallest asymptotic variance among consistent adjusted least squares\nestimators. In this paper we show that the same graphical criteria can be used\nin non-parametric causal graphical models when treatment effects are estimated\nby contrasts involving non-parametrically adjusted estimators of the\ninterventional means. We also provide a graphical criterion for determining the\noptimal adjustment set among the minimal adjustment sets, which is valid for\nboth linear and non-parametric estimators. We provide a new graphical criterion\nfor comparing time dependent adjustment sets, that is, sets comprised by\ncovariates that adjust for future treatments and that are themselves affected\nby earlier treatments. We show by example that uniformly optimal time dependent\nadjustment sets do not always exist. In addition, for point interventions, we\nprovide a sound and complete graphical criterion for determining when a\nnon-parametric optimally adjusted estimator of an interventional mean, or of a\ncontrast of interventional means, is as efficient as an efficient estimator of\nthe same parameter that exploits the information in the conditional\nindependencies encoded in the non-parametric causal graphical model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 02:35:53 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 23:24:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Rotnitzky", "Andrea", ""], ["Smucler", "Ezequiel", ""]]}, {"id": "1912.00396", "submitter": "Bernd Sturmfels", "authors": "Michael F. Adamer, Andr\\'as C. L\\H{o}rincz, Anna-Laura Sattelberger,\n  and Bernd Sturmfels", "title": "Algebraic Analysis of Rotation Data", "comments": "21 pages", "journal-ref": "Alg. Stat. 11 (2020) 189-211", "doi": "10.2140/astat.2020.11.189", "report-no": null, "categories": "math.ST cs.SC math.AG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop algebraic tools for statistical inference from samples of rotation\nmatrices. This rests on the theory of D-modules in algebraic analysis.\nNoncommutative Gr\\\"obner bases are used to design numerical algorithms for\nmaximum likelihood estimation, building on the holonomic gradient method of\nSei, Shibata, Takemura, Ohara, and Takayama. We study the Fisher model for\nsampling from rotation matrices, and we apply our algorithms for data from the\napplied sciences. On the theoretical side, we generalize the underlying\nequivariant D-modules from SO(3) to arbitrary Lie groups. For compact groups,\nour D-ideals encode the normalizing constant of the Fisher model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 12:45:20 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Adamer", "Michael F.", ""], ["L\u0151rincz", "Andr\u00e1s C.", ""], ["Sattelberger", "Anna-Laura", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1912.00478", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou", "title": "Anisotropic Functional Deconvolution for the irregular design with\n  dependent long-memory errors", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic functional deconvolution model is investigated in the bivariate\ncase under long-memory errors when the design points $t_i$, $i=1, 2, \\cdots,\nN$, and $x_l$, $l=1, 2, \\cdots, M$, are irregular and follow known densities\n$h_1$, $h_2$, respectively. In particular, we focus on the case when the\ndensities $h_1$ and $h_2$ have singularities, but $1/h_1$ and $1/h_2$ are still\nintegrable on $[0, 1]$. Under both Gaussian and sub-Gaussian errors, we\nconstruct an adaptive wavelet estimator that attains asymptotically\nnear-optimal convergence rates that deteriorate as long-memory strengthens. The\nconvergence rates are completely new and depend on a balance between the\nsmoothness and the spatial homogeneity of the unknown function $f$, the degree\nof ill-posed-ness of the convolution operator, the long-memory parameter in\naddition to the degrees of spatial irregularity associated with $h_1$ and\n$h_2$. Nevertheless, the spatial irregularity affects convergence rates only\nwhen $f$ is spatially inhomogeneous in either direction.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 19:03:19 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:15:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Benhaddou", "Rida", ""]]}, {"id": "1912.00610", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "On a generalization of the Jensen-Shannon divergence", "comments": "19 pages, 3 figures", "journal-ref": "Entropy 2020, 22(2), 221", "doi": "10.3390/e22020221", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jensen-Shannon divergence is a renown bounded symmetrization of the\nKullback-Leibler divergence which does not require probability densities to\nhave matching supports. In this paper, we introduce a vector-skew\ngeneralization of the scalar $\\alpha$-Jensen-Bregman divergences and derive\nthereof the vector-skew $\\alpha$-Jensen-Shannon divergences. We study the\nproperties of these novel divergences and show how to build parametric families\nof symmetric Jensen-Shannon-type divergences. Finally, we report an iterative\nalgorithm to numerically compute the Jensen-Shannon-type centroids for a set of\nprobability densities belonging to a mixture family: This includes the case of\nthe Jensen-Shannon centroid of a set of categorical distributions or normalized\nhistograms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:41:30 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 11:31:59 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 07:58:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1912.00636", "submitter": "Vrettos Moulos", "authors": "Vrettos Moulos", "title": "Optimal Best Markovian Arm Identification with Fixed Confidence", "comments": "Neural Information Processing Systems (NeurIPS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of the sampling complexity of best\nMarkovian arm identification in one-parameter Markovian bandit models. We\nderive instance specific nonasymptotic and asymptotic lower bounds which\ngeneralize those of the IID setting. We analyze the Track-and-Stop strategy,\ninitially proposed for the IID setting, and we prove that asymptotically it is\nat most a factor of four apart from the lower bound. Our one-parameter\nMarkovian bandit model is based on the notion of an exponential family of\nstochastic matrices for which we establish many useful properties. For the\nanalysis of the Track-and-Stop strategy we derive a novel concentration\ninequality for Markov chains that may be of interest in its own right.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 08:54:55 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 07:15:56 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 07:36:17 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Moulos", "Vrettos", ""]]}, {"id": "1912.00671", "submitter": "Tim Sullivan", "authors": "Ilja Klebanov and Ingmar Schuster and T. J. Sullivan", "title": "A Rigorous Theory of Conditional Mean Embeddings", "comments": "30 pages, 3 figures", "journal-ref": "SIAM Journal on Mathematics of Data Science 2(3):583--606, 2020", "doi": "10.1137/19M1305069", "report-no": null, "categories": "math.ST math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional mean embeddings (CMEs) have proven themselves to be a powerful\ntool in many machine learning applications. They allow the efficient\nconditioning of probability distributions within the corresponding reproducing\nkernel Hilbert spaces (RKHSs) by providing a linear-algebraic relation for the\nkernel mean embeddings of the respective joint and conditional probability\ndistributions. Both centred and uncentred covariance operators have been used\nto define CMEs in the existing literature. In this paper, we develop a\nmathematically rigorous theory for both variants, discuss the merits and\nproblems of each, and significantly weaken the conditions for applicability of\nCMEs. In the course of this, we demonstrate a beautiful connection to Gaussian\nconditioning in Hilbert spaces.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:30:21 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:51:09 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 15:26:51 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 14:04:20 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Klebanov", "Ilja", ""], ["Schuster", "Ingmar", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1912.00798", "submitter": "Khaled Masoumifard", "authors": "Khaled Masoumifard", "title": "Equivalence of the Hazard Rate and Usual Stochastic Orders for Parallel\n  Systems", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate stochastic comparisons of parallel systems, and\nobtain two characterization results in this regard. First, we compare a\nparallel system with independent heterogeneous components to a parallel system\nwith homogeneous components, and establish some certain assumptions under which\nthe hazard rate and usual stochastic orders between the lifetimes of two\nparallel systems are equivalent. Next, we turn our attention to two parallel\nsystems with their component lifetimes following multiple-outlier model and\nprove that under some specified assumptions, the $p$-larger order between the\nvectors of scale parameters is equivalent to the hazard rate order as well as\nthe usual stochastic order between the lifetimes of these systems. The results\nestablished here are applicable to compute an upper bound for the hazard rate\nfunction and a lower bound for the survival function of a parallel systems\nconsisting of heterogeneous components.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 08:17:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Masoumifard", "Khaled", ""]]}, {"id": "1912.00894", "submitter": "Nikolas Nuesken", "authors": "A. Duncan and N. Nuesken and L. Szpruch", "title": "On the geometry of Stein variational gradient descent", "comments": "39 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference problems require sampling or approximating\nhigh-dimensional probability distributions. The focus of this paper is on the\nrecently introduced Stein variational gradient descent methodology, a class of\nalgorithms that rely on iterated steepest descent steps with respect to a\nreproducing kernel Hilbert space norm. This construction leads to interacting\nparticle systems, the mean-field limit of which is a gradient flow on the space\nof probability distributions equipped with a certain geometrical structure. We\nleverage this viewpoint to shed some light on the convergence properties of the\nalgorithm, in particular addressing the problem of choosing a suitable positive\ndefinite kernel function. Our analysis leads us to considering certain\nnondifferentiable kernels with adjusted tails. We demonstrate significant\nperforms gains of these in various numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:20:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Duncan", "A.", ""], ["Nuesken", "N.", ""], ["Szpruch", "L.", ""]]}, {"id": "1912.01103", "submitter": "Tianhong Sheng", "authors": "Tianhong Sheng and Bharath K. Sriperumbudur", "title": "On Distance and Kernel Measures of Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional independence is one of the important tasks in\nstatistical inference and is fundamental in causal discovery, feature\nselection, dimensionality reduction, Bayesian network learning, and others. In\nthis work, we explore the connection between conditional independence measures\ninduced by distances on a metric space and reproducing kernels associated with\na reproducing kernel Hilbert space (RKHS). For certain distance and kernel\npairs, we show the distance-based conditional independence measures to be\nequivalent to that of kernel-based measures. On the other hand, we also show\nthat some popular---in machine learning---kernel conditional independence\nmeasures based on the Hilbert-Schmidt norm of a certain cross-conditional\ncovariance operator, do not have a simple distance representation, except in\nsome limiting cases. This paper, therefore, shows the distance and kernel\nmeasures of conditional independence to be not quite equivalent unlike in the\ncase of joint independence as shown by Sejdinovic et al. (2013).\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 22:37:21 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 16:05:52 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sheng", "Tianhong", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1912.01134", "submitter": "Robert Staudte", "authors": "Robert G. Staudte", "title": "Evidence for goodness of fit in Karl Pearson chi-squared statistics", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chi-squared tests for lack of fit are traditionally employed to find evidence\nagainst a hypothesized model, with the model accepted if the Karl Pearson\nstatistic comparing observed and expected numbers of observations falling\nwithin cells is not significantly large. However, if one really wants evidence\nfor goodness of fit, it is better to adopt an equivalence testing approach in\nwhich small values of the chi-squared statistic are evidence for the desired\nmodel. This method requires one to define what is meant by equivalence to the\ndesired model, and guidelines are proposed. Then a simple extension of the\nclassical normalizing transformation for the non-central chi-squared\ndistribution places these values on a simple to interpret calibration scale for\nevidence. It is shown that the evidence can distinguish between normal and\nnearby models, as well between the Poisson and over-dispersed models.\nApplications to evaluation of random number generators and to uniformity of the\ndigits of pi are included. Sample sizes required to obtain a desired expected\nevidence for goodness of fit are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 00:24:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Staudte", "Robert G.", ""]]}, {"id": "1912.01157", "submitter": "Xu Han", "authors": "Xu Han", "title": "Nonparametric Screening under Conditional Strictly Convex Loss for\n  Ultrahigh Dimensional Sparse Data", "comments": "Supplementary materials including the technical proofs are available\n  online at Annals of Statistics", "journal-ref": "Annals of Statistics, 2019, Vol 47, No 4, 1995-2022", "doi": "10.1214/18-AOS1738", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sure screening technique has been considered as a powerful tool to handle the\nultrahigh dimensional variable selection problems, where the dimensionality p\nand the sample size n can satisfy the NP dimensionality log p=O(n^a) for some\na>0 (Fan & Lv 2008). The current paper aims to simultaneously tackle the\n\"universality\" and \"effectiveness\" of sure screening procedures. For the\n\"universality\", we develop a general and unified framework for nonparametric\nscreening methods from a loss function perspective. Consider a loss function to\nmeasure the divergence of the response variable and the underlying\nnonparametric function of covariates. We newly propose a class of loss\nfunctions called conditional strictly convex loss, which contains, but is not\nlimited to, negative log likelihood loss from one-parameter exponential\nfamilies, exponential loss for binary classification and quantile regression\nloss. The sure screening property and model selection size control will be\nestablished within this class of loss functions. For the ``effectiveness\", we\nfocus on a goodness of fit nonparametric screening (Goffins) method under\nconditional strictly convex loss. Interestingly, we can achieve a better\nconvergence probability of containing the true model compared with related\nliterature. The superior performance of our proposed method has been further\ndemonstrated by extensive simulation studies and some real scientific data\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:26:48 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Han", "Xu", ""]]}, {"id": "1912.01194", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn and Kosaku Takanashi", "title": "Mean-shift least squares model averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new estimator for selecting weights to average over\nleast squares estimates obtained from a set of models. Our proposed estimator\nbuilds on the Mallows model average (MMA) estimator of Hansen (2007), but,\nunlike MMA, simultaneously controls for location bias and regression error\nthrough a common constant. We show that our proposed estimator-- the mean-shift\nMallows model average (MSA) estimator-- is asymptotically optimal to the\noriginal MMA estimator in terms of mean squared error. A simulation study is\npresented, where we show that our proposed estimator uniformly outperforms the\nMMA estimator.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:19:17 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Takanashi", "Kosaku", ""]]}, {"id": "1912.01200", "submitter": "Murthy Mittinty N", "authors": "Murthy N Mittinty and Stijn Vansteelandt", "title": "Longitudinal Mediation Analysis Using Natural Effect Models", "comments": "30 pages, 5 figures, 3 Tables, 3 Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis is concerned with the decomposition of the total effect of\nan exposure on an outcome into the indirect effect through a given mediator,\nand the remaining direct effect. This is ideally done using longitudinal\nmeasurements of the mediator, as these capture the mediator process more\nfinely. However, longitudinal measurements pose challenges for mediation\nanalysis. This is because the mediators and outcomes measured at a given\ntime-point can act as confounders for the association between mediators and\noutcomes at a later time-point; these confounders are themselves affected by\nthe prior exposure and outcome. Such post-treatment confounding cannot be dealt\nwith using standard methods (e.g. generalized estimating equations). Analysis\nis further complicated by the need for so-called cross-world counterfactuals to\ndecompose the total effect. This article addresses these challenges. In\nparticular, we introduce so-called natural effect models, which parameterize\nthe direct and indirect effect of a baseline exposure w.r.t. a longitudinal\nmediator and outcome. These can be viewed as a generalization of marginal\nstructural models to enable effect decomposition. We introduce inverse\nprobability weighting techniques for fitting these models, adjusting for\n(measured) time-varying confounding of the mediator-outcome association.\nApplication of this methodology uses data from the Millennium Cohort Study, UK.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:45:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mittinty", "Murthy N", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1912.01245", "submitter": "Robert Gaunt", "authors": "Robert E. Gaunt", "title": "A simple proof of the characteristic function of Student's\n  $t$-distribution", "comments": "3 pages. To appear in Communications in Statistics - Theory and\n  Methods, 2019+", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a simple proof of the characteristic function of Student's\n$t$-distribution. The method of proof, which involves finding a differential\nequation satisfied by the characteristic function, is applicable to many other\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:54:22 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Gaunt", "Robert E.", ""]]}, {"id": "1912.01308", "submitter": "Othmane Mazhar", "authors": "Othmane Mazhar, Cristian R. Rojas, Carlo Fischione and Mohammad R.\n  Hesamzadeh", "title": "Bayesian Model Selection for Change Point Detection and Clustering", "comments": "37 page, 4 figures, Proceedings of the 35th International Conference\n  on Machine Learning (ICML), PMLR 80:3433-3442, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the new problem of estimating a piece-wise constant signal with\nthe purpose of detecting its change points and the levels of clusters. Our\napproach is to model it as a nonparametric penalized least square model\nselection on a family of models indexed over the collection of partitions of\nthe design points and propose a computationally efficient algorithm to\napproximately solve it. Statistically, minimizing such a penalized criterion\nyields an approximation to the maximum a posteriori probability (MAP)\nestimator. The criterion is then analyzed and an oracle inequality is derived\nusing a Gaussian concentration inequality. The oracle inequality is used to\nderive on one hand conditions for consistency and on the other hand an adaptive\nupper bound on the expected square risk of the estimator, which statistically\nmotivates our approximation. Finally, we apply our algorithm to simulated data\nto experimentally validate the statistical guarantees and illustrate its\nbehavior.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 11:28:05 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mazhar", "Othmane", ""], ["Rojas", "Cristian R.", ""], ["Fischione", "Carlo", ""], ["Hesamzadeh", "Mohammad R.", ""]]}, {"id": "1912.01388", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Copula versions of distance multivariance and dHSIC via the\n  distributional transform -- a general approach to construct invariant\n  dependence measures", "comments": "improved notation", "journal-ref": "Statistics (2020)", "doi": "10.1080/02331888.2020.1748029", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Hilbert-Schmidt-Independence-Criterion (dHSIC) and distance\nmultivariance allow to measure and test independence of an arbitrary number of\nrandom vectors with arbitrary dimensions. Here we define versions which only\ndepend on an underlying copula. The approach is based on the distributional\ntransform, yielding dependence measures which always feature a natural\ninvariance with respect to scalings and translations. Moreover, it requires no\ndistributional assumptions, i.e., the distributions can be of pure type or any\nmixture of discrete and continuous distributions and (in our setting) no\nexistence of moments is required.\n  Empirical estimators and tests, which are consistent against all\nalternatives, are provided based on a Monte Carlo distributional transform. In\nparticular, it is shown that the new estimators inherit the exact limiting\ndistributional properties of the original estimators. Examples illustrate that\ntests based on the new measures can be more powerful than tests based on other\ncopula dependence measures.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:20:56 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:25:02 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "1912.01417", "submitter": "Dominic Richards", "authors": "Dominic Richards, Sahand N. Negahban, Patrick Rebeschini", "title": "Decentralised Sparse Multi-Task Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse multi-task regression framework for fitting a collection\nof related sparse models. Representing models as nodes in a graph with edges\nbetween related models, a framework that fuses lasso regressions with the total\nvariation penalty is investigated. Under a form of restricted eigenvalue\nassumption, bounds on prediction and squared error are given that depend upon\nthe sparsity of each model and the differences between related models. This\nassumption relates to the smallest eigenvalue restricted to the intersection of\ntwo cone sets of the covariance matrix constructed from each of the agents'\ncovariances. We show that this assumption can be satisfied if the constructed\ncovariance matrix satisfies a restricted isometry property. In the case of a\ngrid topology high-probability bounds are given that match, up to log factors,\nthe no-communication setting of fitting a lasso on each model, divided by the\nnumber of agents. A decentralised dual method that exploits a convex-concave\nformulation of the penalised problem is proposed to fit the models and its\neffectiveness demonstrated on simulations against the group lasso and variants.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:39:22 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Richards", "Dominic", ""], ["Negahban", "Sahand N.", ""], ["Rebeschini", "Patrick", ""]]}, {"id": "1912.01463", "submitter": "Hamid El Maroufy", "authors": "El Omari Mohamed, Hamid El Maroufy and Christiane Fuchs", "title": "Statistical inference for fractional diffusion process with random\n  effects at discrete observations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of inference associated with linear\nfractional diffusion process with random effects in the drift. In particular we\nare concerned with the maximum likelihood estimators (MLE) of the random effect\nparameters. First of all, we estimate the Hurst parameter H from one single\nsubject. Second, assuming the Hurst index H is known, we derive the MLE and\nexamine their asymptotic behavior as the number of subjects under study becomes\nlarge, with random effects normally distributed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 15:26:36 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mohamed", "El Omari", ""], ["Maroufy", "Hamid El", ""], ["Fuchs", "Christiane", ""]]}, {"id": "1912.01599", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, Ilias Zadik", "title": "Stationary Points of Shallow Neural Networks with Quadratic Activation\n  Function", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the teacher-student setting of learning shallow neural networks\nwith quadratic activations and planted weight matrix $W^*\\in\\mathbb{R}^{m\\times\nd}$, where $m$ is the width of the hidden layer and $d\\le m$ is the data\ndimension. We study the optimization landscape associated with the empirical\nand the population squared risk of the problem. Under the assumption the\nplanted weights are full-rank we obtain the following results. First, we\nestablish that the landscape of the empirical risk admits an \"energy barrier\"\nseparating rank-deficient $W$ from $W^*$: if $W$ is rank deficient, then its\nrisk is bounded away from zero by an amount we quantify. We then couple this\nresult by showing that, assuming number $N$ of samples grows at least like a\npolynomial function of $d$, all full-rank approximate stationary points of the\nempirical risk are nearly global optimum. These two results allow us to prove\nthat gradient descent, when initialized below the energy barrier, approximately\nminimizes the empirical risk and recovers the planted weights in\npolynomial-time. Next, we show that initializing below this barrier is in fact\neasily achieved when the weights are randomly generated under relatively weak\nassumptions. We show that provided the network is sufficiently\noverparametrized, initializing with an appropriate multiple of the identity\nsuffices to obtain a risk below the energy barrier. At a technical level, the\nlast result is a consequence of the semicircle law for the Wishart ensemble and\ncould be of independent interest. Finally, we study the minimizers of the\nempirical risk and identify a simple necessary and sufficient geometric\ncondition on the training data under which any minimizer has necessarily zero\ngeneralization error. We show that as soon as $N\\ge N^*=d(d+1)/2$, randomly\ngenerated data enjoys this geometric condition almost surely, while that ceases\nto be true if $N<N^*$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:52:37 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:21:23 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 22:02:14 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "1912.01607", "submitter": "Duy Nguyen", "authors": "J. Lars Kirkby, Dang Nguyen, Duy Nguyen", "title": "Moments of Student's t-distribution: A Unified Approach", "comments": "corrected two typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we derive the closed form formulae for moments of Student's\nt-distribution in the one dimensional case as well as in higher dimensions\nthrough a unified probability framework. Interestingly, the closed form\nexpressions for the moments of Student's t-distribution can be written in terms\nof the familiar Gamma function, Kummer's confluent hypergeometric function, and\nthe hypergeometric function.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 13:41:48 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 02:15:22 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kirkby", "J. Lars", ""], ["Nguyen", "Dang", ""], ["Nguyen", "Duy", ""]]}, {"id": "1912.01691", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli and Agnes Desolneux and Alain Durmus and Bruno\n  Galerne and Arthur Leclaire", "title": "Maximum entropy methods for texture synthesis: theory and practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the rise of convolutional neural network techniques in\nexemplar-based image synthesis. These methods often rely on the minimization of\nsome variational formulation on the image space for which the minimizers are\nassumed to be the solutions of the synthesis problem. In this paper we\ninvestigate, both theoretically and experimentally, another framework to deal\nwith this problem using an alternate sampling/minimization scheme. First, we\nuse results from information geometry to assess that our method yields a\nprobability measure which has maximum entropy under some constraints in\nexpectation. Then, we turn to the analysis of our method and we show, using\nrecent results from the Markov chain literature, that its error can be\nexplicitly bounded with constants which depend polynomially in the dimension\neven in the non-convex setting. This includes the case where the constraints\nare defined via a differentiable neural network. Finally, we present an\nextensive experimental study of the model, including a comparison with\nstate-of-the-art methods and an extension to style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:36:44 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Desolneux", "Agnes", ""], ["Durmus", "Alain", ""], ["Galerne", "Bruno", ""], ["Leclaire", "Arthur", ""]]}, {"id": "1912.01698", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Yifang Chen, Krishnakumar Balasubramanian, Prasant\n  Mohapatra", "title": "Online and Bandit Algorithms for Nonstationary Stochastic Saddle-Point\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saddle-point optimization problems are an important class of optimization\nproblems with applications to game theory, multi-agent reinforcement learning\nand machine learning. A majority of the rich literature available for\nsaddle-point optimization has focused on the offline setting. In this paper, we\nstudy nonstationary versions of stochastic, smooth, strongly-convex and\nstrongly-concave saddle-point optimization problem, in both online (or\nfirst-order) and multi-point bandit (or zeroth-order) settings. We first\npropose natural notions of regret for such nonstationary saddle-point\noptimization problems. We then analyze extragradient and Frank-Wolfe\nalgorithms, for the unconstrained and constrained settings respectively, for\nthe above class of nonstationary saddle-point optimization problems. We\nestablish sub-linear regret bounds on the proposed notions of regret in both\nthe online and bandit setting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:52:38 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Roy", "Abhishek", ""], ["Chen", "Yifang", ""], ["Balasubramanian", "Krishnakumar", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1912.02008", "submitter": "Benjamin Aubin", "authors": "Benjamin Aubin, Bruno Loureiro, Antoine Baker, Florent Krzakala and\n  Lenka Zdeborov\\'a", "title": "Exact asymptotics for phase retrieval and compressed sensing with random\n  generative priors", "comments": "13+3 pages, 7 figures, v2 revised and accepted at MSML", "journal-ref": "Proceedings of The First Mathematical and Scientific Machine\n  Learning Conference, PMLR 107:55-73, 2020", "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of compressed sensing and of (real-valued) phase\nretrieval with random measurement matrix. We derive sharp asymptotics for the\ninformation-theoretically optimal performance and for the best known polynomial\nalgorithm for an ensemble of generative priors consisting of fully connected\ndeep neural networks with random weight matrices and arbitrary activations. We\ncompare the performance to sparse separable priors and conclude that generative\npriors might be advantageous in terms of algorithmic performance. In\nparticular, while sparsity does not allow to perform compressive phase\nretrieval efficiently close to its information-theoretic limit, it is found\nthat under the random generative prior compressed phase retrieval becomes\ntractable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:20:34 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 07:40:52 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Aubin", "Benjamin", ""], ["Loureiro", "Bruno", ""], ["Baker", "Antoine", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1912.02051", "submitter": "Lei Yu", "authors": "Lei Yu", "title": "Asymptotics for Strassen's Optimal Transport Problem", "comments": "31 pages, 5 figures. Section IV in the previous version has been\n  removed. Lemma 13, previously wrongly stated, has been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Strassen's version of optimal transport (OT)\nproblem. That is, we minimize the excess-cost probability (i.e., the\nprobability that the cost is larger than a given value) over all couplings of\ntwo given distributions. We derive large deviation, moderate deviation, and\ncentral limit theorems for this problem. Our proof is based on Strassen's dual\nformulation of the OT problem, Sanov's theorem on the large deviation principle\n(LDP) of empirical measures, as well as the moderate deviation principle (MDP)\nand central limit theorems (CLT) of empirical measures. In order to apply the\nLDP, MDP, and CLT to Strassen's OT problem, two nested optimal transport\nformulas for Strassen's OT problem are derived. Based on these nested formulas\nand using a splitting technique, we carefully design asymptotically optimal\nsolutions to Strassen's OT problem and its dual formulation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 15:19:43 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 08:08:45 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 23:05:52 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yu", "Lei", ""]]}, {"id": "1912.02090", "submitter": "HongVan Le", "authors": "H\\^ong V\\^an L\\^e", "title": "Diffeological statistical models, the Fisher metric and probabilistic\n  mappings", "comments": "16 p., final version, accepted to Journal Mathematics/MDPI", "journal-ref": "Mathematics 2020, 8(2), 167", "doi": "10.3390/math8020167", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we introduce the notion of a $C^k$-diffeological statistical\nmodel, which allows us to apply the theory of diffeological spaces to (possibly\nsingular) statistical models. In particular, we introduce a class of almost\n2-integrable $C^k$-diffeological statistical models that encompasses all known\nstatistical models for which the Fisher metric is defined. This class contains\na statistical model which does not appear in the Ay-Jost-L\\^e-Schwachh\\\"ofer\ntheory of parametrized measure models. Then we show that for any positive\ninteger $k$ the class of almost 2-integrable $C^k$-diffeological statistical\nmodels is preserved under probabilistic mappings. Furthermore, the monotonicity\ntheorem for the Fisher metric also holds for this class. As a consequence, the\nFisher metric on an almost 2-integrable $C^k$-diffeological statistical model\n$P \\subset {\\cal P}({\\cal X})$ is preserved under any probabilistic mapping $T:\n{\\cal X}\\leadsto {\\cal Y}$ that is sufficient w.r.t. $P$. Finally we extend the\nCram\\'er-Rao inequality to the class of 2-integrable $C^k$-diffeological\nstatistical models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:36:32 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 09:25:25 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["L\u00ea", "H\u00f4ng V\u00e2n", ""]]}, {"id": "1912.02151", "submitter": "Mingli Chen", "authors": "Alexandre Belloni, Mingli Chen, Oscar Hernan Madrid Padilla, Zixuan\n  (Kevin) Wang", "title": "High Dimensional Latent Panel Quantile Regression with an Application to\n  Asset Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of the linear panel quantile regression model to\naccommodate both \\textit{sparse} and \\textit{dense} parts: sparse means while\nthe number of covariates available is large, potentially only a much smaller\nnumber of them have a nonzero impact on each conditional quantile of the\nresponse variable; while the dense part is represent by a low-rank matrix that\ncan be approximated by latent factors and their loadings. Such a structure\nposes problems for traditional sparse estimators, such as the\n$\\ell_1$-penalised Quantile Regression, and for traditional latent factor\nestimator, such as PCA. We propose a new estimation procedure, based on the\nADMM algorithm, consists of combining the quantile loss function with $\\ell_1$\n\\textit{and} nuclear norm regularization. We show, under general conditions,\nthat our estimator can consistently estimate both the nonzero coefficients of\nthe covariates and the latent low-rank matrix.\n  Our proposed model has a \"Characteristics + Latent Factors\" Asset Pricing\nModel interpretation: we apply our model and estimator with a large-dimensional\npanel of financial data and find that (i) characteristics have sparser\npredictive power once latent factors were controlled (ii) the factors and\ncoefficients at upper and lower quantiles are different from the median.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:03:53 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Belloni", "Alexandre", "", "Kevin"], ["Chen", "Mingli", "", "Kevin"], ["Padilla", "Oscar Hernan Madrid", "", "Kevin"], ["Zixuan", "", "", "Kevin"], ["Wang", "", ""]]}, {"id": "1912.02265", "submitter": "Pratik Misra", "authors": "Pratik Misra, Seth Sullivant", "title": "Gaussian graphical models with toric vanishing ideals", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are semi-algebraic subsets of the cone of positive\ndefinite covariance matrices. They are widely used throughout natural sciences,\ncomputational biology and many other fields. Computing the vanishing ideal of\nthe model gives us an implicit description of the model.\n  In this paper, we resolve two conjectures of Sturmfels and Uhler from\n\\cite{BS n CU}. In particular, we characterize those graphs for which the\nvanishing ideal of the Gaussian graphical model is generated in degree $1$ and\n$2$. These turn out to be the Gaussian graphical models whose ideals are toric\nideals, and the resulting graphs are the $1$-clique sums of complete graphs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:49:59 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 21:06:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Misra", "Pratik", ""], ["Sullivant", "Seth", ""]]}, {"id": "1912.02392", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "KoPA: Automated Kronecker Product Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matrix approximation and denoising induced by the\nKronecker product decomposition. Specifically, we propose to approximate a\ngiven matrix by the sum of a few Kronecker products of matrices, which we refer\nto as the Kronecker product approximation (KoPA). Because the Kronecker product\nis an extension of the outer product from vectors to matrices, KoPA extends the\nlow rank matrix approximation, and includes it as a special case. Comparing\nwith the latter, KoPA also offers a greater flexibility, since it allows the\nuser to choose the configuration, which are the dimensions of the two smaller\nmatrices forming the Kronecker product. On the other hand, the configuration to\nbe used is usually unknown, and needs to be determined from the data in order\nto achieve the optimal balance between accuracy and parsimony. We propose to\nuse extended information criteria to select the configuration. Under the\nparadigm of high dimensional analysis, we show that the proposed procedure is\nable to select the true configuration with probability tending to one, under\nsuitable conditions on the signal-to-noise ratio. We demonstrate the\nsuperiority of KoPA over the low rank approximations through numerical studies,\nand several benchmark image examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:27:01 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:52:55 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 20:30:09 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1912.02517", "submitter": "Yasin Asar", "authors": "Yasin Asar and R. Arabi Belaghi", "title": "Inference for Two Lomax Populations Under Joint Type-II Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lomax distribution has been widely used in economics, business and actuarial\nsciences. Due to its importance, we consider the statistical inference of this\nmodel under joint type-II censoring scenario. In order to estimate the\nparameters, we derive the Newton-Raphson(NR) procedure and we observe that most\nof the times in the simulation NR algorithm does not converge. Consequently, we\nmake use of the expectation-maximization (EM) algorithm. Moreover, Bayesian\nestimations are also provided based on squared error, linear-exponential and\ngeneralized entropy loss functions together with the importance sampling method\ndue to the structure of posterior density function. In the sequel, we perform a\nMonte Carlo simulation experiment to compare the performances of the listed\nmethods. Mean squared error values, averages of estimated values as well as\ncoverage probabilities and average interval lengths are considered to compare\nthe performances of different methods. The approximate confidence intervals,\nbootstrap-p and bootstrap-t confidence intervals are computed for EM\nestimations. Also, Bayesian coverage probabilities and credible intervals are\nobtained. Finally, we consider the Bladder Cancer data to illustrate the\napplicability of the methods covered in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 11:49:54 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Asar", "Yasin", ""], ["Belaghi", "R. Arabi", ""]]}, {"id": "1912.02595", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya and Jan Beirlant", "title": "Outlier detection and a tail-adjusted boxplot based on extreme value\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether an extreme observation is an outlier or not, depends strongly on the\ncorresponding tail behaviour of the underlying distribution. We develop an\nautomatic, data-driven method to identify extreme tail behaviour that deviates\nfrom the intermediate and central characteristics. This allows for detecting\nextreme outliers or sets of extreme data that show less spread than the bulk of\nthe data. To this end we extend a testing method proposed in Bhattacharya et al\n2019 for the specific case of heavy tailed models, to all max-domains of\nattraction. Consequently we propose a tail-adjusted boxplot which yields a more\naccurate representation of possible outliers. Several examples and simulation\nresults illustrate the finite sample behaviour of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:33:40 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Beirlant", "Jan", ""]]}, {"id": "1912.02628", "submitter": "Song Fang", "authors": "Song Fang, Quanyan Zhu", "title": "Fundamental Limitations in Sequential Prediction and Recursive\n  Algorithms: $\\mathcal{L}_{p}$ Bounds via an Entropic Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.06742.\n  text overlap with arXiv:1912.05541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain fundamental $\\mathcal{L}_{p}$ bounds in sequential\nprediction and recursive algorithms via an entropic analysis. Both classes of\nproblems are examined by investigating the underlying entropic relationships of\nthe data and/or noises involved, and the derived lower bounds may all be\nquantified in a conditional entropy characterization. We also study the\nconditions to achieve the generic bounds from an innovations' viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:52:15 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:56:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1912.02724", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Kailash Budhathoki, Lenon Minorics, and Patrick\n  Bl\\\"obaum", "title": "Causal structure based root cause analysis of outliers", "comments": "11 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a formal approach to identify 'root causes' of outliers observed\nin $n$ variables $X_1,\\dots,X_n$ in a scenario where the causal relation\nbetween the variables is a known directed acyclic graph (DAG). To this end, we\nfirst introduce a systematic way to define outlier scores. Further, we\nintroduce the concept of 'conditional outlier score' which measures whether a\nvalue of some variable is unexpected *given the value of its parents* in the\nDAG, if one were to assume that the causal structure and the corresponding\nconditional distributions are also valid for the anomaly. Finally, we quantify\nto what extent the high outlier score of some target variable can be attributed\nto outliers of its ancestors. This quantification is defined via Shapley values\nfrom cooperative game theory.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:00:20 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Janzing", "Dominik", ""], ["Budhathoki", "Kailash", ""], ["Minorics", "Lenon", ""], ["Bl\u00f6baum", "Patrick", ""]]}, {"id": "1912.02765", "submitter": "Ishaq Aden-Ali", "authors": "Ishaq Aden-Ali, Hassan Ashtiani", "title": "On the Sample Complexity of Learning Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPNs) can be regarded as a form of deep graphical\nmodels that compactly represent deeply factored and mixed distributions. An SPN\nis a rooted directed acyclic graph (DAG) consisting of a set of leaves\n(corresponding to base distributions), a set of sum nodes (which represent\nmixtures of their children distributions) and a set of product nodes\n(representing the products of its children distributions).\n  In this work, we initiate the study of the sample complexity of PAC-learning\nthe set of distributions that correspond to SPNs. We show that the sample\ncomplexity of learning tree structured SPNs with the usual type of leaves\n(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)\nwith the number of parameters of the SPN. More specifically, we show that the\nclass of distributions that corresponds to tree structured Gaussian SPNs with\n$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned\nwithin Total Variation error $\\epsilon$ using at most\n$\\widetilde{O}(\\frac{ed^2+k}{\\epsilon^2})$ samples. A similar result holds for\ntree structured SPNs with discrete leaves.\n  We obtain the upper bounds based on the recently proposed notion of\ndistribution compression schemes. More specifically, we show that if a (base)\nclass of distributions $\\mathcal{F}$ admits an \"efficient\" compression, then\nthe class of tree structured SPNs with leaves from $\\mathcal{F}$ also admits an\nefficient compression.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 17:57:58 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:18:11 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Aden-Ali", "Ishaq", ""], ["Ashtiani", "Hassan", ""]]}, {"id": "1912.02819", "submitter": "Zhiqiang Hou", "authors": "Dandan Jiang, Jiang Hu, Zhiqiang Hou", "title": "The limits of the sample spiked eigenvalues for a high-dimensional\n  generalized Fisher matrix and its applications", "comments": "21 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized spiked Fisher matrix is considered in this paper. We establish\na criterion for the description of the support of the limiting spectral\ndistribution of high-dimensional generalized Fisher matrix and study the almost\nsure limits of the sample spiked eigenvalues where the population covariance\nmatrices are arbitrary which successively removed an unrealistic condition\nposed in the previous works, that is, the covariance matrices are assumed to be\ndiagonal or diagonal block-wise structure. In addition, we also give a\nconsistent estimator of the population spiked eigenvalues. A series of\nsimulations are conducted that support the theoretical results and illustrate\nthe accuracy of our estimators.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 01:40:22 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Jiang", "Dandan", ""], ["Hu", "Jiang", ""], ["Hou", "Zhiqiang", ""]]}, {"id": "1912.02879", "submitter": "William Leeb", "authors": "William Leeb", "title": "A note on identifiability conditions in confirmatory factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chen, Li and Zhang established conditions characterizing asymptotic\nidentifiability of latent factors in confirmatory factor analysis. We give an\nelementary proof showing that a similar characterization holds\nnon-asymptotically, and prove a related result for identifiability of factor\nloadings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:21:19 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 03:00:32 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "1912.02891", "submitter": "Liron Ravner", "authors": "Michel Mandjes and Liron Ravner", "title": "Hypothesis testing for a L\\'evy-driven storage system by Poisson\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on hypothesis testing for the input of a L\\'evy-driven\nstorage system by sampling of the storage level. As the likelihood is not\nexplicit we propose two tests that rely on transformation of the data. The\nfirst approach uses i.i.d. `quasi-busy-periods' between observations of zero\nworkload. The distribution of the duration of quasi-busy-periods is determined.\nThe second method is a conditional likelihood ratio test based on the Bernoulli\nevents of observing a zero or positive workload, conditional on the previous\nworkload. Performance analysis is presented for both tests along with\nspeed-of-convergence results, that are of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:38:22 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 10:10:26 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mandjes", "Michel", ""], ["Ravner", "Liron", ""]]}, {"id": "1912.03108", "submitter": "Bojana Rosic", "authors": "M.S. Sarfaraz, B. Rosic, H.G. Matthies, A. Ibrahimbegovic", "title": "Bayesian stochastic multi-scale analysis via energy considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper physical multi-scale processes governed by their own principles\nfor evolution or equilibrium on each scale are coupled by matching the stored\nand dissipated energy, in line with the Hill-Mandel principle. In our view the\ncorrect representations of stored and dissipated energy is essential to the\nrepresentation irreversible material behaviour, and this matching is also used\nfor upscaling. The small scales, here the meso-scale, is assumed to be\ndescribed probabilistically, and so on the macroscale also a probabilistic\nmodel is identified in a Bayesian setting, reflecting the randomness of the\nmeso-scale, the loss of resolution due to upscaling, and the uncertainty\ninvolved in the Bayesian process. In this way multi-scale processes become\nhierarchical systems in which the information is transferred across the scales\nby Bayesian identification on coarser levels. The quantities to be matched on\nthe coarse-scale model are the stored and dissipated energies. In this way\nprobability distributions of macro-scale material parameters are determined,\nand not only in the elastic region, but also for the irreversible and highly\nnonlinear elasto-damage regimes, refelcting the aleatory uncetainty at the\nmeso-scale level. For this purpose high dimensional meso-scale stochastic\nsimulations in a non-intrusive functional approximation forms are mapped to the\nmacro-scale models in an approximative manner by employing a generalised\nversion of the Kalman filter. To reduce the overall computational cost, a model\nreduction of the meso-scale simulation is achieved by combining the\nunsupervised learning techniques based on the Bayesian copula variartional\ninference with the classical functional approximation forms from the field of\nuncertainty quantification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:39:41 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Sarfaraz", "M. S.", ""], ["Rosic", "B.", ""], ["Matthies", "H. G.", ""], ["Ibrahimbegovic", "A.", ""]]}, {"id": "1912.03109", "submitter": "Etienne Roquain", "authors": "Etienne Roquain and Nicolas Verzelen", "title": "False discovery rate control with unknown null distribution: is it\n  possible to mimic the oracle?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multiple testing theory prescribes the null distribution, which is\noften a too stringent assumption for nowadays large scale experiments. This\npaper presents theoretical foundations to understand the limitations caused by\nignoring the null distribution, and how it can be properly learned from the\n(same) data-set, when possible. We explore this issue in the case where the\nnull distributions are Gaussian with an unknown rescaling parameters (mean and\nvariance) and the alternative distribution is let arbitrary. While an oracle\nprocedure in that case is the Benjamini Hochberg procedure applied with the\ntrue (unknown) null distribution, we pursue the aim of building a procedure\nthat asymptotically mimics the performance of the oracle (AMO in short). Our\nmain result states that an AMO procedure exists if and only if the sparsity\nparameter $k$ (number of false nulls) is of order less than $n/\\log(n)$, where\n$n$ is the total number of tests. Further sparsity boundaries are derived for\ngeneral location models where the shape of the null distribution is not\nnecessarily Gaussian. Given our impossibility results, we also pursue a weaker\nobjective, which is to find a confidence region for the oracle. To this end, we\ndevelop a distribution-dependent confidence region for the null distribution.\nAs practical by-products, this provides a goodness of fit test for the null\ndistribution, as well as a visual method assessing the reliability of empirical\nnull multiple testing methods. Our results are illustrated with numerical\nexperiments and a companion vignette \\cite{RVvignette2020}.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:40:00 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:21:14 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 18:07:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Roquain", "Etienne", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1912.03155", "submitter": "Clement Dombry", "authors": "Benjamin Bobbia (LMB), Cl\\'ement Dombry (LMB), Davit Varron (LMB)", "title": "The coupling method in extreme value theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coupling method is developed for univariate extreme value theory ,\nproviding an alternative to the use of the tail empirical/quantile processes.\nEmphasizing the Peak-over-Threshold approach that approximates the distribution\nabove high threshold by the Generalized Pareto distribution, we compare the\nempirical distribution of exceedances and the empirical distribution associated\nto the limit Generalized Pareto model and provide sharp bounds for their\nWasser-stein distance in the second order Wasserstein space. As an application\n, we recover standard results on the asymptotic behavior of the Hill estimator,\nthe Weissman extreme quantile estimator or the probability weighted moment\nestimators, shedding some new light on the theory.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 14:39:09 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bobbia", "Benjamin", "", "LMB"], ["Dombry", "Cl\u00e9ment", "", "LMB"], ["Varron", "Davit", "", "LMB"]]}, {"id": "1912.03306", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj and Markus Pauly", "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random\n  Forest Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in sparse regression models is an important task as\napplications ranging from biomedical research to econometrics have shown.\nEspecially for higher dimensional regression problems, for which the link\nfunction between response and covariates cannot be directly detected, the\nselection of informative variables is challenging. Under these circumstances,\nthe Random Forest method is a helpful tool to predict new outcomes while\ndelivering measures for variable selection. One common approach is the usage of\nthe permutation importance. Due to its intuitive idea and flexible usage, it is\nimportant to explore circumstances, for which the permutation importance based\non Random Forest correctly indicates informative covariates. Regarding the\nlatter, we deliver theoretical guarantees for the validity of the permutation\nimportance measure under specific assumptions and prove its (asymptotic)\nunbiasedness. An extensive simulation study verifies our findings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:11:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Ramosaj", "Burim", ""], ["Pauly", "Markus", ""]]}, {"id": "1912.03358", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir, Julio Isidro Sanchez", "title": "Adventures in Multi-Omics I: Combining heterogeneous data sets via\n  relationships matrices", "comments": "This project was supported by WheatSustain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.GN stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a covariance based method for combining partial\ndata sets in the genotype to phenotype spectrum. In particular, an\nexpectation-maximization algorithm that can be used to combine partially\noverlapping relationship/covariance matrices is introduced. Combining data this\nway, based on relationship matrices, can be contrasted with a feature\nimputation based approach. We used several public genomic data sets to explore\nthe accuracy of combining genomic relationship matrices. We have also used the\nheterogeneous genotype/phenotype data sets in the https://triticeaetoolbox.org/\nto illustrate how this new method can be used in genomic prediction, phenomics,\nand graphical modeling.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:39:58 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 15:55:39 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Akdemir", "Deniz", ""], ["Sanchez", "Julio Isidro", ""]]}, {"id": "1912.03387", "submitter": "Octavio Mesner", "authors": "Octavio C\\'esar Mesner and Cosma Rohilla Shalizi", "title": "Conditional Mutual Information Estimation for Mixed Discrete and\n  Continuous Variables with Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fields like public health, public policy, and social science often want to\nquantify the degree of dependence between variables whose relationships take on\nunknown functional forms. Typically, in fact, researchers in these fields are\nattempting to evaluate causal theories, and so want to quantify dependence\nafter conditioning on other variables that might explain, mediate or confound\ncausal relations. One reason conditional mutual information is not more widely\nused for these tasks is the lack of estimators which can handle combinations of\ncontinuous and discrete random variables, common in applications. This paper\ndevelops a new method for estimating mutual and conditional mutual information\nfor data samples containing a mix of discrete and continuous variables. We\nprove that this estimator is consistent and show, via simulation, that it is\nmore accurate than similar estimators.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 23:28:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mesner", "Octavio C\u00e9sar", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1912.03528", "submitter": "Ervin T\\'anczos", "authors": "Robert Nowak, Ervin T\\'anczos", "title": "Tighter Confidence Intervals for Rating Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating systems are ubiquitous, with applications ranging from product\nrecommendation to teaching evaluations. Confidence intervals for functionals of\nrating data such as empirical means or quantiles are critical to\ndecision-making in various applications including recommendation/ranking\nalgorithms. Confidence intervals derived from standard Hoeffding and Bernstein\nbounds can be quite loose, especially in small sample regimes, since these\nbounds do not exploit the geometric structure of the probability simplex. We\npropose a new approach to deriving confidence intervals that are tailored to\nthe geometry associated with multi-star/value rating systems using a\ncombination of techniques from information theory, including Kullback-Leibler,\nSanov, and Csisz{\\'a}r inequalities.\n  The new confidence intervals are almost always as good or better than all\nstandard methods and are significantly tighter in many situations. The standard\nbounds can require several times more samples than our new bounds to achieve\nspecified confidence interval widths.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 16:49:15 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Nowak", "Robert", ""], ["T\u00e1nczos", "Ervin", ""]]}, {"id": "1912.03636", "submitter": "Li-Xin Zhang", "authors": "Li-Xin Zhang", "title": "Theory on Covariate-Adaptive Randomized Clinical Trials: Efficiency,\n  Selection bias and Randomization Methods", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theocratical properties of the power of the conventional testing\nhypotheses and the selection bias are usually unknown under covariate-adaptive\nrandomized clinical trials. In the literature, most studies are based on\nsimulations. In this article, we provide theoretical foundation of the power of\nthe hypothesis testing and the selection bias under covariate-adaptive\nrandomization based on linear models. We study the asymptotic relative loss of\npower of hypothesis testing to compare the treatment effects and the asymptotic\nselection bias. Under the covariate-adaptive randomization, (i) the hypothesis\ntesting usually losses power, the more covariates in testing model are not\nincorporated in the randomization procedure, the more the power is lost; (ii)\nthe hypothesis testing is usually more powerful than the one under complete\nrandomization; and (iii) comparing to complete randomization, most of the\npopular covariate-adaptive randomization procedures in the literature, for\nexample, Pocock and Simon's marginal procedure, stratified permuted block\ndesign, etc, produce nontrivial selection bias. A new family of\ncovariate-adaptive randomization procedures are proposed for considering the\npower and selection bias simultaneously, under which, the covariate imbalances\nare small enough so that the power of testing the treatment effects would be\nasymptotically the largest and at the same time, the selection bias is\nasymptotically the optimal. The theocratical properties give a full picture how\nthe power of the hypothesis testing, the selection bias of the randomization\nprocedure, and the randomization method affect each other.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 08:05:09 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 03:23:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Li-Xin", ""]]}, {"id": "1912.03662", "submitter": "Duyeol Lee", "authors": "Duyeol Lee, Kai Zhang, and Michael R. Kosorok", "title": "The Binary Expansion Randomized Ensemble Test (BERET)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the binary expansion testing framework was introduced to test the\nindependence of two continuous random variables by utilizing symmetry\nstatistics that are complete sufficient statistics for dependence. We develop a\nnew test based on an ensemble approach that uses the sum of squared symmetry\nstatistics and distance correlation. Simulation studies suggest that this\nmethod improves the power while preserving the clear interpretation of the\nbinary expansion testing. We extend this method to tests of independence of\nrandom vectors in arbitrary dimension. Through random projections, the proposed\nbinary expansion randomized ensemble test transforms the multivariate\nindependence testing problem into a univariate problem. Simulation studies and\ndata example analyses show that the proposed method provides relatively robust\nperformance compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:54:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:25:20 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 23:37:36 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 20:08:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Duyeol", ""], ["Zhang", "Kai", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.03800", "submitter": "Anirudh Sridhar", "authors": "Anirudh Sridhar and H. Vincent Poor", "title": "Sequential Estimation of Network Cascades", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.SI eess.SP math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of locating the source of a network cascade, given a\nnoisy time-series of network data. Initially, the cascade starts with one\nunknown, affected vertex and spreads deterministically at each time step. The\ngoal is to find an adaptive procedure that outputs an estimate for the source\nas fast as possible, subject to a bound on the estimation error. For a general\nclass of graphs, we describe a family of matrix sequential probability ratio\ntests (MSPRTs) that are first-order asymptotically optimal up to a constant\nfactor as the estimation error tends to zero. We apply our results to lattices\nand regular trees, and show that MSPRTs are asymptotically optimal for regular\ntrees. We support our theoretical results with simulations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 00:58:00 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 16:25:22 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 19:04:00 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 16:12:45 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Sridhar", "Anirudh", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1912.03807", "submitter": "Ryan Martin", "authors": "Chang Liu and Ryan Martin", "title": "An empirical $G$-Wishart prior for sparse high-dimensional Gaussian\n  graphical models", "comments": "36 pages, 4 tables, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Gaussian graphical models, the zero entries in the precision matrix\ndetermine the dependence structure, so estimating that sparse precision matrix\nand, thereby, learning this underlying structure, is an important and\nchallenging problem. We propose an empirical version of the $G$-Wishart prior\nfor sparse precision matrices, where the prior mode is informed by the data in\na suitable way. Paired with a prior on the graph structure, a marginal\nposterior distribution for the same is obtained that takes the form of a ratio\nof two $G$-Wishart normalizing constants. We show that this ratio can be easily\nand accurately computed using a Laplace approximation, which leads to fast and\nefficient posterior sampling even in high-dimensions. Numerical results\ndemonstrate the proposed method's superior performance, in terms of speed and\naccuracy, across a variety of settings, and theoretical support is provided in\nthe form of a posterior concentration rate theorem.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:22:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Chang", ""], ["Martin", "Ryan", ""]]}, {"id": "1912.03903", "submitter": "Koki Shimizu", "authors": "Koki Shimizu and Hiroki Hashiguchi", "title": "Heterogeneous hypergeometric functions with two matrix arguments and the\n  exact distribution of the largest eigenvalue of a singular beta-Wishart\n  matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses certain properties of heterogeneous hypergeometric\nfunctions with two matrix arguments. These functions are newly defined but have\nalready appeared in statistical literature and are useful when dealing with the\nderivation of certain distributions for the eigenvalues of singular\nbeta-Wishart matrices. The joint density function of the eigenvalues and the\ndistribution of the largest eigenvalue can be expressed in terms of certain\nheterogeneous hypergeometric functions. Exact computation of the distribution\nof the largest eigenvalue is conducted here for a real case.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 08:49:00 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 03:21:58 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 10:34:40 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Shimizu", "Koki", ""], ["Hashiguchi", "Hiroki", ""]]}, {"id": "1912.03921", "submitter": "Michael Kohler", "authors": "Alina Braun and Michael Kohler and Harro Walk", "title": "On the rate of convergence of a neural network regression estimate\n  learned by gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression with random design is considered. Estimates are\ndefined by minimzing a penalized empirical $L_2$ risk over a suitably chosen\nclass of neural networks with one hidden layer via gradient descent. Here, the\ngradient descent procedure is repeated several times with randomly chosen\nstarting values for the weights, and from the list of constructed estimates the\none with the minimal empirical $L_2$ risk is chosen. Under the assumption that\nthe number of randomly chosen starting values and the number of steps for\ngradient descent are sufficiently large it is shown that the resulting estimate\nachieves (up to a logarithmic factor) the optimal rate of convergence in a\nprojection pursuit model. The final sample size performance of the estimates is\nillustrated by using simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:36:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Braun", "Alina", ""], ["Kohler", "Michael", ""], ["Walk", "Harro", ""]]}, {"id": "1912.03925", "submitter": "Michael Kohler", "authors": "Michael Kohler and Adam Krzyzak", "title": "Over-parametrized deep neural networks do not generalize well", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was shown in several papers that backpropagation is able to find\nthe global minimum of the empirical risk on the training data using\nover-parametrized deep neural networks. In this paper a similar result is shown\nfor deep neural networks with the sigmoidal squasher activation function in a\nregression setting, and a lower bound is presented which proves that these\nnetworks do not generalize well on a new data in the sense that they do not\nachieve the optimal minimax rate of convergence for estimation of smooth\nregression functions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:48:01 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 14:13:01 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kohler", "Michael", ""], ["Krzyzak", "Adam", ""]]}, {"id": "1912.04011", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen", "title": "A new inequality for maximum likelihood estimation in statistical models\n  with latent variables", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum-likelihood estimation (MLE) is arguably the most important tool for\nstatisticians, and many methods have been developed to find the MLE. We present\na new inequality involving posterior distributions of a latent variable that\nholds under very general conditions. It is related to the EM algorithm and has\na clear potential for being used in a similar fashion.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:46:13 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Olsen", "Niels Lundtorp", ""]]}, {"id": "1912.04089", "submitter": "Jakob Peterlin Institute for", "authors": "Rok Blagus, Jakob Peterlin, Nata\\v{s}a Kej\\v{z}ar", "title": "Goodness-of-fit tests for functional form of Linear Mixed effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed effects models (LMMs) are a popular and powerful tool for\nanalyzing clustered or repeated observations for numeric outcomes. LMMs consist\nof a fixed and a random component, specified in the model through their\nrespective design matrices. Checking if the two design matrices are correctly\nspecified is crucial since mis-specifying them can affect the validity and\nefficiency of the analysis. We show how to use random processes defined as\ncumulative sums of appropriately ordered model's residuals to test if the\nfunctional form of the fitted LMM is correctly specified. We show how these\nprocesses can be used to test goodness-of-fit of the functional form of the\nentire model, or only its fixed and/or random component. Inspecting plots of\nthe proposed processes is shown to be highly informative about the potential\nmis-specification of the functional form of the model, providing clues for\npotential improvement of the model's fit. We show how the visual inspection can\nbe objectified by using a novel procedure for estimating $p$-values which can\nbe based on sign-flipping/bootstrap or simulations and show its validity by\nusing theoretical results and a large Monte Carlo simulation study. The\nproposed methodology can be used with LMMs with multi-level or crossed random\neffects.% and could also be extended to generalized LMMs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 14:45:24 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Blagus", "Rok", ""], ["Peterlin", "Jakob", ""], ["Kej\u017ear", "Nata\u0161a", ""]]}, {"id": "1912.04151", "submitter": "Forrest Crawford", "authors": "Xiaoxuan Cai, Wen Wei Loh, Forrest W. Crawford", "title": "Identification of causal intervention effects under contagion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining and identifying causal intervention effects for transmissible\ninfectious disease outcomes is challenging because a treatment -- such as a\nvaccine -- given to one individual may affect the infection outcomes of others.\nEpidemiologists have proposed causal estimands to quantify effects of\ninterventions under contagion using a two-person partnership model. These\nsimple conceptual models have helped researchers develop causal estimands\nrelevant to clinical evaluation of vaccine effects. However, many of these\npartnership models are formulated under structural assumptions that preclude\nrealistic infectious disease transmission dynamics, limiting their conceptual\nusefulness in defining and identifying causal treatment effects in empirical\nintervention trials. In this paper, we propose causal intervention effects in\ntwo-person partnerships under arbitrary infectious disease transmission\ndynamics, and give nonparametric identification results showing how effects can\nbe estimated in empirical trials using time-to-infection or binary outcome\ndata. The key insight is that contagion is a causal phenomenon that induces\nconditional independencies on infection outcomes that can be exploited for the\nidentification of clinically meaningful causal estimands. These new estimands\nare compared to existing quantities, and results are illustrated using a\nrealistic simulation of an HIV vaccine trial.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:22:05 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:49:28 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Cai", "Xiaoxuan", ""], ["Loh", "Wen Wei", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1912.04533", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski, Feynman Liang and Michael W. Mahoney", "title": "Exact expressions for double descent and implicit regularization via\n  surrogate random design", "comments": "Minor typo corrections and clarifications; moved the proofs into the\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double descent refers to the phase transition that is exhibited by the\ngeneralization error of unregularized learning models when varying the ratio\nbetween the number of parameters and the number of training samples. The recent\nsuccess of highly over-parameterized machine learning models such as deep\nneural networks has motivated a theoretical analysis of the double descent\nphenomenon in classical models such as linear regression which can also\ngeneralize well in the over-parameterized regime. We provide the first exact\nnon-asymptotic expressions for double descent of the minimum norm linear\nestimator. Our approach involves constructing a special determinantal point\nprocess which we call surrogate random design, to replace the standard i.i.d.\ndesign of the training sample. This surrogate design admits exact expressions\nfor the mean squared error of the estimator while preserving the key properties\nof the standard design. We also establish an exact implicit regularization\nresult for over-parameterized training samples. In particular, we show that,\nfor the surrogate design, the implicit bias of the unregularized minimum norm\nestimator precisely corresponds to solving a ridge-regularized least squares\nproblem on the population distribution. In our analysis we introduce a new\nmathematical tool of independent interest: the class of random matrices for\nwhich determinant commutes with expectation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 06:49:46 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 00:36:41 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 17:03:42 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Liang", "Feynman", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1912.04629", "submitter": "Thomas Berrett", "authors": "Thomas Berrett and Cristina Butucea", "title": "Classification under local differential privacy", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the binary classification problem in a setup that preserves the\nprivacy of the original sample. We provide a privacy mechanism that is locally\ndifferentially private and then construct a classifier based on the private\nsample that is universally consistent in Euclidean spaces. Under stronger\nassumptions, we establish the minimax rates of convergence of the excess risk\nand see that they are slower than in the case when the original sample is\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:37:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Berrett", "Thomas", ""], ["Butucea", "Cristina", ""]]}, {"id": "1912.04677", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Testing and Estimating Change-Points in the Covariance Matrix of a\n  High-Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies methods for testing and estimating change-points in the\ncovariance structure of a high-dimensional linear time series. The assumed\nframework allows for a large class of multivariate linear processes (including\nvector autoregressive moving average (VARMA) models) of growing dimension and\nspiked covariance models. The approach uses bilinear forms of the centered or\nnon-centered sample variance-covariance matrix. Change-point testing and\nestimation are based on maximally selected weighted cumulated sum (CUSUM)\nstatistics. Large sample approximations under a change-point regime are\nprovided including a multivariate CUSUM transform of increasing dimension. For\nthe unknown asymptotic variance and covariance parameters associated to (pairs\nof) CUSUM statistics we propose consistent estimators. Based on weak laws of\nlarge numbers for their sequential versions, we also consider stopped sample\nestimation where observations until the estimated change-point are used. Finite\nsample properties of the procedures are investigated by simulations and their\napplication is illustrated by analyzing a real data set from environmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 13:28:05 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 13:18:21 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1912.04738", "submitter": "Hanyuan Hang", "authors": "Hanyuan Hang, Zhouchen Lin, Xiaoyu Liu, Hongwei Wen", "title": "Histogram Transform Ensembles for Large-scale Regression", "comments": "arXiv admin note: text overlap with arXiv:1911.11581", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for large-scale regression problems named\nhistogram transform ensembles (HTE), composed of random rotations, stretchings,\nand translations. First of all, we investigate the theoretical properties of\nHTE when the regression function lies in the H\\\"{o}lder space $C^{k,\\alpha}$,\n$k \\in \\mathbb{N}_0$, $\\alpha \\in (0,1]$. In the case that $k=0, 1$, we adopt\nthe constant regressors and develop the na\\\"{i}ve histogram transforms (NHT).\nWithin the space $C^{0,\\alpha}$, although almost optimal convergence rates can\nbe derived for both single and ensemble NHT, we fail to show the benefits of\nensembles over single estimators theoretically. In contrast, in the subspace\n$C^{1,\\alpha}$, we prove that if $d \\geq 2(1+\\alpha)/\\alpha$, the lower bound\nof the convergence rates for single NHT turns out to be worse than the upper\nbound of the convergence rates for ensemble NHT. In the other case when $k \\geq\n2$, the NHT may no longer be appropriate in predicting smoother regression\nfunctions. Instead, we apply kernel histogram transforms (KHT) equipped with\nsmoother regressors such as support vector machines (SVMs), and it turns out\nthat both single and ensemble KHT enjoy almost optimal convergence rates. Then\nwe validate the above theoretical results by numerical experiments. On the one\nhand, simulations are conducted to elucidate that ensemble NHT outperform\nsingle NHT. On the other hand, the effects of bin sizes on accuracy of both NHT\nand KHT also accord with theoretical analysis. Last but not least, in the\nreal-data experiments, comparisons between the ensemble KHT, equipped with\nadaptive histogram transforms, and other state-of-the-art large-scale\nregression estimators verify the effectiveness and accuracy of our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 16:39:02 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hang", "Hanyuan", ""], ["Lin", "Zhouchen", ""], ["Liu", "Xiaoyu", ""], ["Wen", "Hongwei", ""]]}, {"id": "1912.04858", "submitter": "Sara Mazzonetto", "authors": "Sara Mazzonetto", "title": "Rates of convergence to the local time of Oscillating and Skew Brownian\n  Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a class of statistics based on high frequency observations of\noscillating Brownian motions and skew Brownian motions is considered. Their\nconvergence rate towards the local time of the underling process is obtained in\nform of a Central Limit Theorem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:55:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mazzonetto", "Sara", ""]]}, {"id": "1912.04869", "submitter": "Franz Besold", "authors": "Franz Besold, Vladimir Spokoiny", "title": "Adaptive Manifold Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods seek to partition data such that elements are more similar\nto elements in the same cluster than to elements in different clusters. The\nmain challenge in this task is the lack of a unified definition of a cluster,\nespecially for high dimensional data. Different methods and approaches have\nbeen proposed to address this problem. This paper continues the study\noriginated by Efimov, Adamyan and Spokoiny (2019) where a novel approach to\nadaptive nonparametric clustering called Adaptive Weights Clustering (AWC) was\noffered. The method allows analyzing high-dimensional data with an unknown\nnumber of unbalanced clusters of arbitrary shape under very weak modeling\nassumptions. The procedure demonstrates a state-of-the-art performance and is\nvery efficient even for large data dimension D. However, the theoretical study\nin Efimov, Adamyan and Spokoiny (2019) is very limited and did not really\naddress the question of efficiency. This paper makes a significant step in\nunderstanding the remarkable performance of the AWC procedure, particularly in\nhigh dimension. The approach is based on combining the ideas of adaptive\nclustering and manifold learning. The manifold hypothesis means that high\ndimensional data can be well approximated by a d-dimensional manifold for small\nd helping to overcome the curse of dimensionality problem and to get sharp\nbounds on the cluster separation which only depend on the intrinsic dimension\nd. We also address the problem of parameter tuning. Our general theoretical\nresults are illustrated by some numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 18:24:36 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 17:52:27 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 12:13:21 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Besold", "Franz", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1912.04946", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch", "title": "Frequentist Consistency of Generalized Variational Inference", "comments": "31 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Frequentist consistency properties of the posterior\ndistributions constructed via Generalized Variational Inference (GVI). A number\nof generic and novel strategies are given for proving consistency, relying on\nthe theory of $\\Gamma$-convergence. Specifically, this paper shows that under\nminimal regularity conditions, the sequence of GVI posteriors is consistent and\ncollapses to a point mass at the population-optimal parameter value as the\nnumber of observations goes to infinity. The results extend to the latent\nvariable case without additional assumptions and hold under misspecification.\nLastly, the paper explains how to apply the results to a selection of GVI\nposteriors with especially popular variational families. For example,\nconsistency is established for GVI methods using the mean field normal\nvariational family, normal mixtures, Gaussian process variational families as\nwell as neural networks indexing a normal (mixture) distribution.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:38:35 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Knoblauch", "Jeremias", ""]]}, {"id": "1912.05306", "submitter": "Andrew Sills", "authors": "Andrew V. Sills", "title": "Integer Partitions Probability Distributions", "comments": "9 pages; this is the revised version that has been accepted for\n  publication in Communications in Statistics: Theory and Methods", "journal-ref": "Communications in Statistics - Theory and Methods 50 (2021) 3556 -\n  3563", "doi": "10.1080/03610926.2019.1708396", "report-no": null, "categories": "math.CO math.NT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two closely related discrete probability distributions are introduced. In\neach case the support is a set of vectors in $\\mathbb{R}^n$ obtained from the\npartitions of the fixed positive integer $n$. These distributions arise\nnaturally when considering equally-likely random permutations on the set of $n$\nletters. For one of the distributions, the expectation vector and covariance\nmatrix is derived. For the other distribution, conjectures for several elements\nof the expectation vector are provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 13:48:46 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 17:56:20 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Sills", "Andrew V.", ""]]}, {"id": "1912.05436", "submitter": "Michael Kohler", "authors": "Alina Braun and Michael Kohler and Adam Krzyzak", "title": "Analysis of the rate of convergence of neural network regression\n  estimates which are easy to implement", "comments": "arXiv admin note: text overlap with arXiv:1912.03921", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in nonparametric regression show that for deep learning, i.e.,\nfor neural network estimates with many hidden layers, we are able to achieve\ngood rates of convergence even in case of high-dimensional predictor variables,\nprovided suitable assumptions on the structure of the regression function are\nimposed. The estimates are defined by minimizing the empirical $L_2$ risk over\na class of neural networks. In practice it is not clear how this can be done\nexactly. In this article we introduce a new neural network regression estimate\nwhere most of the weights are chosen regardless of the data motivated by some\nrecent approximation results for neural networks, and which is therefore easy\nto implement. We show that for this estimate we can derive rates of convergence\nresults in case the regression function is smooth. We combine this estimate\nwith the projection pursuit, where we choose the directions randomly, and we\nshow that for sufficiently many repititions we get a neural network regression\nestimate which is easy to implement and which achieves the one-dimensional rate\nof convergence (up to some logarithmic factor) in case that the regression\nfunction satisfies the assumptions of projection pursuit.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 09:44:11 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Braun", "Alina", ""], ["Kohler", "Michael", ""], ["Krzyzak", "Adam", ""]]}, {"id": "1912.05473", "submitter": "Haoyu Wang", "authors": "Haoyu Wang", "title": "Quantitative Universality for the Largest Eigenvalue of Sample\n  Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the first explicit rate of convergence to the Tracy-Widom\ndistribution for the fluctuation of the largest eigenvalue of sample covariance\nmatrices that are not integrable. Our primary focus is matrices of type $ X^*X\n$ and the proof follows the Erd\\\"{o}s-Schlein-Yau dynamical method. We use a\nrecent approach to the analysis of the Dyson Brownian motion from [5] to obtain\na quantitative error estimate for the local relaxation flow at the edge.\nTogether with a quantitative version of the Green function comparison theorem,\nthis gives the rate of convergence.\n  Combined with a result of Lee-Schnelli [26], some quantitative estimates also\nhold for more general separable sample covariance matrices $ X^* \\Sigma X $\nwith general diagonal population $ \\Sigma $.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 17:10:03 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Haoyu", ""]]}, {"id": "1912.05573", "submitter": "Giuseppe Vinci", "authors": "Giuseppe Vinci, Gautam Dasarathy, Genevera I. Allen", "title": "Graph quilting: graphical model selection from partially observed\n  covariances", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of conditional dependence graph estimation when\nseveral pairs of nodes have no joint observation. For these pairs even the\nsimplest metric of covariability, the sample covariance, is unavailable. This\nproblem arises, for instance, in calcium imaging recordings where the\nactivities of a large population of neurons are typically observed by recording\nfrom smaller subsets of cells at once, and several pairs of cells are never\nrecorded simultaneously. With no additional assumption, the unavailability of\nparts of the covariance matrix translates into the unidentifiability of the\nprecision matrix that, in the Gaussian graphical model setting, specifies the\ngraph. Recovering a conditional dependence graph in such settings is\nfundamentally an extremely hard challenge, because it requires to infer\nconditional dependences between network nodes with no empirical evidence of\ntheir covariability. We call this challenge the \"graph quilting problem\". We\ndemonstrate that, under mild conditions, it is possible to correctly identify\nnot only the edges connecting the observed pairs of nodes, but also a superset\nof those connecting the variables that are never observed jointly. We propose\nan $\\ell_1$ regularized graph estimator based on a partially observed sample\ncovariance matrix and establish its rates of convergence in high-dimensions. We\nfinally present a simulation study and the analysis of calcium imaging data of\nten thousand neurons in mouse visual cortex.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:04:17 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Vinci", "Giuseppe", ""], ["Dasarathy", "Gautam", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1912.05642", "submitter": "Jonas Wallin", "authors": "David Bolin and Jonas Wallin", "title": "Local scale invariance and robustness of proper scoring rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Averages of proper scoring rules are often used to rank probabilistic\nforecasts. In many cases, the variance of the individual observations and their\npredictive distributions vary in these averages. We show that some of the most\npopular proper scoring rules, such as the continuous ranked probability score\n(CRPS) which is the go-to score for continuous observation ensemble forecasts,\ngive more importance to observations with large uncertainty which can lead to\nunintuitive rankings. To describe this issue, we define the concept of local\nscale invariance for scoring rules. A new class of generalized proper kernel\nscoring rules is derived and as a member of this class we propose the scaled\nCRPS (SCRPS). This new proper scoring rule is locally scale invariant and\ntherefore works in the case of varying uncertainty. Like CRPS it is\ncomputationally available for output from ensemble forecasts, and does not\nrequire ability to evaluate the density of the forecast.\n  We further define robustness of scoring rules, show why this also is an\nimportant concept for average scores, and derive new proper scoring rules that\nare robust against outliers. The theoretical findings are illustrated in three\ndifferent applications from spatial statistics, stochastic volatility models,\nand regression for count data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:28:58 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 16:00:11 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 11:42:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "1912.05680", "submitter": "Nan Wu", "authors": "David B Dunson, Hau-Tieng Wu, Nan Wu", "title": "Spectral Convergence of Graph Laplacian and Heat Kernel Reconstruction\n  in $L^\\infty$ from Random Samples", "comments": "53 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the manifold setting, we provide a series of spectral convergence results\nquantifying how the eigenvectors and eigenvalues of the graph Laplacian\nconverge to the eigenfunctions and eigenvalues of the Laplace-Beltrami operator\nin the $L^\\infty$ sense. The convergence rate is also provided. Based on these\nresults, convergence of the proposed heat kernel approximation algorithm, as\nwell as the convergence rate, to the exact heat kernel is guaranteed. To our\nknowledge, this is the first work exploring the spectral convergence in the\n$L^\\infty$ sense and providing a numerical heat kernel reconstruction from the\npoint cloud with theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:56:04 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 23:39:49 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 14:23:28 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 17:09:35 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 14:48:57 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Dunson", "David B", ""], ["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "1912.05737", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier", "title": "Finite sample properties of parametric MMD estimation: robustness to\n  misspecification and dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in statistics aim at designing a universal estimation procedure,\nthat is, an estimator that would converge to the best approximation of the\n(unknown) data generating distribution in a model, without any assumption on\nthis distribution. This question is of major interest, in particular because\nthe universality property leads to the robustness of the estimator. In this\npaper, we tackle the problem of universal estimation using a minimum distance\nestimator presented in Briol et al. (2019) based on the Maximum Mean\nDiscrepancy. We show that the estimator is robust to both dependence and to the\npresence of outliers in the dataset. Finally, we provide a theoretical study of\nthe stochastic gradient descent algorithm used to compute the estimator, and we\nsupport our findings with numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:28:13 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:52:14 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 02:11:15 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 02:00:55 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 08:52:02 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1912.05738", "submitter": "Sheng Jiang", "authors": "Sheng Jiang, Surya T. Tokdar", "title": "Variable Selection Consistency of Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric regression under a rescaled Gaussian process prior\noffers smoothness-adaptive function estimation with near minimax-optimal error\nrates. Hierarchical extensions of this approach, equipped with stochastic\nvariable selection, are known to also adapt to the unknown intrinsic dimension\nof a sparse true regression function. But it remains unclear if such extensions\noffer variable selection consistency, i.e., if the true subset of important\nvariables could be consistently learned from the data. It is shown here that\nvariable consistency may indeed be achieved with such models at least when the\ntrue regression function has finite smoothness to induce a polynomially larger\npenalty on inclusion of false positive predictors. Our result covers the high\ndimensional asymptotic setting where the predictor dimension is allowed to grow\nwith the sample size. The proof utilizes Schwartz theory to establish that the\nposterior probability of wrong selection vanishes asymptotically. A necessary\nand challenging technical development involves providing sharp upper and lower\nbounds to small ball probabilities at all rescaling levels of the Gaussian\nprocess prior, a result that could be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:29:14 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 02:43:18 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jiang", "Sheng", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1912.05810", "submitter": "Owen Thomas", "authors": "Owen Thomas, Jukka Corander", "title": "Diagnosing model misspecification and performing generalized Bayes'\n  updates via probabilistic classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model misspecification is a long-standing enigma of the Bayesian inference\nframework as posteriors tend to get overly concentrated on ill-informed\nparameter values towards the large sample limit. Tempering of the likelihood\nhas been established as a safer way to do updates from prior to posterior in\nthe presence of model misspecification. At one extreme tempering can ignore the\ndata altogether and at the other extreme it provides the standard Bayes' update\nwhen no misspecification is assumed to be present. However, it is an open issue\nhow to best recognize misspecification and choose a suitable level of tempering\nwithout access to the true generating model. Here we show how probabilistic\nclassifiers can be employed to resolve this issue. By training a probabilistic\nclassifier to discriminate between simulated and observed data provides an\nestimate of the ratio between the model likelihood and the likelihood of the\ndata under the unobserved true generative process, within the discriminatory\nabilities of the classifier. The expectation of the logarithm of a ratio with\nrespect to the data generating process gives an estimation of the negative\nKullback-Leibler divergence between the statistical generative model and the\ntrue generative distribution. Using a set of canonical examples we show that\nthis divergence provides a useful misspecification diagnostic, a model\ncomparison tool, and a method to inform a generalised Bayesian update in the\npresence of misspecification for likelihood-based models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 07:45:21 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Thomas", "Owen", ""], ["Corander", "Jukka", ""]]}, {"id": "1912.06116", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ruodu Wang", "title": "E-values: Calibration, combination, and applications", "comments": "48 pages, 5 figures, 4 algorithms. A new title and improved\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing of a single hypothesis and testing multiple hypotheses are\nusually done in terms of p-values. In this paper we replace p-values with their\nnatural competitor, e-values, which are closely related to betting, Bayes\nfactors, and likelihood ratios. We demonstrate that e-values are often\nmathematically more tractable; in particular, in multiple testing of a single\nhypothesis, e-values can be merged simply by averaging them. This allows us to\ndevelop efficient procedures using e-values for testing multiple hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:29:28 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 08:24:25 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 07:26:19 GMT"}, {"version": "v4", "created": "Sat, 19 Sep 2020 17:15:37 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vovk", "Vladimir", ""], ["Wang", "Ruodu", ""]]}, {"id": "1912.06307", "submitter": "Andrii Babii", "authors": "Andrii Babii and Eric Ghysels and Jonas Striaukas", "title": "High-Dimensional Granger Causality Tests with an Application to VIX and\n  News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Granger causality testing for high-dimensional time series using\nregularized regressions. To perform proper inference, we rely on\nheteroskedasticity and autocorrelation consistent (HAC) estimation of the\nasymptotic variance and develop the inferential theory in the high-dimensional\nsetting. To recognize the time series data structures we focus on the\nsparse-group LASSO estimator, which includes the LASSO and the group LASSO as\nspecial cases. We establish the debiased central limit theorem for low\ndimensional groups of regression coefficients and study the HAC estimator of\nthe long-run variance based on the sparse-group LASSO residuals. This leads to\nvalid time series inference for individual regression coefficients as well as\ngroups, including Granger causality tests. The treatment relies on a new\nFuk-Nagaev inequality for a class of $\\tau$-mixing processes with heavier than\nGaussian tails, which is of independent interest. In an empirical application,\nwe study the Granger causal relationship between the VIX and financial news.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:20:51 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 20:11:21 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:51:44 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 15:48:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Babii", "Andrii", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "1912.06357", "submitter": "Zeng Li", "authors": "Zeng Li, Qinwen Wang, Runze Li", "title": "Central Limit Theorem for Linear Spectral Statistics of Large\n  Dimensional Kendall's Rank Correlation Matrices and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the limiting spectral behaviors of large\ndimensional Kendall's rank correlation matrices generated by samples with\nindependent and continuous components. We do not require the components to be\nidentically distributed, and do not need any moment conditions, which is very\ndifferent from the assumptions imposed in the literature of random matrix\ntheory. The statistical setting in this paper covers a wide range of highly\nskewed and heavy-tailed distributions. We establish the central limit theorem\n(CLT) for the linear spectral statistics of the Kendall's rank correlation\nmatrices under the Marchenko-Pastur asymptotic regime, in which the dimension\ndiverges to infinity proportionally with the sample size. We further propose\nthree nonparametric procedures for high dimensional independent test and their\nlimiting null distributions are derived by implementing this CLT. Our numerical\ncomparisons demonstrate the robustness and superiority of our proposed test\nstatistics under various mixed and heavy-tailed cases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 08:29:54 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Li", "Zeng", ""], ["Wang", "Qinwen", ""], ["Li", "Runze", ""]]}, {"id": "1912.06542", "submitter": "Boris Ryabko", "authors": "Boris Ryabko", "title": "On asymptotically optimal tests for random number generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constructing effective statistical tests for random number\ngenerators (RNG) is considered. Currently, statistical tests for RNGs are a\nmandatory part of cryptographic information protection systems, but their\neffectiveness is mainly estimated based on experiments with various RNGs.\n  We find an asymptotic estimate for the p-value of an optimal test in the case\nwhere the alternative hypothesis is a known stationary ergodic source, and then\ndescribe a family of tests each of which has the same asymptotic estimate of\nthe p-value for any (unknown) stationary ergodic source.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 15:00:40 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Ryabko", "Boris", ""]]}, {"id": "1912.06689", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "Data-driven confidence bands for distributed nonparametric regression", "comments": "COLT2020 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process Regression and Kernel Ridge Regression are popular\nnonparametric regression approaches. Unfortunately, they suffer from high\ncomputational complexity rendering them inapplicable to the modern massive\ndatasets. To that end a number of approximations have been suggested, some of\nthem allowing for a distributed implementation. One of them is the divide and\nconquer approach, splitting the data into a number of partitions, obtaining the\nlocal estimates and finally averaging them. In this paper we suggest a novel\ncomputationally efficient fully data-driven algorithm, quantifying uncertainty\nof this method, yielding frequentist $L_2$-confidence bands. We rigorously\ndemonstrate validity of the algorithm. Another contribution of the paper is a\nminimax-optimal high-probability bound for the averaged estimator,\ncomplementing and generalizing the known risk bounds.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 20:13:55 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:17:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "1912.06780", "submitter": "Clark Taylor", "authors": "Shane Lubold and Clark N. Taylor", "title": "Formal Definitions of Conservative PDFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under ideal conditions, the probability density function (PDF) of a random\nvariable, such as a sensor measurement, would be well known and amenable to\ncomputation and communication tasks. However, this is often not the case, so\nthe user looks for some other PDF that approximates the true but intractable\nPDF. Conservativeness is a commonly sought property of this approximating PDF,\nespecially in distributed or unstructured data systems where the data being\nfused may contain un-known correlations. Roughly, a conservative approximation\nis one that overestimates the uncertainty of a system. While prior work has\nintroduced some definitions of conservativeness, these definitions either apply\nonly to normal distributions or violate some of the intuitive appeal of\n(Gaussian) conservative definitions. This work provides a general and intuitive\ndefinition of conservativeness that is applicable to any probability\ndistribution that is a measure over $\\mathbb{R}^m$ or an infinite subset\nthereof, including multi-modal and uniform distributions. Unfortunately, we\nshow that this \\emph{strong} definition of conservative cannot be used to\nevaluate data fusion techniques. Therefore, we also describe a weaker\ndefinition of conservative and show it is preserved through common data fusion\nmethods such as the linear and log-linear opinion pool, and homogeneous\nfunctionals, \\rev{assuming the input distributions can be factored into\nindependent and common distributions}. In addition, we show that after fusion,\nweak conservativeness is preserved by Bayesian updates. These strong and weak\ndefinitions of conservativeness can help design and evaluate potential\ncorrelation-agnostic data fusion techniques.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 03:58:22 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 21:33:07 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lubold", "Shane", ""], ["Taylor", "Clark N.", ""]]}, {"id": "1912.06830", "submitter": "Somayeh Aghashahi", "authors": "Somayeh Aghashahi, Samaneh Aghashahi, Zolfa Zeinalpour-Yazdi, Aliakbar\n  Tadaion, Arash Asadi", "title": "Stochastic Modeling of Beam Management in mmWave Vehicular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility management is a major challenge for the wide-spread deployment of\nmillimeter-wave (mmWave) cellular networks. In particular, directional\nbeamforming in mmWave devices renders high-speed mobility support very complex.\nThis complexity, however, is not limited to system design but also the\nperformance estimation and evaluation. Hence, some have turned their attention\nto stochastic modeling of mmWave vehicular communication to derive closed-form\nexpressions characterizing the coverage and rate behavior of the network. In\nthis article, we model and analyze the beam management for mmWave vehicular\nnetworks. To the best of our knowledge, this is the first work that goes beyond\ncoverage and rate analysis. Specifically, we focus on a multi-lane divided\nhighway scenario in which base stations and vehicles are present on both sides\nof the highway. In addition to providing analytical expressions for the average\nnumber of beam switching and handover events, we provide design insights for\nthe network operators to fine-tune their network within the flexibility\nprovided by the standard in the choice of system parameters, including the\nnumber of resources dedicated to channel feedback and beam alignment\noperations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 12:09:46 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Aghashahi", "Somayeh", ""], ["Aghashahi", "Samaneh", ""], ["Zeinalpour-Yazdi", "Zolfa", ""], ["Tadaion", "Aliakbar", ""], ["Asadi", "Arash", ""]]}, {"id": "1912.06926", "submitter": "Stephen Berg", "authors": "Stephen Berg, Jun Zhu, Murray K. Clayton", "title": "Control variates and Rao-Blackwellization for deterministic sweep Markov\n  chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study control variate methods for Markov chain Monte Carlo (MCMC) in the\nsetting of deterministic sweep sampling using $K\\geq 2$ transition kernels. New\nvariance reduction results are provided for MCMC averages based on sweeps over\ngeneral transition kernels, leading to a particularly simple control variate\nestimator in the setting of deterministic sweep Gibbs sampling. Theoretical\ncomparisons of our proposed control variate estimators with existing literature\nare made, and a simulation study is performed to examine the amount of variance\nreduction in some example cases. We also relate control variate approaches to\napproaches based on conditioning (or Rao-Blackwellization), and show that the\nlatter can be viewed as an approximation of the former. Our theoretical results\nhold for Markov chains under standard geometric drift assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 20:57:19 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Berg", "Stephen", ""], ["Zhu", "Jun", ""], ["Clayton", "Murray K.", ""]]}, {"id": "1912.06928", "submitter": "Modou Ngom", "authors": "Gane Samb Lo, Modou Ngom, Moumouni Diallo", "title": "Extremes, extremal index estimation, records, moment problem for the\n  Pseudo-Lindley distribution and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-Lindley distribution which was introduced in Zeghdoudi and Nedjar\n(2016) is studied with regards to its upper tail. In that regard, and when the\nunderlying distribution function follows the Pseudo-Lindley law, we investigate\nthe behavior of its values, the asymptotic normality of the Hill estimator and\nthe double-indexed generalized Hill statistic process (Ngom and Lo), the\nasymptotic normality of the records values and the moment problem.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:09:02 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lo", "Gane Samb", ""], ["Ngom", "Modou", ""], ["Diallo", "Moumouni", ""]]}, {"id": "1912.06987", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Lei Wu", "title": "The Generalization Error of the Minimum-norm Solutions for\n  Over-parameterized Neural Networks", "comments": "Published version", "journal-ref": "Pure and Applied Functional Analysis, Volume 5, Number 6,\n  1145-1460, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization properties of minimum-norm solutions for three\nover-parametrized machine learning models including the random feature model,\nthe two-layer neural network model and the residual network model. We proved\nthat for all three models, the generalization error for the minimum-norm\nsolution is comparable to the Monte Carlo rate, up to some logarithmic terms,\nas long as the models are sufficiently over-parametrized.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 06:05:14 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 15:16:12 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wu", "Lei", ""]]}, {"id": "1912.06996", "submitter": "Chandima N. P. G. Arachchige", "authors": "Chandima N.P.G. Arachchige, Luke A. Prendergast", "title": "Mean skewness measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skewness measures can be used to measure the level of asymmetry of a\ndistribution. Given the prevalence of statistical methods that assume\nunderlying symmetry, and also the desire for symmetry in order to make\nmeaningful judgements for common summary measures (e.g. the sample mean),\nreliably quantifying asymmetry is an important problem. There are several\nmeasures, among them generalizations of Bowley's well known skewness\ncoefficient, that use sample quartiles and other quantile-based measures. The\nmain drawbacks of many measures is that they are either limited to quartiles\nand do not take into account more extreme tail behavior, or that they require\none to choose other quantiles (i.e. choose a value for $p$ different from 0.25)\nin place of the quartiles. Our objective is to (i) average the skewness\nmeasures over all $p$ and (ii) provide interval estimators for the new measure\nwith good coverage properties. Our simulation results show that the interval\nestimators perform very well for all distributions considered.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 07:39:03 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 22:38:24 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Arachchige", "Chandima N. P. G.", ""], ["Prendergast", "Luke A.", ""]]}, {"id": "1912.07012", "submitter": "Michal Balcerek PhD", "authors": "Michal Balcerek and Krzysztof Burnecki", "title": "Testing of fractional Brownian motion in a noisy environment", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.chaos.2020.110097", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional Brownian motion (FBM) is the only Gaussian self-similar process\nwith stationary increments. Its increment process, called fractional Gaussian\nnoise, is ergodic and exhibits a property of power-like decaying\nautocorrelation function (ACF) which leads to the notion of long memory. These\nproperties have made FBM important in modelling real-world data recorded in\ndifferent experiments ranging from biology to telecommunication. These\nexperiments are often disturbed by a noise which source can be just the\ninstrument error. In this paper we propose a rigorous statistical test based on\nthe ACF for FBM with added white Gaussian noise. To this end we derive a\ndistribution of the test statistic which is given explicitly by the generalized\nchi-squared distribution. This allows us to find critical regions for the test\nwith a given significance level. We check the quality of the introduced test by\nstudying its power and comparing with other tests existing in the literature.\nWe also note that the introduced test procedure can be applied to an arbitrary\nGaussian process.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 09:48:28 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 20:47:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Balcerek", "Michal", ""], ["Burnecki", "Krzysztof", ""]]}, {"id": "1912.07041", "submitter": "Natsuki Kariya", "authors": "Natsuki Kariya, Sumio Watanabe", "title": "Testing Homogeneity for Normal Mixture Models: Variational Bayes\n  Approach", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": "10.1587/transfun.2019EAP1172", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test of homogeneity for normal mixtures has been conducted in diverse\nresearch areas, but constructing a theory of the test of homogeneity is\nchallenging because the parameter set for the null hypothesis corresponds to\nsingular points in the parameter space. In this paper, we examine this problem\nfrom a new perspective and offer a theory of hypothesis testing for homogeneity\nbased on a variational Bayes framework. In the conventional theory, the\nconstant order term of the free energy has remained unknown, however, we\nclarify its asymptotic behavior because it is necessary for constructing a\nhypothesis test. Numerical experiments shows the validity of our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 13:41:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kariya", "Natsuki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1912.07075", "submitter": "C\\'ecile Haberstich", "authors": "C\\'ecile Haberstich and Anthony Nouy and Guillaume Perrin", "title": "Boosted optimal weighted least-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the approximation of a function $u$ in a given\napproximation space $V_m$ of dimension $m$ from evaluations of the function at\n$n$ suitably chosen points. The aim is to construct an approximation of $u$ in\n$V_m$ which yields an error close to the best approximation error in $V_m$ and\nusing as few evaluations as possible. Classical least-squares regression, which\ndefines a projection in $V_m$ from $n$ random points, usually requires a large\n$n$ to guarantee a stable approximation and an error close to the best\napproximation error. This is a major drawback for applications where $u$ is\nexpensive to evaluate. One remedy is to use a weighted least squares projection\nusing $n$ samples drawn from a properly selected distribution. In this paper,\nwe introduce a boosted weighted least-squares method which allows to ensure\nalmost surely the stability of the weighted least squares projection with a\nsample size close to the interpolation regime $n=m$. It consists in sampling\naccording to a measure associated with the optimization of a stability\ncriterion over a collection of independent $n$-samples, and resampling\naccording to this measure until a stability condition is satisfied. A greedy\nmethod is then proposed to remove points from the obtained sample.\nQuasi-optimality properties are obtained for the weighted least-squares\nprojection, with or without the greedy procedure. The proposed method is\nvalidated on numerical examples and compared to state-of-the-art interpolation\nand weighted least squares methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 17:50:51 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 14:03:24 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Haberstich", "C\u00e9cile", ""], ["Nouy", "Anthony", ""], ["Perrin", "Guillaume", ""]]}, {"id": "1912.07086", "submitter": "Maria D. Ruiz-Medina", "authors": "M. Dolores Ruiz-Medina", "title": "Spectral analysis of long range dependence functional time series", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new modeling framework for the spectral analysis of\nlong--range dependence (LRD) in functional sequences, beyond the usual\nstructural modeling assumptions of the linear setting. Specifically, a\nsemiparametric non--linear model is adopted in the functional spectral domain,\ninvolving a long--memory operator. We prove that this operator also\ncharacterizes the heavy--tail behavior, in time, of the inverse functional\nFourier transform in the space of bounded linear operators. The\nnon--summability in time of its trace norm then follows. Some particular cases\nare analyzed, including space varying fractionally integrated functional\nautoregressive moving averages processes. In the Gaussian case, a\nweak--consistent parametric estimator of the long--memory operator is obtained,\nby minimizing the operator norm of a divergence information based functional\nloss.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 18:40:01 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 18:02:49 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 07:08:18 GMT"}, {"version": "v4", "created": "Sun, 21 Feb 2021 18:40:42 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 16:13:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ruiz-Medina", "M. Dolores", ""]]}, {"id": "1912.07104", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Jeffrey W. Miller", "title": "Robust Inference and Model Criticism Using Bagged Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Bayesian inference is known to be sensitive to model\nmisspecification, leading to unreliable uncertainty quantification and poor\npredictive performance. However, finding generally applicable and\ncomputationally feasible methods for robust Bayesian inference under\nmisspecification has proven to be a difficult challenge. An intriguing,\neasy-to-use, and widely applicable approach is to use bagging on the Bayesian\nposterior (\"BayesBag\"); that is, to use the average of posterior distributions\nconditioned on bootstrapped datasets. In this paper, we develop the asymptotic\ntheory of BayesBag, propose a model-data mismatch index for model criticism\nusing BayesBag, and empirically validate our theory and methodology on\nsynthetic and real-world data in linear regression, sparse logistic regression,\nand a hierarchical mixed effects model. We find that in the presence of\nsignificant misspecification, BayesBag yields more reproducible inferences and\nhas better predictive accuracy than the standard Bayesian posterior; on the\nother hand, when the model is correctly specified, BayesBag produces superior\nor equally good results. Overall, our results demonstrate that BayesBag\ncombines the attractive modeling features of standard Bayesian inference with\nthe distributional robustness properties of frequentist methods, providing\nbenefits over both Bayes alone and the bootstrap alone.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:29:12 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 20:11:35 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 00:48:35 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Miller", "Jeffrey W.", ""]]}, {"id": "1912.07215", "submitter": "Jingwei Liu", "authors": "Jingwei Liu", "title": "Extension of Donsker's Invariance Principle with Incomplete Partial-Sum\n  Process", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on deleting-item central limit theory, the classical Donsker's theorem\nof partial-sum process of independent and identically distributed (i.i.d.)\nrandom variables is extended to incomplete partial-sum process. The incomplete\npartial-sum process Donsker's invariance principles are constructed and derived\nfor general partial-sum process of i.i.d random variables and empirical process\nrespectively, they are not only the extension of functional central limit\ntheory, but also the extension of deleting-item central limit theory. Our work\nenriches the random elements structure of weak convergence.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 06:40:53 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "1912.07242", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran", "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this expository note we describe a surprising phenomenon in\noverparameterized linear regression, where the dimension exceeds the number of\nsamples: there is a regime where the test risk of the estimator found by\ngradient descent increases with additional samples. In other words, more data\nactually hurts the estimator. This behavior is implicit in a recent line of\ntheoretical works analyzing \"double-descent\" phenomenon in linear models. In\nthis note, we isolate and understand this behavior in an extremely simple\nsetting: linear regression with isotropic Gaussian covariates. In particular,\nthis occurs due to an unconventional type of bias-variance tradeoff in the\noverparameterized regime: the bias decreases with more samples, but variance\nincreases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 08:28:26 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Nakkiran", "Preetum", ""]]}, {"id": "1912.07546", "submitter": "Prateek Raj Srivastava", "authors": "Prateek R. Srivastava, Purnamrita Sarkar, Grani A. Hanasusanto", "title": "A Robust Spectral Clustering Algorithm for Sub-Gaussian Mixture Models\n  with Outliers", "comments": "54 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering datasets in the presence of arbitrary\noutliers. Traditional clustering algorithms such as k-means and spectral\nclustering are known to perform poorly for datasets contaminated with even a\nsmall number of outliers. In this paper, we develop a provably robust spectral\nclustering algorithm that applies a simple rounding scheme to denoise a\nGaussian kernel matrix built from the data points and uses vanilla spectral\nclustering to recover the cluster labels of data points. We analyze the\nperformance of our algorithm under the assumption that the \"good\" data points\nare generated from a mixture of sub-gaussians (we term these \"inliers\"), while\nthe outlier points can come from any arbitrary probability distribution. For\nthis general class of models, we show that the misclassification error decays\nat an exponential rate in the signal-to-noise ratio, provided the number of\noutliers is a small fraction of the inlier points. Surprisingly, this derived\nerror bound matches with the best-known bound for semidefinite programs (SDPs)\nunder the same setting without outliers. We conduct extensive experiments on a\nvariety of simulated and real-world datasets to demonstrate that our algorithm\nis less sensitive to outliers compared to other state-of-the-art algorithms\nproposed in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 17:48:58 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 00:47:37 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 08:32:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Srivastava", "Prateek R.", ""], ["Sarkar", "Purnamrita", ""], ["Hanasusanto", "Grani A.", ""]]}, {"id": "1912.07560", "submitter": "Mohammad Arashi", "authors": "M. Arashi, A. Bekker, D. de Waal and S. Makgai", "title": "Developing multivariate distributions using Dirichlet generator", "comments": "30 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist several endeavors proposing a new family of extended\ndistributions using the beta-generating technique. This is a well-known\nmechanism in developing flexible distributions, by embedding the cumulative\ndistribution function (cdf) of a baseline distribution within the beta\ndistribution that acts as a generator. Univariate beta-generated distributions\noffer many fruitful and tractable properties and have applications in\nhydrology, biology and environmental sciences amongst other fields. In the\nunivariate cases, this extension works well, however, for multivariate cases,\nthe beta distribution generator delivers complex expressions. In this document,\nthe proposed extension from the univariate to the multivariate domain addresses\nthe need of flexible multivariate distributions that can model a wide range of\nmultivariate data. This new family of multivariate distributions, whose\nmarginals are beta-generated distributed, is constructed with the function\nH(x_{1},...,x_{p})=F(G_{1}(x_{1}),G_{2}(x_{2}),...,G_{p}(x_{p})), where\n$G_{i}(x_{i})$ are the cdfs of the gamma (baseline) distribution and F(.) as\nthe cdf of the Dirichlet distribution. Hence as the main example, a general\nmodel having the support [0,1]^{p} (for p variates), using the Dirichlet as the\ngenerator, is developed together with some distributional properties, such as\nthe moment generating function. The proposed Dirichlet-generated distributions\ncan be applied to compositional data. The parameters of the model are estimated\nby using the maximum likelihood method. The effectiveness and prominence of the\nproposed family are illustrated by analyzing simulated as well as two real\ndatasets. A new model testing technique is introduced to evaluate the\nperformance of the multivariate models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:15:51 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Arashi", "M.", ""], ["Bekker", "A.", ""], ["de Waal", "D.", ""], ["Makgai", "S.", ""]]}, {"id": "1912.07572", "submitter": "Matyas Barczy", "authors": "Matyas Barczy", "title": "A new example for a proper scoring rule", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new example for a proper scoring rule motivated by the form of\nAnderson--Darling distance of distribution functions and Example 5 in Brehmer\nand Gneiting (2020).\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:36:33 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 18:38:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Barczy", "Matyas", ""]]}, {"id": "1912.07592", "submitter": "Hang Liu", "authors": "Hang Liu and Kanchan Mukherjee", "title": "R-estimators in GARCH models; asymptotics, applications and\n  bootstrapping", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quasi-maximum likelihood estimation is a commonly-used method for\nestimating GARCH parameters. However, such estimators are sensitive to outliers\nand their asymptotic normality is proved under the finite fourth moment\nassumption on the underlying error distribution. In this paper, we propose a\nnovel class of estimators of the GARCH parameters based on ranks, called\nR-estimators, with the property that they are asymptotic normal under the\nexistence of a more than second moment of the errors and are highly efficient.\nWe also consider the weighted bootstrap approximation of the finite sample\ndistributions of the R-estimators. We propose fast algorithms for computing the\nR-estimators and their bootstrap replicates. Both real data analysis and\nsimulations show the superior performance of the proposed estimators under the\nnormal and heavy-tailed distributions. Our extensive simulations also reveal\nexcellent coverage rates of the weighted bootstrap approximations. In addition,\nwe discuss empirical and simulation results of the R-estimators for the higher\norder GARCH models such as the GARCH~($2, 1$) and asymmetric models such as the\nGJR model.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:51:23 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 15:35:17 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Liu", "Hang", ""], ["Mukherjee", "Kanchan", ""]]}, {"id": "1912.07879", "submitter": "Alexander Meister", "authors": "Aurore Delaigle and Alexander Meister", "title": "Nonparametric density estimation for intentionally corrupted functional\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical models where functional data are artificially\ncontaminated by independent Wiener processes in order to satisfy privacy\nconstraints. We show that the corrupted observations have a Wiener density\nwhich determines the distribution of the original functional random variables,\nmasked near the origin, uniquely, and we construct a nonparametric estimator of\nthat density. We derive an upper bound for its mean integrated squared error\nwhich has a polynomial convergence rate, and we establish an asymptotic lower\nbound on the minimax convergence rates which is close to the rate attained by\nour estimator. Our estimator requires the choice of a basis and of two\nsmoothing parameters. We propose data-driven ways of choosing them and prove\nthat the asymptotic quality of our estimator is not significantly affected by\nthe empirical parameter selection. We examine the numerical performance of our\nmethod via simulated examples.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 08:56:50 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Delaigle", "Aurore", ""], ["Meister", "Alexander", ""]]}, {"id": "1912.07948", "submitter": "Rostyslav Maiboroda", "authors": "Rostyslav Maiboroda, Olena Sugakova", "title": "Jackknife covariance matrix estimation for observations from mixture", "comments": "Published at https://doi.org/10.15559/19-VMSTA145 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2019, Vol. 6, No. 4,\n  495-513", "doi": "10.15559/19-VMSTA145", "report-no": "VTeX-VMSTA-VMSTA145", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general jackknife estimator for the asymptotic covariance of moment\nestimators is considered in the case when the sample is taken from a mixture\nwith varying concentrations of components. Consistency of the estimator is\ndemonstrated. A fast algorithm for its calculation is described. The estimator\nis applied to construction of confidence sets for regression parameters in the\nlinear regression with errors in variables. An application to sociological data\nanalysis is considered.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:04:53 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Maiboroda", "Rostyslav", ""], ["Sugakova", "Olena", ""]]}, {"id": "1912.07967", "submitter": "Mahdi Doostparast", "authors": "M. Doostparast, M. Hashempour, E. Velayati Moghaddam 1", "title": "Weibull analysis with sequential order statistics under a power trend\n  model for hazard rates", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering systems, it is usually assumed that lifetimes of components\nare independent and identically distributed (iid). But, the failure of a\ncomponent results in a higher load on the remaining components and hence causes\nthe distribution of the surviving components change. For modeling this kind of\nsystems, the theory of sequential order statistics (SOS) can be used. Assuming\nWeibull distribution for lifetimes of components and conditionally proportional\nhazard rates model as a special case of the SOS theory, the maximum likelihood\nestimates of the unknown parameters are obtained in different cases. A new\nmodel, denoted by PTCPHM, as a generalization of the iid case is proposed, and\nthen statistical inferential methods including point and interval estimation as\nwell as hypothesis tests under PTCPHM are then developed. Finally, a real data\non failure times of aircraft components, due to Mann and Fertig (1973), is\nanalyzed to illustrate the model and inferential methods developed here.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:29:23 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Doostparast", "M.", ""], ["Hashempour", "M.", ""], ["1", "E. Velayati Moghaddam", ""]]}, {"id": "1912.08003", "submitter": "Renata Talska", "authors": "R. Talska, A. Menafoglio, K. Hron, J.J.Egozcue, J.Palarea-Albaladejo", "title": "Changing reference measure in Bayes spaces with applications to\n  functional data analysis", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability density functions (PDFs) can be understood as continuous\ncompositions by the theory of Bayes spaces. The origin of a Bayes space is\ndetermined by a given reference measure. This can be easily changed through the\nwell-known chain rule which has an impact on the geometry of the Bayes space.\nThis work provides a mathematical framework for setting a reference measure. It\nis used to develop a weighting scheme on the bounded domain of distributional\ndata. The impact on statistical analysis is shown from the perspective of\nsimplicial functional principal component analysis. Moreover, a novel centered\nlog-ratio transformation is proposed to map a weighted Bayes spaces into an\nunweighted $L^2$ space, enabling to use most tools developed in functional data\nanalysis (e.g. clustering, regression analysis, etc.) while accounting for the\nweighting strategy. The potential of our proposal is shown through simulation\nand on a real case study using Italian income data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 13:39:24 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Talska", "R.", ""], ["Menafoglio", "A.", ""], ["Hron", "K.", ""], ["Egozcue", "J. J.", ""], ["Palarea-Albaladejo", "J.", ""]]}, {"id": "1912.08103", "submitter": "Rodrigo A. Gonz\\'alez", "authors": "Rodrigo A. Gonz\\'alez and Cristian R. Rojas", "title": "A Finite-Sample Deviation Bound for Stable Autoregressive Processes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study non-asymptotic deviation bounds of the least squares\nestimator in Gaussian AR($n$) processes. By relying on martingale concentration\ninequalities and a tail-bound for $\\chi^2$ distributed variables, we provide a\nconcentration bound for the sample covariance matrix of the process output.\nWith this, we present a problem-dependent finite-time bound on the deviation\nprobability of any fixed linear combination of the estimated parameters of the\nAR$(n)$ process. We discuss extensions and limitations of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:55:54 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 12:06:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Gonz\u00e1lez", "Rodrigo A.", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "1912.08233", "submitter": "Dennis Dobler", "authors": "Dennis Dobler", "title": "Randomization empirical processes", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article creates a link between two well-established fields in\nmathematical statistics: empirical processes and inference based on\nrandomization via algebraic groups. To this end, a broadly applicable\nconditional weak convergence theorem is developed for empirical processes that\nare based on randomized observations. Random elements of an algebraic group are\napplied to the data vectors from which the randomized version of a statistic is\nderived. Combining a variant of the functional delta-method with a suitable\nstudentization of the statistic, asymptotically exact hypothesis tests can be\ndeduced, while the finite sample exactness property under group-invariant\nsub-hypotheses is preserved. The methodology is exemplified with three\nexamples: the Pearson correlation coefficient, a Mann-Whitney effect based on\nright-censored paired data, and a competing risks analysis. The practical\nusefulness of the approaches is assessed through simulation studies and an\napplication to data from patients suffering from diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:09:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 07:19:11 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Dobler", "Dennis", ""]]}, {"id": "1912.08335", "submitter": "Andres Masegosa R", "authors": "Andres R. Masegosa", "title": "Learning under Model Misspecification: Applications to Variational and\n  Ensemble methods", "comments": "Camera-Ready Version. NeurIPS 2020. Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually any model we use in machine learning to make predictions does not\nperfectly represent reality. So, most of the learning happens under model\nmisspecification. In this work, we present a novel analysis of the\ngeneralization performance of Bayesian model averaging under model\nmisspecification and i.i.d. data using a new family of second-order PAC-Bayes\nbounds. This analysis shows, in simple and intuitive terms, that Bayesian model\naveraging provides suboptimal generalization performance when the model is\nmisspecified. In consequence, we provide strong theoretical arguments showing\nthat Bayesian methods are not optimal for learning predictive models, unless\nthe model class is perfectly specified. Using novel second-order PAC-Bayes\nbounds, we derive a new family of Bayesian-like algorithms, which can be\nimplemented as variational and ensemble methods. The output of these algorithms\nis a new posterior distribution, different from the Bayesian posterior, which\ninduces a posterior predictive distribution with better generalization\nperformance. Experiments with Bayesian neural networks illustrate these\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:38:58 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 09:29:53 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 12:29:11 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 11:10:41 GMT"}, {"version": "v5", "created": "Thu, 22 Oct 2020 10:08:28 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Masegosa", "Andres R.", ""]]}, {"id": "1912.08337", "submitter": "Enrique del Castillo", "authors": "Enrique del Castillo and Rainer Goeb", "title": "A Bivariate Dead Band Process Adjustment Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate extension to Box and Jenkins (1963) feedback adjustment problem\nis presented in this paper. The model balances the fixed cost of making an\nadjustment, which is assumed independent of the magnitude of the adjustments,\nwith the cost of running the process off-target, which is assumed quadratic. It\nis also assumed that two controllable factors are available to compensate for\nthe deviations from target of two responses in the presence of a bivariate\nIMA(1,1) disturbance. The optimal policy has the form of a \"dead band\", in\nwhich adjustments are justified only when the predicted process responses\nexceed some boundary in $\\mathbb{R}^2$. This boundary indicates when the\nresponses are predicted to be far enough from their targets that an additional\nadjustment or intervention in the process is justified. Although originally\ndeveloped to control a machine tool, dead band control policies have\napplication in other areas. For example, they could be used to control a\ndisease through the application of a drug to a patient depending on the level\nof a substance in the body (e.g., diabetes control). This paper presents\nanalytical formulae for the computation of the loss function that combines\noff-target and adjustment costs per time unit. Expressions are derived for the\naverage adjustment interval and for the scaled mean square deviations from\ntarget. The minimization of the loss function and the practical use of the\nresulting dead band adjustment strategy is illustrated with an application to a\nsemiconductor manufacturing process.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:46:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["del Castillo", "Enrique", ""], ["Goeb", "Rainer", ""]]}, {"id": "1912.08580", "submitter": "Christina Stoehr", "authors": "Claudia Kirch and Christina Stoehr", "title": "Sequential change point tests based on U-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework of sequential testing procedures based on\n$U$-statistics which contains as an example a sequential CUSUM test based on\ndifferences in mean but also includes a robust sequential Wilcoxon change point\nprocedure. Within this framework, we consider several monitoring schemes that\ntake different observations into account to make a decision at a given time\npoint. Unlike the originally proposed scheme that takes all observations of the\nmonitoring period into account, we also consider a modified moving-sum-version\nas well as a version of a Page-monitoring scheme. The latter behave almost as\ngood for early changes while being advantageous for later changes. For all\nproposed procedures we provide the limit distribution under the null hypothesis\nwhich yields the threshold to control the asymptotic type-I-error. Furthermore,\nwe show that the proposed tests have asymptotic power one. In a simulation\nstudy we compare the performance of the sequential procedures via their\nempirical size, power and detection delay, which is further illustrated by\nmeans of a temperature data set.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:12:08 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kirch", "Claudia", ""], ["Stoehr", "Christina", ""]]}, {"id": "1912.08584", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher and Tobias Jennessen", "title": "Method of moments estimators for the extremal index of a stationary time\n  series", "comments": "46 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremal index $\\theta$, a number in the interval $[0,1]$, is known to be\na measure of primal importance for analyzing the extremes of a stationary time\nseries. New rank-based estimators for $\\theta$ are proposed which rely on the\nconstruction of approximate samples from the exponential distribution with\nparameter $\\theta$ that is then to be fitted via the method of moments. The new\nestimators are analyzed both theoretically as well as empirically through a\nlarge-scale simulation study. In specific scenarios, in particular for time\nseries models with $\\theta \\approx 1$, they are found to be superior to recent\ncompetitors from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 13:22:37 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 04:21:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Jennessen", "Tobias", ""]]}, {"id": "1912.08799", "submitter": "Hazem Al-Mofleh", "authors": "Hazem Al-Mofleh and Ahmed Z. Afify", "title": "A generalization of Ramos-Louzada distribution: Properties and\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a new two-parameter model called generalized Ramos-Louzada\n(GRL) distribution is proposed. The new model provides more flexibility in\nmodeling data with increasing, decreasing, j shaped and reversed-J shaped\nhazard rate function. Several statistical and reliability properties of the GRL\nmodel are also presented in this paper. The unknown parameters of the GRL\ndistribution are discussed using eight frequentist estimation approaches. These\napproaches are important to develop a guideline to choose the best method of\nestimation for the GRL parameters, that would be of great interest to\npractitioners and applied statisticians. A detailed numerical simulation study\nis carried out to examine the bias and the mean square error of the proposed\nestimators. We illustrate the performance of the GRL distribution using two\nreal data sets from the fields of medicine and geology and both data sets show\nthat the new model is more appropriate as compared to the gamma, Marshall-Olkin\nexponential, exponentiated exponential, beta exponential, generalized Lindley,\nPoisson-Lomax, Lindley geometric and Lindley distributions, among others.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:52:53 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Al-Mofleh", "Hazem", ""], ["Afify", "Ahmed Z.", ""]]}, {"id": "1912.08865", "submitter": "Zetong Qi", "authors": "Zetong Qi, T.J. Wilder", "title": "Adversarial VC-dimension and Sample Complexity of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks during the testing phase of neural networks pose a\nchallenge for the deployment of neural networks in security critical settings.\nThese attacks can be performed by adding noise that is imperceptible to humans\non top of the original data. By doing so, an attacker can create an adversarial\nsample, which will cause neural networks to misclassify. In this paper, we seek\nto understand the theoretical limits of what can be learned by neural networks\nin the presence of an adversary. We first defined the hypothesis space of a\nneural network, and showed the relationship between the growth number of the\nentire neural network and the growth number of each neuron. Combine that with\nthe adversarial Vapnik-Chervonenkis(VC)-dimension of halfspace classifiers, we\nconcluded the adversarial VC-dimension of the neural networks with sign\nactivation functions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:10:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Qi", "Zetong", ""], ["Wilder", "T. J.", ""]]}, {"id": "1912.08877", "submitter": "Vladimir Koltchinskii", "authors": "Vladimir Koltchinskii and Mayya Zhilova", "title": "Estimation of Smooth Functionals in Normal Models: Bias Reduction and\n  Asymptotic Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,\\dots, X_n$ be i.i.d. random variables sampled from a normal\ndistribution $N(\\mu,\\Sigma)$ in ${\\mathbb R}^d$ with unknown parameter\n$\\theta=(\\mu,\\Sigma)\\in \\Theta:={\\mathbb R}^d\\times {\\mathcal C}_+^d,$ where\n${\\mathcal C}_+^d$ is the cone of positively definite covariance operators in\n${\\mathbb R}^d.$ Given a smooth functional $f:\\Theta \\mapsto {\\mathbb R}^1,$\nthe goal is to estimate $f(\\theta)$ based on $X_1,\\dots, X_n.$ Let $$\n\\Theta(a;d):={\\mathbb R}^d\\times \\Bigl\\{\\Sigma\\in {\\mathcal C}_+^d:\n\\sigma(\\Sigma)\\subset [1/a, a]\\Bigr\\}, a\\geq 1, $$ where $\\sigma(\\Sigma)$ is\nthe spectrum of covariance $\\Sigma.$ Let $\\hat \\theta:=(\\hat \\mu, \\hat\n\\Sigma),$ where $\\hat \\mu$ is the sample mean and $\\hat \\Sigma$ is the sample\ncovariance, based on the observations $X_1,\\dots, X_n.$ For an arbitrary\nfunctional $f\\in C^s(\\Theta),$ $s=k+1+\\rho, k\\geq 0, \\rho\\in (0,1],$ we define\na functional $f_k:\\Theta \\mapsto {\\mathbb R}$ such that \\begin{align*} &\n\\sup_{\\theta\\in \\Theta(a;d)}\\|f_k(\\hat \\theta)-f(\\theta)\\|_{L_2({\\mathbb\nP}_{\\theta})} \\lesssim_{s, \\beta} \\|f\\|_{C^{s}(\\Theta)}\n\\biggr[\\biggl(\\frac{a}{\\sqrt{n}} \\bigvee a^{\\beta\ns}\\biggl(\\sqrt{\\frac{d}{n}}\\biggr)^{s} \\biggr)\\wedge 1\\biggr], \\end{align*}\nwhere $\\beta =1$ for $k=0$ and $\\beta>s-1$ is arbitrary for $k\\geq 1.$ This\nerror rate is minimax optimal and similar bounds hold for more general loss\nfunctions. If $d=d_n\\leq n^{\\alpha}$ for some $\\alpha\\in (0,1)$ and $s\\geq\n\\frac{1}{1-\\alpha},$ the rate becomes $O(n^{-1/2}).$ Moreover, for\n$s>\\frac{1}{1-\\alpha},$ the estimators $f_k(\\hat \\theta)$ is shown to be\nasymptotically efficient. The crucial part of the construction of estimator\n$f_k(\\hat \\theta)$ is a bias reduction method studied in the paper for more\ngeneral statistical models than normal.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:28:46 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["Zhilova", "Mayya", ""]]}, {"id": "1912.08993", "submitter": "Bai Jiang", "authors": "Bai Jiang, Qiang Sun", "title": "Bayesian high-dimensional linear regression with generic spike-and-slab\n  priors", "comments": "17 pages for main file, 13 pages for appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-and-slab priors are popular Bayesian solutions for high-dimensional\nlinear regression problems. Previous theoretical studies on spike-and-slab\nmethods focus on specific prior formulations and use prior-dependent conditions\nand analyses, and thus can not be generalized directly. In this paper, we\npropose a class of generic spike-and-slab priors and develop a unified\nframework to rigorously assess their theoretical properties. Technically, we\nprovide general conditions under which generic spike-and-slab priors can\nachieve the nearly-optimal posterior contraction rate and the model selection\nconsistency. Our results include those of Narisetty and He (2014) and Castillo\net al. (2015) as special cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 02:42:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:03:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Jiang", "Bai", ""], ["Sun", "Qiang", ""]]}, {"id": "1912.09002", "submitter": "Eduardo Mendes", "authors": "Ricardo P. Masini and Marcelo C. Medeiros and Eduardo F. Mendes", "title": "Regularized Estimation of High-Dimensional Vector AutoRegressions with\n  Weakly Dependent Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable advance in understanding the properties of sparse\nregularization procedures in high-dimensional models. In time series context,\nit is mostly restricted to Gaussian autoregressions or mixing sequences. We\nstudy oracle properties of LASSO estimation of weakly sparse\nvector-autoregressive models with heavy tailed, weakly dependent innovations\nwith virtually no assumption on the conditional heteroskedasticity. In contrast\nto current literature, our innovation process satisfy an $L^1$ mixingale type\ncondition on the centered conditional covariance matrices. This condition\ncovers $L^1$-NED sequences and strong ($\\alpha$-) mixing sequences as\nparticular examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 03:19:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 15:56:40 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 01:17:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""], ["Mendes", "Eduardo F.", ""]]}, {"id": "1912.09146", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Roland Fried and Markus Pauly", "title": "QANOVA: Quantile-based Permutation Methods For General Factorial Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population means and standard deviations are the most common estimands to\nquantify effects in factorial layouts. In fact, most statistical procedures in\nsuch designs are built towards inferring means or contrasts thereof. For more\nrobust analyses, we consider the population median, the interquartile range\n(IQR) and more general quantile combinations as estimands in which we formulate\nnull hypotheses and calculate compatible confidence regions. Based upon\nsimultaneous multivariate central limit theorems and corresponding resampling\nresults, we derive asymptotically correct procedures in general, potentially\nheteroscedastic, factorial designs with univariate endpoints. Special cases\ncover robust tests for the population median or the IQR in arbitrary crossed\none-, two- and higher-way layouts with potentially heteroscedastic error\ndistributions. In extensive simulations we analyze their small sample\nproperties and also conduct an illustrating data analysis comparing children's\nheight and weight from different countries.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:54:51 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 15:58:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Fried", "Roland", ""], ["Pauly", "Markus", ""]]}, {"id": "1912.09180", "submitter": "Jonas Moss", "authors": "Jonas Moss", "title": "Infinite Diameter Confidence Sets in Hedges' Publication Bias Model", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-analysis, the statistical analysis of results from separate studies, is\na fundamental building block of science. But the assumptions of classical\nmeta-analysis models are not satisfied whenever publication bias is present,\nwhich causes inconsistent parameter estimates. Hedges' selection function model\ntakes publication bias into account, but estimating and inferring with this\nmodel is tough. Using a generalized Gleser--Hwang theorem, we show there is no\nconfidence set of guaranteed finite diameter for the parameters of Hedges'\nselection model. This result provides a partial explanation for why inference\nwith Hedges' selection model is fraught with difficulties.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:20:17 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 13:11:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Moss", "Jonas", ""]]}, {"id": "1912.09187", "submitter": "Steffen Dereich", "authors": "Steffen Dereich and Sebastian Kassing", "title": "Central limit theorems for stochastic gradient descent with averaging\n  for stable manifolds", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we establish new central limit theorems for Ruppert-Polyak\naveraged stochastic gradient descent schemes. Compared to previous work we do\nnot assume that convergence occurs to an isolated attractor but instead allow\nconvergence to a stable manifold. On the stable manifold the target function is\nconstant and the oscillations in the tangential direction may be significantly\nlarger than the ones in the normal direction. As we show, one still recovers a\ncentral limit theorem with the same rates as in the case of isolated\nattractors. Here we consider step-sizes $\\gamma_n=n^{-\\gamma}$ with\n$\\gamma\\in(\\frac34,1)$, typically.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:37:55 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Dereich", "Steffen", ""], ["Kassing", "Sebastian", ""]]}, {"id": "1912.09334", "submitter": "Uwe Petersohn", "authors": "Uwe Petersohn, Thomas Dedek, Sandra Zimmer, Hans Biskupski", "title": "Causal statistical modeling and calculation of distribution functions of\n  classification features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical system models provide the basis for the examination of various\nsorts of distributions. Classification distributions are a very common and\nversatile form of statistics in e.g. real economic, social, and IT systems. The\nstatistical distributions of classification features can be applied in\ndetermining the a priori probabilities in Bayesian networks. We investigate a\nstatistical model of classification distributions based on finding the critical\npoint of a specialized form of entropy. A distribution function for\nclassification features is derived, with the two parameters $n_0$, minimal\nclass, and $\\bar{N}$, average number of classes. Efficient algorithms for the\ncomputation of the class probabilities and the approximation of real frequency\ndistributions are developed and applied to examples from different domains. The\nmethod is compared to established distributions like Zipf's law. The majority\nof examples can be approximated with a sufficient quality ($3-5\\%$).\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:13:09 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Petersohn", "Uwe", ""], ["Dedek", "Thomas", ""], ["Zimmer", "Sandra", ""], ["Biskupski", "Hans", ""]]}, {"id": "1912.09547", "submitter": "Jos\\'e A. D\\'iaz-Garc\\'ia", "authors": "Jos\\'e A. D\\'iaz-Garc\\'ia and Francisco J. Caro-Lopera", "title": "Singular matrix variate Birnbaum-Saunders distribution under elliptical\n  models", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work sets the matrix variate Birnbaum-Saunders theory in the context of\nsingular distributions and elliptical models. The so termed singular matrix\nvariate generalised Birnbaum-Saunders distribution is obtained with respect the\nHausdorff measure. Several basic properties and particular cases of this\ndistribution are also derived.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:19:14 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["D\u00edaz-Garc\u00eda", "Jos\u00e9 A.", ""], ["Caro-Lopera", "Francisco J.", ""]]}, {"id": "1912.09623", "submitter": "Rui Duan", "authors": "Rui Duan, Yang Ning, Yong Chen", "title": "Heterogeneity-aware and communication-efficient distributed statistical\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multicenter research, individual-level data are often protected against\nsharing across sites. To overcome the barrier of data sharing, many distributed\nalgorithms, which only require sharing aggregated information, have been\ndeveloped. The existing distributed algorithms usually assume the data are\nhomogeneously distributed across sites. This assumption ignores the important\nfact that the data collected at different sites may come from various\nsub-populations and environments, which can lead to heterogeneity in the\ndistribution of the data. Ignoring the heterogeneity may lead to erroneous\nstatistical inference. In this paper, we propose distributed algorithms which\naccount for the heterogeneous distributions by allowing site-specific nuisance\nparameters. The proposed methods extend the surrogate likelihood approach to\nthe heterogeneous setting by applying a novel density ratio tilting method to\nthe efficient score function. The proposed algorithms maintain the same\ncommunication cost as the existing communication-efficient algorithms. We\nestablish a non-asymptotic risk bound for the proposed distributed estimator\nand its limiting distribution in the two-index asymptotic setting which allows\nboth sample size per site and the number of sites to go to infinity. In\naddition, we show that the asymptotic variance of the estimator attains the\nCram\\'er-Rao lower bound when the number of sites is in rate smaller than the\nsample size at each site. Finally, we use simulation studies and a real data\napplication to demonstrate the validity and feasibility of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:14:07 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 21:01:58 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duan", "Rui", ""], ["Ning", "Yang", ""], ["Chen", "Yong", ""]]}, {"id": "1912.09767", "submitter": "Junlin Li", "authors": "Junlin Li", "title": "High-Dimensional Dynamic Systems Identification with Additional\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a unified analysis of the identification of dynamical\nsystems with low-rank constraints under high-dimensional scaling. This\nidentification problem for dynamic systems are challenging due to the intrinsic\ndependency of the data. To alleviate this problem, we first formulate this\nidentification problem into a multivariate linear regression problem with\nrow-sub-Gaussian measurement matrix using the more general input designs and\nthe independent repeated sampling schemes. We then propose a nuclear norm\nheuristic method that estimates the parameter matrix of dynamic system from a\nfew input-state data samples. Based on this, we can extend the existing\nresults. In this paper, we consider two scenarios. (i) In the noiseless\nscenario, nuclear-norm minimization is introduced for promoting low-rank. We\ndefine the notion of weak restricted isometry property, which is weaker than\nthe ordinary restricted isometry property, and show it holds with high\nprobability for the row-sub-Gaussian measurement matrix. Thereby, the\nrank-minimization matrix can be exactly recovered from finite number of data\nsamples. (ii) In the noisy scenario, a regularized framework involving nuclear\nnorm penalty is established. We give the notion of operator norm curvature\ncondition for the loss function, and show it holds for row-sub-Gaussian\nmeasurement matrix with high probability. Consequently, when specifying the\nsuitable choice of the regularization parameter, the operator norm error of the\noptimal solution of this program has a sharp bound given a finite amount of\ndata samples. This operator norm error bound is stronger than the ordinary\nFrobenius norm error bound obtained in the existing work.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:25:12 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Junlin", ""]]}, {"id": "1912.09776", "submitter": "Arta Cika Mrs", "authors": "Arta Cika, Mihai-Alin Badiu, and Justin P. Coon", "title": "Statistical Properties of Transmissions Subject to Rayleigh Fading and\n  Ornstein-Uhlenbeck Mobility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive closed-form expressions for significant statistical\nproperties of the link signal-to-noise ratio (SNR) and the separation distance\nin mobile ad hoc networks subject to Ornstein-Uhlenbeck (OU) mobility and\nRayleigh fading. In these systems, the SNR is a critical parameter as it\ndirectly influences link performance. In the absence of signal fading, the\ndistribution of the link SNR depends exclusively on the squared distance\nbetween nodes, which is governed by the mobility model. In our analysis, nodes\nmove randomly according to an Ornstein-Uhlenbeck process, using one tuning\nparameter to control the temporal dependency in the mobility pattern. We derive\na complete statistical description of the squared distance and show that it\nforms a stationary Markov process. Then, we compute closed-form expressions for\nthe probability density function (pdf), the cumulative distribution function\n(cdf), the bivariate pdf, and the bivariate cdf of the link SNR. Next, we\nintroduce small-scale fading, modelled by a Rayleigh random variable, and\nevaluate the pdf of the link SNR for rational path loss exponents. The validity\nof our theoretical analysis is verified by extensive simulation studies. The\nresults presented in this work can be used to quantify link uncertainty and\nevaluate stability in mobile ad hoc wireless systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:36:59 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Cika", "Arta", ""], ["Badiu", "Mihai-Alin", ""], ["Coon", "Justin P.", ""]]}, {"id": "1912.09834", "submitter": "Andr\\'e Schlichting", "authors": "Antonio Esposito and Francesco S. Patacchini and Andr\\'e Schlichting\n  and Dejan Slep\\v{c}ev", "title": "Nonlocal-interaction equation on graphs: gradient flow structure and\n  continuum limit", "comments": "46 pages. Minor revision with improved presentation and fixed typos", "journal-ref": "Arch Rational Mech Anal (2021)", "doi": "10.1007/s00205-021-01631-w", "report-no": null, "categories": "math.AP cs.NA math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dynamics driven by interaction energies on graphs. We introduce\ngraph analogues of the continuum nonlocal-interaction equation and interpret\nthem as gradient flows with respect to a graph Wasserstein distance. The\nparticular Wasserstein distance we consider arises from the graph analogue of\nthe Benamou-Brenier formulation where the graph continuity equation uses an\nupwind interpolation to define the density along the edges. While this approach\nhas both theoretical and computational advantages, the resulting distance is\nonly a quasi-metric. We investigate this quasi-metric both on graphs and on\nmore general structures where the set of \"vertices\" is an arbitrary positive\nmeasure. We call the resulting gradient flow of the nonlocal-interaction energy\nthe nonlocal nonlocal-interaction equation (NL$^2$IE). We develop the existence\ntheory for the solutions of the NL$^2$IE as curves of maximal slope with\nrespect to the upwind Wasserstein quasi-metric. Furthermore, we show that the\nsolutions of the NL$^2$IE on graphs converge as the empirical measures of the\nset of vertices converge weakly, which establishes a valuable\ndiscrete-to-continuum convergence result.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 14:14:27 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 19:09:54 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Esposito", "Antonio", ""], ["Patacchini", "Francesco S.", ""], ["Schlichting", "Andr\u00e9", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1912.09883", "submitter": "Elvira Di Nardo Prof.", "authors": "Elvira Di Nardo and Rosaria Simone", "title": "A Model-Based Fuzzy Analysis of Questionnaires", "comments": "22 pages, 9 figures, 8 Tables", "journal-ref": "Statistical methods and Applications, 28:187-215 (2019)", "doi": "10.1007/s10260-018-00443-9", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dealing with veracity of data analytics, fuzzy methods are more and more\nrelying on probabilistic and statistical techniques to underpin their\napplicability. Conversely, standard statistical models usually disregard to\ntake into account the inherent fuzziness of choices and this issue is\nparticularly worthy of note in customers' satisfaction surveys, since there are\ndifferent shades of evaluations that classical statistical tools fail to catch.\nGiven these motivations, the paper introduces a model-based fuzzy analysis of\nquestionnaire with sound statistical foundation, driven by the design of a\nhybrid method that sets in between fuzzy evaluation systems and statistical\nmodelling. The proposal is advanced on the basis of \\cub mixture models to\naccount for uncertainty in ordinal data analysis and moves within the general\nframework of Intuitionistic Fuzzy Set theory to allow membership,\nnon-membership, vagueness and accuracy assessments. Particular emphasis is\ngiven to defuzzification procedures that enable uncertainty measures also at an\naggregated level. An application to a survey run at the University of Naples\nFederico II about the evaluation of Orientation Services supports the efficacy\nof the proposal.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:30:43 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Di Nardo", "Elvira", ""], ["Simone", "Rosaria", ""]]}, {"id": "1912.10176", "submitter": "Miranda Holmes-Cerfon", "authors": "Miranda Holmes-Cerfon", "title": "Simulating sticky particles: A Monte Carlo method to sample a\n  stratification", "comments": null, "journal-ref": null, "doi": "10.1063/5.0019550", "report-no": null, "categories": "math.NA cond-mat.soft cond-mat.stat-mech cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in materials science and biology involve particles interacting\nwith strong, short-ranged bonds, that can break and form on experimental\ntimescales. Treating such bonds as constraints can significantly speed up\nsampling their equilibrium distribution, and there are several methods to\nsample probability distributions subject to fixed constraints. We introduce a\nMonte Carlo method to handle the case when constraints can break and form. More\ngenerally, the method samples a probability distribution on a stratification: a\ncollection of manifolds of different dimensions, where the lower-dimensional\nmanifolds lie on the boundaries of the higher-dimensional manifolds. We show\nseveral applications of the method in polymer physics, self-assembly of\ncolloids, and volume calculation in high dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 02:00:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 15:11:52 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 20:38:45 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 11:30:56 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Holmes-Cerfon", "Miranda", ""]]}, {"id": "1912.10238", "submitter": "Vinesh Solanki", "authors": "Vinesh Solanki, Patrick Rubin-Delanchy, Ian Gallagher", "title": "Persistent Homology of Graph Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular network models such as the mixed membership and standard stochastic\nblock model are known to exhibit distinct geometric structure when embedded\ninto $\\mathbb{R}^{d}$ using spectral methods. The resulting point cloud\nconcentrates around a simplex in the first model, whereas it separates into\nclusters in the second. By adopting the formalism of generalised random\ndot-product graphs, we demonstrate that both of these models, and different\nmixing regimes in the case of mixed membership, may be distinguished by the\npersistent homology of the underlying point distribution in the case of\nadjacency spectral embedding. Moreover, despite non-identifiability issues, we\nshow that the persistent homology of the support of the distribution and its\nsuper-level sets can be consistently estimated. As an application of our\nconsistency results, we provide a topological hypothesis test for\ndistinguishing the standard and mixed membership stochastic block models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 10:21:59 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Solanki", "Vinesh", ""], ["Rubin-Delanchy", "Patrick", ""], ["Gallagher", "Ian", ""]]}, {"id": "1912.10266", "submitter": "Patrick Michl", "authors": "Patrick Michl", "title": "Foundations of Structural Statistics: Topological Statistical Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Topological statistical theory provides the foundation for a modern\nmathematical reformulation of classical statistical theory: Structural\nStatistics emphasizes the structural assumptions that accompany distribution\nfamilies and the set of structure preserving transformations between them,\ngiven by their statistical morphisms. The resulting language is designed to\nintegrate complicated structured model spaces like deep-learning models and to\nclose the gap to topology and differential geometry. To preserve the\ncompatibility to classical statistics the language comprises corresponding\nconcepts for standard information criteria like sufficiency and completeness.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 13:28:59 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 19:26:45 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 13:12:40 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Michl", "Patrick", ""]]}, {"id": "1912.10529", "submitter": "Denis Chetverikov", "authors": "Victor Chernozhukov, Denis Chetverikov, Kengo Kato, Yuta Koike", "title": "Improved Central Limit Theorem and bootstrap approximations in high\n  dimensions", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Gaussian and bootstrap approximations to the\ndistribution of the max statistic in high dimensions. This statistic takes the\nform of the maximum over components of the sum of independent random vectors\nand its distribution plays a key role in many high-dimensional econometric\nproblems. Using a novel iterative randomized Lindeberg method, the paper\nderives new bounds for the distributional approximation errors. These new\nbounds substantially improve upon existing ones and simultaneously allow for a\nlarger class of bootstrap methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 20:38:16 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Kato", "Kengo", ""], ["Koike", "Yuta", ""]]}, {"id": "1912.10719", "submitter": "Eustasio del Barrio", "authors": "Eustasio del Barrio, Alberto Gonz\\'alez-Sanz and Marc Hallin", "title": "A note on the Regularity of Center-Outward Distribution and Quantile\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide sufficient conditions under which the center-outward distribution\nand quantile functions introduced in Chernozhukov et al.~(2017) and\nHallin~(2017) are homeomorphisms, thereby extending a recent result by Figalli\n\\cite{Fi2}. Our approach relies on Cafarelli's classical regularity theory for\nthe solutions of the Monge-Amp\\`ere equation, but has to deal with difficulties\nrelated with the unboundedness at the origin of the density of the spherical\nuniform reference measure. Our conditions are satisfied by probabillities on\nEuclidean space with a general (bounded or unbounded) convex support which are\nnot covered in~\\cite{Fi2}. We provide some additional results about\ncenter-outward distribution and quantile functions, including the fact that\nquantile sets exhibit some weak form of convexity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 10:30:59 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Gonz\u00e1lez-Sanz", "Alberto", ""], ["Hallin", "Marc", ""]]}, {"id": "1912.10754", "submitter": "Jaouad Mourtada", "authors": "Jaouad Mourtada", "title": "Exact minimax risk for linear least squares, and the lower tail of\n  sample covariance matrices", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first part of this paper is devoted to the decision-theoretic analysis of\nrandom-design linear prediction. It is known that, under boundedness\nconstraints on the response (and thus on regression coefficients), the minimax\nexcess risk scales, up to constants, as $\\sigma^2 d / n$ in dimension $d$ with\n$n$ samples and noise $\\sigma^2$. Here, we study the expected excess risk with\nrespect to the full linear class. We show that the ordinary least squares\nestimator is exactly minimax optimal in the well-specified case for every\ndistribution of covariates. Further, we express the minimax risk in terms of\nthe distribution of \\emph{statistical leverage scores} of individual samples.\nWe deduce a precise minimax lower bound of $\\sigma^2d/(n-d+1)$ for general\ncovariate distribution, which nearly matches the risk for Gaussian design. We\nthen obtain nonasymptotic upper bounds on the minimax risk for covariates that\nsatisfy a \"small ball\"-type regularity condition, which scale as\n$(1+o(1))\\sigma^2d/n$ as $d=o(n)$, both in the well-specified and misspecified\ncases.\n  Our main technical contribution is the study of the lower tail of the\nsmallest singular value of empirical covariance matrices around $0$. We\nestablish a lower bound on this lower tail, valid for any distribution in\ndimension $d \\geq 2$, together with a matching upper bound under a necessary\nregularity condition. Our proof relies on the PAC-Bayesian technique for\ncontrolling empirical processes, and extends an analysis of Oliveira devoted to\na different part of the lower tail. Equivalently, our upper bound shows that\nthe operator norm of the inverse sample covariance matrix has bounded $L^q$\nnorm up to $q \\asymp n$, and our lower bound implies that this exponent is\nunimprovable. Finally, we show that the regularity condition naturally holds\nfor independent coordinates.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 12:08:09 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:46:12 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mourtada", "Jaouad", ""]]}, {"id": "1912.10784", "submitter": "Jaouad Mourtada", "authors": "Jaouad Mourtada, St\\'ephane Ga\\\"iffas", "title": "An improper estimator with optimal excess risk in misspecified density\n  estimation and logistic regression", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a procedure for predictive conditional density estimation under\nlogarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator\nminimizes a new general excess risk bound for supervised statistical learning.\nOn standard examples, this bound scales as $d/n$ with $d$ the model dimension\nand $n$ the sample size, and critically remains valid under model\nmisspecification. Being an improper (out-of-model) procedure, SMP improves over\nwithin-model estimators such as the maximum likelihood estimator, whose excess\nrisk degrades under misspecification. Compared to approaches reducing to the\nsequential problem, our bounds remove suboptimal $\\log n$ factors, addressing\nan open problem from Gr\\\"unwald and Kotlowski for the considered models, and\ncan handle unbounded classes. For the Gaussian linear model, the predictions\nand risk bound of SMP are governed by leverage scores of covariates, nearly\nmatching the optimal risk in the well-specified case without conditions on the\nnoise variance or approximation error of the linear model. For logistic\nregression, SMP provides a non-Bayesian approach to calibration of\nprobabilistic predictions relying on virtual samples, and can be computed by\nsolving two logistic regressions. It achieves a non-asymptotic excess risk of\n$O ( (d + B^2R^2)/n )$, where $R$ bounds the norm of features and $B$ that of\nthe comparison parameter; by contrast, no within-model estimator can achieve\nbetter rate than $\\min( {B R}/{\\sqrt{n}}, {d e^{BR}}/{n} )$ in general. This\nprovides a computationally more efficient alternative to Bayesian approaches,\nwhich require approximate posterior sampling, thereby partly answering a\nquestion by Foster et al. (2018).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:11:54 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:04:49 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""]]}, {"id": "1912.10896", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR)", "title": "Properties of Chromy's sampling procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromy (1979) proposed a unequal probability sampling algorithm, which\nenables to select a sample in one pass of the sampling frame only. This is the\ndefault sequential method used in the SURVEYSELECT procedure of the SAS\nsoftware. In this article, we study the properties of Chromy sampling. We prove\nthat the Horvitz-Thompson is asymptotically normally distributed, and give an\nexplicit expression for the second-order inclusion probabilities. This makes it\npossible to estimate the variance unbiasedly for the randomized version of the\nmethod programmed in the SURVEYSELECT procedure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:57:44 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"]]}, {"id": "1912.11070", "submitter": "Ivan Panin I.", "authors": "Ivan I. Panin", "title": "Risk of estimators for Sobol' sensitivity indices based on metamodels", "comments": null, "journal-ref": "Electron. J. Statist. 15 (2021), no. 1, 235--281", "doi": "10.1214/20-EJS1793", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sobol' sensitivity indices allow to quantify the respective effects of random\ninput variables and their combinations on the variance of mathematical model\noutput. We focus on the problem of Sobol' indices estimation via a metamodeling\napproach where we replace the true mathematical model with a sample-based\napproximation to compute sensitivity indices. We propose a new method for\nindices quality control and obtain asymptotic and non-asymptotic risk bounds\nfor Sobol' indices estimates based on a general class of metamodels. Our\nanalysis is closely connected with the problem of nonparametric function\nfitting using the orthogonal system of functions in the random design setting.\nIt considers the relation between the metamodel quality and the error of the\ncorresponding estimator for Sobol' indices and shows the possibility of fast\nconvergence rates in the case of noiseless observations. The theoretical\nresults are complemented with numerical experiments for the approximations\nbased on multivariate Legendre and Trigonometric polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:20:53 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:34:12 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Panin", "Ivan I.", ""]]}, {"id": "1912.11071", "submitter": "Samuel Hopkins", "authors": "Yeshwanth Cherapanamjeri, Samuel B. Hopkins, Tarun Kathuria, Prasad\n  Raghavendra, Nilesh Tripuraneni", "title": "Algorithms for Heavy-Tailed Statistics: Regression, Covariance\n  Estimation, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficient algorithms for linear regression and covariance estimation\nin the absence of Gaussian assumptions on the underlying distributions of\nsamples, making assumptions instead about only finitely-many moments. We focus\non how many samples are needed to do estimation and regression with high\naccuracy and exponentially-good success probability.\n  For covariance estimation, linear regression, and several other problems,\nestimators have recently been constructed with sample complexities and rates of\nerror matching what is possible when the underlying distribution is Gaussian,\nbut algorithms for these estimators require exponential time. We narrow the gap\nbetween the Gaussian and heavy-tailed settings for polynomial-time estimators\nwith:\n  1. A polynomial-time estimator which takes $n$ samples from a random vector\n$X \\in R^d$ with covariance $\\Sigma$ and produces $\\hat{\\Sigma}$ such that in\nspectral norm $\\|\\hat{\\Sigma} - \\Sigma \\|_2 \\leq \\tilde{O}(d^{3/4}/\\sqrt{n})$\nw.p. $1-2^{-d}$. The information-theoretically optimal error bound is\n$\\tilde{O}(\\sqrt{d/n})$; previous approaches to polynomial-time algorithms were\nstuck at $\\tilde{O}(d/\\sqrt{n})$.\n  2. A polynomial-time algorithm which takes $n$ samples $(X_i,Y_i)$ where $Y_i\n= \\langle u,X_i \\rangle + \\varepsilon_i$ and produces $\\hat{u}$ such that the\nloss $\\|u - \\hat{u}\\|^2 \\leq O(d/n)$ w.p. $1-2^{-d}$ for any $n \\geq d^{3/2}\n\\log(d)^{O(1)}$. This (information-theoretically optimal) error is achieved by\ninefficient algorithms for any $n \\gg d$; previous polynomial-time algorithms\nsuffer loss $\\Omega(d^2/n)$ and require $n \\gg d^2$.\n  Our algorithms use degree-$8$ sum-of-squares semidefinite programs. We offer\npreliminary evidence that improving these rates of error in polynomial time is\nnot possible in the median of means framework our algorithms employ.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:22:48 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Hopkins", "Samuel B.", ""], ["Kathuria", "Tarun", ""], ["Raghavendra", "Prasad", ""], ["Tripuraneni", "Nilesh", ""]]}, {"id": "1912.11110", "submitter": "He Zhang", "authors": "He Zhang, John Harlim, Xiantao Li", "title": "Estimating linear response statistics using orthogonal polynomials: An\n  RKHS formulation", "comments": "31 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating linear response statistics under external\nperturbations using time series of unperturbed dynamics. Based on the\nfluctuation-dissipation theory, this problem is reformulated as an unsupervised\nlearning task of estimating a density function. We consider a nonparametric\ndensity estimator formulated by the kernel embedding of distributions with\n\"Mercer-type\" kernels, constructed based on the classical orthogonal\npolynomials defined on non-compact domains. While the resulting representation\nis analogous to Polynomial Chaos Expansion (PCE), the connection to the\nreproducing kernel Hilbert space (RKHS) theory allows one to establish the\nuniform convergence of the estimator and to systematically address a practical\nquestion of identifying the PCE basis for a consistent estimation. We also\nprovide practical conditions for the well-posedness of not only the estimator\nbut also of the underlying response statistics. Finally, we provide a\nstatistical error bound for the density estimation that accounts for the\nMonte-Carlo averaging over non-i.i.d time series and the biases due to a finite\nbasis truncation. This error bound provides a means to understand the\nfeasibility as well as limitation of the kernel embedding with Mercer-type\nkernels. Numerically, we verify the effectiveness of the estimator on two\nstochastic dynamics with known, yet, non-trivial equilibrium densities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:14:51 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 19:25:55 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 07:37:23 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Zhang", "He", ""], ["Harlim", "John", ""], ["Li", "Xiantao", ""]]}, {"id": "1912.11398", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu", "title": "An error bound for Lasso and Group Lasso in high dimensions", "comments": "arXiv admin note: text overlap with arXiv:1910.08880", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage recent advances in high-dimensional statistics to derive new L2\nestimation upper bounds for Lasso and Group Lasso in high-dimensions. For\nLasso, our bounds scale as $(k^*/n) \\log(p/k^*)$---$n\\times p$ is the size of\nthe design matrix and $k^*$ the dimension of the ground truth\n$\\boldsymbol{\\beta}^*$---and match the optimal minimax rate. For Group Lasso,\nour bounds scale as $(s^*/n) \\log\\left( G / s^* \\right) + m^* / n$---$G$ is the\ntotal number of groups and $m^*$ the number of coefficients in the $s^*$ groups\nwhich contain $\\boldsymbol{\\beta}^*$---and improve over existing results. We\nadditionally show that when the signal is strongly group-sparse, Group Lasso is\nsuperior to Lasso.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 14:08:26 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 18:05:03 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Dedieu", "Antoine", ""]]}, {"id": "1912.11436", "submitter": "Sivaraman Balakrishnan", "authors": "Larry Wasserman and Aaditya Ramdas and Sivaraman Balakrishnan", "title": "Universal Inference", "comments": "To appear in the Proceedings of the National Academy of Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method for constructing hypothesis tests and confidence\nsets that have finite sample guarantees without regularity conditions. We refer\nto such procedures as \"universal.\" The method is very simple and is based on a\nmodified version of the usual likelihood ratio statistic, that we call \"the\nsplit likelihood ratio test\" (split LRT). The method is especially appealing\nfor irregular statistical models. Canonical examples include mixture models and\nmodels that arise in shape-constrained inference. %mixture models and\nshape-constrained models are just two examples. Constructing tests and\nconfidence sets for such models is notoriously difficult. Typical inference\nmethods, like the likelihood ratio test, are not useful in these cases because\nthey have intractable limiting distributions. In contrast, the method we\nsuggest works for any parametric model and also for some nonparametric models.\nThe split LRT can also be used with profile likelihoods to deal with nuisance\nparameters, and it can also be run sequentially to yield anytime-valid\n$p$-values and confidence sequences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 16:52:08 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 18:48:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 22:38:36 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""], ["Balakrishnan", "Sivaraman", ""]]}, {"id": "1912.11583", "submitter": "Xiao Han", "authors": "Xiao Han, Qing Yang and Yingying Fan", "title": "Universal Rank Inference via Residual Subsampling with Application to\n  Large Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the precise rank is an important problem in many large-scale\napplications with matrix data exploiting low-rank plus noise models. In this\npaper, we suggest a universal approach to rank inference via residual\nsubsampling (RIRS) for testing and estimating rank in a wide family of models,\nincluding many popularly used network models such as the degree corrected mixed\nmembership model as a special case. Our procedure constructs a test statistic\nvia subsampling entries of the residual matrix after extracting the spiked\ncomponents. The test statistic converges in distribution to the standard normal\nunder the null hypothesis, and diverges to infinity with asymptotic probability\none under the alternative hypothesis. The effectiveness of RIRS procedure is\njustified theoretically, utilizing the asymptotic expansions of eigenvectors\nand eigenvalues for large random matrices recently developed in Fan et al.\n(2019a) and Fan et al. (2019b). The advantages of the newly suggested procedure\nare demonstrated through several simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 03:09:03 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 20:34:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Han", "Xiao", ""], ["Yang", "Qing", ""], ["Fan", "Yingying", ""]]}, {"id": "1912.11600", "submitter": "Artyom Kovalevskii", "authors": "Anik Chakrabarty, Mikhail Chebunin, Artyom Kovalevskii, Ilya Pupyshev,\n  Natalia Zakrevskaya, Qianqian Zhou", "title": "A statistical test for correspondence of texts to the Zipf-Mandelbrot\n  law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CL stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse correspondence of a text to a simple probabilistic model. The\nmodel assumes that the words are selected independently from an infinite\ndictionary. The probability distribution correspond to the Zipf---Mandelbrot\nlaw. We count sequentially the numbers of different words in the text and get\nthe process of the numbers of different words. Then we estimate\nZipf---Mandelbrot law parameters using the same sequence and construct an\nestimate of the expectation of the number of different words in the text. Then\nwe subtract the corresponding values of the estimate from the sequence and\nnormalize along the coordinate axes, obtaining a random process on a segment\nfrom 0 to 1. We prove that this process (the empirical text bridge) converges\nweakly in the uniform metric on $C (0,1)$ to a centered Gaussian process with\ncontinuous a.s. paths. We develop and implement an algorithm for approximate\ncalculation of eigenvalues of the covariance function of the limit Gaussian\nprocess, and then an algorithm for calculating the probability distribution of\nthe integral of the square of this process. We use the algorithm to analyze\nuniformity of texts in English, French, Russian and Chinese.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 05:59:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Chakrabarty", "Anik", ""], ["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""], ["Pupyshev", "Ilya", ""], ["Zakrevskaya", "Natalia", ""], ["Zhou", "Qianqian", ""]]}, {"id": "1912.11652", "submitter": "Yuru Zhu", "authors": "Shinyuu Lee and Yuru Zhu", "title": "Confounder Selection via Support Intersection", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confounding matters in almost all observational studies that focus on\ncausality. In order to eliminate bias caused by connfounders, oftentimes a\nsubstantial number of features need to be collected in the analysis. In this\ncase, large p small n problem can arise and dimensional reduction technique is\nrequired. However, the traditional variable selection methods which focus on\nprediction are problematic in this setting. Throughout this paper, we analyze\nthis issue in detail and assume the sparsity of confounders which is different\nfrom the previous works. Under this assumption we propose several variable\nselection methods based on support intersection to pick out the confounders.\nAlso we discussed the different approaches for estimation of causal effect and\nunconfoundedness test. To aid in our description, finally we provide numerical\nsimulations to support our claims and compare to common heuristic methods, as\nwell as applications on real dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 12:23:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lee", "Shinyuu", ""], ["Zhu", "Yuru", ""]]}, {"id": "1912.11693", "submitter": "Tze Siong Lau", "authors": "Tze Siong Lau, Wee Peng Tay", "title": "Asymptotically Optimal Sampling Policy for Quickest Change Detection\n  with Observation-Switching Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickest change detection (QCD) in a signal where\nits observations are obtained using a set of actions, and switching from one\naction to another comes with a cost. The objective is to design a stopping rule\nconsisting of a sampling policy to determine the sequence of actions used to\nobserve the signal and a stopping time to quickly detect for the change,\nsubject to a constraint on the average observation-switching cost. We propose\nan open-loop sampling policy of finite window size and a generalized likelihood\nratio (GLR) Cumulative Sum (CuSum) stopping time for the QCD problem. We show\nthat the GLR CuSum stopping time is asymptotically optimal with a properly\ndesigned sampling policy and formulate the design of this sampling policy as a\nquadratic programming problem. We prove that it is sufficient to consider\npolicies of window size not more than one when designing policies of finite\nwindow size and propose several algorithms that solve this optimization problem\nwith theoretical guarantees. For observation-dependent policies, we propose a\n$2$-threshold stopping time and an observation-dependent sampling policy. We\npresent a method to design the observation-dependent sampling policy based on\nopen-loop sampling policies. Finally, we apply our approach to the problem of\nQCD of a partially observed graph signal and empirically demonstrate the\nperformance of our proposed stopping times.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 17:09:13 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 17:53:32 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 19:09:03 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 18:30:05 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Lau", "Tze Siong", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1912.11832", "submitter": "Teppei Ogihara", "authors": "Teppei Ogihara", "title": "Misspecified diffusion models with high-frequency observations and an\n  application to neural networks", "comments": "45 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic theory of misspecified models for diffusion processes\nwith noisy nonsynchronous observations. Unlike with correctly specified models,\nthe original maximum-likelihood-type estimator has an asymptotic bias under the\nmisspecified setting and fails to achieve an optimal rate of convergence. To\naddress this, we consider a new quasi-likelihood function that arrows\nconstructing a maximum-likelihood-type estimator that achieves the optimal rate\nof convergence. Study of misspecified models enables us to apply\nmachine-learning techniques to the maximum-likelihood approach. With these\ntechniques, we can efficiently study the microstructure of a stock market by\nusing rich information of high-frequency data. Neural networks have\nparticularly good compatibility with the maximum-likelihood approach, so we\nwill consider an example of using a neural network for simulation studies and\nempirical analysis of high-frequency data from the Tokyo Stock Exchange. We\ndemonstrate that the neural network outperforms polynomial models in volatility\npredictions for major stocks in Tokyo Stock Exchange.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 10:54:55 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ogihara", "Teppei", ""]]}, {"id": "1912.11914", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Inverses of Matern Covariances on Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a study of the aliased spectral densities of Mat\\'ern covariance\nfunctions on a regular grid of points, providing clarity on the properties of a\npopular approximation based on stochastic partial differential equations; while\nothers have shown that it can approximate the covariance function well, we find\nthat it assigns too much power at high frequencies and does not provide\nincreasingly accurate approximations to the inverse as the grid spacing goes to\nzero, except in the one-dimensional exponential covariance case. We provide\nnumerical results to support our theory, and in a simulation study, we\ninvestigate the implications for parameter estimation, finding that the SPDE\napproximation tends to overestimate spatial range parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 18:36:06 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 14:44:05 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 19:22:37 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1912.11943", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec and Cun-Hui Zhang", "title": "Second order Poincar\\'e inequalities and de-biasing arbitrary convex\n  regularizers when $p/n \\to \\gamma$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Central Limit Theorem (CLT) is developed for random variables of the\nform $\\xi=z^\\top f(z) - \\text{div} f(z)$ where $z\\sim N(0,I_n)$. The normal\napproximation is proved to hold when the squared norm of $f(z)$ dominates the\nsquared Frobenius norm of $\\nabla f(z)$ in expectation.\n  Applications of this CLT are given for the asymptotic normality of de-biased\nestimators in linear regression with correlated design and convex penalty in\nthe regime $p/n\\to \\gamma\\in (0,{\\infty})$. For the estimation of linear\nfunctions $\\langle a_0,\\beta\\rangle$ of the unknown coefficient vector $\\beta$,\nthis analysis leads to asymptotic normality of the de-biased estimate for most\nnormalized directions $a_0$, where \"most\" is quantified in a precise sense.\nThis asymptotic normality holds for any coercive convex penalty if $\\gamma<1$\nand for any strongly convex penalty if $\\gamma\\ge 1$. In particular the penalty\nneeds not be separable or permutation invariant. For the group Lasso, a simple\ncondition is given that grants asymptotic normality for a fixed direction\n$a_0$. By allowing arbitrary regularizers, the results vastly broaden the scope\nof applicability of de-biasing methodologies to obtain confidence intervals in\nhigh-dimensions.\n  In the absence of strong convexity for p > n, asymptotic normality of the\nde-biased estimate is obtained under additional conditions that are naturally\nsatisfied by the Lasso and the group Lasso.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 22:48:51 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 02:15:27 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 02:23:15 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bellec", "Pierre C", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1912.11965", "submitter": "Cheng Zhang", "authors": "Zhongyang Li, Fei Lu, Mauro Maggioni, Sui Tang, Cheng Zhang", "title": "On the identifiability of interaction functions in systems of\n  interacting particles", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA math.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a fundamental issue in the nonparametric inference for systems of\ninteracting particles: the identifiability of the interaction functions. We\nprove that the interaction functions are identifiable for a class of\nfirst-order stochastic systems, including linear systems with general initial\nlaws and nonlinear systems with stationary distributions. We show that a\ncoercivity condition is sufficient for identifiability and becomes necessary\nwhen the number of particles approaches infinity. The coercivity is equivalent\nto the strict positivity of related integral operators, which we prove by\nshowing that their integral kernels are strictly positive definite by using\nM\\\"untz type theorems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 02:41:44 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:27:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Li", "Zhongyang", ""], ["Lu", "Fei", ""], ["Maggioni", "Mauro", ""], ["Tang", "Sui", ""], ["Zhang", "Cheng", ""]]}, {"id": "1912.12150", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Sambit Panda, Joshua T. Vogelstein", "title": "The Chi-Square Test of Distance Correlation", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:16:40 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 15:08:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 21:53:47 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 21:35:39 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 18:09:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Cencheng", ""], ["Panda", "Sambit", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1912.12213", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Victor Chernozhukov, Whitney K. Newey, Yinchu Zhu", "title": "Minimax Semiparametric Learning With Approximate Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the ability and means to root-n consistently and\nefficiently estimate linear, mean square continuous functionals of a high\ndimensional, approximately sparse regression. Such objects include a wide\nvariety of interesting parameters such as the covariance between two regression\nresiduals, a coefficient of a partially linear model, an average derivative,\nand the average treatment effect. We give lower bounds on the convergence rate\nof estimators of such objects and find that these bounds are substantially\nlarger than in a low dimensional, semiparametric setting. We also give\nautomatic debiased machine learners that are $1/\\sqrt{n}$ consistent and\nasymptotically efficient under minimal conditions. These estimators use no\ncross-fitting or a special kind of cross-fitting to attain efficiency with\nfaster than $n^{-1/4}$ convergence of the regression. This rate condition is\nsubstantially weaker than the product of convergence rates of two functions\nbeing faster than $1/\\sqrt{n},$ as required for many other debiased machine\nlearners.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 16:13:21 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 15:16:10 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 11:53:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bradic", "Jelena", ""], ["Chernozhukov", "Victor", ""], ["Newey", "Whitney K.", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1912.12462", "submitter": "Len Bos", "authors": "L. Bos and N. Levenberg and J. Ortega-Cerda", "title": "Optimal Polynomial Prediction Measures and Extremal Polynomial Growth", "comments": null, "journal-ref": null, "doi": "10.1007/s00365-020-09522-1", "report-no": null, "categories": "math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the problem of finding the measure supported on a compact subset\nK of the complex plane such that the variance of the least squares predictor by\npolynomials of degree at most n at a point exterior to K is a minimum, is\nequivalent to the problem of finding the polynomial of degree at most n,\nbounded by 1 on K with extremal growth at this external point. We use this to\nfind the polynomials of extremal growth for the interval [-1,1] at a purely\nimaginary point. The related problem on the extremal growth of real polynomials\nwas studied by Erd\\H{o}s in 1947.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 14:35:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Bos", "L.", ""], ["Levenberg", "N.", ""], ["Ortega-Cerda", "J.", ""]]}, {"id": "1912.12694", "submitter": "Vladimir Spokoiny", "authors": "Vladimir Spokoiny", "title": "Bayesian inference for nonlinear inverse problems", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.06028", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are actively used for parameter identification and\nuncertainty quantification when solving nonlinear inverse problems with random\nnoise. However, there are only few theoretical results justifying the Bayesian\napproach. Recent papers, see e.g. \\cite{Nickl2017,lu2017bernsteinvon} and\nreferences therein, illustrate the main difficulties and challenges in studying\nthe properties of the posterior distribution in the nonparametric setup. This\npaper offers a new approach for study the frequentist properties of the\nnonparametric Bayes procedures. The idea of the approach is to relax the\nnonlinear structural equation by introducing an auxiliary functional parameter\nand replacing the structural equation with a penalty and by imposing a prior on\nthe auxiliary parameter. For the such extended model, we state sharp bounds on\nposterior concentration and on the accuracy of the penalized MLE and on\nGaussian approximation of the posterior, and a number of further results. All\nthe bounds are given in terms of effective dimension, and we show that the\nproposed calming device does not significantly affect this value.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 16:46:04 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 11:30:31 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Spokoiny", "Vladimir", ""]]}, {"id": "1912.12776", "submitter": "Christian Houdr\\'e", "authors": "Olivier Bousquet and Christian Houdr\\'e", "title": "Iterated Jackknives and Two-Sided Variance Inequalities", "comments": "This paper has appeared in High Dimensional Probability VIII, Part of\n  the Progress in Probability book series (PRPR, volume 74), Pages 33-40. Some\n  typos have been corrected and slight corrections made", "journal-ref": null, "doi": "10.1007/978-3-030-26391-1_4", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the variance of a function of $n$ independent random variables\nand provide new inequalities which, in particular, extend previous results\nobtained for symmetric functions in the i.i.d.~setting. For instance, we obtain\nvarious upper and lower variance bounds based on iterated jackknives statistics\nthat can be considered as generalizations of the Efron-Stein inequality.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 01:36:19 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bousquet", "Olivier", ""], ["Houdr\u00e9", "Christian", ""]]}, {"id": "1912.12783", "submitter": "Shixin Xu", "authors": "Yu Chen, Jin Cheng, Arvind Gupta, Huaxiong Huang, Shixin Xu", "title": "Numerical Method for Parameter Inference of Nonlinear ODEs with Partial\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference of dynamical systems is a challenging task faced by many\nresearchers and practitioners across various fields. In many applications, it\nis common that only limited variables are observable. In this paper, we propose\na method for parameter inference of a system of nonlinear coupled ODEs with\npartial observations. Our method combines fast Gaussian process based gradient\nmatching (FGPGM) and deterministic optimization algorithms. By using initial\nvalues obtained by Bayesian steps with low sampling numbers, our deterministic\noptimization algorithm is both accurate and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 02:23:25 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Yu", ""], ["Cheng", "Jin", ""], ["Gupta", "Arvind", ""], ["Huang", "Huaxiong", ""], ["Xu", "Shixin", ""]]}, {"id": "1912.12923", "submitter": "Shi-Ju Ran", "authors": "Shi-Ju Ran", "title": "Bayesian Tensor Network with Polynomial Complexity for Probabilistic\n  Machine Learning", "comments": "7 pages, 5 figures; in the second version, results of the BTN with a\n  new structure were added; other modifications including the formulation of\n  Bayes' equation in tensor forms were made", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.str-el cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that describing or calculating the conditional probabilities of\nmultiple events is exponentially expensive. In this work, Bayesian tensor\nnetwork (BTN) is proposed to efficiently capture the conditional probabilities\nof multiple sets of events with polynomial complexity. BTN is a directed\nacyclic graphical model that forms a subset of TN. To testify its validity for\nexponentially many events, BTN is implemented to the image recognition, where\nthe classification is mapped to capturing the conditional probabilities in an\nexponentially large sample space. Competitive performance is achieved by the\nBTN with simple tree network structures. Analogous to the tensor network\nsimulations of quantum systems, the validity of the simple-tree BTN implies an\n``area law'' of fluctuations in image recognition problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:37:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 12:36:54 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ran", "Shi-Ju", ""]]}, {"id": "1912.12937", "submitter": "Xiucai Ding", "authors": "Xiucai Ding, Zhou Zhou", "title": "Globally Optimal And Adaptive Short-Term Forecast of Locally Stationary\n  Time Series And A Test for Its Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the evolution of complex systems is one of the grand challenges\nof modern data science. The fundamental difficulty lies in understanding the\nstructure of the observed stochastic process. In this paper, we show that every\nuniformly-positive-definite-in-covariance and sufficiently short-range\ndependent non-stationary and nonlinear time series can be well approximated\nglobally by an auto-regressive process of slowly diverging order. When linear\nprediction with ${\\cal L}^2$ loss is concerned, the latter result facilitates a\nunified globally-optimal short-term forecasting theory for a wide class of\nlocally stationary time series asymptotically. A nonparametric sieve method is\nproposed to globally and adaptively estimate the optimal forecasting\ncoefficient functions and the associated mean squared error of forecast. An\nadaptive stability test is proposed to check whether the optimal forecasting\ncoefficients are time-varying, a frequently-encountered question for\npractitioners and researchers of time series. Furthermore, partial\nauto-correlation functions (PACF) of general non-stationary time series are\nstudied and used as a visual tool to explore the linear dependence structure of\nsuch series. We use extensive numerical simulations and two real data examples\nto illustrate the usefulness of our results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:25:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ding", "Xiucai", ""], ["Zhou", "Zhou", ""]]}, {"id": "1912.13027", "submitter": "Galen Reeves", "authors": "Galen Reeves, Jiaming Xu, Ilias Zadik", "title": "All-or-Nothing Phenomena: From Single-Letter to High Dimensions", "comments": "This paper was presented at the 2019 IEEE International Workshop on\n  Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP),\n  Guadeloupe, French West Indies", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression problem of estimating a $p$-dimensional\nvector $\\beta$ from $n$ observations $Y = X \\beta + W$, where $\\beta_j\n\\stackrel{\\text{i.i.d.}}{\\sim} \\pi$ for a real-valued distribution $\\pi$ with\nzero mean and unit variance, $X_{ij} \\stackrel{\\text{i.i.d.}}{\\sim}\n\\mathcal{N}(0,1)$, and $W_i\\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\n\\sigma^2)$. In the asymptotic regime where $n/p \\to \\delta$ and $ p/ \\sigma^2\n\\to \\mathsf{snr}$ for two fixed constants $\\delta, \\mathsf{snr}\\in (0, \\infty)$\nas $p \\to \\infty$, the limiting (normalized) minimum mean-squared error (MMSE)\nhas been characterized by the MMSE of an associated single-letter (additive\nGaussian scalar) channel.\n  In this paper, we show that if the MMSE function of the single-letter channel\nconverges to a step function, then the limiting MMSE of estimating $\\beta$ in\nthe linear regression problem converges to a step function which jumps from $1$\nto $0$ at a critical threshold. Moreover, we establish that the limiting\nmean-squared error of the (MSE-optimal) approximate message passing algorithm\nalso converges to a step function with a larger threshold, providing evidence\nfor the presence of a computational-statistical gap between the two thresholds.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 17:40:31 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Reeves", "Galen", ""], ["Xu", "Jiaming", ""], ["Zadik", "Ilias", ""]]}, {"id": "1912.13088", "submitter": "Peng Liao", "authors": "Peng Liao, Predrag Klasnja, Susan Murphy", "title": "Off-Policy Estimation of Long-Term Average Outcomes with Applications to\n  Mobile Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent advancements in wearables and sensing technology, health\nscientists are increasingly developing mobile health (mHealth) interventions.\nIn mHealth interventions, mobile devices are used to deliver treatment to\nindividuals as they go about their daily lives. These treatments are generally\ndesigned to impact a near time, proximal outcome such as stress or physical\nactivity. The mHealth intervention policies, often called just-in-time adaptive\ninterventions, are decision rules that map an individual's current state (e.g.,\nindividual's past behaviors as well as current observations of time, location,\nsocial activity, stress and urges to smoke) to a particular treatment at each\nof many time points. The vast majority of current mHealth interventions deploy\nexpert-derived policies. In this paper, we provide an approach for conducting\ninference about the performance of one or more such policies using historical\ndata collected under a possibly different policy. Our measure of performance is\nthe average of proximal outcomes over a long time period should the particular\nmHealth policy be followed. We provide an estimator as well as confidence\nintervals. This work is motivated by HeartSteps, an mHealth physical activity\nintervention.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:22:21 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 19:41:48 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 18:00:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Liao", "Peng", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan", ""]]}, {"id": "1912.13185", "submitter": "Yiren Wang", "authors": "Yiren Wang, Dimitris N. Politis", "title": "Model-free Bootstrap for a General Class of Stationary Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model-free bootstrap procedure for a general class of stationary time\nseries is introduced. The theoretical framework is established, showing\nasymptotic validity of bootstrap confidence intervals for many statistics of\ninterest. In addition, asymptotic validity of one-step ahead bootstrap\nprediction intervals is also demonstrated. Finite-sample experiments are\nconducted to empirically confirm the performance of the new method, and to\ncompare with\n  popular methods such as the block bootstrap and the autoregressive (AR)-sieve\nbootstrap.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 05:58:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Wang", "Yiren", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1912.13188", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah and Aarti Singh", "title": "On Testing for Biases in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of biases in scholarly research, specifically, in peer\nreview. There is a long standing debate on whether exposing author identities\nto reviewers induces biases against certain groups, and our focus is on\ndesigning tests to detect the presence of such biases. Our starting point is a\nremarkable recent work by Tomkins, Zhang and Heavlin which conducted a\ncontrolled, large-scale experiment to investigate existence of biases in the\npeer reviewing of the WSDM conference. We present two sets of results in this\npaper. The first set of results is negative, and pertains to the statistical\ntests and the experimental setup used in the work of Tomkins et al. We show\nthat the test employed therein does not guarantee control over false alarm\nprobability and under correlations between relevant variables coupled with any\nof the following conditions, with high probability, can declare a presence of\nbias when it is in fact absent: (a) measurement error, (b) model mismatch, (c)\nreviewer calibration. Moreover, we show that the setup of their experiment may\nitself inflate false alarm probability if (d) bidding is performed in non-blind\nmanner or (e) popular reviewer assignment procedure is employed. Our second set\nof results is positive and is built around a novel approach to testing for\nbiases that we propose. We present a general framework for testing for biases\nin (single vs. double blind) peer review. We then design hypothesis tests that\nunder minimal assumptions guarantee control over false alarm probability and\nnon-trivial power even under conditions (a)--(c) as well as propose an\nalternative experimental setup which mitigates issues (d) and (e). Finally, we\nshow that no statistical test can improve over the non-parametric tests we\nconsider in terms of the assumptions required to control for the false alarm\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:17:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""]]}, {"id": "1912.13289", "submitter": "Kenichiro Sato", "authors": "Kenichiro Sato and Sumio Watanabe", "title": "Bayesian Generalization Error of Poisson Mixture and Simplex Vandermonde\n  Matrix Type Singularity", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Poisson mixture is one of the practically important models in computer\nscience, biology, and sociology. However, the theoretical property has not been\nstudied because the posterior distribution can not be approximated by any\nnormal distribution. Such a model is called singular and it is known that Real\nLog Canonical Threshold (RLCT) is equal to the coefficient of the\nasymptotically main term of the Bayesian generalization error. In this paper,\nwe derive RLCT of a simplex Vandermonde matrix type singularity which is equal\nto that of a Poisson mixture in general cases.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:32:09 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sato", "Kenichiro", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1912.13292", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ruodu Wang", "title": "True and false discoveries with e-values", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of this paper is multiple hypothesis testing based on e-values,\nwhich are Bayes factors stripped of their Bayesian content. Using e-values\ninstead of p-values, which are standard in this area, leads to simple and\nefficient procedures that control the number of false discoveries under\narbitrary dependence of the base e-values. We prove an optimality result for\nour main procedure and demonstrate advantages of our methods over standard\nmethods using simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 12:48:57 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 16:28:07 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Vovk", "Vladimir", ""], ["Wang", "Ruodu", ""]]}, {"id": "1912.13463", "submitter": "Kaizheng Wang", "authors": "Kaizheng Wang", "title": "Some compact notations for concentration inequalities and user-friendly\n  results", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents compact notations for concentration inequalities and\nconvenient results to streamline probabilistic analysis. The new expressions\ndescribe the typical sizes and tails of random variables, allowing for simple\noperations without heavy use of inessential constants. They bridge classical\nasymptotic notations and modern non-asymptotic tail bounds together. Examples\nof different kinds demonstrate their efficacy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:03:19 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:57:14 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kaizheng", ""]]}]