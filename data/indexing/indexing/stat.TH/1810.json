[{"id": "1810.00274", "submitter": "Jian Kang", "authors": "Qingpo Cai, Jian Kang, Tianwei Yu", "title": "Bayesian network marker selection via the thresholded graph Laplacian\n  Gaussian prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting informative nodes over large-scale networks becomes increasingly\nimportant in many research areas. Most existing methods focus on the local\nnetwork structure and incur heavy computational costs for the large-scale\nproblem. In this work, we propose a novel prior model for Bayesian network\nmarker selection in the generalized linear model (GLM) framework: the\nThresholded Graph Laplacian Gaussian (TGLG) prior, which adopts the graph\nLaplacian matrix to characterize the conditional dependence between neighboring\nmarkers accounting for the global network structure. Under mild conditions, we\nshow the proposed model enjoys the posterior consistency with a diverging\nnumber of edges and nodes in the network. We also develop a Metropolis-adjusted\nLangevin algorithm (MALA) for efficient posterior computation, which is\nscalable to large-scale networks. We illustrate the superiorities of the\nproposed method compared with existing alternatives via extensive simulation\nstudies and an analysis of the breast cancer gene expression dataset in the\nCancer Genome Atlas (TCGA).\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 22:53:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Cai", "Qingpo", ""], ["Kang", "Jian", ""], ["Yu", "Tianwei", ""]]}, {"id": "1810.00289", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "F. Bertrand and M. Maumy-Bertrand", "title": "A Sheet of Maple to Compute Second-Order Edgeworth Expansions and\n  Related Quantities of any Function of the Mean of an iid Sample of an\n  Absolutely Continuous Distribution", "comments": "20 pages, 8 figures, code snippet", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a completely automated Maple ($\\geqslant 15$) worksheet for\nderiving Edgeworth and Cornish-Fisher expansions as well as the acceleration\nconstant of the bootstrap bias-corrected and accelerated technique. It is valid\nfor non-parametric or parametric bootstrap, of any (studentized) statistics\nthat is -a regular enough- function of the mean of an iid sample of an\nabsolutely continuous distribution.\n  This worksheet allowed us to point out one error in the second-order\nCornish-Fisher expansion of the studentized mean stated in Theorem 13.5 by Das\nGupta in [8, p. 194] as well as lay the stress on the influence of the slight\nchange of the normalizing constant when computing the second-order Edgeworth\nand Cornish-Fisher expansions of the t-distribution as stated in Theorem 11.4.2\nby Lehman and Romano in [14, p. 460].\n  In addition, we successfully applied the worksheet to a complex maximum\nlikelihood estimator as a first step to derive more accurate confidence\nintervals in order to enhance quality controls. The worksheet also features\nexport of Maple results into R code. In addition, we provide R code to plot\nthese expansions as well as their increasing rearrangements. All these\nsupplemental materials are available upon request.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 01:26:19 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bertrand", "F.", ""], ["Maumy-Bertrand", "M.", ""]]}, {"id": "1810.00297", "submitter": "Bamdad Hosseini Dr.", "authors": "Bamdad Hosseini and James E Johndrow", "title": "Spectral gaps and error estimates for infinite-dimensional\n  Metropolis-Hastings with non-Gaussian priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of Metropolis-Hastings algorithms for target measures that\nare absolutely continuous with respect to a large class of non-Gaussian prior\nmeasures on Banach spaces. The algorithm is shown to have a spectral gap in a\nWasserstein-like semimetric weighted by a Lyapunov function. A number of error\nbounds are given for computationally tractable approximations of the algorithm\nincluding bounds on the closeness of Ces\\'{a}ro averages and other pathwise\nquantities via perturbation theory. Several applications illustrate the breadth\nof problems to which the results apply such as discretization by Galerkin-type\nprojections and approximate simulation of the proposal.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 02:29:08 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 22:51:00 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Hosseini", "Bamdad", ""], ["Johndrow", "James E", ""]]}, {"id": "1810.00315", "submitter": "Xiaodong Li", "authors": "Xiaodong Li, Yudong Chen, Jiaming Xu", "title": "Convex Relaxation Methods for Community Detection", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys recent theoretical advances in convex optimization\napproaches for community detection. We introduce some important theoretical\ntechniques and results for establishing the consistency of convex community\ndetection under various statistical models. In particular, we discuss the basic\ntechniques based on the primal and dual analysis. We also present results that\ndemonstrate several distinctive advantages of convex community detection,\nincluding robustness against outlier nodes, consistency under weak\nassortativity, and adaptivity to heterogeneous degrees.\n  This survey is not intended to be a complete overview of the vast literature\non this fast-growing topic. Instead, we aim to provide a big picture of the\nremarkable recent development in this area and to make the survey accessible to\na broad audience. We hope that this expository article can serve as an\nintroductory guide for readers who are interested in using, designing, and\nanalyzing convex relaxation methods in network analysis.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 04:32:32 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Li", "Xiaodong", ""], ["Chen", "Yudong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1810.00412", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "Distributed linear regression by averaging", "comments": "V2 adds a new section on iterative averaging methods, adds\n  applications of the calculus of deterministic equivalents, and reorganizes\n  the paper", "journal-ref": "Annals of Statistics, 2020+", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical learning problems arise commonly when dealing with\nlarge datasets. In this setup, datasets are partitioned over machines, which\ncompute locally, and communicate short messages. Communication is often the\nbottleneck. In this paper, we study one-step and iterative weighted parameter\naveraging in statistical linear models under data parallelism. We do linear\nregression on each machine, send the results to a central server, and take a\nweighted average of the parameters. Optionally, we iterate, sending back the\nweighted average and doing local ridge regressions centered at it. How does\nthis work compared to doing linear regression on the full data? Here we study\nthe performance loss in estimation, test error, and confidence interval length\nin high dimensions, where the number of parameters is comparable to the\ntraining data size. We find the performance loss in one-step weighted\naveraging, and also give results for iterative averaging. We also find that\ndifferent problems are affected differently by the distributed framework.\nEstimation error and confidence interval length increase a lot, while\nprediction error increases much less. We rely on recent results from random\nmatrix theory, where we develop a new calculus of deterministic equivalents as\na tool of broader interest.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:59:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:33:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1810.00552", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Ayanendranath Basu", "title": "Power and Level Robustness of A Composite Hypothesis Testing under\n  Independent Non-Homogeneous Data", "comments": "Pre-print, Under review", "journal-ref": "Statistics & Probability Letters (2019)", "doi": "10.1016/j.spl.2018.12.002", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust tests of general composite hypothesis under non-identically\ndistributed observations is always a challenge. Ghosh and Basu (2018,\nStatistica Sinica, 28, 1133--1155) have proposed a new class of test statistics\nfor such problems based on the density power divergence, but their robustness\nwith respect to the size and power are not studied in detail. This note fills\nthis gap by providing a rigorous derivation of power and level influence\nfunctions of these tests to theoretically justify their robustness.\nApplications to the fixed-carrier linear regression model are also provided\nwith empirical illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 07:00:05 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1810.00579", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang", "title": "On valid descriptive inference from non-probability sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the conditions under which descriptive inference can be based\ndirectly on the observed distribution in a non-probability sample, under both\nthe super-population and quasi-randomisation modelling approaches. Review of\nexisting estimation methods reveals that the traditional formulation of these\nconditions may be inadequate due to potential issues of under-coverage or\nheterogeneous mean beyond the assumed model. We formulate unifying conditions\nthat are applicable to both type of modelling approaches. The difficulties of\nempirically validating the required conditions are discussed, as well as valid\ninference approaches using supplementary probability sampling. The key message\nis that probability sampling may still be necessary in some situations, in\norder to ensure the validity of descriptive inference, but it can be much less\nresource-demanding provided the presence of a big non-probability sample.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 08:35:41 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Zhang", "Li-Chun", ""]]}, {"id": "1810.00739", "submitter": "Ryan Martin", "authors": "Chang Liu, Yue Yang, Howard Bondell, Ryan Martin", "title": "Bayesian inference in high-dimensional linear models using an empirical\n  correlation-adaptive prior", "comments": "25 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a high-dimensional linear regression model, we propose the\nuse of an empirical correlation-adaptive prior that makes use of information in\nthe observed predictor variable matrix to adaptively address high collinearity,\ndetermining if parameters associated with correlated predictors should be\nshrunk together or kept apart. Under suitable conditions, we prove that this\nempirical Bayes posterior concentrates around the true sparse parameter at the\noptimal rate asymptotically. A simplified version of a shotgun stochastic\nsearch algorithm is employed to implement the variable selection procedure, and\nwe show, via simulation experiments across different settings and a real-data\napplication, the favorable performance of the proposed method compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:51:26 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Chang", ""], ["Yang", "Yue", ""], ["Bondell", "Howard", ""], ["Martin", "Ryan", ""]]}, {"id": "1810.00828", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Michael I. Jordan, Martin J.\n  Wainwright, Bin Yu", "title": "Singularity, Misspecification, and the Convergence Rate of EM", "comments": "63 pages, 12 figures. The first three authors contributed equally to\n  this work. To appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of recent work has analyzed the behavior of the\nExpectation-Maximization (EM) algorithm in the well-specified setting, in which\nthe population likelihood is locally strongly concave around its maximizing\nargument. Examples include suitably separated Gaussian mixture models and\nmixtures of linear regressions. We consider over-specified settings in which\nthe number of fitted components is larger than the number of components in the\ntrue distribution. Such misspecified settings can lead to singularity in the\nFisher information matrix, and moreover, the maximum likelihood estimator based\non $n$ i.i.d. samples in $d$ dimensions can have a non-standard\n$\\mathcal{O}((d/n)^{\\frac{1}{4}})$ rate of convergence. Focusing on the simple\nsetting of two-component mixtures fit to a $d$-dimensional Gaussian\ndistribution, we study the behavior of the EM algorithm both when the mixture\nweights are different (unbalanced case), and are equal (balanced case). Our\nanalysis reveals a sharp distinction between these two cases: in the former,\nthe EM algorithm converges geometrically to a point at Euclidean distance of\n$\\mathcal{O}((d/n)^{\\frac{1}{2}})$ from the true parameter, whereas in the\nlatter case, the convergence rate is exponentially slower, and the fixed point\nhas a much lower $\\mathcal{O}((d/n)^{\\frac{1}{4}})$ accuracy. Analysis of this\nsingular case requires the introduction of some novel techniques: in\nparticular, we make use of a careful form of localization in the associated\nempirical process, and develop a recursive argument to progressively sharpen\nthe statistical rate.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 17:16:36 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 01:30:19 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Ho", "Nhat", ""], ["Khamaru", "Koulik", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1810.00969", "submitter": "Tommy Reddad", "authors": "Luc Devroye and Tommy Reddad", "title": "On the discovery of the seed in uniform attachment trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.SI math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the size of vertex confidence sets for including part of (or\nthe entirety of) the seed in seeded uniform attachment trees, given knowledge\nof some of the seed's properties, and with a prescribed probability of failure.\nWe also study the problem of identifying the leaves of a seed in a seeded\nuniform attachment tree, given knowledge of the positions of all internal nodes\nof the seed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 20:41:52 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 13:47:03 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 20:33:17 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Devroye", "Luc", ""], ["Reddad", "Tommy", ""]]}, {"id": "1810.01090", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot, Lecu\\'e Guillaume and Lerasle Matthieu", "title": "Statistical learning with Lipschitz and convex loss functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain risk bounds for Empirical Risk Minimizers (ERM) and minmax\nMedian-Of-Means (MOM) estimators based on loss functions that are both\nLipschitz and convex. Results for the ERM are derived without assumptions on\nthe outputs and under subgaussian assumptions on the design and a new \"local\nBernstein assumption\" on the class of predictors. Similar results are shown for\nminmax MOM estimators in a close setting where the design is only supposed to\nsatisfy moment assumptions, relaxing the Subgaussian hypothesis necessary for\nERM. The analysis of minmax MOM estimators is not based on the small ball\nassumption (SBA) as it was the case in the first analysis of minmax MOM\nestimators. In particular, the basic example of non parametric statistics where\nthe learning class is the linear span of localized bases, that does not satisfy\nSBA can now be handled. Finally, minmax MOM estimators are analysed in a\nsetting where the local Bernstein condition is also dropped out. It is shown to\nachieve an oracle inequality with exponentially large probability under minimal\nassumptions insuring the existence of all objects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 06:55:51 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 14:03:29 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Chinot", "Geoffrey", ""], ["Guillaume", "Lecu\u00e9", ""], ["Matthieu", "Lerasle", ""]]}, {"id": "1810.01147", "submitter": "Joni Virta", "authors": "Joni Virta", "title": "On characterizations of the covariance matrix", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The covariance matrix is well-known for its following properties: affine\nequivariance, additivity, independence property and full affine equivariance.\nGeneralizing the first one leads into the study of scatter functionals,\ncommonly used as plug-in estimators to replace the covariance matrix in robust\nstatistics. However, if the application requires also some of the other\nproperties of the covariance matrix listed earlier, the success of the plug-in\ndepends on whether the candidate scatter functional possesses these. In this\nshort note we show that under natural regularity conditions the covariance\nmatrix is the only scatter functional that is additive and the only scatter\nfunctional that is full affine equivariant.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 09:59:49 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Virta", "Joni", ""]]}, {"id": "1810.01203", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Galin L. Jones", "title": "Consistent Maximum Likelihood Estimation Using Subsets with Applications\n  to Multivariate Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results for consistency of maximum likelihood estimators with\na focus on multivariate mixed models. Our theory builds on the idea of using\nsubsets of the full data to establish consistency of estimators based on the\nfull data. It requires neither that the data consist of independent\nobservations, nor that the observations can be modeled as a stationary\nstochastic process. Compared to existing asymptotic theory using the idea of\nsubsets we substantially weaken the assumptions, bringing them closer to what\nsuffices in classical settings. We apply our theory in two multivariate mixed\nmodels for which it was unknown whether maximum likelihood estimators are\nconsistent. The models we consider have non-stochastic predictors and\nmultivariate responses which are possibly mixed-type (some discrete and some\ncontinuous).\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 12:38:42 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 22:56:38 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Jones", "Galin L.", ""]]}, {"id": "1810.01212", "submitter": "Sergey Dolgov", "authors": "Sergey Dolgov, Karim Anaya-Izquierdo, Colin Fox and Robert Scheichl", "title": "Approximation and sampling of multivariate probability distributions in\n  the tensor train decomposition", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General multivariate distributions are notoriously expensive to sample from,\nparticularly the high-dimensional posterior distributions in PDE-constrained\ninverse problems. This paper develops a sampler for arbitrary continuous\nmultivariate distributions that is based on low-rank surrogates in the\ntensor-train format. We construct a tensor-train approximation to the target\nprobability density function using the cross interpolation, which requires a\nsmall number of function evaluations. For sufficiently smooth distributions the\nstorage required for the TT approximation is moderate, scaling linearly with\ndimension. The structure of the tensor-train surrogate allows efficient\nsampling by the conditional distribution method. Unbiased estimates may be\ncalculated by correcting the transformed random seeds using a\nMetropolis--Hastings accept/reject step. Moreover, one can use a more efficient\nquasi-Monte Carlo quadrature that may be corrected either by a control-variate\nstrategy, or by importance weighting. We show that the error in the\ntensor-train approximation propagates linearly into the Metropolis--Hastings\nrejection rate and the integrated autocorrelation time of the resulting Markov\nchain. These methods are demonstrated in three computed examples: fitting\nfailure time of shock absorbers; a PDE-constrained inverse diffusion problem;\nand sampling from the Rosenbrock distribution. The delayed rejection adaptive\nMetropolis (DRAM) algorithm is used as a benchmark. We find that the\nimportance-weight corrected quasi-Monte Carlo quadrature performs best in all\ncomputed examples, and is orders-of-magnitude more efficient than DRAM across a\nwide range of approximation accuracies and sample sizes. Indeed, all the\nmethods developed here significantly outperform DRAM in all computed examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 12:56:16 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 21:04:27 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 23:58:40 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Dolgov", "Sergey", ""], ["Anaya-Izquierdo", "Karim", ""], ["Fox", "Colin", ""], ["Scheichl", "Robert", ""]]}, {"id": "1810.01323", "submitter": "Guang Cheng", "authors": "Xiao Guo and Guang Cheng", "title": "Moderate-Dimensional Inferences on Quadratic Functionals in Ordinary\n  Least Squares", "comments": "To appear in JASA-T&M", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inferences for quadratic functionals of linear regression\nparameter have found wide applications including signal detection, global\ntesting, inferences of error variance and fraction of variance explained.\nClassical theory based on ordinary least squares estimator works perfectly in\nthe low-dimensional regime, but fails when the parameter dimension $p_n$ grows\nproportionally to the sample size $n$. In some cases, its performance is not\nsatisfactory even when $n\\ge 5p_n$.\n  The main contribution of this paper is to develop {\\em dimension-adaptive}\ninferences for quadratic functionals when $\\lim_{n\\to \\infty}\np_n/n=\\tau\\in[0,1)$. We propose a bias-and-variance-corrected test statistic\nand demonstrate that its theoretical validity (such as consistency and\nasymptotic normality) is adaptive to both low dimension with $\\tau = 0$ and\nmoderate dimension with $\\tau \\in(0, 1)$. Our general theory holds, in\nparticular, without Gaussian design/error or structural parameter assumption,\nand applies to a broad class of quadratic functionals covering all\naforementioned applications. As a by-product, we find that the classical\nfixed-dimensional results continue to hold {\\em if and only if} the\nsignal-to-noise ratio is large enough, say when $p_n$ diverges but slower than\n$n$. Extensive numerical results demonstrate the satisfactory performance of\nthe proposed methodology even when $p_n\\ge 0.9n$ in some extreme cases. The\nmathematical arguments are based on the random matrix theory and\nleave-one-observation-out method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 15:23:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:23:34 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Guo", "Xiao", ""], ["Cheng", "Guang", ""]]}, {"id": "1810.01370", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna, Xiaojun Song, Qi Xu", "title": "Covariate Distribution Balance via Propensity Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new estimators for the propensity score that aim to\nmaximize the covariate distribution balance among different treatment groups.\nHeuristically, our proposed procedure attempts to estimate a propensity score\nmodel by making the underlying covariate distribution of different treatment\ngroups as close to each other as possible. Our estimators are data-driven, do\nnot rely on tuning parameters such as bandwidths, admit an asymptotic linear\nrepresentation, and can be used to estimate different treatment effect\nparameters under different identifying assumptions, including unconfoundedness\nand local treatment effects. We derive the asymptotic properties of inverse\nprobability weighted estimators for the average, distributional, and quantile\ntreatment effects based on the proposed propensity score estimator and\nillustrate their finite sample performance via Monte Carlo simulations and two\nempirical applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:00:13 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 20:34:02 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 01:59:46 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2020 02:25:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""], ["Song", "Xiaojun", ""], ["Xu", "Qi", ""]]}, {"id": "1810.01509", "submitter": "Tianxi Li", "authors": "Tianxi Li, Lihua Lei, Sharmodeep Bhattacharyya, Koen Van den Berge,\n  Purnamrita Sarkar, Peter J. Bickel, Elizaveta Levina", "title": "Hierarchical community detection by recursive partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of community detection in networks is usually formulated as\nfinding a single partition of the network into some \"correct\" number of\ncommunities. We argue that it is more interpretable and in some regimes more\naccurate to construct a hierarchical tree of communities instead. This can be\ndone with a simple top-down recursive partitioning algorithm, starting with a\nsingle community and separating the nodes into two communities by spectral\nclustering repeatedly, until a stopping rule suggests there are no further\ncommunities. This class of algorithms is model-free, computationally efficient,\nand requires no tuning other than selecting a stopping rule. We show that there\nare regimes where this approach outperforms K-way spectral clustering, and\npropose a natural framework for analyzing the algorithm's theoretical\nperformance, the binary tree stochastic block model. Under this model, we prove\nthat the algorithm correctly recovers the entire community tree under\nrelatively mild assumptions. We apply the algorithm to a gene network based on\ngene co-occurrence in 1580 research papers on anemia, and identify six clusters\nof genes in a meaningful hierarchy. We also illustrate the algorithm on a\ndataset of statistics papers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:58:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 06:41:52 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 22:43:37 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 19:46:12 GMT"}, {"version": "v5", "created": "Fri, 13 Sep 2019 04:37:41 GMT"}, {"version": "v6", "created": "Thu, 14 May 2020 05:53:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Tianxi", ""], ["Lei", "Lihua", ""], ["Bhattacharyya", "Sharmodeep", ""], ["Berge", "Koen Van den", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1810.01645", "submitter": "Ursula Mueller", "authors": "Ursula U. M\\\"uller, Anton Schick and Wolfgang Wefelmeyer", "title": "Estimating the error distribution function in nonparametric regression", "comments": "Unpublished manuscript (2004), 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an efficient estimator for the error distribution function of\nthe nonparametric regression model Y = r(Z) + e. Our estimator is a kernel\nsmoothed empirical distribution function based on residuals from an\nunder-smoothed local quadratic smoother for the regression function.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 09:14:10 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 14:47:07 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["M\u00fcller", "Ursula U.", ""], ["Schick", "Anton", ""], ["Wefelmeyer", "Wolfgang", ""]]}, {"id": "1810.01702", "submitter": "Kolyan Ray", "authors": "Richard Nickl and Kolyan Ray", "title": "Nonparametric statistical inference for drift vector fields of\n  multi-dimensional diffusions", "comments": "55 pages, to appear in the Annals of Statistics", "journal-ref": "Ann. Statist. 48 (2020), 1383-1408", "doi": "10.1214/19-AOS1851", "report-no": null, "categories": "math.ST math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining a periodic Lipschitz vector field $b=(b_1, \\dots,\nb_d)$ from an observed trajectory of the solution $(X_t: 0 \\le t \\le T)$ of the\nmulti-dimensional stochastic differential equation \\begin{equation*} dX_t =\nb(X_t)dt + dW_t, \\quad t \\geq 0, \\end{equation*} where $W_t$ is a standard\n$d$-dimensional Brownian motion, is considered. Convergence rates of a\npenalised least squares estimator, which equals the maximum a posteriori (MAP)\nestimate corresponding to a high-dimensional Gaussian product prior, are\nderived. These results are deduced from corresponding contraction rates for the\nassociated posterior distributions. The rates obtained are optimal up to\nlog-factors in $L^2$-loss in any dimension, and also for supremum norm loss\nwhen $d \\le 4$. Further, when $d \\le 3$, nonparametric Bernstein-von Mises\ntheorems are proved for the posterior distributions of $b$. From this we deduce\nfunctional central limit theorems for the implied estimators of the invariant\nmeasure $\\mu_b$. The limiting Gaussian process distributions have a covariance\nstructure that is asymptotically optimal from an information-theoretic point of\nview.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 12:01:12 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:33:08 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2019 16:13:00 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Nickl", "Richard", ""], ["Ray", "Kolyan", ""]]}, {"id": "1810.01720", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Sum decomposition of divergence into three divergences", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence functions play a key role as to measure the discrepancy between\ntwo points in the field of machine learning, statistics and signal processing.\nWell-known divergences are the Bregman divergences, the Jensen divergences and\nthe f-divergences. In this paper, we show that the symmetric Bregman divergence\ncan be decomposed into the sum of two types of Jensen divergences and the\nBregman divergence. Furthermore, applying this result, we show another sum\ndecomposition of divergence is possible which includes f-divergences\nexplicitly.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:02:19 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 08:17:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1810.01724", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep (DEEP) Mukhopadhyay and Kaijun Wang", "title": "A Nonparametric Approach to High-dimensional k-sample Comparison\n  Problems", "comments": "Biometrika (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional k-sample comparison is a common applied problem. We\nconstruct a class of easy-to-implement nonparametric distribution-free tests\nbased on new tools and unexplored connections with spectral graph theory. The\ntest is shown to possess various desirable properties along with a\ncharacteristic exploratory flavor that has practical consequences. The\nnumerical examples show that our method works surprisingly well under a broad\nrange of realistic situations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:20:28 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 18:40:50 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Subhadeep", "", "", "DEEP"], ["Mukhopadhyay", "", ""], ["Wang", "Kaijun", ""]]}, {"id": "1810.01864", "submitter": "Aryeh Kontorovich", "authors": "Steve Hanneke, Aryeh Kontorovich, Menachem Sadigurschi", "title": "Agnostic Sample Compression for Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the first positive results for bounded sample compression in the\nagnostic regression setting. We show that for p in {1,infinity}, agnostic\nlinear regression with $\\ell_p$ loss admits a bounded sample compression\nscheme. Specifically, we exhibit efficient sample compression schemes for\nagnostic linear regression in $R^d$ of size $d+1$ under the $\\ell_1$ loss and\nsize $d+2$ under the $\\ell_\\infty$ loss. We further show that for every other\n$\\ell_p$ loss (1 < p < infinity), there does not exist an agnostic compression\nscheme of bounded size. This refines and generalizes a negative result of\nDavid, Moran, and Yehudayoff (2016) for the $\\ell_2$ loss. We close by posing a\ngeneral open question: for agnostic regression with $\\ell_1$ loss, does every\nfunction class admit a compression scheme of size equal to its\npseudo-dimension? This question generalizes Warmuth's classic sample\ncompression conjecture for realizable-case classification (Warmuth, 2003).\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:46:59 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hanneke", "Steve", ""], ["Kontorovich", "Aryeh", ""], ["Sadigurschi", "Menachem", ""]]}, {"id": "1810.01949", "submitter": "Han Lin Shang", "authors": "Francis K. C. Hui, Chong You, Han Lin Shang, Samuel M\\\"uller", "title": "Semiparametric Regression using Variational Approximations", "comments": "35 pages, 3 figures, 2 tables, To appear in Journal of the American\n  Statistical Association: Theory and Methods", "journal-ref": "Francis K. C. Hui, Chong You, Han Lin Shang & Samuel M\\\"{u}ller\n  (2018) Semiparametric Regression using Variational Approximations, Journal of\n  the American Statistical Association: Theory and Methods", "doi": "10.1080/01621459.2018.1518235", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiparametric regression offers a flexible framework for modeling non-linear\nrelationships between a response and covariates. A prime example are\ngeneralized additive models where splines (say) are used to approximate\nnon-linear functional components in conjunction with a quadratic penalty to\ncontrol for overfitting. Estimation and inference are then generally performed\nbased on the penalized likelihood, or under a mixed model framework. The\npenalized likelihood framework is fast but potentially unstable, and choosing\nthe smoothing parameters needs to be done externally using cross-validation,\nfor instance. The mixed model framework tends to be more stable and offers a\nnatural way for choosing the smoothing parameters, but for non-normal responses\ninvolves an intractable integral.\n  In this article, we introduce a new framework for semiparametric regression\nbased on variational approximations. The approach possesses the stability and\nnatural inference tools of the mixed model framework, while achieving\ncomputation times comparable to using penalized likelihood. Focusing on\ngeneralized additive models, we derive fully tractable variational likelihoods\nfor some common response types. We present several features of the variational\napproximation framework for inference, including a variational information\nmatrix for inference on parametric components, and a closed-form update for\nestimating the smoothing parameter. We demonstrate the consistency of the\nvariational approximation estimates, and an asymptotic normality result for the\nparametric component of the model. Simulation studies show the variational\napproximation framework performs similarly to and sometimes better than\ncurrently available software for fitting generalized additive models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 20:30:12 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hui", "Francis K. C.", ""], ["You", "Chong", ""], ["Shang", "Han Lin", ""], ["M\u00fcller", "Samuel", ""]]}, {"id": "1810.02016", "submitter": "R W R Darling Ph. D.", "authors": "R W R Darling, Cheyne Homberger", "title": "The Four Point Permutation Test for Latent Block Structure in Incidence\n  Matrices", "comments": "41 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional data may be represented as a bipartite graph $G:=(L \\cup R,\nE)$, where $L$ denotes agents, $R$ denotes objects visible to many agents, and\nan edge in $E$ denotes an interaction between an agent and an object.\nUnsupervised learning seeks to detect block structures in the adjacency matrix\n$Z$ between $L$ and $R$, thus grouping together sets of agents with similar\nobject interactions. New results on quasirandom permutations suggest a\nnon-parametric \\textbf{four point test} to measure the amount of block\nstructure in $G$, with respect to vertex orderings on $L$ and $R$. Take\ndisjoint 4-edge random samples, order these four edges by left endpoint, and\ncount the relative frequencies of the $4!$ possible orderings of the right\nendpoint. When these orderings are equiprobable, the edge set $E$ corresponds\nto a quasirandom permutation $\\pi$ of $|E|$ symbols. Total variation distance\nof the relative frequency vector away from the uniform distribution on 24\npermutations measures the amount of block structure. Such a test statistic,\nbased on $\\lfloor |E|/4 \\rfloor$ samples, is computable in $O(|E|/p)$ time on\n$p$ processors. Possibly block structure may be enhanced by precomputing\n\\textbf{natural orders} on $L$ and $R$, related to the second eigenvector of\ngraph Laplacians. In practice this takes $O(d |E|)$ time, where $d$ is the\ngraph diameter. Five open problems are described.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 01:23:18 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 17:43:00 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Darling", "R W R", ""], ["Homberger", "Cheyne", ""]]}, {"id": "1810.02030", "submitter": "Chao Gao", "authors": "Chao Gao, Jiyi Liu, Yuan Yao, Weizhi Zhu", "title": "Robust Estimation and Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 02:37:16 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 01:47:46 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 20:09:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gao", "Chao", ""], ["Liu", "Jiyi", ""], ["Yao", "Yuan", ""], ["Zhu", "Weizhi", ""]]}, {"id": "1810.02036", "submitter": "Gerard G. Letac", "authors": "G\\'erard Letac and H\\'el\\`ene Massam", "title": "Gaussian approximation of Gaussian scale mixture", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given positive random variable $V>0$ and a given $Z\\sim N(0,1)$\nindependent of $V$, we compute the scalar $t_0$ such that the distance between\n$Z\\sqrt{V}$ and $Z\\sqrt{t_0}$ in the $L^2(\\R)$ sense, is minimal. We also\nconsider the same problem in several dimensions when $V$ is a random positive\ndefinite matrix.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 03:18:20 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 17:16:37 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 09:18:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Letac", "G\u00e9rard", ""], ["Massam", "H\u00e9l\u00e8ne", ""]]}, {"id": "1810.02088", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama, William E. Strawderman", "title": "A Gaussian sequence approach for proving minimaxity: A Review", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews minimax best equivariant estimation in these invariant\nestimation problems: a location parameter, a scale parameter and a (Wishart)\ncovariance matrix. We briefly review development of the best equivariant\nestimator as a generalized Bayes estimator relative to right invariant Haar\nmeasure in each case. Then we prove minimaxity of the best equivariant\nprocedure by giving a least favorable prior sequence based on non-truncated\nGaussian distributions. The results in this paper are all known, but we bring a\nfresh and somewhat unified approach by using, in contrast to most proofs in the\nliterature, a smooth sequence of non truncated priors. This approach leads to\nsome simplifications in the minimaxity proofs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 08:11:32 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "1810.02183", "submitter": "Ilias Zadik", "authors": "Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik", "title": "Revealing Network Structure, Confidentially: Improved Rates for\n  Node-Private Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by growing concerns over ensuring privacy on social networks, we\ndevelop new algorithms and impossibility results for fitting complex\nstatistical models to network data subject to rigorous privacy guarantees. We\nconsider the so-called node-differentially private algorithms, which compute\ninformation about a graph or network while provably revealing almost no\ninformation about the presence or absence of a particular node in the graph.\n  We provide new algorithms for node-differentially private estimation for a\npopular and expressive family of network models: stochastic block models and\ntheir generalization, graphons. Our algorithms improve on prior work, reducing\ntheir error quadratically and matching, in many regimes, the optimal nonprivate\nalgorithm. We also show that for the simplest random graph models ($G(n,p)$ and\n$G(n,m)$), node-private algorithms can be qualitatively more accurate than for\nmore complex models---converging at a rate of $\\frac{1}{\\epsilon^2 n^{3}}$\ninstead of $\\frac{1}{\\epsilon^2 n^2}$. This result uses a new extension lemma\nfor differentially private algorithms that we hope will be broadly useful.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 13:00:27 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Smith", "Adam", ""], ["Zadik", "Ilias", ""]]}, {"id": "1810.02221", "submitter": "Junxiong Jia", "authors": "Junxiong Jia and Jigen Peng and Jinghuai Gao", "title": "Posterior contraction for empirical Bayesian approach to inverse\n  problems under non-diagonal assumption", "comments": "24 pages; Accepted by Inverse Problems and Imaging", "journal-ref": "Inverse Problems and Imaging, 15(2), 2021, 201-228", "doi": "10.3934/ipi.2020061", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an empirical Bayesian nonparametric approach to a family of\nlinear inverse problems with Gaussian prior and Gaussian noise. We consider a\nclass of Gaussian prior probability measures with covariance operator indexed\nby a hyperparameter that quantifies regularity. By introducing two auxiliary\nproblems, we construct an empirical Bayes method and prove that this method can\nautomatically select the hyperparameter. In addition, we show that this\nadaptive Bayes procedure provides optimal contraction rates up to a slowly\nvarying term and an arbitrarily small constant, without knowledge about the\nregularity index. Our method needs not the prior covariance, noise covariance\nand forward operator have a common basis in their singular value decomposition,\nenlarging the application range compared with the existing results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 13:57:33 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 08:37:53 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jia", "Junxiong", ""], ["Peng", "Jigen", ""], ["Gao", "Jinghuai", ""]]}, {"id": "1810.02294", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Alessandro Rinaldo", "title": "Markov Properties of Discrete Determinantal Point Processes", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are probabilistic models for repulsion.\nWhen used to represent the occurrence of random subsets of a finite base set,\nDPPs allow to model global negative associations in a mathematically elegant\nand direct way. Discrete DPPs have become popular and computationally tractable\nmodels for solving several machine learning tasks that require the selection of\ndiverse objects, and have been successfully applied in numerous real-life\nproblems. Despite their popularity, the statistical properties of such models\nhave not been adequately explored. In this note, we derive the Markov\nproperties of discrete DPPs and show how they can be expressed using graphical\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 16:18:59 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 00:56:41 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1810.02665", "submitter": "Ulrike Schneider", "authors": "Nicolai Amann, Ulrike Schneider", "title": "Uniform Asymptotics and Confidence Regions Based on the Adaptive Lasso\n  with Partially Consistent Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adaptive Lasso estimator with componentwise tuning in the\nframework of a low-dimensional linear regression model. In our setting, at\nleast one of the components is penalized at the rate of consistent model\nselection and certain components may not be penalized at all. We perform a\ndetailed study of the consistency properties and the asymptotic distribution\nwhich includes the effects of componentwise tuning within a so-called\nmoving-parameter framework. These results enable us to explicitly provide a set\n$\\mathcal{M}$ such that every open superset acts as a confidence set with\nuniform asymptotic coverage equal to 1, whereas removing an arbitrarily small\nopen set along the boundary yields a confidence set with uniform asymptotic\ncoverage equal to 0. The shape of the set $\\mathcal{M}$ depends on the\nregressor matrix as well as the deviations within the componentwise tuning\nparameters. Our findings can be viewed as a broad generalization of P\\\"otscher\n& Schneider (2009, 2010) who considered distributional properties and\nconfidence intervals based on components of the adaptive Lasso estimator for\nthe case of orthogonal regressors.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:18:37 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:00:22 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Amann", "Nicolai", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1810.02688", "submitter": "Philippe Besse", "authors": "Philippe Besse (IMT), Brendan Guillouet (IMT), B\\'eatrice Laurent\n  (IMT)", "title": "Wikistat 2.0: Educational Resources for Artificial Intelligence", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data, data science, deep learning, artificial intelligence are the key\nwords of intense hype related with a job market in full evolution, that impose\nto adapt the contents of our university professional trainings. Which\nartificial intelligence is mostly concerned by the job offers? Which\nmethodologies and technologies should be favored in the training programs?\nWhich objectives, tools and educational resources do we needed to put in place\nto meet these pressing needs? We answer these questions in describing the\ncontents and operational resources in the Data Science orientation of the\nspecialty Applied Mathematics at INSA Toulouse. We focus on basic mathematics\ntraining (Optimization, Probability, Statistics), associated with the practical\nimplementation of the most performing statistical learning algorithms, with the\nmost appropriate technologies and on real examples. Considering the huge\nvolatility of the technologies, it is imperative to train students in\nseft-training, this will be their technological watch tool when they will be in\nprofessional activity. This explains the structuring of the educational site\ngithub.com/wikistat into a set of tutorials. Finally, to motivate the thorough\npractice of these tutorials, a serious game is organized each year in the form\nof a prediction contest between students of Master degrees in Applied\nMathematics for IA.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 08:27:59 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 13:07:57 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Besse", "Philippe", "", "IMT"], ["Guillouet", "Brendan", "", "IMT"], ["Laurent", "B\u00e9atrice", "", "IMT"]]}, {"id": "1810.02733", "submitter": "Marco Cuturi", "authors": "Aude Genevay, L\\'enaic Chizat, Francis Bach, Marco Cuturi, Gabriel\n  Peyr\\'e", "title": "Sample Complexity of Sinkhorn divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely\nused in machine learning to compare probability measures. We focus in this\npaper on \\emph{Sinkhorn divergences} (SDs), a regularized variant of OT\ndistances which can interpolate, depending on the regularization strength\n$\\varepsilon$, between OT ($\\varepsilon=0$) and MMD ($\\varepsilon=\\infty$).\nAlthough the tradeoff induced by that regularization is now well understood\ncomputationally (OT, SDs and MMD require respectively $O(n^3\\log n)$, $O(n^2)$\nand $n^2$ operations given a sample size $n$), much less is known in terms of\ntheir \\emph{sample complexity}, namely the gap between these quantities, when\nevaluated using finite samples \\emph{vs.} their respective densities. Indeed,\nwhile the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$\nfor OT in dimension $d$ and $1/\\sqrt{n}$ for MMD, that for SDs has only been\nstudied empirically. In this paper, we \\emph{(i)} derive a bound on the\napproximation error made with SDs when approximating OT as a function of the\nregularizer $\\varepsilon$, \\emph{(ii)} prove that the optimizers of regularized\nOT are bounded in a Sobolev (RKHS) ball independent of the two measures and\n\\emph{(iii)} provide the first sample complexity bound for SDs, obtained,by\nreformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling\nin $1/\\sqrt{n}$ (as in MMD), with a constant that depends however on\n$\\varepsilon$, making the bridge between OT and MMD complete.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:55:05 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:06:13 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Genevay", "Aude", ""], ["Chizat", "L\u00e9naic", ""], ["Bach", "Francis", ""], ["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1810.02767", "submitter": "Vladimir Koltchinskii", "authors": "Vladimir Koltchinskii and Mayya Zhilova", "title": "Efficient Estimation of Smooth Functionals in Gaussian Shift Models", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a problem of estimation of smooth functionals of parameter $\\theta $\nof Gaussian shift model $$ X=\\theta +\\xi,\\ \\theta \\in E, $$ where $E$ is a\nseparable Banach space and $X$ is an observation of unknown vector $\\theta$ in\nGaussian noise $\\xi$ with zero mean and known covariance operator $\\Sigma.$ In\nparticular, we develop estimators $T(X)$ of $f(\\theta)$ for functionals\n$f:E\\mapsto {\\mathbb R}$ of H\\\"older smoothness $s>0$ such that $$\n\\sup_{\\|\\theta\\|\\leq 1} {\\mathbb E}_{\\theta}(T(X)-f(\\theta))^2 \\lesssim\n\\Bigl(\\|\\Sigma\\| \\vee ({\\mathbb E}\\|\\xi\\|^2)^s\\Bigr)\\wedge 1, $$ where\n$\\|\\Sigma\\|$ is the operator norm of $\\Sigma,$ and show that this mean squared\nerror rate is minimax optimal at least in the case of standard Gaussian shift\nmodel ($E={\\mathbb R}^d$ equipped with the canonical Euclidean norm, $\\xi\n=\\sigma Z,$ $Z\\sim {\\mathcal N}(0;I_d)$). Moreover, we determine a sharp\nthreshold on the smoothness $s$ of functional $f$ such that, for all $s$ above\nthe threshold, $f(\\theta)$ can be estimated efficiently with a mean squared\nerror rate of the order $\\|\\Sigma\\|$ in a \"small noise\" setting (that is, when\n${\\mathbb E}\\|\\xi\\|^2$ is small). The construction of efficient estimators is\ncrucially based on a \"bootstrap chain\" method of bias reduction. The results\ncould be applied to a variety of special high-dimensional and\ninfinite-dimensional Gaussian models (for vector, matrix and functional data).\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 15:55:31 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 03:30:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["Zhilova", "Mayya", ""]]}, {"id": "1810.02881", "submitter": "Michael Jauch", "authors": "Michael Jauch, Peter D. Hoff, and David B. Dunson", "title": "Random orthogonal matrices and the Cayley transform", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random orthogonal matrices play an important role in probability and\nstatistics, arising in multivariate analysis, directional statistics, and\nmodels of physical systems, among other areas. Calculations involving random\northogonal matrices are complicated by their constrained support. Accordingly,\nwe parametrize the Stiefel and Grassmann manifolds, represented as subsets of\northogonal matrices, in terms of Euclidean parameters using the Cayley\ntransform. We derive the necessary Jacobian terms for change of variables\nformulas. Given a density defined on the Stiefel or Grassmann manifold, these\nallow us to specify the corresponding density for the Euclidean parameters, and\nvice versa. As an application, we describe and illustrate through examples a\nMarkov chain Monte Carlo approach to simulating from distributions on the\nStiefel and Grassmann manifolds. Finally, we establish an asymptotic\nindependent normal approximation for the distribution of the Euclidean\nparameters which corresponds to the uniform distribution on the Stiefel\nmanifold. This result contributes to the growing literature on normal\napproximations to the entries of random orthogonal matrices or transformations\nthereof.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 20:42:24 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Jauch", "Michael", ""], ["Hoff", "Peter D.", ""], ["Dunson", "David B.", ""]]}, {"id": "1810.02905", "submitter": "Henry Lam", "authors": "Henry Lam and Huajie Qian", "title": "Bounding Optimality Gap in Stochastic Optimization via Bagging:\n  Statistical Efficiency and Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a statistical method to estimate the optimal value, and the\noptimality gap of a given solution for stochastic optimization as an assessment\nof the solution quality. Our approach is based on bootstrap aggregating, or\nbagging, resampled sample average approximation (SAA). We show how this\napproach leads to valid statistical confidence bounds for non-smooth\noptimization. We also demonstrate its statistical efficiency and stability that\nare especially desirable in limited-data situations, and compare these\nproperties with some existing methods. We present our theory that views SAA as\na kernel in an infinite-order symmetric statistic, which can be approximated\nvia bagging. We substantiate our theoretical findings with numerical results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 23:05:01 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Lam", "Henry", ""], ["Qian", "Huajie", ""]]}, {"id": "1810.02923", "submitter": "Baihan Lin", "authors": "Baihan Lin, Nikolaus Kriegeskorte", "title": "Adaptive Geo-Topological Independence Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing two potentially multivariate variables for statistical dependence on\nthe basis finite samples is a fundamental statistical challenge. Here we\nexplore a family of tests that adapt to the complexity of the relationship\nbetween the variables, promising robust power across scenarios. Building on the\ndistance correlation, we introduce a family of adaptive independence criteria\nbased on nonlinear monotonic transformations of distances. We show that these\ncriteria, like the distance correlation and RKHS-based criteria, provide\ndependence indicators. We propose a class of adaptive (multi-threshold) test\nstatistics, which form the basis for permutation tests. These tests empirically\noutperform some of the established tests in average and worst-case statistical\nsensitivity across a range of univariate and multivariate relationships, offer\nuseful insights to the data and may deserve further exploration.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 02:12:21 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:21:04 GMT"}, {"version": "v3", "created": "Sun, 7 Jun 2020 07:14:42 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 05:18:55 GMT"}, {"version": "v5", "created": "Thu, 18 Jun 2020 01:30:58 GMT"}, {"version": "v6", "created": "Thu, 22 Oct 2020 03:44:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Baihan", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1810.02954", "submitter": "Feng Ruan", "authors": "Andrea Montanari, Feng Ruan and Jun Yan", "title": "Adapting to Unknown Noise Distribution in Matrix Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an unknown matrix $\\boldsymbol{X}\\in\n{\\mathbb R}^{m\\times n}$, from observations $\\boldsymbol{Y} =\n\\boldsymbol{X}+\\boldsymbol{W}$ where $\\boldsymbol{W}$ is a noise matrix with\nindependent and identically distributed entries, as to minimize estimation\nerror measured in operator norm. Assuming that the underlying signal\n$\\boldsymbol{X}$ is low-rank and incoherent with respect to the canonical\nbasis, we prove that minimax risk is equivalent to\n$(\\sqrt{m}\\vee\\sqrt{n})/\\sqrt{I_W}$ in the high-dimensional limit\n$m,n\\to\\infty$, where $I_W$ is the Fisher information of the noise. Crucially,\nwe develop an efficient procedure that achieves this risk, adaptively over the\nnoise distribution (under certain regularity assumptions).\n  Letting $\\boldsymbol{X} =\n\\boldsymbol{U}{\\boldsymbol{\\Sigma}}\\boldsymbol{V}^{{\\sf T}}$ --where\n$\\boldsymbol{U}\\in {\\mathbb R}^{m\\times r}$, $\\boldsymbol{V}\\in{\\mathbb\nR}^{n\\times r}$ are orthogonal, and $r$ is kept fixed as $m,n\\to\\infty$-- we\nuse our method to estimate $\\boldsymbol{U}$, $\\boldsymbol{V}$. Standard\nspectral methods provide non-trivial estimates of the factors\n$\\boldsymbol{U},\\boldsymbol{V}$ (weak recovery) only if the singular values of\n$\\boldsymbol{X}$ are larger than $(mn)^{1/4}{\\rm Var}(W_{11})^{1/2}$. We prove\nthat the new approach achieves weak recovery down to the the\ninformation-theoretically optimal threshold $(mn)^{1/4}I_W^{1/2}$.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 07:59:44 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 09:20:39 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 01:39:35 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Montanari", "Andrea", ""], ["Ruan", "Feng", ""], ["Yan", "Jun", ""]]}, {"id": "1810.02955", "submitter": "Anran Hu", "authors": "Xin Guo, Anran Hu, Renyuan Xu, Junzi Zhang", "title": "Consistency and Computation of Regularized MLEs for Multivariate Hawkes\n  Processes", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves the consistency property for the regularized maximum\nlikelihood estimators (MLEs) of multivariate Hawkes processes (MHPs). It also\ndevelops an alternating minimization type algorithm (AA-iPALM) to compute the\nMLEs with guaranteed global convergence to the set of stationary points. The\nperformance of this AA-iPALM algorithm on both synthetic and real-world data\nshows that AA-iPALM consistently improves over iPALM and PALM. Moreover,\nAA-iPALM is able to identify the causality structure in rental listings on\ncraigslist and herd behavior in the NASDAQ ITCH dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 08:04:01 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Guo", "Xin", ""], ["Hu", "Anran", ""], ["Xu", "Renyuan", ""], ["Zhang", "Junzi", ""]]}, {"id": "1810.02991", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Robust and Efficient Estimation in the Parametric Cox Regression Model\n  under Random Censoring", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox proportional hazard regression model is a popular tool to analyze the\nrelationship between a censored lifetime variable with other relevant factors.\nThe semi-parametric Cox model is widely used to study different types of data\narising from applied disciplines like medical science, biology, reliability\nstudies and many more. A fully parametric version of the Cox regression model,\nif properly specified, can yield more efficient parameter estimates leading to\nbetter insight generation. However, the existing maximum likelihood approach of\ngenerating inference under the fully parametric Cox regression model is highly\nnon-robust against data-contamination which restricts its practical usage. In\nthis paper we develop a robust estimation procedure for the parametric Cox\nregression model based on the minimum density power divergence approach. The\nproposed minimum density power divergence estimator is seen to produce highly\nrobust estimates under data contamination with only a slight loss in efficiency\nunder pure data. Further, they are always seen to generate more precise\ninference than the likelihood based estimates under the semi-parametric Cox\nmodels or their existing robust versions. We also sketch the derivation of the\nasymptotic properties of the proposed estimator using the martingale approach\nand justify their robustness theoretically through the influence function\nanalysis. The practical applicability and usefulness of the proposal are\nillustrated through simulations and a real data example.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 12:08:23 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1810.02998", "submitter": "C\\'eline Duval", "authors": "Alexandra Carpentier, C\\'eline Duval, Ester Mariucci", "title": "Total variation distance for discretely observed L\\'evy processes: a\n  Gaussian approximation of the small jumps", "comments": "Important and necessary changes have been made in this new version,\n  this version supersedes version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to treat small jumps of L\\'evy processes as Wiener\nnoise and thus to approximate its marginals by a Gaussian distribution.\nHowever, results that allow to quantify the goodness of this approximation\naccording to a given metric are rare. In this paper, we clarify what happens\nwhen the chosen metric is the total variation distance. Such a choice is\nmotivated by its statistical interpretation. If the total variation distance\nbetween two statistical models converges to zero, then no tests can be\nconstructed to distinguish the two models which are therefore equivalent,\nstatistically speaking. We elaborate a fine analysis of a Gaussian\napproximation for the small jumps of L\\'evy processes with infinite L\\'evy\nmeasure in total variation distance. Non asymptotic bounds for the total\nvariation distance between $n$ discrete observations of small jumps of a L\\'evy\nprocess and the corresponding Gaussian distribution is presented and\nextensively discussed. As a byproduct, new upper bounds for the total variation\ndistance between discrete observations of L\\'evy processes are provided. The\ntheory is illustrated by concrete examples.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 12:54:13 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 16:27:13 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 07:14:51 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Duval", "C\u00e9line", ""], ["Mariucci", "Ester", ""]]}, {"id": "1810.03030", "submitter": "Linh Tran", "authors": "Linh Tran, Maya Petersen, Joshua Schwab, Mark J van der Laan", "title": "Robust variance estimation and inference for causal effect estimation", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a longitudinal data structure consisting of baseline covariates,\ntime-varying treatment variables, intermediate time-dependent covariates, and a\npossibly time dependent outcome. Previous studies have shown that estimating\nthe variance of asymptotically linear estimators using empirical influence\nfunctions in this setting result in anti-conservative estimates with increasing\nmagnitudes of positivity violations, leading to poor coverage and uncontrolled\nType I errors. In this paper, we present two alternative approaches of\nestimating the variance of these estimators: (i) a robust approach which\ndirectly targets the variance of the influence function as a counterfactual\nmean outcome, and (ii) a non-parametric bootstrap based approach that is\ntheoretically valid and lowers the computational cost, thereby increasing the\nfeasibility in non-parametric settings using complex machine learning\nalgorithms. The performance of these approaches are compared to that of the\nempirical influence function in simulations across different levels of\npositivity violations and treatment effect sizes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 17:40:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Tran", "Linh", ""], ["Petersen", "Maya", ""], ["Schwab", "Joshua", ""], ["van der Laan", "Mark J", ""]]}, {"id": "1810.03081", "submitter": "Antoine Dedieu", "authors": "Antoine Dedieu", "title": "Error bounds for sparse classifiers in high-dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an L2 recovery bound for a family of sparse estimators defined as\nminimizers of some empirical loss functions -- which include hinge loss and\nlogistic loss. More precisely, we achieve an upper-bound for coefficients\nestimation scaling as (k*/n)\\log(p/k*): n,p is the size of the design matrix\nand k* the dimension of the theoretical loss minimizer. This is done under\nstandard assumptions, for which we derive stronger versions of a cone condition\nand a restricted strong convexity. Our bound holds with high probability and in\nexpectation and applies to an L1-regularized estimator and to a recently\nintroduced Slope estimator, which we generalize for classification problems.\nSlope presents the advantage of adapting to unknown sparsity. Thus, we propose\na tractable proximal algorithm to compute it and assess its empirical\nperformance. Our results match the best existing bounds for classification and\nregression problems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 03:43:28 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 17:56:15 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 04:28:37 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Dedieu", "Antoine", ""]]}, {"id": "1810.03260", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Edward H. Kennedy", "title": "Visually Communicating and Teaching Intuition for Influence Functions", "comments": "This manuscript version includes 2 additional supplemental figures to\n  further aid intuition. In total: 4 figures, 36 pages (double spaced)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators based on influence functions (IFs) have been shown to be effective\nin many settings, especially when combined with machine learning techniques. By\nfocusing on estimating a specific target of interest (e.g., the average effect\nof a treatment), rather than on estimating the full underlying data generating\ndistribution, IF-based estimators are often able to achieve asymptotically\noptimal mean-squared error. Still, many researchers find IF-based estimators to\nbe opaque or overly technical, which makes their use less prevalent and their\nbenefits less available. To help foster understanding and trust in IF-based\nestimators, we present tangible, visual illustrations of when and how IF-based\nestimators can outperform standard ``plug-in'' estimators. The figures we show\nare based on connections between IFs, gradients, linear approximations, and\nNewton-Raphson.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:46:42 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 00:08:47 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 20:48:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Fisher", "Aaron", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1810.03262", "submitter": "Yiwei Zhang", "authors": "Congping Lin, Yuanfei Huang, Tingwei Quan and Yiwei Zhang", "title": "Modelling brain-wide neuronal morphology via rooted Cayley trees", "comments": "11 pages, 8 figures, Accepted to Scientific reports 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal morphology is an essential element for brain activity and function.\nWe take advantage of current availability of brain-wide neuron digital\nreconstructions of the Pyramidal cells from a mouse brain, and analyze several\nemergent features of brain-wide neuronal morphology. We observe that axonal\ntrees are self-affine while dendritic trees are self-similar. We also show that\ntree size appear to be random, independent of the number of dendrites within\nsingle neurons. Moreover, we consider inhomogeneous branching model which\nstochastically generates rooted 3-Cayley trees for the brain-wide neuron\ntopology. Based on estimated order-dependent branching probability from actual\naxonal and dendritic trees, our inhomogeneous model quantitatively captures a\nnumber of topological features including size and shape of both axons and\ndendrites. This sheds lights on a universal mechanism behind the topological\nformation of brain-wide axonal and dendritic trees.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:52:11 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Lin", "Congping", ""], ["Huang", "Yuanfei", ""], ["Quan", "Tingwei", ""], ["Zhang", "Yiwei", ""]]}, {"id": "1810.03296", "submitter": "Yi Yu", "authors": "Tony Sit, Zhiliang Ying and Yi Yu", "title": "Event History Analysis of Dynamic Communication Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis on networks has received growing attention due to demand\nfrom various emerging applications. In dynamic networks, one of the key\ninterests is to model the event history of time-stamped interactions amongst\nnodes. We propose to model dynamic directed communication networks via\nmultivariate counting processes. A pseudo partial likelihood approach is\nexploited to capture the network dependence structure. Asymptotic results of\nthe resulting estimation are established. Numerical results are performed to\ndemonstrate effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:51:46 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Sit", "Tony", ""], ["Ying", "Zhiliang", ""], ["Yu", "Yi", ""]]}, {"id": "1810.03319", "submitter": "Claudia Neves", "authors": "Christopher Jeffree and Cl\\'audia Neves", "title": "Tilting maximum Lq-Likelihood estimation for extreme values drawing on\n  block maxima", "comments": "Unfinished work with regard to a couple of aspects focused. One\n  author has withdrawn", "journal-ref": null, "doi": "10.13140/RG.2.2.23901.05600", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common anticipated difficulties in applying mainstream\nmaximum likelihood inference upon extreme values is articulated on the scarcity\nof extreme observations for bringing the extreme value theorem to hold across a\nseries of maxima. This paper introduces a new variant of the Lq-likelihood\nmethod through its linkage with a particular deformed logarithm which preserves\nthe self-dual property of the standard logarithm. Since the focus is on\nrelatively small samples consisting of those maximum values within each\nsub-sampled block (by splitting the sample into blocks of equal length), the\nmaximum Lq estimation will favour reducing uncertainty associated with the\nvariance leaving the bias unchallenged. A comprehensive simulation study\ndemonstrates that the introduction of a more sophisticated treatment of maximum\nlikelihood improves the estimation of extreme characteristics, with significant\nimplications for return-level estimation which is a crucial component in risk\nassessment for many operational settings prone to extreme hazards, such as\nearthquakes, floods or epidemics. We provide an illustrative example of how the\nproposed tilting of Lq-likelihood can improve inference on extreme events by\ndrawing on public health data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 08:35:50 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 11:52:36 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Jeffree", "Christopher", ""], ["Neves", "Cl\u00e1udia", ""]]}, {"id": "1810.03477", "submitter": "Yuming Wang", "authors": "Bin Wang, Ruodu Wang and Yuming Wang", "title": "Compatible Matrices of Spearman's Rank Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a negative answer to a long-standing open problem\non the compatibility of Spearman's rho matrices. Following an equivalence of\nSpearman's rho matrices and linear correlation matrices for dimensions up to 9\nin the literature, we show non-equivalence for dimensions 12 or higher. In\nparticular, we connect this problem with the existence of a random vector under\nsome linear projection restrictions in two characterization results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:03:03 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 08:59:06 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 17:57:17 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Bin", ""], ["Wang", "Ruodu", ""], ["Wang", "Yuming", ""]]}, {"id": "1810.03535", "submitter": "Malkhaz Shashiashvili", "authors": "Malkhaz Shashiashvili", "title": "Estimation of the weighted integrated square error of the Grenander\n  estimator by the Kolmogorov-Smirnov statistic", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the Grenander estimator of unbounded, in general,\nnonincreasing densities on the interval [0; 1] without any smoothness\nassumptions. For fixed number n of i.i.d. random vari- ables X1;X2; : : : ;Xn\nwith values in [0; 1] and the nonincreasing den- sity function f(x), 0 < x < 1,\nwe prove an inequality bounding the weighted integrated square error of the\nGrenander estimator with probability one by the classical Kolmogorov-Smirnov\nstatistic. Fur- ther, we consider some interesting implications of the latter\ninequality\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:43:49 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Shashiashvili", "Malkhaz", ""]]}, {"id": "1810.03814", "submitter": "Yueyong Shi", "authors": "Jian Huang, Yuling Jiao, Xiliang Lu, Yueyong Shi, Qinglong Yang", "title": "SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 04:44:42 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["Shi", "Yueyong", ""], ["Yang", "Qinglong", ""]]}, {"id": "1810.03819", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Sufficient and Necessary Conditions for the Identifiability of the\n  $Q$-matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted latent class models (RLCMs) have recently gained prominence in\neducational assessment, psychiatric evaluation, and medical diagnosis.\nDifferent from conventional latent class models, restrictions on the RLCM model\nparameters are imposed by a design matrix to respect practitioners' scientific\nassumptions. The design matrix, called $Q$-matrix in the cognitive diagnosis\nliterature, is usually constructed by practitioners and domain experts, yet it\nis subjective and could be misspecified. To address this problem, researchers\nhave proposed to estimate the $Q$-matrix from data. On the other hand, the\nfundamental learnability issue of the $Q$-matrix and model parameters remains\nunderexplored and existing studies often impose stronger than needed or even\nimpractical conditions. This paper proposes sufficient and necessary conditions\nfor joint identifiability of the $Q$-matrix and the RLCM model parameters under\ndifferent types of RLCMs. The developed identifiability conditions only depend\non the design matrix and are easy to verify in practice.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 05:28:27 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 17:26:59 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1810.04022", "submitter": "Liang Dai", "authors": "Liang Dai, Mohamed-Rafik Bouguelia", "title": "Testing exchangeability with martingale for change-point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new exchangeability test for a random sequence through a\nmartingale based approach. Its main contributions include: 1) an additive\nmartingale which is more amenable for designing exchangeability tests by\nexploiting the Hoeffding-Azuma lemma; 2) different betting functions for\nconstructing the additive martingale are studied. By choosing the underlying\nprobability density function of p-values as a betting function, it can be shown\nthat, when a change-point appears, a satisfying trade-off between the\nsmoothness and expected one-step increment of the martingale sequence can be\nobtained. An online algorithm based on Beta distribution parametrization for\nconstructing this betting function is discussed in detail as well.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 18:14:31 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 15:37:37 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Dai", "Liang", ""], ["Bouguelia", "Mohamed-Rafik", ""]]}, {"id": "1810.04090", "submitter": "Ruofei Zhao", "authors": "Ruofei Zhao, Yuanzhi Li, Yuekai Sun", "title": "Statistical Convergence of the EM Algorithm on Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence behavior of the Expectation Maximization (EM)\nalgorithm on Gaussian mixture models with an arbitrary number of mixture\ncomponents and mixing weights. We show that as long as the means of the\ncomponents are separated by at least $\\Omega(\\sqrt{\\min\\{M,d\\}})$, where $M$ is\nthe number of components and $d$ is the dimension, the EM algorithm converges\nlocally to the global optimum of the log-likelihood. Further, we show that the\nconvergence rate is linear and characterize the size of the basin of attraction\nto the global optimum.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 15:49:58 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Zhao", "Ruofei", ""], ["Li", "Yuanzhi", ""], ["Sun", "Yuekai", ""]]}, {"id": "1810.04443", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "On the Properties of Simulation-based Estimators in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Considering the increasing size of available data, the need for statistical\nmethods that control the finite sample bias is growing. This is mainly due to\nthe frequent settings where the number of variables is large and allowed to\nincrease with the sample size bringing standard inferential procedures to incur\nsignificant loss in terms of performance. Moreover, the complexity of\nstatistical models is also increasing thereby entailing important computational\nchallenges in constructing new estimators or in implementing classical ones. A\ntrade-off between numerical complexity and statistical properties is often\naccepted. However, numerically efficient estimators that are altogether\nunbiased, consistent and asymptotically normal in high dimensional problems\nwould generally be ideal. In this paper, we set a general framework from which\nsuch estimators can easily be derived for wide classes of models. This\nframework is based on the concepts that underlie simulation-based estimation\nmethods such as indirect inference. The approach allows various extensions\ncompared to previous results as it is adapted to possibly inconsistent\nestimators and is applicable to discrete models and/or models with a large\nnumber of parameters. We consider an algorithm, namely the Iterative Bootstrap\n(IB), to efficiently compute simulation-based estimators by showing its\nconvergence properties. Within this framework we also prove the properties of\nsimulation-based estimators, more specifically the unbiasedness, consistency\nand asymptotic normality when the number of parameters is allowed to increase\nwith the sample size. Therefore, an important implication of the proposed\napproach is that it allows to obtain unbiased estimators in finite samples.\nFinally, we study this approach when applied to three common models, namely\nlogistic regression, negative binomial regression and lasso regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:12:54 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 07:37:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1810.04448", "submitter": "Chuan-Long Xie", "authors": "Heng Peng, Chuanlong Xie, Jingxin Zhao", "title": "Fast Inference Procedures for Semivarying Coefficient Models via Local\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semivarying coefficient models are widely used in the application of\nfinance, economics, medical science and many other areas. The functional\ncoefficients are commonly estimated by local smoothing methods, e.g. local\nlinear estimator. This implies that one should implement the estimation\nprocedure for hundreds of times to obtain an estimate of one function. So the\ncomputation cost is very severe. In this paper, we give an insight to the\ntrade-off between statistical efficiency and computation simplicity, and\nproposes a fast inference procedure for semivarying coefficient model. In our\nmethod, the coefficient functions are approximated by piecewise constants,\nwhich is a simple and rough approximation. This makes our estimators easy to\nimplement and avoid repeat estimation. In this work, we shall show that though\nthese estimators are not asymptotically optimal, they are efficient enough for\nbuilding further inference procedure. Furthermore, three tests are brought out\nto check whether certain coefficient is constant. Our results clearly show that\nwhen the room for improving the asymptotic efficiency is limited, a proper\ntrade-off between statistical efficiency and computation simplicity can be\ntaken into consideration to improve the performance of the inference procedure.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:33:51 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 15:25:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Peng", "Heng", ""], ["Xie", "Chuanlong", ""], ["Zhao", "Jingxin", ""]]}, {"id": "1810.04466", "submitter": "Anna Czapkiewicz Dr", "authors": "Anna Czapkiewicz, Antoni Dawidowicz", "title": "On some Limit Theorem for Markov Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to describe conditions which guarantee a central\nlimit theorem for random variables, which distributions are controled by hidden\nMarkov chains. We proved that when a Markov chain is ergodic and random\nvariables fullfiled Lindeberg's condition then the Central Limit Theorem is\ntrue.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 11:45:27 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Czapkiewicz", "Anna", ""], ["Dawidowicz", "Antoni", ""]]}, {"id": "1810.04534", "submitter": "Romain Couillet", "authors": "Romain Couillet and Malik Tiomoko and Steeve Zozor and Eric Moisan", "title": "Random matrix-improved estimation of covariance matrix distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets $x_1^{(1)},\\ldots,x_{n_1}^{(1)}$ and\n$x_1^{(2)},\\ldots,x_{n_2}^{(2)}\\in\\mathbb{R}^p$ (or $\\mathbb{C}^p$) of random\nvectors with zero mean and positive definite covariance matrices $C_1$ and\n$C_2\\in\\mathbb{R}^{p\\times p}$ (or $\\mathbb{C}^{p\\times p}$), respectively,\nthis article provides novel estimators for a wide range of distances between\n$C_1$ and $C_2$ (along with divergences between some zero mean and covariance\n$C_1$ or $C_2$ probability measures) of the form $\\frac1p\\sum_{i=1}^n\nf(\\lambda_i(C_1^{-1}C_2))$ (with $\\lambda_i(X)$ the eigenvalues of matrix $X$).\nThese estimators are derived using recent advances in the field of random\nmatrix theory and are asymptotically consistent as $n_1,n_2,p\\to\\infty$ with\nnon trivial ratios $p/n_1<1$ and $p/n_2<1$ (the case $p/n_2>1$ is also\ndiscussed). A first \"generic\" estimator, valid for a large set of $f$\nfunctions, is provided under the form of a complex integral. Then, for a\nselected set of $f$'s of practical interest (namely, $f(t)=t$, $f(t)=\\log(t)$,\n$f(t)=\\log(1+st)$ and $f(t)=\\log^2(t)$), a closed-form expression is provided.\nBeside theoretical findings, simulation results suggest an outstanding\nperformance advantage for the proposed estimators when compared to the\nclassical \"plug-in\" estimator $\\frac1p\\sum_{i=1}^n f(\\lambda_i(\\hat\nC_1^{-1}\\hat C_2))$ (with $\\hat\nC_a=\\frac1{n_a}\\sum_{i=1}^{n_a}x_i^{(a)}x_i^{(a){\\sf T}}$), and this even for\nvery small values of $n_1,n_2,p$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 13:59:43 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Couillet", "Romain", ""], ["Tiomoko", "Malik", ""], ["Zozor", "Steeve", ""], ["Moisan", "Eric", ""]]}, {"id": "1810.04617", "submitter": "Mingao Yuan", "authors": "Mingao Yuan, Ruiqi Liu, Yang Feng and Zuofeng Shang", "title": "Testing Community Structures for Hypergraphs", "comments": "A revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many complex networks in real world can be formulated as hypergraphs where\ncommunity detection has been widely used. However, the fundamental question of\nwhether communities exist or not in an observed hypergraph still remains\nunresolved. The aim of the present paper is to tackle this important problem.\nSpecifically, we study when a hypergraph with community structure can be\nsuccessfully distinguished from its Erd\\\"{o}s-Renyi counterpart, and propose\nconcrete test statistics based on hypergraph cycles when the models are\ndistinguishable. Our contributions are summarized as follows. For uniform\nhypergraphs, we show that successful testing is always impossible when average\ndegree tends to zero, might be possible when average degree is bounded, and is\npossible when average degree is growing. We obtain asymptotic distributions of\nthe proposed test statistics and analyze their power. Our results for growing\ndegree case are further extended to nonuniform hypergraphs in which a new test\ninvolving both edge and hyperedge information is proposed. The novel aspect of\nour new test is that it is provably more powerful than the classic test\ninvolving only edge information. Simulation and real data analysis support our\ntheoretical findings. The proofs rely on Janson's contiguity theory\n(\\cite{J95}) and a high-moments driven asymptotic normality result by Gao and\nWormald (\\cite{GWALD}).\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:21:17 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 00:35:54 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 15:41:04 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yuan", "Mingao", ""], ["Liu", "Ruiqi", ""], ["Feng", "Yang", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1810.04725", "submitter": "Richard Chen", "authors": "Richard Y. Chen", "title": "Inference for Volatility Functionals of Multivariate It\\^o\n  Semimartingales Observed with Jump and Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the nonparametric inference for nonlinear volatility\nfunctionals of general multivariate It\\^o semimartingales, in high-frequency\nand noisy setting. Pre-averaging and truncation enable simultaneous handling of\nnoise and jumps. Second-order expansion reveals explicit biases and a pathway\nto bias correction. Estimators based on this framework achieve the optimal\nconvergence rate. A class of stable central limit theorems are attained with\nestimable asymptotic covariance matrices. This paper form a basis for infill\nasymptotic results of, for example, the realized Laplace transform, the\nrealized principal component analysis, the continuous-time linear regression,\nand the generalized method of integrated moments, hence helps to extend the\napplication scopes to more frequently sampled noisy data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 19:37:15 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 05:24:55 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chen", "Richard Y.", ""]]}, {"id": "1810.04806", "submitter": "Tamara Fernandez", "authors": "Tamara Fern\\'andez and Nicol\\'as Rivera", "title": "Kaplan-Meier V- and U-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Kaplan-Meier V- and U-statistics respectively defined\nas $\\theta(\\widehat{F}_n)=\\sum_{i,j}K(X_{[i:n]},X_{[j:n]})W_iW_j$ and\n$\\theta_U(\\widehat{F}_n)=\\sum_{i\\neq j}K(X_{[i:n]},X_{[j:n]})W_iW_j/\\sum_{i\\neq\nj}W_iW_j$, where $\\widehat{F}_n$ is the Kaplan-Meier estimator,\n$\\{W_1,\\ldots,W_n\\}$ are the Kaplan-Meier weights and $K:(0,\\infty)^2\\to\\mathbb\nR$ is a symmetric kernel. As in the canonical setting of uncensored data, we\ndifferentiate between two asymptotic behaviours for $\\theta(\\widehat{F}_n)$ and\n$\\theta_U(\\widehat{F}_n)$. Additionally, we derive an asymptotic canonical\nV-statistic representation of the Kaplan-Meier V- and U-statistics. By using\nthis representation we study properties of the asymptotic distribution.\nApplications to hypothesis testing are given.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 00:58:53 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:41:34 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Rivera", "Nicol\u00e1s", ""]]}, {"id": "1810.04859", "submitter": "Ekraam Sabir", "authors": "Dhruva Kartik, Ekraam Sabir, Urbashi Mitra and Prem Natarajan", "title": "Policy Design for Active Sequential Hypothesis Testing using Deep\n  Learning", "comments": "Accepted at 56th Annual Allerton Conference on Communication,\n  Control, and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG cs.SY math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory has been very successful in obtaining performance limits\nfor various problems such as communication, compression and hypothesis testing.\nLikewise, stochastic control theory provides a characterization of optimal\npolicies for Partially Observable Markov Decision Processes (POMDPs) using\ndynamic programming. However, finding optimal policies for these problems is\ncomputationally hard in general and thus, heuristic solutions are employed in\npractice. Deep learning can be used as a tool for designing better heuristics\nin such problems. In this paper, the problem of active sequential hypothesis\ntesting is considered. The goal is to design a policy that can reliably infer\nthe true hypothesis using as few samples as possible by adaptively selecting\nappropriate queries. This problem can be modeled as a POMDP and bounds on its\nvalue function exist in literature. However, optimal policies have not been\nidentified and various heuristics are used. In this paper, two new heuristics\nare proposed: one based on deep reinforcement learning and another based on a\nKL-divergence zero-sum game. These heuristics are compared with\nstate-of-the-art solutions and it is demonstrated using numerical experiments\nthat the proposed heuristics can achieve significantly better performance than\nexisting methods in some scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 06:15:05 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Kartik", "Dhruva", ""], ["Sabir", "Ekraam", ""], ["Mitra", "Urbashi", ""], ["Natarajan", "Prem", ""]]}, {"id": "1810.04900", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Fangyuan Yu", "title": "Central Limit Theorems for Coupled Particle Filters", "comments": null, "journal-ref": "Adv. Appl. Probab. 52 (2020) 942-1001", "doi": "10.1017/apr.2020.27", "report-no": null, "categories": "math.ST math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we prove a new central limit theorem (CLT) for coupled\nparticle filters (CPFs). CPFs are used for the sequential estimation of the\ndifference of expectations w.r.t. filters which are in some sense close.\nExamples include the estimation of the filtering distribution associated to\ndifferent parameters (finite difference estimation) and filters associated to\npartially observed discretized diffusion processes (PODDP) and the\nimplementation of the multilevel Monte Carlo (MLMC) identity. We develop new\ntheory for CPFs and based upon several results, we propose a new CPF which\napproximates the maximal coupling (MCPF) of a pair of predictor distributions.\nIn the context of ML estimation associated to PODDP with discretization\n$\\Delta_l$ we show that the MCPF and the approach in Jasra et al. (2018) have,\nunder assumptions, an asymptotic variance that is upper-bounded by an\nexpression that is (almost) $\\mathcal{O}(\\Delta_l)$, uniformly in time. The\n$\\mathcal{O}(\\Delta_l)$ rate preserves the so-called forward rate of the\ndiffusion in some scenarios which is not the case for the CPF in Jasra et al\n(2017).\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 08:37:56 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 05:53:41 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Jasra", "Ajay", ""], ["Yu", "Fangyuan", ""]]}, {"id": "1810.04946", "submitter": "Chris Oates", "authors": "Alessandro Barp, Chris. J. Oates, Emilio Porcu, Mark Girolami", "title": "A Riemann-Stein Kernel Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and studies a numerical method for approximation of\nposterior expectations based on interpolation with a Stein reproducing kernel.\nFinite-sample-size bounds on the approximation error are established for\nposterior distributions supported on a compact Riemannian manifold, and we\nrelate these to a kernel Stein discrepancy (KSD). Moreover, we prove in our\nsetting that the KSD is equivalent to Sobolev discrepancy and, in doing so, we\ncompletely characterise the convergence-determining properties of KSD. Our\ncontribution is rooted in a novel combination of Stein's method, the theory of\nreproducing kernels, and existence and regularity results for partial\ndifferential equations on a Riemannian manifold.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 10:43:25 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 11:24:14 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 14:50:57 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Barp", "Alessandro", ""], ["Oates", "Chris. J.", ""], ["Porcu", "Emilio", ""], ["Girolami", "Mark", ""]]}, {"id": "1810.05374", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel P. Simpson, Yuling Yao, Andrew Gelman", "title": "Limitations of \"Limitations of Bayesian leave-one-out cross-validation\n  for model selection\"", "comments": "To appear in Computational Brain & Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an invited discussion of the article by Gronau and\nWagenmakers (2018) that can be found at\nhttps://dx.doi.org/10.1007/s42113-018-0011-7.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 06:45:46 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel P.", ""], ["Yao", "Yuling", ""], ["Gelman", "Andrew", ""]]}, {"id": "1810.05398", "submitter": "Ziheng Yang Prof.", "authors": "Ziheng Yang and Tianqi Zhu", "title": "The good, the bad, and the ugly: Bayesian model selection produces\n  spurious posterior probabilities for phylogenetic trees", "comments": "6 pages, plus 3 pages of SI", "journal-ref": "PNAS 2018", "doi": null, "report-no": null, "categories": "math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian method is noted to produce spuriously high posterior\nprobabilities for phylogenetic trees in analysis of large datasets, but the\nprecise reasons for this over-confidence are unknown. In general, the\nperformance of Bayesian selection of misspecified models is poorly understood,\neven though this is of great scientific interest since models are never true in\nreal data analysis. Here we characterize the asymptotic behavior of Bayesian\nmodel selection and show that when the competing models are equally wrong,\nBayesian model selection exhibits surprising and polarized behaviors in large\ndatasets, supporting one model with full force while rejecting the others. If\none model is slightly less wrong than the other, the less wrong model will\neventually win when the amount of data increases, but the method may become\noverconfident before it becomes reliable. We suggest that this extreme behavior\nmay be a major factor for the spuriously high posterior probabilities for\nevolutionary trees. The philosophical implications of our results to the\napplication of Bayesian model selection to evaluate opposing scientific\nhypotheses are yet to be explored, as are the behaviors of non-Bayesian methods\nin similar situations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:19:50 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Yang", "Ziheng", ""], ["Zhu", "Tianqi", ""]]}, {"id": "1810.05478", "submitter": "Mohamed Ndaoud", "authors": "Mohamed Ndaoud", "title": "Interplay of minimax estimation and minimax support recovery under\n  sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new notion of scaled minimaxity for sparse\nestimation in high-dimensional linear regression model. We present more\noptimistic lower bounds than the one given by the classical minimax theory and\nhence improve on existing results. We recover sharp results for the global\nminimaxity as a consequence of our study. Fixing the scale of the\nsignal-to-noise ratio, we prove that the estimation error can be much smaller\nthan the global minimax error. We construct a new optimal estimator for the\nscaled minimax sparse estimation. An optimal adaptive procedure is also\ndescribed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:32:00 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Ndaoud", "Mohamed", ""]]}, {"id": "1810.05594", "submitter": "Friederike Johanna Laus", "authors": "Friederike Laus, Gabriele Steidl", "title": "Multivariate Myriad Filters based on Parameter Estimation of Student-$t$\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this study is twofold: First, we propose an efficient\nalgorithm for the computation of the (weighted) maximum likelihood estimators\nfor the parameters of the multivariate Student-$t$ distribution, which we call\ngeneralized multivariate myriad filter. Second, we use the generalized\nmultivariate myriad filter in a nonlocal framework for the denoising of images\ncorrupted by different kinds of noise. The resulting method is very flexible\nand can handle heavy-tailed noise such as Cauchy noise, as well as the other\nextreme, namely Gaussian noise. Furthermore, we detail how the limiting case\n$\\nu \\rightarrow 0$ of the projected normal distribution in two dimensions can\nbe used for the robust denoising of periodic data, in particular for images\nwith circular data corrupted by wrapped Cauchy noise.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 16:18:05 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 16:12:00 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 10:21:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Laus", "Friederike", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1810.05752", "submitter": "Jeongyeol Kwon", "authors": "Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, Damek\n  Davis", "title": "Global Convergence of EM Algorithm for Mixtures of Two Component Linear\n  Regression", "comments": "To appear in the proceedings of the Conference on Learning Theory\n  (COLT), 2019. This paper results from a merger of work from two groups who\n  work on the problem at the same time", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation-Maximization algorithm is perhaps the most broadly used\nalgorithm for inference of latent variable problems. A theoretical\nunderstanding of its performance, however, largely remains lacking. Recent\nresults established that EM enjoys global convergence for Gaussian Mixture\nModels. For Mixed Linear Regression, however, only local convergence results\nhave been established, and those only for the high SNR regime. We show here\nthat EM converges for mixed linear regression with two components (it is known\nthat it may fail to converge for three or more), and moreover that this\nconvergence holds for random initialization. Our analysis reveals that EM\nexhibits very different behavior in Mixed Linear Regression from its behavior\nin Gaussian Mixture Models, and hence our proofs require the development of\nseveral new ideas.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 22:59:30 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 22:40:37 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 22:29:36 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 21:29:50 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Kwon", "Jeongyeol", ""], ["Qian", "Wei", ""], ["Caramanis", "Constantine", ""], ["Chen", "Yudong", ""], ["Davis", "Damek", ""]]}, {"id": "1810.05935", "submitter": "Jisu Kim", "authors": "Jisu Kim, Jaehyeok Shin, Alessandro Rinaldo, Larry Wasserman", "title": "Uniform Convergence Rate of the Kernel Density Estimator Adaptive to\n  Intrinsic Volume Dimension", "comments": "51 pages, to be published in Proceedings of Thirty-sixth\n  International Conference on Machine Learning (ICML 2019), Volume 97, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive concentration inequalities for the supremum norm of the difference\nbetween a kernel density estimator (KDE) and its point-wise expectation that\nhold uniformly over the selection of the bandwidth and under weaker conditions\non the kernel and the data generating distribution than previously used in the\nliterature. We first propose a novel concept, called the volume dimension, to\nmeasure the intrinsic dimension of the support of a probability distribution\nbased on the rates of decay of the probability of vanishing Euclidean balls.\nOur bounds depend on the volume dimension and generalize the existing bounds\nderived in the literature. In particular, when the data-generating distribution\nhas a bounded Lebesgue density or is supported on a sufficiently well-behaved\nlower-dimensional manifold, our bound recovers the same convergence rate\ndepending on the intrinsic dimension of the support as ones known in the\nliterature. At the same time, our results apply to more general cases, such as\nthe ones of distribution with unbounded densities or supported on a mixture of\nmanifolds with different dimensions. Analogous bounds are derived for the\nderivative of the KDE, of any order. Our results are generally applicable but\nare especially useful for problems in geometric inference and topological data\nanalysis, including level set estimation, density-based clustering, modal\nclustering and mode hunting, ridge estimation and persistent homology.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 22:31:22 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 22:03:53 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 13:53:04 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kim", "Jisu", ""], ["Shin", "Jaehyeok", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1810.06089", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Sifan Liu", "title": "Asymptotics for Sketching in Least Squares Regression", "comments": null, "journal-ref": "Updated manuscript to be consistent with version at NeurIPS 2019", "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a least squares regression problem where the data has been\ngenerated from a linear model, and we are interested to learn the unknown\nregression parameters. We consider \"sketch-and-solve\" methods that randomly\nproject the data first, and do regression after. Previous works have analyzed\nthe statistical and computational performance of such methods. However, the\nexisting analysis is not fine-grained enough to show the fundamental\ndifferences between various methods, such as the Subsampled Randomized Hadamard\nTransform (SRHT) and Gaussian projections. In this paper, we make progress on\nthis problem, working in an asymptotic framework where the number of datapoints\nand dimension of features goes to infinity. We find the limits of the accuracy\nloss (for estimation and test error) incurred by popular sketching methods. We\nshow separation between different methods, so that SRHT is better than Gaussian\nprojections. Our theoretical results are verified on both real and synthetic\ndata. The analysis of SRHT relies on novel methods from random matrix theory\nthat may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 19:48:05 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 19:25:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dobriban", "Edgar", ""], ["Liu", "Sifan", ""]]}, {"id": "1810.06138", "submitter": "Lu Mao", "authors": "Lu Mao", "title": "A unified approach to calculation of information operators in\n  semiparametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinite-dimensional information operator for the nuisance parameter\nplays a key role in semiparametric inference, as it is closely related to the\nregular estimability of the target parameter. Calculation of information\noperators has traditionally proceeded in a case-by-case manner and has easily\nentailed lengthy derivations with complicated arguments. We develop a unified\nframework for this task by exploiting commonality in the form of semiparametric\nlikelihoods. The general formula allows one to derive information operators\nwith simple calculus and, if necessary at all, a minimal amount of\nprobabilistic evaluations. This streamlined approach shows its efficiency and\nversatility in application to a number of popular models in survival analysis,\ninverse problems, and missing data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 00:57:20 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Mao", "Lu", ""]]}, {"id": "1810.06226", "submitter": "Steffen Betsch", "authors": "Steffen Betsch and Bruno Ebner", "title": "Fixed point characterizations of continuous univariate probability\n  distributions and their applications", "comments": "40 pages", "journal-ref": "Annals of the Institute of Statistical Mathematics, Volume 73,\n  Issue 1, pages 31-59, (2021)", "doi": "10.1007/s10463-019-00735-1", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By extrapolating the explicit formula of the zero-bias distribution occurring\nin the context of Stein's method, we construct characterization identities for\na large class of absolutely continuous univariate distributions. Instead of\ntrying to derive characterizing distributional transformations that inherit\ncertain structures for the use in further theoretic endeavours, we focus on\nexplicit representations given through a formula for the density- or\ndistribution function. The results we establish with this ambition feature\nimmediate applications in the area of goodness-of-fit testing. We draw up a\nblueprint for the construction of tests of fit that include procedures for many\ndistributions for which little (if any) practicable tests are known. To\nillustrate this last point, we construct a test for the Burr Type XII\ndistribution for which, to our knowledge, not a single test is known aside from\nthe classical universal procedures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:43:33 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 16:56:33 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""]]}, {"id": "1810.06227", "submitter": "Julyan Arbel", "authors": "Caroline Lawless and Julyan Arbel (Inria)", "title": "A simple proof of Pitman-Yor's Chinese restaurant process from its\n  stick-breaking representation", "comments": "11 pages", "journal-ref": "Dependence modeling, 2019", "doi": "10.1515/demo-2019-0003", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time, the Dirichlet process has been the gold standard discrete\nrandom measure in Bayesian nonparametrics. The Pitman--Yor process provides a\nsimple and mathematically tractable generalization, allowing for a very\nflexible control of the clustering behaviour. Two commonly used representations\nof the Pitman--Yor process are the stick-breaking process and the Chinese\nrestaurant process. The former is a constructive representation of the process\nwhich turns out very handy for practical implementation, while the latter\ndescribes the partition distribution induced. However, the usual proof of the\nconnection between them is indirect and involves measure theory. We provide\nhere an elementary proof of Pitman--Yor's Chinese Restaurant process from its\nstick-breaking representation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 08:45:20 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 14:26:59 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Lawless", "Caroline", "", "Inria"], ["Arbel", "Julyan", "", "Inria"]]}, {"id": "1810.06234", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "On kernel-based estimation of conditional Kendall's tau: finite-distance\n  bounds and asymptotic behavior", "comments": "29 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1802.07613", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric estimators of conditional Kendall's tau, a measure of\nconcordance between two random variables given some covariates. We prove\nnon-asymptotic bounds with explicit constants, that hold with high\nprobabilities. We provide \"direct proofs\" of the consistency and the asymptotic\nlaw of conditional Kendall's tau. A simulation study evaluates the numerical\nperformance of such nonparametric estimators.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 09:10:50 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 17:49:00 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "1810.06348", "submitter": "Jonathan Hill", "authors": "Jonathan B. Hill", "title": "Weak-Identification Robust Wild Bootstrap applied to a Consistent Model\n  Specification Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new robust bootstrap method for a test when there is a nuisance\nparameter under the alternative, and some parameters are possibly weakly or\nnon-identified. We focus on a Bierens (1990)-type conditional moment test of\nomitted nonlinearity for convenience, and because of difficulties that have\nbeen ignored to date. Existing methods include the supremum p-value which\npromotes a conservative test that is generally not consistent, and test\nstatistic transforms like the supremum and average for which bootstrap methods\nare not valid under weak identification. We propose a new wild bootstrap method\nfor p-value computation by targeting specific identification cases. We then\ncombine bootstrapped p-values across polar identification cases to form an\nasymptotically valid p-value approximation that is robust to any identification\ncase. The wild bootstrap does not require knowledge of the covariance structure\nof the bootstrapped processes, whereas Andrews and Cheng's (2012, 2013, 2014)\nsimulation approach generally does. Our method allows for robust bootstrap\ncritical value computation as well. Our bootstrap method (like conventional\nones) does not lead to a consistent p-value approximation for test statistic\nfunctions like the supremum and average. We therefore smooth over the robust\nbootstrapped p-value as the basis for several tests which achieve the correct\nasymptotic level, and are consistent, for any degree of identification. They\nalso achieve uniform size control. A simulation study reveals possibly large\nempirical size distortions in non-robust tests when weak or non-identification\narises. One of our smoothed p-value tests, however, dominates all other tests\nby delivering accurate empirical size and comparatively high power.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 13:45:58 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 14:14:56 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 16:59:12 GMT"}, {"version": "v4", "created": "Wed, 25 Mar 2020 18:53:06 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Hill", "Jonathan B.", ""]]}, {"id": "1810.06380", "submitter": "Michael Krikheli", "authors": "Michael Krikheli and Amir Leshem", "title": "Finite sample performance of linear least squares estimation", "comments": "arXiv admin note: text overlap with arXiv:1710.11594", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear Least Squares is a very well known technique for parameter estimation,\nwhich is used even when sub-optimal, because of its very low computational\nrequirements and the fact that exact knowledge of the noise statistics is not\nrequired. Surprisingly, bounding the probability of large errors with finitely\nmany samples has been left open, especially when dealing with correlated noise\nwith unknown covariance. In this paper we analyze the finite sample performance\nof the linear least squares estimator. Using these bounds we obtain accurate\nbounds on the tail of the estimator's distribution. We show the fast\nexponential convergence of the number of samples required to ensure a given\naccuracy with high probability. We analyze a sub-Gaussian setting with a fixed\nor random design matrix of the linear least squares problem. We also extend the\nresults to the case of a martingale difference noise sequence. Our analysis\nmethod is simple and uses simple $L_{\\infty}$ type bounds on the estimation\nerror. We also provide probabilistic finite sample bounds on the estimation\nerror $L_2$ norm. The tightness of the bounds is tested through simulation. We\ndemonstrate that our results are tighter than previously proposed bounds for\n$L_{\\infty}$ norm of the error. The proposed bounds make it possible to predict\nthe number of samples required for least squares estimation even when the least\nsquares is sub-optimal and is used for computational simplicity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 09:54:05 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Krikheli", "Michael", ""], ["Leshem", "Amir", ""]]}, {"id": "1810.06397", "submitter": "Lei Wu", "authors": "Weinan E, Chao Ma, Lei Wu", "title": "A Priori Estimates of the Population Risk for Two-layer Neural Networks", "comments": "Published version", "journal-ref": "Communications in Mathematical Sciences, Volume 17(2019)", "doi": "10.4310/CMS.2019.v17.n5.a11", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New estimates for the population risk are established for two-layer neural\nnetworks. These estimates are nearly optimal in the sense that the error rates\nscale in the same way as the Monte Carlo error rates. They are equally\neffective in the over-parametrized regime when the network size is much larger\nthan the size of the dataset. These new estimates are a priori in nature in the\nsense that the bounds depend only on some norms of the underlying functions to\nbe fitted, not the parameters in the model, in contrast with most existing\nresults which are a posteriori in nature. Using these a priori estimates, we\nprovide a perspective for understanding why two-layer neural networks perform\nbetter than the related kernel methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:38:56 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 22:15:01 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 23:33:56 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["E", "Weinan", ""], ["Ma", "Chao", ""], ["Wu", "Lei", ""]]}, {"id": "1810.06474", "submitter": "M. Ros\\'ario Oliveira", "authors": "M. Ros\\'ario Oliveira, Margarida Azeitona, Ant\\'onio Pacheco, Rui\n  Valadas", "title": "Association measures for interval variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic Data Analysis (SDA) is a relatively new field of statistics that\nextends conventional data analysis by taking into account intrinsic data\nvariability and structure. Unlike conventional data analysis, in SDA the\nfeatures characterizing the data can be multi-valued, such as intervals or\nhistograms. SDA has been mainly approached from a sampling perspective. In this\nwork, we propose a model that links the micro-data and macro-data of\ninterval-valued symbolic variables, which takes a populational perspective.\nUsing this model, we derive the micro-data assumptions underlying the various\ndefinitions of symbolic covariance matrices proposed in the literature, and\nshow that these assumptions can be too restrictive, raising applicability\nconcerns. We analyze the various definitions using worked examples and four\ndatasets. Our results show that the existence/absence of correlations in the\nmacro-data may not be correctly captured by the definitions of symbolic\ncovariance matrices and that, in real data, there can be a strong divergence\nbetween these definitions. Thus, in order to select the most appropriate\ndefinition, one must have some knowledge about the micro-data structure.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:45:45 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:08:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Oliveira", "M. Ros\u00e1rio", ""], ["Azeitona", "Margarida", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1810.06787", "submitter": "Haihan Tang", "authors": "Christian M. Hafner and Oliver B. Linton and Haihan Tang", "title": "Supplementary Material for \"Estimation of a Multiplicative Correlation\n  Structure in the Large Dimensional Case\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supplementary Material for \"Estimation of a Multiplicative Correlation\nStructure in the Large Dimensional Case\"\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 01:54:18 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 12:48:34 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Hafner", "Christian M.", ""], ["Linton", "Oliver B.", ""], ["Tang", "Haihan", ""]]}, {"id": "1810.06838", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii Ostrovskii (USC), Francis Bach (DI-ENS, SIERRA)", "title": "Finite-sample analysis of M-estimators using self-concordance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical asymptotic theory for parametric $M$-estimators guarantees\nthat, in the limit of infinite sample size, the excess risk has a chi-square\ntype distribution, even in the misspecified case. We demonstrate how\nself-concordance of the loss allows to characterize the critical sample size\nsufficient to guarantee a chi-square type in-probability bound for the excess\nrisk. Specifically, we consider two classes of losses: (i) self-concordant\nlosses in the classical sense of Nesterov and Nemirovski, i.e., whose third\nderivative is uniformly bounded with the $3/2$ power of the second derivative;\n(ii) pseudo self-concordant losses, for which the power is removed. These\nclasses contain losses corresponding to several generalized linear models,\nincluding the logistic loss and pseudo-Huber losses. Our basic result under\nminimal assumptions bounds the critical sample size by $O(d \\cdot\nd_{\\text{eff}}),$ where $d$ the parameter dimension and $d_{\\text{eff}}$ the\neffective dimension that accounts for model misspecification. In contrast to\nthe existing results, we only impose local assumptions that concern the\npopulation risk minimizer $\\theta_*$. Namely, we assume that the calibrated\ndesign, i.e., design scaled by the square root of the second derivative of the\nloss, is subgaussian at $\\theta_*$. Besides, for type-ii losses we require\nboundedness of a certain measure of curvature of the population risk at\n$\\theta_*$.Our improved result bounds the critical sample size from above as\n$O(\\max\\{d_{\\text{eff}}, d \\log d\\})$ under slightly stronger assumptions.\nNamely, the local assumptions must hold in the neighborhood of $\\theta_*$ given\nby the Dikin ellipsoid of the population risk. Interestingly, we find that, for\nlogistic regression with Gaussian design, there is no actual restriction of\nconditions: the subgaussian parameter and curvature measure remain\nnear-constant over the Dikin ellipsoid. Finally, we extend some of these\nresults to $\\ell_1$-penalized estimators in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:39:10 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 14:21:57 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ostrovskii", "Dmitrii", "", "USC"], ["Bach", "Francis", "", "DI-ENS, SIERRA"]]}, {"id": "1810.06839", "submitter": "Alex Nowak-Vila", "authors": "Alex Nowak-Vila (SIERRA, PSL), Francis Bach (SIERRA, PSL), Alessandro\n  Rudi (SIERRA, PSL)", "title": "Sharp Analysis of Learning with Discrete Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of devising learning strategies for discrete losses (e.g.,\nmultilabeling, ranking) is currently addressed with methods and theoretical\nanalyses ad-hoc for each loss. In this paper we study a least-squares framework\nto systematically design learning algorithms for discrete losses, with\nquantitative characterizations in terms of statistical and computational\ncomplexity. In particular we improve existing results by providing explicit\ndependence on the number of labels for a wide class of losses and faster\nlearning rates in conditions of low-noise. Theoretical results are complemented\nwith experiments on real datasets, showing the effectiveness of the proposed\ngeneral approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:44:42 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Nowak-Vila", "Alex", "", "SIERRA, PSL"], ["Bach", "Francis", "", "SIERRA, PSL"], ["Rudi", "Alessandro", "", "SIERRA, PSL"]]}, {"id": "1810.06989", "submitter": "Marianna Pensky", "authors": "Rasika Rajapakshage and Marianna Pensky", "title": "Clustering in statistical ill-posed linear inverse problems", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical linear inverse problems, one needs to recover classes of\nsimilar curves from their noisy images under an operator that does not have a\nbounded inverse. Problems of this kind appear in many areas of application.\nRoutinely, in such problems clustering is carried out at the pre-processing\nstep and then the inverse problem is solved for each of the cluster averages\nseparately. As a result, the errors of the procedures are usually examined for\nthe estimation step only. The objective of this paper is to examine, both\ntheoretically and via simulations, the effect of clustering on the accuracy of\nthe solutions of general ill-posed linear inverse problems. In particular, we\nassume that one observes $X_m = A f_m + \\delta \\epsilon_m$, $m=1, \\cdots, M$,\nwhere functions $f_m$ can be grouped into $K$ classes and one needs to recover\na vector function ${\\bf f}= (f_1,\\cdots, f_M)^T$. We construct an estimators\nfor ${\\bf f}$ as a solution of a penalized optimization problem and derive an\noracle inequality for its precision. By deriving upper and minimax lower bounds\nfor the error, we confirm that the estimator is minimax optimal or nearly\nminimax optimal up to a logarithmic factor of the number of observations. One\nof the advantages of our estimation procedure is that we do not assume that the\nnumber of clusters is known in advance. We conclude that clustering at the\npre-processing step is beneficial when the problem is moderately ill-posed. It\nshould be applied with extreme care when the problem is severely ill-posed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:37:33 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 14:42:15 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 01:02:24 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Rajapakshage", "Rasika", ""], ["Pensky", "Marianna", ""]]}, {"id": "1810.07016", "submitter": "Marianna Pensky", "authors": "Ramchandra Rimal and Marianna Pensky", "title": "Density Deconvolution with Small Berkson Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper studies density deconvolution in the presence of small\nBerkson errors, in particular, when the variances of the errors tend to zero as\nthe sample size grows. It is known that when the Berkson errors are present, in\nsome cases, the unknown density estimator can be obtain by simple averaging\nwithout using kernels. However, this may not be the case when Berkson errors\nare asymptotically small. By treating the former case as a kernel estimator\nwith the zero bandwidth, we obtain the optimal expressions for the bandwidth.\nWe show that the density of Berkson errors acts as a regularizer, so that the\nkernel estimator is unnecessary when the variance of Berkson errors lies above\nsome threshold that depends on the on the shapes of the densities in the model\nand the number of observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:10:52 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Rimal", "Ramchandra", ""], ["Pensky", "Marianna", ""]]}, {"id": "1810.07126", "submitter": "Marius Hofert", "authors": "Marius Hofert, Takaaki Koike", "title": "Compatibility and attainability of matrices of correlation-based\n  measures of concordance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of concordance have been widely used in insurance and risk\nmanagement to summarize non-linear dependence among risks modeled by random\nvariables, which Pearson's correlation coefficient cannot capture. However,\npopular measures of concordance, such as Spearman's rho and Blomqvist's beta,\nappear as classical correlations of transformed random variables. We\ncharacterize a whole class of such concordance measures arising from\ncorrelations of transformed random variables, which includes Spearman's rho,\nBlomqvist's beta and van der Waerden's coefficient as special cases.\nCompatibility and attainability of square matrices with entries given by such\nmeasures are studied, that is, whether a given square matrix of such measures\nof concordance can be realized for some random vector and how such a random\nvector can be constructed. Compatibility and attainability of block matrices\nand hierarchical matrices are also studied due to their practical importance in\ninsurance and risk management. In particular, a subclass of attainable block\nSpearman's rho matrices is proposed to compensate for the drawback that\nSpearman's rho matrices are in general not attainable for dimensions larger\nthan four. Another result concerns a novel analytical form of the Cholesky\nfactor of block matrices which allows one, for example, to construct random\nvectors with given block matrices of van der Waerden's coefficient.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 16:44:45 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 17:26:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Hofert", "Marius", ""], ["Koike", "Takaaki", ""]]}, {"id": "1810.07138", "submitter": "Donald Richards", "authors": "Elena Hadjicosta, Donald Richards", "title": "Integral Transform Methods in Goodness-of-Fit Testing, I: The Gamma\n  Distributions", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the method of Hankel transforms to develop goodness-of-fit tests for\ngamma distributions with given shape parameter and unknown rate parameter,\nthereby extending results of Baringhaus and Taherizadeh (2010) on the\nexponential distributions. We derive the limiting null distribution of the test\nstatistic as an integrated squared Gaussian process, obtain the corresponding\ncovariance operator, and oscillation properties of its eigenfunctions. We show\nthat the eigenvalues of the operator satisfy an interlacing property, and we\napply that property in approximating critical values of the test statistic in\none of the two applications to data considered. Further, we establish the\nconsistency of the test. In studying properties of the test statistic under a\nvariety of contiguous alternatives, we obtain the asymptotic distribution of\nthe test statistic for gamma alternatives with varying rate or shape parameters\nand for a class of contaminated gamma models. We investigate the approximate\nBahadur slope of the test statistic under local alternatives and we establish\nthe validity of the Wieand condition, under which the approaches through the\napproximate Bahadur efficiency and the Pitman efficiency are in accord.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 17:06:31 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hadjicosta", "Elena", ""], ["Richards", "Donald", ""]]}, {"id": "1810.07283", "submitter": "Min Ye", "authors": "Min Ye and Alexander Barg", "title": "Optimal locally private estimation under $\\ell_p$ loss for $1\\le p\\le 2$", "comments": "This paper generalizes the optimality results of the preprint\n  arXiv:1708.00059 from $ell_2$ to a broader class of loss functions. The new\n  approach taken here also results in a much shorter proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimax estimation problem of a discrete distribution with\nsupport size $k$ under locally differential privacy constraints. A\nprivatization scheme is applied to each raw sample independently, and we need\nto estimate the distribution of the raw samples from the privatized samples. A\npositive number $\\epsilon$ measures the privacy level of a privatization\nscheme.\n  In our previous work (IEEE Trans. Inform. Theory, 2018), we proposed a family\nof new privatization schemes and the corresponding estimator. We also proved\nthat our scheme and estimator are order optimal in the regime $e^{\\epsilon} \\ll\nk$ under both $\\ell_2^2$ (mean square) and $\\ell_1$ loss. In this paper, we\nsharpen this result by showing asymptotic optimality of the proposed scheme\nunder the $\\ell_p^p$ loss for all $1\\le p\\le 2.$ More precisely, we show that\nfor any $p\\in[1,2]$ and any $k$ and $\\epsilon,$ the ratio between the\nworst-case $\\ell_p^p$ estimation loss of our scheme and the optimal value\napproaches $1$ as the number of samples tends to infinity. The lower bound on\nthe minimax risk of private estimation that we establish as a part of the proof\nis valid for any loss function $\\ell_p^p, p\\ge 1.$\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 21:23:32 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Ye", "Min", ""], ["Barg", "Alexander", ""]]}, {"id": "1810.07403", "submitter": "Behrooz Ghorbani", "authors": "David L. Donoho, Behrooz Ghorbani", "title": "Optimal Covariance Estimation for Condition Number Loss in the Spiked\n  Model", "comments": "85 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation of the covariance matrix under relative condition number\nloss $\\kappa(\\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2})$, where $\\kappa(\\Delta)$\nis the condition number of matrix $\\Delta$, and $\\hat{\\Sigma}$ and $\\Sigma$ are\nthe estimated and theoretical covariance matrices. Optimality in $\\kappa$-loss\nprovides optimal guarantees in two stylized applications: Multi-User Covariance\nEstimation and Multi-Task Linear Discriminant Analysis. We assume the so-called\nspiked covariance model for $\\Sigma$, and exploit recent advances in\nunderstanding that model, to derive a nonlinear shrinker which is\nasymptotically optimal among orthogonally-equivariant procedures. In our\nasymptotic study, the number of variables $p$ is comparable to the number of\nobservations $n$. The form of the optimal nonlinearity depends on the aspect\nratio $\\gamma=p/n$ of the data matrix and on the top eigenvalue of $\\Sigma$.\nFor $\\gamma > 0.618...$, even dependence on the top eigenvalue can be avoided.\nThe optimal shrinker has two notable properties. First, when $p/n \\rightarrow\n\\gamma \\gg 1$ is large, it shrinks even very large eigenvalues substantially,\nby a factor $1/(1+\\gamma)$. Second, even for moderate $\\gamma$, certain highly\nstatistically significant eigencomponents will be completely suppressed. We\nshow that when $\\gamma \\gg 1$ is large, purely diagonal covariance matrices can\nbe optimal, despite the top eigenvalues being large and the empirical\neigenvalues being highly statistically significant. This aligns with\npractitioner experience. We identify intuitively reasonable procedures with\nsmall worst-case relative regret - the simplest being generalized soft\nthresholding having threshold at the bulk edge and slope $(1+\\gamma)^{-1}$\nabove the bulk. For $\\gamma < 2$ it has at most a few percent relative regret.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 07:11:02 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Donoho", "David L.", ""], ["Ghorbani", "Behrooz", ""]]}, {"id": "1810.07738", "submitter": "Robert MacKay", "authors": "Robert S. MacKay, Nicholas E. Phillips", "title": "A natural 4-parameter family of covariance functions for stationary\n  Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A four-parameter family of covariance functions for stationary Gaussian\nprocesses is presented. We call it 2Dsys. It corresponds to the general\nsolution of an autonomous second-order linear stochastic differential equation,\nthus arises naturally from modelling. It covers underdamped and overdamped\nsystems, so it is proposed to use this family when one wishes to decide if a\ntime-series corresponds to stochastically forced damped oscillations or a\nstochastically forced overdamped system.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 19:05:25 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["MacKay", "Robert S.", ""], ["Phillips", "Nicholas E.", ""]]}, {"id": "1810.07750", "submitter": "Celia Garc\\'ia-Pareja", "authors": "Celia Garc\\'ia-Pareja and Matteo Bottai", "title": "On mean decomposition for summarizing conditional distributions", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.208", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a summary measure defined as the expected value of a random\nvariable over disjoint subsets of its support that are specified by a given\ngrid of proportions, and consider its use in a regression modeling framework.\nThe obtained regression coefficients provide information about the effect of a\nset of given covariates on the variable's expectation in each specified subset.\nWe derive asymptotic properties for a general estimation approach that are\nbased on those of the chosen quantile function estimator for the underlying\nprobability distribution. A bound on the variance of this general estimator is\nalso provided, which relates its precision to the given grid of proportions and\nthat of the quantile function estimator, as shown in a simulation example. We\nillustrate the use of our method and its advantages in two real data\napplications, where we show its potential for solving resource-allocation and\nintervention-evaluation problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 19:42:23 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Garc\u00eda-Pareja", "Celia", ""], ["Bottai", "Matteo", ""]]}, {"id": "1810.07921", "submitter": "Remi Gribonval", "authors": "Ivan Dokmani\\'c, R\\'emi Gribonval (PANAMA)", "title": "Concentration of the Frobenius norm of generalized matrix inverses", "comments": "Revised/condensed/renamed version of arXiv:1706.08701", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications it is useful to replace the Moore-Penrose pseudoinverse\n(MPP) by a different generalized inverse with more favorable properties. We may\nwant, for example, to have many zero entries, but without giving up too much of\nthe stability of the MPP. One way to quantify stability is by how much the\nFrobenius norm of a generalized inverse exceeds that of the MPP. In this paper\nwe derive finite-size concentration bounds for the Frobenius norm of\n$\\ell^p$-minimal general inverses of iid Gaussian matrices, with $1 \\leq p \\leq\n2$. For $p = 1$ we prove exponential concentration of the Frobenius norm of the\nsparse pseudoinverse; for $p = 2$, we get a similar concentration bound for the\nMPP. Our proof is based on the convex Gaussian min-max theorem, but unlike\nprevious applications which give asymptotic results, we derive finite-size\nbounds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 06:38:04 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 10:59:33 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 07:19:02 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Dokmani\u0107", "Ivan", "", "PANAMA"], ["Gribonval", "R\u00e9mi", "", "PANAMA"]]}, {"id": "1810.08057", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Antonio Cuevas", "title": "Set Estimation Under Biconvexity Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set in the Euclidean plane is said to be biconvex if, for some angle\n$\\theta\\in[0,\\pi/2)$, all its sections along straight lines with inclination\nangles $\\theta$ and $\\theta+\\pi/2$ are convex sets (i.e, empty sets or\nsegments). Biconvexity is a natural notion with some useful applications in\noptimization theory. It has also be independently used, under the name of\n\"rectilinear convexity\", in computational geometry. We are concerned here with\nthe problem of asymptotically reconstructing (or estimating) a biconvex set $S$\nfrom a random sample of points drawn on $S$. By analogy with the classical\nconvex case, one would like to define the \"biconvex hull\" of the sample points\nas a natural estimator for $S$. However, as previously pointed out by several\nauthors, the notion of \"hull\" for a given set $A$ (understood as the \"minimal\"\nset including $A$ and having the required property) has no obvious, useful\ntranslation to the biconvex case. This is in sharp contrast with the well-known\nelementary definition of convex hull. Thus, we have selected the most commonly\naccepted notion of \"biconvex hull\" (often called \"rectilinear convex hull\"): we\nfirst provide additional motivations for this definition, proving some useful\nrelations with other convexity-related notions. Then, we prove some results\nconcerning the consistent approximation of a biconvex set $S$ and and the\ncorresponding biconvex hull. An analogous result is also provided for the\nboundaries. A method to approximate, from a sample of points on $S$, the\nbiconvexity angle $\\theta$ is also given.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:51:47 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:20:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1810.08068", "submitter": "Bangti Jin", "authors": "Chen Zhang, Simon Arridge, Bangti Jin", "title": "Expectation Propagation for Poisson Data", "comments": "25 pages, to be published at Inverse Problems", "journal-ref": null, "doi": "10.1088/1361-6420/ab15a3", "report-no": null, "categories": "math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson distribution arises naturally when dealing with data involving\ncounts, and it has found many applications in inverse problems and imaging. In\nthis work, we develop an approximate Bayesian inference technique based on\nexpectation propagation for approximating the posterior distribution formed\nfrom the Poisson likelihood function and a Laplace type prior distribution,\ne.g., the anisotropic total variation prior. The approach iteratively yields a\nGaussian approximation, and at each iteration, it updates the Gaussian\napproximation to one factor of the posterior distribution by moment matching.\nWe derive explicit update formulas in terms of one-dimensional integrals, and\nalso discuss stable and efficient quadrature rules for evaluating these\nintegrals. The method is showcased on two-dimensional PET images.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 14:15:47 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 14:35:36 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Chen", ""], ["Arridge", "Simon", ""], ["Jin", "Bangti", ""]]}, {"id": "1810.08219", "submitter": "Giles Hooker", "authors": "Yuefeng Wu and Giles Hooker", "title": "Asymptotic Properties for Methods Combining Minimum Hellinger Distance\n  Estimates and Bayesian Nonparametric Density Estimates", "comments": null, "journal-ref": null, "doi": "10.3390/e20120955", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In frequentist inference, minimizing the Hellinger distance between a kernel\ndensity estimate and a parametric family produces estimators that are both\nrobust to outliers and statistically efficienty when the parametric model is\ncorrect. This paper seeks to extend these results to the use of nonparametric\nBayesian density estimators within disparity methods. We propose two\nestimators: one replaces the kernel density estimator with the expected\nposterior density from a random histogram prior; the other induces a posterior\nover parameters through the posterior for the random histogram. We show that it\nis possible to adapt the mathematical machinery of efficient influence\nfunctions from semiparametric models to demonstrate that both our estimators\nare efficient in the sense of achieving the Cramer-Rao lower bound. We further\ndemonstrate a Bernstein-von-Mises result for our second estimator indicating\nthat it's posterior is asymptotically Gaussian. In addition, the robustness\nproperties of classical minimum Hellinger distance estimators continue to hold.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 18:04:20 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 15:06:03 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Wu", "Yuefeng", ""], ["Hooker", "Giles", ""]]}, {"id": "1810.08240", "submitter": "Steven Howard", "authors": "Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, Jasjeet Sekhon", "title": "Time-uniform, nonparametric, nonasymptotic confidence sequences", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A confidence sequence is a sequence of confidence intervals that is uniformly\nvalid over an unbounded time horizon. Our work develops confidence sequences\nwhose widths go to zero, with nonasymptotic coverage guarantees under\nnonparametric conditions. We draw connections between the Cram\\'er-Chernoff\nmethod for exponential concentration, the law of the iterated logarithm (LIL),\nand the sequential probability ratio test---our confidence sequences are\ntime-uniform extensions of the first; provide tight, nonasymptotic\ncharacterizations of the second; and generalize the third to nonparametric\nsettings, including sub-Gaussian and Bernstein conditions, self-normalized\nprocesses, and matrix martingales. We illustrate the generality of our proof\ntechniques by deriving an empirical-Bernstein bound growing at a LIL rate, as\nwell as a novel upper LIL for the maximum eigenvalue of a sum of random\nmatrices. Finally, we apply our methods to covariance matrix estimation and to\nestimation of sample average treatment effect under the Neyman-Rubin potential\noutcomes model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:05:00 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 01:50:08 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 23:29:35 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 22:05:53 GMT"}, {"version": "v5", "created": "Sat, 30 May 2020 04:24:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Howard", "Steven R.", ""], ["Ramdas", "Aaditya", ""], ["McAuliffe", "Jon", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1810.08278", "submitter": "Jean Feydy", "authors": "Jean Feydy, Thibault S\\'ejourn\\'e, Fran\\c{c}ois-Xavier Vialard,\n  Shun-ichi Amari, Alain Trouv\\'e, Gabriel Peyr\\'e", "title": "Interpolating between Optimal Transport and MMD using Sinkhorn\n  Divergences", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing probability distributions is a fundamental problem in data\nsciences. Simple norms and divergences such as the total variation and the\nrelative entropy only compare densities in a point-wise manner and fail to\ncapture the geometric nature of the problem. In sharp contrast, Maximum Mean\nDiscrepancies (MMD) and Optimal Transport distances (OT) are two classes of\ndistances between measures that take into account the geometry of the\nunderlying space and metrize the convergence in law.\n  This paper studies the Sinkhorn divergences, a family of geometric\ndivergences that interpolates between MMD and OT. Relying on a new notion of\ngeometric entropy, we provide theoretical guarantees for these divergences:\npositivity, convexity and metrization of the convergence in law. On the\npractical side, we detail a numerical scheme that enables the large scale\napplication of these divergences for machine learning: on the GPU, gradients of\nthe Sinkhorn loss can be computed for batches of a million samples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 21:13:45 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Feydy", "Jean", ""], ["S\u00e9journ\u00e9", "Thibault", ""], ["Vialard", "Fran\u00e7ois-Xavier", ""], ["Amari", "Shun-ichi", ""], ["Trouv\u00e9", "Alain", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1810.08292", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft and Holger Dette", "title": "A similarity measure for second order properties of non-stationary\n  functional time series with applications to clustering and testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the surge of data storage techniques, the need for the development of\nappropriate techniques to identify patterns and to extract knowledge from the\nresulting enormous data sets, which can be viewed as collections of dependent\nfunctional data, is of increasing interest in many scientific areas. We develop\na similarity measure for spectral density operators of a collection of\nfunctional time series, which is based on the aggregation of Hilbert-Schmidt\ndifferences of the individual time-varying spectral density operators. Under\nfairly general conditions, the asymptotic properties of the corresponding\nestimator are derived and asymptotic normality is established. The introduced\nstatistic lends itself naturally to quantify (dis)-similarity between\nfunctional time series, which we subsequently exploit in order to build a\nspectral clustering algorithm. Our algorithm is the first of its kind in the\nanalysis of non-stationary (functional) time series and enables to discover\nparticular patterns by grouping together `similar' series into clusters,\nthereby reducing the complexity of the analysis considerably. The algorithm is\nsimple to implement and computationally feasible. As a further application we\nprovide a simple test for the hypothesis that the second order properties of\ntwo non-stationary functional time series coincide.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:23:49 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 16:58:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["van Delft", "Anne", ""], ["Dette", "Holger", ""]]}, {"id": "1810.08316", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and T. Tony Cai and Yihong Wu", "title": "Heteroskedastic PCA: Algorithm, Optimality, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A general framework for principal component analysis (PCA) in the presence of\nheteroskedastic noise is introduced. We propose an algorithm called HeteroPCA,\nwhich involves iteratively imputing the diagonal entries of the sample\ncovariance matrix to remove estimation bias due to heteroskedasticity. This\nprocedure is computationally efficient and provably optimal under the\ngeneralized spiked covariance model. A key technical step is a deterministic\nrobust perturbation analysis on singular subspaces, which can be of independent\ninterest. The effectiveness of the proposed algorithm is demonstrated in a\nsuite of problems in high-dimensional statistics, including singular value\ndecomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for\nheteroskedastic and incomplete data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:22:25 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 01:29:04 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 14:00:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Cai", "T. Tony", ""], ["Wu", "Yihong", ""]]}, {"id": "1810.08417", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki", "title": "Characterizations of indicator functions and contrast representations of\n  fractional factorial designs with multi-level factors", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A polynomial indicator function of designs is first introduced by Fontana,\nPistone and Rogantin (2000) for two-level designs. They give the structure of\nthe indicator function of two-level designs, especially from the viewpoints of\nthe orthogonality of the designs. Based on these structure, they use the\nindicator functions to classify all the orthogonal fractional factorial designs\nwith given sizes using computational algebraic software. In this paper,\ngeneralizing the results on two-level designs, the structure of the indicator\nfunctions for multi-level designs is derived. We give a system of algebraic\nequations for the coefficients of indicator functions of fractional factorial\ndesigns with given orthogonality. We also give another representation of the\nindicator function, a contrast representation, which reflects the size and the\northogonality of the corresponding design directly. The contrast representation\nis determined by a contrast matrix, and does not depend on the level-coding,\nwhich is one of the advantages of it. We use these results to classify\northogonal $2^3\\times 3$ designs with strength $2$ and orthogonal $2^4\\times 3$\ndesigns with strength $3$ by computational algebraic software.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 09:20:24 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 08:22:36 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 06:01:30 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Aoki", "Satoshi", ""]]}, {"id": "1810.08516", "submitter": "Mahendra Saha", "authors": "Abhimanyu Singh Yadav, Mahendra Saha, Harsh Tripathi, Sumit Kumar", "title": "The exponentiated xgammma distribution: Estimation and its application", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article aims to introduced a new lifetime distribution named as\nexponentiated xgamma distribution (EXGD). The new generalization obtained from\nxgamma distribution, a special finite mixture of exponential and gamma\ndistributions. The proposed model is very flexible and positively skewed.\nDifferent statistical properties of the proposed model, viz., reliability\ncharacteristics, moments, generating function, mean deviation, quantile\nfunction, conditional moments, order statistics, reliability curves and indices\nand random variate generation etc. have been derived. The estimation of the of\nthe survival and hazard rate functions of the EXGD has been approached by\ndifferent methods estimation, viz., moment estimate (ME),maximum likelihood\nestimate (MLE), ordinary least square and weighted least square estimates (LSE\nand WLSE), Cram\\`er-von-Mises estimate (CME) and maximum product spacing\nestimate (MPSE). At last, one medical data set has been used to illustrate the\napplicability of the proposed model in real life scenario.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 09:01:08 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Yadav", "Abhimanyu Singh", ""], ["Saha", "Mahendra", ""], ["Tripathi", "Harsh", ""], ["Kumar", "Sumit", ""]]}, {"id": "1810.08693", "submitter": "Abbas Mehrabian", "authors": "Luc Devroye and Abbas Mehrabian and Tommy Reddad", "title": "The total variation distance between high-dimensional Gaussians", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a lower bound and an upper bound for the total variation distance\nbetween two high-dimensional Gaussians, which are within a constant factor of\none another.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 21:38:06 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 19:18:14 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 22:22:54 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 13:35:54 GMT"}, {"version": "v5", "created": "Fri, 22 May 2020 00:33:31 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Devroye", "Luc", ""], ["Mehrabian", "Abbas", ""], ["Reddad", "Tommy", ""]]}, {"id": "1810.08805", "submitter": "Marius Soltane", "authors": "Marius Soltane", "title": "Asymptotic efficiency in the Autoregressive process driven by a\n  stationary Gaussian noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first purpose of this article is to obtain a.s. asymptotic properties of\nthe maximum likelihood estimator in the autoregressive process driven by a\nstationary Gaussian noise. The second purpose is to show the local asymptotic\nnormality property of the likelihoods ratio in order to get a notion of\nasymptotic efficiency and to build an asymptotically uniformly invariant most\npowerful procedure for testing the significance of the autoregressive\nparameter.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 13:49:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Soltane", "Marius", ""]]}, {"id": "1810.08826", "submitter": "Qian Qin", "authors": "Qian Qin and James P. Hobert", "title": "Wasserstein-based methods for convergence complexity analysis of MCMC\n  with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last 25 years, techniques based on drift and minorization (d&m) have\nbeen mainstays in the convergence analysis of MCMC algorithms. However, results\npresented herein suggest that d&m may be less useful in the emerging area of\nconvergence complexity analysis, which is the study of how the convergence\nbehavior of Monte Carlo Markov chains scale with sample size, $n$, and/or\nnumber of covariates, $p$. The problem appears to be that minorization can\nbecome a serious liability as dimension increases. Alternative methods for\nconstructing convergence rate bounds (with respect to total variation distance)\nthat do not require minorization are investigated. Based on Wasserstein\ndistances and random mappings, these methods can produce bounds that are\nsubstantially more robust to increasing dimension than those based on d&m. The\nWasserstein-based bounds are used to develop strong convergence complexity\nresults for MCMC algorithms used in Bayesian probit regression and random\neffects models in the challenging asymptotic regime where $n$ and $p$ are both\nlarge.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 16:56:28 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 02:01:33 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 05:52:42 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""]]}, {"id": "1810.09006", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and Yuchen Zhou", "title": "On the Non-asymptotic and Sharp Lower Tail Bounds of Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The non-asymptotic tail bounds of random variables play crucial roles in\nprobability, statistics, and machine learning. Despite much success in\ndeveloping upper bounds on tail probability in literature, the lower bounds on\ntail probabilities are relatively fewer. In this paper, we introduce systematic\nand user-friendly schemes for developing non-asymptotic lower bounds of tail\nprobabilities. In addition, we develop sharp lower tail bounds for the sum of\nindependent sub-Gaussian and sub-exponential random variables, which match the\nclassic Hoeffding-type and Bernstein-type concentration inequalities,\nrespectively. We also provide non-asymptotic matching upper and lower tail\nbounds for a suite of distributions, including gamma, beta, (regular, weighted,\nand noncentral) chi-square, binomial, Poisson, Irwin-Hall, etc. We apply the\nresult to establish the matching upper and lower bounds for extreme value\nexpectation of the sum of independent sub-Gaussian and sub-exponential random\nvariables. A statistical application of signal identification from sparse\nheterogeneous mixtures is finally considered.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:47:38 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 17:40:08 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 02:12:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhang", "Anru R.", ""], ["Zhou", "Yuchen", ""]]}, {"id": "1810.09013", "submitter": "Stefan Roth", "authors": "Stefan Roth", "title": "On a linear functional for infinitely divisible moving average random\n  fields", "comments": "Published at https://doi.org/10.15559/19-VMSTA143 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2019, Vol. 6, No. 4,\n  443-478", "doi": "10.15559/19-VMSTA143", "report-no": "VTeX-VMSTA-VMSTA143", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a low-frequency sample of the infinitely divisible moving average\nrandom field $\\{\\int_{\\mathbb{R}^d}f(t-x)\\Lambda (dx), t\\in \\mathbb{R}^d\\}$, in\n[13] we proposed an estimator $\\hat{uv_0}$ for the function $\\mathbb{R}\\ni\nx\\mapsto u(x)v_0(x)=(uv_0)(x)$, with $u(x)=x$ and $v_0$ being the L\\'{e}vy\ndensity of the integrator random measure $\\Lambda$. In this paper, we study\nasymptotic properties of the linear functional $L^2(\\mathbb{R})\\ni v\\mapsto\n\\left \\langle v,\\hat{uv_0}\\right \\rangle_{L^2(\\mathbb{R})}$, if the (known)\nkernel function $f$ has a compact support. We provide conditions that ensure\nconsistency (in mean) and prove a central limit theorem for it.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 19:44:02 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 06:46:26 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Roth", "Stefan", ""]]}, {"id": "1810.09022", "submitter": "Ted Westling", "authors": "Ted Westling, Mark van der Laan, and Marco Carone", "title": "Correcting an estimator of a multivariate monotone function with\n  isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems, a sensible estimator of a possibly multivariate monotone\nfunction may itself fail to be monotone. We study the correction of such an\nestimator obtained via projection onto the space of functions monotone over a\nfinite grid in the domain. We demonstrate that this corrected estimator has no\nworse supremal estimation error than the initial estimator, and that\nanalogously corrected confidence bands contain the true function whenever the\ninitial bands do, at no loss to average or maximal band width. Additionally, we\ndemonstrate that the corrected estimator is uniformly asymptotically equivalent\nto the initial estimator provided that the initial estimator satisfies a\nstochastic equicontinuity condition and that the true function is Lipschitz and\nstrictly monotone. We provide simple sufficient conditions for our stochastic\nequicontinuity condition in the important special case that the initial\nestimator is uniformly asymptotically linear, and illustrate the use of these\nresults for estimation of a G-computed distribution function. Our stochastic\nequicontinuity condition is weaker than standard uniform stochastic\nequicontinuity, which has been required for alternative correction procedures.\nCrucially, this allows us to apply our results to the bivariate correction of\nthe local linear estimator of a conditional distribution function known to be\nmonotone in its conditioning argument. Our experiments suggest that the\nprojection step can yield significant practical improvements in performance for\nboth the estimator and confidence band.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 20:36:05 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 01:25:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Westling", "Ted", ""], ["van der Laan", "Mark", ""], ["Carone", "Marco", ""]]}, {"id": "1810.09192", "submitter": "Torben Martinussen", "authors": "Torben Martinussen, Stijn Vansteelandt and Per Kragh Andersen", "title": "Subtleties in the interpretation of hazard ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hazard ratio is one of the most commonly reported measures of treatment\neffect in randomised trials, yet the source of much misinterpretation. This\npoint was made clear by (Hernan, 2010) in commentary, which emphasised that the\nhazard ratio contrasts populations of treated and untreated individuals who\nsurvived a given period of time, populations that will typically fail to be\ncomparable - even in a randomised trial - as a result of different pressures or\nintensities acting on both populations. The commentary has been very\ninfluential, but also a source of surprise and confusion. In this note, we aim\nto provide more insight into the subtle interpretation of hazard ratios and\ndifferences, by investigating in particular what can be learned about treatment\neffect from the hazard ratio becoming 1 after a certain period of time.\nThroughout, we will focus on the analysis of randomised experiments, but our\nresults have immediate implications for the interpretation of hazard ratios in\nobservational studies.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 11:57:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Martinussen", "Torben", ""], ["Vansteelandt", "Stijn", ""], ["Andersen", "Per Kragh", ""]]}, {"id": "1810.09207", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy", "title": "Halfspace depth does not characterize probability distributions", "comments": null, "journal-ref": "Stat Papers 62, 1135-1139 (2021)", "doi": "10.1007/s00362-019-01130-x", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give examples of different multivariate probability distributions whose\nhalfspace depths coincide at all points of the sample space.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:25:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Nagy", "Stanislav", ""]]}, {"id": "1810.09229", "submitter": "Jack Noonan", "authors": "Jack Noonan, Anatoly Zhigljavsky", "title": "Approximations for the boundary crossing probabilities of moving sums of\n  normal random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study approximations for boundary crossing probabilities for\nthe moving sums of i.i.d. normal random variables. We propose approximating a\ndiscrete time problem with a continuous time problem allowing us to apply\ndeveloped theory for stationary Gaussian processes and to consider a number of\napproximations (some well known and some not). We bring particular attention to\nthe strong performance of a newly developed approximation that corrects the use\nof continuous time results in a discrete time setting. Results of extensive\nnumerical comparisons are reported. These results show that the developed\napproximation is very accurate even for small window length.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 13:01:44 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 15:49:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1810.09285", "submitter": "Andriy Olenko", "authors": "Andriy Olenko, Volodymyr Vaskovych", "title": "Non-central limit theorems for functionals of random fields on\n  hypersurfaces", "comments": "35 pages. arXiv admin note: text overlap with arXiv:1703.05900", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives non-central asymptotic results for non-linear integral\nfunctionals of homogeneous isotropic Gaussian random fields defined on\nhypersurfaces in $\\mathbb{R}^d$. We obtain the rate of convergence for these\nfunctionals. The results extend recent findings for solid figures. We apply the\nobtained results to the case of sojourn measures and demonstrate different\nlimit situations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 12:29:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Olenko", "Andriy", ""], ["Vaskovych", "Volodymyr", ""]]}, {"id": "1810.09498", "submitter": "Yi Yu", "authors": "Daren Wang, Yi Yu and Alessandro Rinaldo", "title": "Univariate Mean Change Point Detection: Penalization, CUSUM and\n  Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of univariate mean change point detection and localization based\non a sequence of $n$ independent observations with piecewise constant means has\nbeen intensively studied for more than half century, and serves as a blueprint\nfor change point problems in more complex settings. We provide a complete\ncharacterization of this classical problem in a general framework in which the\nupper bound $\\sigma^2$ on the noise variance, the minimal spacing $\\Delta$\nbetween two consecutive change points and the minimal magnitude $\\kappa$ of the\nchanges, are allowed to vary with $n$. We first show that consistent\nlocalization of the change points, when the signal-to-noise ratio $\\frac{\\kappa\n\\sqrt{\\Delta}}{\\sigma} < \\sqrt{\\log(n)}$, is impossible. In contrast, when\n$\\frac{\\kappa \\sqrt{\\Delta}}{\\sigma}$ diverges with $n$ at the rate of at least\n$\\sqrt{\\log(n)}$, we demonstrate that two computationally-efficient change\npoint estimators, one based on the solution to an $\\ell_0$-penalized least\nsquares problem and the other on the popular wild binary segmentation\nalgorithm, are both consistent and achieve a localization rate of the order\n$\\frac{\\sigma^2}{\\kappa^2} \\log(n)$. We further show that such rate is minimax\noptimal, up to a $\\log(n)$ term.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:42:05 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 08:51:16 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 15:06:27 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 08:47:03 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1810.09533", "submitter": "Bas Kleijn", "authors": "B. J. K. Kleijn, J. van Waaij", "title": "Recovery, detection and confidence sets of communities in a sparse\n  stochastic block model", "comments": "22 pp., 2 fig", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior distributions for community assignment in the planted bi-section\nmodel are shown to achieve frequentist exact recovery and detection under sharp\nlower bounds on sparsity. Assuming posterior recovery (or detection), one may\ninterpret credible sets (or enlarged credible sets) as consistent confidence\nsets. If credible levels grow to one quickly enough, credible sets can be\ninterpreted as frequentist confidence sets without conditions on the\nparameters. In the regime where within-class and between-class\nedge-probabilities are very close, credible sets may be enlarged to achieve\nfrequentist asymptotic coverage. The diameters of credible sets are controlled\nand match rates of posterior convergence.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 20:31:07 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kleijn", "B. J. K.", ""], ["van Waaij", "J.", ""]]}, {"id": "1810.09564", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "Average group effect of strongly correlated predictor variables is\n  estimable", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that individual parameters of strongly correlated predictor\nvariables in a linear model cannot be accurately estimated by the least squares\nregression due to multicollinearity generated by such variables. Surprisingly,\nan average of these parameters can be extremely accurately estimated. We find\nthis average and briefly discuss its applications in the least squares\nregression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 21:28:05 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "1810.09762", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong", "title": "Adaptation Bounds for Confidence Bands under Self-Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive bounds on the scope for a confidence band to adapt to the unknown\nregularity of a nonparametric function that is observed with noise, such as a\nregression function or density, under the self-similarity condition proposed by\nGine and Nickl (2010). We find that adaptation can only be achieved up to a\nterm that depends on the choice of the constant used to define self-similarity,\nand that this term becomes arbitrarily large for conservative choices of the\nself-similarity constant. We construct a confidence band that achieves this\nbound, up to a constant term that does not depend on the self-similarity\nconstant. Our results suggest that care must be taken in choosing and\ninterpreting the constant that defines self-similarity, since the dependence of\nadaptive confidence bands on this constant cannot be made to disappear\nasymptotically.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 10:37:48 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 22:23:33 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 21:31:48 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Armstrong", "Timothy B.", ""]]}, {"id": "1810.09880", "submitter": "Marcel Klatt", "authors": "Marcel Klatt, Carla Tameling and Axel Munk", "title": "Empirical Regularized Optimal Transport: Statistical Theory and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive limit distributions for certain empirical regularized optimal\ntransport distances between probability distributions supported on a finite\nmetric space and show consistency of the (naive) bootstrap. In particular, we\nprove that the empirical regularized transport plan itself asymptotically\nfollows a Gaussian law. The theory includes the Boltzmann-Shannon entropy\nregularization and hence a limit law for the widely applied Sinkhorn\ndivergence. Our approach is based on an application of the implicit function\ntheorem to necessary and sufficient optimality conditions for the regularized\ntransport problem. The asymptotic results are investigated in Monte Carlo\nsimulations. We further discuss computational and statistical applications,\ne.g. confidence bands for colocalization analysis of protein interaction\nnetworks based on regularized optimal transport.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 14:29:13 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 14:07:11 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 12:41:35 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Klatt", "Marcel", ""], ["Tameling", "Carla", ""], ["Munk", "Axel", ""]]}, {"id": "1810.09909", "submitter": "Sourabh Bhattacharya", "authors": "Minerva Mukhopadhyay and Sourabh Bhattacharya", "title": "Bayes Factor Asymptotics for Variable Selection in the Gaussian Process\n  Framework", "comments": "A very significantly updated version, with extensive treatment of the\n  \"large p, large n\" paradigm, even when p>>n. Substantial methodological\n  development added with TTMCMC based Bayes factor oriented variable selection,\n  along with ample simulation experiments and a real data analysis in the bona\n  fide \"large p, small n\" premise", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although variable selection is one of the most popular areas of modern\nstatistical research, much of its development has taken place in the classical\nparadigm compared to the Bayesian counterpart. Somewhat surprisingly, both the\nparadigms have focussed almost completely on linear models, in spite of the\nvast scope offered by the model liberation movement brought about by modern\nadvancements in studying real, complex phenomena.\n  In this article, we investigate general Bayesian variable selection in models\ndriven by Gaussian processes, which allows us to treat linear, non-linear and\nnonparametric models, in conjunction with even dependent setups, in the same\nvein. We consider the Bayes factor route to variable selection, and develop a\ngeneral asymptotic theory for the Gaussian process framework in the \"large p,\nlarge n\" settings even with p>>n, establishing almost sure exponential\nconvergence of the Bayes factor under appropriately mild conditions. The fixed\np setup is included as a special case.\n  To illustrate, we apply our general result to variable selection in linear\nregression, Gaussian process model with squared exponential covariance function\naccommodating the covariates, and a first order autoregressive process with\ntime-varying covariates. We also follow up our theoretical investigations with\nample simulation experiments in the above regression contexts and variable\nselection in a real, riboflavin data consisting of 71 observations but 4088\ncovariates. For implementation of variable selection using Bayes factors, we\ndevelop a novel and effective general-purpose transdimensional, transformation\nbased Markov chain Monte Carlo algorithm, which has played a crucial role in\nour simulated and real data applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:22:50 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:42:22 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 15:55:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1810.10172", "submitter": "Qiang Sun", "authors": "Xiucai Ding and Qiang Sun", "title": "Modified Multidimensional Scaling and High Dimensional Clustering", "comments": "There are critical errors in the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling is an important dimension reduction tool in\nstatistics and machine learning. Yet few theoretical results characterizing its\nstatistical performance exist, not to mention any in high dimensions. By\nconsidering a unified framework that includes low, moderate and high\ndimensions, we study multidimensional scaling in the setting of clustering\nnoisy data. Our results suggest that, the classical multidimensional scaling\ncan be modified to further improve the quality of embedded samples, especially\nwhen the noise level increases. To this end, we propose {\\it modified\nmultidimensional scaling} which applies a nonlinear transformation to the\nsample eigenvalues. The nonlinear transformation depends on the dimensionality,\nsample size and moment of noise. We show that modified multidimensional scaling\nfollowed by various clustering algorithms can achieve exact recovery, i.e., all\nthe cluster labels can be recovered correctly with probability tending to one.\nNumerical simulations and two real data applications lend strong support to our\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:51:26 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 23:53:54 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 03:20:14 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 01:03:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Ding", "Xiucai", ""], ["Sun", "Qiang", ""]]}, {"id": "1810.10214", "submitter": "Jeha Yang", "authors": "David Morales-Jimenez, Iain M. Johnstone, Matthew R. McKay, Jeha Yang", "title": "Asymptotics of eigenstructure of sample correlation matrices for\n  high-dimensional spiked models", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample correlation matrices are employed ubiquitously in statistics. However,\nquite surprisingly, little is known about their asymptotic spectral properties\nfor high-dimensional data, particularly beyond the case of \"null models\" for\nwhich the data is assumed independent. Here, considering the popular class of\nspiked models, we apply random matrix theory to derive asymptotic first-order\nand distributional results for both the leading eigenvalues and eigenvectors of\nsample correlation matrices. These results are obtained under high-dimensional\nsettings for which the number of samples n and variables p approach infinity,\nwith p/n tending to a constant. To first order, the spectral properties of\nsample correlation matrices are seen to coincide with those of sample\ncovariance matrices; however their asymptotic distributions can differ\nsignificantly, with fluctuations of both the sample eigenvalues and\neigenvectors often being remarkably smaller than those of their sample\ncovariance counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:03:32 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 05:02:15 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 05:40:37 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Morales-Jimenez", "David", ""], ["Johnstone", "Iain M.", ""], ["McKay", "Matthew R.", ""], ["Yang", "Jeha", ""]]}, {"id": "1810.10276", "submitter": "Gery Geenens", "authors": "Gery Geenens and Pierre Lafaye de Micheaux", "title": "The Hellinger Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the defining properties of a valid measure of the dependence\nbetween two random variables are reviewed and complemented with two original\nones, shown to be more fundamental than other usual postulates. While other\npopular choices are proved to violate some of these requirements, a class of\ndependence measures satisfying all of them is identified. One particular\nmeasure, that we call the Hellinger correlation, appears as a natural choice\nwithin that class due to both its theoretical and intuitive appeal. A simple\nand efficient nonparametric estimator for that quantity is proposed. Synthetic\nand real-data examples finally illustrate the descriptive ability of the\nmeasure, which can also be used as test statistic for exact independence\ntesting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 10:03:43 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 05:43:26 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 07:22:32 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 03:35:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Geenens", "Gery", ""], ["de Micheaux", "Pierre Lafaye", ""]]}, {"id": "1810.10427", "submitter": "Jeha Yang", "authors": "Iain M. Johnstone, Jeha Yang", "title": "Notes on asymptotics of sample eigenstructure for spiked covariance\n  models with non-Gaussian data", "comments": "Cross-reference added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These expository notes serve as a reference for an accompanying post\nMorales-Jimenez et al. [2018]. In the spiked covariance model, we develop\nresults on asymptotic normality of sample leading eigenvalues and certain\nprojections of the corresponding sample eigenvectors. The results parallel\nthose of Paul [2007], but are given using the non-Gaussian model of Bai and Yao\n[2008]. The results are not new, and citations are given, but proofs are\ncollected and organized as a point of departure for Morales-Jimenez et al.\n[2018].\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 14:50:06 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 04:26:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Johnstone", "Iain M.", ""], ["Yang", "Jeha", ""]]}, {"id": "1810.10495", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Posterior Convergence of Gaussian and General Stochastic Process\n  Regression Under Possible Misspecifications", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate posterior convergence in nonparametric\nregression models where the unknown regression function is modeled by some\nappropriate stochastic process. In this regard, we consider two setups. The\nfirst setup is based on Gaussian processes, where the covariates are either\nrandom or non-random and the noise may be either normally or\ndouble-exponentially distributed. In the second setup, we assume that the\nunderlying regression function is modeled by some reasonably smooth, but\nunspecified stochastic process satisfying reasonable conditions. The\ndistribution of the noise is also left unspecified, but assumed to be\nthick-tailed. As in the previous studies regarding the same problems, we do not\nassume that the truth lies in the postulated parameter space, thus explicitly\nallowing the possibilities of misspecification. We exploit the general results\nof Shalizi (2009) for our purpose and establish not only posterior consistency,\nbut also the rates at which the posterior probabilities converge, which turns\nout to be the Kullback-Leibler divergence rate. We also investigate the more\nfamiliar posterior convergence rates. Interestingly, we show that the posterior\npredictive distribution can accurately approximate the best possible predictive\ndistribution in the sense that the Hellinger distance, as well as the total\nvariation distance between the two distributions can tend to zero, in spite of\nmisspecifications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 16:57:10 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 04:42:41 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1810.10633", "submitter": "Erkan Nane", "authors": "Erkan Nane and Yimin Xiao and Aklilu Zeleke", "title": "Strong laws of large numbers for arrays of random variables and stable\n  random fields", "comments": "21 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong laws of large numbers are established for random fields with weak or\nstrong dependence. These limit theorems are applicable to random fields with\nheavy-tailed distributions including fractional stable random fields.\n  The conditions for SLLN are described in terms of the $p$-th moments of the\npartial sums of the random fields, which are convenient to verify. The main\ntechnical tool in this paper is a maximal inequality for the moments of partial\nsums of random fields that extends the technique of Levental, Chobanyan and\nSalehi \\cite{chobanyan-l-s} for a sequence of random variables indexed by a\none-parameter.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 21:36:35 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Nane", "Erkan", ""], ["Xiao", "Yimin", ""], ["Zeleke", "Aklilu", ""]]}, {"id": "1810.10883", "submitter": "Tim van Erven", "authors": "Tim van Erven and Botond Szabo", "title": "Fast Exact Bayesian Inference for Sparse Signals in the Normal Sequence\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exact algorithms for Bayesian inference with model selection\npriors (including spike-and-slab priors) in the sparse normal sequence model.\nBecause the best existing exact algorithm becomes numerically unstable for\nsample sizes over n=500, there has been much attention for alternative\napproaches like approximate algorithms (Gibbs sampling, variational Bayes,\netc.), shrinkage priors (e.g. the Horseshoe prior and the Spike-and-Slab LASSO)\nor empirical Bayesian methods. However, by introducing algorithmic ideas from\nonline sequential prediction, we show that exact calculations are feasible for\nmuch larger sample sizes: for general model selection priors we reach n=25000,\nand for certain spike-and-slab priors we can easily reach n=100000. We further\nprove a de Finetti-like result for finite sample sizes that characterizes\nexactly which model selection priors can be expressed as spike-and-slab priors.\nThe computational speed and numerical accuracy of the proposed methods are\ndemonstrated in experiments on simulated data, on a differential gene\nexpression data set, and to compare the effect of multiple hyper-parameter\nsettings in the beta-binomial prior. In our experimental evaluation we compute\nguaranteed bounds on the numerical accuracy of all new algorithms, which shows\nthat the proposed methods are numerically reliable whereas an alternative based\non long division is not.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:58:27 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 14:24:30 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["van Erven", "Tim", ""], ["Szabo", "Botond", ""]]}, {"id": "1810.10971", "submitter": "Ilya Chevyrev", "authors": "Ilya Chevyrev and Harald Oberhauser", "title": "Signature moments to characterize laws of stochastic processes", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized sequence of moments characterizes the law of any\nfinite-dimensional random variable. We prove an analogous result for\npath-valued random variables, that is stochastic processes, by using the\nnormalized sequence of signature moments. We use this to define a metric for\nlaws of stochastic processes. This metric can be efficiently estimated from\nfinite samples, even if the stochastic processes themselves evolve in\nhigh-dimensional state spaces. As an application, we provide a non-parametric\ntwo-sample hypothesis test for laws of stochastic processes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 16:48:20 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chevyrev", "Ilya", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1810.11107", "submitter": "Karine Bertin", "authors": "Karine Bertin, Salima El Kolei, Nicolas Klutchnikoff", "title": "Adaptive Density Estimation on Bounded Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation, in Lp-norm, of density functions defined on [0,1]^d.\nWe construct a new family of kernel density estimators that do not suffer from\nthe so-called boundary bias problem and we propose a data-driven procedure\nbased on the Goldenshluger and Lepski approach that jointly selects a kernel\nand a bandwidth. We derive two estimators that satisfy oracle-type\ninequalities. They are also proved to be adaptive over a scale of anisotropic\nor isotropic Sobolev-Slobodetskii classes (which are particular cases of Besov\nor Sobolev classical classes). The main interest of the isotropic procedure is\nto obtain adaptive results without any restriction on the smoothness parameter.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 21:01:37 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Bertin", "Karine", ""], ["Kolei", "Salima El", ""], ["Klutchnikoff", "Nicolas", ""]]}, {"id": "1810.11180", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen and Yun Yang", "title": "Hanson-Wright inequality in Hilbert spaces with application to $K$-means\n  clustering for non-Euclidean data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a dimension-free Hanson-Wright inequality for quadratic forms of\nindependent sub-gaussian random variables in a separable Hilbert space. Our\ninequality is an infinite-dimensional generalization of the classical\nHanson-Wright inequality for finite-dimensional Euclidean random vectors. We\nillustrate an application to the generalized $K$-means clustering problem for\nnon-Euclidean data. Specifically, we establish the exponential rate of\nconvergence for a semidefinite relaxation of the generalized $K$-means, which\ntogether with a simple rounding algorithm imply the exact recovery of the true\nclustering structure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 03:55:54 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 03:28:46 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 01:17:58 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Chen", "Xiaohui", ""], ["Yang", "Yun", ""]]}, {"id": "1810.11204", "submitter": "Anne Philippe", "authors": "Remigijus Leipus, Anne Philippe (LMJL), Vytaute Pilipauskaite, Donatas\n  Surgailis", "title": "Sample covariances of random-coefficient AR(1) panel model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper obtains a complete description of the limit distributions\nof sample covariances in N x n panel data when N and n jointly increase,\npossibly at different rate. The panel is formed by N independent samples of\nlength n from random-coefficient AR(1) process with the tail distribution\nfunction of the random coefficient regularly varying at the unit root with\nexponent $\\beta$ > 0. We show that for $\\beta$ $\\in$ (0, 2) the sample\ncovariances may display a variety of stable and non-stable limit behaviors with\nstability parameter depending on $\\beta$ and the mutual increase rate of N and\nn.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 06:53:57 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 13:56:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Leipus", "Remigijus", "", "LMJL"], ["Philippe", "Anne", "", "LMJL"], ["Pilipauskaite", "Vytaute", ""], ["Surgailis", "Donatas", ""]]}, {"id": "1810.11223", "submitter": "Yi Yu", "authors": "Mark Fiecas, Chenlei Leng, Weidong Liu, and Yi Yu", "title": "Spectral Analysis of High-dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful approach for analysing multiple time series is via characterising\ntheir spectral density matrix as the frequency domain analog of the covariance\nmatrix. When the dimension of the time series is large compared to their\nlength, regularisation based methods can overcome the curse of dimensionality,\nbut the existing ones lack theoretical justification. This paper develops the\nfirst non-asymptotic result for characterising the difference between the\nsample and population versions of the spectral density matrix, allowing one to\njustify a range of high-dimensional models for analysing time series. As a\nconcrete example, we apply this result to establish the convergence of the\nsmoothed periodogram estimators and sparse estimators of the inverse of\nspectral density matrices, namely precision matrices. These results, novel in\nthe frequency domain time series analysis, are corroborated by simulations and\nan analysis of the Google Flu Trends data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 08:23:22 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Fiecas", "Mark", ""], ["Leng", "Chenlei", ""], ["Liu", "Weidong", ""], ["Yu", "Yi", ""]]}, {"id": "1810.11397", "submitter": "Xinwei Ma", "authors": "Xinwei Ma and Jingshen Wang", "title": "Robust Inference Using Inverse Probability Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Probability Weighting (IPW) is widely used in empirical work in\neconomics and other disciplines. As Gaussian approximations perform poorly in\nthe presence of \"small denominators,\" trimming is routinely employed as a\nregularization strategy. However, ad hoc trimming of the observations renders\nusual inference procedures invalid for the target estimand, even in large\nsamples. In this paper, we first show that the IPW estimator can have different\n(Gaussian or non-Gaussian) asymptotic distributions, depending on how \"close to\nzero\" the probability weights are and on how large the trimming threshold is.\nAs a remedy, we propose an inference procedure that is robust not only to small\nprobability weights entering the IPW estimator but also to a wide range of\ntrimming threshold choices, by adapting to these different asymptotic\ndistributions. This robustness is achieved by employing resampling techniques\nand by correcting a non-negligible trimming bias. We also propose an\neasy-to-implement method for choosing the trimming threshold by minimizing an\nempirical analogue of the asymptotic mean squared error. In addition, we show\nthat our inference procedure remains valid with the use of a data-driven\ntrimming threshold. We illustrate our method by revisiting a dataset from the\nNational Supported Work program.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 15:47:29 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 19:13:22 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ma", "Xinwei", ""], ["Wang", "Jingshen", ""]]}, {"id": "1810.11480", "submitter": "Muhyiddin Izadi", "authors": "Muhyiddin Izadi, Sirous Fathimanesh", "title": "Testing Exponentiality Against a Trend Change in Mean Time to Failure in\n  Age Replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean time to failure in age replacement evaluates the performance and\neffectiveness of the age replacement policy. In this paper, we propose a test\nfor exponentiality against a trend change in mean time to failure in age\nreplacement. We derive the asymptotic distribution of the test statistics under\nthe null hypothesis to approximate the critical values. We conduct a simulation\nstudy to investigate the performance of the proposed test and compare it with\nsome well known tests in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:19:20 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Izadi", "Muhyiddin", ""], ["Fathimanesh", "Sirous", ""]]}, {"id": "1810.11526", "submitter": "Dennis Leung", "authors": "Dennis Leung, Mathias Drton", "title": "Algebraic tests of general Gaussian latent tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider general Gaussian latent tree models in which the observed\nvariables are not restricted to be leaves of the tree. Extending related recent\nwork, we give a full semi-algebraic description of the set of covariance\nmatrices of any such model. In other words, we find polynomial constraints that\ncharacterize when a matrix is the covariance matrix of a distribution in a\ngiven latent tree model. However, leveraging these constraints to test a given\nsuch model is often complicated by the number of constraints being large and by\nsingularities of individual polynomials, which may invalidate standard\napproximations to relevant probability distributions. Illustrating with the\nstar tree, we propose a new testing methodology that circumvents singularity\nissues by trading off some statistical estimation efficiency and handles cases\nwith many constraints through recent advances on Gaussian approximation for\nmaxima of sums of high-dimensional random vectors. Our test avoids the need to\nmaximize the possibly multimodal likelihood function of such models and is\napplicable to models with larger number of variables. These points are\nillustrated in numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 20:52:26 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Leung", "Dennis", ""], ["Drton", "Mathias", ""]]}, {"id": "1810.11571", "submitter": "Puning Zhao", "authors": "Puning Zhao and Lifeng Lai", "title": "Analysis of KNN Information Estimators for Smooth Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KSG mutual information estimator, which is based on the distances of each\nsample to its k-th nearest neighbor, is widely used to estimate mutual\ninformation between two continuous random variables. Existing work has analyzed\nthe convergence rate of this estimator for random variables whose densities are\nbounded away from zero in its support. In practice, however, KSG estimator also\nperforms well for a much broader class of distributions, including not only\nthose with bounded support and densities bounded away from zero, but also those\nwith bounded support but densities approaching zero, and those with unbounded\nsupport. In this paper, we analyze the convergence rate of the error of KSG\nestimator for smooth distributions, whose support of density can be both\nbounded and unbounded. As KSG mutual information estimator can be viewed as an\nadaptive recombination of KL entropy estimators, in our analysis, we also\nprovide convergence analysis of KL entropy estimator for a broad class of\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 00:56:28 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 06:55:13 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 22:28:40 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Zhao", "Puning", ""], ["Lai", "Lifeng", ""]]}, {"id": "1810.11589", "submitter": "Ziv Goldfeld", "authors": "Ziv Goldfeld, Kristjan Greenewald and Yury Polyanskiy", "title": "Estimating Differential Entropy under Gaussian Convolutions", "comments": "A significantly updated version with a different set of authors\n  replaces this manuscript. New version available at arXiv:1905.13576", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating the differential entropy\n$h(S+Z)$, where $S$ and $Z$ are independent $d$-dimensional random variables\nwith $Z\\sim\\mathcal{N}(0,\\sigma^2 \\mathrm{I}_d)$. The distribution of $S$ is\nunknown, but $n$ independently and identically distributed (i.i.d) samples from\nit are available. The question is whether having access to samples of $S$ as\nopposed to samples of $S+Z$ can improve estimation performance. We show that\nthe answer is positive. More concretely, we first show that despite the\nregularizing effect of noise, the number of required samples still needs to\nscale exponentially in $d$. This result is proven via a random-coding argument\nthat reduces the question to estimating the Shannon entropy on a\n$2^{O(d)}$-sized alphabet. Next, for a fixed $d$ and $n$ large enough, it is\nshown that a simple plugin estimator, given by the differential entropy of the\nempirical distribution from $S$ convolved with the Gaussian density, achieves\nthe loss of $O\\left((\\log n)^{d/4}/\\sqrt{n}\\right)$. Note that the plugin\nestimator amounts here to the differential entropy of a $d$-dimensional\nGaussian mixture, for which we propose an efficient Monte Carlo computation\nalgorithm. At the same time, estimating $h(S+Z)$ via popular differential\nentropy estimators (based on kernel density estimation (KDE) or k nearest\nneighbors (kNN) techniques) applied to samples from $S+Z$ would only attain\nmuch slower rates of order $O(n^{-1/d})$, despite the smoothness of $P_{S+Z}$.\nAs an application, which was in fact our original motivation for the problem,\nwe estimate information flows in deep neural networks and discuss Tishby's\nInformation Bottleneck and the compression conjecture, among others.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 03:19:32 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 14:27:25 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 00:40:53 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Goldfeld", "Ziv", ""], ["Greenewald", "Kristjan", ""], ["Polyanskiy", "Yury", ""]]}, {"id": "1810.11591", "submitter": "Leonardo Moreno L. Moreno", "authors": "R. Fraiman and F. Gamboa and L. Moreno", "title": "Sensitivity indices for output on a Riemannian manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of computer code experiments, sensitivity analysis of a\ncomplicated input-output system is often performed by ranking the so-called\nSobol indices. One reason of the popularity of Sobol's approach relies on the\nsimplicity of the statistical estimation of these indices using the so-called\nPick and Freeze method. In this work we propose and study sensitivity indices\nfor the case where the output lies on a Riemannian manifold. These indices are\nbased on a Cram\\'er von Mises like criterion that takes into account the\ngeometry of the output support. We propose a Pick-Freeze like estimator of\nthese indices based on an $U$--statistic. The asymptotic properties of these\nestimators are studied. Further, we provide and discuss some interesting\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 03:42:06 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Fraiman", "R.", ""], ["Gamboa", "F.", ""], ["Moreno", "L.", ""]]}, {"id": "1810.11711", "submitter": "Chandler Zuo", "authors": "Chandler Zuo", "title": "Regularization Effect of Fast Gradient Sign Method and its\n  Generalization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Gradient Sign Method (FGSM) is a popular method to generate adversarial\nexamples that make neural network models robust against perturbations. Despite\nits empirical success, its theoretical property is not well understood. This\npaper develops theory to explain the regularization effect of Generalized FGSM,\na class of methods to generate adversarial examples. Motivated from the\nrelationship between FGSM and LASSO penalty, the asymptotic properties of\nGeneralized FGSM are derived in the Generalized Linear Model setting, which is\nessentially the 1-layer neural network setting with certain activation\nfunctions. In such simple neural network models, I prove that Generalized FGSM\nestimation is root n-consistent and weakly oracle under proper conditions. The\nasymptotic results are also highly similar to penalized likelihood estimation.\nNevertheless, Generalized FGSM introduces additional bias when data sampling is\nnot sign neutral, a concept I introduce to describe the balance-ness of the\nnoise signs. Although the theory in this paper is developed under simple neural\nnetwork settings, I argue that it may give insights and justification for FGSM\nin deep neural network settings as well.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 21:22:06 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 04:44:50 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zuo", "Chandler", ""]]}, {"id": "1810.11859", "submitter": "Badr-Eddine Ch\\'erief-Abdellatif", "authors": "Badr-Eddine Ch\\'erief-Abdellatif", "title": "Consistency of ELBO maximization for model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Evidence Lower Bound (ELBO) is a quantity that plays a key role in\nvariational inference. It can also be used as a criterion in model selection.\nHowever, though extremely popular in practice in the variational Bayes\ncommunity, there has never been a general theoretic justification for selecting\nbased on the ELBO. In this paper, we show that the ELBO maximization strategy\nhas strong theoretical guarantees, and is robust to model misspecification\nwhile most works rely on the assumption that one model is correctly specified.\nWe illustrate our theoretical results by an application to the selection of the\nnumber of principal components in probabilistic PCA.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 18:47:16 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 15:41:01 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""]]}, {"id": "1810.11905", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Sujay Sanghavi, Alexandros G. Dimakis", "title": "Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the effectiveness of a classical algorithm for recovering the\nMarkov graph of a general discrete pairwise graphical model from i.i.d.\nsamples. The algorithm is (appropriately regularized) maximum conditional\nlog-likelihood, which involves solving a convex program for each node; for\nIsing models this is $\\ell_1$-constrained logistic regression, while for more\ngeneral alphabets an $\\ell_{2,1}$ group-norm constraint needs to be used. We\nshow that this algorithm can recover any arbitrary discrete pairwise graphical\nmodel, and also characterize its sample complexity as a function of model\nwidth, alphabet size, edge parameter accuracy, and the number of variables. We\nshow that along every one of these axes, it matches or improves on all existing\nresults and algorithms for this problem. Our analysis applies a sharp\ngeneralization error bound for logistic regression when the weight vector has\nan $\\ell_1$ constraint (or $\\ell_{2,1}$ constraint) and the sample vector has\nan $\\ell_{\\infty}$ constraint (or $\\ell_{2, \\infty}$ constraint). We also show\nthat the proposed convex programs can be efficiently solved in $\\tilde{O}(n^2)$\nrunning time (where $n$ is the number of variables) under the same statistical\nguarantees. We provide experimental results to support our analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 23:40:42 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 03:40:47 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 20:53:05 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wu", "Shanshan", ""], ["Sanghavi", "Sujay", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1810.11917", "submitter": "Gerard Letac G.", "authors": "Mauro Piccioni, Bartosz Ko{\\l}odziejek, G\\'erard Letac", "title": "Location and scale behaviour of the quantiles of a natural exponential\n  family", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $P_0$ be a probability on the real line generating a natural exponential\nfamily $(P_t)_{t\\in \\mathbb {R}}$. Fix $\\alpha$ in $ (0,1).$ We show that the\nproperty that $P_t((-\\infty,t)) \\leq \\alpha \\leq P_t((-\\infty,t])$ for all $t$\nimplies that there exists a number $\\mu_\\alpha$ such that $P_0$ is the Gaussian\ndistribution $N(\\mu_{\\alpha},1).$ In other terms, if for all $t$, $t$ is a\nquantile of $P_t$ associated to some threshold $\\alpha\\in (0,1)$, then the\nexponential family must be Gaussian. The case $\\alpha=1/2$, \\textit{i.e.} $t$\nis always a median of $P_t,$ has been considered in Letac \\textit{et al.}\n(2018). Analogously let $Q$ be a measure on $[0,\\infty)$ generating a natural\nexponential family $(Q_{-t})_{t>0}$. We show that $Q_{-t}([0,t^{-1}))\\leq\n\\alpha \\leq Q_{-t}([0,t^{-1}])$ for all $t>0$ implies that there exists a\nnumber $p=p_{\\alpha}>0$ such that $Q(dx)\\propto x^{p-1}dx,$ and thus $Q_{-t}$\nhas to be a gamma distribution with parameters $p$ and $t.$\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 01:25:14 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Piccioni", "Mauro", ""], ["Ko\u0142odziejek", "Bartosz", ""], ["Letac", "G\u00e9rard", ""]]}, {"id": "1810.12132", "submitter": "Harsha Honnappa", "authors": "Harsha Honnappa and Raghu Pasupathy and Prateek Jaiswal", "title": "Dominating Points of Gaussian Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the large deviations of Gaussian extreme value statistics on\nclosed convex sets in d-dimensional Euclidean space. The asymptotics imply that\nthe extreme value distribution exhibits a rate function that is a simple\nquadratic function of a unique \"dominating point\" located on the boundary of\nthe convex set. Furthermore, the dominating point is identified as the\noptimizer of a certain convex quadratic programming problem, indicating a\n\"collusion\" between the dependence structure of the Gaussian random vectors and\nthe geometry of the convex set in determining the asymptotics. We specialize\nour main result to polyhedral sets which appear frequently in other contexts\ninvolving logarithmic asymptotics. We also extend the main result to\ncharacterize the large deviations of Gaussian-mixture extreme value statistics\non general convex sets. Our results have implications to contexts arising in\nrare-event probability estimation and stochastic optimization, since the nature\nof the dominating point and the rate function suggest importance sampling\nmeasures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:58:13 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Honnappa", "Harsha", ""], ["Pasupathy", "Raghu", ""], ["Jaiswal", "Prateek", ""]]}, {"id": "1810.12169", "submitter": "Marie Szafranski", "authors": "Florent Guinot (LaMME), Marie Szafranski (LaMME), Julien Chiquet\n  (MIA-Paris), Anouk Zancarini, Christine Le Signor, Christophe Mougel (IGEPP),\n  Christophe Ambroise (LaMME)", "title": "Fast Computation of Genome-Metagenome Interaction Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Association studies have been widely used to search for\nassociations between common genetic variants observations and a given\nphenotype. However, it is now generally accepted that genes and environment\nmust be examined jointly when estimating phenotypic variance. In this work we\nconsider two types of biological markers: genotypic markers, which characterize\nan observation in terms of inherited genetic information, and metagenomic\nmarker which are related to the environment. Both types of markers are\navailable in their millions and can be used to characterize any observation\nuniquely. Objective. Our focus is on detecting interactions between groups of\ngenetic and metagenomic markers in order to gain a better understanding of the\ncomplex relationship between environment and genome in the expression of a\ngiven phenotype. Contributions. We propose a novel approach for efficiently\ndetecting interactions between complementary datasets in a high-dimensional\nsetting with a reduced computational cost. The method, named SICOMORE, reduces\nthe dimension of the search space by selecting a subset of supervariables in\nthe two complementary datasets. These supervariables are given by a weighted\ngroup structure defined on sets of variables at different scales. A Lasso\nselection is then applied on each type of supervariable to obtain a subset of\npotential interactions that will be explored via linear model testing. Results.\nWe compare SICOMORE with other approaches in simulations, with varying sample\nsizes, noise, and numbers of true interactions. SICOMORE exhibits convincing\nresults in terms of recall, as well as competitive performances with respect to\nrunning time. The method is also used to detect interaction between genomic\nmarkers in Medicago truncatula and metagenomic markers in its rhizosphere\nbacterial community. Software availability. A R package is available, along\nwith its documentation and associated scripts, allowing the reader to reproduce\nthe results presented in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:57:02 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:41:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 16:19:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Guinot", "Florent", "", "LaMME"], ["Szafranski", "Marie", "", "LaMME"], ["Chiquet", "Julien", "", "MIA-Paris"], ["Zancarini", "Anouk", "", "IGEPP"], ["Signor", "Christine Le", "", "IGEPP"], ["Mougel", "Christophe", "", "IGEPP"], ["Ambroise", "Christophe", "", "LaMME"]]}, {"id": "1810.12518", "submitter": "Ilias Zadik", "authors": "Christian Borgs, Jennifer Chayes, Adam Smith, Ilias Zadik", "title": "Private Algorithms Can Always Be Extended", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following fundamental question on $\\epsilon$-differential\nprivacy. Consider an arbitrary $\\epsilon$-differentially private algorithm\ndefined on a subset of the input space. Is it possible to extend it to an\n$\\epsilon'$-differentially private algorithm on the whole input space for some\n$\\epsilon'$ comparable with $\\epsilon$? In this note we answer affirmatively\nthis question for $\\epsilon'=2\\epsilon$. Our result applies to every input\nmetric space and space of possible outputs. This result originally appeared in\na recent paper by the authors [BCSZ18]. We present a self-contained version in\nthis note, in the hopes that it will be broadly useful.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:12:13 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 15:37:29 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Smith", "Adam", ""], ["Zadik", "Ilias", ""]]}, {"id": "1810.12609", "submitter": "Jiang Hu Dr.", "authors": "Zhidong Bai, Yasunori Fujikoshi and Jiang Hu", "title": "Strong consistency of the AIC, BIC, $C_p$ and KOO methods in\n  high-dimensional multivariate linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is essential for improving inference and interpretation in\nmultivariate linear regression. Although a number of alternative regressor\nselection criteria have been suggested, the most prominent and widely used are\nthe Akaike information criterion (AIC), Bayesian information criterion (BIC),\nMallow's $C_p$, and their modifications. However, for high-dimensional data,\nexperience has shown that the performance of these classical criteria is not\nalways satisfactory. In the present article, we begin by presenting the\nnecessary and sufficient conditions (NSC) for the strong consistency of the\nhigh-dimensional AIC, BIC, and $C_p$, based on which we can identify some\nreasons for their poor performance. Specifically, we show that under certain\nmild high-dimensional conditions, if the BIC is strongly consistent, then the\nAIC is strongly consistent, but not vice versa. This result contradicts the\nclassical understanding. In addition, we consider some NSC for the strong\nconsistency of the high-dimensional kick-one-out (KOO) methods introduced by\nZhao et al. (1986) and Nishii et al. (1988). Furthermore, we propose two\ngeneral methods based on the KOO methods and prove their strong consistency.\nThe proposed general methods remove the penalties while simultaneously reducing\nthe conditions for the dimensions and sizes of the regressors. A simulation\nstudy supports our consistency conclusions and shows that the convergence rates\nof the two proposed general KOO methods are much faster than those of the\noriginal methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 09:46:18 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 14:18:05 GMT"}, {"version": "v3", "created": "Sun, 5 Jan 2020 23:50:51 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Bai", "Zhidong", ""], ["Fujikoshi", "Yasunori", ""], ["Hu", "Jiang", ""]]}, {"id": "1810.12760", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Prudhvi Gurram, and Gene Whipps", "title": "Quickest Detection Of Deviations From Periodic Statistical Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT eess.SP math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of stochastic processes called independent and periodically\nidentically distributed (i.p.i.d.) processes is defined to capture periodically\nvarying statistical behavior. Algorithms are proposed to detect changes in such\ni.p.i.d. processes. It is shown that the algorithms can be computed recursively\nand are asymptotically optimal. This problem has applications in anomaly\ndetection in traffic data, social network data, and neural data, where periodic\nstatistical behavior has been observed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 14:14:03 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Banerjee", "Taposh", ""], ["Gurram", "Prudhvi", ""], ["Whipps", "Gene", ""]]}, {"id": "1810.12794", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Divergence Network: Graphical calculation method of divergence functions", "comments": "17 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce directed networks called `divergence network' in\norder to perform graphical calculation of divergence functions. By using the\ndivergence networks, we can easily understand the geometric meaning of\ncalculation results and grasp relations among divergence functions intuitively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:02:55 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 13:18:18 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1810.12862", "submitter": "David Hong", "authors": "David Hong, Jeffrey A. Fessler and Laura Balzano", "title": "Optimally Weighted PCA for High-Dimensional Heteroscedastic Data", "comments": "52 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications increasingly involve high-dimensional and heterogeneous\ndata, e.g., datasets formed by combining numerous measurements from myriad\nsources. Principal Component Analysis (PCA) is a classical method for reducing\ndimensionality by projecting such data onto a low-dimensional subspace\ncapturing most of their variation, but PCA does not robustly recover underlying\nsubspaces in the presence of heteroscedastic noise. Specifically, PCA suffers\nfrom treating all data samples as if they are equally informative. This paper\nanalyzes a weighted variant of PCA that accounts for heteroscedasticity by\ngiving samples with larger noise variance less influence. The analysis provides\nexpressions for the asymptotic recovery of underlying low-dimensional\ncomponents from samples with heteroscedastic noise in the high-dimensional\nregime, i.e., for sample dimension on the order of the number of samples.\nSurprisingly, it turns out that whitening the noise by using inverse noise\nvariance weights is suboptimal. We derive optimal weights, characterize the\nperformance of weighted PCA, and consider the problem of optimally collecting\nsamples under budget constraints.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:06:25 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 22:51:21 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Hong", "David", ""], ["Fessler", "Jeffrey A.", ""], ["Balzano", "Laura", ""]]}, {"id": "1810.13246", "submitter": "Lei Yu", "authors": "Lei Yu and Vincent Y. F. Tan", "title": "Exact Channel Synthesis", "comments": "This is the final version. To appear in IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CC math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the exact channel synthesis problem. This problem concerns the\ndetermination of the minimum amount of information required to create exact\ncorrelation remotely when there is a certain rate of randomness shared by two\nterminals. This problem generalizes an existing approximate version, in which\nthe generated joint distribution is required to be close to a target\ndistribution under the total variation (TV) distance measure (instead being\nexactly equal to the target distribution). We provide single-letter inner and\nouter bounds on the admissible region of the shared randomness rate and the\ncommunication rate for the exact channel synthesis problem. These two bounds\ncoincide for doubly symmetric binary sources. We observe that for such sources,\nthe admissible rate region for exact channel synthesis is strictly included in\nthat for the TV-approximate version. We also extend the exact and\nTV-approximate channel synthesis problems to sources with countably infinite\nalphabets and continuous sources; the latter includes Gaussian sources. As\nby-products, lemmas concerning soft-covering under R\\'enyi divergence measures\nare derived.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:23:41 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 09:22:40 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Yu", "Lei", ""], ["Tan", "Vincent Y. F.", ""]]}]