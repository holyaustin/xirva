[{"id": "1906.00030", "submitter": "Ting-Kam Leonard Wong", "authors": "Ting-Kam Leonard Wong, Jiaowen Yang", "title": "Pseudo-Riemannian geometry embeds information geometry in optimal\n  transport", "comments": "28 pages, 2 figures. Substantially revised. [Originally titled\n  \"Optimal transport and information geometry.]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport and information geometry both study geometric structures on\nspaces of probability distributions. Optimal transport characterizes the\ncost-minimizing movement from one distribution to another, while information\ngeometry originates from coordinate-invariant properties of statistical\ninference. Their connections and applications in statistics and machine\nlearning have started to gain more attention. In this paper we give a new\ndifferential geometric connection between the two fields. Namely, the\npseudo-Riemannian framework of Kim and McCann, a geometric perspective on the\nfundamental Ma-Trudinger-Wang (MTW) condition in the regularity theory of\noptimal transport maps, encodes the dualistic structure of statistical\nmanifold. This general relation is described using the natural framework of\n$c$-divergence, a divergence defined by an optimal transport map. As a\nby-product, we obtain a new information-geometric interpretation of the MTW\ntensor. This connection sheds light on old and new aspects of information\ngeometry. The dually flat geometry of Bregman divergence corresponds to the\nquadratic cost and the pseudo-Euclidean space, and the\n$L^{(\\alpha)}$-divergence introduced by Pal and the first author has constant\nsectional curvature in a sense to be made precise. In these cases we give a\ngeometric interpretation of the information-geometric curvature in terms of the\ndivergence between a primal-dual pair of geodesics.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 18:51:50 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 22:56:17 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 23:02:42 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 02:16:46 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wong", "Ting-Kam Leonard", ""], ["Yang", "Jiaowen", ""]]}, {"id": "1906.00193", "submitter": "Roberto Imbuzeiro Oliveira", "authors": "Dyego Ara\\'ujo, Roberto I. Oliveira, Daniel Yukimura", "title": "A mean-field limit for certain deep neural networks", "comments": "79 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding deep neural networks (DNNs) is a key challenge in the theory of\nmachine learning, with potential applications to the many fields where DNNs\nhave been successfully used. This article presents a scaling limit for a DNN\nbeing trained by stochastic gradient descent. Our networks have a fixed (but\narbitrary) number $L\\geq 2$ of inner layers; $N\\gg 1$ neurons per layer; full\nconnections between layers; and fixed weights (or \"random features\" that are\nnot trained) near the input and output. Our results describe the evolution of\nthe DNN during training in the limit when $N\\to +\\infty$, which we relate to a\nmean field model of McKean-Vlasov type. Specifically, we show that network\nweights are approximated by certain \"ideal particles\" whose distribution and\ndependencies are described by the mean-field model. A key part of the proof is\nto show existence and uniqueness for our McKean-Vlasov problem, which does not\nseem to be amenable to existing theory. Our paper extends previous work on the\n$L=1$ case by Mei, Montanari and Nguyen; Rotskoff and Vanden-Eijnden; and\nSirignano and Spiliopoulos. We also complement recent independent work on $L>1$\nby Sirignano and Spiliopoulos (who consider a less natural scaling limit) and\nNguyen (who nonrigorously derives similar results).\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 09:51:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ara\u00fajo", "Dyego", ""], ["Oliveira", "Roberto I.", ""], ["Yukimura", "Daniel", ""]]}, {"id": "1906.00211", "submitter": "Boris Landa", "authors": "Boris Landa and Yoel Shkolnisky", "title": "Multi-reference factor analysis: low-rank covariance estimation under\n  unknown translations", "comments": null, "journal-ref": null, "doi": "10.1093/imaiai/iaaa019", "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the covariance matrix of a random\nsignal observed through unknown translations (modeled by cyclic shifts) and\ncorrupted by noise. Solving this problem allows to discover low-rank structures\nmasked by the existence of translations (which act as nuisance parameters),\nwith direct application to Principal Components Analysis (PCA). We assume that\nthe underlying signal is of length $L$ and follows a standard factor model with\nmean zero and $r$ normally-distributed factors. To recover the covariance\nmatrix in this case, we propose to employ the second- and fourth-order\nshift-invariant moments of the signal known as the $\\textit{power spectrum}$\nand the $\\textit{trispectrum}$. We prove that they are sufficient for\nrecovering the covariance matrix (under a certain technical condition) when\n$r<\\sqrt{L}$. Correspondingly, we provide a polynomial-time procedure for\nestimating the covariance matrix from many (translated and noisy) observations,\nwhere no explicit knowledge of $r$ is required, and prove the procedure's\nstatistical consistency. While our results establish that covariance estimation\nis possible from the power spectrum and the trispectrum for low-rank covariance\nmatrices, we prove that this is not the case for full-rank covariance matrices.\nWe conduct numerical experiments that corroborate our theoretical findings, and\ndemonstrate the favorable performance of our algorithms in various settings,\nincluding in high levels of noise.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 12:12:50 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:10:41 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1906.00232", "submitter": "Rahul Singh", "authors": "Rahul Singh, Maneesh Sahani, Arthur Gretton", "title": "Kernel Instrumental Variable Regression", "comments": "41 pages, 11 figures. Advances in Neural Information Processing\n  Systems. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) regression is a strategy for learning causal\nrelationships in observational data. If measurements of input X and output Y\nare confounded, the causal relationship can nonetheless be identified if an\ninstrumental variable Z is available that influences X directly, but is\nconditionally independent of Y given X and the unmeasured confounder. The\nclassic two-stage least squares algorithm (2SLS) simplifies the estimation\nproblem by modeling all relationships as linear functions. We propose kernel\ninstrumental variable regression (KIV), a nonparametric generalization of 2SLS,\nmodeling relations among X, Y, and Z as nonlinear functions in reproducing\nkernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild\nassumptions, and derive conditions under which convergence occurs at the\nminimax optimal rate for unconfounded, single-stage RKHS regression. In doing\nso, we obtain an efficient ratio between training sample sizes used in the\nalgorithm's first and second stages. In experiments, KIV outperforms state of\nthe art alternatives for nonparametric IV regression.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 14:30:03 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 15:51:06 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 13:58:17 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 00:09:43 GMT"}, {"version": "v5", "created": "Wed, 4 Dec 2019 16:59:02 GMT"}, {"version": "v6", "created": "Wed, 15 Jul 2020 18:53:19 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Singh", "Rahul", ""], ["Sahani", "Maneesh", ""], ["Gretton", "Arthur", ""]]}, {"id": "1906.00255", "submitter": "Nathan Kallus", "authors": "Vishal Gupta and Nathan Kallus", "title": "Data-Pooling in Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing large-scale systems often involves simultaneously solving thousands\nof unrelated stochastic optimization problems, each with limited data.\nIntuition suggests one can decouple these unrelated problems and solve them\nseparately without loss of generality. We propose a novel data-pooling\nalgorithm called Shrunken-SAA that disproves this intuition. In particular, we\nprove that combining data across problems can outperform decoupling, even when\nthere is no a priori structure linking the problems and data are drawn\nindependently. Our approach does not require strong distributional assumptions\nand applies to constrained, possibly non-convex, non-smooth optimization\nproblems such as vehicle-routing, economic lot-sizing or facility location. We\ncompare and contrast our results to a similar phenomenon in statistics (Stein's\nPhenomenon), highlighting unique features that arise in the optimization\nsetting that are not present in estimation. We further prove that as the number\nof problems grows large, Shrunken-SAA learns if pooling can improve upon\ndecoupling and the optimal amount to pool, even if the average amount of data\nper problem is fixed and bounded. Importantly, we highlight a simple intuition\nbased on stability that highlights when and why data-pooling offers a benefit,\nelucidating this perhaps surprising phenomenon. This intuition further suggests\nthat data-pooling offers the most benefits when there are many problems, each\nof which has a small amount of relevant data. Finally, we demonstrate the\npractical benefits of data-pooling using real data from a chain of retail drug\nstores in the context of inventory management.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 16:39:25 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 20:58:23 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 06:24:32 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Gupta", "Vishal", ""], ["Kallus", "Nathan", ""]]}, {"id": "1906.00455", "submitter": "Harrison Quick", "authors": "Harrison Quick", "title": "Generating Poisson-Distributed Differentially Private Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dissemination of synthetic data can be an effective means of making\ninformation from sensitive data publicly available while reducing the risk of\ndisclosure associated with releasing the sensitive data directly. While\nmechanisms exist for synthesizing data that satisfy formal privacy guarantees,\nthe utility of the synthetic data is often an afterthought. More recently, the\nuse of methods from the disease mapping literature has been proposed to\ngenerate spatially-referenced synthetic data with high utility, albeit without\nformal privacy guarantees. The objective for this paper is to help bridge the\ngap between the disease mapping and the formal privacy literatures. In\nparticular, we extend an existing approach for generating formally private\nsynthetic data to the case of Poisson-distributed count data in a way that\nallows for the infusion of prior information. To evaluate the utility of the\nsynthetic data, we conducted a simulation study inspired by publicly available,\ncounty-level heart disease-related death counts. The results of this study\ndemonstrate that the proposed approach for generating differentially private\nsynthetic data outperforms a popular technique when the counts correspond to\nevents arising from subgroups with unequal population sizes or unequal event\nrates.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 17:26:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Quick", "Harrison", ""]]}, {"id": "1906.00456", "submitter": "Rajarshi Mukherjee", "authors": "Rajarshi Mukherjee, Gourab Ray", "title": "On Testing for Parameters in Ising Models", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing for the parameters of Ferromagnetic Ising models. While\ntesting for the presence of possibly sparse magnetizations, we provide a\ngeneral lower bound of minimax separation rates which yields sharp results in\nhigh temperature regimes. Our matching upper bounds are adaptive over both\nunderlying dependence graph and temperature parameter. Moreover our results\ninclude the nearest neighbor model on lattices, the sparse Erd\\\"{o}s-R\\'{e}nyi\nrandom graphs, and regular rooted trees -- right up to the critical parameter\nin the high temperature regime. We also provide parallel results for the entire\nlow temperature regime in nearest neighbor model on lattices -- however in the\nplus boundary pure phase. Our results for the nearest neighbor model crucially\ndepends on finite volume analogues of correlation decay property for both high\nand low temperature regimes -- the derivation of which borrows crucial ideas\nfrom FK-percolation theory and might be of independent interest. Finally, we\nalso derive lower bounds for estimation and testing rates in two parameter\nIsing models -- which turn out to be optimal according to several recent\nresults in this area.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 17:38:34 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Ray", "Gourab", ""]]}, {"id": "1906.00464", "submitter": "Dimitrios Giannakis", "authors": "Romeo Alexander, Dimitrios Giannakis", "title": "Operator-theoretic framework for forecasting nonlinear time series with\n  kernel analog techniques", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.physd.2020.132520", "report-no": null, "categories": "math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel analog forecasting (KAF), alternatively known as kernel principal\ncomponent regression, is a kernel method used for nonparametric statistical\nforecasting of dynamically generated time series data. This paper synthesizes\ndescriptions of kernel methods and Koopman operator theory in order to provide\na single consistent account of KAF. The framework presented here illuminates\nthe property of the KAF method that, under measure-preserving and ergodic\ndynamics, it consistently approximates the conditional expectation of\nobservables that are acted upon by the Koopman operator of the dynamical system\nand are conditioned on the observed data at forecast initialization. More\nprecisely, KAF yields optimal predictions, in the sense of minimal root mean\nsquare error with respect to the invariant measure, in the asymptotic limit of\nlarge data. The presented framework facilitates, moreover, the analysis of\ngeneralization error and quantification of uncertainty. Extensions of KAF to\nthe construction of conditional variance and conditional probability functions,\nas well as to non-symmetric kernels, are also shown. Illustrations of various\naspects of KAF are provided with applications to simple examples, namely a\nperiodic flow on the circle and the chaotic Lorenz 63 system.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:47:22 GMT"}, {"version": "v2", "created": "Sat, 22 Jun 2019 18:09:05 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2020 00:53:20 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Alexander", "Romeo", ""], ["Giannakis", "Dimitrios", ""]]}, {"id": "1906.00505", "submitter": "Yotam Hechtlinger", "authors": "Yoav Benjamini, Yotam Hechtlinger and Philip B. Stark", "title": "Confidence Intervals for Selected Parameters", "comments": "36 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Practical or scientific considerations often lead to selecting a subset of\nparameters as ``important.'' Inferences about those parameters often are based\non the same data used to select them in the first place. That can make the\nreported uncertainties deceptively optimistic: confidence intervals that ignore\nselection generally have less than their nominal coverage probability.\nControlling the probability that one or more intervals for selected parameters\ndo not cover---the ``simultaneous over the selected'' (SoS) error rate---is\ncrucial in many scientific problems. Intervals that control the SoS error rate\ncan be constructed in ways that take advantage of knowledge of the selection\nrule. We construct SoS-controlling confidence intervals for parameters deemed\nthe most ``important'' $k$ of $m$ shift parameters because they are estimated\n(by independent estimators) to be the largest. The new intervals improve\nsubstantially over \\v{S}id\\'{a}k intervals when $k$ is small compared to $m$,\nand approach the standard Bonferroni-corrected intervals when $k \\approx m$.\nStandard, unadjusted confidence intervals for location parameters have the\ncorrect coverage probability for $k=1$, $m=2$ if, when the true parameters are\nzero, the estimators are exchangeable and symmetric.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 23:58:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Benjamini", "Yoav", ""], ["Hechtlinger", "Yotam", ""], ["Stark", "Philip B.", ""]]}, {"id": "1906.00531", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Akshay Krishnamurthy and Haipeng Luo", "title": "Model selection for contextual bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of model selection for contextual bandits, where a\nlearner must adapt to the complexity of the optimal policy while balancing\nexploration and exploitation. Our main result is a new model selection\nguarantee for linear contextual bandits. We work in the stochastic realizable\nsetting with a sequence of nested linear policy classes of dimension $d_1 < d_2\n< \\ldots$, where the $m^\\star$-th class contains the optimal policy, and we\ndesign an algorithm that achieves $\\tilde{O}(T^{2/3}d^{1/3}_{m^\\star})$ regret\nwith no prior knowledge of the optimal dimension $d_{m^\\star}$. The algorithm\nalso achieves regret $\\tilde{O}(T^{3/4} + \\sqrt{Td_{m^\\star}})$, which is\noptimal for $d_{m^{\\star}}\\geq{}\\sqrt{T}$. This is the first model selection\nresult for contextual bandits with non-vacuous regret for all values of\n$d_{m^\\star}$, and to the best of our knowledge is the first positive result of\nthis type for any online learning setting with partial information. The core of\nthe algorithm is a new estimator for the gap in the best loss achievable by two\nlinear policy classes, which we show admits a convergence rate faster than the\nrate required to learn the parameters for either class.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 02:28:45 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 11:05:11 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 06:08:57 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Foster", "Dylan J.", ""], ["Krishnamurthy", "Akshay", ""], ["Luo", "Haipeng", ""]]}, {"id": "1906.00629", "submitter": "Kosuke Tanizaki", "authors": "Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani and\n  Ichiro Takeuchi", "title": "Computing Valid p-values for Image Segmentation by Selective Inference", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is one of the most fundamental tasks of computer vision.\nIn many practical applications, it is essential to properly evaluate the\nreliability of individual segmentation results. In this study, we propose a\nnovel framework to provide the statistical significance of segmentation results\nin the form of p-values. Specifically, we consider a statistical hypothesis\ntest for determining the difference between the object and the background\nregions. This problem is challenging because the difference can be deceptively\nlarge (called segmentation bias) due to the adaptation of the segmentation\nalgorithm to the data. To overcome this difficulty, we introduce a statistical\napproach called selective inference, and develop a framework to compute valid\np-values in which the segmentation bias is properly accounted for. Although the\nproposed framework is potentially applicable to various segmentation\nalgorithms, we focus in this paper on graph cut-based and threshold-based\nsegmentation algorithms, and develop two specific methods to compute valid\np-values for the segmentation results obtained by these algorithms. We prove\nthe theoretical validity of these two methods and demonstrate their\npracticality by applying them to segmentation problems for medical images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:27:27 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:23:18 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Tanizaki", "Kosuke", ""], ["Hashimoto", "Noriaki", ""], ["Inatsu", "Yu", ""], ["Hontani", "Hidekata", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1906.00723", "submitter": "Yair Goldberg", "authors": "Yair Goldberg and Malka Gorfine", "title": "Semiparametric Analysis of the Proportional Likelihood Ratio Model and\n  Omnibus Estimation Procedure", "comments": "The paper includes a correction to an error in the proof of Theorem\n  3. It also discusses the connection between the model of Luo & Tsai (2012)\n  and the proportional odds model discussed by Liang and Qin (2000), Chen\n  (2003,2007), and Tchetgen Tchetgen et al.(2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a semi-parametric analysis for the proportional likelihood ratio\nmodel, proposed by Luo & Tsai (2012). We study the tangent spaces for both the\nparameter of interest and the nuisance parameter, and obtain an explicit\nexpression for the efficient score function. We propose a family of\nZ-estimators based on the score functions, including an approximated efficient\nestimator. Using inverse probability weighting, the proposed estimators can\nalso be applied to different missing-data mechanisms, such as right censored\ndata and non-random sampling. A simulation study that illustrates the\nfinite-sample performance of the estimators is presented.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:42:49 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 06:39:51 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 19:47:07 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Goldberg", "Yair", ""], ["Gorfine", "Malka", ""]]}, {"id": "1906.00752", "submitter": "Peter Lindqvist", "authors": "Peter Lindqvist", "title": "A test against trend in random sequences", "comments": "This is essentially a revision of REPORT-MAT-A234, Helsinki\n  University of Technology 1986. (Some details are from REPORT-MAT-A131\n  (1978).)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a modification of Kendall's tau-test, replacing his permutations of\nn different numbers by sequences of length n, where repetition is allowed. In\nparticular, binary sequences are included. Random sequences can be tested.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 12:38:49 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Lindqvist", "Peter", ""]]}, {"id": "1906.00797", "submitter": "Michael Oberguggenberger", "authors": "Michael Oberguggenberger, Martin Schwarz", "title": "Deterministic and stochastic damage detection via dynamic response\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a method of damage detection in elastic materials, which\nis based on analyzing the time-dependent (dynamic) response of the material\nexcited by an acoustic signal. A case study is presented consisting of\nexperimental measurements and their mathematical analysis. The decisive\nparameters (wave speed and damping coefficient) of a mathematical model of the\nacoustic wave are calibrated by comparing the measurement data with the\nnumerically evaluated exact solution predicted by the mathematical model. The\ncalibration is done both deterministically by minimizing the square error over\ntime and stochastically by a Bayesian approach, implemented through the\nMetropolis-Hastings algorithm. The resulting posterior distribution of the\nparameters can be used to construct a Bayesian test for damage.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 08:27:58 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 14:00:37 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Oberguggenberger", "Michael", ""], ["Schwarz", "Martin", ""]]}, {"id": "1906.00875", "submitter": "Xiaoxi Shen", "authors": "Xiaoxi Shen, Chang Jiang, Lyudmila Sakhanenko and Qing Lu", "title": "Asymptotic Properties of Neural Network Sieve Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are one of the most popularly used methods in machine\nlearning and artificial intelligence nowadays. Due to the universal\napproximation theorem (Hornik et al. (1989)), a neural network with one hidden\nlayer can approximate any continuous function on a compact support as long as\nthe number of hidden units is sufficiently large. Statistically, a neural\nnetwork can be classified into a nonlinear regression framework. However, if we\nconsider it parametrically, due to the unidentifiability of the parameters, it\nis difficult to derive its asymptotic properties. Instead, we considered the\nestimation problem in a nonparametric regression framework and use the results\nfrom sieve estimation to establish the consistency, the rates of convergence\nand the asymptotic normality of the neural network estimators. We also\nillustrate the validity of the theories via simulations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:30:01 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:32:03 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Shen", "Xiaoxi", ""], ["Jiang", "Chang", ""], ["Sakhanenko", "Lyudmila", ""], ["Lu", "Qing", ""]]}, {"id": "1906.00904", "submitter": "Boris Hanin", "authors": "Boris Hanin, David Rolnick", "title": "Deep ReLU Networks Have Surprisingly Few Activation Patterns", "comments": "18 page, 7 figures", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep networks has been attributed in part to their\nexpressivity: per parameter, deep networks can approximate a richer class of\nfunctions than shallow networks. In ReLU networks, the number of activation\npatterns is one measure of expressivity; and the maximum number of patterns\ngrows exponentially with the depth. However, recent work has showed that the\npractical expressivity of deep networks - the functions they can learn rather\nthan express - is often far from the theoretical maximum. In this paper, we\nshow that the average number of activation patterns for ReLU networks at\ninitialization is bounded by the total number of neurons raised to the input\ndimension. We show empirically that this bound, which is independent of the\ndepth, is tight both at initialization and during training, even on\nmemorization tasks that should maximize the number of activation patterns. Our\nwork suggests that realizing the full expressivity of deep networks may not be\npossible in practice, at least with current methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:13:15 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 16:26:04 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hanin", "Boris", ""], ["Rolnick", "David", ""]]}, {"id": "1906.01139", "submitter": "Daniel Hsu", "authors": "Ji Xu and Daniel Hsu", "title": "On the number of variables to use in principal component regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study least squares linear regression over $N$ uncorrelated Gaussian\nfeatures that are selected in order of decreasing variance. When the number of\nselected features $p$ is at most the sample size $n$, the estimator under\nconsideration coincides with the principal component regression estimator; when\n$p>n$, the estimator is the least $\\ell_2$ norm solution over the selected\nfeatures. We give an average-case analysis of the out-of-sample prediction\nerror as $p,n,N \\to \\infty$ with $p/N \\to \\alpha$ and $n/N \\to \\beta$, for some\nconstants $\\alpha \\in [0,1]$ and $\\beta \\in (0,1)$. In this average-case\nsetting, the prediction error exhibits a \"double descent\" shape as a function\nof $p$. We also establish conditions under which the minimum risk is achieved\nin the interpolating ($p>n$) regime.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 00:22:10 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 13:10:42 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Xu", "Ji", ""], ["Hsu", "Daniel", ""]]}, {"id": "1906.01204", "submitter": "Paulo Orenstein", "authors": "Paulo Orenstein", "title": "Robust Mean Estimation with the Bayesian Median of Means", "comments": "54 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample mean is often used to aggregate different unbiased estimates of a\nparameter, producing a final estimate that is unbiased but possibly\nhigh-variance. This paper introduces the Bayesian median of means, an\naggregation rule that roughly interpolates between the sample mean and median,\nresulting in estimates with much smaller variance at the expense of bias. While\nthe procedure is non-parametric, its squared bias is asymptotically negligible\nrelative to the variance, similar to maximum likelihood estimators. The\nBayesian median of means is consistent, and concentration bounds for the\nestimator's bias and $L_1$ error are derived, as well as a fast non-randomized\napproximating algorithm. The performances of both the exact and the approximate\nprocedures match that of the sample mean in low-variance settings, and exhibit\nmuch better results in high-variance scenarios. The empirical performances are\nexamined in real and simulated data, and in applications such as importance\nsampling, cross-validation and bagging.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:41:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Orenstein", "Paulo", ""]]}, {"id": "1906.01235", "submitter": "Trevor Campbell", "authors": "Trevor Campbell and Xinglong Li", "title": "Universal Boosting Variational Inference", "comments": "In Advances in Neural Information Processing Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting variational inference (BVI) approximates an intractable probability\ndensity by iteratively building up a mixture of simple component distributions\none at a time, using techniques from sparse convex optimization to provide both\ncomputational scalability and approximation error guarantees. But the\nguarantees have strong conditions that do not often hold in practice, resulting\nin degenerate component optimization problems; and we show that the ad-hoc\nregularization used to prevent degeneracy in practice can cause BVI to fail in\nunintuitive ways. We thus develop universal boosting variational inference\n(UBVI), a BVI scheme that exploits the simple geometry of probability densities\nunder the Hellinger metric to prevent the degeneracy of other gradient-based\nBVI methods, avoid difficult joint optimizations of both component and weight,\nand simplify fully-corrective weight optimizations. We show that for any target\ndensity and any mixture component family, the output of UBVI converges to the\nbest possible approximation in the mixture family, even when the mixture family\nis misspecified. We develop a scalable implementation based on exponential\nfamily mixture components and standard stochastic optimization techniques.\nFinally, we discuss statistical benefits of the Hellinger distance as a\nvariational objective through bounds on posterior probability, moment, and\nimportance sampling errors. Experiments on multiple datasets and models show\nthat UBVI provides reliable, accurate posterior approximations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 07:08:50 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 08:24:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Campbell", "Trevor", ""], ["Li", "Xinglong", ""]]}, {"id": "1906.01302", "submitter": "Jad Beyhum", "authors": "Jad Beyhum (UT1, TSE)", "title": "Inference robust to outliers with l1-norm penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inference in a linear regression model\nwith outliers where the number of outliers can grow with sample size but their\nproportion goes to 0. We apply the square-root lasso estimator penalizing the\nl1-norm of a random vector which is non-zero for outliers. We derive rates of\nconvergence and asymptotic normality. Our estimator has the same asymptotic\nvariance as the OLS estimator in the standard linear model. This enables to\nbuild tests and confidence sets in the usual and simple manner. The proposed\nprocedure is also computationally advantageous as it amounts to solving a\nconvex optimization program. Overall, the suggested approach constitutes a\npractical robust alternative to the ordinary least squares estimator.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 09:45:57 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Beyhum", "Jad", "", "UT1, TSE"]]}, {"id": "1906.01341", "submitter": "Toru Imai", "authors": "Toru Imai", "title": "Estimating Real Log Canonical Thresholds", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of the marginal likelihood plays an important role in model\nselection problems. The widely applicable Bayesian information criterion (WBIC)\nand singular Bayesian information criterion (sBIC) give approximations to the\nlog marginal likelihood, which can be applied to both regular and singular\nmodels. When the real log canonical thresholds are known, the performance of\nsBIC is considered to be better than that of WBIC, but only few real log\ncanonical thresholds are known. In this paper, we propose a new estimator of\nthe real log canonical thresholds based on the variance of thermodynamic\nintegration with an inverse temperature. In addition, we propose an application\nto make sBIC widely applicable. Finally, we investigate the performance of the\nestimator and model selection by simulation studies and application to real\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:59:33 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 07:14:18 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Imai", "Toru", ""]]}, {"id": "1906.01547", "submitter": "Fabien Navarro", "authors": "Marie du Roy de Chaumaray, Matthieu Marbac, Fabien Navarro", "title": "Mixture of hidden Markov models for accelerometer data", "comments": null, "journal-ref": "Ann. Appl. Stat. 14 (2020), no. 4, 1834--1855", "doi": "10.1214/20-AOAS1375", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of accelerometer data, we introduce a specific\nfinite mixture of hidden Markov models with particular characteristics that\nadapt well to the specific nature of this type of data. Our model allows for\nthe computation of statistics that characterize the physical activity of a\nsubject (\\emph{e.g.}, the mean time spent at different activity levels and the\nprobability of the transition between two activity levels) without specifying\nthe activity levels in advance but by estimating them from the data. In\naddition, this approach allows the heterogeneity of the population to be taken\ninto account and subpopulations with homogeneous physical activity behavior to\nbe defined. We prove that, under mild assumptions, this model implies that the\nprobability of misclassifying a subject decreases at an exponential decay with\nthe length of its measurement sequence. Model identifiability is also\ninvestigated. We also report a comprehensive suite of numerical simulations to\nsupport our theoretical findings. Method is motivated by and applied to the PAT\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:06:22 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 14:20:03 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["de Chaumaray", "Marie du Roy", ""], ["Marbac", "Matthieu", ""], ["Navarro", "Fabien", ""]]}, {"id": "1906.01614", "submitter": "Nian Si", "authors": "Jose Blanchet, Karthyek Murthy, Nian Si", "title": "Confidence Regions in Wasserstein Distributionally Robust Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein distributionally robust optimization estimators are obtained as\nsolutions of min-max problems in which the statistician selects a parameter\nminimizing the worst-case loss among all probability models within a certain\ndistance (in a Wasserstein sense) from the underlying empirical measure. While\nmotivated by the need to identify optimal model parameters or decision choices\nthat are robust to model misspecification, these distributionally robust\nestimators recover a wide range of regularized estimators, including\nsquare-root lasso and support vector machines, among others, as particular\ncases. This paper studies the asymptotic normality of these distributionally\nrobust estimators as well as the properties of an optimal (in a suitable sense)\nconfidence region induced by the Wasserstein distributionally robust\noptimization formulation. In addition, key properties of min-max\ndistributionally robust optimization problems are also studied, for example, we\nshow that distributionally robust estimators regularize the loss based on its\nderivative and we also derive general sufficient conditions which show the\nequivalence between the min-max distributionally robust optimization problem\nand the corresponding max-min formulation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:45:32 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 11:43:52 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 06:42:17 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 08:21:04 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Blanchet", "Jose", ""], ["Murthy", "Karthyek", ""], ["Si", "Nian", ""]]}, {"id": "1906.01850", "submitter": "F. Richard Guo", "authors": "F. Richard Guo and Thomas S. Richardson", "title": "On Testing Marginal versus Conditional Independence", "comments": "Revisions and updated references", "journal-ref": null, "doi": "10.1093/biomet/asaa040", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing marginal independence versus conditional independence in\na trivariate Gaussian setting. The two models are non-nested and their\nintersection is a union of two marginal independences. We consider two\nsequences of such models, one from each type of independence, that are closest\nto each other in the Kullback-Leibler sense as they approach the intersection.\nThey become indistinguishable if the signal strength, as measured by the\nproduct of two correlation parameters, decreases faster than the standard\nparametric rate. Under local alternatives at such rate, we show that the\nasymptotic distribution of the likelihood ratio depends on where and how the\nlocal alternatives approach the intersection. To deal with this non-uniformity,\nwe study a class of \"envelope\" distributions by taking pointwise suprema over\nasymptotic cumulative distribution functions. We show that these envelope\ndistributions are well-behaved and lead to model selection procedures with\nrate-free uniform error guarantees and near-optimal power. To control the error\neven when the two models are indistinguishable, rather than insist on a\ndichotomous choice, the proposed procedure will choose either or both models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:45:12 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 06:19:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Guo", "F. Richard", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1906.01908", "submitter": "Guillaume Ausset", "authors": "Guillaume Ausset, St\\'ephan Cl\\'emen\\c{c}on, Fran\\c{c}ois Portier", "title": "Empirical Risk Minimization under Random Censorship: Theory and Practice", "comments": "Submitted to JMLR. 18 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic supervised learning problem, where a continuous\nnon-negative random label $Y$ (i.e. a random duration) is to be predicted based\nupon observing a random vector $X$ valued in $\\mathbb{R}^d$ with $d\\geq 1$ by\nmeans of a regression rule with minimum least square error. In various\napplications, ranging from industrial quality control to public health through\ncredit risk analysis for instance, training observations can be right censored,\nmeaning that, rather than on independent copies of $(X,Y)$, statistical\nlearning relies on a collection of $n\\geq 1$ independent realizations of the\ntriplet $(X, \\; \\min\\{Y,\\; C\\},\\; \\delta)$, where $C$ is a nonnegative r.v.\nwith unknown distribution, modeling censorship and $\\delta=\\mathbb{I}\\{Y\\leq\nC\\}$ indicates whether the duration is right censored or not. As ignoring\ncensorship in the risk computation may clearly lead to a severe underestimation\nof the target duration and jeopardize prediction, we propose to consider a\nplug-in estimate of the true risk based on a Kaplan-Meier estimator of the\nconditional survival function of the censorship $C$ given $X$, referred to as\nKaplan-Meier risk, in order to perform empirical risk minimization. It is\nestablished, under mild conditions, that the learning rate of minimizers of\nthis biased/weighted empirical risk functional is of order\n$O_{\\mathbb{P}}(\\sqrt{\\log(n)/n})$ when ignoring model bias issues inherent to\nplug-in estimation, as can be attained in absence of censorship. Beyond\ntheoretical results, numerical experiments are presented in order to illustrate\nthe relevance of the approach developed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:40:41 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ausset", "Guillaume", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1906.02158", "submitter": "Osama Idais", "authors": "Osama Idais", "title": "Locally optimal designs for generalized linear models within the family\n  of Kiefer $\\Phi_k$-criteria", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally optimal designs for generalized linear models are derived at certain\nvalues of the regression parameters. In the present paper a general setup of\nthe generalized linear model is considered. Analytic solutions for optimal\ndesigns are developed under Kiefer $\\Phi_k$-criteria highlighting the D- and\nA-optimal designs. By means of The General Equivalence Theorem necessary and\nsufficient conditions in term of intensity values are obtained to characterize\nthe locally optimal designs. In this context, linear predictors are assumed\nconstituting first order models with and without intercept on appropriate\nexperimental regions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:41:23 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Idais", "Osama", ""]]}, {"id": "1906.02196", "submitter": "Jos\\'e Gonz\\'alez-Barrios Dr.", "authors": "Jos\\'e M. Gonz\\'alez-Barrios, Eduardo Guti\\'errez-Pe\\~na, Juan D.\n  Nieves and Ra\\'ul Rueda", "title": "A novel characterization and new simple tests of multivariate\n  independence using copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is twofold. First, we provide a novel\ncharacterization of independence of random vectors based on the checkerboard\napproximation to a multivariate copula. Using this result, we then propose a\nnew family of tests of multivariate independence for continuous random vectors.\nThe tests rely on estimating the checkerboard approximation by means of the\nsample copula recently introduced in [9] and improved in [11]. Such estimators\nhave nice properties, including a Glivenko-Cantelli-type theorem that\nguarantees almost-sure uniform convergence to the checkerboard approximation.\nEach of our test statistics is defined in terms of one of a number of different\nmetrics, including the supremum, total variation and Hellinger distances, as\nwell as the Kullback-Leibler divergence. All of these tests can be easily\nimplemented since the corresponding test statistics can be efficiently\nsimulated under any alternative hypothesis, even for moderate and large sample\nsizes in relatively large dimensions. Finally, we assess the performance of our\ntests by means of a simulation study and provide one real data example.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:54:47 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Gonz\u00e1lez-Barrios", "Jos\u00e9 M.", ""], ["Guti\u00e9rrez-Pe\u00f1a", "Eduardo", ""], ["Nieves", "Juan D.", ""], ["Rueda", "Ra\u00fal", ""]]}, {"id": "1906.02265", "submitter": "Ruizhi Zhang", "authors": "Ruizhi Zhang, Yajun Mei, Jianjun Shi", "title": "Robust real-time monitoring of high-dimensional data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust real-time monitoring of high-dimensional data streams has many\nimportant real-world applications such as industrial quality control, signal\ndetection, biosurveillance, but unfortunately it is highly non-trivial to\ndevelop efficient schemes due to two challenges: (1) the unknown sparse number\nor subset of affected data streams and (2) the uncertainty of model\nspecification for high-dimensional data. In this article, motivated by the\ndetection of smaller persistent changes in the presence of larger transient\noutliers, we develop a family of efficient real-time robust detection schemes\nfor high-dimensional data streams through monitoring feature spaces such as PCA\nor wavelet coefficients when the feature coefficients are from Tukey-Huber's\ngross error models with outliers. We propose to construct a new local detection\nstatistic for each feature called $L_{\\alpha}$-CUSUM statistic that can reduce\nthe effect of outliers by using the Box-Cox transformation of the likelihood\nfunction, and then raise a global alarm based upon the sum of the\nsoft-thresholding transformation of these local $L_{\\alpha}$-CUSUM statistics\nso that to filter out unaffected features. In addition, we propose a new\nconcept called false alarm breakdown point to measure the robustness of online\nmonitoring schemes, and also characterize the breakdown point of our proposed\nschemes. Asymptotic analysis, extensive numerical simulations and case study of\nnonlinear profile monitoring are conducted to illustrate the robustness and\nusefulness of our proposed schemes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:24:11 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Ruizhi", ""], ["Mei", "Yajun", ""], ["Shi", "Jianjun", ""]]}, {"id": "1906.02272", "submitter": "Ruizhi Zhang", "authors": "Ruizhi Zhang, Yajun Mei, Jianjun Shi, Huan Xu", "title": "Robustness and Tractability for Non-convex M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two important properties of M-estimator, namely, robustness\nand tractability, in linear regression setting, when the observations are\ncontaminated by some arbitrary outliers. Specifically, robustness means the\nstatistical property that the estimator should always be close to the\nunderlying true parameters {\\em regardless of the distribution of the\noutliers}, and tractability indicates the computational property that the\nestimator can be computed efficiently, even if the objective function of the\nM-estimator is {\\em non-convex}. In this article, by learning the landscape of\nthe empirical risk, we show that under mild conditions, many M-estimators enjoy\nnice robustness and tractability properties simultaneously, when the percentage\nof outliers is small. We further extend our analysis to the high-dimensional\nsetting, where the number of parameters is greater than the number of samples,\n$p \\gg n$, and prove that when the proportion of outliers is small, the\npenalized M-estimators with {\\em $L_1$} penalty will enjoy robustness and\ntractability simultaneously. Our research provides an analytic approach to see\nthe effects of outliers and tuning parameters on the robustness and\ntractability for some families of M-estimators. Simulation and case study are\npresented to illustrate the usefulness of our theoretical results for\nM-estimators under Welsch's exponential squared loss.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:37:48 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Ruizhi", ""], ["Mei", "Yajun", ""], ["Shi", "Jianjun", ""], ["Xu", "Huan", ""]]}, {"id": "1906.02389", "submitter": "Guang Cheng", "authors": "Ching-Wei Cheng and Guang Cheng", "title": "Enhancing Multi-model Inference with Natural Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-model inference covers a wide range of modern statistical applications\nsuch as variable selection, model confidence set, model averaging and variable\nimportance. The performance of multi-model inference depends on the\navailability of candidate models, whose quality has been rarely studied in\nliterature. In this paper, we study genetic algorithm (GA) in order to obtain\nhigh-quality candidate models. Inspired by the process of natural selection, GA\nperforms genetic operations such as selection, crossover and mutation\niteratively to update a collection of potential solutions (models) until\nconvergence. The convergence properties are studied based on the Markov chain\ntheory and used to design an adaptive termination criterion that vastly reduces\nthe computational cost. In addition, a new schema theory is established to\ncharacterize how the current model set is improved through evolutionary\nprocess. Extensive numerical experiments are carried out to verify our theory\nand demonstrate the empirical power of GA, and new findings are obtained for\ntwo real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 02:51:08 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Cheng", "Ching-Wei", ""], ["Cheng", "Guang", ""]]}, {"id": "1906.02493", "submitter": "Aude Sportisse", "authors": "Aude Sportisse (LPSM (UMR\\_8001)), Claire Boyer (LPSM (UMR\\_8001),\n  DMA), Julie Josse (CMAP)", "title": "Estimation and imputation in Probabilistic Principal Component Analysis\n  with Missing Not At Random data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing Not At Random (MNAR) values lead to significant biases in the data,\nsince the probability of missingness depends on the unobserved values.They are\n''not ignorable'' in the sense that they often require defining a model for the\nmissing data mechanism, which makes inference or imputation tasks more complex.\nFurthermore, this implies a strong \\textit{a priori} on the parametric form of\nthe distribution.However, some works have obtained guarantees on the estimation\nof parameters in the presence of MNAR data, without specifying the distribution\nof missing data \\citep{mohan2018estimation, tang2003analysis}. This is very\nuseful in practice, but is limited to simple cases such as self-masked MNAR\nvalues in data generated according to linear regression models.We continue this\nline of research, but extend it to a more general MNAR mechanism, in a more\ngeneral model of the probabilistic principal component analysis (PPCA),\n\\textit{i.e.}, a low-rank model with random effects. We prove identifiability\nof the PPCA parameters. We then propose an estimation of the loading\ncoefficients and a data imputation method. They are based on estimators of\nmeans, variances and covariances of missing variables, for which consistency is\ndiscussed. These estimators have the great advantage of being calculated using\nonly the observed data, leveraging the underlying low-rank structure of the\ndata. We illustrate the relevance of the method with numerical experiments on\nsynthetic data and also on real data collected from a medical register.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:23:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:16:07 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 15:38:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sportisse", "Aude", "", "LPSM"], ["Boyer", "Claire", "", "LPSM"], ["Josse", "Julie", "", "CMAP"]]}, {"id": "1906.02733", "submitter": "Daniel Bonn\\'ery", "authors": "Daniel Bonnery and Joseph Sedransk", "title": "On the definition of informative vs. ignorable nuisance process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an early version.\n  We propose to generalise the notion of \"ignoring\" a random process as well as\nthe notions of informative and ignorable random processes in a very general\nsetup and for different types of inference (Bayesian or frequentist), and for\ndifferent purposes (estimation, prediction or testing). We then confront the\ndefinitions we propose to mentions or definitions of informative and ignorable\nprocesses found in the litterature. To that purpose, we provide a very general\nstatistical framework for survey sampling in order to define precisely the\nnotions of design and selection, and to serve to illustrate and discuss the\nnotions proposed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 17:53:09 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Bonnery", "Daniel", ""], ["Sedransk", "Joseph", ""]]}, {"id": "1906.02746", "submitter": "Mihai Cucuringu", "authors": "Alexandre d'Aspremont, Mihai Cucuringu, Hemant Tyagi", "title": "Ranking and synchronization from pairwise measurements via SVD", "comments": "49 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a measurement graph $G= (V,E)$ and an unknown signal $r \\in\n\\mathbb{R}^n$, we investigate algorithms for recovering $r$ from pairwise\nmeasurements of the form $r_i - r_j$; $\\{i,j\\} \\in E$. This problem arises in a\nvariety of applications, such as ranking teams in sports data and time\nsynchronization of distributed networks. Framed in the context of ranking, the\ntask is to recover the ranking of $n$ teams (induced by $r$) given a small\nsubset of noisy pairwise rank offsets. We propose a simple SVD-based\nalgorithmic pipeline for both the problem of time synchronization and ranking.\nWe provide a detailed theoretical analysis in terms of robustness against both\nsampling sparsity and noise perturbations with outliers, using results from\nmatrix perturbation and random matrix theory. Our theoretical findings are\ncomplemented by a detailed set of numerical experiments on both synthetic and\nreal data, showcasing the competitiveness of our proposed algorithms with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 16:17:36 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 19:46:59 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 15:22:50 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["d'Aspremont", "Alexandre", ""], ["Cucuringu", "Mihai", ""], ["Tyagi", "Hemant", ""]]}, {"id": "1906.02830", "submitter": "Thomas Steinke", "authors": "Mark Bun, Thomas Steinke", "title": "Average-Case Averages: Private Algorithms for Smooth Sensitivity and\n  Mean Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplest and most widely applied method for guaranteeing differential\nprivacy is to add instance-independent noise to a statistic of interest that is\nscaled to its global sensitivity. However, global sensitivity is a worst-case\nnotion that is often too conservative for realized dataset instances. We\nprovide methods for scaling noise in an instance-dependent way and demonstrate\nthat they provide greater accuracy under average-case distributional\nassumptions.\n  Specifically, we consider the basic problem of privately estimating the mean\nof a real distribution from i.i.d.~samples. The standard empirical mean\nestimator can have arbitrarily-high global sensitivity. We propose the trimmed\nmean estimator, which interpolates between the mean and the median, as a way of\nattaining much lower sensitivity on average while losing very little in terms\nof statistical accuracy.\n  To privately estimate the trimmed mean, we revisit the smooth sensitivity\nframework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a\nframework for using instance-dependent sensitivity. We propose three new\nadditive noise distributions which provide concentrated differential privacy\nwhen scaled to smooth sensitivity. We provide theoretical and experimental\nevidence showing that our noise distributions compare favorably to others in\nthe literature, in particular, when applied to the mean estimation problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:55:02 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Bun", "Mark", ""], ["Steinke", "Thomas", ""]]}, {"id": "1906.02838", "submitter": "Omer Tamuz", "authors": "Xiaosheng Mu, Luciano Pomatto, Philipp Strack, and Omer Tamuz", "title": "From Blackwell Dominance in Large Samples to Renyi Divergences and Back\n  Again", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.TH math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study repeated independent Blackwell experiments; standard examples\ninclude drawing multiple samples from a population, or performing a measurement\nin different locations. In the baseline setting of a binary state of nature, we\ncompare experiments in terms of their informativeness in large samples.\nAddressing a question due to Blackwell (1951), we show that generically an\nexperiment is more informative than another in large samples if and only if it\nhas higher Renyi divergences.\n  We apply our analysis to the problem of measuring the degree of dissimilarity\nbetween distributions by means of divergences. A useful property of Renyi\ndivergences is their additivity with respect to product distributions. Our\ncharacterization of Blackwell dominance in large samples implies that every\nadditive divergence that satisfies the data processing inequality is an\nintegral of Renyi divergences.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 23:15:02 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 03:50:40 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 14:43:50 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 14:59:13 GMT"}, {"version": "v5", "created": "Fri, 4 Sep 2020 18:39:39 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Mu", "Xiaosheng", ""], ["Pomatto", "Luciano", ""], ["Strack", "Philipp", ""], ["Tamuz", "Omer", ""]]}, {"id": "1906.02903", "submitter": "Hongji Wei", "authors": "T. Tony Cai, Hongji Wei", "title": "Transfer Learning for Nonparametric Classification: Minimax Rate and\n  Adaptive Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human learners have the natural ability to use knowledge gained in one\nsetting for learning in a different but related setting. This ability to\ntransfer knowledge from one task to another is essential for effective\nlearning. In this paper, we study transfer learning in the context of\nnonparametric classification based on observations from different distributions\nunder the posterior drift model, which is a general framework and arises in\nmany practical problems.\n  We first establish the minimax rate of convergence and construct a\nrate-optimal two-sample weighted $K$-NN classifier. The results characterize\nprecisely the contribution of the observations from the source distribution to\nthe classification task under the target distribution. A data-driven adaptive\nclassifier is then proposed and is shown to simultaneously attain within a\nlogarithmic factor of the optimal rate over a large collection of parameter\nspaces. Simulation studies and real data applications are carried out where the\nnumerical results further illustrate the theoretical analysis. Extensions to\nthe case of multiple source distributions are also considered.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 05:29:48 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "1906.02996", "submitter": "Maria Mohr", "authors": "Maria Mohr, Natalie Neumeyer", "title": "Nonparametric volatility change detection", "comments": "20 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonparametric heteroscedastic time series regression model and\nsuggest testing procedures to detect changes in the conditional variance\nfunction. The tests are based on a sequential marked empirical process and thus\ncombine classical CUSUM tests with marked empirical process approaches known\nfrom goodness-of-fit testing. The tests are consistent against general\nalternatives of a change in the conditional variance function, a feature that\nclassical CUSUM tests are lacking. We derive a simple limiting distribution and\nin the case of univariate covariates even obtain asymptotically\ndistribution-free tests. We demonstrate the good performance of the tests in a\nsimulation study and consider exchange rate data as a real data application.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 10:08:23 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Mohr", "Maria", ""], ["Neumeyer", "Natalie", ""]]}, {"id": "1906.03058", "submitter": "Guillaume Lecu\\'e", "authors": "Jules Depersin and Guillaume Lecu\\'e", "title": "Robust subgaussian estimation of a mean vector in nearly linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an algorithm, running in time $\\tilde{\\mathcal O}(N d + uK d)$,\nwhich is robust to outliers and heavy-tailed data and which achieves the\nsubgaussian rate from [Lugosi, Mendelson]\n\\begin{equation}\\label{eq:intro_subgaus_rate} \\sqrt{\\frac{{\\rm\nTr}(\\Sigma)}{N}}+\\sqrt{\\frac{||\\Sigma||_{op}K}{N}} \\end{equation}with\nprobability at least $1-\\exp(-c_0K)-\\exp(-c_1 u)$ where $\\Sigma$ is the\ncovariance matrix of the informative data, $K\\in\\{1, \\ldots, K\\}$ is some\nparameter (number of block means) and $u>0$ is another parameter of the\nalgorithm. This rate is achieved when $K\\geq c_1 |\\mathcal O|$ where $|\\mathcal\nO|$ is the number of outliers in the database and under the only assumption\nthat the informative data have a second moment. The algorithm is fully\ndata-dependent and does not use in its construction the proportion of outliers\nnor the rate above. Its construction combines recently developed tools for\nMedian-of-Means estimators and covering-Semi-definite Programming [Chen,\nDiakonikolas, Ge] and [Peng, Tangwongsan, Zhang].\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 13:09:43 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 10:03:37 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 15:42:35 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 07:56:50 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Depersin", "Jules", ""], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "1906.03179", "submitter": "Alexander Krei{\\ss}", "authors": "Alexander Kreiss", "title": "Correlation bounds, mixing and m-dependence under random time-varying\n  network distances with an application to Cox-Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will consider multivariate stochastic processes indexed either by vertices\nor pairs of vertices of a dynamic network. Under a dynamic network we\nunderstand a network with a fixed vertex set and an edge set which changes\nrandomly over time. We will assume that the spatial dependence-structure of the\nprocesses conditional on the network behaves in the following way: Close\nvertices (or pairs of vertices) are dependent, while we assume that the\ndependence decreases conditionally on that the distance in the network\nincreases. We make this intuition mathematically precise by considering three\nconcepts based on correlation, beta-mixing with time-varying beta-coefficients\nand conditional independence. These concepts allow proving weak-dependence\nresults, e.g. an exponential inequality, which might be of independent\ninterest. In order to demonstrate the use of these concepts in an application\nwe study the asymptotics (for growing networks) of a goodness of fit test in a\ndynamic interaction network model based on a Cox-type model for counting\nprocesses. This model is then applied to bike-sharing data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:45:38 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 14:38:37 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 18:13:55 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kreiss", "Alexander", ""]]}, {"id": "1906.03225", "submitter": "Tobias Kley", "authors": "Josua G\\\"osmann, Tobias Kley, Holger Dette", "title": "A new approach for open-end sequential change point monitoring", "comments": "31 pages, 5 figures; online appendix (20 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sequential monitoring scheme for changes in the parameters\nof a multivariate time series. In contrast to procedures proposed in the\nliterature which compare an estimator from the training sample with an\nestimator calculated from the remaining data, we suggest to divide the sample\nat each time point after the training sample. Estimators from the sample before\nand after all separation points are then continuously compared calculating a\nmaximum of norms of their differences. For open-end scenarios our approach\nyields an asymptotic level $\\alpha$ procedure, which is consistent under the\nalternative of a change in the parameter. By means of a simulation study it is\ndemonstrated that the new method outperforms the commonly used procedures with\nrespect to power and the feasibility of our approach is illustrated by\nanalyzing two data examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:56:13 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:32:58 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 14:59:30 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 13:46:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["G\u00f6smann", "Josua", ""], ["Kley", "Tobias", ""], ["Dette", "Holger", ""]]}, {"id": "1906.03317", "submitter": "Saied Mahdian", "authors": "Saied Mahdian, Jose Blanchet, Peter Glynn", "title": "Optimal Transport Relaxations with Application to Wasserstein GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of relaxations of the optimal transport problem which\nregularize the problem by introducing an additional minimization step over a\nsmall region around one of the underlying transporting measures. The type of\nregularization that we obtain is related to smoothing techniques studied in the\noptimization literature. When using our approach to estimate optimal transport\ncosts based on empirical measures, we obtain statistical learning bounds which\nare useful to guide the amount of regularization, while maintaining good\ngeneralization properties. To illustrate the computational advantages of our\nregularization approach, we apply our method to training Wasserstein GANs. We\nobtain running time improvements, relative to current benchmarks, with no\ndeterioration in testing performance (via FID). The running time improvement\noccurs because our new optimality-based threshold criterion reduces the number\nof expensive iterates of the generating networks, while increasing the number\nof actor-critic iterations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 20:01:56 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Mahdian", "Saied", ""], ["Blanchet", "Jose", ""], ["Glynn", "Peter", ""]]}, {"id": "1906.03371", "submitter": "Jan-Christian H\\\"utter", "authors": "Jan-Christian H\\\"utter, Philippe Rigollet", "title": "Estimation Rates for Sparse Linear Cyclic Causal Models", "comments": "48 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal models are important tools to understand complex phenomena and predict\nthe outcome of controlled experiments, also known as interventions. In this\nwork, we present statistical rates of estimation for linear cyclic causal\nmodels under the assumption of homoscedastic Gaussian noise by analyzing both\nthe LLC estimator introduced by Hyttinen, Eberhardt and Hoyer and a novel\ntwo-step penalized maximum likelihood estimator. We establish asymptotic near\nminimax optimality for the maximum likelihood estimator over a class of sparse\ncausal graphs in the case of near-optimally chosen interventions. Moreover, we\nfind evidence for practical advantages of this estimator compared to LLC in\nsynthetic numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 01:54:37 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["H\u00fctter", "Jan-Christian", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1906.03401", "submitter": "Preetam Nandy", "authors": "Kinjal Basu and Preetam Nandy", "title": "Optimal Convergence for Stochastic Optimization with Multiple\n  Expectation Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of stochastic optimization where the\nobjective function can be written as an expectation function over a closed\nconvex set. We also consider multiple expectation constraints which restrict\nthe domain of the problem. We extend the cooperative stochastic approximation\nalgorithm from Lan and Zhou [2016] to solve the particular problem. We close\nthe gaps in the previous analysis and provide a novel proof technique to show\nthat our algorithm attains the optimal rate of convergence for both optimality\ngap and constraint violation when the functions are generally convex. We also\ncompare our algorithm empirically to the state-of-the-art and show improved\nconvergence in many situations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 06:56:39 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 17:08:45 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Basu", "Kinjal", ""], ["Nandy", "Preetam", ""]]}, {"id": "1906.03486", "submitter": "Kweku Abraham", "authors": "Kweku Abraham and Richard Nickl", "title": "On statistical Calder\\'on problems", "comments": "To appear in \"Mathematical Statistics and Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For $D$ a bounded domain in $\\mathbb R^d, d \\ge 2,$ with smooth boundary\n$\\partial D$, the non-linear inverse problem of recovering the unknown\nconductivity $\\gamma$ determining solutions $u=u_{\\gamma, f}$ of the partial\ndifferential equation \\begin{equation*} \\begin{split} \\nabla \\cdot(\\gamma\n\\nabla u)&=0 \\quad \\text{ in }D, \\\\ u&=f \\quad \\text { on } \\partial D,\n\\end{split} \\end{equation*} from noisy observations $Y$ of the\nDirichlet-to-Neumann map \\[f \\mapsto \\Lambda_\\gamma(f) = {\\gamma \\frac{\\partial\nu_{\\gamma,f}}{\\partial \\nu}}\\Big|_{\\partial D},\\] with $\\partial/\\partial \\nu$\ndenoting the outward normal derivative, is considered. The data $Y$ consists of\n$\\Lambda_\\gamma$ corrupted by additive Gaussian noise at noise level\n$\\varepsilon>0$, and a statistical algorithm $\\hat \\gamma(Y)$ is constructed\nwhich is shown to recover $\\gamma$ in supremum-norm loss at a statistical\nconvergence rate of the order $\\log(1/\\varepsilon)^{-\\delta}$ as $\\varepsilon\n\\to 0$. It is further shown that this convergence rate is optimal, up to the\nprecise value of the exponent $\\delta>0$, in an information theoretic sense.\nThe estimator $\\hat \\gamma(Y)$ has a Bayesian interpretation in terms of the\nposterior mean of a suitable Gaussian process prior and can be computed by MCMC\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 16:31:19 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 14:05:07 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 15:40:04 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2020 17:32:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Abraham", "Kweku", ""], ["Nickl", "Richard", ""]]}, {"id": "1906.03794", "submitter": "Yi Hao", "authors": "Yi Hao, Alon Orlitsky", "title": "The Broad Optimality of Profile Maximum Likelihood", "comments": "Added a new section (Section 8) about truncated PML (TPML) and\n  derived several new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three fundamental statistical-learning problems: distribution\nestimation, property estimation, and property testing. We establish the profile\nmaximum likelihood (PML) estimator as the first unified sample-optimal approach\nto a wide range of learning tasks. In particular, for every alphabet size $k$\nand desired accuracy $\\varepsilon$:\n  $\\textbf{Distribution estimation}$ Under $\\ell_1$ distance, PML yields\noptimal $\\Theta(k/(\\varepsilon^2\\log k))$ sample complexity for\nsorted-distribution estimation, and a PML-based estimator empirically\noutperforms the Good-Turing estimator on the actual distribution;\n  $\\textbf{Additive property estimation}$ For a broad class of additive\nproperties, the PML plug-in estimator uses just four times the sample size\nrequired by the best estimator to achieve roughly twice its error, with\nexponentially higher confidence;\n  $\\boldsymbol{\\alpha}\\textbf{-R\\'enyi entropy estimation}$ For integer\n$\\alpha>1$, the PML plug-in estimator has optimal $k^{1-1/\\alpha}$ sample\ncomplexity; for non-integer $\\alpha>3/4$, the PML plug-in estimator has sample\ncomplexity lower than the state of the art;\n  $\\textbf{Identity testing}$ In testing whether an unknown distribution is\nequal to or at least $\\varepsilon$ far from a given distribution in $\\ell_1$\ndistance, a PML-based tester achieves the optimal sample complexity up to\nlogarithmic factors of $k$.\n  Most of these results also hold for a near-linear-time computable variant of\nPML. Stronger results hold for a different and novel variant called truncated\nPML (TPML).\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 04:59:45 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:54:50 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 06:01:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hao", "Yi", ""], ["Orlitsky", "Alon", ""]]}, {"id": "1906.03807", "submitter": "Miaoyan Wang", "authors": "Miaoyan Wang and Yuchen Zeng", "title": "Multiway clustering via tensor block models", "comments": "add the supplements", "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of identifying multiway block structure from a large\nnoisy tensor. Such problems arise frequently in applications such as genomics,\nrecommendation system, topic modeling, and sensor network localization. We\npropose a tensor block model, develop a unified least-square estimation, and\nobtain the theoretical accuracy guarantees for multiway clustering. The\nstatistical convergence of the estimator is established, and we show that the\nassociated clustering procedure achieves partition consistency. A sparse\nregularization is further developed for identifying important blocks with\nelevated means. The proposal handles a broad range of data types, including\nbinary, continuous, and hybrid observations. Through simulation and application\nto two real datasets, we demonstrate the outperformance of our approach over\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 06:07:41 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 22:10:42 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 06:14:01 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2021 23:25:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Miaoyan", ""], ["Zeng", "Yuchen", ""]]}, {"id": "1906.03866", "submitter": "David Rindt", "authors": "David Rindt, Dino Sejdinovic, David Steinsaltz", "title": "A kernel- and optimal transport- based test of independence between\n  covariates and right-censored lifetimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric test of independence, termed optHSIC, between a\ncovariate and a right-censored lifetime. Because the presence of censoring\ncreates a challenge in applying the standard permutation-based testing\napproaches, we use optimal transport to transform the censored dataset into an\nuncensored one, while preserving the relevant dependencies. We then apply a\npermutation test using the kernel-based dependence measure as a statistic to\nthe transformed dataset. The type 1 error is proven to be correct in the case\nwhere censoring is independent of the covariate. Experiments indicate that\noptHSIC has power against a much wider class of alternatives than Cox\nproportional hazards regression and that it has the correct type 1 control even\nin the challenging cases where censoring strongly depends on the covariate.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:32:57 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 15:02:36 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 13:58:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Rindt", "David", ""], ["Sejdinovic", "Dino", ""], ["Steinsaltz", "David", ""]]}, {"id": "1906.04125", "submitter": "Partha Hazarika", "authors": "S. Shah, S. Chakraborty and P. J. Hazarika", "title": "A New One Parameter Bimodal Skew Logistic Distribution and its\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper an attempt is made to develop a new bimodal alpha skew logistic\ndistribution under Balakrishnan (2002) mechanism. Some of its distributional as\nwell as moments properties are studied. Some extensions of this new\ndistribution are also briefly introduced. Finally, suitability of the proposed\ndistribution is checked with the help of data fitting examples of real life.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:01:29 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Shah", "S.", ""], ["Chakraborty", "S.", ""], ["Hazarika", "P. J.", ""]]}, {"id": "1906.04159", "submitter": "Cong Ma", "authors": "Yuxin Chen, Jianqing Fan, Cong Ma, Yuling Yan", "title": "Inference and Uncertainty Quantification for Noisy Matrix Completion", "comments": "published at Proceedings of the National Academy of Sciences Nov\n  2019, 116 (46) 22931-22937", "journal-ref": null, "doi": "10.1073/pnas.1910053116", "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy matrix completion aims at estimating a low-rank matrix given only\npartial and corrupted entries. Despite substantial progress in designing\nefficient estimation algorithms, it remains largely unclear how to assess the\nuncertainty of the obtained estimates and how to perform statistical inference\non the unknown matrix (e.g.~constructing a valid and short confidence interval\nfor an unseen entry).\n  This paper takes a step towards inference and uncertainty quantification for\nnoisy matrix completion. We develop a simple procedure to compensate for the\nbias of the widely used convex and nonconvex estimators. The resulting\nde-biased estimators admit nearly precise non-asymptotic distributional\ncharacterizations, which in turn enable optimal construction of confidence\nintervals\\,/\\,regions for, say, the missing entries and the low-rank factors.\nOur inferential procedures do not rely on sample splitting, thus avoiding\nunnecessary loss of data efficiency. As a byproduct, we obtain a sharp\ncharacterization of the estimation accuracy of our de-biased estimators, which,\nto the best of our knowledge, are the first tractable algorithms that provably\nachieve full statistical efficiency (including the preconstant). The analysis\nherein is built upon the intimate link between convex and nonconvex\noptimization --- an appealing feature recently discovered by\n\\cite{chen2019noisy}.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:57:03 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 14:37:59 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Yan", "Yuling", ""]]}, {"id": "1906.04175", "submitter": "Mariusz Kubkowski", "authors": "Mariusz Kubkowski, Jan Mielniczuk", "title": "Selection consistency of Lasso-based procedures for misspecified\n  high-dimensional binary model and random regressors", "comments": null, "journal-ref": null, "doi": "10.3390/e22020153", "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider selection of random predictors for high-dimensional regression\nproblem with binary response for a general loss function. Important special\ncase is when the binary model is semiparametric and the response function is\nmisspecified under parametric model fit. Selection for such a scenario aims at\nrecovering the support of the minimizer of the associated risk with large\nprobability. We propose a two-step selection procedure which consists of\nscreening and ordering predictors by Lasso method and then selecting a subset\nof predictors which minimizes Generalized Information Criterion on the\ncorresponding nested family of models. We prove consistency of the selection\nmethod under conditions which allow for much larger number of predictors than\nnumber of observations. For the semiparametric case when distribution of random\npredictors satisfies linear regression conditions the true and the estimated\nparameters are collinear and their common support can be consistently\nidentified.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 15:25:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kubkowski", "Mariusz", ""], ["Mielniczuk", "Jan", ""]]}, {"id": "1906.04280", "submitter": "Gabor Lugosi", "authors": "Gabor Lugosi and Shahar Mendelson", "title": "Mean estimation and regression under heavy-tailed distributions--a\n  survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey some of the recent advances in mean estimation and regression\nfunction estimation. In particular, we describe sub-Gaussian mean estimators\nfor possibly heavy-tailed data both in the univariate and multivariate\nsettings. We focus on estimators based on median-of-means techniques but other\nmethods such as the trimmed mean and Catoni's estimator are also reviewed. We\ngive detailed proofs for the cornerstone results. We dedicate a section on\nstatistical learning problems--in particular, regression function\nestimation--in the presence of possibly heavy-tailed data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 21:25:55 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Lugosi", "Gabor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1906.04396", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Venkata K Jandhyala, Stergios B Fotopoulos", "title": "Detection and estimation of parameters in high dimensional multiple\n  change point regression models via $\\ell_1/\\ell_0$ regularization and\n  discrete optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary segmentation, which is sequential in nature is thus far the most\nwidely used method for identifying multiple change points in statistical\nmodels. Here we propose a top down methodology called arbitrary segmentation\nthat proceeds in a conceptually reverse manner. We begin with an arbitrary\nsuperset of the parametric space of the change points, and locate unknown\nchange points by suitably filtering this space down. Critically, we reframe the\nproblem as that of variable selection in the change point parameters, this\nenables the filtering down process to be achieved in a single step with the aid\nof an $\\ell_0$ regularization, thus avoiding the sequentiality of binary\nsegmentation. We study this method under a high dimensional multiple change\npoint linear regression model and show that rates convergence of the error in\nthe regression and change point estimates are near optimal. We propose a\nsimulated annealing (SA) approach to implement a key finite state space\ndiscrete optimization that arises in our method. Theoretical results are\nnumerically supported via simulations. The proposed method is shown to possess\nthe ability to agnostically detect the `no change' scenario. Furthermore, its\ncomputational complexity is of order $O(Np^2)$+SA, where SA is the cost of a SA\noptimization on a $N$(no. of change points) dimensional grid. Thus, the\nproposed methodology is significantly more computationally efficient than\nexisting approaches. Finally, our theoretical results are obtained under weaker\nmodel conditions than those assumed in the current literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 05:15:24 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kaul", "Abhishek", ""], ["Jandhyala", "Venkata K", ""], ["Fotopoulos", "Stergios B", ""]]}, {"id": "1906.04544", "submitter": "Jasha Sommer-Simpson", "authors": "Jasha Sommer-Simpson", "title": "Convergence of D\\\"umbgen's Algorithm for Estimation of Tail Inflation", "comments": "UChicago MS thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a density $f$ on the non-negative real line, D\\\"umbgen's algorithm is a\nroutine for finding the (unique) log-convex, non-decreasing function $\\hat\\phi$\nsuch that $\\int\\hat\\phi(x)f(x)dx=1$ and such that the likelihood\n$\\prod_{i=1}^{n}f(x_i)\\hat\\phi(x_i)$ of given data $x_1,\\ldots,x_n$ under\ndensity $x\\mapsto \\hat\\phi(x)f(x)$ is maximized. We summarize D\\\"umbgen's\nalgorithm for finding this MLE $\\hat\\phi$, and we present a novel guarantee of\nthe algorithm's termination and convergence.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 07:47:52 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Sommer-Simpson", "Jasha", ""]]}, {"id": "1906.04607", "submitter": "Pierre L'Ecuyer", "authors": "Pierre L'Ecuyer, Florian Puchhammer, Amal Ben Abdellah", "title": "Monte Carlo and Quasi-Monte Carlo Density Estimation via Conditioning", "comments": "Main manuscript: 34 pages, 6 figures, 5 tables. Supplement: 11 pages,\n  5 figures, 5 tables. We are very thankful to the anonymous referees, whose\n  comments were considered in this submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the unknown density from which a given independent sample\noriginates is more difficult than estimating the mean, in the sense that for\nthe best popular density estimators, the mean integrated square error converges\nmore slowly than at the canonical rate of $\\mathcal{O}(1/n)$. When the sample\nis generated from a simulation model and we have control over how this is done,\nwe can do better. We examine an approach in which conditional Monte Carlo\npermits one to obtain a smooth estimator of the cumulative distribution\nfunction, whose sample derivative is, under certain conditions, an unbiased\nestimator of the density at any point, and therefore converges at a faster rate\nthan the usual density estimators. We can achieve an even faster rate by\ncombining this with randomized quasi-Monte Carlo to generate the samples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:56:18 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 19:32:55 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 06:28:16 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 21:17:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["L'Ecuyer", "Pierre", ""], ["Puchhammer", "Florian", ""], ["Abdellah", "Amal Ben", ""]]}, {"id": "1906.04661", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Rong Ge, David Woodruff", "title": "Faster Algorithms for High-Dimensional Robust Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the covariance matrix of a\nhigh-dimensional distribution when a small constant fraction of the samples can\nbe arbitrarily corrupted. Recent work gave the first polynomial time algorithms\nfor this problem with near-optimal error guarantees for several natural\nstructured distributions. Our main contribution is to develop faster algorithms\nfor this problem whose running time nearly matches that of computing the\nempirical covariance.\n  Given $N = \\tilde{\\Omega}(d^2/\\epsilon^2)$ samples from a $d$-dimensional\nGaussian distribution, an $\\epsilon$-fraction of which may be arbitrarily\ncorrupted, our algorithm runs in time\n$\\tilde{O}(d^{3.26})/\\mathrm{poly}(\\epsilon)$ and approximates the unknown\ncovariance matrix to optimal error up to a logarithmic factor. Previous robust\nalgorithms with comparable error guarantees all have runtimes\n$\\tilde{\\Omega}(d^{2 \\omega})$ when $\\epsilon = \\Omega(1)$, where $\\omega$ is\nthe exponent of matrix multiplication. We also provide evidence that improving\nthe running time of our algorithm may require new algorithmic techniques.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:41:44 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Ge", "Rong", ""], ["Woodruff", "David", ""]]}, {"id": "1906.04709", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Themis Gouleakis and Daniel M. Kane and\n  Sankeerth Rao", "title": "Communication and Memory Efficient Testing of Discrete Distributions", "comments": "Full version of COLT 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distribution testing with communication and memory constraints in\nthe following computational models: (1) The {\\em one-pass streaming model}\nwhere the goal is to minimize the sample complexity of the protocol subject to\na memory constraint, and (2) A {\\em distributed model} where the data samples\nreside at multiple machines and the goal is to minimize the communication cost\nof the protocol. In both these models, we provide efficient algorithms for\nuniformity/identity testing (goodness of fit) and closeness testing (two sample\ntesting). Moreover, we show nearly-tight lower bounds on (1) the sample\ncomplexity of any one-pass streaming tester for uniformity, subject to the\nmemory constraint, and (2) the communication cost of any uniformity testing\nprotocol, in a restricted `one-pass' model of communication.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:26:21 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Kane", "Daniel M.", ""], ["Rao", "Sankeerth", ""]]}, {"id": "1906.04735", "submitter": "Florent Krzakala", "authors": "Alia Abbara, Antoine Baker, Florent Krzakala and Lenka Zdeborov\\'a", "title": "On the Universality of Noiseless Linear Estimation with Respect to the\n  Measurement Matrix", "comments": "13 pages, 4 figures", "journal-ref": "Journal of Physics A: Mathematical and Theoretical (2019)", "doi": "10.1088/1751-8121/ab59ef", "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a noiseless linear estimation problem, one aims to reconstruct a vector x*\nfrom the knowledge of its linear projections y=Phi x*. There have been many\ntheoretical works concentrating on the case where the matrix Phi is a random\ni.i.d. one, but a number of heuristic evidence suggests that many of these\nresults are universal and extend well beyond this restricted case. Here we\nrevisit this problematic through the prism of development of message passing\nmethods, and consider not only the universality of the l1 transition, as\npreviously addressed, but also the one of the optimal Bayesian reconstruction.\nWe observed that the universality extends to the Bayes-optimal minimum\nmean-squared (MMSE) error, and to a range of structured matrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:10:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Abbara", "Alia", ""], ["Baker", "Antoine", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1906.04785", "submitter": "Robert Gaunt", "authors": "Robert E. Gaunt", "title": "Stein's method and the distribution of the product of zero mean\n  correlated normal random variables", "comments": "6 pages. To appear in Communications in Statistics - Theory and\n  Methods", "journal-ref": "Communications in Statistics - Theory and Methods 50 (2021), pp.\n  280-285", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last 80 years there has been much interest in the problem of finding\nan explicit formula for the probability density function of two zero mean\ncorrelated normal random variables. Motivated by this historical interest, we\nuse a recent technique from the Stein's method literature to obtain a simple\nnew proof, which also serves as an exposition of a general method that may be\nuseful in related problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 19:42:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gaunt", "Robert E.", ""]]}, {"id": "1906.04863", "submitter": "Kimon Fountoulakis", "authors": "Wooseok Ha and Kimon Fountoulakis and Michael W. Mahoney", "title": "Statistical guarantees for local graph clustering", "comments": "52 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local graph clustering methods aim to find small clusters in very large\ngraphs. These methods take as input a graph and a seed node, and they return as\noutput a good cluster in a running time that depends on the size of the output\ncluster but that is independent of the size of the input graph. In this paper,\nwe adopt a statistical perspective on local graph clustering, and we analyze\nthe performance of the l1-regularized PageRank method~(Fountoulakis et. al.)\nfor the recovery of a single target cluster, given a seed node inside the\ncluster. Assuming the target cluster has been generated by a random model, we\npresent two results. In the first, we show that the optimal support of\nl1-regularized PageRank recovers the full target cluster, with bounded false\npositives. In the second, we show that if the seed node is connected solely to\nthe target cluster then the optimal support of l1-regularized PageRank recovers\nexactly the target cluster. We also show empirically that l1-regularized\nPageRank has a state-of-the-art performance on many real graphs, demonstrating\nthe superiority of the method. From a computational perspective, we show that\nthe solution path of l1-regularized PageRank is monotonic. This allows for the\napplication of the forward stagewise algorithm, which approximates the solution\npath in running time that does not depend on the size of the whole graph.\nFinally, we show that l1-regularized PageRank and approximate personalized\nPageRank (APPR), another very popular method for local graph clustering, are\nequivalent in the sense that we can lower and upper bound the output of one\nwith the output of the other. Based on this relation, we establish for APPR\nsimilar results to those we establish for l1-regularized PageRank.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 23:40:32 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 04:29:17 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 02:51:57 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ha", "Wooseok", ""], ["Fountoulakis", "Kimon", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1906.05014", "submitter": "Nikita Puchkin", "authors": "Nikita Puchkin and Vladimir Spokoiny", "title": "Structure-adaptive manifold estimation", "comments": "53 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of manifold estimation from noisy observations. Many\nmanifold learning procedures locally approximate a manifold by a weighted\naverage over a small neighborhood. However, in the presence of large noise, the\nassigned weights become so corrupted that the averaged estimate shows very poor\nperformance. We suggest a novel computationally efficient structure-adaptive\nprocedure which simultaneously reconstructs a smooth manifold and estimates\nprojections of the point cloud onto this manifold. The proposed approach\niteratively refines the weights on each step, using the structural information\nobtained at previous steps. After several iterations, we obtain nearly \"oracle\"\nweights, so that the final estimates are nearly efficient even in the presence\nof relatively large noise. In our theoretical study we establish tight lower\nand upper bounds proving asymptotic optimality of the method for manifold\nestimation under the Hausdorff loss, provided that the noise degrades to zero\nfast enough.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 09:10:59 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 10:44:03 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 09:29:38 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 02:38:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Puchkin", "Nikita", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1906.05082", "submitter": "Mohamed Hebiri", "authors": "Evgenii Chzhen (LAMA, LMO, CELESTE), Christophe Denis (LAMA), Mohamed\n  Hebiri (LAMA), Luca Oneto, Massimiliano Pontil (IIT, UCL)", "title": "Leveraging Labeled and Unlabeled Data for Consistent Fair Binary\n  Classification", "comments": null, "journal-ref": "NeurIPS 2019 - 33th Annual Conference on Neural Information\n  Processing Systems, Dec 2019, Vancouver, Canada", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of fair binary classification using the notion of Equal\nOpportunity. It requires the true positive rate to distribute equally across\nthe sensitive groups. Within this setting we show that the fair optimal\nclassifier is obtained by recalibrating the Bayes classifier by a\ngroup-dependent threshold. We provide a constructive expression for the\nthreshold. This result motivates us to devise a plug-in classification\nprocedure based on both unlabeled and labeled datasets. While the latter is\nused to learn the output conditional probability, the former is used for\ncalibration. The overall procedure can be computed in polynomial time and it is\nshown to be statistically consistent both in terms of the classification error\nand fairness measure. Finally, we present numerical experiments which indicate\nthat our method is often superior or competitive with the state-of-the-art\nmethods on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:25:25 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 07:30:31 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chzhen", "Evgenii", "", "LAMA, LMO, CELESTE"], ["Denis", "Christophe", "", "LAMA"], ["Hebiri", "Mohamed", "", "LAMA"], ["Oneto", "Luca", "", "IIT, UCL"], ["Pontil", "Massimiliano", "", "IIT, UCL"]]}, {"id": "1906.05098", "submitter": "Xiaowei Zhang", "authors": "Liang Ding, L. Jeff Hong, Haihui Shen, Xiaowei Zhang", "title": "Knowledge Gradient for Selection with Covariates: Consistency and\n  Computation", "comments": "40 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge gradient is a design principle for developing Bayesian sequential\nsampling policies to solve optimization problems. In this paper we consider the\nranking and selection problem in the presence of covariates, where the best\nalternative is not universal but depends on the covariates. In this context, we\nprove that under minimal assumptions, the sampling policy based on knowledge\ngradient is consistent, in the sense that following the policy the best\nalternative as a function of the covariates will be identified almost surely as\nthe number of samples grows. We also propose a stochastic gradient ascent\nalgorithm for computing the sampling policy and demonstrate its performance via\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:53:55 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 12:44:56 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 14:13:43 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 06:50:40 GMT"}, {"version": "v5", "created": "Wed, 23 Jun 2021 14:35:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ding", "Liang", ""], ["Hong", "L. Jeff", ""], ["Shen", "Haihui", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1906.05316", "submitter": "Martin Bladt", "authors": "Hansjoerg Albrecher, Martin Bladt, Mogens Bladt", "title": "Matrix Mittag--Leffler distributions and modeling heavy-tailed risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define the class of matrix Mittag-Leffler distributions and\nstudy some of its properties. We show that it can be interpreted as a\nparticular case of an inhomogeneous phase-type distribution with random scaling\nfactor, and alternatively also as the absorption time of a semi-Markov process\nwith Mittag-Leffler distributed interarrival times. We then identify this class\nand its power transforms as a remarkably parsimonious and versatile family for\nthe modelling of heavy-tailed risks, which overcomes some disadvantages of\nother approaches like the problem of threshold selection in extreme value\ntheory. We illustrate this point both on simulated data as well as on a set of\nreal-life MTPL insurance data that were modeled differently in the past.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 18:09:19 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 10:26:29 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Bladt", "Martin", ""], ["Bladt", "Mogens", ""]]}, {"id": "1906.05461", "submitter": "Yo Sheena", "authors": "Yo Sheena", "title": "Efficiency of maximum likelihood estimation for a multinomial\n  distribution with known probability sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a multinomial distribution, suppose that we have prior knowledge of the\nsum of the probabilities of some categories. This allows us to construct a\nsubmodel in a full (i.e., no-restriction) model. Maximum likelihood estimation\n(MLE) under this submodel is expected to have better estimation efficiency than\nMLE under the full model. This article presents the asymptotic expansion of the\nrisk of MLE with respect to Kullback--Leibler divergence for both the full\nmodel and submodel. The results reveal that, using the submodel, the reduction\nof the risk is quite small in some cases. Furthermore, when the sample size is\nsmall, the use of the subomodel can increase the risk.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 02:53:01 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 01:30:35 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sheena", "Yo", ""]]}, {"id": "1906.05545", "submitter": "Maurizio Daniele", "authors": "Maurizio Daniele, Winfried Pohlmeier, Aygul Zagidullina", "title": "Sparse Approximate Factor Estimation for High-Dimensional Covariance\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST q-fin.PM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimation approach for the covariance matrix based on the\n$l_1$-regularized approximate factor model. Our sparse approximate factor (SAF)\ncovariance estimator allows for the existence of weak factors and hence relaxes\nthe pervasiveness assumption generally adopted for the standard approximate\nfactor model. We prove consistency of the covariance matrix estimator under the\nFrobenius norm as well as the consistency of the factor loadings and the\nfactors.\n  Our Monte Carlo simulations reveal that the SAF covariance estimator has\nsuperior properties in finite samples for low and high dimensions and different\ndesigns of the covariance matrix. Moreover, in an out-of-sample portfolio\nforecasting application the estimator uniformly outperforms alternative\nportfolio strategies based on alternative covariance estimation approaches and\nmodeling strategies including the $1/N$-strategy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:29:32 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Daniele", "Maurizio", ""], ["Pohlmeier", "Winfried", ""], ["Zagidullina", "Aygul", ""]]}, {"id": "1906.05566", "submitter": "Ghislaine Gayraud", "authors": "V Barbu, Ghislaine Gayraud, N. Limnios (LMAC), I. Votsi (LMM)", "title": "Hypotheses testing and posterior concentration rates for semi-Markov\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adopt a nonparametric Bayesian approach and investigate the\nasymptotic behavior of the posterior distribution in continuous time and\ngeneral state space semi-Markov processes. In particular, we obtain posterior\nconcentration rates for semi-Markov kernels. For the purposes of this study, we\nconstruct robust statistical tests between Hellinger balls around semi-Markov\nkernels and present some specifications to particular cases, including\ndiscrete-time semi-Markov processes and finite state space Markov processes.\nThe objective of this paper is to provide sufficient conditions on priors and\nsemi-Markov kernels that enable us to establish posterior concentration rates.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:34:25 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Barbu", "V", "", "LMAC"], ["Gayraud", "Ghislaine", "", "LMAC"], ["Limnios", "N.", "", "LMAC"], ["Votsi", "I.", "", "LMM"]]}, {"id": "1906.05944", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Alessandro Barp, Andrew B. Duncan, Mark\n  Girolami", "title": "Statistical Inference for Generative Models with Maximum Mean\n  Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While likelihood-based inference and its variants provide a statistically\nefficient and widely applicable approach to parametric inference, their\napplication to models involving intractable likelihoods poses challenges. In\nthis work, we study a class of minimum distance estimators for intractable\ngenerative models, that is, statistical models for which the likelihood is\nintractable, but simulation is cheap. The distance considered, maximum mean\ndiscrepancy (MMD), is defined through the embedding of probability measures\ninto a reproducing kernel Hilbert space. We study the theoretical properties of\nthese estimators, showing that they are consistent, asymptotically normal and\nrobust to model misspecification. A main advantage of these estimators is the\nflexibility offered by the choice of kernel, which can be used to trade-off\nstatistical efficiency and robustness. On the algorithmic side, we study the\ngeometry induced by MMD on the parameter space and use this to introduce a\nnovel natural gradient descent-like algorithm for efficient implementation of\nthese estimators. We illustrate the relevance of our theoretical results on\nseveral classes of models including a discrete-time latent Markov process and\ntwo multivariate stochastic differential equation models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:53:55 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Barp", "Alessandro", ""], ["Duncan", "Andrew B.", ""], ["Girolami", "Mark", ""]]}, {"id": "1906.05951", "submitter": "Victoria Zinde-Walsh", "authors": "Jean-Marie Dufour, Eric Renault and Victoria Zinde-Walsh", "title": "A technical note on divergence of the Wald statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wald test statistic has been shown to diverge (Dufour et al, 2013, 2017)\nunder some conditions. This note links the divergence to eigenvalues of a\npolynomial matrix and establishes the divergence rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:30:32 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Dufour", "Jean-Marie", ""], ["Renault", "Eric", ""], ["Zinde-Walsh", "Victoria", ""]]}, {"id": "1906.06122", "submitter": "Debabrota Basu", "authors": "Naheed Anjum Arafat, Debabrota Basu, St\\'ephane Bressan", "title": "Topological Data Analysis with $\\epsilon$-net Induced Lazy Witness\n  Complex", "comments": "16 pages, 7 figures, Accepted in DEXA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Topological data analysis computes and analyses topological features of the\npoint clouds by constructing and studying a simplicial representation of the\nunderlying topological structure. The enthusiasm that followed the initial\nsuccesses of topological data analysis was curbed by the computational cost of\nconstructing such simplicial representations. The lazy witness complex is a\ncomputationally feasible approximation of the underlying topological structure\nof a point cloud. It is built in reference to a subset of points, called\nlandmarks, rather than considering all the points as in the \\v{C}ech and\nVietoris-Rips complexes. The choice and the number of landmarks dictate the\neffectiveness and efficiency of the approximation. We adopt the notion of\n$\\epsilon$-cover to define $\\epsilon$-net. We prove that $\\epsilon$-net, as a\nchoice of landmarks, is an $\\epsilon$-approximate representation of the point\ncloud and the induced lazy witness complex is a $3$-approximation of the\ninduced Vietoris-Rips complex. Furthermore, we propose three algorithms to\nconstruct $\\epsilon$-net landmarks. We establish the relationship of these\nalgorithms with the existing landmark selection algorithms. We empirically\nvalidate our theoretical claims. We empirically and comparatively evaluate the\neffectiveness, efficiency, and stability of the proposed algorithms on\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 10:51:33 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 19:38:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Arafat", "Naheed Anjum", ""], ["Basu", "Debabrota", ""], ["Bressan", "St\u00e9phane", ""]]}, {"id": "1906.06197", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu and Samuel Livingstone", "title": "Peskun-Tierney ordering for Markov chain and process Monte Carlo: beyond\n  the reversible scenario", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically time-reversibility of the transitions or processes underpinning\nMarkov chain Monte Carlo methods (MCMC) has played a key r\\^ole in their\ndevelopment, while the self-adjointness of associated operators together with\nthe use of classical functional analysis techniques on Hilbert spaces have led\nto powerful and practically successful tools to characterize and compare their\nperformance. Similar results for algorithms relying on nonreversible Markov\nprocesses are scarce. We show that for a type of nonreversible Monte Carlo\nMarkov chains and processes, of current or renewed interest in the Physics and\nStatistical literatures, it is possible to develop comparison results which\nclosely mirror those available in the reversible scenario. We show that these\nresults shed light on earlier literature, proving some conjectures and\nstrengthening some earlier results.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:37:15 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Andrieu", "Christophe", ""], ["Livingstone", "Samuel", ""]]}, {"id": "1906.06276", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini", "title": "Spectrally-truncated kernel ridge regression and its free lunch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression (KRR) is a well-known and popular nonparametric\nregression approach with many desirable properties, including minimax\nrate-optimality in estimating functions that belong to common reproducing\nkernel Hilbert spaces (RKHS). The approach, however, is computationally\nintensive for large data sets, due to the need to operate on a dense $n \\times\nn$ kernel matrix, where $n$ is the sample size. Recently, various approximation\nschemes for solving KRR have been considered, and some analyzed. Some\napproaches such as Nystr\\\"{o}m approximation and sketching have been shown to\npreserve the rate optimality of KRR. In this paper, we consider the simplest\napproximation, namely, spectrally truncating the kernel matrix to its largest\n$r < n$ eigenvalues. We derive an exact expression for the maximum risk of this\ntruncated KRR, over the unit ball of the RKHS. This result can be used to study\nthe exact trade-off between the level of spectral truncation and the\nregularization parameter. We show that, as long as the RKHS is\ninfinite-dimensional, there is a threshold on $r$, above which, the\nspectrally-truncated KRR surprisingly outperforms the full KRR in terms of the\nminimax risk, where the minimum is taken over the regularization parameter.\nThis strengthens the existing results on approximation schemes, by showing that\nnot only one does not lose in terms of the rates, truncation can in fact\nimprove the performance, for all finite samples (above the threshold).\nMoreover, we show that the implicit regularization achieved by spectral\ntruncation is not a substitute for Hilbert norm regularization. Both are needed\nto achieve the best performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 16:29:25 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 18:11:38 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Amini", "Arash A.", ""]]}, {"id": "1906.06513", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Bayesian Network Models for Incomplete and Dynamic Data", "comments": "24 pages, 4 figures", "journal-ref": "Statistica Neerlandica (2020), 74(3), 397-419", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a versatile and powerful tool to model complex\nphenomena and the interplay of their components in a probabilistically\nprincipled way. Moving beyond the comparatively simple case of completely\nobserved, static data, which has received the most attention in the literature,\nin this paper we will review how Bayesian networks can model dynamic data and\ndata with incomplete observations. Such data are the norm at the forefront of\nresearch and in practical applications, and Bayesian networks are uniquely\npositioned to model them due to their explainability and interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 09:59:04 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 12:17:33 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1906.06713", "submitter": "Zhigang Yao", "authors": "Yan Liu, Zhiqiang Hou, Zhigang Yao, Zhidong Bai, Jiang Hu, Shurong\n  Zheng", "title": "Community Detection Based on the $L_\\infty$ convergence of eigenvectors\n  in DCBM", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular algorithms for community\ndetection in network analysis. Based on this rationale, in this paper we give\nthe convergence rate of eigenvectors for the adjacency matrix in the $l_\\infty$\nnorm, under the stochastic block model (BM) and degree corrected stochastic\nblock model (DCBM), adding some mild and rational conditions. We also extend\nthis result to a more general model, presented based on the DCBM such that the\nvalue of random variables in the adjacency matrix is not 0 or 1, but an\narbitrary real number. During the process of proving the above conclusion, we\nobtain the relationship of the eigenvalues in the adjacency matrix and the\ncorresponding `population' matrix, which vary in dimension from the\ncommunity-wise edge probability matrix. Using that result, we can give an\nestimate of the number of the communities in a known set of network data.\nMeanwhile we proved the consistency of the estimator. Furthermore, according to\nthe derivation of proof for the convergence of eigenvectors, we propose a new\napproach to community detection -- Spectral Clustering based on Difference of\nRatios of Eigenvectors (SCDRE). Our simulation experiments demonstrate the\nsuperiority of our method in community detection.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 15:01:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liu", "Yan", ""], ["Hou", "Zhiqiang", ""], ["Yao", "Zhigang", ""], ["Bai", "Zhidong", ""], ["Hu", "Jiang", ""], ["Zheng", "Shurong", ""]]}, {"id": "1906.06749", "submitter": "David Jones", "authors": "David E. Jones and Xiao-Li Meng", "title": "Designing Test Information and Test Information in Design", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeGroot (1962) developed a general framework for constructing Bayesian\nmeasures of the expected information that an experiment will provide for\nestimation. We propose an analogous framework for measures of information for\nhypothesis testing. In contrast to estimation information measures that are\ntypically used for surface estimation, test information measures are more\nuseful in experimental design for hypothesis testing and model selection. In\nparticular, we obtain a probability based measure, which has more appealing\nproperties than variance based measures in design contexts where decision\nproblems are of interest. The underlying intuition of our design proposals is\nstraightforward: to distinguish between models we should collect data from\nregions of the covariate space for which the models differ most. Nicolae et al.\n(2008) gave an asymptotic equivalence between their test information measures\nand Fisher information. We extend this result to all test information measures\nunder our framework. Simulation studies and an application in astronomy\ndemonstrate the utility of our approach, and provide comparison to other\nmethods including that of Box and Hill (1967).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:49:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Jones", "David E.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1906.06776", "submitter": "Yudong Chen", "authors": "Wei Qian, Yuqian Zhang, Yudong Chen", "title": "Global Convergence of Least Squares EM for Demixing Two Log-Concave\n  Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the location estimation problem for a mixture of two\nrotation invariant log-concave densities. We demonstrate that Least Squares EM,\na variant of the EM algorithm, converges to the true location parameter from a\nrandomly initialized point. We establish the explicit convergence rates and\nsample complexity bounds, revealing their dependence on the signal-to-noise\nratio and the tail property of the log-concave distribution. Moreover, we show\nthat this global convergence property is robust under model mis-specification.\n  Our analysis generalizes previous techniques for proving the convergence\nresults for Gaussian mixtures. In particular, we make use of an\nangle-decreasing property for establishing global convergence of Least Squares\nEM beyond Gaussian settings, as $\\ell_2$ distance contraction no longer holds\nglobally for general log-concave mixtures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 21:26:17 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 21:43:46 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Qian", "Wei", ""], ["Zhang", "Yuqian", ""], ["Chen", "Yudong", ""]]}, {"id": "1906.06835", "submitter": "Athanasios Georgiadis", "authors": "Galatia Cleanthous, Athanasios G. Georgiadis and Emilio Porcu", "title": "Minimax Density Estimation on Sobolev Spaces With Dominating Mixed\n  Smoothness", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax density estimation on the product space\n$\\mathbb{R}^{d_1}\\times\\mathbb{R}^{d_2}$. We consider $L^p$-risk for\nprobability density functions defined over regularity spaces that allow for\ndifferent level of smoothness in each of the variables. Precisely, we study\nprobabilities on Sobolev spaces with dominating mixed-smoothness. We provide\nthe rate of convergence that is optimal even for the classical Sobolev spaces.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 04:14:12 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Cleanthous", "Galatia", ""], ["Georgiadis", "Athanasios G.", ""], ["Porcu", "Emilio", ""]]}, {"id": "1906.07120", "submitter": "Bj\\\"orn Sprungk", "authors": "Bj\\\"orn Sprungk", "title": "On the Local Lipschitz Stability of Bayesian Inverse Problems", "comments": "28 pages", "journal-ref": null, "doi": "10.1088/1361-6420/ab6f43", "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider the stability of posterior measures occuring in\nBayesian inference w.r.t. perturbations of the prior measure and the\nlog-likelihood function. This extends the well-posedness analysis of Bayesian\ninverse problems. In particular, we prove a general local Lipschitz continuous\ndependence of the posterior on the prior and the log-likelihood w.r.t. various\ncommon distances of probability measures. These include the total variation,\nHellinger, and Wasserstein distance and the Kullback-Leibler divergence. We\nonly assume the boundedness of the likelihoods and measure their perturbations\nin an $L^p$-norm w.r.t. the prior. The obtained stability yields under mild\nassumptions the well-posedness of Bayesian inverse problems, in particular, a\nwell-posedness w.r.t. the Wasserstein distance. Moreover, our results indicate\nan increasing sensitivity of Bayesian inference as the posterior becomes more\nconcentrated, e.g., due to more or more accurate data. This confirms and\nextends previous observations made in the sensitivity analysis of Bayesian\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 16:41:14 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:38:53 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 07:51:46 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "1906.07186", "submitter": "Thomas Pitschel", "authors": "Thomas Pitschel", "title": "Efficient computation of the cumulative distribution function of a\n  linear mixture of independent random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a variant of the algorithm in [Pit19] (arXiv:1903.10816) to compute the\napproximate density or distribution function of a linear mixture of independent\nrandom variables known by a finite sample, it is presented a proof of the\nfunctional correctness, i.e. the convergence of the computed distribution\nfunction towards the true distribution function (given the observations) as the\nalgorithm resolution is increased to infinity. The algorithm (like its\npredecessor version) bears elements which are closely related to early known\nmethods for numerical inversion of the characteristic function of a probability\ndistribution, however here efficiently computes the complete distribution\nfunction. Possible applications are in computing the distribution of the\nbootstrap estimate in any linear bootstrap method (e.g. in the block bootstrap\nfor the mean as parameter of interest, or residual bootstrap in linear\nregression with fixed design), or in elementary analysis-of-variance hypothesis\ntesting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:45:56 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Pitschel", "Thomas", ""]]}, {"id": "1906.07265", "submitter": "Keith Levin", "authors": "Keith Levin and Asad Lodhia and Elizaveta Levina", "title": "Recovering shared structure from multiple networks with unknown edge\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In increasingly many settings, data sets consist of multiple samples from a\npopulation of networks, with vertices aligned across these networks. For\nexample, brain connectivity networks in neuroscience consist of measures of\ninteraction between brain regions that have been aligned to a common template.\nWe consider the setting where the observed networks have a shared expectation,\nbut may differ in the noise structure on their edges. Our approach exploits the\nshared mean structure to denoise edge-level measurements of the observed\nnetworks and estimate the underlying population-level parameters. We also\nexplore the extent to which edge-level errors influence estimation and\ndownstream inference. We establish a finite-sample concentration inequality for\nthe low-rank eigenvalue truncation of a random weighted adjacency matrix that\nmay be of independent interest. The proposed approach is illustrated on\nsynthetic networks and on data from an fMRI study of schizophrenia.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 01:30:27 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 20:46:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Levin", "Keith", ""], ["Lodhia", "Asad", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1906.07395", "submitter": "Ching-Kang Ing", "authors": "Ching-Kang Ing", "title": "Model selection for high-dimensional linear regression with dependent\n  observations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the prediction capability of the orthogonal greedy algorithm\n(OGA) in high-dimensional regression models with dependent observations. The\nrates of convergence of the prediction error of OGA are obtained under a\nvariety of sparsity conditions. To prevent OGA from overfitting, we introduce a\nhigh-dimensional Akaike's information criterion (HDAIC) to determine the number\nof OGA iterations. A key contribution of this work is to show that OGA, used in\nconjunction with HDAIC, can achieve the optimal convergence rate without\nknowledge of how sparse the underlying high-dimensional model is.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 06:17:30 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Ing", "Ching-Kang", ""]]}, {"id": "1906.07418", "submitter": "Xavier Pennec", "authors": "Xavier Pennec (ASCLEPIOS)", "title": "Curvature effects on the empirical mean in Riemannian and affine\n  Manifolds: a non-asymptotic high concentration expansion in the small-sample\n  regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic concentration of the Fr{\\'e}chet mean of IID random variables\non a Rieman-nian manifold was established with a central limit theorem by\nBhattacharya \\& Patrangenaru (BP-CLT) [6]. This asymptotic result shows that\nthe Fr{\\'e}chet mean behaves almost as the usual Euclidean case for\nsufficiently concentrated distributions. However, the asymptotic covariance\nmatrix of the empirical mean is modified by the expected Hessian of the squared\ndistance. This Hessian matrix was explicitly computed in [5] for constant\ncurvature spaces in order to relate it to the sectional curvature. Although\nexplicit, the formula remains quite difficult to interpret, and the intuitive\neffect of the curvature on the asymptotic convergence remains unclear.\nMoreover, we are most often interested in the mean of a finite sample of small\nsize in practice. In this work, we aim at understanding the effect of the\nmanifold curvature in this small sample regime. Last but not least, one would\nlike computable and interpretable approximations that can be extended from the\nempirical Fr{\\'e}chet mean in Rie-mannian manifolds to the empirical\nexponential barycenters in affine connection manifolds. For distributions that\nare highly concentrated around their mean, and for any finite number of\nsamples, we establish explicit Taylor expansions on the first and second moment\nof the empirical mean thanks to a new Taylor expansion of the Riemannian\nlog-map in affine connection spaces. This shows that the empirical mean has a\nbias in 1/n proportional to the gradient of the curvature tensor contracted\ntwice with the covariance matrix, and a modulation of the convergence rate of\nthe covariance matrix proportional to the covariance-curvature tensor. We show\nthat our non-asymptotic high concentration expansion is consistent with the\nasymptotic expansion of the BP-CLT. Experiments on constant curvature spaces\ndemonstrate that both expansions are very accurate in their domain of validity.\nMoreover, the modulation of the convergence rate of the empirical mean's\ncovariance matrix is explicitly encoded using a scalar multiplicative factor\nthat gives an intuitive vision of the impact of the curvature: the variance of\nthe empirical mean decreases faster than in the Euclidean case in negatively\ncurved space forms, with an infinite speed for an infinite negative curvature.\nThis suggests potential links with the stickiness of the Fr{\\'e}chet mean\ndescribed in stratified spaces. On the contrary, the variance of the empirical\nmean decreases more slowly than in the Euclidean case in positive curvature\nspace forms, with divergence when we approach the limits of the Karcher \\&\nKendall concentration conditions with a uniform distribution on the equator of\nthe sphere, for which the Fr{\\'e}chet mean is not a single point any more.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:42:29 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Pennec", "Xavier", "", "ASCLEPIOS"]]}, {"id": "1906.07424", "submitter": "Partha Hazarika", "authors": "P. J. Hazarika, S. Shah and S. Chakraborty", "title": "Balakrishnan Alpha Skew Normal Distribution: Properties and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new type of alpha skew distribution is proposed under\nBalakrishnan Mechanism and some of its related distributions are investigated.\nThe moments and distributional properties are also studied. Suitability of the\nproposed distribution is tested by conducting data fitting experiments and\nmodel adequacy is checked via AIC, BIC in comparison with some related\ndistributions. Likelihood ratio test is carried out to discriminate between\nnormal and proposed distribution.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:47:00 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Hazarika", "P. J.", ""], ["Shah", "S.", ""], ["Chakraborty", "S.", ""]]}, {"id": "1906.07514", "submitter": "Fumiyasu Komaki", "authors": "Michiko Okudo and Fumiyasu Komaki", "title": "Bayes Extended Estimators for Curved Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian predictive density has complex representation and does not\nbelong to any finite-dimensional statistical model except for in limited\nsituations. In this paper, we introduce its simple approximate representation\nemploying its projection onto a finite-dimensional exponential family. Its\ntheoretical properties are established parallelly to those of the Bayesian\npredictive density when the model belongs to curved exponential families. It is\nalso demonstrated that the projection asymptotically coincides with the plugin\ndensity with the posterior mean of the expectation parameter of the exponential\nfamily, which we refer to as the Bayes extended estimator.\nInformation-geometric correspondence indicates that the Bayesian predictive\ndensity can be represented as the posterior mean of the infinite-dimensional\nexponential family. The Kullback--Leibler risk performance of the approximation\nis demonstrated by numerical simulations and it indicates that the posterior\nmean of the expectation parameter approaches the Bayesian predictive density as\nthe dimension of the exponential family increases. It also suggests that\napproximation by projection onto an exponential family of reasonable size is\npractically advantageous with respect to risk performance and computational\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 12:04:52 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 08:01:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Okudo", "Michiko", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1906.07530", "submitter": "Pierre Druilhet", "authors": "Erwan Saint Loubert Bi\\'e (LMBP), Pierre Druilhet (LMBP), Erwan Saint,\n  Loubert Bi\\'e", "title": "Improper vs finitely additive distributions as limits of countably\n  additive probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian statistics, improper distributions and finitely additive\nprobabilities (FAPs) are the two main alternatives to proper distributions,\ni.e. countably additive probabilities. Both of them can be seen as limits of\nproper distribution sequences w.r.t. to some specific convergence modes.\nTherefore, some authors attempt to link these two notions by this means, partly\nusing heuristic arguments. The aim of the paper is to compare these two kinds\nof limits. We show that improper distributions and FAPs represent two distinct\ncharacteristics of a sequence of proper distributions and therefore,\nsurprisingly, cannot be connected by the mean of proper distribution sequences.\nMore specifically, for a sequence of proper distribution which converge to both\nan improper distribution and a set of FAPs, we show that another sequence of\nproper distributions can be constructed having the same FAP limits and\nconverging to any given improper distribution. This result can be mainly\nexplained by the fact that improper distributions describe the behavior of the\nsequence inside the domain after rescaling, whereas FAP limits describe how the\nmass concentrates on the boundary of the domain. We illustrate our results with\nseveral examples and we show the difficulty to define properly a uniform FAP\ndistribution on the natural numbers as an equivalent of the improper flat\nprior. MSC 2010 subject classifications: Primary 62F15; secondary 62E17,60B10.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 12:36:10 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 12:46:24 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Bi\u00e9", "Erwan Saint Loubert", "", "LMBP"], ["Druilhet", "Pierre", "", "LMBP"], ["Saint", "Erwan", "", "LMBP"], ["Bi\u00e9", "Loubert", "", "LMBP"]]}, {"id": "1906.07608", "submitter": "Christian Hirsch", "authors": "Christophe Ange Napol\\'eon Biscio, Nicolas Chenavier, Christian\n  Hirsch, Anne Marie Svane", "title": "Testing goodness of fit for point processes via topological data\n  analysis", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tests for the goodness of fit of point patterns via methods from\ntopological data analysis. More precisely, the persistent Betti numbers give\nrise to a bivariate functional summary statistic for observed point patterns\nthat is asymptotically Gaussian in large observation windows. We analyze the\npower of tests derived from this statistic on simulated point patterns and\ncompare its performance with global envelope tests. Finally, we apply the tests\nto a point pattern from an application context in neuroscience. As the main\nmethodological contribution, we derive sufficient conditions for a functional\ncentral limit theorem on bounded persistent Betti numbers of point processes\nwith exponential decay of correlations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 14:29:40 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Biscio", "Christophe Ange Napol\u00e9on", ""], ["Chenavier", "Nicolas", ""], ["Hirsch", "Christian", ""], ["Svane", "Anne Marie", ""]]}, {"id": "1906.07695", "submitter": "Fabien Navarro", "authors": "Christophe Chesneau, Salima El Kolei, Junke Kou, Fabien Navarro", "title": "Nonparametric estimation in a regression model with additive and\n  multiplicative noise", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, Volume 380, 2020", "doi": "10.1016/j.cam.2020.112971", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an unknown functional estimation problem in a\ngeneral nonparametric regression model with the feature of having both\nmultiplicative and additive noise.We propose two new wavelet estimators in this\ngeneral context. We prove that they achieve fast convergence rates under the\nmean integrated square error over Besov spaces. The obtained rates have the\nparticularity of being established under weak conditions on the model. A\nnumerical study in a context comparable to stochastic frontier estimation (with\nthe difference that the boundary is not necessarily a production function)\nsupports the theory.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 17:16:32 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 08:16:53 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Chesneau", "Christophe", ""], ["Kolei", "Salima El", ""], ["Kou", "Junke", ""], ["Navarro", "Fabien", ""]]}, {"id": "1906.07801", "submitter": "Rianne de Heide", "authors": "Peter Gr\\\"unwald, Rianne de Heide, and Wouter Koolen", "title": "Safe Testing", "comments": "Preliminary version, not yet submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the theory of hypothesis testing based on the e-value, a notion of\nevidence that, unlike the p-value, allows for effortlessly combining results\nfrom several tests. Even in the common scenario of optional continuation, where\nthe decision to perform a new test depends on previous test outcomes, 'safe'\ntests based on e-values generally preserve Type-I error guarantees. Our main\nresult shows that e-values exist for completely general testing problems with\ncomposite null and alternatives. Their prime interpretation is in terms of\ngambling or investing, each e-value corresponding to a particular investment.\nSurprisingly, optimal 'GROW' e-values, which lead to fastest capital growth,\nare fully characterized by the joint information projection (JIPr) between the\nset of all Bayes marginal distributions on H0 and H1. Thus, optimal e-values\nalso have an interpretation as Bayes factors, with priors given by the JIPr. We\nillustrate the theory using several 'classic' examples including a one-sample\nsafe t-test and the 2 x 2 contingency table. Sharing Fisherian, Neymanian and\nJeffreys-Bayesian interpretations, e-values and safe tests may provide a\nmethodology acceptable to adherents of all three schools.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:39:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 08:38:35 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["de Heide", "Rianne", ""], ["Koolen", "Wouter", ""]]}, {"id": "1906.07972", "submitter": "J\\\"urgen Kampf", "authors": "J\\\"urgen Kampf", "title": "Variances of surface area estimators based on pixel configuration counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surface area of a set which is only observed as a binary pixel image is\noften estimated by a weighted sum of pixel configurations counts. In this paper\nwe examine these estimators in a design based setting -- we assume that the\nobserved set is shifted uniformly randomly. Bounds for the difference between\nthe essential supremum and the essential infimum of such an estimator are\nderived, which imply that the variance is in $O(t^2)$ as the lattice distance\n$t$ tends to zero. In particular, it is asymptotically neglectable compared to\nthe bias. A simulation study shows that the theoretically derived convergence\norder is optimal in general, but further improvements are possible in special\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 08:41:24 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Kampf", "J\u00fcrgen", ""]]}, {"id": "1906.08017", "submitter": "Frank Werner", "authors": "Farida Enikeeva, Axel Munk, Markus Pohlmann and Frank Werner", "title": "Bump detection in the presence of dependency: Does it ease or does it\n  load?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the asymptotic minimax detection boundary for a bump, i.e. an\nabrupt change, in the mean function of a stationary Gaussian process. This will\nbe characterized in terms of the asymptotic behavior of the bump length and\nheight as well as the dependency structure of the process. A major finding is\nthat the asymptotic minimax detection boundary is generically determined by the\nvalue of its spectral density at zero. Finally, our asymptotic analysis is\ncomplemented by non-asymptotic results for AR($p$) processes and confirmed to\nserve as a good proxy for finite sample scenarios in a simulation study. Our\nproofs are based on laws of large numbers for non-independent and\nnon-identically distributed arrays of random variables and the asymptotically\nsharp analysis of the precision matrix of the process.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 10:42:38 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 11:09:30 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 07:01:11 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Enikeeva", "Farida", ""], ["Munk", "Axel", ""], ["Pohlmann", "Markus", ""], ["Werner", "Frank", ""]]}, {"id": "1906.08062", "submitter": "Fabian Mies", "authors": "Fabian Mies", "title": "Rate-optimal estimation of the Blumenthal-Getoor index of a L\\'evy\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Blumenthal-Getoor (BG) index characterizes the jump measure of an\ninfinitely active L\\'evy process. It determines sample path properties and\naffects the behavior of various econometric procedures. If the process contains\na diffusion term, existing estimators of the BG index based on high-frequency\nobservations only achieve rates of convergence which are suboptimal by a\npolynomial factor. In this paper, a novel estimator for the BG index and the\nsuccessive BG indices is presented, attaining the optimal rate of convergence.\nIf an additional proportionality factor needs to be inferred, the proposed\nestimator is rate-optimal up to logarithmic factors. Furthermore, our method\nyields a new efficient volatility estimator which accounts for jumps of\ninfinite variation. All parameters are estimated jointly by the generalized\nmethod of moments. A simulation study compares the finite sample behavior of\nthe proposed estimators with competing methods from the financial econometrics\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:24:37 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Mies", "Fabian", ""]]}, {"id": "1906.08080", "submitter": "Chenguang Liu", "authors": "Chenguang Liu", "title": "Central limit theorem for a partially observed interacting system of\n  Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe the actions of a $K$ sub-sample of $N$ individuals up to time $t$\nfor some large $K\\le N$. We model the relationships of individuals by i.i.d.\nBernoulli($p$)-random variables, where $p\\in (0,1]$ is an unknown parameter.\nThe rate of action of each individual depends on some unknown parameter $\\mu>\n0$ and on the sum of some function $\\phi$ of the ages of the actions of the\nindividuals which influence him. The parameters $\\mu$ and $\\phi$ are considered\nas nuisance parameters. The aim of this paper is to obtain a central limit\ntheorem for the estimator of $p$ that we introduced in \\cite{D}, both in the\nsubcritical and supercritical cases.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:01:32 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Liu", "Chenguang", ""]]}, {"id": "1906.08208", "submitter": "Pol del Aguila Pla", "authors": "Pol del Aguila Pla and Lissy Pellaco and Satyam Dwivedi and Peter\n  H\\\"andel and Joakim Jald\\'en", "title": "Clock synchronization over networks -- Identifiability of the sawtooth\n  model", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the two-node joint clock synchronization and\nranging problem. We focus on the case of nodes that employ time-to-digital\nconverters to determine the range between them precisely. This specific design\nchoice leads to a sawtooth model for the captured signal, which has not been\nstudied before from an estimation theoretic standpoint. In the study of this\nmodel, we recover the basic conclusion of a well-known article by Freris,\nGraham, and Kumar in clock synchronization. More importantly, we discover a\nsurprising identifiability result on the sawtooth signal model: noise improves\nthe theoretical condition of the estimation of the phase and offset parameters.\nTo complete our study, we provide performance references for joint clock\nsynchronization and ranging using the sawtooth signal model by presenting an\nexhaustive simulation study on basic estimation strategies under different\nrealistic conditions. With our contributions in this paper, we enable further\nresearch in the estimation of sawtooth signal models and pave the path towards\ntheir industrial use for clock synchronization and ranging.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 16:30:43 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 19:42:35 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 17:12:40 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Pla", "Pol del Aguila", ""], ["Pellaco", "Lissy", ""], ["Dwivedi", "Satyam", ""], ["H\u00e4ndel", "Peter", ""], ["Jald\u00e9n", "Joakim", ""]]}, {"id": "1906.08283", "submitter": "Francois-Xavier Briol", "authors": "Alessandro Barp, Francois-Xavier Briol, Andrew B. Duncan, Mark\n  Girolami, Lester Mackey", "title": "Minimum Stein Discrepancy Estimators", "comments": "Accepted for publication at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When maximum likelihood estimation is infeasible, one often turns to score\nmatching, contrastive divergence, or minimum probability flow to obtain\ntractable parameter estimates. We provide a unifying perspective of these\ntechniques as minimum Stein discrepancy estimators, and use this lens to design\nnew diffusion kernel Stein discrepancy (DKSD) and diffusion score matching\n(DSM) estimators with complementary strengths. We establish the consistency,\nasymptotic normality, and robustness of DKSD and DSM estimators, then derive\nstochastic Riemannian gradient descent algorithms for their efficient\noptimisation. The main strength of our methodology is its flexibility, which\nallows us to design estimators with desirable properties for specific models at\nhand by carefully selecting a Stein discrepancy. We illustrate this advantage\nfor several challenging problems for score matching, such as non-smooth,\nheavy-tailed or light-tailed densities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 18:04:28 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 10:11:43 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Barp", "Alessandro", ""], ["Briol", "Francois-Xavier", ""], ["Duncan", "Andrew B.", ""], ["Girolami", "Mark", ""], ["Mackey", "Lester", ""]]}, {"id": "1906.08343", "submitter": "Holger Dette", "authors": "Holger Dette, Viatcheslav B. Melas, Petr Shpilev", "title": "Optimal designs for estimating individual coefficients in polynomial\n  regression with no intercept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper \\cite{studden1968} characterized $c$-optimal designs in\nregression models, where the regression functions form a Chebyshev system. He\nused these results to determine the optimal design for estimating the\nindividual coefficients in a polynomial regression model on the interval\n$[-1,1]$ explicitly. In this note we identify the optimal design for estimating\nthe individual coefficients in a polynomial regression model with no intercept\n(here the regression functions do not form a Chebyshev system).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 20:32:31 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Dette", "Holger", ""], ["Melas", "Viatcheslav B.", ""], ["Shpilev", "Petr", ""]]}, {"id": "1906.08372", "submitter": "Yvik Swan", "authors": "Marie Ernst, Gesine Reinert and Yvik Swan", "title": "First order covariance inequalities via Stein's method", "comments": "32 pages, 3 tables, 2 figures. This is an updated version of the\n  first part of our previous arXiv submission on the same topic\n  (arXiv:1812.10344) which we leave on the arXiv as a separate submission\n  because it contains some material which may still be of interest", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose probabilistic representations for inverse Stein operators (i.e.\nsolutions to Stein equations) under general conditions; in particular we deduce\nnew simple expressions for the Stein kernel. These representations allow to\ndeduce uniform and non-uniform Stein factors (i.e. bounds on solutions to Stein\nequations) and lead to new covariance identities expressing the covariance\nbetween arbitrary functionals of an arbitrary {univariate} target in terms of a\nweighted covariance of the derivatives of the functionals. Our weights are\nexplicit, easily computable in most cases, and expressed in terms of objects\nfamiliar within the context of Stein's method. Applications of the\nCauchy-Schwarz inequality to these weighted covariance identities lead to sharp\nupper and lower covariance bounds and, in particular, weighted Poincar\\'e\ninequalities. Many examples are given and, in particular, classical variance\nbounds due to Klaassen, Brascamp and Lieb or Otto and Menz are corollaries.\nConnections with more recent literature are also detailed.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:42:17 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Ernst", "Marie", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "1906.08376", "submitter": "Yvik Swan", "authors": "Marie Ernst, Gesine Reinert and Yvik Swan", "title": "On infinite covariance expansions", "comments": "23 pages. This is an updated version of the second part of our\n  previous arXiv submission on the same topic (arXiv:1812.10344) which we leave\n  on the arXiv as a separate submission because it contains some material which\n  may still be of interest", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a probabilistic representation of Lagrange's\nidentity which we use to obtain Papathanasiou-type variance expansions of\narbitrary order. Our expansions lead to generalized sequences of weights which\ndepend on an arbitrarily chosen sequence of (non-decreasing) test functions.\nThe expansions hold for arbitrary univariate target distribution under weak\nassumptions, in particular they hold for continuous and discrete distributions\nalike. The weights are studied under different sets of assumptions either on\nthe test functions or on the underlying distributions. Many concrete\nillustrations for standard probability distributions are provided (including\nPearson, Ord, Laplace, Rayleigh, Cauchy, and Levy distributions).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:47:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Ernst", "Marie", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "1906.08383", "submitter": "Kaiqing Zhang", "authors": "Kaiqing Zhang, Alec Koppel, Hao Zhu, Tamer Ba\\c{s}ar", "title": "Global Convergence of Policy Gradient Methods to (Almost) Locally\n  Optimal Policies", "comments": "Initially submitted in Jan. 2019. Accepted to SIAM Journal on Control\n  and Optimization (SICON)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient (PG) methods are a widely used reinforcement learning\nmethodology in many applications such as video games, autonomous driving, and\nrobotics. In spite of its empirical success, a rigorous understanding of the\nglobal convergence of PG methods is lacking in the literature. In this work, we\nclose the gap by viewing PG methods from a nonconvex optimization perspective.\nIn particular, we propose a new variant of PG methods for infinite-horizon\nproblems that uses a random rollout horizon for the Monte-Carlo estimation of\nthe policy gradient. This method then yields an unbiased estimate of the policy\ngradient with bounded variance, which enables the tools from nonconvex\noptimization to be applied to establish global convergence. Employing this\nperspective, we first recover the convergence results with rates to the\nstationary-point policies in the literature. More interestingly, motivated by\nadvances in nonconvex optimization, we modify the proposed PG method by\nintroducing periodically enlarged stepsizes. The modified algorithm is shown to\nescape saddle points under mild assumptions on the reward and the policy\nparameterization. Under a further strict saddle points assumption, this result\nestablishes convergence to essentially locally-optimal policies of the\nunderlying problem, and thus bridges the gap in existing literature on the\nconvergence of PG methods. Results from experiments on the inverted pendulum\nare then provided to corroborate our theory, namely, by slightly reshaping the\nreward function to satisfy our assumption, unfavorable saddle points can be\navoided and better limit points can be attained. Intriguingly, this empirical\nfinding justifies the benefit of reward-reshaping from a nonconvex optimization\nperspective.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 22:33:25 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 12:39:19 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 21:54:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Koppel", "Alec", ""], ["Zhu", "Hao", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1906.08385", "submitter": "Dominik St\\\"oger", "authors": "Felix Krahmer, Dominik St\\\"oger", "title": "Complex phase retrieval from subgaussian measurements", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval refers to the problem of reconstructing an unknown vector\n$x_0 \\in \\mathbb{C}^n$ or $x_0 \\in \\mathbb{R}^n $ from $m$ measurements of the\nform $y_i = \\big\\vert \\langle \\xi^{\\left(i\\right)}, x_0 \\rangle \\big\\vert^2 $,\nwhere $ \\left\\{ \\xi^{\\left(i\\right)} \\right\\}^m_{i=1} \\subset \\mathbb{C}^m $\nare known measurement vectors. While Gaussian measurements allow for recovery\nof arbitrary signals provided the number of measurements scales at least\nlinearly in the number of dimensions, it has been shown that ambiguities may\narise for certain other classes of measurements $ \\left\\{ \\xi^{\\left(i\\right)}\n\\right\\}^{m}_{i=1}$ such as Bernoulli measurements or Fourier measurements. In\nthis paper, we will prove that even when a subgaussian vector $\n\\xi^{\\left(i\\right)} \\in \\mathbb{C}^m $ does not fulfill a small-ball\nprobability assumption, the PhaseLift method is still able to reconstruct a\nlarge class of signals $x_0 \\in \\mathbb{R}^n$ from the measurements. This\nextends recent work by Krahmer and Liu from the real-valued to the\ncomplex-valued case. However, our proof strategy is quite different and we\nexpect some of the new proof ideas to be useful in several other measurement\nscenarios as well. We then extend our results $x_0 \\in \\mathbb{C}^n $ up to an\nadditional assumption which, as we show, is necessary.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 22:36:45 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:50:15 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Krahmer", "Felix", ""], ["St\u00f6ger", "Dominik", ""]]}, {"id": "1906.08396", "submitter": "Ehsan Abbasi", "authors": "Ehsan Abbasi, Fariborz Salehi, Babak Hassibi", "title": "Universality in Learning from Linear Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering a structured signal from independently and\nidentically drawn linear measurements. A convex penalty function $f(\\cdot)$ is\nconsidered which penalizes deviations from the desired structure, and signal\nrecovery is performed by minimizing $f(\\cdot)$ subject to the linear\nmeasurement constraints. The main question of interest is to determine the\nminimum number of measurements that is necessary and sufficient for the perfect\nrecovery of the unknown signal with high probability. Our main result states\nthat, under some mild conditions on $f(\\cdot)$ and on the distribution from\nwhich the linear measurements are drawn, the minimum number of measurements\nrequired for perfect recovery depends only on the first and second order\nstatistics of the measurement vectors. As a result, the required of number of\nmeasurements can be determining by studying measurement vectors that are\nGaussian (and have the same mean vector and covariance matrix) for which a rich\nliterature and comprehensive theory exists. As an application, we show that the\nminimum number of random quadratic measurements (also known as rank-one\nprojections) required to recover a low rank positive semi-definite matrix is\n$3nr$, where $n$ is the dimension of the matrix and $r$ is its rank. As a\nconsequence, we settle the long standing open question of determining the\nminimum number of measurements required for perfect signal recovery in phase\nretrieval using the celebrated PhaseLift algorithm, and show it to be $3n$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 00:08:15 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Abbasi", "Ehsan", ""], ["Salehi", "Fariborz", ""], ["Hassibi", "Babak", ""]]}, {"id": "1906.08530", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan, Avetik Karagulyan and Lionel Riou-Durand", "title": "Bounding the error of discretized Langevin algorithms for non-strongly\n  log-concave targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide non-asymptotic upper bounds on the error of\nsampling from a target density using three schemes of discretized Langevin\ndiffusions. The first scheme is the Langevin Monte Carlo (LMC) algorithm, the\nEuler discretization of the Langevin diffusion. The second and the third\nschemes are, respectively, the kinetic Langevin Monte Carlo (KLMC) for\ndifferentiable potentials and the kinetic Langevin Monte Carlo for\ntwice-differentiable potentials (KLMC2). The main focus is on the target\ndensities that are smooth and log-concave on $\\mathbb R^p$, but not necessarily\nstrongly log-concave. Bounds on the computational complexity are obtained under\ntwo types of smoothness assumption: the potential has a Lipschitz-continuous\ngradient and the potential has a Lipschitz-continuous Hessian matrix. The error\nof sampling is measured by Wasserstein-$q$ distances. We advocate for the use\nof a new dimension-adapted scaling in the definition of the computational\ncomplexity, when Wasserstein-$q$ distances are considered. The obtained results\nshow that the number of iterations to achieve a scaled-error smaller than a\nprescribed value depends only polynomially in the dimension.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:57:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 08:49:18 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Karagulyan", "Avetik", ""], ["Riou-Durand", "Lionel", ""]]}, {"id": "1906.08799", "submitter": "James Grant", "authors": "James A. Grant, David S. Leslie", "title": "Posterior Contraction Rates for Gaussian Cox Processes with\n  Non-identically Distributed Data", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the posterior contraction of non-parametric Bayesian\ninference on non-homogeneous Poisson processes. We consider the quality of\ninference on a rate function $\\lambda$, given non-identically distributed\nrealisations, whose rates are transformations of $\\lambda$. Such data arises\nfrequently in practice due, for instance, to the challenges of making\nobservations with limited resources or the effects of weather on detectability\nof events. We derive contraction rates for the posterior estimates arising from\nthe Sigmoidal Gaussian Cox Process and Quadratic Gaussian Cox Process models.\nThese are popular models where $\\lambda$ is modelled as a logistic and\nquadratic transformation of a Gaussian Process respectively. Our work extends\nbeyond existing analyses in several regards. Firstly, we consider\nnon-identically distributed data, previously unstudied in the Poisson process\nsetting. Secondly, we consider the Quadratic Gaussian Cox Process model, of\nwhich there was previously little theoretical understanding. Thirdly, we\nprovide rates on the shrinkage of both the width of balls around the true\n$\\lambda$ in which the posterior mass is concentrated and on the shrinkage of\nposterior mass outside these balls - usually only the former is explicitly\ngiven. Finally, our results hold for certain finite numbers of observations,\nrather than only asymptotically, and we relate particular choices of\nhyperparameter/prior to these results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 18:16:27 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 11:59:47 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Grant", "James A.", ""], ["Leslie", "David S.", ""]]}, {"id": "1906.08843", "submitter": "Arnab Chakraborty", "authors": "Arnab Chakraborty and Soumendra N. Lahiri", "title": "On Statistical Properties of A Veracity Scoring Method for Spatial Data", "comments": "37 pages, 4 figures, 6 tables, submitted to JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring veracity or reliability of noisy data is of utmost importance,\nespecially in the scenarios where the information are gathered through\nautomated systems. In a recent paper, Chakraborty et. al. (2019) have\nintroduced a veracity scoring technique for geostatistical data. The authors\nhave used a high-quality `reference' data to measure the veracity of the\nvarying-quality observations and incorporated the veracity scores in their\nanalysis of mobile-sensor generated noisy weather data to generate efficient\npredictions of the ambient temperature process. In this paper, we consider the\nscenario when no reference data is available and hence, the veracity scores\n(referred as VS) are defined based on `local' summaries of the observations. We\ndevelop a VS-based estimation method for parameters of a spatial regression\nmodel. Under a non-stationary noise structure and fairly general assumptions on\nthe underlying spatial process, we show that the VS-based estimators of the\nregression parameters are consistent. Moreover, we establish the advantage of\nthe VS-based estimators as compared to the ordinary least squares (OLS)\nestimator by analyzing their asymptotic mean squared errors. We illustrate the\nmerits of the VS-based technique through simulations and apply the methodology\nto a real data set on mass percentages of ash in coal seams in Pennsylvania.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 20:47:38 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chakraborty", "Arnab", ""], ["Lahiri", "Soumendra N.", ""]]}, {"id": "1906.08884", "submitter": "Yuchao Liu", "authors": "Yuchao Liu, Ery Arias-Castro", "title": "A Multiscale Scan Statistic for Adaptive Submatrix Localization", "comments": "The original version was accepted by KDD2019 Research Track. Detail\n  of the proof is available at https://escholarship.org/uc/item/9wt627dg", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing a submatrix with larger-than-usual\nentry values inside a data matrix, without the prior knowledge of the submatrix\nsize. We establish an optimization framework based on a multiscale scan\nstatistic, and develop algorithms in order to approach the optimizer. We also\nshow that our estimator only requires a signal strength of the same order as\nthe minimax estimator with oracle knowledge of the submatrix size, to exactly\nrecover the anomaly with high probability. We perform some simulations that\nshow that our estimator has superior performance compared to other estimators\nwhich do not require prior submatrix knowledge, while being comparatively\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:36:12 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Liu", "Yuchao", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1906.08899", "submitter": "Andrea Montanari", "authors": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Limitations of Lazy Training of Two-layers Neural Networks", "comments": "39 pages; 2 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the supervised learning problem under either of the following two\nmodels: (1) Feature vectors ${\\boldsymbol x}_i$ are $d$-dimensional Gaussians\nand responses are $y_i = f_*({\\boldsymbol x}_i)$ for $f_*$ an unknown quadratic\nfunction; (2) Feature vectors ${\\boldsymbol x}_i$ are distributed as a mixture\nof two $d$-dimensional centered Gaussians, and $y_i$'s are the corresponding\nclass labels. We use two-layers neural networks with quadratic activations, and\ncompare three different learning regimes: the random features (RF) regime in\nwhich we only train the second-layer weights; the neural tangent (NT) regime in\nwhich we train a linearization of the neural network around its initialization;\nthe fully trained neural network (NN) regime in which we train all the weights\nin the network. We prove that, even for the simple quadratic model of point\n(1), there is a potentially unbounded gap between the prediction risk achieved\nin these three training regimes, when the number of neurons is smaller than the\nambient dimension. When the number of neurons is larger than the number of\ndimensions, the problem is significantly easier and both NT and NN learning\nachieve zero risk.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 00:29:54 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ghorbani", "Behrooz", ""], ["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "1906.08908", "submitter": "Haihan Tang", "authors": "Oliver B. Linton and Haihan Tang", "title": "Estimation of the Kronecker Covariance Model by Quadratic Form", "comments": null, "journal-ref": null, "doi": "10.1017/S026646662000050X", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator, the quadratic form estimator, of the Kronecker\nproduct model for covariance matrices. We show that this estimator has good\nproperties in the large dimensional case (i.e., the cross-sectional dimension\n$n$ is large relative to the sample size $T$). In particular, the quadratic\nform estimator is consistent in a relative Frobenius norm sense provided\n$\\log^3n/T\\to 0$. We obtain the limiting distributions of Lagrange multiplier\n(LM) and Wald tests under both the null and local alternatives concerning the\nmean vector $\\mu$. Testing linear restrictions of $\\mu$ is also investigated.\nFinally, our methodology performs well in the finite-sample situations both\nwhen the Kronecker product model is true, and when it is not true.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:25:31 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 04:12:06 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 08:25:41 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 13:34:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Linton", "Oliver B.", ""], ["Tang", "Haihan", ""]]}, {"id": "1906.09103", "submitter": "Jiaowen Yang", "authors": "Ting-Kam Leonard Wong, Jiaowen Yang", "title": "Logarithmic divergences: geometry and interpretation of curvature", "comments": "10 pages, International Conference on Geometric Science of\n  Information. Springer, 2019. arXiv admin note: text overlap with\n  arXiv:1906.00030", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the logarithmic $L^{(\\alpha)}$-divergence which extrapolates the\nBregman divergence and corresponds to solutions to novel optimal transport\nproblems. We show that this logarithmic divergence is equivalent to a conformal\ntransformation of the Bregman divergence, and, via an explicit affine\nimmersion, is equivalent to Kurose's geometric divergence. In particular, the\n$L^{(\\alpha)}$-divergence is a canonical divergence of a statistical manifold\nwith constant sectional curvature $-\\alpha$. For such a manifold, we give a\ngeometric interpretation of its sectional curvature in terms of how the\ndivergence between a pair of primal and dual geodesics differ from the dually\nflat case. Further results can be found in our follow-up paper [27] which\nuncovers a novel relation between optimal transport and information geometry.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 21:00:01 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Wong", "Ting-Kam Leonard", ""], ["Yang", "Jiaowen", ""]]}, {"id": "1906.09143", "submitter": "Bogdan \\'Cmiel", "authors": "Bogdan \\'Cmiel, Tadeusz Inglot, Teresa Ledwina", "title": "Intermediate efficiency of some weighted goodness-of-fit statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares the Anderson-Darling and some Eicker-Jaeschke statistics\nto the classical unweighted Kolmogorov-Smirnov statistic. The goal is to\nprovide a quantitative comparison of such tests and to study real possibilities\nof using them to detect departures from the hypothesized distribution that\noccur in the tails. This contribution covers the case when under the\nalternative a moderately large portion of probability mass is allocated towards\nthe tails. It is demonstrated that the approach allows for tractable, analytic\ncomparison between the given test and the benchmark, and for reliable\nquantitative evaluation of weighted statistics. Finite sample results\nillustrate the proposed approach and confirm the theoretical findings. In the\ncourse of the investigation we also prove that a slight and natural\nmodification of the solution proposed by Borovkov and Sycheva (1968) leads to a\nstatistic which is a member of Eicker-Jaeschke class and can be considered an\nattractive competitor of the very popular supremum-type Anderson-Darling\nstatistic.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 13:52:16 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["\u0106miel", "Bogdan", ""], ["Inglot", "Tadeusz", ""], ["Ledwina", "Teresa", ""]]}, {"id": "1906.09231", "submitter": "Om Thakkar", "authors": "Ryan Rogers, Aaron Roth, Adam Smith, Nathan Srebro, Om Thakkar, Blake\n  Woodworth", "title": "Guaranteed Validity for Empirical Approaches to Adaptive Data Analysis", "comments": "Accepted to appear in the proceedings of the 23rd International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a general framework for answering adaptive statistical queries that\nfocuses on providing explicit confidence intervals along with point estimates.\nPrior work in this area has either focused on providing tight confidence\nintervals for specific analyses, or providing general worst-case bounds for\npoint estimates. Unfortunately, as we observe, these worst-case bounds are\nloose in many settings --- often not even beating simple baselines like sample\nsplitting. Our main contribution is to design a framework for providing valid,\ninstance-specific confidence intervals for point estimates that can be\ngenerated by heuristics. When paired with good heuristics, this method gives\nguarantees that are orders of magnitude better than the best worst-case bounds.\nWe provide a Python library implementing our method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:33:02 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:30:40 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Rogers", "Ryan", ""], ["Roth", "Aaron", ""], ["Smith", "Adam", ""], ["Srebro", "Nathan", ""], ["Thakkar", "Om", ""], ["Woodworth", "Blake", ""]]}, {"id": "1906.09255", "submitter": "Ashwin Pananjady", "authors": "Avishek Ghosh, Ashwin Pananjady, Adityanand Guntuboyina, Kannan\n  Ramchandran", "title": "Max-Affine Regression: Provable, Tractable, and Near-Optimal Statistical\n  Estimation", "comments": "The first two authors contributed equally to this work and are\n  ordered alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-affine regression refers to a model where the unknown regression function\nis modeled as a maximum of $k$ unknown affine functions for a fixed $k \\geq 1$.\nThis generalizes linear regression and (real) phase retrieval, and is closely\nrelated to convex regression. Working within a non-asymptotic framework, we\nstudy this problem in the high-dimensional setting assuming that $k$ is a fixed\nconstant, and focus on estimation of the unknown coefficients of the affine\nfunctions underlying the model. We analyze a natural alternating minimization\n(AM) algorithm for the non-convex least squares objective when the design is\nrandom. We show that the AM algorithm, when initialized suitably, converges\nwith high probability and at a geometric rate to a small ball around the\noptimal coefficients. In order to initialize the algorithm, we propose and\nanalyze a combination of a spectral method and a random search scheme in a\nlow-dimensional space, which may be of independent interest. The final rate\nthat we obtain is near-parametric and minimax optimal (up to a poly-logarithmic\nfactor) as a function of the dimension, sample size, and noise variance. In\nthat sense, our approach should be viewed as a direct and implementable method\nof enforcing regularization to alleviate the curse of dimensionality in\nproblems of the convex regression type. As a by-product of our analysis, we\nalso obtain guarantees on a classical algorithm for the phase retrieval problem\nunder considerably weaker assumptions on the design distribution than was\npreviously known. Numerical experiments illustrate the sharpness of our bounds\nin the various problem parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 17:47:18 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ghosh", "Avishek", ""], ["Pananjady", "Ashwin", ""], ["Guntuboyina", "Adityanand", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1906.09332", "submitter": "Marcel Br\\\"autigam", "authors": "Marcel Br\\\"autigam and Marie Kratz", "title": "Bivariate FCLT for the Sample Quantile and Measures of Dispersion for\n  Augmented GARCH($p$,$q$) processes", "comments": "16 pages, 1 figure, 3 tables; Changes to previous version: Precised\n  conditions in Lemma 7 and Proposition 8. Corrected (and simplified) Step 3 in\n  the proof of Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon the asymptotic theory for GARCH processes,\nconsidering the general class of augmented GARCH($p$, $q$) processes. Our\ncontribution is to complement the well-known univariate asymptotics by\nproviding a joint (bivariate) functional central limit theorem of the sample\nquantile and the r-th absolute centred sample moment. This extends existing\nresults in the case of identically and independently distributed random\nvariables.\n  We show that the conditions for the convergence of the estimators in the\nunivariate case suffice even for the joint bivariate asymptotics. We illustrate\nthe general results with various specific examples from the class of augmented\nGARCH($p$, $q$) processes and show explicitly under which conditions on the\nmoments and parameters of the process the joint asymptotics hold.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 21:20:38 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 15:17:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Br\u00e4utigam", "Marcel", ""], ["Kratz", "Marie", ""]]}, {"id": "1906.09484", "submitter": "Michael Evans", "authors": "Michael Evans", "title": "The Measurement of Statistical Evidence as the Basis for Statistical\n  Reasoning", "comments": "This paper serves as the basis for a talk at the OBayes Conference at\n  the U. of Warwick, June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various approaches to the problem of how one is supposed to conduct\na statistical analysis. Different analyses can lead to contradictory\nconclusions in some problems so this is not a satisfactory state of affairs. It\nseems that all approaches make reference to the evidence in the data concerning\nquestions of interest as a justification for the methodology employed. It is\nfair to say, however, that none of the most commonly used methodologies is\nabsolutely explicit about how statistical evidence is to be characterized and\nmeasured. We will discuss the general problem of statistical reasoning and the\ndevelopment of a theory for this that is based on being precise about\nstatistical evidence. This will be shown to lead to the resolution of a number\nof problems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 18:59:06 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Evans", "Michael", ""]]}, {"id": "1906.09485", "submitter": "Amadou Sawadogo", "authors": "C\\'elestin C.Kokonendji, Aboubacar Y. Tour\\'e, Amadou Sawadogo", "title": "Relative variation indexes for multivariate continuous distributions on\n  $[0,\\infty)^k$ and extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some new indexes to measure the departure of any multivariate\ncontinuous distribution on non-negative orthant from a given reference one such\nthe uncorrelated exponential model, similar to the relative Fisher dispersion\nindexes of multivariate count models.\n  The proposed multivariate variation indexes are scalar quantities, defined as\nratios of two quadratic forms of the mean vector and the covariance matrix.\nThey can be used to discriminate between continuous positive distributions.\nGeneralized and multiple marginal variation indexes with and without\ncorrelation structure, respectively, and their relative extensions are\ndiscussed. The asymptotic behavior and other properties are studied.\nIllustrative examples and numerical applications are analyzed under several\nscenarios, leading to appropriate choices of multivariate models. Some\nconcluding remarks and possible extensions are made.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 19:01:34 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Kokonendji", "C\u00e9lestin C.", ""], ["Tour\u00e9", "Aboubacar Y.", ""], ["Sawadogo", "Amadou", ""]]}, {"id": "1906.09501", "submitter": "Vasiliki Velona", "authors": "G\\'abor Lugosi, Jakub Truszkowski, Vasiliki Velona, Piotr Zwiernik", "title": "Structure learning in graphical models by covariance queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering the structure underlying large Gaussian\ngraphical models or, more generally, partial correlation graphs. In\nhigh-dimensional problems it is often too costly to store the entire sample\ncovariance matrix. We propose a new input model in which one can query single\nentries of the covariance matrix. We prove that it is possible to recover the\nsupport of the inverse covariance matrix with low query and computational\ncomplexity. Our algorithms work in a regime when this support is represented by\ntree-like graphs and, more generally, for graphs of small treewidth. Our\nresults demonstrate that for large classes of graphs, the structure of the\ncorresponding partial correlation graphs can be determined much faster than\neven computing the empirical covariance matrix.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 20:44:52 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 11:05:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lugosi", "G\u00e1bor", ""], ["Truszkowski", "Jakub", ""], ["Velona", "Vasiliki", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1906.09507", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Saifuddin Syed, Chiao-Yu Yang, Michael I. Jordan, and\n  Tamara Broderick", "title": "Local Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeability -- in which the distribution of an infinite sequence is\ninvariant to reorderings of its elements -- implies the existence of a simple\nconditional independence structure that may be leveraged in the design of\nprobabilistic models, efficient inference algorithms, and randomization-based\ntesting procedures. In practice, however, this assumption is too strong an\nidealization; the distribution typically fails to be exactly invariant to\npermutations and de Finetti's representation theory does not apply. Thus there\nis the need for a distributional assumption that is both weak enough to hold in\npractice, and strong enough to guarantee a useful underlying representation. We\nintroduce a relaxed notion of local exchangeability -- where swapping data\nassociated with nearby covariates causes a bounded change in the distribution.\nWe prove that locally exchangeable processes correspond to independent\nobservations from an underlying measure-valued stochastic process. We thereby\nshow that de Finetti's theorem is robust to perturbation and provide further\njustification for the Bayesian modelling approach. Using this probabilistic\nresult, we develop three novel statistical procedures for (1) estimating the\nunderlying process via local empirical measures, (2) testing via local\nrandomization, and (3) estimating the canonical premetric of local\nexchangeability. These three procedures extend the applicability of previous\nexchangeability-based methods without sacrificing rigorous statistical\nguarantees. The paper concludes with examples of popular statistical models\nthat exhibit local exchangeability.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 21:17:08 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 19:04:19 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 19:55:16 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Campbell", "Trevor", ""], ["Syed", "Saifuddin", ""], ["Yang", "Chiao-Yu", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1906.09537", "submitter": "Caroline Uhler", "authors": "Marta Casanellas, Sonja Petrovi\\'c and Caroline Uhler", "title": "Algebraic Statistics in Practice: Applications to Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic statistics uses tools from algebra (especially from multilinear\nalgebra, commutative algebra and computational algebra), geometry and\ncombinatorics to provide insight into knotty problems in mathematical\nstatistics. In this survey we illustrate this on three problems related to\nnetworks, namely network models for relational data, causal structure discovery\nand phylogenetics. For each problem we give an overview of recent results in\nalgebraic statistics with emphasis on the statistical achievements made\npossible by these tools and their practical relevance for applications to other\nscientific disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 02:29:11 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Casanellas", "Marta", ""], ["Petrovi\u0107", "Sonja", ""], ["Uhler", "Caroline", ""]]}, {"id": "1906.09639", "submitter": "Fang Han", "authors": "Zeng Li, Fang Han, and Jianfeng Yao", "title": "Asymptotic joint distribution of extreme eigenvalues and trace of large\n  sample covariance matrix in a generalized spiked population model", "comments": "to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the joint limiting behavior of extreme eigenvalues and\ntrace of large sample covariance matrix in a generalized spiked population\nmodel, where the asymptotic regime is such that the dimension and sample size\ngrow proportionally. The form of the joint limiting distribution is applied to\nconduct Johnson-Graybill-type tests, a family of approaches testing for signals\nin a statistical model. For this, higher order correction is further made,\nhelping alleviate the impact of finite-sample bias. The proof rests on\ndetermining the joint asymptotic behavior of two classes of spectral processes,\ncorresponding to the extreme and linear spectral statistics respectively.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 19:49:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Li", "Zeng", ""], ["Han", "Fang", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1906.09712", "submitter": "Steven Howard", "authors": "Steven R. Howard, Aaditya Ramdas", "title": "Sequential estimation of quantiles with applications to A/B-testing and\n  best-arm identification", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose confidence sequences -- sequences of confidence intervals which\nare valid uniformly over time -- for quantiles of any distribution over a\ncomplete, fully-ordered set, based on a stream of i.i.d. observations. We give\nmethods both for tracking a fixed quantile and for tracking all quantiles\nsimultaneously. Specifically, we provide explicit expressions with small\nconstants for intervals whose widths shrink at the fastest possible\n$\\sqrt{t^{-1} \\log\\log t}$ rate, along with a non-asymptotic concentration\ninequality for the empirical distribution function which holds uniformly over\ntime with the same rate. The latter strengthens Smirnov's empirical process law\nof the iterated logarithm and extends the Dvoretzky-Kiefer-Wolfowitz inequality\nto hold uniformly over time. We give a new algorithm and sample complexity\nbound for selecting an arm with an approximately best quantile in a multi-armed\nbandit framework. In simulations, our method requires fewer samples than\nexisting methods by a factor of five to fifty.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:47:58 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 18:51:58 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:58:30 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Howard", "Steven R.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1906.09757", "submitter": "Xuan Yin", "authors": "Xuan Yin and Liangjie Hong", "title": "The Identification and Estimation of Direct and Indirect Effects in A/B\n  Tests through Causal Mediation Analysis", "comments": "Accepted by The 25th ACM SIGKDD Conference on Knowledge Discovery and\n  DataMining (KDD '19), August 4-8, 2019, Anchorage, AK, USA", "journal-ref": "The 25th ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD '19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY,\n  USA", "doi": "10.1145/3292500.3330769", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce companies have a number of online products, such as organic\nsearch, sponsored search, and recommendation modules, to fulfill customer\nneeds. Although each of these products provides a unique opportunity for users\nto interact with a portion of the overall inventory, they are all similar\nchannels for users and compete for limited time and monetary budgets of users.\nTo optimize users' overall experiences on an E-commerce platform, instead of\nunderstanding and improving different products separately, it is important to\ngain insights into the evidence that a change in one product would induce users\nto change their behaviors in others, which may be due to the fact that these\nproducts are functionally similar. In this paper, we introduce causal mediation\nanalysis as a formal statistical tool to reveal the underlying causal\nmechanisms. Existing literature provides little guidance on cases where\nmultiple unmeasured causally-dependent mediators exist, which are common in A/B\ntests. We seek a novel approach to identify in those scenarios direct and\nindirect effects of the treatment. In the end, we demonstrate the effectiveness\nof the proposed method in data from Etsy's real A/B tests and shed lights on\ncomplex relationships between different products.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:22:30 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yin", "Xuan", ""], ["Hong", "Liangjie", ""]]}, {"id": "1906.09855", "submitter": "Roi Weiss", "authors": "Steve Hanneke, Aryeh Kontorovich, Sivan Sabato, Roi Weiss", "title": "Universal Bayes consistency in metric spaces", "comments": "To appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a recently proposed 1-nearest-neighbor based multiclass learning\nalgorithm and prove that our modification is universally strongly\nBayes-consistent in all metric spaces admitting any such learner, making it an\n\"optimistically universal\" Bayes-consistent learner. This is the first learning\nalgorithm known to enjoy this property; by comparison, the $k$-NN classifier\nand its variants are not generally universally Bayes-consistent, except under\nadditional structural assumptions, such as an inner product, a norm, finite\ndimension, or a Besicovitch-type property. The metric spaces in which universal\nBayes consistency is possible are the \"essentially separable\" ones -- a notion\nthat we define, which is more general than standard separability. The existence\nof metric spaces that are not essentially separable is widely believed to be\nindependent of the ZFC axioms of set theory. We prove that essential\nseparability exactly characterizes the existence of a universal\nBayes-consistent learner for the given metric space. In particular, this yields\nthe first impossibility result for universal Bayes consistency. Taken together,\nour results completely characterize strong and weak universal Bayes consistency\nin metric spaces.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 11:29:00 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 06:50:37 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 21:16:08 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 17:20:57 GMT"}, {"version": "v5", "created": "Sun, 11 Oct 2020 16:31:23 GMT"}, {"version": "v6", "created": "Fri, 16 Oct 2020 09:55:46 GMT"}, {"version": "v7", "created": "Wed, 6 Jan 2021 22:39:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hanneke", "Steve", ""], ["Kontorovich", "Aryeh", ""], ["Sabato", "Sivan", ""], ["Weiss", "Roi", ""]]}, {"id": "1906.09883", "submitter": "Olivier Roustant", "authors": "O Roustant (Limos, Gdr Mascot-Num, Fayol-Ensmse), F. Gamboa (Imt), B\n  Iooss (Edf R\\&D Mri, Imt, Gdr Mascot-Num)", "title": "Sensitivity Analysis and Generalized Chaos Expansions. Lower Bounds for\n  Sobol indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so-called polynomial chaos expansion is widely used in computer\nexperiments. For example, it is a powerful tool to estimate Sobol' sensitivity\nindices. In this paper, we consider generalized chaos expansions built on\ngeneral tensor Hilbert basis. In this frame, we revisit the computation of the\nSobol' indices and give general lower bounds for these indices. The case of the\neigenfunctions system associated with a Poincar{\\'e} differential operator\nleads to lower bounds involving the derivatives of the analyzed function and\nprovides an efficient tool for variable screening. These lower bounds are put\nin action both on toy and real life models demonstrating their accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:30:51 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Roustant", "O", "", "Limos, Gdr Mascot-Num, Fayol-Ensmse"], ["Gamboa", "F.", "", "Imt"], ["Iooss", "B", "", "Edf R\\&D Mri, Imt, Gdr Mascot-Num"]]}, {"id": "1906.09982", "submitter": "Alberto Ferrari", "authors": "Alberto Ferrari", "title": "A note on sum and difference of correlated chi-squared variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate distributions for sum and difference of linearly correlated\n$\\chi^{2}$ distributed random variables are derived. It is shown that they can\nbe reduced to conveniently parametrized gamma and Variance-Gamma distributions,\nrespectively. The proposed distributions are very flexible, and the one for sum\nin particular has straight-forward generalizations to cases where multiple\n$\\chi^{2}$ variables with different parameters are involved. The results\npromptly extend to every sum of gamma variables with common scale and to every\ndifference between gamma variables with common shape and scale. The fit of the\ndistributions is tested on simulated data with remarkable results.The\napproximations presented are expected to be especially useful to researchers\nworking on gamma-distributed variables.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:12:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ferrari", "Alberto", ""]]}, {"id": "1906.10026", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Avanti Athreya, Joshua Cape, Guodong Chen, Carey E.\n  Priebe, and Joshua T. Vogelstein", "title": "Inference for multiple heterogeneous networks with a common invariant\n  subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of models for multiple heterogeneous network data is of\ncritical importance both in statistical network theory and across multiple\napplication domains. Although single-graph inference is well-studied, multiple\ngraph inference is largely unexplored, in part because of the challenges\ninherent in appropriately modeling graph differences and yet retaining\nsufficient model simplicity to render estimation feasible. This paper addresses\nexactly this gap, by introducing a new model, the common subspace\nindependent-edge (COSIE) multiple random graph model, which describes a\nheterogeneous collection of networks with a shared latent structure on the\nvertices but potentially different connectivity patterns for each graph. The\nCOSIE model encompasses many popular network representations, including the\nstochastic blockmodel. The model is both flexible enough to meaningfully\naccount for important graph differences and tractable enough to allow for\naccurate inference in multiple networks. In particular, a joint spectral\nembedding of adjacency matrices - the multiple adjacency spectral embedding\n(MASE) - leads, in a COSIE model, to simultaneous consistent estimation of\nunderlying parameters for each graph. Under mild additional assumptions, MASE\nestimates satisfy asymptotic normality and yield improvements for graph\neigenvalue estimation and hypothesis testing. In both simulated and real data,\nthe COSIE model and the MASE embedding can be deployed for a number of\nsubsequent network inference tasks, including dimensionality reduction,\nclassification, hypothesis testing and community detection. Specifically, when\nMASE is applied to a dataset of connectomes constructed through diffusion\nmagnetic resonance imaging, the result is an accurate classification of brain\nscans by patient and a meaningful determination of heterogeneity across scans\nof different subjects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:29:25 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 21:24:58 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 06:21:07 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2020 05:41:17 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Athreya", "Avanti", ""], ["Cape", "Joshua", ""], ["Chen", "Guodong", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1906.10056", "submitter": "Shogo Nakakita", "authors": "Shogo H Nakakita and Masayuki Uchida", "title": "Parametric estimation for convolutionally observed diffusion processes", "comments": null, "journal-ref": null, "doi": "10.3390/e22091031", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical observation scheme of diffusion processes named\nconvolutional observation, where it is possible to deal with smoother\nobservation than ordinary diffusion processes by considering convolution of\ndiffusion processes and some kernel functions with respect to time parameter.\nWe discuss the estimation and test theories for the parameter determining the\nsmoothness of the observation, as well as the least-square-type estimation for\nthe parameters in the diffusion coefficient and the drift one of the latent\ndiffusion process. In addition to the theoretical discussion, we also examine\nthe performance of the estimation and the test with computational simulation,\nand show an example of real data analysis for one EEG data whose observation\ncan be regarded as smoother one than ordinary diffusion processes with\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:25:41 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Nakakita", "Shogo H", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1906.10075", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Themis Gouleakis and Christos Tzamos", "title": "Distribution-Independent PAC Learning of Halfspaces with Massart Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of {\\em distribution-independent} PAC learning of\nhalfspaces in the presence of Massart noise. Specifically, we are given a set\nof labeled examples $(\\mathbf{x}, y)$ drawn from a distribution $\\mathcal{D}$\non $\\mathbb{R}^{d+1}$ such that the marginal distribution on the unlabeled\npoints $\\mathbf{x}$ is arbitrary and the labels $y$ are generated by an unknown\nhalfspace corrupted with Massart noise at noise rate $\\eta<1/2$. The goal is to\nfind a hypothesis $h$ that minimizes the misclassification error\n$\\mathbf{Pr}_{(\\mathbf{x}, y) \\sim \\mathcal{D}} \\left[ h(\\mathbf{x}) \\neq y\n\\right]$.\n  We give a $\\mathrm{poly}\\left(d, 1/\\epsilon \\right)$ time algorithm for this\nproblem with misclassification error $\\eta+\\epsilon$. We also provide evidence\nthat improving on the error guarantee of our algorithm might be computationally\nhard. Prior to our work, no efficient weak (distribution-independent) learner\nwas known in this model, even for the class of disjunctions. The existence of\nsuch an algorithm for halfspaces (or even disjunctions) has been posed as an\nopen question in various works, starting with Sloan (1988), Cohen (1997), and\nwas most recently highlighted in Avrim Blum's FOCS 2003 tutorial.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:54:46 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 16:46:00 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""]]}, {"id": "1906.10125", "submitter": "Osama Idais", "authors": "Osama Idais", "title": "A note on locally optimal designs for generalized linear models with\n  restricted support", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal designs for generalized linear models require a prior knowledge of\nthe regression parameters. At certain values of the parameters we propose\nparticular assumptions which allow to derive a locally optimal design for a\nmodel without intercept from a locally optimal design for the corresponding\nmodel with intercept and vice versa. Applications to Poisson and logistic\nmodels and Extensions to nonlinear models are provided.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:14:52 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Idais", "Osama", ""]]}, {"id": "1906.10240", "submitter": "Tim Sullivan", "authors": "T. J. Sullivan", "title": "Comments on the article \"A Bayesian conjugate gradient method\"", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent article \"A Bayesian conjugate gradient method\" by Cockayne, Oates,\nIpsen, and Girolami proposes an approximately Bayesian iterative procedure for\nthe solution of a system of linear equations, based on the conjugate gradient\nmethod, that gives a sequence of Gaussian/normal estimates for the exact\nsolution. The purpose of the probabilistic enrichment is that the covariance\nstructure is intended to provide a posterior measure of uncertainty or\nconfidence in the solution mean. This note gives some comments on the article,\nposes some questions, and suggests directions for further research.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 21:26:45 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Sullivan", "T. J.", ""]]}, {"id": "1906.10246", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "Uniformly consistently estimating the proportion of false null\n  hypotheses for composite null hypotheses via Lebesgue-Stieltjes integral\n  equations", "comments": "55 pages in 1.5-line spacing; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This is the second part of the series on constructing uniformly consistent\nestimators of the proportion of false null hypotheses via solutions to\nLebesgue-Stieltjes integral equations. We consider estimating the proportion of\nrandom variables for two types of composite null hypotheses: (i) their means or\nmedians belonging to a non-empty, bounded interval; (ii) their means or medians\nbelonging to an unbounded interval that is not the whole real line. For each\ntype of composite null hypotheses, uniform consistent estimators of the\nproportion of false null hypotheses are constructed respectively for random\nvariable that follow Type I location-shift family of distributions and for\nrandom variables whose distributions form continuous natural exponential\nfamilies with separable moments. Further, uniformly consistent estimators of\nthe proportion induced by a function of bounded variation on a non-empty,\nbounded interval are provided for the two types of random variables mentioned\nearlier. For each proposed estimator, its uniform consistency class and speed\nof convergence are provided under independence.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 21:53:19 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "1906.10300", "submitter": "Matthew J. Holland", "authors": "Matthew J. Holland", "title": "Distribution-robust mean estimation via smoothed random perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of mean estimation assuming only finite variance. We\nstudy a new class of mean estimators constructed by integrating over random\nnoise applied to a soft-truncated empirical mean estimator. For appropriate\nchoices of noise, we show that this can be computed in closed form, and\nutilizing relative entropy inequalities, these estimators enjoy deviations with\nexponential tails controlled by the second moment of the underlying\ndistribution. We consider both additive and multiplicative noise, and several\nnoise distribution families in our analysis. Furthermore, we empirically\ninvestigate the sensitivity to the mean-standard deviation ratio for numerous\nconcrete manifestations of the estimator class of interest. Our main take-away\nis that an inexpensive new estimator can achieve nearly sub-Gaussian\nperformance for a wide variety of data distributions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 02:54:28 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Holland", "Matthew J.", ""]]}, {"id": "1906.10305", "submitter": "Zheng Fang", "authors": "Zheng Fang", "title": "Refinements of the Kiefer-Wolfowitz Theorem and a Test of Concavity", "comments": "Forthcoming in Electronic Journal of Statistics. Compared to the\n  journal version, the difference is that this version contains additional\n  simulation results, collected in Appendix C", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies estimation of and inference on a distribution function $F$\nthat is concave on the nonnegative half line and admits a density function $f$\nwith potentially unbounded support. When $F$ is strictly concave, we show that\nthe supremum distance between the Grenander distribution estimator and the\nempirical distribution may still be of order $O(n^{-2/3}(\\log n)^{2/3})$ almost\nsurely, which reduces to an existing result of Kiefer and Wolfowitz when $f$\nhas bounded support. We further refine this result by allowing $F$ to be not\nstrictly concave or even non-concave and instead requiring it be\n\"asymptotically\" strictly concave. Building on these results, we then develop a\ntest of concavity of $F$ or equivalently monotonicity of $f$, which is shown to\nhave asymptotically pointwise level control under the entire null as well as\nconsistency under any fixed alternative. In fact, we show that our test has\nlocal size control and nontrivial local power against any local alternatives\nthat do not approach the null too fast, which may be of interest given the\nirregularity of the problem. Extensions to settings involving testing\nconcavity/convexity/monotonicity are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 03:17:27 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 22:02:50 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Fang", "Zheng", ""]]}, {"id": "1906.10319", "submitter": "Michael Celentano", "authors": "Michael Celentano", "title": "Approximate separability of symmetrically penalized least squares in\n  high dimensions: characterization and consequences", "comments": "54 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the high-dimensional behavior of symmetrically penalized least\nsquares with a possibly non-separable, symmetric, convex penalty in both (i)\nthe Gaussian sequence model and (ii) the linear model with uncorrelated\nGaussian designs nearly matches the behavior of least squares with an\nappropriately chosen separable penalty in these same models. The similarity in\nbehavior is precisely quantified by a finite-sample concentration inequality in\nboth cases. Our results help clarify the role non-separability can play in\nhigh-dimensional M-estimation. In particular, if the empirical distribution of\nthe coordinates of the parameter is known --exactly or approximately-- there\nare at most limited advantages to using non-separable, symmetric penalties over\nseparable ones. In contrast, if the empirical distribution of the coordinates\nof the parameter is unknown, we argue that non-separable, symmetric penalties\nautomatically implement an adaptive procedure which we characterize. We also\nprovide a partial converse which characterizes adaptive procedures which can be\nimplemented in this way.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 04:36:24 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Celentano", "Michael", ""]]}, {"id": "1906.10461", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Depth induced regression medians and uniqueness", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notion of median in one dimension is a foundational element in nonparametric\nstatistics. It has been extended to multi-dimensional cases both in location\nand in regression via notions of data depth.\n  Regression depth (RD) and projection regression depth (PRD) represent the two\nmost promising notions in regression. Carrizosa depth $D_C$ is another depth\nnotion in regression.Depth induced regression medians (maximum depth\nestimators) serve as robust alternatives to the classical least squares\nestimator.\n  The uniqueness of regression medians is indispensable in the discussion of\ntheir properties and the asymptotics (consistency and limiting distribution) of\nsample regression medians. Are the regression medians induced from RD, PRD, and\n$D_C$ unique? Answering this question is the main goal of this article. It is\nfound that only the regression median induced from PRD possesses the desired\nuniqueness property. The conventional remedy measure for non-uniqueness, taking\naverage of all medians, might yield an estimator that no longer possesses the\nmaximum depth in both RD and $D_C$ cases.\n  These and other findings indicate that the PRD and its induced median are\nhighly favorable among their leading competitors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 11:32:22 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 12:31:45 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 15:28:39 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2020 04:30:41 GMT"}, {"version": "v5", "created": "Sat, 14 Mar 2020 23:50:28 GMT"}, {"version": "v6", "created": "Fri, 27 Mar 2020 17:57:45 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "1906.10529", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Jaouad Mourtada and St\\'ephane Ga\\\"iffas and Erwan Scornet", "title": "AMF: Aggregated Mondrian Forests for Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests (RF) is one of the algorithms of choice in many supervised\nlearning applications, be it classification or regression. The appeal of such\ntree-ensemble methods comes from a combination of several characteristics: a\nremarkable accuracy in a variety of tasks, a small number of parameters to\ntune, robustness with respect to features scaling, a reasonable computational\ncost for training and prediction, and their suitability in high-dimensional\nsettings. The most commonly used RF variants however are \"offline\" algorithms,\nwhich require the availability of the whole dataset at once. In this paper, we\nintroduce AMF, an online random forest algorithm based on Mondrian Forests.\nUsing a variant of the Context Tree Weighting algorithm, we show that it is\npossible to efficiently perform an exact aggregation over all prunings of the\ntrees; in particular, this enables to obtain a truly online parameter-free\nalgorithm which is competitive with the optimal pruning of the Mondrian tree,\nand thus adaptive to the unknown regularity of the regression function.\nNumerical experiments show that AMF is competitive with respect to several\nstrong baselines on a large number of datasets for multi-class classification.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 13:50:22 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 15:45:45 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Scornet", "Erwan", ""]]}, {"id": "1906.10733", "submitter": "Flavio Goncalves", "authors": "Fl\\'avio B. Gon\\c{c}alves and Pedro Franklin", "title": "On the definition of likelihood function", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general definition of likelihood function in terms of\nRadon-Nikod\\'{y}m derivatives. The definition is validated by the Likelihood\nPrinciple once we establish a result regarding the proportionality of\nlikelihood functions under different dominating measures. This general\nframework is particularly useful when there exists no or more than one obvious\nchoice for a dominating measure as in some infinite-dimensional models. We\ndiscuss the importance of considering continuous versions of densities and how\nthese are related to the Likelihood Principle and the basic concept of\nlikelihood. We also discuss the use of the predictive measure as a dominating\nmeasure in the Bayesian approach. Finally, some examples illustrate the general\ndefinition of likelihood function and the importance of choosing particular\ndominating measures in some cases.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 19:22:33 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 21:36:05 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 15:58:27 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 15:09:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B.", ""], ["Franklin", "Pedro", ""]]}, {"id": "1906.10869", "submitter": "Giacomo Capodaglio", "authors": "Giacomo Capodaglio and Max Gunzburger", "title": "Piecewise polynomial approximation of probability density functions with\n  application to uncertainty quantification for stochastic PDEs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-48721-8_5", "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability density function (PDF) associated with a given set of samples\nis approximated by a piecewise-linear polynomial constructed with respect to a\nbinning of the sample space. The kernel functions are a compactly supported\nbasis for the space of such polynomials, i.e. finite element hat functions,\nthat are centered at the bin nodes rather than at the samples, as is the case\nfor the standard kernel density estimation approach. This feature naturally\nprovides an approximation that is scalable with respect to the sample size. On\nthe other hand, unlike other strategies that use a finite element approach, the\nproposed approximation does not require the solution of a linear system. In\naddition, a simple rule that relates the bin size to the sample size eliminates\nthe need for bandwidth selection procedures. The proposed density estimator has\nunitary integral, does not require a constraint to enforce positivity, and is\nconsistent. The proposed approach is validated through numerical examples in\nwhich samples are drawn from known PDFs. The approach is also used to determine\napproximations of (unknown) PDFs associated with outputs of interest that\ndepend on the solution of a stochastic partial differential equation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 06:49:40 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 23:33:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Capodaglio", "Giacomo", ""], ["Gunzburger", "Max", ""]]}, {"id": "1906.10920", "submitter": "Remi Leluc", "authors": "R\\'emi Leluc, Fran\\c{c}ois Portier and Johan Segers", "title": "Control variate selection for Monte Carlo integration", "comments": "Accepted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo integration with variance reduction by means of control variates\ncan be implemented by the ordinary least squares estimator for the intercept in\na multiple linear regression model with the integrand as response and the\ncontrol variates as covariates. Even without special knowledge on the\nintegrand, significant efficiency gains can be obtained if the control variate\nspace is sufficiently large. Incorporating a large number of control variates\nin the ordinary least squares procedure may however result in (i) a certain\ninstability of the ordinary least squares estimator and (ii) a possibly\nprohibitive computation time. Regularizing the ordinary least squares estimator\nby preselecting appropriate control variates via the Lasso turns out to\nincrease the accuracy without additional computational cost. The findings in\nthe numerical experiment are confirmed by concentration inequalities for the\nintegration error.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 09:00:32 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 10:34:30 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 11:21:51 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 12:55:51 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 15:54:51 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Leluc", "R\u00e9mi", ""], ["Portier", "Fran\u00e7ois", ""], ["Segers", "Johan", ""]]}, {"id": "1906.10921", "submitter": "Azadeh Khaleghi", "authors": "Azadeh Khaleghi and Daniil Ryabko", "title": "Clustering piecewise stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of time-series clustering is considered in the case where each\ndata-point is a sample generated by a piecewise stationary ergodic process.\nStationary processes are perhaps the most general class of processes considered\nin non-parametric statistics and allow for arbitrary long-range dependence\nbetween variables. Piecewise stationary processes studied here for the first\ntime in the context of clustering, relax the last remaining assumption in this\nmodel: stationarity. A natural formulation is proposed for this problem and a\nnotion of consistency is introduced which requires the samples to be placed in\nthe same cluster if and only if the piecewise stationary distributions that\ngenerate them have the same set of stationary distributions. Simple,\ncomputationally efficient algorithms are proposed and are shown to be\nconsistent without any additional assumptions beyond piecewise stationarity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 09:01:29 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Khaleghi", "Azadeh", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1906.10923", "submitter": "Marta Ferreira", "authors": "Helena Ferreira, Marta Ferreira, Lu\\'is A. Alexandre", "title": "A crossinggram for random fields on lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of risk situations that occur in a space-time framework can be\ndone using max-stable random fields on lattices. Although the summary\ncoefficients for the spatial and temporal behaviour do not characterize the\nfinite-dimensional distributions of the random field, they have the advantage\nof being immediate to interpret and easier to estimate. The coefficients that\nwe propose give us information about the tendency of a random field for local\noscillations of its values in relation to real valued high levels. It is not\nthe magnitude of the oscillations that is being evaluated, but rather the\ngreater or lesser number of oscillations, that is, the tendency of the\ntrajectories to oscillate. We can observe surface trajectories more smooth over\na region according to higher crossinggram value. It takes value in $[0,1]$ and\nincreases with the concordance of the variables of the random field.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 09:07:09 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 13:37:45 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""], ["Alexandre", "Lu\u00eds A.", ""]]}, {"id": "1906.10951", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "The Rescaled Polya Urn: local reinforcement and chi-squared goodness of\n  fit test", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent studies of big samples, this work aims at constructing a\nparametric model which is characterized by the following features: (i) a\n\"local\" reinforcement, i.e. a reinforcement mechanism mainly based on the last\nobservations, (ii) a random persistent fluctuation of the predictive mean, and\n(iii) a long-term convergence of the empirical mean to a deterministic limit,\ntogether with a chi-squared goodness of fit result. This triple purpose has\nbeen achieved by the introduction of a new variant of the Eggenberger-Polya\nurn, that we call the \"Rescaled\" Polya urn. We provide a complete asymptotic\ncharacterization of this model, pointing out that, for a certain choice of the\nparameters, it has properties different from the ones typically exhibited from\nthe other urn models in the literature. Therefore, beyond the possible\nstatistical application, this work could be interesting for those who are\nconcerned with stochastic processes with reinforcement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:16:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:47:22 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "1906.10967", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine, Jos\\'ea Rasoafaraniaina, Thomas Verdebout", "title": "Preliminary test estimation in ULAN models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preliminary test estimation, which is a natural procedure when it is\nsuspected a priori that the parameter to be estimated might take value in a\nsubmodel of the model at hand, is a classical topic in estimation theory. In\nthe present paper, we establish general results on the asymptotic behavior of\npreliminary test estimators. More precisely, we show that, in uniformly locally\nasymptotically normal (ULAN) models, a general asymptotic theory can be derived\nfor preliminary test estimators based on estimators admitting generic\nBahadur-type representations. This allows for a detailed comparison between\nclassical estimators and preliminary test estimators in ULAN models. Our\nresults, that, in standard linear regression models, are shown to reduce to\nsome classical results, are also illustrated in more modern and involved\nsetups, such as the multisample one where $m$ covariance matrices\n${\\pmb\\Sigma}_1, \\ldots, {\\pmb\\Sigma}_m$ are to be estimated when it is\nsuspected that these matrices might be equal, might be proportional, or might\nshare a common \"scale\". Simulation results confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:50:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Paindaveine", "Davy", ""], ["Rasoafaraniaina", "Jos\u00e9a", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1906.11020", "submitter": "Bardia Panahbehagh Ph.D.", "authors": "Bardia Panahbehagh, Rainer Bruggemann and Mohammad M. Salehi", "title": "Sampling of multiple variables based on partial order set theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is going to introduce a new method for ranked set sampling with\nmultiple criteria. The method is based on a version of ranked set sampling,\nintroduced by Panahbehagh et al. (2017), which relaxes the restriction of\nselecting just one individual variable from each ranked set. Under the new\nmethod for ranking, elements are ranked in sets based on linear extensions in\npartial order sets theory, where based on all the variables simultaneously.\nResults will be evaluated by some simulations and two real case study on\neconomical, medicinal use of flowers and the pollution of herb-layer by Lead,\nCadmium, Zinc and Sulfur in regions in the southwest of Germany.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:19:48 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:05:44 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Panahbehagh", "Bardia", ""], ["Bruggemann", "Rainer", ""], ["Salehi", "Mohammad M.", ""]]}, {"id": "1906.11043", "submitter": "Anne Sabourin", "authors": "Holger Drees, Anne Sabourin (LTCI)", "title": "Principal Component Analysis for Multivariate Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first order behavior of multivariate heavy-tailed random vectors above\nlarge radial thresholds is ruled by a limit measure in a regular variation\nframework. For a high dimensional vector, a reasonable assumption is that the\nsupport of this measure is concentrated on a lower dimensional subspace,\nmeaning that certain linear combinations of the components are much likelier to\nbe large than others. Identifying this subspace and thus reducing the dimension\nwill facilitate a refined statistical analysis. In this work we apply Principal\nComponent Analysis (PCA) to a re-scaled version of radially thresholded\nobservations. Within the statistical learning framework of empirical risk\nminimization, our main focus is to analyze the squared reconstruction error for\nthe exceedances over large radial thresholds. We prove that the empirical risk\nconverges to the true risk, uniformly over all projection subspaces. As a\nconsequence, the best projection subspace is shown to converge in probability\nto the optimal one, in terms of the Hausdorff distance between their\nintersections with the unit sphere. In addition, if the exceedances are\nre-scaled to the unit ball, we obtain finite sample uniform guarantees to the\nreconstruction error pertaining to the estimated projection sub-space.\nNumerical experiments illustrate the relevance of the proposed framework for\npractical purposes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:44:54 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Drees", "Holger", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1906.11291", "submitter": "Xinran Li", "authors": "Xinran Li and Peng Ding", "title": "Rerandomization and Regression Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization is a basis for the statistical inference of treatment effects\nwithout strong assumptions on the outcome-generating process. Appropriately\nusing covariates further yields more precise estimators in randomized\nexperiments. R. A. Fisher suggested blocking on discrete covariates in the\ndesign stage or conducting analysis of covariance (ANCOVA) in the analysis\nstage. We can embed blocking into a wider class of experimental design called\nrerandomization, and extend the classical ANCOVA to more general regression\nadjustment. Rerandomization trumps complete randomization in the design stage,\nand regression adjustment trumps the simple difference-in-means estimator in\nthe analysis stage. It is then intuitive to use both rerandomization and\nregression adjustment. Under the randomization-inference framework, we\nestablish a unified theory allowing the designer and analyzer to have access to\ndifferent sets of covariates. We find that asymptotically (a) for any given\nestimator with or without regression adjustment, rerandomization never hurts\neither the sampling precision or the estimated precision, and (b) for any given\ndesign with or without rerandomization, our regression-adjusted estimator never\nhurts the estimated precision. Therefore, combining rerandomization and\nregression adjustment yields better coverage properties and thus improves\nstatistical inference. To theoretically quantify these statements, we discuss\noptimal regression-adjusted estimators in terms of the sampling precision and\nthe estimated precision, and then measure the additional gains of the designer\nand the analyzer. We finally suggest using rerandomization in the design and\nregression adjustment in the analysis followed by the Huber--White robust\nstandard error.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 18:33:00 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 01:28:49 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""]]}, {"id": "1906.11293", "submitter": "Xavier D'Haultf{\\oe}uille", "authors": "Laurent Davezies, Xavier D'Haultfoeuille and Yannick Guyonvarch", "title": "Empirical Process Results for Exchangeable Arrays", "comments": "main paper until page 23, then supplement. The paper supersedes our\n  previous paper \"Asymptotic results under multiway clustering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable arrays are natural tools to model common forms of dependence\nbetween units of a sample. Jointly exchangeable arrays are well suited to\ndyadic data, where observed random variables are indexed by two units from the\nsame population. Examples include trade flows between countries or\nrelationships in a network. Separately exchangeable arrays are well suited to\nmultiway clustering, where units sharing the same cluster (e.g. geographical\nareas or sectors of activity when considering individual wages) may be\ndependent in an unrestricted way. We prove uniform laws of large numbers and\ncentral limit theorems for such exchangeable arrays. We obtain these results\nunder the same moment restrictions and conditions on the class of functions as\nthose typically assumed with i.i.d. data. We also show the convergence of\nbootstrap processes adapted to such arrays.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:55:24 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 14:45:22 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 16:27:10 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 05:56:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Davezies", "Laurent", ""], ["D'Haultfoeuille", "Xavier", ""], ["Guyonvarch", "Yannick", ""]]}, {"id": "1906.11300", "submitter": "Phil Long", "authors": "Peter L. Bartlett, Philip M. Long, G\\'abor Lugosi and Alexander\n  Tsigler", "title": "Benign Overfitting in Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of benign overfitting is one of the key mysteries uncovered by\ndeep learning methodology: deep neural networks seem to predict well, even with\na perfect fit to noisy training data. Motivated by this phenomenon, we consider\nwhen a perfect fit to training data in linear regression is compatible with\naccurate prediction. We give a characterization of linear regression problems\nfor which the minimum norm interpolating prediction rule has near-optimal\nprediction accuracy. The characterization is in terms of two notions of the\neffective rank of the data covariance. It shows that overparameterization is\nessential for benign overfitting in this setting: the number of directions in\nparameter space that are unimportant for prediction must significantly exceed\nthe sample size. By studying examples of data covariance properties that this\ncharacterization shows are required for benign overfitting, we find an\nimportant role for finite-dimensional data: the accuracy of the minimum norm\ninterpolating prediction rule approaches the best possible accuracy for a much\nnarrower range of properties of the data distribution when the data lies in an\ninfinite dimensional space versus when the data lies in a finite dimensional\nspace whose dimension grows faster than the sample size.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:09:56 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 00:04:53 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 21:01:57 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Long", "Philip M.", ""], ["Lugosi", "G\u00e1bor", ""], ["Tsigler", "Alexander", ""]]}, {"id": "1906.11320", "submitter": "Silvia Lavagnini", "authors": "Fred Espen Benth and Silvia Lavagnini", "title": "Correlators of Polynomial Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.CP q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of polynomial jump-diffusion dynamics, we provide an explicit\nformula for computing correlators, namely, cross-moments of the process at\ndifferent time points along its path. The formula appears as a linear\ncombination of exponentials of the generator matrix, extending the well-known\nmoment formula for polynomial processes. The developed framework can, for\nexample, be applied in financial pricing, such as for path-dependent options\nand in a stochastic volatility models context. In applications to options,\nhaving closed and compact formulations is attractive for sensitivity analysis\nand risk management, since Greeks can be derived explicitly.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:57:59 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 15:55:59 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 15:44:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Benth", "Fred Espen", ""], ["Lavagnini", "Silvia", ""]]}, {"id": "1906.11364", "submitter": "Daren Wang", "authors": "Daren Wang, Kevin Lin, and Rebecca Willett", "title": "Statistically and Computationally Efficient Change Point Localization in\n  Regression Settings", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting when the underlying distribution changes from the observed time\nseries is a fundamental problem arising in a broad spectrum of applications.\nChange point localization is particularly challenging when we only observe\nlow-dimensional projections of high-dimensional random variables. Specifically,\nwe assume we observe $\\{ x_t, y_t\\}_{t=1}^n$ where $ \\{ x_t\\}_{t=1}^n$ are\n$p$-dimensional covariates, $\\{y_t\\}_{t=1}^n $ are the univariate responses\nsatisfying $E(y_t)=x_t^\\top \\beta_t^*\\text{ for all }1\\let\\le n$ and that\n$\\{\\beta_t^*\\}_{t=1}^n $ are the unobserved regression parameters that change\nover time in a piecewise constant manner. We first propose a novel algorithm\ncalled Binary Segmentation through Estimated CUSUM statistics (BSE), which\ncomputes the change points through direct estimates of the CUSUM statistics of\n$\\{\\beta_t^*\\}_{t=1}^n $. We show that BSE can consistently estimate the\nunknown location of the change points, achieving error bounds of order $O\n(\\log(p)/n) $. To the best of our knowledge, this is a significant improvement,\nas the state-of-the-art methods are only shown to achieve error bounds of order\n$O(\\log(p)/\\sqrt n)$ in the multiple change point setting. However, BSE can be\ncomputationally costly. To overcome this limitation, we introduce another\nalgorithm called Binary Segmentation through Lasso Estimators (BSLE). We show\nthat BSLE can consistently localize change points with a slightly worse\nlocalization error rate compared to BSE, but BSLE is much more computationally\nefficient. Finally, we leverage the insights gained from BSE to develop a novel\n\"local screening\" algorithm that can input a coarse estimate of change point\nlocations together with the observed data and efficiently refine that estimate,\nallowing us to improve the practical performance of past estimators. We also\njustify our theoretical finding in simulated experiments.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 22:21:56 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wang", "Daren", ""], ["Lin", "Kevin", ""], ["Willett", "Rebecca", ""]]}, {"id": "1906.11366", "submitter": "Jerry Li", "authors": "Yihe Dong, Samuel B. Hopkins, Jerry Li", "title": "Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved\n  Outlier Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two problems in high-dimensional robust statistics: \\emph{robust\nmean estimation} and \\emph{outlier detection}. In robust mean estimation the\ngoal is to estimate the mean $\\mu$ of a distribution on $\\mathbb{R}^d$ given\n$n$ independent samples, an $\\varepsilon$-fraction of which have been corrupted\nby a malicious adversary. In outlier detection the goal is to assign an\n\\emph{outlier score} to each element of a data set such that elements more\nlikely to be outliers are assigned higher scores. Our algorithms for both\nproblems are based on a new outlier scoring method we call QUE-scoring based on\n\\emph{quantum entropy regularization}. For robust mean estimation, this yields\nthe first algorithm with optimal error rates and nearly-linear running time\n$\\widetilde{O}(nd)$ in all parameters, improving on the previous fastest\nrunning time $\\widetilde{O}(\\min(nd/\\varepsilon^6, nd^2))$. For outlier\ndetection, we evaluate the performance of QUE-scoring via extensive experiments\non synthetic and real data, and demonstrate that it often performs better than\npreviously proposed algorithms. Code for these experiments is available at\nhttps://github.com/twistedcubic/que-outlier-detection .\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 22:23:14 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Dong", "Yihe", ""], ["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""]]}, {"id": "1906.11592", "submitter": "Jonathan Rougier", "authors": "Jonathan Rougier and Carey Priebe", "title": "The exact form of the 'Ockham factor' in model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the arguments for maximizing the `evidence' as an algorithm for\nmodel selection. We show, using a new definition of model complexity which we\nterm `flexibility', that maximizing the evidence should appeal to both Bayesian\nand Frequentist statisticians. This is due to flexibility's unique position in\nthe exact decomposition of log-evidence into log-fit minus flexibility. In the\nGaussian linear model, flexibility is asymptotically equal to the Bayesian\nInformation Criterion (BIC) penalty, but we caution against using BIC in place\nof flexibility for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:42:17 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:09:09 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Rougier", "Jonathan", ""], ["Priebe", "Carey", ""]]}, {"id": "1906.11609", "submitter": "Sebastian Mueller", "authors": "Abraham Gutierrez, Sebastian Mueller", "title": "Quality analysis in acyclic production networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production network under examination consists of a number of\nworkstations. Each workstation is a parallel configuration of machines\nperforming the same kind of tasks on a given part. Parts move from one\nworkstation to another and at each workstation a part is assigned randomly to a\nmachine. We assume that the production network is acyclic, that is, a part does\nnot return to a workstation where it previously received service. Furthermore,\nwe assume that the quality of the end product is additive, that is, the sum of\nthe quality contributions of the machines along the production path. The\ncontribution of each machine is modeled by a separate random variable.\n  Our main result is the construction of estimators that allow pairwise and\nmultiple comparison of the means and variances of machines in the same\nworkstation. These comparisons then may lead to the identification of\nunreliable machines. We also discuss the asymptotic distributions of the\nestimators that allow the use of standard statistical tests and decision\nmaking.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:17:28 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 10:25:43 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gutierrez", "Abraham", ""], ["Mueller", "Sebastian", ""]]}, {"id": "1906.11923", "submitter": "Victor-Emmanuel Brunel", "authors": "Marco Avella-Medina and Victor-Emmanuel Brunel", "title": "Differentially private sub-Gaussian location estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of estimating a location parameter with differential\nprivacy guarantees and sub-Gaussian deviations. Recent work in statistics has\nfocused on the study of estimators that achieve sub-Gaussian type deviations\neven for heavy tailed data. We revisit some of these estimators through the\nlens of differential privacy and show that a naive application of the Laplace\nmechanism can lead to sub-optimal results. We design two private algorithms for\nestimating the median that lead to estimators with sub-Gaussian type errors.\nUnlike most existing differentially private median estimators, both algorithms\nare well defined for unbounded random variables that are not even required to\nhave finite moments. We then turn to the problem of sub-Gaussian mean\nestimation and show that under heavy tails natural differentially private\nalternatives lead to strictly worse deviations than their non-private\nsub-Gaussian counterparts. This is in sharp contrast with recent results that\nshow that from an asymptotic perspective the cost of differential privacy is\nnegligible.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:21:30 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Avella-Medina", "Marco", ""], ["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1906.12072", "submitter": "Yohann de Castro", "authors": "J.-M. Aza\\\"is and Y. De Castro", "title": "Multiple Testing and Variable Selection along Least Angle Regression's\n  path", "comments": "62 pages; new: FDR control and power comparison between Knockoff,\n  FCD, Slope and our proposed method; new: the introduction has been revised\n  and now present a synthetic presentation of the main results. We believe that\n  this introduction brings new insists compared to previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate multiple testing and variable selection using\nLeast Angle Regression (LARS) algorithm in high dimensions under the Gaussian\nnoise assumption. LARS is known to produce a piecewise affine solutions path\nwith change points referred to as knots of the LARS path. The cornerstone of\nthe present work is the expression in closed form of the exact joint law of\nK-uplets of knots conditional on the variables selected by LARS, namely the\nso-called post-selection joint law of the LARS knots. Numerical experiments\ndemonstrate the perfect fit of our finding.\n  Our main contributions are three fold. First, we build testing procedures on\nvariables entering the model along the LARS path in the general design case\nwhen the noise level can be unknown. This testing procedures are referred to as\nthe Generalized t-Spacing tests (GtSt) and we prove that they have exact\nnon-asymptotic level (i.e., Type I error is exactly controlled). In that way,\nwe extend a work from (Taylor et al., 2014) where the Spacing test works for\nconsecutive knots and known variance. Second, we introduce a new exact multiple\nfalse negatives test after model selection in the general design case when the\nnoise level can be unknown. We prove that this testing procedure has exact\nnon-asymptotic level for general design and unknown noise level. Last, we give\nan exact control of the false discovery rate (FDR) under orthogonal design\nassumption. Monte-Carlo simulations and a real data experiment are provided to\nillustrate our results in this case. Of independent interest, we introduce an\nequivalent formulation of LARS algorithm based on a recursive function.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:43:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 22:50:10 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 12:16:42 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Aza\u00efs", "J. -M.", ""], ["De Castro", "Y.", ""]]}, {"id": "1906.12125", "submitter": "Richard Samworth", "authors": "Ziwei Zhu, Tengyao Wang and Richard J. Samworth", "title": "High-dimensional principal component analysis with heterogeneous\n  missingness", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional Principal Component Analysis (PCA)\nwith missing observations. In simple, homogeneous missingness settings with a\nnoise level of constant order, we show that an existing inverse-probability\nweighted (IPW) estimator of the leading principal components can (nearly)\nattain the minimax optimal rate of convergence. However, deeper investigation\nreveals both that, particularly in more realistic settings where the\nmissingness mechanism is heterogeneous, the empirical performance of the IPW\nestimator can be unsatisfactory, and moreover that, in the noiseless case, it\nfails to provide exact recovery of the principal components. Our main\ncontribution, then, is to introduce a new method for high-dimensional PCA,\ncalled `primePCA', that is designed to cope with situations where observations\nmay be missing in a heterogeneous manner. Starting from the IPW estimator,\nprimePCA iteratively projects the observed entries of the data matrix onto the\ncolumn space of our current estimate to impute the missing entries, and then\nupdates our estimate by computing the leading right singular space of the\nimputed data matrix. It turns out that the interaction between the\nheterogeneity of missingness and the low-dimensional structure is crucial in\ndetermining the feasibility of the problem. We therefore introduce an\nincoherence condition on the principal components and prove that in the\nnoiseless case, the error of primePCA converges to zero at a geometric rate\nwhen the signal strength is not too small. An important feature of our\ntheoretical guarantees is that they depend on average, as opposed to\nworst-case, properties of the missingness mechanism. Our numerical studies on\nboth simulated and real data reveal that primePCA exhibits very encouraging\nperformance across a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:37:16 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhu", "Ziwei", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1906.12208", "submitter": "Junmo Song", "authors": "Junmo Song", "title": "Robust test for dispersion parameter change in discretely observed\n  diffusion processes", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of testing for dispersion parameter change\nin discretely observed diffusion processes when the observations are\ncontaminated by outliers. To lessen the impact of outliers, we first calculate\nresiduals using a robust estimate and then propose a trimmed-residual based\nCUSUM test. The proposed test is shown to converge weakly to a function of the\nBrownian bridge under the null hypothesis of no parameter change. We conduct\nsimulations to evaluate performances of the proposed test in the presence of\noutliers. Numerical results confirm that the proposed test posses a strong\nrobust property against outliers. In real data analysis, we fit the\nOrnstein-Uhlenbeck process to KOSPI200 volatility index data and locate some\nchange points that are not detected by a naive CUSUM test.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:28:07 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Song", "Junmo", ""]]}]