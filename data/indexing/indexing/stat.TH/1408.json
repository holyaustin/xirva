[{"id": "1408.0241", "submitter": "Mathieu Rosenbaum", "authors": "Alexandre Belloni, Mathieu Rosenbaum and Alexandre Tsybakov", "title": "Linear and Conic Programming Estimators in High-Dimensional\n  Errors-in-variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression model with observation error in the design.\nIn this setting, we allow the number of covariates to be much larger than the\nsample size. Several new estimation methods have been recently introduced for\nthis model. Indeed, the standard Lasso estimator or Dantzig selector turn out\nto become unreliable when only noisy regressors are available, which is quite\ncommon in practice. We show in this work that under suitable sparsity\nassumptions, the procedure introduced in Rosenbaum and Tsybakov (2013) is\nalmost optimal in a minimax sense and, despite non-convexities, can be\nefficiently computed by a single linear programming problem. Furthermore, we\nprovide an estimator attaining the minimax efficiency bound. This estimator is\nwritten as a second order cone programming minimisation problem which can be\nsolved numerically in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 17:30:00 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 21:26:44 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 17:34:36 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Rosenbaum", "Mathieu", ""], ["Tsybakov", "Alexandre", ""]]}, {"id": "1408.0324", "submitter": "Peng Ding", "authors": "Peng Ding and Luke Miratrix", "title": "To Adjust or Not to Adjust? Sensitivity Analysis of M-Bias and\n  Butterfly-Bias", "comments": "Journal of Causal Inference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"M-Bias,\" as it is called in the epidemiologic literature, is the bias\nintroduced by conditioning on a pretreatment covariate due to a particular\n\"M-Structure\" between two latent factors, an observed treatment, an outcome,\nand a \"collider.\" This potential source of bias, which can occur even when the\ntreatment and the outcome are not confounded, has been a source of considerable\ncontroversy. We here present formulae for identifying under which circumstances\nbiases are inflated or reduced. In particular, we show that the magnitude of\nM-Bias in linear structural equation models tends to be relatively small\ncompared to confounding bias, suggesting that it is generally not a serious\nconcern in many applied settings. These theoretical results are consistent with\nrecent empirical findings from simulation studies. We also generalize the\nM-Bias setting (1) to allow for the correlation between the latent factors to\nbe nonzero, and (2) to allow for the collider to be a confounder between the\ntreatment and the outcome. These results demonstrate that mild deviations from\nthe M-Structure tend to increase confounding bias more rapidly than M-Bias,\nsuggesting that choosing to condition on any given covariate is generally the\nsuperior choice. As an application, we re-examine a controversial example\nbetween Professors Donald Rubin and Judea Pearl.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 01:31:25 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Ding", "Peng", ""], ["Miratrix", "Luke", ""]]}, {"id": "1408.0361", "submitter": "Aymeric Dieuleveut", "authors": "Aymeric Dieuleveut and Francis Bach", "title": "Non-parametric Stochastic Approximation with Large Step sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the random-design least-squares regression problem within the\nreproducing kernel Hilbert space (RKHS) framework. Given a stream of\nindependent and identically distributed input/output data, we aim to learn a\nregression function within an RKHS $\\mathcal{H}$, even if the optimal predictor\n(i.e., the conditional expectation) is not in $\\mathcal{H}$. In a stochastic\napproximation framework where the estimator is updated after each observation,\nwe show that the averaged unregularized least-mean-square algorithm (a form of\nstochastic gradient), given a sufficient large step-size, attains optimal rates\nof convergence for a variety of regimes for the smoothnesses of the optimal\nprediction function and the functions in $\\mathcal{H}$.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 11:22:52 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 07:05:28 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 00:03:47 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Dieuleveut", "Aymeric", ""], ["Bach", "Francis", ""]]}, {"id": "1408.0412", "submitter": "Yongbum Cho", "authors": "Yongbum Cho, Richard A. Davis, Souvik Ghosh", "title": "Asymptotic Properties of the Empirical Spatial Extremogram", "comments": "30 pages, 6 fugures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremogram, proposed by Davis and Mikosch (2008), is a useful tool for\nmeasuring extremal dependence and checking model adequacy in a time series. We\ndefine the extremogram in the spatial domain when the data is observed on a\nlattice or at locations distributed as a Poisson point process in d-dimensional\nspace. Under mixing and other conditions, we establish a central limit theorem\nfor the empirical spatial extremogram. We show these conditions are applicable\nfor max-moving average processes and Brown-Resnick processes and illustrate the\nempirical extremogram's performance via simulation. We also demonstrate its\npractical use with a data set related to rainfall in a region in Florida.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 20:01:13 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 20:30:57 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Cho", "Yongbum", ""], ["Davis", "Richard A.", ""], ["Ghosh", "Souvik", ""]]}, {"id": "1408.0424", "submitter": "David Gerard", "authors": "David Gerard and Peter Hoff", "title": "Equivariant minimax dominators of the MLE in the array normal model", "comments": null, "journal-ref": "Journal of Multivariate Analysis 137 (2015) 32--49", "doi": "10.1016/j.jmva.2015.01.020", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference about dependencies in a multiway data array can be made using the\narray normal model, which corresponds to the class of multivariate normal\ndistributions with separable covariance matrices. Maximum likelihood and\nBayesian methods for inference in the array normal model have appeared in the\nliterature, but there have not been any results concerning the optimality\nproperties of such estimators. In this article, we obtain results for the array\nnormal model that are analogous to some classical results concerning covariance\nestimation for the multivariate normal model. We show that under a lower\ntriangular product group, a uniformly minimum risk equivariant estimator\n(UMREE) can be obtained via a generalized Bayes procedure. Although this UMREE\nis minimax and dominates the MLE, it can be improved upon via an orthogonally\nequivariant modification. Numerical comparisons of the risks of these\nestimators show that the equivariant estimators can have substantially lower\nrisks than the MLE.\n", "versions": [{"version": "v1", "created": "Sat, 2 Aug 2014 21:40:22 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Gerard", "David", ""], ["Hoff", "Peter", ""]]}, {"id": "1408.0619", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "Simultaneous causal inference for multiple treatments via sufficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some units from a population receive the same treatment that is different\nfrom treatments available for other reservoir populations. The minimal\nsufficient statistic $s$ for the pre-treatment $x$-covariates's distributions\nin the populations is the coarsest balancing score. $s$ is used to select\nmatching units for simultaneous causal comparisons of multiple\ntreatments.Necessary and sufficient conditions on the posterior distribution of\nthe treatment variable (given $x$) determine whether a statistic is either\nsufficient or minimal sufficient for the x-covariates' distributions. Results\nin the literature are thus extended. Strong ignorability of treatment\nassignment given $s(x)$ is also established. Consequently, the expected\ntreatments' differences given $s(x)$ are shown to be simultaneously unbiased\nfor the average causal effects of all treatments' differences. The existing\nstatistical theory for $s$ and its estimates support their use in causal\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 09:26:45 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "1408.0722", "submitter": "Sharif Rahman", "authors": "Sharif Rahman", "title": "A Generalized ANOVA Dimensional Decomposition for Dependent Probability\n  Measures", "comments": "27 pages, 2 figures, accepted SIAM/ASA Journal on Uncertainty\n  Quantification, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores the generalized analysis-of-variance or ANOVA\ndimensional decomposition (ADD) for multivariate functions of dependent random\nvariables. Two notable properties, stemming from weakened annihilating\nconditions, reveal that the component functions of the generalized ADD have\n\\emph{zero} means and are hierarchically orthogonal. By exploiting these\nproperties, a simple, alternative approach is presented to derive a coupled\nsystem of equations that the generalized ADD component functions satisfy. The\ncoupled equations, which subsume as a special case the classical ADD, reproduce\nthe component functions for independent probability measures. To determine the\ncomponent functions of the generalized ADD, a new constructive method is\nproposed by employing measure-consistent, multivariate orthogonal polynomials\nas bases and calculating the expansion coefficients involved from the solution\nof linear algebraic equations. New generalized formulae are presented for the\nsecond-moment characteristics, including triplets of global sensitivity\nindices, for dependent probability distributions. Furthermore, the generalized\nADD leads to extended definitions of effective dimensions, reported in the\ncurrent literature for the classical ADD. Numerical results demonstrate that\nthe correlation structure of random variables can significantly alter the\ncomposition of component functions, producing widely varying global sensitivity\nindices and, therefore, distinct rankings of random variables. An application\nto random eigenvalue analysis demonstrates the usefulness of the proposed\napproximation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 15:57:22 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Rahman", "Sharif", ""]]}, {"id": "1408.0788", "submitter": "Emanuel Ben-David", "authors": "Emanuel Ben-David and Bala Rajaratnam", "title": "The Letac-Massam conjecture and existence of high dimensional Bayes\n  estimators for Graphical Models", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a variety of useful extensions of the Wishart have been\nproposed in the literature for the purposes of studying Markov random\nfields/graphical models. In particular, generalizations of the Wishart,\nreferred to as Type I and Type II Wishart distributions, have been introduced\nby Letac and Massam (\\emph{Annals of Statistics} 2006) and play important roles\nin both frequentist and Bayesian inference for Gaussian graphical models. These\ndistributions have been especially useful in high-dimensional settings due to\nthe flexibility offered by their multiple shape parameters. The domain of In\nthis paper we resolve a long-standing conjecture of Letac and Massam (LM)\nconcerning the domains of the multi-parameters of graphical Wishart type\ndistributions. This conjecture, posed in \\emph{Annals of Statistics}, also\nrelates fundamentally to the existence of Bayes estimators corresponding to\nthese high dimensional priors. To achieve our goal, we first develop novel\ntheory in the context of probabilistic analysis of graphical models. Using\nthese tools, and a recently introduced class of Wishart distributions for\ndirected acyclic graph (DAG) models, we proceed to give counterexamples to the\nLM conjecture, thus completely resolving the problem. Our analysis also\nproceeds to give useful insights on graphical Wishart distributions with\nimplications for Bayesian inference for such models.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 19:51:57 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Ben-David", "Emanuel", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1408.0881", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "Volumes of logistic regression models with applications to model\n  selection", "comments": "Improved the section on volume jumps and added a new volume bound\n  (Theorem 13) for models with generic design matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models with $n$ observations and $q$ linearly-independent\ncovariates are shown to have Fisher information volumes which are bounded below\nby $\\pi^q$ and above by ${n \\choose q} \\pi^q$. This is proved with a novel\ngeneralization of the classical theorems of Pythagoras and de Gua, which is of\nindependent interest. The finding that the volume is always finite is new, and\nit implies that the volume can be directly interpreted as a measure of model\ncomplexity. The volume is shown to be a continuous function of the design\nmatrix $X$ at generic $X$, but to be discontinuous in general. This means that\nmodels with sparse design matrices can be significantly less complex than\nnearby models, so the resulting model-selection criterion prefers sparse\nmodels. This is analogous to the way that $\\ell^1$-regularisation tends to\nprefer sparse model fits, though in our case this behaviour arises\nspontaneously from general principles. Lastly, an unusual topological duality\nis shown to exist between the ideal boundaries of the natural and expectation\nparameter spaces of logistic regression models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 07:29:02 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 06:37:46 GMT"}, {"version": "v3", "created": "Fri, 17 Oct 2014 01:56:04 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1408.0938", "submitter": "Yuta Koike", "authors": "Yuta Koike", "title": "Quadratic covariation estimation of an irregularly observed\n  semimartingale with jumps and noise", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ714 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 3, 1894-1936", "doi": "10.3150/15-BEJ714", "report-no": "IMS-BEJ-BEJ714", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a central limit theorem for a pre-averaged version of the\nrealized covariance estimator for the quadratic covariation of a discretely\nobserved semimartingale with noise. The semimartingale possibly has jumps,\nwhile the observation times show irregularity, non-synchronicity, and some\ndependence on the observed process. It is shown that the observation times'\neffect on the asymptotic distribution of the estimator is only through two\ncharacteristics: the observation frequency and the covariance structure of the\nnoise. This is completely different from the case of the realized covariance in\na pure semimartingale setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 11:59:11 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 09:42:07 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 11:43:18 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Koike", "Yuta", ""]]}, {"id": "1408.1028", "submitter": "Thomas Royen", "authors": "T. Royen", "title": "A simple proof of the Gaussian correlation conjecture extended to\n  multivariate gamma distributions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension of the Gaussian correlation conjecture (GCC) is proved for\nmultivariate gamma distributions (in the sense of Krishnamoorthy and\nParthasarathy). The classical GCC for Gaussian probability measures is obtained\nby the special case with one degree of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 16:41:47 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 17:27:59 GMT"}], "update_date": "2017-04-01", "authors_parsed": [["Royen", "T.", ""]]}, {"id": "1408.1156", "submitter": "Ting Yan", "authors": "Ting Yan, Chenlei Leng, Ji Zhu", "title": "Asymptotics in directed exponential random graph models with an\n  increasing bi-degree sequence", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1343 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 31-57", "doi": "10.1214/15-AOS1343", "report-no": "IMS-AOS-AOS1343", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although asymptotic analyses of undirected network models based on degree\nsequences have started to appear in recent literature, it remains an open\nproblem to study statistical properties of directed network models. In this\npaper, we provide for the first time a rigorous analysis of directed\nexponential random graph models using the in-degrees and out-degrees as\nsufficient statistics with binary as well as continuous weighted edges. We\nestablish the uniform consistency and the asymptotic normality for the maximum\nlikelihood estimate, when the number of parameters grows and only one realized\nobservation of the graph is available. One key technique in the proofs is to\napproximate the inverse of the Fisher information matrix using a simple matrix\nwith high accuracy. Numerical studies confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 01:30:25 GMT"}, {"version": "v2", "created": "Thu, 2 Oct 2014 00:47:07 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2015 14:02:33 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2016 11:42:06 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Yan", "Ting", ""], ["Leng", "Chenlei", ""], ["Zhu", "Ji", ""]]}, {"id": "1408.1234", "submitter": "Tong Zhang", "authors": "Dong Dai and Lei Han and Ting Yang and Tong Zhang", "title": "Bayesian Model Averaging with Exponentiated Least Square Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model averaging problem is to average multiple models to achieve a\nprediction accuracy not much worse than that of the best single model in terms\nof mean squared error. It is known that if the models are misspecified, model\naveraging is superior to model selection. Specifically, let $n$ be the sample\nsize, then the worst case regret of the former decays at a rate of $O(1/n)$\nwhile the worst case regret of the latter decays at a rate of $O(1/\\sqrt{n})$.\nThe recently proposed $Q$-aggregation algorithm \\citep{DaiRigZhang12} solves\nthe model averaging problem with the optimal regret of $O(1/n)$ both in\nexpectation and in deviation; however it suffers from two limitations: (1) for\ncontinuous dictionary, the proposed greedy algorithm for solving\n$Q$-aggregation is not applicable; (2) the formulation of $Q$-aggregation\nappears ad hoc without clear intuition. This paper examines a different\napproach to model averaging by considering a Bayes estimator for deviation\noptimal model averaging by using exponentiated least squares loss. We establish\na primal-dual relationship of this estimator and that of $Q$-aggregation and\npropose new greedy procedures that satisfactorily resolve the above mentioned\nlimitations of $Q$-aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 10:14:39 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 14:08:41 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 11:51:37 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Dai", "Dong", ""], ["Han", "Lei", ""], ["Yang", "Ting", ""], ["Zhang", "Tong", ""]]}, {"id": "1408.1239", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "The Minimum S-Divergence Estimator under Continuous Models: The\n  Basu-Lindsay Approach", "comments": "Pre-Print, 34 pages", "journal-ref": null, "doi": "10.1007/s00362-015-0701-3", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical maximum likelihood based\ntechniques. Recently Ghosh et al. (2013) proposed a general class of divergence\nmeasures for robust statistical inference, named the S-Divergence Family. Ghosh\n(2014) discussed its asymptotic properties for the discrete model of densities.\nIn the present paper, we develop the asymptotic properties of the proposed\nminimum S-Divergence estimators under continuous models. Here we use the\nBasu-Lindsay approach (1994) of smoothing the model densities that, unlike\nprevious approaches, avoids much of the complications of the kernel bandwidth\nselection. Illustrations are presented to support the performance of the\nresulting estimators both in terms of efficiency and robustness through\nextensive simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 10:50:45 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:33:29 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1408.1381", "submitter": "Jos\\'{e} E. Chac\\'{o}n", "authors": "Jos\\'e E. Chac\\'on", "title": "A Population Background for Nonparametric Density-Based Clustering", "comments": "Published at http://dx.doi.org/10.1214/15-STS526 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: substantial text\n  overlap with arXiv:1212.1384", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 518-532", "doi": "10.1214/15-STS526", "report-no": "IMS-STS-STS526", "categories": "math.ST math.CA math.DG math.GT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its popularity, it is widely recognized that the investigation of\nsome theoretical aspects of clustering has been relatively sparse. One of the\nmain reasons for this lack of theoretical results is surely the fact that,\nwhereas for other statistical problems the theoretical population goal is\nclearly defined (as in regression or classification), for some of the\nclustering methodologies it is difficult to specify the population goal to\nwhich the data-based clustering algorithms should try to get close. This paper\naims to provide some insight into the theoretical foundations of clustering by\nfocusing on two main objectives: to provide an explicit formulation for the\nideal population goal of the modal clustering methodology, which understands\nclusters as regions of high density; and to present two new loss functions,\napplicable in fact to any clustering methodology, to evaluate the performance\nof a data-based clustering algorithm with respect to the ideal population goal.\nIn particular, it is shown that only mild conditions on a sequence of density\nestimators are needed to ensure that the sequence of modal clusterings that\nthey induce is consistent.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 19:15:09 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 13:07:27 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1408.1469", "submitter": "Waheed Bajwa", "authors": "Waheed U. Bajwa and Dustin G. Mixon", "title": "A Multiple Hypothesis Testing Approach to Low-Complexity Subspace\n  Unmixing", "comments": "Submitted for journal publication; 33 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace-based signal processing traditionally focuses on problems involving\na few subspaces. Recently, a number of problems in different application areas\nhave emerged that involve a significantly larger number of subspaces relative\nto the ambient dimension. It becomes imperative in such settings to first\nidentify a smaller set of active subspaces that contribute to the observation\nbefore further processing can be carried out. This problem of identification of\na small set of active subspaces among a huge collection of subspaces from a\nsingle (noisy) observation in the ambient space is termed subspace unmixing.\nThis paper formally poses the subspace unmixing problem under the parsimonious\nsubspace-sum (PS3) model, discusses connections of the PS3 model to problems in\nwireless communications, hyperspectral imaging, high-dimensional statistics and\ncompressed sensing, and proposes a low-complexity algorithm, termed marginal\nsubspace detection (MSD), for subspace unmixing. The MSD algorithm turns the\nsubspace unmixing problem for the PS3 model into a multiple hypothesis testing\n(MHT) problem and its analysis in the paper helps control the family-wise error\nrate of this MHT problem at any level $\\alpha \\in [0,1]$ under two random\nsignal generation models. Some other highlights of the analysis of the MSD\nalgorithm include: (i) it is applicable to an arbitrary collection of subspaces\non the Grassmann manifold; (ii) it relies on properties of the collection of\nsubspaces that are computable in polynomial time; and ($iii$) it allows for\nlinear scaling of the number of active subspaces as a function of the ambient\ndimension. Finally, numerical results are presented in the paper to better\nunderstand the performance of the MSD algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 02:44:24 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 01:05:23 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Bajwa", "Waheed U.", ""], ["Mixon", "Dustin G.", ""]]}, {"id": "1408.1532", "submitter": "Allan McRobie", "authors": "Allan McRobie", "title": "An Intuitive Curve-Fit Approach to Probability-Preserving Prediction of\n  Extremes", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is described for predicting extremes values beyond the span of\nhistorical data. The method - based on extending a curve fitted to a location-\nand scale-invariant variation of the double-logarithmic QQ-plot - is simple and\nintuitive, yet it preserves probability to a good approximation. The procedure\nis developed on the Generalised Pareto Distribution (GPD), but is applicable to\nthe upper order statistics of a wide class of distributions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 10:33:21 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["McRobie", "Allan", ""]]}, {"id": "1408.1681", "submitter": "Ankur Moitra", "authors": "Ankur Moitra", "title": "Super-resolution, Extremal Functions and the Condition Number of\n  Vandermonde Matrices", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution is a fundamental task in imaging, where the goal is to\nextract fine-grained structure from coarse-grained measurements. Here we are\ninterested in a popular mathematical abstraction of this problem that has been\nwidely studied in the statistics, signal processing and machine learning\ncommunities. We exactly resolve the threshold at which noisy super-resolution\nis possible. In particular, we establish a sharp phase transition for the\nrelationship between the cutoff frequency ($m$) and the separation ($\\Delta$).\nIf $m > 1/\\Delta + 1$, our estimator converges to the true values at an inverse\npolynomial rate in terms of the magnitude of the noise. And when $m <\n(1-\\epsilon) /\\Delta$ no estimator can distinguish between a particular pair of\n$\\Delta$-separated signals even if the magnitude of the noise is exponentially\nsmall.\n  Our results involve making novel connections between {\\em extremal functions}\nand the spectral properties of Vandermonde matrices. We establish a sharp phase\ntransition for their condition number which in turn allows us to give the first\nnoise tolerance bounds for the matrix pencil method. Moreover we show that our\nmethods can be interpreted as giving preconditioners for Vandermonde matrices,\nand we use this observation to design faster algorithms for super-resolution.\nWe believe that these ideas may have other applications in designing faster\nalgorithms for other basic tasks in signal processing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 18:54:19 GMT"}, {"version": "v2", "created": "Fri, 8 Aug 2014 12:42:01 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2015 20:35:47 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2015 02:18:44 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Moitra", "Ankur", ""]]}, {"id": "1408.1809", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Graphs for margins of Bayesian networks", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, Volume 43, Issue 3, Pages\n  625-920, 2016", "doi": "10.1111/sjos.12194", "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graph (DAG) models, also called Bayesian networks, impose\nconditional independence constraints on a multivariate probability\ndistribution, and are widely used in probabilistic reasoning, machine learning\nand causal inference. If latent variables are included in such a model, then\nthe set of possible marginal distributions over the remaining (observed)\nvariables is generally complex, and not represented by any DAG. Larger classes\nof mixed graphical models, which use multiple edge types, have been introduced\nto overcome this; however, these classes do not represent all the models which\ncan arise as margins of DAGs. In this paper we show that this is because\nordinary mixed graphs are fundamentally insufficiently rich to capture the\nvariety of marginal models.\n  We introduce a new class of hyper-graphs, called mDAGs, and a latent\nprojection operation to obtain an mDAG from the margin of a DAG. We show that\neach distinct marginal of a DAG model is represented by at least one mDAG, and\nprovide graphical results towards characterizing when two such marginal models\nare the same. Finally we show that mDAGs correctly capture the marginal\nstructure of causally-interpreted DAGs under interventions on the observed\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 10:25:15 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 17:17:36 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "1408.2156", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu", "title": "Statistical guarantees for the EM algorithm: From population to\n  sample-based analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for proving rigorous guarantees on the\nperformance of the EM algorithm and a variant known as gradient EM. Our\nanalysis is divided into two parts: a treatment of these algorithms at the\npopulation level (in the limit of infinite data), followed by results that\napply to updates based on a finite set of samples. First, we characterize the\ndomain of attraction of any global maximizer of the population likelihood. This\ncharacterization is based on a novel view of the EM updates as a perturbed form\nof likelihood ascent, or in parallel, of the gradient EM updates as a perturbed\nform of standard gradient ascent. Leveraging this characterization, we then\nprovide non-asymptotic guarantees on the EM and gradient EM algorithms when\napplied to a finite set of samples. We develop consequences of our general\ntheory for three canonical examples of incomplete-data problems: mixture of\nGaussians, mixture of regressions, and linear regression with covariates\nmissing completely at random. In each case, our theory guarantees that with a\nsuitable initialization, a relatively small number of EM (or gradient EM) steps\nwill yield (with high probability) an estimate that is within statistical error\nof the MLE. We provide simulations to confirm this theoretically predicted\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 21:40:15 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1408.2287", "submitter": "Cael Hasse", "authors": "Cael L. Hasse", "title": "In principle determination of generic priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability theory as extended logic is completed such that essentially any\nprobability may be determined. This is done by considering propositional logic\n(as opposed to predicate logic) as syntactically suffcient and imposing a\nsymmetry from propositional logic. It is shown how the notions of `possibility'\nand `property' may be suffciently represented in propositional logic such that\n1) the principle of indifference drops out and becomes essentially combinatoric\nin nature and 2) one may appropriately represent assumptions where one assumes\nthere is a space of possibilities but does not assume the size of the space.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 00:12:46 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Hasse", "Cael L.", ""]]}, {"id": "1408.2417", "submitter": "Dennis Dobler", "authors": "Dennis Dobler", "title": "Direct Bootstrapping and Permuting of Observations fail for\n  Aalen-Johansen Estimators", "comments": "This paper has been withdrawn by the author. The Scandinavian Journal\n  of Statistics article Conditional Studentized Survival Tests for Randomly\n  Censored Models by A. Janssen and C.-D. Mayer (2001) already shows that the\n  analyzed resampling procedures alter the covariance structure in the simpler\n  survival setup, thus letting my results become trivial", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides rigorous proofs that neither Efron's bootstrap nor\npermutation techniques can be applied directly to the observations to construct\nconsistent resampling tests for transition probability matrices of finite-state\nMarkov processes. These methods modify the covariance functions of the limiting\ndistributions of the involved Aalen-Johansen processes, even in the case of\nfully observable individuals. An example for the failure of these resampling\nmethods is given by cumulative incidence functions in competing risks set-ups.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 14:17:31 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 07:25:31 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Dobler", "Dennis", ""]]}, {"id": "1408.2489", "submitter": "Tamas Rudas", "authors": "Tamas Rudas", "title": "Directionally collapsible parameterizations of multivariate binary\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Odds ratios and log-linear parameters are not collapsible, meaning that\nincluding a variable into the analysis or omitting one from it, may change the\nstrength of association among the remaining variables. Even the direction of\nassociation may be reversed, a fact that is often discussed under the name of\nSimpson's paradox. A parameter of association is directionally collapsible, if\nthis reversal cannot occur. The paper investigates the existence of parameters\nof association which are directionally collapsible. It is shown, that subject\nto two simple assumptions, no parameter of association, which depends only on\nthe conditional distributions, like the odds ratio does, can be directionally\ncollapsible. The main result is that every directionally collapsible parameter\nof association gives the same direction of association as a linear contrast of\nthe cell probabilities does. The implication for dealing with Simpson's paradox\nis that there is exactly one way to associate direction with the association in\nany table, so that the paradox never occurs\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 18:48:20 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Rudas", "Tamas", ""]]}, {"id": "1408.2566", "submitter": "Metin Ata", "authors": "Metin Ata, Francisco-Shu Kitaura, Volker M\\\"uller", "title": "Bayesian inference of cosmic density fields from non-linear,\n  scale-dependent, and stochastic biased tracers", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1093/mnras/stu2347", "report-no": null, "categories": "astro-ph.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian reconstruction algorithm to generate unbiased samples\nof the underlying dark matter field from halo catalogues. Our new contribution\nconsists of implementing a non-Poisson likelihood including a deterministic\nnon-linear and scale-dependent bias. In particular we present the Hamiltonian\nequations of motions for the negative binomial (NB) probability distribution\nfunction. This permits us to efficiently sample the posterior distribution\nfunction of density fields given a sample of galaxies using the Hamiltonian\nMonte Carlo technique implemented in the Argo code. We have tested our\nalgorithm with the Bolshoi $N$-body simulation at redshift $z = 0$, inferring\nthe underlying dark matter density field from sub-samples of the halo catalogue\nwith biases smaller and larger than one. Our method shows that we can draw\nclosely unbiased samples (compatible within 1-$\\sigma$) from the posterior\ndistribution up to scales of about $k$~1 h/Mpc in terms of power-spectra and\ncell-to-cell correlations. We find that a Poisson likelihood yields\nreconstructions with power spectra deviating more than 10% at $k$=0.2 h/Mpc.\nOur reconstruction algorithm is especially suited for emission line galaxy data\nfor which a complex non-linear stochastic biasing treatment beyond Poissonity\nbecomes indispensable.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 21:45:55 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 20:54:03 GMT"}, {"version": "v3", "created": "Tue, 21 Oct 2014 15:25:33 GMT"}, {"version": "v4", "created": "Wed, 29 Oct 2014 19:20:33 GMT"}, {"version": "v5", "created": "Wed, 5 Nov 2014 16:08:50 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Ata", "Metin", ""], ["Kitaura", "Francisco-Shu", ""], ["M\u00fcller", "Volker", ""]]}, {"id": "1408.2581", "submitter": "Eric Chicken", "authors": "Senthil B. Girimurugan and Eric Chicken", "title": "Distribution of a Non-parametric Wavelet-based Statistic for Functional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical formulations and proofs for a wavelet based statistic employed\nin functional data analysis is elaborately discussed in this report. The\npropositions and derivations discussed here apply to a wavelet based statistic\nwith hard thresholding. The proposed analytic distribution is made feasible\nonly due to the assumption of normality. Since the statistic is developed for\napplications in high dimensional data analysis, the assumption holds true in\nmost practical situations. In the future, the work here could be extended to\naddress data that are non-Gaussian. Aside from establishing a rigorous\nmathematical foundation for the distribution of the statistic, the report also\nexplores a few approximations for the proposed statistic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 22:56:45 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Girimurugan", "Senthil B.", ""], ["Chicken", "Eric", ""]]}, {"id": "1408.2951", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda and Fumiyasu Komaki", "title": "Singular Value Shrinkage Priors for Bayesian Prediction", "comments": null, "journal-ref": "Biometrika, Volume 102, Issue 4, Pages 843--854, 2015", "doi": "10.1093/biomet/asv036", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop singular value shrinkage priors for the mean matrix parameters in\nthe matrix-variate normal model with known covariance matrices. Our priors are\nsuperharmonic and put more weight on matrices with smaller singular values.\nThey are a natural generalization of the Stein prior. Bayes estimators and\nBayesian predictive densities based on our priors are minimax and dominate\nthose based on the uniform prior in finite samples. In particular, our priors\nwork well when the true value of the parameter has low rank.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 09:13:29 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 15:58:14 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 12:36:29 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Matsuda", "Takeru", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1408.2998", "submitter": "Gesine Reinert", "authors": "Christophe Ley, Gesine Reinert and Yvik Swan", "title": "Stein's method for comparison of univariate distributions", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new general version of Stein's method for univariate\ndistributions. In particular we propose a canonical definition of the Stein\noperator of a probability distribution {which is based on a linear difference\nor differential-type operator}. The resulting Stein identity highlights the\nunifying theme behind the literature on Stein's method (both for continuous and\ndiscrete distributions). Viewing the Stein operator as an operator acting on\npairs of functions, we provide an extensive toolkit for distributional\ncomparisons. Several abstract approximation theorems are provided. Our approach\nis illustrated for comparison of several pairs of distributions : normal vs\nnormal, sums of independent Rademacher vs normal, normal vs Student, and\nmaximum of random variables vs exponential, Frechet and Gumbel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 13:04:55 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 14:39:34 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 15:31:47 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Ley", "Christophe", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "1408.3100", "submitter": "Manfred K. Warmuth", "authors": "Manfred K. Warmuth, Dima Kuzmin", "title": "A Bayesian Probability Calculus for Density Matrices", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-503-511", "categories": "quant-ph cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main concepts in quantum physics is a density matrix, which is a\nsymmetric positive definite matrix of trace one. Finite probability\ndistributions are a special case where the density matrix is restricted to be\ndiagonal. Density matrices are mixtures of dyads, where a dyad has the form uu'\nfor any any unit column vector u. These unit vectors are the elementary events\nof the generalized probability space. Perhaps the simplest case to see that\nsomething unusual is going on is the case of uniform density matrix, i.e. 1/n\ntimes identity. This matrix assigns probability 1/n to every unit vector, but\nof course there are infinitely many of them. The new normalization rule thus\nsays that sum of probabilities over any orthonormal basis of directions is one.\nWe develop a probability calculus based on these more general distributions\nthat includes definitions of joints, conditionals and formulas that relate\nthese, i.e. analogs of the theorem of total probability, various Bayes rules\nfor the calculation of posterior density matrices, etc. The resulting calculus\nparallels the familiar 'classical' probability calculus and always retains the\nlatter as a special case when all matrices are diagonal.\n  Whereas the classical Bayesian methods maintain uncertainty about which model\nis 'best', the generalization maintains uncertainty about which unit direction\nhas the largest variance. Surprisingly the bounds also generalize: as in the\nclassical setting we bound the negative log likelihood of the data by the\nnegative log likelihood of the MAP estimator.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 05:20:28 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Warmuth", "Manfred K.", ""], ["Kuzmin", "Dima", ""]]}, {"id": "1408.3169", "submitter": "Jan Leike", "authors": "Jan Leike and Marcus Hutter", "title": "Indefinitely Oscillating Martingales", "comments": "ALT 2014, extended technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We construct a class of nonnegative martingale processes that oscillate\nindefinitely with high probability. For these processes, we state a uniform\nrate of the number of oscillations and show that this rate is asymptotically\nclose to the theoretical upper bound. These bounds on probability and\nexpectation of the number of upcrossings are compared to classical bounds from\nthe martingale literature. We discuss two applications. First, our results\nimply that the limit of the minimum description length operator may not exist.\nSecond, we give bounds on how often one can change one's belief in a given\nhypothesis when observing a stream of data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 00:19:03 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1408.3205", "submitter": "Ce Shi", "authors": "Ce Shi, Yu Tang, Jianxing Yin", "title": "Optimum mixed level detecting arrays", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1228 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1546-1563", "doi": "10.1214/14-AOS1228", "report-no": "IMS-AOS-AOS1228", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a type of search design, a detecting array can be used to generate test\nsuites to identify and detect faults caused by interactions of factors in a\ncomponent-based system. Recently, the construction and optimality of detecting\narrays have been investigated in depth in the case where all the factors are\nassumed to have the same number of levels. However, for real world\napplications, it is more desirable to use detecting arrays in which the various\nfactors may have different numbers of levels. This paper gives a general\ncriterion to measure the optimality of a mixed level detecting array in terms\nof its size. Based on this optimality criterion, the combinatorial\ncharacteristics of mixed level detecting arrays of optimum size are\ninvestigated. This enables us to construct optimum mixed level detecting arrays\nwith a heuristic optimization algorithm and combinatorial methods. As a result,\nsome existence results for optimum mixed level detecting arrays achieving a\nlower bound are provided for practical use.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 07:37:13 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Shi", "Ce", ""], ["Tang", "Yu", ""], ["Yin", "Jianxing", ""]]}, {"id": "1408.3221", "submitter": "Efang Kong", "authors": "Efang Kong, Yingcun Xia", "title": "An adaptive composite quantile approach to dimension reduction", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1242 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1657-1688", "doi": "10.1214/14-AOS1242", "report-no": "IMS-AOS-AOS1242", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction [J. Amer. Statist. Assoc. 86 (1991) 316-342]\nhas long been a prominent issue in multivariate nonparametric regression\nanalysis. To uncover the central dimension reduction space, we propose in this\npaper an adaptive composite quantile approach. Compared to existing methods,\n(1) it requires minimal assumptions and is capable of revealing all dimension\nreduction directions; (2) it is robust against outliers and (3) it is\nstructure-adaptive, thus more efficient. Asymptotic results are proved and\nnumerical examples are provided, including a real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 08:52:10 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Kong", "Efang", ""], ["Xia", "Yingcun", ""]]}, {"id": "1408.3362", "submitter": "Rajesh Singh", "authors": "Hemant K.Verma, Rajesh Singh and Florentin Smarandache", "title": "A family of median based estimators in simple random sampling", "comments": "9 pages, 4 tables; The efficient use of supplementary information in\n  finite population sampling. Education Publishing, USA, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed a median based estimator using known value of\nsome population parameter(s) in simple random sampling. Various existing\nestimators are shown particular members of the proposed estimator. The bias and\nmean squared error of the proposed estimator is obtained up to the first order\nof approximation under simple random sampling without replacement. An empirical\nstudy is carried out to judge the superiority of proposed estimator over\nothers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 05:57:11 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Verma", "Hemant K.", ""], ["Singh", "Rajesh", ""], ["Smarandache", "Florentin", ""]]}, {"id": "1408.3386", "submitter": "Marianna Pensky", "authors": "Marianna Pensky", "title": "Solution of linear ill-posed problems using overcomplete dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. Construction of an\nadaptive optimal solution for such problems usually relies either on a singular\nvalue decomposition or representation of the solution via an orthonormal basis.\nThe shortcoming of both approaches lies in the fact that, in many situations,\nneither the eigenbasis of the linear operator nor a standard orthonormal basis\nconstitutes an appropriate collection of functions for sparse representation of\nthe unknown function. In the context of regression problems, there have been an\nenormous amount of effort to recover an unknown function using an overcomplete\ndictionary. One of the most popular methods, Lasso, is based on minimizing the\nempirical likelihood and requires stringent assumptions on the dictionary, the,\nso called, compatibility conditions. While these conditions may be satisfied\nfor the original dictionary functions, they usually do not hold for their\nimages due to contraction imposed by the linear operator. In what follows, we\nbypass this difficulty by a novel approach which is based on inverting each of\nthe dictionary functions and matching the resulting expansion to the true\nfunction, thus, avoiding unrealistic assumptions on the dictionary and using\nLasso in a predictive setting. We examine both the white noise and the\nobservational model formulations and also discuss how exact inverse images of\nthe dictionary functions can be replaced by their approximate counterparts.\nFurthermore, we show how the suggested methodology can be extended to the\nproblem of estimation of a mixing density in a continuous mixture. For all the\nsituations listed above, we provide the oracle inequalities for the risk in a\nfinite sample setting. Simulation studies confirm good computational properties\nof the Lasso-based technique.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 19:00:11 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 17:53:01 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Pensky", "Marianna", ""]]}, {"id": "1408.3388", "submitter": "Mun-Chol Kim", "authors": "Kyong-Hui Kim, Myong-Guk Sin, Ok-Kyong Kim", "title": "A goodness-of-fit test of the errors in nonlinear autoregressive time\n  series models with stationary $\\alpha$-mixing error terms", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": "KISU-MATH-2014-E-R-003", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we deal with the problem of fitting an error density to the\ngoodness-of-fit test of the errors in nonlinear autoregressive time series\nmodels with stationary $\\alpha$-mixing error terms. The test statistic is based\non the integrated squared error of the nonparametric error density estimate and\nthe null error density. By deriving the asymptotic normality of test statistics\nin these models, we extend the result of Cheng and Sun (Statist. Probab. Lett.\n\\textbf{78}, 1(2008), 50-59) in the model with i.i.d error terms to the more\ngeneral case.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 07:04:44 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Kim", "Kyong-Hui", ""], ["Sin", "Myong-Guk", ""], ["Kim", "Ok-Kyong", ""]]}, {"id": "1408.3525", "submitter": "Kamil Jurczak", "authors": "Kamil Jurczak", "title": "The critical threshold level on Kendall's tau statistic concerning\n  minimax estimation of sparse correlation matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sparse high-dimensional elliptical model we consider a hard threshold\nestimator for the correlation matrix based on Kendall's tau with threshold\nlevel $\\alpha(\\frac{\\log p}{n})^{1/2}$. Parameters $\\alpha$ are identified such\nthat the threshold estimator achieves the minimax rate under the squared\nFrobenius norm and the squared spectral norm. This allows a reasonable\ncalibration of the estimator without any quantitative information about the\ntails of the underlying distribution. For Gaussian observations we even\nestablish a critical threshold constant $\\alpha^\\ast$ under the squared\nFrobenius norm, i.e. the proposed estimator attains the minimax rate for\n$\\alpha>\\alpha^\\ast$ but in general not for $\\alpha<\\alpha^\\ast$. To the best\nof the author's knowledge this is the first work concerning critical threshold\nconstants. The main ingredient to provide the critical threshold level is a\nsharp large deviation expansion for Kendall's tau sample correlation evolved\nfrom an asymptotic expansion of the number of permutations with a certain\nnumber of inversions.\n  The investigation of this paper also covers further statistical problems like\nthe estimation of the latent correlation matrix in the transelliptical and\nnonparanormal family.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 13:00:01 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 15:45:13 GMT"}, {"version": "v3", "created": "Mon, 8 Dec 2014 14:40:08 GMT"}, {"version": "v4", "created": "Wed, 26 Aug 2015 07:07:54 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Jurczak", "Kamil", ""]]}, {"id": "1408.3536", "submitter": "Timothy Armstrong", "authors": "Timothy Armstrong", "title": "Adaptive testing on a regression function at a point", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1342 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 2086-2101", "doi": "10.1214/15-AOS1342", "report-no": "IMS-AOS-AOS1342", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference on a regression function at a point when\nthe entire function satisfies a sign or shape restriction under the null. We\npropose a test that achieves the optimal minimax rate adaptively over a range\nof H\\\"{o}lder classes, up to a $\\log\\log n$ term, which we show to be necessary\nfor adaptation. We apply the results to adaptive one-sided tests for the\nregression discontinuity parameter under a monotonicity restriction, the value\nof a monotone regression function at the boundary and the proportion of true\nnull hypotheses in a multiple testing problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 13:39:28 GMT"}, {"version": "v2", "created": "Sun, 5 Oct 2014 16:09:08 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 16:36:23 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2015 09:33:08 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Armstrong", "Timothy", ""]]}, {"id": "1408.3768", "submitter": "Markus Rei{\\ss}", "authors": "Markus Bibinger, Moritz Jirak, Markus Rei{\\ss}", "title": "Volatility estimation under one-sided errors with applications to limit\n  order books", "comments": "Extended version including empirical example", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a semi-martingale $X_t$, which forms a stochastic boundary, a\nrate-optimal estimator for its quadratic variation $\\langle X, X \\rangle_t$ is\nconstructed based on observations in the vicinity of $X_t$. The problem is\nembedded in a Poisson point process framework, which reveals an interesting\nconnection to the theory of Brownian excursion areas. We derive $n^{-1/3}$ as\noptimal convergence rate in a high-frequency framework with $n$ observations\n(in mean). We discuss a potential application for the estimation of the\nintegrated squared volatility of an efficient price process $X_t$ from\nintra-day order book quotes.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 20:02:34 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 07:20:18 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 13:16:32 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 08:24:47 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Bibinger", "Markus", ""], ["Jirak", "Moritz", ""], ["Rei\u00df", "Markus", ""]]}, {"id": "1408.3979", "submitter": "Holger Drees", "authors": "Holger Drees, Natalie Neumeyer, Leonie Selk", "title": "Hypotheses tests in boundary regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a nonparametric regression model with one-sided errors and\nregression function in a general H\\\"older class. We estimate the regression\nfunction via minimization of the local integral of a polynomial approximation.\nWe show uniform rates of convergence for the simple regression estimator as\nwell as for a smooth version. These rates carry over to mean regression models\nwith a symmetric and bounded error distribution. In such a setting, one obtains\nfaster rates for irregular error distributions concentrating sufficient mass\nnear the endpoints than for the usual regular distributions. The results are\napplied to prove asymptotic $\\sqrt{n}$-equivalence of a residual-based\n(sequential) empirical distribution function to the (sequential) empirical\ndistribution function of unobserved errors in the case of irregular error\ndistributions. This result is remarkably different from corresponding results\nin mean regression with regular errors. It can readily be applied to develop\ngoodness-of-fit tests for the error distribution. We present some examples and\ninvestigate the small sample performance in a simulation study. We further\ndiscuss asymptotically distribution-free hypotheses tests for independence of\nthe error distribution from the points of measurement and for monotonicity of\nthe boundary function as well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 11:26:18 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 15:27:04 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Drees", "Holger", ""], ["Neumeyer", "Natalie", ""], ["Selk", "Leonie", ""]]}, {"id": "1408.4045", "submitter": "Soledad Villar", "authors": "Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar\n  Krishnaswamy, Soledad Villar, Rachel Ward", "title": "Relax, no need to round: integrality of clustering formulations", "comments": "30 pages, ITCS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exact recovery conditions for convex relaxations of point cloud\nclustering problems, focusing on two of the most common optimization problems\nfor unsupervised clustering: $k$-means and $k$-median clustering. Motivations\nfor focusing on convex relaxations are: (a) they come with a certificate of\noptimality, and (b) they are generic tools which are relatively parameter-free,\nnot tailored to specific assumptions over the input. More precisely, we\nconsider the distributional setting where there are $k$ clusters in\n$\\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a\nsymmetric distribution within a ball of unit radius. We ask: what is the\nminimal separation distance between cluster centers needed for convex\nrelaxations to exactly recover these $k$ clusters as the optimal integral\nsolution? For the $k$-median linear programming relaxation we show a tight\nbound: exact recovery is obtained given arbitrarily small pairwise separation\n$\\epsilon > 0$ between the balls. In other words, the pairwise center\nseparation is $\\Delta > 2+\\epsilon$. Under the same distributional model, the\n$k$-means LP relaxation fails to recover such clusters at separation as large\nas $\\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get\nexact cluster recovery at center separation $\\Delta > 2\\sqrt2(1+\\sqrt{1/m})$.\nIn contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means\nalgorithm) can fail to recover clusters in this setting; even with arbitrarily\nlarge cluster separation, k-means++ with overseeding by any constant factor\nfails with high probability at exact cluster recovery. To complement the\ntheoretical analysis, we provide an experimental study of the recovery\nguarantees for these various methods, and discuss several open problems which\nthese experiments suggest.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 15:42:16 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:37:34 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 18:07:10 GMT"}, {"version": "v4", "created": "Tue, 10 Feb 2015 16:11:36 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2015 02:11:54 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Bandeira", "Afonso S.", ""], ["Charikar", "Moses", ""], ["Krishnaswamy", "Ravishankar", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1408.4057", "submitter": "Tim Patschkowski", "authors": "Tim Patschkowski, Angelika Rohde", "title": "Adaptation to lowest density regions with application to support\n  recovery", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1366 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 255-287", "doi": "10.1214/15-AOS1366", "report-no": "IMS-AOS-AOS1366", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scheme for locally adaptive bandwidth selection is proposed which\nsensitively shrinks the bandwidth of a kernel estimator at lowest density\nregions such as the support boundary which are unknown to the statistician. In\ncase of a H\\\"{o}lder continuous density, this locally minimax-optimal bandwidth\nis shown to be smaller than the usual rate, even in case of homogeneous\nsmoothness. Some new type of risk bound with respect to a density-dependent\nstandardized loss of this estimator is established. This bound is fully\nnonasymptotic and allows to deduce convergence rates at lowest density regions\nthat can be substantially faster than $n^{-1/2}$. It is complemented by a\nweighted minimax lower bound which splits into two regimes depending on the\nvalue of the density. The new estimator adapts into the second regime, and it\nis shown that simultaneous adaptation into the fastest regime is not possible\nin principle as long as the H\\\"{o}lder exponent is unknown. Consequences on\nplug-in rules for support recovery are worked out in detail. In contrast to\nthose with classical density estimators, the plug-in rules based on the new\nconstruction are minimax-optimal, up to some logarithmic factor.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 16:29:08 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 10:53:20 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2016 13:41:05 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Patschkowski", "Tim", ""], ["Rohde", "Angelika", ""]]}, {"id": "1408.4224", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti, Giulio Caravagna, Loes Olde Loohuis, Alex\n  Graudenzi, Ilya Korsunsky, Giancarlo Mauri, Marco Antoniotti, Bud Mishra", "title": "CAPRI: Efficient Inference of Cancer Progression Models from\n  Cross-sectional Data", "comments": null, "journal-ref": "Bioinformatics 2015: btv296v1-btv296 (2015)", "doi": "10.1093/bioinformatics/btv296", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a novel inference algorithm to effectively solve the cancer\nprogression model reconstruction problem. Our empirical analysis of the\naccuracy and convergence rate of our algorithm, CAncer PRogression Inference\n(CAPRI), shows that it outperforms the state-of-the-art algorithms addressing\nsimilar problems.\n  Motivation: Several cancer-related genomic data have become available (e.g.,\nThe Cancer Genome Atlas, TCGA) typically involving hundreds of patients. At\npresent, most of these data are aggregated in a cross-sectional fashion\nproviding all measurements at the time of diagnosis.Our goal is to infer cancer\nprogression models from such data. These models are represented as directed\nacyclic graphs (DAGs) of collections of selectivity relations, where a mutation\nin a gene A selects for a later mutation in a gene B. Gaining insight into the\nstructure of such progressions has the potential to improve both the\nstratification of patients and personalized therapy choices.\n  Results: The CAPRI algorithm relies on a scoring method based on a\nprobabilistic theory developed by Suppes, coupled with bootstrap and maximum\nlikelihood inference. The resulting algorithm is efficient, achieves high\naccuracy, and has good complexity, also, in terms of convergence properties.\nCAPRI performs especially well in the presence of noise in the data, and with\nlimited sample sizes. Moreover CAPRI, in contrast to other approaches, robustly\nreconstructs different types of confluent trajectories despite irregularities\nin the data.We also report on an ongoing investigation using CAPRI to study\natypical Chronic Myeloid Leukemia, in which we uncovered non trivial\nselectivity relations and exclusivity patterns among key genomic events.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 06:49:06 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 12:57:47 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Ramazzotti", "Daniele", ""], ["Caravagna", "Giulio", ""], ["Loohuis", "Loes Olde", ""], ["Graudenzi", "Alex", ""], ["Korsunsky", "Ilya", ""], ["Mauri", "Giancarlo", ""], ["Antoniotti", "Marco", ""], ["Mishra", "Bud", ""]]}, {"id": "1408.4527", "submitter": "Ksenia Volkova Yu.", "authors": "K. Yu. Volkova", "title": "On asymptotic efficiency of goodness-of-fit tests for the Pareto\n  distribution based on its characterization", "comments": "21 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1405.7210", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new characterization of Pareto distribution and construct\nintegral and supremum type goodness-of-fit tests based on it. Limiting\ndistribution and large deviations of new statistics are described and their\nlocal Bahadur efficiency for parametric alternatives is calculated. Conditions\nof local optimality of new statistics are given.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 05:55:14 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Volkova", "K. Yu.", ""]]}, {"id": "1408.4643", "submitter": "Karim Lounici", "authors": "Vladimir Koltchinskii and Karim Lounici", "title": "Asymptotics and Concentration Bounds for Bilinear Forms of Spectral\n  Projectors of Sample Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X,X_1,\\dots, X_n$ be i.i.d. Gaussian random variables with zero mean and\ncovariance operator $\\Sigma={\\mathbb E}(X\\otimes X)$ taking values in a\nseparable Hilbert space ${\\mathbb H}.$ Let $$ {\\bf r}(\\Sigma):=\\frac{{\\rm\ntr}(\\Sigma)}{\\|\\Sigma\\|_{\\infty}} $$ be the effective rank of $\\Sigma,$ ${\\rm\ntr}(\\Sigma)$ being the trace of $\\Sigma$ and $\\|\\Sigma\\|_{\\infty}$ being its\noperator norm. Let $$\\hat \\Sigma_n:=n^{-1}\\sum_{j=1}^n (X_j\\otimes X_j)$$ be\nthe sample (empirical) covariance operator based on $(X_1,\\dots, X_n).$ The\npaper deals with a problem of estimation of spectral projectors of the\ncovariance operator $\\Sigma$ by their empirical counterparts, the spectral\nprojectors of $\\hat \\Sigma_n$ (empirical spectral projectors). The focus is on\nthe problems where both the sample size $n$ and the effective rank ${\\bf\nr}(\\Sigma)$ are large. This framework includes and generalizes well known\nhigh-dimensional spiked covariance models. Given a spectral projector $P_r$\ncorresponding to an eigenvalue $\\mu_r$ of covariance operator $\\Sigma$ and its\nempirical counterpart $\\hat P_r,$ we derive sharp concentration bounds for\nbilinear forms of empirical spectral projector $\\hat P_r$ in terms of sample\nsize $n$ and effective dimension ${\\bf r}(\\Sigma).$ Building upon these\nconcentration bounds, we prove the asymptotic normality of bilinear forms of\nrandom operators $\\hat P_r -{\\mathbb E}\\hat P_r$ under the assumptions that\n$n\\to \\infty$ and ${\\bf r}(\\Sigma)=o(n).$ In a special case of eigenvalues of\nmultiplicity one, these results are rephrased as concentration bounds and\nasymptotic normality for linear forms of empirical eigenvectors. Other results\ninclude bounds on the bias ${\\mathbb E}\\hat P_r-P_r$ and a method of bias\nreduction as well as a discussion of possible applications to statistical\ninference in high-dimensional principal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 13:20:50 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 13:58:19 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2015 06:25:01 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Koltchinskii", "Vladimir", ""], ["Lounici", "Karim", ""]]}, {"id": "1408.4747", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee and Venugopal. V. Veeravalli", "title": "Data-Efficient Minimax Quickest Change Detection in a Decentralized\n  System", "comments": "Submitted to Sequential Analysis, Aug. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensor network is considered where a sequence of random variables is\nobserved at each sensor. At each time step, a processed version of the\nobservations is transmitted from the sensors to a common node called the fusion\ncenter. At some unknown point in time the distribution of the observations at\nall the sensor nodes changes. The objective is to detect this change in\ndistribution as quickly as possible, subject to constraints on the false alarm\nrate and the cost of observations taken at each sensor. Minimax problem\nformulations are proposed for the above problem. A data-efficient algorithm is\nproposed in which an adaptive sampling strategy is used at each sensor to\ncontrol the cost of observations used before change. To conserve the cost of\ncommunication an occasional binary digit is transmitted from each sensor to the\nfusion center. It is shown that the proposed algorithm is globally\nasymptotically optimal for the proposed formulations, as the false alarm rate\ngoes to zero.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 18:07:28 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Banerjee", "Taposh", ""], ["Veeravalli", "Venugopal. V.", ""]]}, {"id": "1408.4755", "submitter": "Roger Silva Ph.d", "authors": "Marina Muniz, Roger Silva, Rosangela Loschi", "title": "Shannon Entropy and Kullback-Leibler Divergence in Multivariate Log\n  Fundamental Skew-Normal and Related Distributions", "comments": "21 pages", "journal-ref": "Canadian Journal of Statistics, 44(2), 219-237, 2016", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly focuses on studying the Shannon Entropy and\nKullback-Leibler divergence of the multivariate log canonical fundamental\nskew-normal (LCFUSN) and canonical fundamental skew-normal (CFUSN) families of\ndistributions, extending previous works. We relate our results with other well\nknown distributions entropies. As a byproduct, we also obtain the Mutual\nInformation for distributions in these families. Shannon entropy is used to\ncompare models fitted to analyze the USA monthly precipitation data.\nKullback-Leibler divergence is used to cluster regions in Atlantic ocean\naccording to their air humidity level.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 18:48:21 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 18:38:49 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Muniz", "Marina", ""], ["Silva", "Roger", ""], ["Loschi", "Rosangela", ""]]}, {"id": "1408.4908", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael\n  Mitzenmacher", "title": "Theoretical Foundations of Equitability and the Maximal Information\n  Coefficient", "comments": "46 pages, 3 figures, 2 tables. This paper has been subsumed by\n  arXiv:1505.02213 and arXiv:1505.02212. Please cite those papers instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal information coefficient (MIC) is a tool for finding the strongest\npairwise relationships in a data set with many variables (Reshef et al., 2011).\nMIC is useful because it gives similar scores to equally noisy relationships of\ndifferent types. This property, called {\\em equitability}, is important for\nanalyzing high-dimensional data sets.\n  Here we formalize the theory behind both equitability and MIC in the language\nof estimation theory. This formalization has a number of advantages. First, it\nallows us to show that equitability is a generalization of power against\nstatistical independence. Second, it allows us to compute and discuss the\npopulation value of MIC, which we call MIC_*. In doing so we generalize and\nstrengthen the mathematical results proven in Reshef et al. (2011) and clarify\nthe relationship between MIC and mutual information. Introducing MIC_* also\nenables us to reason about the properties of MIC more abstractly: for instance,\nwe show that MIC_* is continuous and that there is a sense in which it is a\ncanonical \"smoothing\" of mutual information. We also prove an alternate,\nequivalent characterization of MIC_* that we use to state new estimators of it\nas well as an algorithm for explicitly computing it when the joint probability\ndensity function of a pair of random variables is known. Our hope is that this\npaper provides a richer theoretical foundation for MIC and equitability going\nforward.\n  This paper will be accompanied by a forthcoming companion paper that performs\nextensive empirical analysis and comparison to other methods and discusses the\npractical aspects of both equitability and the use of MIC and its related\nstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 08:17:13 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 19:08:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2015 19:58:17 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1408.5087", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Philippe Rigollet, Weichen Wang", "title": "Estimation of functionals of sparse covariance matrices", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1357 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2706-2737", "doi": "10.1214/15-AOS1357", "report-no": "IMS-AOS-AOS1357", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional statistical tests often ignore correlations to gain\nsimplicity and stability leading to null distributions that depend on\nfunctionals of correlation matrices such as their Frobenius norm and other\n$\\ell_r$ norms. Motivated by the computation of critical values of such tests,\nwe investigate the difficulty of estimation the functionals of sparse\ncorrelation matrices. Specifically, we show that simple plug-in procedures\nbased on thresholded estimators of correlation matrices are sparsity-adaptive\nand minimax optimal over a large class of correlation matrices. Akin to\nprevious results on functional estimation, the minimax rates exhibit an elbow\nphenomenon. Our results are further illustrated in simulated data as well as an\nempirical study of data arising in financial econometrics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 17:59:32 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 11:02:09 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Rigollet", "Philippe", ""], ["Wang", "Weichen", ""]]}, {"id": "1408.5297", "submitter": "Eric Marchand", "authors": "Tatsuya Kubokawa, \\'Eric Marchand, William E. Strawderman", "title": "On Predictive Density Estimation for Location Families under Integrated\n  $L_2$ and $L_1$ Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our investigation concerns the estimation of predictive densities and a study\nof efficiency as measured by the frequentist risk of such predictive densities\nwith integrated $L_2$ and $L_1$ losses. Our findings relate to a $p-$variate\nspherically symmetric observable $X \\sim p_X(\\|x-\\mu\\|^2)$ and the objective of\nestimating the density of $Y \\sim q_Y(\\|y-\\mu\\|^2)$ based on $X$. For $L_2$\nloss, we describe Bayes estimation, minimum risk equivariant estimation (MRE),\nand minimax estimation. We focus on the risk performance of the benchmark\nminimum risk equivariant estimator, plug-in estimators, and plug-in type\nestimators with expanded scale. For the multivariate normal case, we make use\nof a duality result with a point estimation problem bringing into play\nreflected normal loss. In three of more dimensions (i.e., $p \\geq 3$), we show\nthat the MRE estimator is inadmissible under $L_2$ loss and provide dominating\nestimators. This brings into play Stein-type results for estimating a\nmultivariate normal mean with a loss which is a concave and increasing function\nof $\\|\\hat{\\mu}-\\mu\\|^2$. We also study the phenomenon of improvement on the\nplug-in density estimator of the form $q_Y(\\|y-aX\\|^2)\\,, 0<a \\leq 1\\,,$ by a\nsubclass of scale expansions $\\frac{1}{c^p} \\, q_Y(\\|(y -aX)/c \\|^2)$ with\n$c>1$, showing in some cases, inevitably for large enough $p$, that all choices\n$c>1$ are dominating estimators. Extensions are obtained for scale mixture of\nnormals including a general inadmissibility result of the MRE estimator for $p\n\\geq 3$. Finally, we describe and expand on analogous plug-in dominance results\nfor spherically symmetric distributions with $p \\geq 4$ under $L_1$ loss.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 14:06:58 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Kubokawa", "Tatsuya", ""], ["Marchand", "\u00c9ric", ""], ["Strawderman", "William E.", ""]]}, {"id": "1408.5339", "submitter": "Debashis Paul", "authors": "Debashis Paul, Jie Peng and Prabir Burman", "title": "Nonparametric estimation of dynamics of monotone trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of nonlinear nonparametric inverse problems. Specifically,\nwe propose a nonparametric estimator of the dynamics of a monotonically\nincreasing trajectory defined on a finite time interval. Under suitable\nregularity conditions, we prove consistency of the proposed estimator and show\nthat in terms of $L^2$-loss, the optimal rate of convergence for the proposed\nestimator is the same as that for the estimation of the derivative of a\ntrajectory. This is a new contribution to the area of nonlinear nonparametric\ninverse problems. We conduct a simulation study to examine the finite sample\nbehavior of the proposed estimator and apply it to the Berkeley growth data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 15:53:50 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Paul", "Debashis", ""], ["Peng", "Jie", ""], ["Burman", "Prabir", ""]]}, {"id": "1408.5355", "submitter": "Debdeep Pati", "authors": "Andriy Norets and Debdeep Pati", "title": "Adaptive Bayesian Estimation of Conditional Densities", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-parametric Bayesian model for conditional densities. The\nmodel is a finite mixture of normal distributions with covariate dependent\nmultinomial logit mixing probabilities. A prior for the number of mixture\ncomponents is specified on positive integers. The marginal distribution of\ncovariates is not modeled. We study asymptotic frequentist behavior of the\nposterior in this model. Specifically, we show that when the true conditional\ndensity has a certain smoothness level, then the posterior contraction rate\naround the truth is equal up to a log factor to the frequentist minimax rate of\nestimation. An extension to the case when the covariate space is unbounded is\nalso established. As our result holds without a priori knowledge of the\nsmoothness level of the true density, the established posterior contraction\nrates are adaptive. Moreover, we show that the rate is not affected by\ninclusion of irrelevant covariates in the model. In Monte Carlo simulations, a\nversion of the model compares favorably to a cross-validated kernel conditional\ndensity estimator.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 16:37:41 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 21:53:20 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 22:28:10 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Norets", "Andriy", ""], ["Pati", "Debdeep", ""]]}, {"id": "1408.5369", "submitter": "Tengyao Wang", "authors": "Tengyao Wang, Quentin Berthet, Richard J. Samworth", "title": "Statistical and computational trade-offs in estimation of sparse\n  principal components", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1369 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 5, 1896-1930", "doi": "10.1214/15-AOS1369", "report-no": "IMS-AOS-AOS1369", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sparse principal component analysis has emerged as an\nextremely popular dimension reduction technique for high-dimensional data. The\ntheoretical challenge, in the simplest case, is to estimate the leading\neigenvector of a population covariance matrix under the assumption that this\neigenvector is sparse. An impressive range of estimators have been proposed;\nsome of these are fast to compute, while others are known to achieve the\nminimax optimal rate over certain Gaussian or sub-Gaussian classes. In this\npaper, we show that, under a widely-believed assumption from computational\ncomplexity theory, there is a fundamental trade-off between statistical and\ncomputational performance in this problem. More precisely, working with new,\nlarger classes satisfying a restricted covariance concentration condition, we\nshow that there is an effective sample size regime in which no randomised\npolynomial time algorithm can achieve the minimax optimal rate. We also study\nthe theoretical performance of a (polynomial time) variant of the well-known\nsemidefinite relaxation estimator, revealing a subtle interplay between\nstatistical and computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2014 17:42:35 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 13:16:21 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Wang", "Tengyao", ""], ["Berthet", "Quentin", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1408.5604", "submitter": "Caroline Uhler", "authors": "Piotr Zwiernik, Caroline Uhler and Donald Richards", "title": "Maximum Likelihood Estimation for Linear Gaussian Covariance Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameter estimation in linear Gaussian covariance models, which are\n$p$-dimensional Gaussian models with linear constraints on the covariance\nmatrix. Maximum likelihood estimation for this class of models leads to a\nnon-convex optimization problem which typically has many local maxima. Using\nrecent results on the asymptotic distribution of extreme eigenvalues of the\nWishart distribution, we provide sufficient conditions for any hill-climbing\nmethod to converge to the global maximum. Although we are primarily interested\nin the case in which $n>\\!\\!>p$, the proofs of our results utilize large-sample\nasymptotic theory under the scheme $n/p \\to \\gamma > 1$. Remarkably, our\nnumerical simulations indicate that our results remain valid for $p$ as small\nas $2$. An important consequence of this analysis is that for sample sizes $n\n\\simeq 14 p$, maximum likelihood estimation for linear Gaussian covariance\nmodels behaves as if it were a convex optimization problem.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 13:41:40 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2016 22:10:38 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Zwiernik", "Piotr", ""], ["Uhler", "Caroline", ""], ["Richards", "Donald", ""]]}, {"id": "1408.5621", "submitter": "Marian Grendar", "authors": "Marian Grend\\'ar and Vladim\\'ir \\v{S}pitalsk\\'y", "title": "Multinomial and empirical likelihood under convex constraints:\n  directions of recession, Fenchel duality, perturbations", "comments": null, "journal-ref": "Electron. J. Statist. 11 (2017), no. 1, 2547--2612", "doi": "10.1214/17-EJS1294", "report-no": null, "categories": "math.ST math.OC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primal problem of multinomial likelihood maximization restricted to a\nconvex closed subset of the probability simplex is studied. Contrary to widely\nheld belief, a solution of this problem may assign a positive mass to an\noutcome with zero count. Related flaws in the simplified Lagrange and Fenchel\ndual problems, which arise because the recession directions are ignored, are\nidentified and corrected.\n  A solution of the primal problem can be obtained by the PP (perturbed primal)\nalgorithm, that is, as the limit of a sequence of solutions of perturbed primal\nproblems. The PP algorithm may be implemented by the simplified Fenchel dual.\n  The results permit us to specify linear sets and data such that the empirical\nlikelihood-maximizing distribution exists and is the same as the multinomial\nlikelihood-maximizing distribution. The multinomial likelihood ratio reaches,\nin general, a different conclusion than the empirical likelihood ratio.\n  Implications for minimum discrimination information, compositional data\nanalysis, Lindsay geometry, bootstrap with auxiliary information, and Lagrange\nmultiplier tests are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 24 Aug 2014 17:35:39 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Grend\u00e1r", "Marian", ""], ["\u0160pitalsk\u00fd", "Vladim\u00edr", ""]]}, {"id": "1408.5724", "submitter": "St\\'ephanie van der Pas", "authors": "St\\'ephanie van der Pas and Peter Gr\\\"unwald", "title": "Almost the Best of Three Worlds: Risk, Consistency and Optional Stopping\n  for the Switch Criterion in Nested Model Selection", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the switch distribution, introduced by Van Erven et al. (2012),\napplied to model selection and subsequent estimation. While switching was known\nto be strongly consistent, here we show that it achieves minimax optimal\nparametric risk rates up to a $\\log\\log n$ factor when comparing two nested\nexponential families, partially confirming a conjecture by Lauritzen (2012) and\nCavanaugh (2012) that switching behaves asymptotically like the Hannan-Quinn\ncriterion. Moreover, like Bayes factor model selection but unlike standard\nsignificance testing, when one of the models represents a simple hypothesis,\nthe switch criterion defines a robust null hypothesis test, meaning that its\nType-I error probability can be bounded irrespective of the stopping rule.\nHence, switching is consistent, insensitive to optional stopping and almost\nminimax risk optimal, showing that, Yang's (2005) impossibility result\nnotwithstanding, it is possible to `almost' combine the strengths of AIC and\nBayes factor model selection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 11:19:00 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 12:23:52 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 09:00:42 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["van der Pas", "St\u00e9phanie", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "1408.5820", "submitter": "Pierre Alquier", "authors": "The Tien Mai, Pierre Alquier", "title": "A Bayesian Approach for Noisy Matrix Completion: Optimal Rate under\n  General Sampling Distribution", "comments": null, "journal-ref": "Electronic Journal of Statistics 9, pp. 823-841, 2015", "doi": "10.1214/15-EJS1020", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods for low-rank matrix completion with noise have been shown to\nbe very efficient computationally. While the behaviour of penalized\nminimization methods is well understood both from the theoretical and\ncomputational points of view in this problem, the theoretical optimality of\nBayesian estimators have not been explored yet. In this paper, we propose a\nBayesian estimator for matrix completion under general sampling distribution.\nWe also provide an oracle inequality for this estimator. This inequality proves\nthat, whatever the rank of the matrix to be estimated, our estimator reaches\nthe minimax-optimal rate of convergence (up to a logarithmic factor). We end\nthe paper with a short simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 16:08:11 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 13:37:40 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Mai", "The Tien", ""], ["Alquier", "Pierre", ""]]}, {"id": "1408.5907", "submitter": "Anru Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Inference for High-dimensional Differential Correlation Matrices", "comments": "Accepted for publication in Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2015.08.019", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by differential co-expression analysis in genomics, we consider in\nthis paper estimation and testing of high-dimensional differential correlation\nmatrices. An adaptive thresholding procedure is introduced and theoretical\nguarantees are given. Minimax rate of convergence is established and the\nproposed estimator is shown to be adaptively rate-optimal over collections of\npaired correlation matrices with approximately sparse differences. Simulation\nresults show that the procedure significantly outperforms two other natural\nmethods that are based on separate estimation of the individual correlation\nmatrices. The procedure is also illustrated through an analysis of a breast\ncancer dataset, which provides evidence at the gene co-expression level that\nseveral genes, of which a subset has been previously verified, are associated\nwith the breast cancer. Hypothesis testing on the differential correlation\nmatrices is also considered. A test, which is particularly well suited for\ntesting against sparse alternatives, is introduced. In addition, other related\nproblems, including estimation of a single sparse correlation matrix,\nestimation of the differential covariance matrices, and estimation of the\ndifferential cross-correlation matrices, are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 20:02:16 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 16:55:05 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1408.6015", "submitter": "Karthik Sriram", "authors": "Karthik Sriram and R.V. Ramamoorthi", "title": "Posterior consistency in misspecified models for i.n.i.d response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive conditions for posterior consistency when the responses are\nindependent but not identically distributed ($i.n.i.d$) and the model is\n\"misspecified\" to be a family of densities parametrized by a possibly infinite\ndimensional parameter. Our approach has connections to key ideas developed for\n$i.i.d$ models in Kleijn and van der Vaart(2006) and it's subsequent\nsimplification in Ramamoorthi, et al.(2014) (unpublished manuscript). While key\nresults in these two papers rely heavily on the convexity of the specified\nfamily of densities, parametric families are seldom convex. In this note, we\ntake a direct approach to deriving posterior consistency with respect to\nnatural topologies on the parameter space without having to impose conditions\non the convex hull of the parametric family. We first derive our results for\nthe case when the responses are $i.i.d$ and then extend it to the $i.n.i.d$\ncase. As an example, we demonstrate the applicability of the results to the\nBayesian quantile estimation problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 05:35:20 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Sriram", "Karthik", ""], ["Ramamoorthi", "R. V.", ""]]}, {"id": "1408.6211", "submitter": "Amanda Turner", "authors": "John Whitehead, Faye Cleary and Amanda Turner", "title": "Bayesian sample sizes for exploratory clinical trials comparing multiple\n  experimental treatments with a control", "comments": "28 pages, 3 tables, 2 figures", "journal-ref": "Statist. Med., 34, 2048-2061, 2015", "doi": "10.1002/sim.6469", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian approach is developed for simultaneously comparing\nmultiple experimental treatments with a common control treatment in an\nexploratory clinical trial. The sample size is set to ensure that, at the end\nof the study, there will be at least one treatment for which the investigators\nhave a strong belief that it is better than control, or else they have a strong\nbelief that none of the experimental treatments are substantially better than\ncontrol. This criterion bears a direct relationship with conventional\nfrequentist power requirements, while allowing prior opinion to feature in the\nanalysis with a consequent reduction in sample size. If it is concluded that at\nleast one of the experimental treatments shows promise, then it is envisaged\nthat one or more of these promising treatments will be developed further in a\ndefinitive phase III trial. The approach is developed in the context of\nnormally distributed responses sharing a common standard deviation regardless\nof treatment. To begin with, the standard deviation will be assumed known when\nthe sample size is calculated. The final analysis will not rely upon this\nassumption, although the intended properties of the design may not be achieved\nif the anticipated standard deviation turns out to be inappropriate. Methods\nthat formally allow for uncertainty about the standard deviation, expressed in\nthe form of a Bayesian prior, are then explored. Illustrations of the sample\nsizes computed from the new method are presented, and comparisons are made with\nfrequentist methods devised for the same situation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:07:20 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Whitehead", "John", ""], ["Cleary", "Faye", ""], ["Turner", "Amanda", ""]]}, {"id": "1408.6218", "submitter": "Joseph Salmon", "authors": "Olga Klopp (MODAL'X, CREST-INSEE), Jean Lafond (LTCI), Eric Moulines\n  (LTCI), Joseph Salmon (LTCI)", "title": "Adaptive Multinomial Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of estimating a matrix given a sample of observed entries is known\nas the \\emph{matrix completion problem}. Most works on matrix completion have\nfocused on recovering an unknown real-valued low-rank matrix from a random\nsample of its entries. Here, we investigate the case of highly quantized\nobservations when the measurements can take only a small number of values.\nThese quantized outputs are generated according to a probability distribution\nparametrized by the unknown matrix of interest. This model corresponds, for\nexample, to ratings in recommender systems or labels in multi-class\nclassification. We consider a general, non-uniform, sampling scheme and give\ntheoretical guarantees on the performance of a constrained, nuclear norm\npenalized maximum likelihood estimator. One important advantage of this\nestimator is that it does not require knowledge of the rank or an upper bound\non the nuclear norm of the unknown matrix and, thus, it is adaptive. We provide\nlower bounds showing that our estimator is minimax optimal. An efficient\nalgorithm based on lifted coordinate gradient descent is proposed to compute\nthe estimator. A limited Monte-Carlo experiment, using both simulated and real\ndata is provided to support our claims.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:19:39 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Klopp", "Olga", "", "MODAL'X, CREST-INSEE"], ["Lafond", "Jean", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"], ["Salmon", "Joseph", "", "LTCI"]]}, {"id": "1408.6323", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian and Philip Gloor and Omar Ghattas", "title": "On Bayesian A- and D-optimal experimental designs in infinite dimensions", "comments": "16 pages, minor changes, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian linear inverse problems in infinite-dimensional\nseparable Hilbert spaces, with a Gaussian prior measure and additive Gaussian\nnoise model, and provide an extension of the concept of Bayesian D-optimality\nto the infinite-dimensional case. To this end, we derive the\ninfinite-dimensional version of the expression for the Kullback-Leibler\ndivergence from the posterior measure to the prior measure, which is\nsubsequently used to derive the expression for the expected information gain.\nWe also study the notion of Bayesian A-optimality in the infinite-dimensional\nsetting, and extend the well known (in the finite-dimensional case) equivalence\nof the Bayes risk of the MAP estimator with the trace of the posterior\ncovariance, for the Gaussian linear case, to the infinite-dimensional Hilbert\nspace case.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 06:23:43 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 05:18:53 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Alexanderian", "Alen", ""], ["Gloor", "Philip", ""], ["Ghattas", "Omar", ""]]}, {"id": "1408.6327", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Peter Hall", "title": "Double-bootstrap methods that use a single double-bootstrap simulation", "comments": null, "journal-ref": "Biometrika 2015, Vol. 102, No. 1, 203-214", "doi": "10.1093/biomet/asu060", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that, when the double bootstrap is used to improve performance of\nbootstrap methods for bias correction, techniques based on using a single\ndouble-bootstrap sample for each single-bootstrap sample can be particularly\neffective. In particular, they produce third-order accuracy for much less\ncomputational expense than is required by conventional double-bootstrap\nmethods. However, this improved level of performance is not available for the\nsingle double-bootstrap methods that have been suggested to construct\nconfidence intervals or distribution estimators.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 06:40:45 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Chang", "Jinyuan", ""], ["Hall", "Peter", ""]]}, {"id": "1408.6333", "submitter": "Steffen Winter", "authors": "Evgeny Spodarev and Peter Straka and Steffen Winter", "title": "Estimation of fractal dimension and fractal curvatures from digital\n  images", "comments": "30 pages, 8 Figures", "journal-ref": null, "doi": "10.1016/j.chaos.2015.02.011", "report-no": null, "categories": "math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the known methods for estimating the fractal dimension of fractal\nsets are based on the evaluation of a single geometric characteristic, e.g. the\nvolume of its parallel sets. We propose a method involving the evaluation of\nseveral geometric characteristics, namely all the intrinsic volumes (i.e.\\\nvolume, surface area, Euler characteristic etc.) of the parallel sets of a\nfractal. Motivated by recent results on their limiting behaviour, we use these\nfunctionals to estimate the fractal dimension of sets from digital images.\nSimultaneously, we also obtain estimates of the fractal curvatures of these\nsets, some fractal counterpart of intrinsic volumes, allowing a finer\nclassification of fractal sets than by means of fractal dimension only. We show\nthe consistency of our estimators and test them on some digital images of\nself-similar sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 07:30:06 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Spodarev", "Evgeny", ""], ["Straka", "Peter", ""], ["Winter", "Steffen", ""]]}, {"id": "1408.6366", "submitter": "Fred Cerou", "authors": "Frederic Cerou, Arnaud Guyader", "title": "Fluctuation Analysis of Adaptive Multilevel Splitting", "comments": "68 pages, introduction changed, Metropolis-Hastings kernel case now\n  included, some modifications in the proofs, some typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel Splitting is a Sequential Monte Carlo method to simulate\nrealisations of a rare event as well as to estimate its probability. This\narticle is concerned with the convergence and the fluctuation analysis of\nAdaptive Multilevel Splitting techniques. In contrast to their fixed level\nversion, adaptive techniques estimate the sequence of levels on the fly and in\nan optimal way, with only a low additional computational cost. However, very\nfew convergence results are available for this class of adaptive branching\nmodels, mainly because the sequence of levels depends on the occupation\nmeasures of the particle systems. This article proves the consistency of these\nmethods as well as a central limit theorem. In particular, we show that the\nprecision of the adaptive version is the same as the one of the fixed-levels\nversion where the levels would have been placed in an optimal manner.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 09:56:46 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 14:15:54 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 16:00:13 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Cerou", "Frederic", ""], ["Guyader", "Arnaud", ""]]}, {"id": "1408.6440", "submitter": "Didier Ch\\'etelat", "authors": "Didier Ch\\'etelat and Martin T. Wells", "title": "Noise Estimation in the Spiked Covariance Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a spiked covariance matrix in high dimensions under\nFrobenius loss, and the parallel problem of estimating the noise in spiked PCA\nis investigated. We propose an estimator of the noise parameter by minimizing\nan unbiased estimator of the invariant Frobenius risk using calculus of\nvariations. The resulting estimator is shown, using random matrix theory, to be\nstrongly consistent and essentially asymptotically normal and minimax for the\nnoise estimation problem. We apply the construction to construct a robust\nspiked covariance matrix estimator with consistent eigenvalues.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 15:41:48 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Ch\u00e9telat", "Didier", ""], ["Wells", "Martin T.", ""]]}, {"id": "1408.6618", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Falsifiable implies Learnable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper demonstrates that falsifiability is fundamental to learning. We\nprove the following theorem for statistical learning and sequential prediction:\nIf a theory is falsifiable then it is learnable -- i.e. admits a strategy that\npredicts optimally. An analogous result is shown for universal induction.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 03:29:06 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1408.6731", "submitter": "Uwe Saint-Mont", "authors": "Uwe Saint-Mont", "title": "Comparing Different Information Levels", "comments": "14 pages, 3 figures", "journal-ref": "The Open Probability & Statistics J., 2017, 8, 7-18", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence of random variables ${\\bf X}=X_1,X_2,\\ldots$ suppose the aim\nis to maximize one's return by picking a `favorable' $X_i$. Obviously, the\nexpected payoff crucially depends on the information at hand. An optimally\ninformed person knows all the values $X_i=x_i$ and thus receives $E (\\sup\nX_i)$. We will compare this return to the expected payoffs of a number of\nobservers having less information, in particular $\\sup_i (EX_i)$, the value of\nthe sequence to a person who only knows the first moments of the random\nvariables.\n  In general, there is a stochastic environment (i.e. a class of random\nvariables $\\cal C$), and several levels of information. Given some ${\\bf X} \\in\n{\\cal C}$, an observer possessing information $j$ obtains $r_j({\\bf X})$. We\nare going to study `information sets' of the form $$ R_{\\cal C}^{j,k} = \\{\n(x,y) | x = r_j({\\bf X}), y=r_k({\\bf X}), {\\bf X} \\in {\\cal C} \\}, $$\ncharacterizing the advantage of $k$ relative to $j$. Since such a set measures\nthe additional payoff by virtue of increased information, its analysis yields a\nnumber of interesting results, in particular `prophet-type' inequalities.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 14:10:46 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Saint-Mont", "Uwe", ""]]}, {"id": "1408.6800", "submitter": "Jaehyung Choi", "authors": "Jaehyung Choi, Andrew P. Mullhaupt", "title": "Geometric shrinkage priors for K\\\"ahlerian signal filters", "comments": "10 pages, published version", "journal-ref": "Entropy 17(3), 1347-1357 (2015)", "doi": "10.3390/e17031347", "report-no": null, "categories": "math.ST cs.IT math.DG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct geometric shrinkage priors for K\\\"ahlerian signal filters. Based\non the characteristics of K\\\"ahler manifolds, an efficient and robust algorithm\nfor finding superharmonic priors which outperform the Jeffreys prior is\nintroduced. Several ans\\\"atze for the Bayesian predictive priors are also\nsuggested. In particular, the ans\\\"atze related to K\\\"ahler potential are\ngeometrically intrinsic priors to the information manifold of which the\ngeometry is derived from the potential. The implication of the algorithm to\ntime series models is also provided.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 18:27:00 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 20:17:10 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2015 15:57:23 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Choi", "Jaehyung", ""], ["Mullhaupt", "Andrew P.", ""]]}, {"id": "1408.6876", "submitter": "James P. Crutchfield", "authors": "Sarah Marzen and James P. Crutchfield", "title": "Informational and Causal Architecture of Discrete-Time Renewal Processes", "comments": "18 pages, 9 figures, 1 table;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/dtrp.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewal processes are broadly used to model stochastic behavior consisting of\nisolated events separated by periods of quiescence, whose durations are\nspecified by a given probability law. Here, we identify the minimal sufficient\nstatistic for their prediction (the set of causal states), calculate the\nhistorical memory capacity required to store those states (statistical\ncomplexity), delineate what information is predictable (excess entropy), and\ndecompose the entropy of a single measurement into that shared with the past,\nfuture, or both. The causal state equivalence relation defines a new subclass\nof renewal processes with a finite number of causal states despite having an\nunbounded interevent count distribution. We use these formulae to analyze the\noutput of the parametrized Simple Nonunifilar Source, generated by a simple\ntwo-state hidden Markov model, but with an infinite-state epsilon-machine\npresentation. All in all, the results lay the groundwork for analyzing\nprocesses with infinite statistical complexity and infinite excess entropy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 22:40:39 GMT"}, {"version": "v2", "created": "Sun, 9 Nov 2014 22:18:34 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Marzen", "Sarah", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1408.6937", "submitter": "Aleksey Polunchenko", "authors": "Wenyu Du, Grigory Sokolov, Aleksey S. Polunchenko", "title": "An Exact Formula for the Average Run Length to False Alarm of the\n  Generalized Shiryaev-Roberts Procedure for Change-Point Detection under\n  Exponential Observations", "comments": "9 pages; Accepted for publication in Proceedings of the 12-th\n  German-Polish Workshop on Stochastic Models, Statistics and Their\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive analytically an exact closed-form formula for the standard minimax\nAverage Run Length (ARL) to false alarm delivered by the Generalized\nShiryaev-Roberts (GSR) change-point detection procedure devised to detect a\nshift in the baseline mean of a sequence of independent exponentially\ndistributed observations. Specifically, the formula is found through direct\nsolution of the respective integral (renewal) equation, and is a general result\nin that the GSR procedure's headstart is not restricted to a bounded range, nor\nis there a \"ceiling\" value for the detection threshold. Apart from the\ntheoretical significance (in change-point detection, exact closed-form\nperformance formulae are typically either difficult or impossible to get,\nespecially for the GSR procedure), the obtained formula is also useful to a\npractitioner: in cases of practical interest, the formula is a function linear\nin both the detection threshold and the headstart, and, therefore, the ARL to\nfalse alarm of the GSR procedure can be easily computed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 07:36:22 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 14:48:01 GMT"}, {"version": "v3", "created": "Tue, 21 Oct 2014 20:35:02 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Du", "Wenyu", ""], ["Sokolov", "Grigory", ""], ["Polunchenko", "Aleksey S.", ""]]}]