[{"id": "1910.00131", "submitter": "Dominik Liebl", "authors": "Dominik Liebl and Matthew Reimherr", "title": "Fast and Fair Simultaneous Confidence Bands for Functional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying uncertainty using confidence regions is a central goal of\nstatistical inference. Despite this, methodologies for confidence bands in\nFunctional Data Analysis are underdeveloped compared to estimation and\nhypothesis testing. This work represents a major leap forward in this area by\npresenting a new methodology for constructing simultaneous confidence bands for\nfunctional parameter estimates. These bands possess a number of striking\nqualities: (1) they have a nearly closed-form expression, (2) they give nearly\nexact coverage, (3) they have a finite sample correction, (4) they do not\nrequire an estimate of the full covariance of the parameter estimate, and (5)\nthey can be constructed adaptively according to a desired criteria. One option\nfor choosing bands we find especially interesting is the concept of fair bands\nwhich allows us to do fair (or equitable) inference over subintervals and could\nbe especially useful in longitudinal studies over long time scales. Our bands\nare constructed by integrating and extending tools from Random Field Theory, an\narea that has yet to overlap with Functional Data Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:11:54 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:46:14 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Liebl", "Dominik", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1910.00229", "submitter": "Chandima N. P. G. Arachchige", "authors": "Chandima N. P. G. Arachchige, Luke A. Prendergast", "title": "Confidence intervals for median absolute deviations", "comments": "13 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The median absolute deviation (MAD) is a robust measure of scale that is\nsimple to implement and easy to interpret. Motivated by this, we introduce\ninterval estimators of the MAD to make reliable inferences for dispersion for a\nsingle population and ratios and differences of MADs for comparing two\npopulations. Our simulation results show that the coverage probabilities of the\nintervals are very close to the nominal coverage for a variety of\ndistributions. We have used partial influence functions to investigate the\nrobustness properties of the difference and ratios of independent MADs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 07:15:52 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 03:05:15 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 07:42:12 GMT"}, {"version": "v4", "created": "Fri, 1 Nov 2019 07:39:28 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Arachchige", "Chandima N. P. G.", ""], ["Prendergast", "Luke A.", ""]]}, {"id": "1910.00402", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Monotonically Decreasing Sequence of Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergences are quantities that measure discrepancy between two probability\ndistributions and play an important role in various fields such as statistics\nand machine learning. Divergences are non-negative and are equal to zero if and\nonly if two distributions are the same. In addition, some important divergences\nsuch as the f-divergence have convexity, which we call \"convex divergence\". In\nthis paper, we show new properties of the convex divergences by using integral\nand differential operators that we introduce. For the convex divergence, the\nresult applied the integral or differential operator is also a divergence. In\nparticular, the integral operator preserves convexity. Furthermore, the results\napplied the integral operator multiple times constitute a monotonically\ndecreasing sequence of the convex divergences. We derive new sequences of the\nconvex divergences that include the Kullback-Leibler divergence or the reverse\nKullback-Leibler divergence from these properties.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:06:50 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 13:04:26 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 13:05:36 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1910.00423", "submitter": "Keith Levin", "authors": "Keith Levin, Fred Roosta, Minh Tang, Michael W. Mahoney, Carey E.\n  Priebe", "title": "Limit theorems for out-of-sample extensions of the adjacency and\n  Laplacian spectral embeddings", "comments": "Portions of this work originally appeared in ICML2018 as\n  \"Out-of-sample extension of graph adjacency spectral embedding\" (accompanying\n  technical report available at arXiv:1802.06307). This work extends the\n  results of that earlier paper to a second graph embedding technique called\n  the Laplacian spectral embedding and presents additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings, a class of dimensionality reduction techniques designed for\nrelational data, have proven useful in exploring and modeling network\nstructure. Most dimensionality reduction methods allow out-of-sample\nextensions, by which an embedding can be applied to observations not present in\nthe training set. Applied to graphs, the out-of-sample extension problem\nconcerns how to compute the embedding of a vertex that is added to the graph\nafter an embedding has already been computed. In this paper, we consider the\nout-of-sample extension problem for two graph embedding procedures: the\nadjacency spectral embedding and the Laplacian spectral embedding. In both\ncases, we prove that when the underlying graph is generated according to a\nlatent space model called the random dot product graph, which includes the\npopular stochastic block model as a special case, an out-of-sample extension\nbased on a least-squares objective obeys a central limit theorem about the true\nlatent position of the out-of-sample vertex. In addition, we prove a\nconcentration inequality for the out-of-sample extension of the adjacency\nspectral embedding based on a maximum-likelihood objective. Our results also\nyield a convenient framework in which to analyze trade-offs between estimation\naccuracy and computational expense, which we explore briefly.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 04:02:10 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Levin", "Keith", ""], ["Roosta", "Fred", ""], ["Tang", "Minh", ""], ["Mahoney", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1910.00585", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Non-algorithmic theory of randomness", "comments": "15 pages", "journal-ref": "Fields of Logic and Computation III: Esseays Dedicated to Yuri\n  Gurevich on the Occasion of his 80th Birthday. Lecture Notes in Computer\n  Science 12180, pages 323-340 (2020)", "doi": "10.1007/978-3-030-48006-6", "report-no": "25", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternative language for expressing results of the\nalgorithmic theory of randomness. The language is more precise in that it does\nnot involve unspecified additive or multiplicative constants, making\nmathematical results, in principle, applicable in practice. Our main testing\nground for the proposed language is the problem of defining Bernoulli\nsequences, which was of great interest to Andrei Kolmogorov and his students.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:44:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1910.00667", "submitter": "Eduardo Pavez", "authors": "Eduardo Pavez, Antonio Ortega", "title": "Covariance Matrix Estimation with Non Uniform and Data Dependent Missing\n  Observations", "comments": "16 pages, 4 figures. Accepted at IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2020.3039118", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study covariance estimation with missing data. We consider\nmissing data mechanisms that can be independent of the data, or have a time\nvarying dependency. Additionally, observed variables may have arbitrary (non\nuniform) and dependent observation probabilities. For each mechanism, we\nconstruct an unbiased estimator and obtain bounds for the expected value of\ntheir estimation error in operator norm. Our bounds are equivalent, up to\nconstant and logarithmic factors, to state of the art bounds for complete and\nuniform missing observations. Furthermore, for the more general non uniform and\ndependent cases, the proposed bounds are new or improve upon previous results.\nOur error estimates depend on quantities we call scaled effective rank, which\ngeneralize the effective rank to account for missing observations. All the\nestimators studied in this work have the same asymptotic convergence rate (up\nto logarithmic factors).\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:12:13 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 05:57:46 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 06:02:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pavez", "Eduardo", ""], ["Ortega", "Antonio", ""]]}, {"id": "1910.00675", "submitter": "Yujia Ding", "authors": "Yujia Ding and Qidi Peng", "title": "Series Representation of Jointly S$\\alpha$S Distribution via Symmetric\n  Covariations", "comments": null, "journal-ref": "Communications in Mathematics and Statistics 9 (2021) 203-238", "doi": "10.1007/s40304-020-00216-5", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of symmetric covariation, which is a new measure of\ndependence between two components of a symmetric $\\alpha$-stable random vector,\nwhere the stability parameter $\\alpha$ measures the heavy-tailedness of its\ndistribution. Unlike covariation that exists only when $\\alpha\\in(1,2]$,\nsymmetric covariation is well defined for all $\\alpha\\in(0,2]$. We show that\nsymmetric covariation can be defined using the proposed generalized fractional\nderivative, which has broader usages than those involved in this work. Several\nproperties of symmetric covariation have been derived. These are either similar\nto or more general than those of the covariance functions in the Gaussian case.\nThe main contribution of this framework is the representation of the\ncharacteristic function of bivariate symmetric $\\alpha$-stable distribution via\nconvergent series based on a sequence of symmetric covariations. This series\nrepresentation extends the one of bivariate Gaussian.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 21:21:23 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 05:50:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ding", "Yujia", ""], ["Peng", "Qidi", ""]]}, {"id": "1910.00943", "submitter": "Jos\\'e Ant\\'onio Ferreira", "authors": "Jos\\'e A. Ferreira", "title": "Data-generating models under which the random forest algorithm performs\n  badly", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples are given of data-generating models under which some versions of the\nrandom forest algorithm may fail to be consistent or be extremely slow to\nconverge to the optimal predictor. The evidence provided for these properties\nis based on mostly intuitive arguments, similar to those used earlier with\nsimpler examples, and on numerical experiments. Although one can always choose\na model under which random forests perform very badly, it is shown that when\nsubstantial improvement is possible simple methods based on statistics of\n'variable use' and 'variable importance' may indicate a better predictor based\non a sort of mixture of random forests; thus, by acknowledging the difficulties\nposed by some models one may improve the performance of random forests in some\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 13:33:33 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 08:15:42 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 07:52:44 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 10:23:39 GMT"}, {"version": "v5", "created": "Tue, 14 Jan 2020 12:21:14 GMT"}, {"version": "v6", "created": "Fri, 7 May 2021 20:17:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ferreira", "Jos\u00e9 A.", ""]]}, {"id": "1910.01004", "submitter": "Florian Hildebrandt", "authors": "Florian Hildebrandt and Mathias Trabs", "title": "Parameter estimation for SPDEs based on discrete observations in time\n  and space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for a parabolic linear stochastic partial differential\nequation in one space dimension is studied observing the solution field on a\ndiscrete grid in a fixed bounded domain. Considering an infill asymptotic\nregime in both coordinates, we prove central limit theorems for realized\nquadratic variations based on temporal and spatial increments as well as on\ndouble increments in time and space. Resulting method of moments estimators for\nthe diffusivity and the volatility parameter inherit the asymptotic normality\nand can be constructed robustly with respect to the sampling frequencies in\ntime and space. Upper and lower bounds reveal that in general the optimal\nconvergence rate for joint estimation of the parameters is slower than the\nusual parametric rate. The theoretical results are illustrated in a numerical\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 15:08:06 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 08:19:30 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hildebrandt", "Florian", ""], ["Trabs", "Mathias", ""]]}, {"id": "1910.01079", "submitter": "Sourav Chatterjee", "authors": "Sourav Chatterjee", "title": "A deterministic theory of low rank matrix completion", "comments": "22 pages. The numberings of sections, theorems, lemmas, definitions\n  and equations have been changed to correspond to the version published in\n  IEEE Trans. Inf. Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of completing a large low rank matrix using a subset of revealed\nentries has received much attention in the last ten years. The main result of\nthis paper gives a necessary and sufficient condition, stated in the language\nof graph limit theory, for a sequence of matrix completion problems with\narbitrary missing patterns to be asymptotically solvable. It is then shown that\na small modification of the Cand\\`es-Recht nuclear norm minimization algorithm\nprovides the required asymptotic solution whenever the sequence of problems is\nasymptotically solvable. The theory is fully deterministic, with no assumption\nof randomness. A number of open questions are listed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 16:54:58 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 19:56:20 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 08:07:02 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 06:16:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chatterjee", "Sourav", ""]]}, {"id": "1910.01084", "submitter": "Sricharan Shah", "authors": "Sricharan Shah, Partha Jyoti Hazarika and Subrata Chakraborty", "title": "The Balakrishnan Alpha Skew Laplace Distribution: Properties and Its\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study by considering Balakrishnan mechanism a new form of alpha skew\ndistribution is proposed and properties are investigated. The suitability of\nthe proposed distribution has tested at the end with appropriate data fitting\nexperiment and then comparing the values of Akaike Information Criterion (AIC),\nand Bayesian Information Criterion (BIC) with the values of some other related\ndistributions. Likelihood ratio test is carried out for nested models, that is,\nfor Laplace and the proposed distributions.\n  Keywords: Skew Distribution, Alpha-Skew Distribution, Bimodal Distribution,\nAlpha-Skew-Normal Distribution, Alpha-Skew-Laplace Distribution,\nBalakrishnan-Alpha-Skew-Normal Distribution\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 04:35:42 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Shah", "Sricharan", ""], ["Hazarika", "Partha Jyoti", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "1910.01199", "submitter": "Lu Wei Dr.", "authors": "Lu Wei", "title": "Skewness of von Neumann entanglement entropy", "comments": null, "journal-ref": "J. Phys. A: Math. Theor. 53 075302, 2020", "doi": "10.1088/1751-8121/ab63a7", "report-no": null, "categories": "cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quantum bipartite systems in a random pure state, where von Neumann\nentropy is considered as a measure of the entanglement. Expressions of the\nfirst and second exact cumulants of von Neumann entropy, relevant respectively\nto the average and fluctuation behavior, are known in the literature. The focus\nof this paper is on its skewness that specifies the degree of asymmetry of the\ndistribution. Computing the skewness requires additionally the third cumulant,\nan exact formula of which is the main result of this work. In proving the main\nresult, we obtain as a byproduct various summation identities involving\npolygamma and related functions. The derived third cumulant also leads to an\nimproved approximation to the distribution of von Neumann entropy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 20:11:48 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Wei", "Lu", ""]]}, {"id": "1910.01327", "submitter": "Wanrong Zhang", "authors": "Rachel Cummings, Sara Krehbiel, Yuliia Lut, Wanrong Zhang", "title": "Privately detecting changes in unknown distributions", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (2020) Pages 958-968", "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change-point detection problem seeks to identify distributional changes\nin streams of data. Increasingly, tools for change-point detection are applied\nin settings where data may be highly sensitive and formal privacy guarantees\nare required, such as identifying disease outbreaks based on hospital records,\nor IoT devices detecting activity within a home. Differential privacy has\nemerged as a powerful technique for enabling data analysis while preventing\ninformation leakage about individuals. Much of the prior work on change-point\ndetection---including the only private algorithms for this problem---requires\ncomplete knowledge of the pre-change and post-change distributions. However,\nthis assumption is not realistic for many practical applications of interest.\nThis work develops differentially private algorithms for solving the\nchange-point problem when the data distributions are unknown. Additionally, the\ndata may be sampled from distributions that change smoothly over time, rather\nthan fixed pre-change and post-change distributions. We apply our algorithms to\ndetect changes in the linear trends of such data streams. Finally, we also\nprovide experimental results to empirically validate the performance of our\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:05:59 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:35:49 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cummings", "Rachel", ""], ["Krehbiel", "Sara", ""], ["Lut", "Yuliia", ""], ["Zhang", "Wanrong", ""]]}, {"id": "1910.01392", "submitter": "Denis Gaidashev", "authors": "Denis Gaidashev, Ralf Pihlstr\\\"om and Martin Ryner", "title": "On some spectral properties of stochastic similarity matrices for data\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering in image analysis is a central technique that allows to classify\nelements of an image. We describe a simple clustering technique that uses the\nmethod of similarity matrices. We expand upon recent results in spectral\nanalysis for Gaussian mixture distributions, and in particular, provide\nconditions for the existence of a spectral gap between the leading and\nremaining eigenvalues for matrices with entries from a Gaussian mixture with\ntwo real univariate components. Furthermore, we describe an algorithm in which\na collection of image elements is treated as a dynamical system in which the\nexistence of the mentioned spectral gap results in an efficient clustering.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:42:06 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Gaidashev", "Denis", ""], ["Pihlstr\u00f6m", "Ralf", ""], ["Ryner", "Martin", ""]]}, {"id": "1910.01396", "submitter": "Subhro Ghosh", "authors": "Subhro Ghosh and Sanjay Chaudhuri", "title": "Maximum Likelihood under constraints: Degeneracies and Random Critical\n  Points", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of semi-parametric maximum likelihood under\nconstraints on summary statistics. Such a procedure results in a discrete\nprobability distribution that maximises the likelihood among all such\ndistributions under the specified constraints (called estimating equations),\nand is an approximation to the underlying population distribution. The study of\nsuch empirical likelihood originates from the seminal work of Owen. We\ninvestigate this procedure in the setting of mis-specified (or biased)\nestimating equations, i.e. when the null hypothesis is not true. We establish\nthat the behaviour of the optimal distribution under such mis-specification\ndiffer markedly from their properties under the null, i.e. when the estimating\nequations are unbiased and correctly specified. This is manifested by certain\ndegeneracies in the optimal distribution which define the likelihood. Such\ndegeneracies are not observed under the null. Furthermore, we establish an\nanomalous behaviour of the log-likelihood based Wilks statistic, which, unlike\nunder the null, does not exhibit a chi-squared limit. In the Bayesian setting,\nwe rigorously establish the posterior consistency of procedures based on these\nideas, where instead of a parametric likelihood, an empirical likelihood is\nused to define the posterior distribution. In particular, we show that this\nposterior, as a random probability measure, rapidly converges to the delta\nmeasure at the true parameter value. A novel feature of our approach is the\ninvestigation of critical points of random functions in the context of such\nempirical likelihood. In particular, we obtain the location and the mass of the\ndegenerate optimal weights as the leading and sub-leading terms in a canonical\nexpansion of a particular critical point of a random function that is naturally\nassociated with the model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:50:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:59:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ghosh", "Subhro", ""], ["Chaudhuri", "Sanjay", ""]]}, {"id": "1910.01399", "submitter": "Mario Teixeira Parente", "authors": "Mario Teixeira Parente, Jonas Wallin, Barbara Wohlmuth", "title": "Generalized bounds for active subspaces", "comments": "27 pages, 6 figures", "journal-ref": "Electronic Journal of Statistics 14 (1), 917-943, 2020", "doi": "10.1214/20-EJS1684", "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider scenarios in which traditional estimates for the\nactive subspace method based on probabilistic Poincar\\'e inequalities are not\nvalid due to unbounded Poincar\\'e constants. Consequently, we propose a\nframework that allows to derive generalized estimates in the sense that it\nenables to control the trade-off between the size of the Poincar\\'e constant\nand a weaker order of the final error bound. In particular, we investigate\nindependently exponentially distributed random variables in dimension two or\nlarger and give explicit expressions for corresponding Poincar\\'e constants\nshowing their dependence on the dimension of the problem. Finally, we suggest\npossibilities for future work that aim for extending the class of distributions\napplicable to the active subspace method as we regard this as an opportunity to\nenlarge its usability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:53:33 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 13:31:08 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 19:13:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Parente", "Mario Teixeira", ""], ["Wallin", "Jonas", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1910.01420", "submitter": "Matyas Barczy", "authors": "Matyas Barczy, Bojan Basrak, P\\'eter Kevei, Gyula Pap, Hrvoje\n  Planini\\'c", "title": "Statistical inference of subcritical strongly stationary Galton--Watson\n  processes with regularly varying immigration", "comments": "59 pages", "journal-ref": "Stochastic Processes and their Applications 132, (2021), 33-75", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the asymptotic behavior of the conditional least squares\nestimator of the offspring mean for subcritical strongly stationary\nGalton--Watson processes with regularly varying immigration with tail index\n$\\alpha \\in (1,2)$. The limit law is the ratio of two dependent stable random\nvariables with indices $\\alpha/2$ and $2\\alpha/3$, respectively, and it has a\ncontinuously differentiable density function. We use point process technique in\nthe proofs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 12:06:26 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:24:58 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 17:35:10 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Barczy", "Matyas", ""], ["Basrak", "Bojan", ""], ["Kevei", "P\u00e9ter", ""], ["Pap", "Gyula", ""], ["Planini\u0107", "Hrvoje", ""]]}, {"id": "1910.01625", "submitter": "Leighton Barnes", "authors": "Leighton Pate Barnes and Ayfer Ozgur", "title": "Minimax Bounds for Distributed Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributed logistic regression problem where labeled data\npairs $(X_i,Y_i)\\in \\mathbb{R}^d\\times\\{-1,1\\}$ for $i=1,\\ldots,n$ are\ndistributed across multiple machines in a network and must be communicated to a\ncentralized estimator using at most $k$ bits per labeled pair. We assume that\nthe data $X_i$ come independently from some distribution $P_X$, and that the\ndistribution of $Y_i$ conditioned on $X_i$ follows a logistic model with some\nparameter $\\theta\\in\\mathbb{R}^d$. By using a Fisher information argument, we\ngive minimax lower bounds for estimating $\\theta$ under different assumptions\non the tail of the distribution $P_X$. We consider both $\\ell^2$ and logistic\nlosses, and show that for the logistic loss our sub-Gaussian lower bound is\norder-optimal and cannot be improved.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:46:45 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Barnes", "Leighton Pate", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "1910.01692", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross and Vishesh Karwa and Sonja Petrovi\\'c", "title": "Algebraic statistics, tables, and networks: The Fienberg advantage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stephen Fienberg's affinity for contingency table problems and reinterpreting\nmodels with a fresh look gave rise to a new approach for hypothesis testing of\nnetwork models that are linear exponential families. We outline his vision and\ninfluence in this fundamental problem, as well as generalizations to\nmultigraphs and hypergraphs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:10:56 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Gross", "Elizabeth", ""], ["Karwa", "Vishesh", ""], ["Petrovi\u0107", "Sonja", ""]]}, {"id": "1910.01734", "submitter": "Jinchi Lv", "authors": "Jianqing Fan, Yingying Fan, Xiao Han, Jinchi Lv", "title": "SIMPLE: Statistical Inference on Membership Profiles in Large Networks", "comments": "60 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data is prevalent in many contemporary big data applications in which\na common interest is to unveil important latent links between different pairs\nof nodes. Yet a simple fundamental question of how to precisely quantify the\nstatistical uncertainty associated with the identification of latent links\nstill remains largely unexplored. In this paper, we propose the method of\nstatistical inference on membership profiles in large networks (SIMPLE) in the\nsetting of degree-corrected mixed membership model, where the null hypothesis\nassumes that the pair of nodes share the same profile of community memberships.\nIn the simpler case of no degree heterogeneity, the model reduces to the mixed\nmembership model for which an alternative more robust test is also proposed.\nBoth tests are of the Hotelling-type statistics based on the rows of empirical\neigenvectors or their ratios, whose asymptotic covariance matrices are very\nchallenging to derive and estimate. Nevertheless, their analytical expressions\nare unveiled and the unknown covariance matrices are consistently estimated.\nUnder some mild regularity conditions, we establish the exact limiting\ndistributions of the two forms of SIMPLE test statistics under the null\nhypothesis and contiguous alternative hypothesis. They are the chi-square\ndistributions and the noncentral chi-square distributions, respectively, with\ndegrees of freedom depending on whether the degrees are corrected or not. We\nalso address the important issue of estimating the unknown number of\ncommunities and establish the asymptotic properties of the associated test\nstatistics. The advantages and practical utility of our new procedures in terms\nof both size and power are demonstrated through several simulation examples and\nreal network applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:01:39 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Fan", "Jianqing", ""], ["Fan", "Yingying", ""], ["Han", "Xiao", ""], ["Lv", "Jinchi", ""]]}, {"id": "1910.01809", "submitter": "Andrew Ying", "authors": "Andrew Ying, Wen-Xin Zhou", "title": "On the Asymptotic Distribution of the Scan Statistic for Empirical\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic behavior of several variants of the scan\nstatistic applied to empirical distributions, which can be applied to detect\nthe presence of an anomalous interval with any length. Of particular interest\nis Studentized scan statistic that is preferable in practice. The main\ningredients in the proof are Kolmogorov's theorem, a Poisson approximation, and\nrecent technical results by Kabluchko et al (2014).\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 06:21:23 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 23:43:31 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Ying", "Andrew", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1910.01931", "submitter": "Marianna Pensky", "authors": "Majid Noroozi, Marianna Pensky and Ramchandra Rimal", "title": "Sparse Popularity Adjusted Stochastic Block Model", "comments": "4 figures. arXiv admin note: text overlap with arXiv:1902.00431", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we study a sparse stochastic network enabled with a\nblock structure. The popular Stochastic Block Model (SBM) and the Degree\nCorrected Block Model (DCBM) address sparsity by placing an upper bound on the\nmaximum probability of connections between any pair of nodes. As a result,\nsparsity describes only the behavior of network as a whole, without\ndistinguishing between the block-dependent sparsity patterns. To the best of\nour knowledge, the recently introduced Popularity Adjusted Block Model (PABM)\nis the only block model that allows to introduce a {\\it structural sparsity}\nwhere some probabilities of connections are identically equal to zero while the\nrest of them remain above a certain threshold. The latter presents a more\nnuanced view of the network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:20:05 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:06:34 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Noroozi", "Majid", ""], ["Pensky", "Marianna", ""], ["Rimal", "Ramchandra", ""]]}, {"id": "1910.01964", "submitter": "Feriel Bouhadjera", "authors": "Bouhadjera Feriel (ULCO, Facult\\'e des Sciences Universit\\'e Badji\n  Mokhtar), Elias Ould Said (ULCO)", "title": "On the strong uniform consistency for relative error of the regression\n  function estimator for censoring times series model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector (X, T), where X is d-dimensional and T is\none-dimensional. We suppose that the random variable T is subject to random\nright censoring and satisfies the $\\alpha$-mixing property. The aim of this\npaper is to study the behavior of the kernel estimator of the relative error\nregression and to establish its uniform almost sure consistency with rate.\nFurthermore, we have highlighted the covariance term which measures the\ndependency. The simulation study shows that the proposed estimator performs\nwell for a finite sample size in different cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 14:15:15 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Feriel", "Bouhadjera", "", "ULCO, Facult\u00e9 des Sciences Universit\u00e9 Badji\n  Mokhtar"], ["Said", "Elias Ould", "", "ULCO"]]}, {"id": "1910.02008", "submitter": "Ying Zhang", "authors": "Ying Zhang, \\\"Omer Deniz Akyildiz, Theodoros Damoulas, Sotirios\n  Sabanis", "title": "Nonasymptotic estimates for Stochastic Gradient Langevin Dynamics under\n  local conditions in nonconvex optimization", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the context of empirical risk minimization, see Raginsky, Rakhlin, and\nTelgarsky (2017), we are concerned with a non-asymptotic analysis of sampling\nalgorithms used in optimization. In particular, we obtain non-asymptotic error\nbounds for a popular class of algorithms called Stochastic Gradient Langevin\nDynamics (SGLD). These results are derived in Wasserstein-1 and Wasserstein-2\ndistances in the absence of log-concavity of the target distribution. More\nprecisely, the stochastic gradient $H(\\theta, x)$ is assumed to be locally\nLipschitz continuous in both variables, and furthermore, the dissipativity\ncondition is relaxed by removing its uniform dependence in $x$. This relaxation\nallows us to present two key paradigms within the framework of scalable\nposterior sampling for Bayesian inference and of nonconvex optimization;\nnamely, examples from minibatch logistic regression and from variational\ninference are given by providing theoretical guarantees for the sampling\nbehaviour of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:02:44 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 15:37:03 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 19:46:42 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 05:26:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhang", "Ying", ""], ["Akyildiz", "\u00d6mer Deniz", ""], ["Damoulas", "Theodoros", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "1910.02220", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "A Fundamental Bound on Performance of Non-Intrusive Load Monitoring with\n  Application to Smart Meter Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.SY eess.SY math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the expected estimation error of non-intrusive load monitoring\nalgorithms is lower bounded by the trace of the inverse of the\ncross-correlation matrix between the derivatives of the load profiles of the\nappliances. We use this fundamental bound to develop privacy-preserving\npolicies. Particularly, we devise a load-scheduling policy by maximizing the\nlower bound on the expected estimation error of non-intrusive load monitoring\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 06:34:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "1910.02316", "submitter": "Rachel Oliver", "authors": "Rabi Bhattacharya and Rachel Oliver", "title": "Superiority of Bayes estimators over the MLE in high dimensional\n  multinomial models and its implication for nonparametric Bayes theory", "comments": "49 pages, 11 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the performance of Bayes estimators, in comparison\nwith the MLE, in multinomial models with a relatively large number of cells.\nThe prior for the Bayes estimator is taken to be the conjugate Dirichlet, i.e.,\nthe multivariate Beta, with exchangeable distributions over the coordinates,\nincluding the non-informative uniform distribution. The choice of the\nmultinomial is motivated by its many applications in business and industry, but\nalso by its use in providing a simple nonparametric estimator of an unknown\ndistribution. It is striking that the Bayes procedure outperforms the\nasymptotically efficient MLE over most of the parameter spaces for even\nmoderately large dimensional parameter space and rather large sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 19:25:24 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Bhattacharya", "Rabi", ""], ["Oliver", "Rachel", ""]]}, {"id": "1910.02345", "submitter": "Elina Robeva", "authors": "Ali Zartash and Elina Robeva", "title": "Convolutions of Totally Positive Distributions with applications to\n  Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the estimation of the density of a totally positive\nrandom vector. Total positivity of the distribution of a random vector implies\na strong form of positive dependence between its coordinates and, in\nparticular, it implies positive association. Since estimating a totally\npositive density is a non-parametric problem, we take on a (modified) kernel\ndensity estimation approach. Our main result is that the sum of scaled standard\nGaussian bumps centered at a min-max closed set provably yields a totally\npositive distribution. Hence, our strategy for producing a totally positive\nestimator is to form the min-max closure of the set of samples, and output a\nsum of Gaussian bumps centered at the points in this set. We can frame this sum\nas a convolution between the uniform distribution on a min-max closed set and a\nscaled standard Gaussian. We further conjecture that convolving any totally\npositive density with a standard Gaussian remains totally positive.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 00:18:12 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 02:24:12 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2020 23:20:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zartash", "Ali", ""], ["Robeva", "Elina", ""]]}, {"id": "1910.02373", "submitter": "Sifan Liu", "authors": "Sifan Liu, Edgar Dobriban", "title": "Ridge Regression: Structure, Cross-Validation, and Sketching", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following three fundamental problems about ridge regression: (1)\nwhat is the structure of the estimator? (2) how to correctly use\ncross-validation to choose the regularization parameter? and (3) how to\naccelerate computation without losing too much accuracy? We consider the three\nproblems in a unified large-data linear model. We give a precise representation\nof ridge regression as a covariance matrix-dependent linear combination of the\ntrue parameter and the noise. We study the bias of $K$-fold cross-validation\nfor choosing the regularization parameter, and propose a simple\nbias-correction. We analyze the accuracy of primal and dual sketching for ridge\nregression, showing they are surprisingly accurate. Our results are illustrated\nby simulations and by analyzing empirical data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:00:40 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 18:12:43 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 04:14:36 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liu", "Sifan", ""], ["Dobriban", "Edgar", ""]]}, {"id": "1910.02386", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar and Prashant Jha and Mohammad Arshad Rahman and\n  Joydeep Dutta", "title": "A New Graphical Device and Related Tests for the Shape of Non-parametric\n  Regression Function", "comments": "There were errors in mathematical proofs of Theorem 1 and related\n  lemmas. Major revisions were needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-parametric regression model $y = m(x) + \\epsilon$ and\npropose a novel graphical device to check whether the $r$-th ($r \\geqslant 1$)\nderivative of the regression function $m(x)$ is positive or otherwise. Since\nthe shape of the regression function can be completely characterized by its\nderivatives, the graphical device can correctly identify the shape of the\nregression function. The proposed device includes the check for monotonicity\nand convexity of the function as special cases. We also present an example to\nelucidate the practical utility of the graphical device. In addition, we employ\nthe graphical device to formulate a class of test statistics and derive its\nasymptotic distribution. The tests are exhibited in various simulated and real\ndata examples.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 06:44:42 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 07:28:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Rahman", "Mohammad Arshad", ""], ["Dutta", "Joydeep", ""]]}, {"id": "1910.02488", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Ying Cui, Yufeng Liu, Jong-Shi Pang", "title": "Statistical Analysis of Stationary Solutions of Coupled Nonconvex\n  Nonsmooth Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has two main goals: (a) establish several statistical\nproperties---consistency, asymptotic distributions, and convergence rates---of\nstationary solutions and values of a class of coupled nonconvex and\nnonsmoothempirical risk minimization problems, and (b) validate these\nproperties by a noisy amplitude-based phase retrieval problem, the latter being\nof much topical interest.Derived from available data via sampling, these\nempirical risk minimization problems are the computational workhorse of a\npopulation risk model which involves the minimization of an expected value of a\nrandom functional. When these minimization problems are nonconvex, the\ncomputation of their globally optimal solutions is elusive. Together with the\nfact that the expectation operator cannot be evaluated for general probability\ndistributions, it becomes necessary to justify whether the stationary solutions\nof the empirical problems are practical approximations of the stationary\nsolution of the population problem. When these two features, general\ndistribution and nonconvexity, are coupled with nondifferentiability that often\nrenders the problems \"non-Clarke regular\", the task of the justification\nbecomes challenging. Our work aims to address such a challenge within an\nalgorithm-free setting. The resulting analysis is therefore different from the\nmuch of the analysis in the recent literature that is based on local search\nalgorithms. Furthermore, supplementing the classical minimizer-centric\nanalysis, our results offer a first step to close the gap between computational\noptimization and asymptotic analysis of coupled nonconvex nonsmooth statistical\nestimation problems, expanding the former with statistical properties of the\npractically obtained solution and providing the latter with a more practical\nfocus pertaining to computational tractability.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 18:25:33 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Qi", "Zhengling", ""], ["Cui", "Ying", ""], ["Liu", "Yufeng", ""], ["Pang", "Jong-Shi", ""]]}, {"id": "1910.02546", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "A theorem of Kalman and minimal state-space realization of Vector\n  Autoregressive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY eess.SY math.AG q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a concept of $autoregressive$ (AR)state-space realization that\ncould be applied to all transfer functions $\\boldsymbol{T}(L)$ with\n$\\boldsymbol{T}(0)$ invertible. We show that a theorem of Kalman implies each\nVector Autoregressive model (with exogenous variables) has a minimal\n$AR$-state-space realization of form $\\boldsymbol{y}_t =\n\\sum_{i=1}^p\\boldsymbol{H}\\boldsymbol{F}^{i-1}\\boldsymbol{G}\\boldsymbol{x}_{t-i}+\\boldsymbol{\\epsilon}_t$\nwhere $\\boldsymbol{F}$ is a nilpotent Jordan matrix and $\\boldsymbol{H},\n\\boldsymbol{G}$ satisfy certain rank conditions. The case $VARX(1)$ corresponds\nto reduced-rank regression. Similar to that case, for a fixed Jordan form\n$\\boldsymbol{F}$, $\\boldsymbol{H}$ could be estimated by least square as a\nfunction of $\\boldsymbol{G}$. The likelihood function is a determinant ratio\ngeneralizing the Rayleigh quotient. It is unchanged if $\\boldsymbol{G}$ is\nreplaced by $\\boldsymbol{S}\\boldsymbol{G}$ for an invertible matrix\n$\\boldsymbol{S}$ commuting with $\\boldsymbol{F}$. Using this invariant\nproperty, the search space for maximum likelihood estimate could be constrained\nto equivalent classes of matrices satisfying a number of orthogonal relations,\nextending the results in reduced-rank analysis. Our results could be considered\na multi-lag canonical-correlation-analysis. The method considered here provides\na solution in the general case to the polynomial product regression model of\nVelu et. al. We provide estimation examples. We also explore how the estimates\nvary with different Jordan matrix configurations and discuss methods to select\na configuration. Our approach could provide an important dimensional reduction\ntechnique with potential applications in time series analysis and linear system\nidentification. In the appendix, we link the reduced configuration space of\n$\\boldsymbol{G}$ with a geometric object called a vector bundle.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 23:01:53 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "1910.02829", "submitter": "Ansgar Steland", "authors": "Yuan-Tsung Chang, Ansgar Steland", "title": "High-Confident Nonparametric Fixed-Width Uncertainty Intervals and\n  Applications to Projected High-Dimensional Data and Common Mean Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two-stage procedures to construct fixed-width confidence\nintervals are studied to quantify uncertainty. It is shown that the validity of\nthe random central limit theorem (RCLT) accompanied by a consistent and\nasymptotically unbiased estimator of the asymptotic variance already guarantees\nconsistency and first as well as second order efficiency of the two-stage\nprocedures. This holds under the common asymptotics where the length of the\nconfidence interval tends to $0$ as well as under the novel proposed\nhigh-confident asymptotics where the confidence level tends to $1$. The\napproach is motivated by and applicable to data analysis from distributed big\ndata with non-negligible costs of data queries. The following problems are\ndiscussed: Fixed-width intervals for a the mean, for a projection when\nobserving high-dimensional data and for the common mean when using nonlinear\ncommon mean estimators under order constraints. The procedures are investigated\nby simulations and illustrated by a real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:43:30 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chang", "Yuan-Tsung", ""], ["Steland", "Ansgar", ""]]}, {"id": "1910.02859", "submitter": "Paul McNicholas", "authors": "Nikola Pocuca, Michael P.B. Gallaugher, Katharine M. Clark and Paul D.\n  McNicholas", "title": "Assessing and Visualizing Matrix Variate Normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for assessing the matrix variate normality of three-way data is\ndeveloped. The framework comprises a visual method and a goodness of fit test\nbased on the Mahalanobis squared distance (MSD). The MSD of multivariate and\nmatrix variate normal estimators, respectively, are used as an assessment tool\nfor matrix variate normality. Specifically, these are used in the form of a\ndistance-distance (DD) plot as a graphical method for visualizing matrix\nvariate normality. In addition, we employ the popular Kolmogorov-Smirnov\ngoodness of fit test in the context of assessing matrix variate normality for\nthree-way data. Finally, an appropriate simulation study spanning a large range\nof dimensions and data sizes shows that for various settings, the test proves\nitself highly robust.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:38:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Pocuca", "Nikola", ""], ["Gallaugher", "Michael P. B.", ""], ["Clark", "Katharine M.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1910.02866", "submitter": "Dehan Kong", "authors": "Mark Koudstaal, Dengdeng Yu, Dehan Kong and Fang Yao", "title": "Nonparametric principal subspace regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific applications, multivariate observations often come in tandem\nwith temporal or spatial covariates, with which the underlying signals vary\nsmoothly. The standard approaches such as principal component analysis and\nfactor analysis neglect the smoothness of the data, while multivariate linear\nor nonparametric regression fail to leverage the correlation information among\nmultivariate response variables. We propose a novel approach named\nnonparametric principal subspace regression to overcome these issues. By\ndecoupling the model discrepancy, a simple and general two-step framework is\nintroduced, which leaves much flexibility in choice of model fitting. We\nestablish theoretical property of the general framework, and offer\nimplementation procedures that fulfill requirements and enjoy the theoretical\nguarantee. We demonstrate the favorable finite-sample performance of the\nproposed method through simulations and a real data application from an\nelectroencephalogram study.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:45:55 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 17:56:55 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 05:16:31 GMT"}, {"version": "v4", "created": "Sat, 12 Oct 2019 15:24:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Koudstaal", "Mark", ""], ["Yu", "Dengdeng", ""], ["Kong", "Dehan", ""], ["Yao", "Fang", ""]]}, {"id": "1910.02884", "submitter": "Kumar Abhishek", "authors": "Kumar Abhishek, Sneha Maheshwari, Sujit Gujar", "title": "Introduction to Concentration Inequalities", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we aim to exemplify concentration inequalities and provide\neasy to understand proofs for it. Our focus is on the inequalities which are\nhelpful in the design and analysis of machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 06:50:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Abhishek", "Kumar", ""], ["Maheshwari", "Sneha", ""], ["Gujar", "Sujit", ""]]}, {"id": "1910.02997", "submitter": "Emilija Perkovi\\'c", "authors": "Emilija Perkovi\\'c", "title": "Identifying causal effects in maximally oriented partially directed\n  acyclic graphs", "comments": "17 pages, 5 figures, 2 columns", "journal-ref": "Proceedings of UAI 2020", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a necessary and sufficient causal identification criterion for\nmaximally oriented partially directed acyclic graphs (MPDAGs). MPDAGs as a\nclass of graphs include directed acyclic graphs (DAGs), completed partially\ndirected acyclic graphs (CPDAGs), and CPDAGs with added background knowledge.\nAs such, they represent the type of graph that can be learned from\nobservational data and background knowledge under the assumption of no latent\nvariables. Our identification criterion can be seen as a generalization of the\ng-formula of Robins (1986). We further obtain a generalization of the truncated\nfactorization formula (Pearl, 2009) and compare our criterion to the\ngeneralized adjustment criterion of Perkovi\\'c et al. (2017) which is\nsufficient, but not necessary for causal identification.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:28:28 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 00:02:00 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Perkovi\u0107", "Emilija", ""]]}, {"id": "1910.03025", "submitter": "Hyenkyun Woo", "authors": "Hyenkyun Woo", "title": "Bregman-divergence-guided Legendre exponential dispersion model with\n  finite cumulants (K-LED)", "comments": "21pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT cs.LG math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential dispersion model is a useful framework in machine learning and\nstatistics. Primarily, thanks to the additive structure of the model, it can be\nachieved without difficulty to estimate parameters including mean. However,\ntight conditions on cumulant function, such as analyticity, strict convexity,\nand steepness, reduce the class of exponential dispersion model. In this work,\nwe present relaxed exponential dispersion model K-LED (Legendre exponential\ndispersion model with K cumulants). The cumulant function of the proposed model\nis a convex function of Legendre type having continuous partial derivatives of\nK-th order on the interior of a convex domain. Most of the K-LED models are\ndeveloped via Bregman-divergence-guided log-concave density function with\ncoercivity shape constraints. The main advantage of the proposed model is that\nthe first cumulant (or the mean parameter space) of the 1-LED model is easily\ncomputed through the extended global optimum property of Bregman divergence. An\nextended normal distribution is introduced as an example of 1-LED based on\nTweedie distribution. On top of that, we present 2-LED satisfying mean-variance\nrelation of quasi-likelihood function. There is an equivalence between a\nsubclass of quasi-likelihood function and a regular 2-LED model, of which the\ncanonical parameter space is open. A typical example is a regular 2-LED model\nwith power variance function, i.e., a variance is in proportion to the power of\nthe mean of observations. This model is equivalent to a subclass of\nbeta-divergence (or a subclass of quasi-likelihood function with power variance\nfunction). Furthermore, a new parameterized K-LED model, the cumulant function\nof which is the convex extended logistic loss function, is proposed. This model\nincludes Bernoulli distribution and Poisson distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 11:24:31 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Woo", "Hyenkyun", ""]]}, {"id": "1910.03134", "submitter": "Javier Zapata", "authors": "Javier Zapata, Sang-Yun Oh, Alexander Petersen", "title": "Partial Separability and Functional Graphical Models for Multivariate\n  Gaussian Processes", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The covariance structure of multivariate functional data can be highly\ncomplex, especially if the multivariate dimension is large, making extension of\nstatistical methods for standard multivariate data to the functional data\nsetting quite challenging. For example, Gaussian graphical models have recently\nbeen extended to the setting of multivariate functional data by applying\nmultivariate methods to the coefficients of truncated basis expansions.\nHowever, a key difficulty compared to multivariate data is that the covariance\noperator is compact, and thus not invertible. The methodology in this paper\naddresses the general problem of covariance modeling for multivariate\nfunctional data, and functional Gaussian graphical models in particular. As a\nfirst step, a new notion of separability for multivariate functional data is\nproposed, termed partial separability, leading to a novel Karhunen-Lo\\`eve-type\nexpansion for such data. Next, the partial separability structure is shown to\nbe particularly useful in order to provide a well-defined Gaussian graphical\nmodel that can be identified with a sequence of finite-dimensional graphical\nmodels, each of fixed dimension. This motivates a simple and efficient\nestimation procedure through application of the joint graphical lasso.\nEmpirical performance of the method for graphical model estimation is assessed\nthrough simulation and analysis of functional brain connectivity during a motor\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:42:21 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:58:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 03:16:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zapata", "Javier", ""], ["Oh", "Sang-Yun", ""], ["Petersen", "Alexander", ""]]}, {"id": "1910.03444", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Jon A. Wellner", "title": "The density ratio of Poisson binomial versus Poisson distributions", "comments": null, "journal-ref": null, "doi": "10.1016/j.spl.2020.108862", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $b(x)$ be the probability that a sum of independent Bernoulli random\nvariables with parameters $p_1, p_2, p_3, \\ldots \\in [0,1)$ equals $x$, where\n$\\lambda := p_1 + p_2 + p_3 + \\cdots$ is finite. We prove two inequalities for\nthe maximal ratio $b(x)/\\pi_\\lambda(x)$, where $\\pi_\\lambda$ is the weight\nfunction of the Poisson distribution with parameter $\\lambda$.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:05:12 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 13:41:40 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 22:47:13 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2020 08:22:16 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Duembgen", "Lutz", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1910.03643", "submitter": "Denis Belomestny", "authors": "D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov and S. Samsonov", "title": "Variance reduction for Markov chains with application to MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel variance reduction approach for additive\nfunctionals of Markov chains based on minimization of an estimate for the\nasymptotic variance of these functionals over suitable classes of control\nvariates. A distinctive feature of the proposed approach is its ability to\nsignificantly reduce the overall finite sample variance. This feature is\ntheoretically demonstrated by means of a deep non asymptotic analysis of a\nvariance reduced functional as well as by a thorough simulation study. In\nparticular we apply our method to various MCMC Bayesian estimation problems\nwhere it favourably compares to the existing variance reduction approaches.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:05:36 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 08:37:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""]]}, {"id": "1910.03660", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Mohammad Arashi and Fikri Akdeniz", "title": "Penalized regression via the restricted bridge estimator", "comments": null, "journal-ref": "Soft Computing, 2021", "doi": "10.1007/s00500-021-05763-9", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with the Bridge Regression, which is a special\nfamily in penalized regression with penalty function\n$\\sum_{j=1}^{p}|\\beta_j|^q$ with $q>0$, in a linear model with linear\nrestrictions. The proposed restricted bridge (RBRIDGE) estimator simultaneously\nestimates parameters and selects important variables when a prior information\nabout parameters are available in either low dimensional or high dimensional\ncase. Using local quadratic approximation, the penalty term can be approximated\naround a local initial values vector and the RBRIDGE estimator enjoys a\nclosed-form expression which can be solved when $q>0$. Special cases of our\nproposal are the restricted LASSO ($q=1$), restricted RIDGE ($q=2$), and\nrestricted Elastic Net ($1< q < 2$) estimators. We provide some theoretical\nproperties of the RBRIDGE estimator under for the low dimensional case, whereas\nthe computational aspects are given for both low and high dimensional cases. An\nextensive Monte Carlo simulation study is conducted based on different prior\npieces of information and the performance of the RBRIDGE estiamtor is compared\nwith some competitive penalty estimators as well as the ORACLE. We also\nconsider four real data examples analysis for comparison sake. The numerical\nresults show that the suggested RBRIDGE estimator outperforms outstandingly\nwhen the prior is true or near exact\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 19:39:16 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Arashi", "Mohammad", ""], ["Akdeniz", "Fikri", ""]]}, {"id": "1910.03669", "submitter": "Michael Perlman", "authors": "Michael D. Perlman", "title": "On the feasibility of parsimonious variable selection for Hotelling's\n  $T^2$-test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM q-bio.QM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hotelling's $T^2$-test for the mean of a multivariate normal distribution is\none of the triumphs of classical multivariate analysis. It is uniformly most\npowerful among invariant tests, and admissible, proper Bayes, and locally and\nasymptotically minimax among all tests. Nonetheless, investigators often prefer\nnon-invariant tests, especially those obtained by selecting only a small subset\nof variables from which the $T^2$-statistic is to be calculated, because such\nreduced statistics are more easily interpretable for their specific\napplication. Thus it is relevant to ask the extent to which power is lost when\nvariable selection is limited to very small subsets of variables, e.g. of size\none (yielding univariate Student-$t^2$ tests) or size two (yielding bivariate\n$T^2$-tests). This study presents some evidence, admittedly fragmentary and\nincomplete, suggesting that in some cases no power may be lost over a wide\nrange of alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:28:00 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Perlman", "Michael D.", ""]]}, {"id": "1910.03718", "submitter": "Min-Hsiu Hsieh", "authors": "Chao Zhang, Min-Hsiu Hsieh, Dacheng Tao", "title": "On Dimension-free Tail Inequalities for Sums of Random Matrices and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math-ph math.MP math.ST quant-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new framework to obtain tail inequalities for\nsums of random matrices. Compared with existing works, our tail inequalities\nhave the following characteristics: 1) high feasibility--they can be used to\nstudy the tail behavior of various matrix functions, e.g., arbitrary matrix\nnorms, the absolute value of the sum of the sum of the $j$ largest singular\nvalues (resp. eigenvalues) of complex matrices (resp. Hermitian matrices); and\n2) independence of matrix dimension --- they do not have the matrix-dimension\nterm as a product factor, and thus are suitable to the scenario of\nhigh-dimensional or infinite-dimensional random matrices. The price we pay to\nobtain these advantages is that the convergence rate of the resulting\ninequalities will become slow when the number of summand random matrices is\nlarge. We also develop the tail inequalities for matrix random series and\nmatrix martingale difference sequence. We also demonstrate usefulness of our\ntail bounds in several fields. In compressed sensing, we employ the resulted\ntail inequalities to achieve a proof of the restricted isometry property when\nthe measurement matrix is the sum of random matrices without any assumption on\nthe distributions of matrix entries. In probability theory, we derive a new\nupper bound to the supreme of stochastic processes. In machine learning, we\nprove new expectation bounds of sums of random matrices matrix and obtain\nmatrix approximation schemes via random sampling. In quantum information, we\nshow a new analysis relating to the fractional cover number of quantum\nhypergraphs. In theoretical computer science, we obtain randomness-efficient\nsamplers using matrix expander graphs that can be efficiently implemented in\ntime without dependence on matrix dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 23:38:51 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zhang", "Chao", ""], ["Hsieh", "Min-Hsiu", ""], ["Tao", "Dacheng", ""]]}, {"id": "1910.03821", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi, Matteo Luciani", "title": "Quasi Maximum Likelihood Estimation and Inference of Large Approximate\n  Dynamic Factor Models via the EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Quasi Maximum Likelihood estimation of dynamic factor\nmodels for large panels of time series. Specifically, we consider the case in\nwhich the autocorrelation of the factors is explicitly accounted for and\ntherefore the model has a state-space form. Estimation of the factors and of\ntheir loadings is implemented by means of the Expectation Maximization (EM)\nalgorithm, jointly with the Kalman smoother.~We prove that, as both the\ndimension of the panel $n$ and the sample size $T$ diverge to infinity: (i) the\nestimated loadings are $\\sqrt T$-consistent and asymptotically normal if $\\sqrt\nT/n\\to 0$; (ii) the estimated factors are $\\sqrt n$-consistent and\nasymptotically normal if $\\sqrt n/T\\to 0$; (iii) the estimated common component\nis $\\min(\\sqrt T,\\sqrt n)$-consistent and asymptotically normal regardless of\nthe relative rate of divergence of $n$ and $T$. Although the model is estimated\nas if the idiosyncratic terms were cross-sectionally and serially uncorrelated,\nwe show that these mis-specifications do not affect consistency. Moreover, the\nestimated loadings are asymptotically as efficient as those obtained with the\nPrincipal Components estimator, whereas numerical results show that the loss in\nefficiency of the estimated factors becomes negligible as $n$ and $T$\nincrease.~We then propose robust estimators of the asymptotic covariances,\nwhich can be used to conduct inference on the loadings and to compute\nconfidence intervals for the factors and common components. In a MonteCarlo\nsimulation exercise and an analysis of US macroeconomic data, we study the\nperformance of our estimators and we compare them with the traditional\nPrincipal Components approach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 07:31:59 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:47:21 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 10:00:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Luciani", "Matteo", ""]]}, {"id": "1910.03834", "submitter": "Song Liu Dr.", "authors": "Song Liu, Takafumi Kanamori, Daniel J. Williams", "title": "Estimating Density Models with Truncation Boundaries using Score\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated densities are probability density functions defined on truncated\ndomains. They share the same parametric form with their non-truncated\ncounterparts up to a normalizing constant. Since the computation of their\nnormalizing constants is usually infeasible, Maximum Likelihood Estimation\ncannot be easily applied to estimate truncated density models. Score Matching\n(SM) is a powerful tool for fitting parameters using only unnormalized models.\nHowever, it cannot be directly applied here as boundary conditions used to\nderive a tractable SM objective are not satisfied by truncated densities. In\nthis paper, we study parameter estimation for truncated probability densities\nusing SM. The estimator minimizes a weighted Fisher divergence. The weight\nfunction is simply the shortest distance from the data point to the boundary of\nthe domain. We show this choice of weight function naturally arises from\nminimizing the Stein discrepancy as well as upperbounding the finite-sample\nstatistical estimation error. The usefulness of our method is demonstrated by\nnumerical experiments and a Chicago crime dataset. We also show that proposed\ndensity estimation can correct the outlier-trimming bias caused by aggressive\noutlier detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:18:20 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 18:01:45 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 00:22:22 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 15:01:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Song", ""], ["Kanamori", "Takafumi", ""], ["Williams", "Daniel J.", ""]]}, {"id": "1910.03911", "submitter": "Yuncai Yu", "authors": "Yuncai Yu, Xinsheng Liu, Ling Liu, Weisi Liu", "title": "On adaptivity of wavelet thresholding estimators with negatively\n  super-additive dependent noise", "comments": "14 pages;2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the nonparametric regression model with negatively\nsuper-additive dependent (NSD) noise and investigates the convergence rates of\nthresholding estimators. It is shown that the term-by-term thresholding\nestimator achieves nearly optimal and the block thresholding estimator attains\noptimal (or nearly optimal) convergence rates over Besov spaces. Additionally,\nsome numerical simulations are implemented to substantiate the validity and\nadaptivity of the thresholding estimators with the presence of NSD noise.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 11:41:46 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Yu", "Yuncai", ""], ["Liu", "Xinsheng", ""], ["Liu", "Ling", ""], ["Liu", "Weisi", ""]]}, {"id": "1910.04085", "submitter": "Guillaume Staerman", "authors": "Guillaume Staerman, Pavlo Mozharovskyi and Stephan Cl\\'emen\\c{c}on", "title": "The Area of the Convex Hull of Sampled Curves: a Robust Functional\n  Statistical Depth Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ubiquity of sensors in the IoT era, statistical observations are\nbecoming increasingly available in the form of massive (multivariate)\ntime-series. Formulated as unsupervised anomaly detection tasks, an abundance\nof applications like aviation safety management, the health monitoring of\ncomplex infrastructures or fraud detection can now rely on such functional\ndata, acquired and stored with an ever finer granularity. The concept of\nstatistical depth, which reflects centrality of an arbitrary observation w.r.t.\na statistical population may play a crucial role in this regard, anomalies\ncorresponding to observations with 'small' depth. Supported by sound\ntheoretical and computational developments in the recent decades, it has proven\nto be extremely useful, in particular in functional spaces. However, most\napproaches documented in the literature consist in evaluating independently the\ncentrality of each point forming the time series and consequently exhibit a\ncertain insensitivity to possible shape changes. In this paper, we propose a\nnovel notion of functional depth based on the area of the convex hull of\nsampled curves, capturing gradual departures from centrality, even beyond the\nenvelope of the data, in a natural fashion. We discuss practical relevance of\ncommonly imposed axioms on functional depths and investigate which of them are\nsatisfied by the notion of depth we promote here. Estimation and computational\nissues are also addressed and various numerical experiments provide empirical\nevidence of the relevance of the approach proposed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:13 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:14:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Staerman", "Guillaume", ""], ["Mozharovskyi", "Pavlo", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""]]}, {"id": "1910.04086", "submitter": "David Ginsbourger", "authors": "Poompol Buathong, David Ginsbourger, Tipaluck Krityakierne", "title": "Kernels over Sets of Finite Sets using RKHS Embeddings, with Application\n  to Bayesian (Combinatorial) Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on kernel methods for set-valued inputs and their application to\nBayesian set optimization, notably combinatorial optimization. We investigate\ntwo classes of set kernels that both rely on Reproducing Kernel Hilbert Space\nembeddings, namely the ``Double Sum'' (DS) kernels recently considered in\nBayesian set optimization, and a class introduced here called ``Deep\nEmbedding'' (DE) kernels that essentially consists in applying a radial kernel\non Hilbert space on top of the canonical distance induced by another kernel\nsuch as a DS kernel. We establish in particular that while DS kernels typically\nsuffer from a lack of strict positive definiteness, vast subclasses of DE\nkernels built upon DS kernels do possess this property, enabling in turn\ncombinatorial optimization without requiring to introduce a jitter parameter.\nProofs of theoretical results about considered kernels are complemented by a\nfew practicalities regarding hyperparameter fitting. We furthermore demonstrate\nthe applicability of our approach in prediction and optimization tasks, relying\nboth on toy examples and on two test cases from mechanical engineering and\nhydrogeology, respectively. Experimental results highlight the applicability\nand compared merits of the considered approaches while opening new perspectives\nin prediction and sequential design with set inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:38 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:55:58 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Buathong", "Poompol", ""], ["Ginsbourger", "David", ""], ["Krityakierne", "Tipaluck", ""]]}, {"id": "1910.04102", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Miko{\\l}aj Kasprzak, Trevor Campbell, Tamara\n  Broderick", "title": "Validated Variational Inference via Practical Posterior Error Bounds", "comments": "A python package for carrying out our validated variational inference\n  workflow -- including doing black-box variational inference and computing the\n  bounds we develop in this paper -- is available at\n  https://github.com/jhuggins/viabel. The same repository also contains code\n  for reproducing all of our experiments", "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational inference has become an increasingly attractive fast alternative\nto Markov chain Monte Carlo methods for approximate Bayesian inference.\nHowever, a major obstacle to the widespread use of variational methods is the\nlack of post-hoc accuracy measures that are both theoretically justified and\ncomputationally efficient. In this paper, we provide rigorous bounds on the\nerror of posterior mean and uncertainty estimates that arise from\nfull-distribution approximations, as in variational inference. Our bounds are\nwidely applicable, as they require only that the approximating and exact\nposteriors have polynomial moments. Our bounds are also computationally\nefficient for variational inference because they require only standard values\nfrom variational objectives, straightforward analytic calculations, and simple\nMonte Carlo estimates. We show that our analysis naturally leads to a new and\nimproved workflow for validated variational inference. Finally, we demonstrate\nthe utility of our proposed workflow and error bounds on a robust regression\nproblem and on a real-data example with a widely used multilevel hierarchical\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:29:21 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 14:44:19 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:04:06 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 04:06:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1910.04259", "submitter": "Rafail Kartsioukas", "authors": "Rafail Kartsioukas, Zheng Gao, Stilian Stoev", "title": "On the rate of concentration of maxima in Gaussian arrays", "comments": "29 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently in Gao and Stoev (2018) it was established that the concentration of\nmaxima phenomenon is the key to solving the exact sparse support recovery\nproblem in high dimensions. This phenomenon, known also as relative stability,\nhas been little studied in the context of dependence. Here, we obtain bounds on\nthe rate of concentration of maxima in Gaussian triangular arrays. These\nresults are used to establish sufficient conditions for the uniform relative\nstability of functions of Gaussian arrays, leading to new models that exhibit\nphase transitions in the exact support recovery problem. Finally, the optimal\nrate of concentration for Gaussian arrays is studied under more general\nassumptions than the ones implied by the classic condition of Berman (1964).\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:24:16 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 01:59:47 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kartsioukas", "Rafail", ""], ["Gao", "Zheng", ""], ["Stoev", "Stilian", ""]]}, {"id": "1910.04267", "submitter": "Changxiao Cai", "authors": "Changxiao Cai, Gen Li, Yuejie Chi, H. Vincent Poor, Yuxin Chen", "title": "Subspace Estimation from Unbalanced and Incomplete Data Matrices:\n  $\\ell_{2,\\infty}$ Statistical Guarantees", "comments": "Accepted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with estimating the column space of an unknown\nlow-rank matrix $\\boldsymbol{A}^{\\star}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$,\ngiven noisy and partial observations of its entries. There is no shortage of\nscenarios where the observations -- while being too noisy to support faithful\nrecovery of the entire matrix -- still convey sufficient information to enable\nreliable estimation of the column space of interest. This is particularly\nevident and crucial for the highly unbalanced case where the column dimension\n$d_{2}$ far exceeds the row dimension $d_{1}$, which is the focal point of the\ncurrent paper. We investigate an efficient spectral method, which operates upon\nthe sample Gram matrix with diagonal deletion. While this algorithmic idea has\nbeen studied before, we establish new statistical guarantees for this method in\nterms of both $\\ell_{2}$ and $\\ell_{2,\\infty}$ estimation accuracy, which\nimprove upon prior results if $d_{2}$ is substantially larger than $d_{1}$. To\nillustrate the effectiveness of our findings, we derive matching minimax lower\nbounds with respect to the noise levels, and develop consequences of our\ngeneral theory for three applications of practical importance: (1) tensor\ncompletion from noisy data, (2) covariance estimation / principal component\nanalysis with missing data, and (3) community recovery in bipartite graphs. Our\ntheory leads to improved performance guarantees for all three cases.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 21:39:04 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 22:16:18 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 14:01:08 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 00:25:35 GMT"}, {"version": "v5", "created": "Sun, 15 Nov 2020 21:04:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cai", "Changxiao", ""], ["Li", "Gen", ""], ["Chi", "Yuejie", ""], ["Poor", "H. Vincent", ""], ["Chen", "Yuxin", ""]]}, {"id": "1910.04333", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Efficient Estimation for Random Dot Product Graphs via a One-step\n  Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a one-step procedure to estimate the latent positions in random\ndot product graphs efficiently. Unlike the classical spectral-based methods\nsuch as the adjacency and Laplacian spectral embedding, the proposed one-step\nprocedure takes advantage of both the low-rank structure of the expected\nadjacency matrix and the Bernoulli likelihood information of the sampling model\nsimultaneously. We show that for each vertex, the corresponding row of the\none-step estimator converges to a multivariate normal distribution after proper\nscaling and centering up to an orthogonal transformation, with an efficient\ncovariance matrix. The initial estimator for the one-step procedure needs to\nsatisfy the so-called approximate linearization property. The one-step\nestimator improves the commonly-adopted spectral embedding methods in the\nfollowing sense: Globally for all vertices, it yields an asymptotic sum of\nsquares error no greater than those of the spectral methods, and locally for\neach vertex, the asymptotic covariance matrix of the corresponding row of the\none-step estimator dominates those of the spectral embeddings in spectra. The\nusefulness of the proposed one-step procedure is demonstrated via numerical\nexamples and the analysis of a real-world Wikipedia graph dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:25:45 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 00:53:31 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1910.04355", "submitter": "Guang Cheng", "authors": "Jincheng Bai, Qifan Song, Guang Cheng", "title": "Adaptive Variational Bayesian Inference for Sparse Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on variational Bayesian inference on the sparse Deep\nNeural Network (DNN) modeled under a class of spike-and-slab priors. Given a\npre-specified sparse DNN structure, the corresponding variational posterior\ncontraction rate is characterized that reveals a trade-off between the\nvariational error and the approximation error, which are both determined by the\nnetwork structural complexity (i.e., depth, width and sparsity). However, the\noptimal network structure, which strikes the balance of the aforementioned\ntrade-off and yields the best rate, is generally unknown in reality. Therefore,\nour work further develops an {\\em adaptive} variational inference procedure\nthat can automatically select a reasonably good (data-dependent) network\nstructure that achieves the best contraction rate, without knowing the optimal\nnetwork structure. In particular, when the true function is H{\\\"o}lder smooth,\nthe adaptive variational inference is capable to attain (near-)optimal rate\nwithout the knowledge of smoothness level. The above rate still suffers from\nthe curse of dimensionality, and thus motivates the teacher-student setup,\ni.e., the true function is a sparse DNN model, under which the rate only\nlogarithmically depends on the input dimension.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 03:44:09 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 21:26:17 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 02:44:59 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bai", "Jincheng", ""], ["Song", "Qifan", ""], ["Cheng", "Guang", ""]]}, {"id": "1910.04408", "submitter": "Tiebin Mi", "authors": "Tiebin Mi and Robert Caiming Qiu", "title": "Asymptotics of empirical eigenvalues for large separable covariance\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the asymptotics of eigenvalues of sample covariance matrices\nassociated with a class of non-independent Gaussian processes (separable and\ntemporally stationary) under the Kolmogorov asymptotic regime. The limiting\nspectral distribution (LSD) is shown to depend explicitly on the Kolmogorov\nconstant (a fixed limiting ratio of the sample size to the dimensionality) and\nparameters representing the spatio- and temporal- correlations. The Cauchy, M-\nand N-transforms from free harmonic analysis play key roles to this LSD\ncalculation problem. The free multiplication law of free random variables is\nemployed to give a semi-closed-form expression (only the final step is\nnumerical based) of the LSD for the spatio-covariance matrix being a diagonally\ndominant Wigner matrix and temporal-covariance matrix an exponential\noff-diagonal decay (Toeplitz) matrix. Furthermore, we also derive a nonlinear\nshrinkage estimator for the top eigenvalues associated with a low rank matrix\n(Hermitian) from its noisy measurements. Numerical studies about the\neffectiveness of the estimator are also presented.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 07:42:32 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Mi", "Tiebin", ""], ["Qiu", "Robert Caiming", ""]]}, {"id": "1910.04460", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Louis Pujol", "title": "Still no free lunches: the price to pay for tighter PAC-Bayes bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \"No free lunch\" results state the impossibility of obtaining meaningful\nbounds on the error of a learning algorithm without prior assumptions and\nmodelling. Some models are expensive (strong assumptions, such as as\nsubgaussian tails), others are cheap (simply finite variance). As it is well\nknown, the more you pay, the more you get: in other words, the most expensive\nmodels yield the more interesting bounds. Recent advances in robust statistics\nhave investigated procedures to obtain tight bounds while keeping the cost\nminimal. The present paper explores and exhibits what the limits are for\nobtaining tight PAC-Bayes bounds in a robust setting for cheap models,\naddressing the question: is PAC-Bayes good value for money?\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:01:02 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Guedj", "Benjamin", ""], ["Pujol", "Louis", ""]]}, {"id": "1910.04464", "submitter": "Benjamin Guedj", "authors": "Kento Nozawa and Pascal Germain and Benjamin Guedj", "title": "PAC-Bayesian Contrastive Unsupervised Representation Learning", "comments": "Published in the proceedings of the Conference on Uncertainty in\n  Artificial Intelligence 2020 (UAI)", "journal-ref": "PMLR, volume 124 (UAI 2020), 2020", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contrastive unsupervised representation learning (CURL) is the\nstate-of-the-art technique to learn representations (as a set of features) from\nunlabelled data. While CURL has collected several empirical successes recently,\ntheoretical understanding of its performance was still missing. In a recent\nwork, Arora et al. (2019) provide the first generalisation bounds for CURL,\nrelying on a Rademacher complexity. We extend their framework to the flexible\nPAC-Bayes setting, allowing us to deal with the non-iid setting. We present\nPAC-Bayesian generalisation bounds for CURL, which are then used to derive a\nnew representation learning algorithm. Numerical experiments on real-life\ndatasets illustrate that our algorithm achieves competitive accuracy, and\nyields non-vacuous generalisation bounds.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:13:01 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 05:21:33 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 18:30:17 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 13:41:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nozawa", "Kento", ""], ["Germain", "Pascal", ""], ["Guedj", "Benjamin", ""]]}, {"id": "1910.04610", "submitter": "Hiroaki Kaido", "authors": "Hiroaki Kaido, Yi Zhang", "title": "Robust Likelihood Ratio Tests for Incomplete Economic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a framework for testing hypotheses on structural\nparameters in incomplete models. Such models make set-valued predictions and\nhence do not generally yield a unique likelihood function. The model structure,\nhowever, allows us to construct tests based on the least favorable pairs of\nlikelihoods using the theory of Huber and Strassen (1973). We develop tests\nrobust to model incompleteness that possess certain optimality properties. We\nalso show that sharp identifying restrictions play a role in constructing such\ntests in a computationally tractable manner. A framework for analyzing the\nlocal asymptotic power of the tests is developed by embedding the least\nfavorable pairs into a model that allows local approximations under the limits\nof experiments argument. Examples of the hypotheses we consider include those\non the presence of strategic interaction effects in discrete games of complete\ninformation. Monte Carlo experiments demonstrate the robust performance of the\nproposed tests.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 14:41:10 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 15:24:17 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kaido", "Hiroaki", ""], ["Zhang", "Yi", ""]]}, {"id": "1910.04832", "submitter": "Sui Tang", "authors": "Fei Lu, Mauro Maggioni, Sui Tang", "title": "Learning interaction kernels in heterogeneous systems of agents from\n  multiple trajectories", "comments": "63 pages, revised various places", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems of interacting particles or agents have wide applications in many\ndisciplines such as Physics, Chemistry, Biology and Economics. These systems\nare governed by interaction laws, which are often unknown: estimating them from\nobservation data is a fundamental task that can provide meaningful insights and\naccurate predictions of the behaviour of the agents. In this paper, we consider\nthe inverse problem of learning interaction laws given data from multiple\ntrajectories, in a nonparametric fashion, when the interaction kernels depend\non pairwise distances. We establish a condition for learnability of interaction\nkernels, and construct estimators that are guaranteed to converge in a suitable\n$L^2$ space, at the optimal min-max rate for 1-dimensional nonparametric\nregression. We propose an efficient learning algorithm based on least squares,\nwhich can be implemented in parallel for multiple trajectories and is therefore\nwell-suited for the high dimensional, big data regime. Numerical simulations on\na variety examples, including opinion dynamics, predator-swarm dynamics and\nheterogeneous particle dynamics, suggest that the learnability condition is\nsatisfied in models used in practice, and the rate of convergence of our\nestimator is consistent with the theory. These simulations also suggest that\nour estimators are robust to noise in the observations, and produce accurate\npredictions of dynamics in relative large time intervals, even when they are\nlearned from data collected in short time intervals.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 19:54:04 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:44:42 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 00:48:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Lu", "Fei", ""], ["Maggioni", "Mauro", ""], ["Tang", "Sui", ""]]}, {"id": "1910.04900", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Aaditya Ramdas", "title": "Online control of the familywise error rate", "comments": "Submitted to Biostatistics; added the real data example; renewed some\n  of the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological research often involves testing a growing number of null\nhypotheses as new data is accumulated over time. We study the problem of online\ncontrol of the familywise error rate (FWER), that is testing an apriori\nunbounded sequence of hypotheses (p-values) one by one over time without\nknowing the future, such that with high probability there are no false\ndiscoveries in the entire sequence. This paper unifies algorithmic concepts\ndeveloped for offline (single batch) FWER control and online false discovery\nrate control to develop novel online FWER control methods. Though many offline\nFWER methods (e.g. Bonferroni, fallback procedures and Sidak's method) can\ntrivially be extended to the online setting, our main contribution is the\ndesign of new, powerful, adaptive online algorithms that control the FWER when\nthe p-values are independent or locally dependent in time. Our experiments\ndemonstrate substantial gains in power, that are also formally proved in a\nGaussian sequence model. Multiple testing, FWER control, online setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 22:50:03 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:21:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tian", "Jinjin", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1910.04930", "submitter": "Qilong Gu", "authors": "Arindam Banerjee, Qilong Gu, Vidyashankar Sivakumar, and Zhiwei Steven\n  Wu", "title": "Random Quadratic Forms with Dependence: Applications to Restricted\n  Isometry and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important families of computational and statistical results in\nmachine learning and randomized algorithms rely on uniform bounds on quadratic\nforms of random vectors or matrices. Such results include the\nJohnson-Lindenstrauss (J-L) Lemma, the Restricted Isometry Property (RIP),\nrandomized sketching algorithms, and approximate linear algebra. The existing\nresults critically depend on statistical independence, e.g., independent\nentries for random vectors, independent rows for random matrices, etc., which\nprevent their usage in dependent or adaptive modeling settings. In this paper,\nwe show that such independence is in fact not needed for such results which\ncontinue to hold under fairly general dependence structures. In particular, we\npresent uniform bounds on random quadratic forms of stochastic processes which\nare conditionally independent and sub-Gaussian given another (latent) process.\nOur setup allows general dependencies of the stochastic process on the history\nof the latent process and the latent process to be influenced by realizations\nof the stochastic process. The results are thus applicable to adaptive modeling\nsettings and also allows for sequential design of random vectors and matrices.\nWe also discuss stochastic process based forms of J-L, RIP, and sketching, to\nillustrate the generality of the results.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:30:46 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 05:32:02 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Banerjee", "Arindam", ""], ["Gu", "Qilong", ""], ["Sivakumar", "Vidyashankar", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1910.04993", "submitter": "Pascal Helson", "authors": "Pascal Helson (TOSCA, MATHNEURO)", "title": "A Mathematical Analysis of Memory Lifetime in a simple Network Model of\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the learning of an external signal by a neural network and the time\nto forget it when this network is submitted to noise. The presentation of an\nexternal stimulus to the recurrent network of binary neurons may change the\nstate of the synapses. Multiple presentations of a unique signal leads to its\nlearning. Then, during the forgetting time, the presentation of other signals\n(noise) may also modify the synaptic weights. We construct an estimator of the\ninitial signal thanks to the synaptic currents and define by this way a\nprobability of error. In our model, these synaptic currents evolve as Markov\nchains. We study the dynamics of these Markov chains and obtain a lower bound\non the number of external stimuli that the network can receive before the\ninitial signal is considered as forgotten (probability of error above a given\nthreshold). Our results hold for finite size networks as well as in the large\nsize asymptotic. Our results are based on a finite time analysis rather than\nlarge time asymptotic. We finally present numerical illustrations of our\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:48:21 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:31:17 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 15:05:49 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Helson", "Pascal", "", "TOSCA, MATHNEURO"]]}, {"id": "1910.05084", "submitter": "Hariharan Narayanan", "authors": "Charles Fefferman, Sergei Ivanov, Matti Lassas, Hariharan Narayanan", "title": "Fitting a manifold of large reach to noisy data", "comments": "77 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\mathcal M}\\subset {\\mathbb R}^n$ be a $C^2$-smooth compact submanifold\nof dimension $d$. Assume that the volume of ${\\mathcal M}$ is at most $V$ and\nthe reach (i.e.\\ the normal injectivity radius) of ${\\mathcal M}$ is greater\nthan $\\tau$. Moreover, let $\\mu$ be a probability measure on ${\\mathcal M}$\nwhich density on ${\\mathcal M}$ is a strictly positive Lipschitz-smooth\nfunction. Let $x_j\\in {\\mathcal M}$, $j=1,2,\\dots,N$ be $N$ independent random\nsamples from distribution $\\mu$. Also, let $\\xi_j$, $j=1,2,\\dots, N$ be\nindependent random samples from a Gaussian random variable in ${\\mathbb R}^n$\nhaving covariance $\\sigma^2I$, where $\\sigma$ is less than a certain specified\nfunction of $d, V$ and $\\tau$. We assume that we are given the data points\n$y_j=x_j+\\xi_j,$ $j=1,2,\\dots,N$, modelling random points of ${\\mathcal M}$\nwith measurement noise. We develop an algorithm which produces from these data,\nwith high probability, a $d$ dimensional submanifold ${\\mathcal M}_o\\subset\n{\\mathbb R}^n$ whose Hausdorff distance to ${\\mathcal M}$ is less than\n$Cd\\sigma^2/\\tau$ and whose reach is greater than $c{\\tau}/d^6$ with universal\nconstants $C,c > 0$. The number $N$ of random samples required depends almost\nlinearly on $n$, polynomially on $\\sigma^{-1}$ and exponentially on $d$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:24:19 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 05:38:08 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 13:36:57 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Fefferman", "Charles", ""], ["Ivanov", "Sergei", ""], ["Lassas", "Matti", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "1910.05087", "submitter": "Benito Hern\\'andez-Bermejo", "authors": "Benito Hern\\'andez-Bermejo, Albert Sorribas", "title": "Analytical Quantile Solution for the S-distribution, Random Number\n  Generation and Statistical Data Modeling", "comments": "20 pages, 6 Figures", "journal-ref": "Biometrical Journal 43(8), 1007-1025 (2001)", "doi": "10.1002/1521-4036(200112)43:8<1007::AID-BIMJ1007>3.0.CO;2-F", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of a specific statistical distribution is seldom a simple\nproblem. One strategy consists in testing different distributions (normal,\nlognormal, Weibull, etc.), and selecting the one providing the best fit to the\nobserved data and being the most parsimonious. Alternatively, one can make a\nchoice based on theoretical arguments and simply fit the corresponding\nparameters to the observed data. In either case, different distributions can\ngive similar results and provide almost equivalent results. Model selection can\nbe more complicated when the goal is to describe a trend in the distribution of\na given variable. In those cases, changes in shape and skewness are difficult\nto represent by a single distributional form. As an alternative to the use of\ncomplicated families of distributions as models for data, the S-distribution\n[{\\sc Voit, E.O. }(1992) Biom.J. 7:855-878] provides a highly flexible\nmathematical form in which the density is defined as a function of the\ncumulative. Besides representing well-known distributions, S-distributions\nprovide an infinity of new possibilities that do not correspond with known\nclassical distributions. In this paper we obtain an analytical solution for the\nquantile equation that highly simplifies the use of S-distributions. We show\nthe utility of this solution in different applications. After classifying the\ndifferent qualitative behaviors of the S-distribution in parameter space, we\nshow how to obtain different S-distributions that accomplish specific\nconstraints. One interesting case is the possibility of obtaining distributions\nthat acomplish P(X <= X_c)=0. Then, we show that the quantile solution\nfacilitates the use of S-distributions in Monte-Carlo experiments through the\ngeneration of random samples. Finally, we show how to fit an S-distribution to\nactual data, so that the resulting distribution can be used as a statistical\nmodel for them.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:32:14 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Hern\u00e1ndez-Bermejo", "Benito", ""], ["Sorribas", "Albert", ""]]}, {"id": "1910.05269", "submitter": "Andrew Ledoan", "authors": "Christopher Corley and Andrew Ledoan", "title": "The density of complex zeros of random sums", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{\\eta_{j}\\}_{j = 0}^{N}$ be a sequence of independent, identically\ndistributed random complex Gaussian variables, and let $\\{f_{j} (z)\\}_{j =\n0}^{N}$ be a sequence of given analytic functions that are real-valued on the\nreal number line. We prove an exact formula for the expected density of the\ndistribution of complex zeros of the random equation $\\sum_{j = 0}^{N} \\eta_{j}\nf_{j} (z) = \\mathbf{K}$, where $\\mathbf{K} \\in \\mathds{C}$. The method of proof\nemploys a formula for the expected absolute value of quadratic forms of\nGaussian random variables. We then obtain the limiting behaviour of the density\nfunction as $N$ tends to infinity and provide numerical computations for the\ndensity function and empirical distributions for random sums with certain\nfunctions $f_{j} (z)$. Finally, we study the case when the $f_{j} (z)$ are\npolynomials orthogonal on the real line and the unit circle.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 16:00:54 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Corley", "Christopher", ""], ["Ledoan", "Andrew", ""]]}, {"id": "1910.05270", "submitter": "Aryeh Kontorovich", "authors": "Klim Efremenko, Aryeh Kontorovich, Moshe Noivirt", "title": "Fast and Bayes-consistent nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on nearest-neighbor methods tends to focus somewhat dichotomously\neither on the statistical or the computational aspects -- either on, say, Bayes\nconsistency and rates of convergence or on techniques for speeding up the\nproximity search. This paper aims at bridging these realms: to reap the\nadvantages of fast evaluation time while maintaining Bayes consistency, and\nfurther without sacrificing too much in the risk decay rate. We combine the\nlocality-sensitive hashing (LSH) technique with a novel missing-mass argument\nto obtain a fast and Bayes-consistent classifier. Our algorithm's prediction\nruntime compares favorably against state of the art approximate NN methods,\nwhile maintaining Bayes-consistency and attaining rates comparable to minimax.\nOn samples of size $n$ in $\\R^d$, our pre-processing phase has runtime $O(d n\n\\log n)$, while the evaluation phase has runtime $O(d\\log n)$ per query point.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:46:37 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 21:15:47 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 21:46:51 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Efremenko", "Klim", ""], ["Kontorovich", "Aryeh", ""], ["Noivirt", "Moshe", ""]]}, {"id": "1910.05480", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec, Arun K Kuchibhotla", "title": "First order expansion of convex regularized estimators", "comments": "Accepted at NeurIPS 2019 and published at\n  https://papers.nips.cc/paper/8606-first-order-expansion-of-convex-regularized-estimators\n  . The version here includes the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider first order expansions of convex penalized estimators in\nhigh-dimensional regression problems with random designs. Our setting includes\nlinear regression and logistic regression as special cases. For a given penalty\nfunction $h$ and the corresponding penalized estimator $\\hat\\beta$, we\nconstruct a quantity $\\eta$, the first order expansion of $\\hat\\beta$, such\nthat the distance between $\\hat\\beta$ and $\\eta$ is an order of magnitude\nsmaller than the estimation error $\\|\\hat{\\beta} - \\beta^*\\|$. In this sense,\nthe first order expansion $\\eta$ can be thought of as a generalization of\ninfluence functions from the mathematical statistics literature to regularized\nestimators in high-dimensions. Such first order expansion implies that the risk\nof $\\hat{\\beta}$ is asymptotically the same as the risk of $\\eta$ which leads\nto a precise characterization of the MSE of $\\hat\\beta$; this characterization\ntakes a particularly simple form for isotropic design. Such first order\nexpansion also leads to inference results based on $\\hat{\\beta}$. We provide\nsufficient conditions for the existence of such first order expansion for three\nregularizers: the Lasso in its constrained form, the lasso in its penalized\nform, and the Group-Lasso. The results apply to general loss functions under\nsome conditions and those conditions are satisfied for the squared loss in\nlinear regression and for the logistic loss in the logistic model.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 03:56:44 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 17:20:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Bellec", "Pierre C", ""], ["Kuchibhotla", "Arun K", ""]]}, {"id": "1910.05486", "submitter": "Edsel Pena", "authors": "Edsel A. Pena", "title": "The Search for Truth through Data: NP Decision Processes, ROC Functions,\n  $P$-Functionals, Knowledge Updating and Sequential Learning", "comments": "39 pages with several figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-visits the problem of deciding between two simple hypotheses,\nthe setting considered by Neyman and Pearson in developing their fundamental\nlemma. It studies the decision process induced by the most powerful test and\nthe receiver operating characteristic function associated with this decision\nprocess. It addresses the question of how to report the decision arising from\nthe decision function. It also examines the P-functional (the P-value\nstatistic) and its role in the decision-making process. The impetus of this\nwork is the continuing criticisms of statistical decision-making procedures\nthat use the P-functional and a level of significance (LoS) of 0.05. A point\nmade is that if one is going to use the value of the P-functional, then it\nshould be used in an equivalent manner as the most powerful decision function,\nbut if one wants to obtain from its value the degree of support for either\nhypotheses, then the value of its density under the alternative is the proper\nquantity to use. Replicability of results are discussed. Knowledge updating\nthrough Bayes theorem when given the decision or the value of the P-functional\nis also discussed, and it is argued that sequential learning is a coherent way\nof finding the truth. But the impact of publication bias is also demonstrated\nto be quite serious in the search for truth. It is argued that decision-makers\nare free to choose their own LoS, since the additional summary measures will\nautomatically take their LoS choices into consideration. Three approaches for\nchoosing an optimal LoS are discussed and a procedure for sample size\ndetermination is described. Ideas are illustrated by concrete problems and by\nthe lady tea-tasting experiment of Fisher which ushered null hypothesis\nsignificance testing. It is hoped that by considering this fundamental setting\nof simple hypotheses, a better understanding of more complex settings will\nensue.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 04:18:49 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 14:00:19 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Pena", "Edsel A.", ""]]}, {"id": "1910.05701", "submitter": "Zheng Gao", "authors": "Zheng Gao", "title": "Five Shades of Grey: Phase Transitions in High-dimensional Multiple\n  Testing", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by marginal screenings of categorical variables, and study\nhigh-dimensional multiple testing problems where test statistics have\napproximate chi-square distributions. We characterize four new phase\ntransitions in high-dimensional chi-square models, and derive the signal sizes\nnecessary and sufficient for statistical procedures to simultaneously control\nfalse discovery (in terms of family-wise error rate or false discovery rate)\nand missed detection (in terms of family-wise non-discovery rate or false\nnon-discovery rate) in large dimensions. Remarkably, degrees of freedom in the\nchi-square distributions do not affect the boundaries in all four phase\ntransitions. Several well-known procedures are shown to attain these\nboundaries. Two new phase transitions are also identified in the Gaussian\nlocation model under one-sided alternatives.\n  We then elucidate on the nature of signal sizes in association tests by\ncharacterizing its relationship with marginal frequencies, odds ratio, and\nsample sizes in $2\\times2$ contingency tables. This allows us to illustrate an\ninteresting manifestation of the phase transition phenomena in genome-wide\nassociation studies (GWAS). We also show, perhaps surprisingly, that given\ntotal sample sizes, balanced designs in such association studies rarely deliver\noptimal power.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:50:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Gao", "Zheng", ""]]}, {"id": "1910.05904", "submitter": "Jun Yang", "authors": "Robert M. Anderson and Haosui Duanmu and Aaron Smith and Jun Yang", "title": "Drift, Minorization, and Hitting Times", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"drift-and-minorization\" method, introduced and popularized in\n(Rosenthal, 1995; Meyn and Tweedie, 1994; Meyn and Tweedie, 2012), remains the\nmost popular approach for bounding the convergence rates of Markov chains used\nin statistical computation. This approach requires estimates of two quantities:\nthe rate at which a single copy of the Markov chain \"drifts\" towards a fixed\n\"small set\", and a \"minorization condition\" which gives the worst-case time for\ntwo Markov chains started within the small set to couple with moderately large\nprobability. In this paper, we build on (Oliveira, 2012; Peres and Sousi, 2015)\nand our work (Anderson, Duanmu, Smith, 2019a; Anderson, Duanmu, Smith, 2019b)\nto replace the \"minorization condition\" with an alternative \"hitting condition\"\nthat is stated in terms of only one Markov chain, and illustrate how this can\nbe used to obtain similar bounds that can be easier to use.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 03:21:42 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 19:03:47 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 08:09:11 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Anderson", "Robert M.", ""], ["Duanmu", "Haosui", ""], ["Smith", "Aaron", ""], ["Yang", "Jun", ""]]}, {"id": "1910.05956", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy, Rainer Dyckerhoff and Pavlo Mozharovskyi", "title": "Uniform convergence rates for the approximated halfspace and projection\n  depth", "comments": null, "journal-ref": "Electron. J. Statist. 14 (2) 3939 - 3975, 2020", "doi": "10.1214/20-EJS1759", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of some depths that satisfy the projection\nproperty, such as the halfspace depth or the projection depth, is known to be\nhigh, especially for data of higher dimensionality. In such scenarios, the\nexact depth is frequently approximated using a randomized approach: The data\nare projected into a finite number of directions uniformly distributed on the\nunit sphere, and the minimal depth of these univariate projections is used to\napproximate the true depth. We provide a theoretical background for this\napproximation procedure. Several uniform consistency results are established,\nand the corresponding uniform convergence rates are provided. For elliptically\nsymmetric distributions and the halfspace depth it is shown that the obtained\nuniform convergence rates are sharp. In particular, guidelines for the choice\nof the number of random projections in order to achieve a given precision of\nthe depths are stated.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 07:25:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Nagy", "Stanislav", ""], ["Dyckerhoff", "Rainer", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1910.06028", "submitter": "Vladimir Spokoiny", "authors": "Vladimir Spokoiny and Maxim Panov", "title": "Accuracy of Gaussian approximation in nonparametric Bernstein -- von\n  Mises Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prominent Bernstein -- von Mises (BvM) result claims that the posterior\ndistribution after centering by the efficient estimator and standardizing by\nthe square root of the total Fisher information is nearly standard normal. In\nparticular, the prior completely washes out from the asymptotic posterior\ndistribution. This fact is fundamental and justifies the Bayes approach from\nthe frequentist viewpoint. In the nonparametric setup the situation changes\ndramatically and the impact of prior becomes essential even for the contraction\nof the posterior; see [vdV2008], [Bo2011], [CaNi2013,CaNi2014] for different\nmodels like Gaussian regression or i.i.d. model in different weak topologies.\nThis paper offers another non-asymptotic approach to studying the behavior of\nthe posterior for a special but rather popular and useful class of statistical\nmodels and for Gaussian priors. First we derive tight finite sample bounds on\nposterior contraction in terms of the so called effective dimension of the\nparameter space. Our main results describe the accuracy of Gaussian\napproximation of the posterior. In particular, we show that restricting to the\nclass of all centrally symmetric credible sets around pMLE allows to get\nGaussian approximation up to order (n^{-1}). We also show that the posterior\ndistribution mimics well the distribution of the penalized maximum likelihood\nestimator (pMLE) and reduce the question of reliability of credible sets to\nconsistency of the pMLE-based confidence sets. The obtained results are\nspecified for nonparametric log-density estimation and generalized regression.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 10:32:49 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 14:53:47 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 15:48:24 GMT"}, {"version": "v4", "created": "Mon, 30 Dec 2019 09:12:10 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2020 10:40:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Spokoiny", "Vladimir", ""], ["Panov", "Maxim", ""]]}, {"id": "1910.06081", "submitter": "Leszek Szczecinski", "authors": "Leszek Szczecinski and Aymen Djebbi", "title": "Understanding and Pushing the Limits of the Elo Rating Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with the rating of players/teams in face-to-face games\nwith three possible outcomes: loss, win, and draw. This is one of the\nfundamental problems in sport analytics, where the very simple and popular,\nnon-trivial algorithm was proposed by Arpad Elo in late fifties to rate chess\nplayers. In this work we explain the mathematical model underlying the Elo\nalgorithm and, in particular, we explain what is the implicit but not yet\nspelled out, assumption about the model of draws. We further extend the model\nto provide flexibility and remove the unrealistic implicit assumptions of the\nElo algorithm. This yields the new rating algorithm, we call $\\kappa$-Elo,\nwhich is equally simple as the Elo algorithm but provides a possibility to\nadjust to the frequency of draws. The discussion of the importance of the\nappropriate choice of the parameters is carried out and illustrated using\nresults from English Premier League football seasons.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 22:23:02 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Szczecinski", "Leszek", ""], ["Djebbi", "Aymen", ""]]}, {"id": "1910.06235", "submitter": "Shuang Zhou", "authors": "Shuang Zhou, Debdeep Pati, Tianying Wang, Yun Yang and Raymond J.\n  Carroll", "title": "Gaussian Processes with Errors in Variables: Theory and Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate measurement error in nonparametric regression is a common problem\nin nutritional epidemiology and geostatistics, and other fields. Over the last\ntwo decades, this problem has received substantial attention in the frequentist\nliterature. Bayesian approaches for handling measurement error have only been\nexplored recently and are surprisingly successful, although the lack of a\nproper theoretical justification regarding the asymptotic performance of the\nestimators. By specifying a Gaussian process prior on the regression function\nand a Dirichlet process Gaussian mixture prior on the unknown distribution of\nthe unobserved covariates, we show that the posterior distribution of the\nregression function and the unknown covariates density attain optimal rates of\ncontraction adaptively over a range of H\\\"{o}lder classes, up to logarithmic\nterms. This improves upon the existing classical frequentist results which\nrequire knowledge of the smoothness of the underlying function to deliver\noptimal risk bounds. We also develop a novel surrogate prior for approximating\nthe Gaussian process prior that leads to efficient computation and preserves\nthe covariance structure, thereby facilitating easy prior elicitation. We\ndemonstrate the empirical performance of our approach and compare it with\ncompetitors in a wide range of simulation experiments and a real data example.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:09:41 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhou", "Shuang", ""], ["Pati", "Debdeep", ""], ["Wang", "Tianying", ""], ["Yang", "Yun", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1910.06324", "submitter": "Fengpei Li", "authors": "Henry Lam, Fengpei Li, Siddharth Prusty", "title": "Robust Importance Weighting for Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning problems, the training and testing data follow different\ndistributions and a particularly common situation is the \\textit{covariate\nshift}. To correct for sampling biases, most approaches, including the popular\nkernel mean matching (KMM), focus on estimating the importance weights between\nthe two distributions. Reweighting-based methods, however, are exposed to high\nvariance when the distributional discrepancy is large and the weights are\npoorly estimated. On the other hand, the alternate approach of using\nnonparametric regression (NR) incurs high bias when the training size is\nlimited. In this paper, we propose and analyze a new estimator that\nsystematically integrates the residuals of NR with KMM reweighting, based on a\ncontrol-variate perspective. The proposed estimator can be shown to either\nstrictly outperform or match the best-known existing rates for both KMM and NR,\nand thus is a robust combination of both estimators. The experiments shows the\nestimator works well in practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:20:04 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 22:23:46 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Lam", "Henry", ""], ["Li", "Fengpei", ""], ["Prusty", "Siddharth", ""]]}, {"id": "1910.06386", "submitter": "Arun Kuchibhotla", "authors": "Arun K. Kuchibhotla, Lawrence D. Brown, Andreas Buja and Junhui Cai", "title": "All of Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares linear regression is one of the oldest and widely used data\nanalysis tools. Although the theoretical analysis of the ordinary least squares\n(OLS) estimator is as old, several fundamental questions are yet to be\nanswered. Suppose regression observations\n$(X_1,Y_1),\\ldots,(X_n,Y_n)\\in\\mathbb{R}^d\\times\\mathbb{R}$ (not necessarily\nindependent) are available. Some of the questions we deal with are as follows:\nunder what conditions, does the OLS estimator converge and what is the limit?\nWhat happens if the dimension is allowed to grow with $n$? What happens if the\nobservations are dependent with dependence possibly strengthening with $n$? How\nto do statistical inference under these kinds of misspecification? What happens\nto the OLS estimator under variable selection? How to do inference under\nmisspecification and variable selection?\n  We answer all the questions raised above with one simple deterministic\ninequality which holds for any set of observations and any sample size. This\nimplies that all our results are a finite sample (non-asymptotic) in nature. In\nthe end, one only needs to bound certain random quantities under specific\nsettings of interest to get concrete rates and we derive these bounds for the\ncase of independent observations. In particular, the problem of inference after\nvariable selection is studied, for the first time, when $d$, the number of\ncovariates increases (almost exponentially) with sample size $n$. We provide\ncomments on the ``right'' statistic to consider for inference under variable\nselection and efficient computation of quantiles.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 19:21:46 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Brown", "Lawrence D.", ""], ["Buja", "Andreas", ""], ["Cai", "Junhui", ""]]}, {"id": "1910.06420", "submitter": "Lohit Vandanapu", "authors": "Lohit Vandanapu, Michael D. Shields", "title": "3rd-order Spectral Representation Method: Part I -- Multi-dimensional\n  random fields with fast Fourier transform implementation", "comments": "62 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generalised 3rd-order Spectral Representation Method\nfor the simulation of multi-dimensional stochastic fields with asymmetric\nnon-linearities. The simulated random fields satisfy a prescribed Power\nSpectrum and Bispectrum. The general d-dimensional simulation equations are\npresented and the method is applied to simulate 2D and 3D random fields. The\ndifferences between samples generated by the proposed methodology and the\nexisting classical Spectral Representation Method are analysed. An important\nfeature of this methodology is that the formula can be implemented efficiently\nwith the Fast Fourier Transform, details of which are presented. Computational\nsavings are shown to grow exponentially with dimensionality as a testament of\nthe scalability of the simulation methodology.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:59:35 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 22:06:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Vandanapu", "Lohit", ""], ["Shields", "Michael D.", ""]]}, {"id": "1910.06623", "submitter": "Johannes Hertrich", "authors": "Marzieh Hasannasab, Johannes Hertrich, Friederike Laus, Gabriele\n  Steidl", "title": "Alternatives to the EM Algorithm for ML-Estimation of Location, Scatter\n  Matrix and Degree of Freedom of the Student-$t$ Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider maximum likelihood estimations of the degree of\nfreedom parameter $\\nu$, the location parameter $\\mu$ and the scatter matrix\n$\\Sigma$ of the multivariate Student-$t$ distribution. In particular, we are\ninterested in estimating the degree of freedom parameter $\\nu$ that determines\nthe tails of the corresponding probability density function and was rarely\nconsidered in detail in the literature so far. We prove that under certain\nassumptions a minimizer of the negative log-likelihood function exists, where\nwe have to take special care of the case $\\nu \\rightarrow \\infty$, for which\nthe Student-$t$ distribution approaches the Gaussian distribution. As\nalternatives to the classical EM algorithm we propose three other algorithms\nwhich cannot be interpreted as EM algorithm. For fixed $\\nu$, the first\nalgorithm is an accelerated EM algorithm known from the literature. However,\nsince we do not fix $\\nu$, we cannot apply standard convergence results for the\nEM algorithm. The other two algorithms differ from this algorithm in the\niteration step for $\\nu$. We show how the objective function behaves for the\ndifferent updates of $\\nu$ and prove for all three algorithms that it decreases\nin each iteration step. We compare the algorithms as well as some accelerated\nversions by numerical simulation and apply one of them for estimating the\ndegree of freedom parameter in images corrupted by Student-$t$ noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:57:08 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 09:52:45 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Hasannasab", "Marzieh", ""], ["Hertrich", "Johannes", ""], ["Laus", "Friederike", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1910.06742", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Generic Bounds on the Maximum Deviations in Sequential Prediction: An\n  Information-Theoretic Analysis", "comments": "arXiv admin note: text overlap with arXiv:1904.04765. text overlap\n  with arXiv:2001.03813", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive generic bounds on the maximum deviations in\nprediction errors for sequential prediction via an information-theoretic\napproach. The fundamental bounds are shown to depend only on the conditional\nentropy of the data point to be predicted given the previous data points. In\nthe asymptotic case, the bounds are achieved if and only if the prediction\nerror is white and uniformly distributed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:31:17 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:54:54 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 15:01:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1910.06846", "submitter": "Dan Vilenchik", "authors": "Guy Holtzman, Adam Soffer and Dan Vilenchik", "title": "A greedy anytime algorithm for sparse PCA", "comments": "improving results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The taxing computational effort that is involved in solving some\nhigh-dimensional statistical problems, in particular problems involving\nnon-convex optimization, has popularized the development and analysis of\nalgorithms that run efficiently (polynomial-time) but with no general guarantee\non statistical consistency. In light of the ever-increasing compute power and\ndecreasing costs, a more useful characterization of algorithms is by their\nability to calibrate the invested computational effort with various\ncharacteristics of the input at hand and with the available computational\nresources. For example, design an algorithm that always guarantees statistical\nconsistency of its output by increasing the running time as the SNR weakens. We\npropose a new greedy algorithm for the $\\ell_0$-sparse PCA problem which\nsupports the calibration principle. We provide both a rigorous analysis of our\nalgorithm in the spiked covariance model, as well as simulation results and\ncomparison with other existing methods. Our findings show that our algorithm\nrecovers the spike in SNR regimes where all polynomial-time algorithms fail\nwhile running in a reasonable parallel-time on a cluster.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:09:13 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 19:47:25 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 16:07:42 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 13:44:58 GMT"}, {"version": "v5", "created": "Wed, 12 Feb 2020 12:32:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Holtzman", "Guy", ""], ["Soffer", "Adam", ""], ["Vilenchik", "Dan", ""]]}, {"id": "1910.06914", "submitter": "Natalia Bochkina", "authors": "Natalia Bochkina and Jenovah Rodrigues", "title": "Bayesian Inverse Problems with Heterogeneous Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inverse problems in Hilbert spaces contaminated by Gaussian\nnoise, and use a Bayesian approach to find its regularised smooth solution. We\nconsider the so called conjugate diagonal setting where the covariance\noperators of the noise and of the prior are diagnolisable in the orthogonal\nbases associated with the forward operator of the inverse problem. Firstly, we\nderive the minimax rate of convergence in such problems with known covariance\noperator of the noise, showing that in the case of heterogeneous variance the\nill posed inverse problem can become self regularised in some cases when the\neigenvalues of the variance operator decay to zero, achieving parametric rate\nof convergence; as far as we are aware, this is a striking novel result that\nhave not been observed before in nonparametric problems. Secondly, we give a\ngeneral expression of the rate of contraction of the posterior distribution in\ncase of known noise covariance operator in case the noise level is small, for a\ngiven prior distribution. We also investigate when this contraction rate\ncoincides with the optimal rate in the minimax sense which is typically used as\na benchmark for studying the posterior contraction rates. We apply our results\nto known variance operators with polynomially decreasing or increasing\neigenvalues as an example. We also discuss when the plug in estimator of the\neigenvalues of the covariance operator of the noise does not affect the rate of\nthe contraction of the posterior distribution of the signal. We show that\nplugging in the maximum marginal likelihood estimator of the prior scaling\nparameter leads to the optimal posterior contraction rate, adaptively. Effect\nof the choice of the prior parameters on the contraction in such models is\nillustrated on simulated data with Volterra operator.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:40:37 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 09:56:12 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 20:04:22 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Bochkina", "Natalia", ""], ["Rodrigues", "Jenovah", ""]]}, {"id": "1910.06996", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Csaba Szepesvari", "title": "Adaptive Exploration in Linear Contextual Bandit", "comments": "Accepted at AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits serve as a fundamental model for many sequential decision\nmaking tasks. The most popular theoretically justified approaches are based on\nthe optimism principle. While these algorithms can be practical, they are known\nto be suboptimal asymptotically. On the other hand, existing asymptotically\noptimal algorithms for this problem do not exploit the linear structure in an\noptimal way and suffer from lower-order terms that dominate the regret in all\npractically interesting regimes. We start to bridge the gap by designing an\nalgorithm that is asymptotically optimal and has good finite-time empirical\nperformance. At the same time, we make connections to the recent literature on\nwhen exploration-free methods are effective. Indeed, if the distribution of\ncontexts is well behaved, then our algorithm acts mostly greedily and enjoys\nsub-logarithmic regret. Furthermore, our approach is adaptive in the sense that\nit automatically detects the nice case. Numerical results demonstrate\nsignificant regret reductions by our method relative to several baselines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:26:52 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 01:01:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1910.07095", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, James V. Burke, and Daiwei He", "title": "IRLS for Sparse Recovery Revisited: Examples of Failure and a Remedy", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing is a central topic in signal processing with myriad\napplications, where the goal is to recover a signal from as few observations as\npossible. Iterative re-weighting is one of the fundamental tools to achieve\nthis goal. This paper re-examines the iteratively reweighted least squares\n(IRLS) algorithm for sparse recovery proposed by Daubechies, Devore, Fornasier,\nand G\\\"unt\\\"urk in \\emph{Iteratively reweighted least squares minimization for\nsparse recovery}, {\\sf Communications on Pure and Applied Mathematics}, {\\bf\n63}(2010) 1--38. Under the null space property of order $K$, the authors show\nthat their algorithm converges to the unique $k$-sparse solution for $k$\nstrictly bounded above by a value strictly less than $K$, and this $k$-sparse\nsolution coincides with the unique $\\ell_1$ solution. On the other hand, it is\nknown that, for $k$ less than or equal to $K$, the $k$-sparse and $\\ell_1$\nsolutions are unique and coincide. The authors emphasize that their proof\nmethod does not apply for $k$ sufficiently close to $K$, and remark that they\nwere unsuccessful in finding an example where the algorithm fails for these\nvalues of $k$.\n  In this note we construct a family of examples where the\nDaubechies-Devore-Fornasier-G\\\"unt\\\"urk IRLS algorithm fails for $k=K$, and\nprovide a modification to their algorithm that provably converges to the unique\n$k$-sparse solution for $k$ less than or equal to $K$ while preserving the\nlocal linear rate. The paper includes numerical studies of this family as well\nas the modified IRLS algorithm, testing their robustness under perturbations\nand to parameter selection.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 23:10:19 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["He", "Daiwei", ""]]}, {"id": "1910.07120", "submitter": "Shuyang (Ray) Bai", "authors": "Shuyang Bai", "title": "Representations of Hermite processes using local time of intersecting\n  stationary stable regenerative sets", "comments": "17 pages, to appear in Journal of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hermite processes are a class of self-similar processes with stationary\nincrements. They often arise in limit theorems under long-range dependence. We\nderive new representations of Hermite processes with multiple Wiener-It\\^o\nintegrals, whose integrands involve the local time of intersecting stationary\nstable regenerative sets. The proof relies on an approximation of regenerative\nsets and local times based on a scheme of random interval covering.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:20:17 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 20:02:46 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 14:28:20 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Bai", "Shuyang", ""]]}, {"id": "1910.07158", "submitter": "Chuancun Yin", "authors": "Chuancun Yin", "title": "Stochastic Orderings of Multivariate Elliptical Distributions", "comments": "21pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\bf X}$ and ${\\bf X}$ be two $n$-dimensional elliptical random vectors,\nwe establish an identity for $E[f({\\bf Y})]-E[f({\\bf X})]$, where $f: \\Bbb{R}^n\n\\rightarrow \\Bbb{R}$ fulfilling some regularity conditions. Using this identity\nwe provide a unified derivation of sufficient and necessary conditions for\nclassifying multivariate elliptical random vectors according to several main\nintegral stochastic orders. As a consequence we obtain new inequalities by\napplying it to multivariate elliptical distributions. The results generalize\nthe corresponding ones for multivariate normal random vectors in the\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:50:37 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 08:09:02 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 12:23:54 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Yin", "Chuancun", ""]]}, {"id": "1910.07183", "submitter": "Xu Zhang", "authors": "Xu Zhang, Wei Cui, Yulong Liu", "title": "Covariance Matrix Estimation from Correlated Sub-Gaussian Samples", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating a covariance matrix from\ncorrelated sub-Gaussian samples. We consider using the correlated sample\ncovariance matrix estimator to approximate the true covariance matrix. We\nestablish non-asymptotic error bounds for this estimator in both real and\ncomplex cases. Our theoretical results show that the error bounds are\ndetermined by the signal dimension $n$, the sample size $m$ and the correlation\npattern $\\textbf{B}$. In particular, when the correlation pattern $\\textbf{B}$\nsatisfies $tr(\\textbf{B})=m$, $||\\textbf{B}||_{F}=O(m^{1/2})$, and\n$||\\textbf{B}||=O(1)$, these results reveal that $O(n)$ samples are sufficient\nto accurately estimate the covariance matrix from correlated sub-Gaussian\nsamples. Numerical simulations are presented to show the correctness of the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 06:18:38 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zhang", "Xu", ""], ["Cui", "Wei", ""], ["Liu", "Yulong", ""]]}, {"id": "1910.07200", "submitter": "Saman Hosseini", "authors": "Saman Hosseini, Dler Hussein Kadir, Kostas Triantafyllopoulos", "title": "Lomax distribution and asymptotical ML estimations based on record\n  values for probability density function and cumulative distribution function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here in this paper, it is tried to obtain and compare the ML estimations\nbased on upper record values and a random sample. In continue, some theorems\nhave been proven about the behavior of these estimations asymptotically.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:50:31 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 15:51:08 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hosseini", "Saman", ""], ["Kadir", "Dler Hussein", ""], ["Triantafyllopoulos", "Kostas", ""]]}, {"id": "1910.07213", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Youssef Esstafa (LMB), Bruno\n  Saussereau (LMB)", "title": "Estimating FARIMA models with uncorrelated but non-independent error\n  terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive the asymptotic properties of the least squares\nestimator (LSE) of fractionally integrated autoregressive moving-average\n(FARIMA) models under the assumption that the errors are uncorrelated but not\nnecessarily independent nor martingale differences. We relax considerably the\nindependence and even the martingale difference assumptions on the innovation\nprocess to extend the range of application of the FARIMA models. We propose a\nconsistent estimator of the asymptotic covariance matrix of the LSE which may\nbe very different from that obtained in the standard framework. A\nself-normalized approach to confidence interval construction for weak FARIMA\nmodel parameters is also presented. All our results are done under a mixing\nassumption on the noise. Finally, some simulation studies and an application to\nthe daily returns of stock market indices are presented to corroborate our\ntheoretical work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:25:55 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:35:10 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Esstafa", "Youssef", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1910.07341", "submitter": "Hiba Nassar", "authors": "Xijia Liu, Hiba Nassar, Krzysztof Podg\\'Orski", "title": "Splinets -- efficient orthonormalization of the B-splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new efficient orthogonalization of the B-spline basis is proposed and\ncontrasted with some previous orthogonalized methods. The resulting orthogonal\nbasis of splines is best visualized as a net of functions rather than a\nsequence of them. For this reason, the basis is referred to as a splinet. The\nsplinets feature clear advantages over other spline bases. They efficiently\nexploit 'near-orthogonalization' featured by the B-splines and gains are\nachieved at two levels: locality that is exhibited through small size of the\ntotal support of a splinet and computational efficiency that follows from a\nsmall number of orthogonalization procedures needed to be performed on the\nB-splines to achieve orthogonality. These efficiencies are formally proven by\nshowing the asymptotic rates with respect to the number of elements in a\nsplinet. The natural symmetry of the B-splines in the case of the equally\nspaced knots is preserved in the splinets, while quasi-symmetrical features are\nalso seen for the case of arbitrarily spaced knots.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:43:14 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 10:12:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Liu", "Xijia", ""], ["Nassar", "Hiba", ""], ["Podg\u00d3rski", "Krzysztof", ""]]}, {"id": "1910.07343", "submitter": "Matteo Giordano", "authors": "Matteo Giordano and Richard Nickl", "title": "Consistency of Bayesian inference with Gaussian process priors in an\n  elliptic inverse problem", "comments": "34 pages, to appear in Inverse Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.AP math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $\\mathcal{O}$ a bounded domain in $\\mathbb{R}^d$ and a given smooth\nfunction $g:\\mathcal{O}\\to\\mathbb{R}$, we consider the statistical nonlinear\ninverse problem of recovering the conductivity $f>0$ in the divergence form\nequation $$\n  \\nabla\\cdot(f\\nabla u)=g\\ \\textrm{on}\\ \\mathcal{O}, \\quad\n  u=0\\ \\textrm{on}\\ \\partial\\mathcal{O}, $$ from $N$ discrete noisy point\nevaluations of the solution $u=u_f$ on $\\mathcal O$. We study the statistical\nperformance of Bayesian nonparametric procedures based on a flexible class of\nGaussian (or hierarchical Gaussian) process priors, whose implementation is\nfeasible by MCMC methods. We show that, as the number $N$ of measurements\nincreases, the resulting posterior distributions concentrate around the true\nparameter generating the data, and derive a convergence rate $N^{-\\lambda},\n\\lambda>0,$ for the reconstruction error of the associated posterior means, in\n$L^2(\\mathcal{O})$-distance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:45:48 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 14:16:08 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 13:31:09 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Giordano", "Matteo", ""], ["Nickl", "Richard", ""]]}, {"id": "1910.07434", "submitter": "Asad Lodhia", "authors": "Asad Lodhia, Keith Levin, Elizaveta Levina", "title": "Matrix Means and a Novel High-Dimensional Shrinkage Phenomenon", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical settings call for estimating a population parameter, most\ntypically the population mean, based on a sample of matrices. The most natural\nestimate of the population mean is the arithmetic mean, but there are many\nother matrix means that may behave differently, especially in high dimensions.\nHere we consider the matrix harmonic mean as an alternative to the arithmetic\nmatrix mean. We show that in certain high-dimensional regimes, the harmonic\nmean yields an improvement over the arithmetic mean in estimation error as\nmeasured by the operator norm. Counter-intuitively, studying the asymptotic\nbehavior of these two matrix means in a spiked covariance estimation problem,\nwe find that this improvement in operator norm error does not imply better\nrecovery of the leading eigenvector. We also show that a Rao-Blackwellized\nversion of the harmonic mean is equivalent to a linear shrinkage estimator\nstudied previously in the high-dimensional covariance estimation literature,\nwhile applying a similar Rao-Blackwellization to regularized sample covariance\nmatrices yields a novel nonlinear shrinkage estimator. Simulations complement\nthe theoretical results, illustrating the conditions under which the harmonic\nmatrix mean yields an empirically better estimate.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:52:16 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 15:11:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lodhia", "Asad", ""], ["Levin", "Keith", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1910.07572", "submitter": "Tetsuya Kaji", "authors": "Tetsuya Kaji", "title": "Asymptotic Theory of $L$-Statistics and Integrable Empirical Processes", "comments": "30 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops asymptotic theory of integrals of empirical quantile\nfunctions with respect to random weight functions, which is an extension of\nclassical $L$-statistics. They appear when sample trimming or Winsorization is\napplied to asymptotically linear estimators. The key idea is to consider\nempirical processes in the spaces appropriate for integration. First, we\ncharacterize weak convergence of empirical distribution functions and random\nweight functions in the space of bounded integrable functions. Second, we\nestablish the delta method for empirical quantile functions as integrable\nfunctions. Third, we derive the delta method for $L$-statistics. Finally, we\nprove weak convergence of their bootstrap processes, showing validity of\nnonparametric bootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 19:01:51 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Kaji", "Tetsuya", ""]]}, {"id": "1910.07635", "submitter": "Veronika Rockova", "authors": "Ismael Castillo and Veronika Rockova", "title": "Uncertainty Quantification for Bayesian CART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work affords new insights into Bayesian CART in the context of\nstructured wavelet shrinkage. The main thrust is to develop a formal\ninferential framework for Bayesian tree-based regression. We reframe Bayesian\nCART as a g-type prior which departs from the typical wavelet product priors by\nharnessing correlation induced by the tree topology. The practically used\nBayesian CART priors are shown to attain adaptive near rate-minimax posterior\nconcentration in the supremum norm in regression models. For the fundamental\ngoal of uncertainty quantification, we construct adaptive confidence bands for\nthe regression function with uniform coverage under self-similarity. In\naddition, we show that tree-posteriors enable optimal inference in the form of\nefficient confidence sets for smooth functionals of the regression function.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:56:31 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 17:54:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Castillo", "Ismael", ""], ["Rockova", "Veronika", ""]]}, {"id": "1910.07689", "submitter": "Zheng Fang", "authors": "Zheng Fang, Juwon Seo", "title": "A Projection Framework for Testing Shape Restrictions That Form Convex\n  Cones", "comments": "A previous version of this paper was circulated under the title \"A\n  General Framework for Inference on Shape Restrictions.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a uniformly valid and asymptotically nonconservative test\nbased on projection for a class of shape restrictions. The key insight we\nexploit is that these restrictions form convex cones, a simple and yet elegant\nstructure that has been barely harnessed in the literature. Based on a\nmonotonicity property afforded by such a geometric structure, we construct a\nbootstrap procedure that, unlike many studies in nonstandard settings,\ndispenses with estimation of local parameter spaces, and the critical values\nare obtained in a way as simple as computing the test statistic. Moreover, by\nappealing to strong approximations, our framework accommodates nonparametric\nregression models as well as distributional/density-related and structural\nsettings. Since the test entails a tuning parameter (due to the nonstandard\nnature of the problem), we propose a data-driven choice and prove its validity.\nMonte Carlo simulations confirm that our test works well.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 03:00:42 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 02:52:22 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 00:19:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fang", "Zheng", ""], ["Seo", "Juwon", ""]]}, {"id": "1910.07698", "submitter": "Wenpin Tang", "authors": "Xin Guo, Fengmin Tang, Wenpin Tang", "title": "The Buckley-Osthus model and the block preferential attachment model:\n  statistical analysis and application", "comments": "12 pages, 2 figures, 4 tables. This paper is published by\n  http://proceedings.mlr.press/v119/tang20b.html", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning (ICML 2020), PMLR 119, 9377-9386", "doi": null, "report-no": null, "categories": "math.ST cs.NI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with statistical estimation of two preferential\nattachment models: the Buckley-Osthus model and the hierarchical preferential\nattachment model. We prove that the maximum likelihood estimates for both\nmodels are consistent. We perform simulation studies to corroborate our\ntheoretical findings. We also apply both models to study the evolution of a\nreal-world network. A list of open problems are presented.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 03:43:48 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 05:05:53 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Guo", "Xin", ""], ["Tang", "Fengmin", ""], ["Tang", "Wenpin", ""]]}, {"id": "1910.07773", "submitter": "Masaaki Imaizumi", "authors": "Masaaki Imaizumi, Hirofumi Ota, Takuo Hamaguchi", "title": "Hypothesis Test and Confidence Analysis with Wasserstein Distance with\n  General Dimension", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for statistical inference with the Wasserstein\ndistance. Recently, the Wasserstein distance has attracted much attention and\nbeen applied to various machine learning tasks due to its celebrated\nproperties. Despite the importance, hypothesis tests and confidence analysis\nwith the Wasserstein distance have not been available in a general setting,\nsince a limit distribution of empirical distribution with Wasserstein distance\nhas been unavailable without strong restrictions. In this study, we develop a\nnovel \\textit{non-asymptotic Gaussian approximation} for the empirical\nWasserstein distance, which can avoid the problem of unavailable limit\ndistribution. By the approximation method, we develop a hypothesis test and\nconfidence analysis for the empirical Wasserstein distance. We also provide a\ntheoretical guarantee and an efficient algorithm for the proposed\napproximation. Our experiments validate its performance numerically.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 08:48:26 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 13:38:39 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Imaizumi", "Masaaki", ""], ["Ota", "Hirofumi", ""], ["Hamaguchi", "Takuo", ""]]}, {"id": "1910.07816", "submitter": "J\\'anos Marcell Benke", "authors": "J\\'anos Marcell Benke, Gyula Pap", "title": "Nearly unstable family of stochastic processes given by stochastic\n  differential equations with time delay", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $a$ be a finite signed measure on $[-r, 0]$ with $r \\in (0, \\infty)$.\nConsider a stochastic process $(X^{(\\vartheta)}(t))_{t\\in[-r,\\infty)}$ given by\na linear stochastic delay differential equation \\[ \\mathrm{d}\nX^{(\\vartheta)}(t) = \\vartheta \\int_{[-r,0]} X^{(\\vartheta)}(t + u) \\,\na(\\mathrm{d} u) \\, \\mathrm{d} t + \\mathrm{d} W(t) , \\qquad t \\ge 0, \\] where\n$\\vartheta \\in \\mathbb{R}$ is a parameter and $(W(t))_{t\\ge 0}$ is a standard\nWiener process. Consider a point $\\vartheta \\in \\mathbb{R}$, where this model\nis unstable in the sense that it is locally asymptotically Brownian functional\nwith certain scalings $(r_{\\vartheta,T})_{T\\in(0,\\infty)}$ satisfying\n$r_{\\vartheta,T} \\to 0$ as $T \\to \\infty$. A family\n$\\{(X^{(\\vartheta_T)}(t))_{t\\in[-r,T]} : T \\in (0, \\infty)\\}$ is said to be\nnearly unstable as $T \\to \\infty$ if $\\vartheta_T \\to \\vartheta$ as $T \\to\n\\infty$. For every $\\alpha \\in \\mathbb{R}$, we prove convergence of the\nlikelihood ratio processes of the nearly unstable families\n$\\{(X^{(\\vartheta+\\alpha \\ r_{\\vartheta,T})}(t))_{t\\in[-r,T]}: T \\in (0,\n\\infty)\\}$ as $T \\to \\infty$. As a consequence, we obtain weak convergence of\nthe maximum likelihood estimator $\\hat{\\alpha}_T$ of $\\alpha$ based on the\nobservations $(X^{(\\vartheta+\\alpha \\ r_{\\vartheta,T})}(t))_{t\\in[-r,T]}$ as $T\n\\to \\infty$. It turns out that the limit distribution of $\\hat{\\alpha}_T$ as $T\n\\to \\infty$ can be represented as the maximum likelihood estimator of a\nparameter of a process satisfying a stochastic differential equation without\ntime delay.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 10:36:51 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Benke", "J\u00e1nos Marcell", ""], ["Pap", "Gyula", ""]]}, {"id": "1910.07912", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Rafael Frongillo, Jana Hlavinov\\'a, Birgit Rudloff", "title": "Forecast Evaluation of Quantiles, Prediction Intervals, and other\n  Set-Valued Functionals", "comments": "46 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1907.01306", "journal-ref": "lectronic Journal of Statistics, Volume 15, Number 1 (2021),\n  1034-1084", "doi": "10.1214/21-EJS1808", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a theoretical framework of elicitability and identifiability of\nset-valued functionals, such as quantiles, prediction intervals, and systemic\nrisk measures. A functional is elicitable if it is the unique minimiser of an\nexpected scoring function, and identifiable if it is the unique zero of an\nexpected identification function; both notions are essential for forecast\nranking and validation, and $M$- and $Z$-estimation. Our framework\ndistinguishes between exhaustive forecasts, being set-valued and aiming at\ncorrectly specifying the entire functional, and selective forecasts, content\nwith solely specifying a single point in the correct functional. We establish a\nmutual exclusivity result: A set-valued functional can be either selectively\nelicitable or exhaustively elicitable or not elicitable at all. Notably, since\nquantiles are well known to be selectively elicitable, they fail to be\nexhaustively elicitable. We further show that the class of prediction intervals\nand Vorob'ev quantiles turn out to be exhaustively elicitable and selectively\nidentifiable. In particular, we provide a mixture representation of elementary\nexhaustive scores, leading the way to Murphy diagrams. We give possibility and\nimpossibility results for the shortest prediction interval and prediction\nintervals specified by an endpoint or a midpoint. We end with a comprehensive\nliterature review on common practice in forecast evaluation of set-valued\nfunctionals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:23:48 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 19:05:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fissler", "Tobias", ""], ["Frongillo", "Rafael", ""], ["Hlavinov\u00e1", "Jana", ""], ["Rudloff", "Birgit", ""]]}, {"id": "1910.08018", "submitter": "Xinjie Fan", "authors": "Xinjie Fan, Yuguang Yue, Purnamrita Sarkar, Y. X. Rachel Wang", "title": "A Unified Framework for Tuning Hyperparameters in Clustering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting hyperparameters for unsupervised learning problems is challenging\nin general due to the lack of ground truth for validation. Despite the\nprevalence of this issue in statistics and machine learning, especially in\nclustering problems, there are not many methods for tuning these\nhyperparameters with theoretical guarantees. In this paper, we provide a\nframework with provable guarantees for selecting hyperparameters in a number of\ndistinct models. We consider both the subgaussian mixture model and network\nmodels to serve as examples of i.i.d. and non-i.i.d. data. We demonstrate that\nthe same framework can be used to choose the Lagrange multipliers of penalty\nterms in semi-definite programming (SDP) relaxations for community detection,\nand the bandwidth parameter for constructing kernel similarity matrices for\nspectral clustering. By incorporating a cross-validation procedure, we show the\nframework can also do consistent model selection for network models. Using a\nvariety of simulated and real data examples, we show that our framework\noutperforms other widely used tuning procedures in a broad range of parameter\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:40:42 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 02:12:38 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Fan", "Xinjie", ""], ["Yue", "Yuguang", ""], ["Sarkar", "Purnamrita", ""], ["Wang", "Y. X. Rachel", ""]]}, {"id": "1910.08107", "submitter": "Bowen Gang", "authors": "Luella Fu, Bowen Gang, Gareth M. James and Wenguang Sun", "title": "Heterocedasticity-Adjusted Ranking and Thresholding for Large-Scale\n  Multiple Testing", "comments": "55 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardization has been a widely adopted practice in multiple testing, for\nit takes into account the variability in sampling and makes the test statistics\ncomparable across different study units. However, despite conventional wisdom\nto the contrary, we show that there can be a significant loss in information\nfrom basing hypothesis tests on standardized statistics rather than the full\ndata. We develop a new class of heteroscedasticity--adjusted ranking and\nthresholding (HART) rules that aim to improve existing methods by\nsimultaneously exploiting commonalities and adjusting heterogeneities among the\nstudy units. The main idea of HART is to bypass standardization by directly\nincorporating both the summary statistic and its variance into the testing\nprocedure. A key message is that the variance structure of the alternative\ndistribution, which is subsumed under standardized statistics, is highly\ninformative and can be exploited to achieve higher power. The proposed HART\nprocedure is shown to be asymptotically valid and optimal for false discovery\nrate (FDR) control. Our simulation results demonstrate that HART achieves\nsubstantial power gain over existing methods at the same FDR level. We\nillustrate the implementation through a microarray analysis of myeloma.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:37:07 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 01:03:33 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Fu", "Luella", ""], ["Gang", "Bowen", ""], ["James", "Gareth M.", ""], ["Sun", "Wenguang", ""]]}, {"id": "1910.08390", "submitter": "Rodrigo A. Gonz\\'alez", "authors": "Rodrigo A. Gonz\\'alez and Cristian R. Rojas", "title": "Finite sample deviation and variance bounds for first order\n  autoregressive processes", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study finite-sample properties of the least squares\nestimator in first order autoregressive processes. By leveraging a result from\ndecoupling theory, we derive upper bounds on the probability that the estimate\ndeviates by at least a positive $\\varepsilon$ from its true value. Our results\nconsider both stable and unstable processes. Afterwards, we obtain\nproblem-dependent non-asymptotic bounds on the variance of this estimator,\nvalid for sample sizes greater than or equal to seven. Via simulations we\nanalyze the conservatism of our bounds, and show that they reliably capture the\ntrue behavior of the quantities of interest.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:59:38 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 11:12:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Gonz\u00e1lez", "Rodrigo A.", ""], ["Rojas", "Cristian R.", ""]]}, {"id": "1910.08442", "submitter": "Hang Liu", "authors": "Marc Hallin, Davide La Vecchia, Hang Liu", "title": "Center-Outward R-Estimation for Semiparametric VARMA Models", "comments": "55 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of R-estimators for semiparametric VARMA models in\nwhich the innovation density plays the role of the nuisance parameter. Our\nestimators are based on the novel concepts of multivariate center-outward ranks\nand signs. We show that these concepts, combined with Le Cam's asymptotic\ntheory of statistical experiments, yield a class of semiparametric estimation\nprocedures, which are efficient (at a given reference density), root-$n$\nconsistent, and asymptotically normal under a broad class of (possibly non\nelliptical) actual innovation densities. No kernel density estimation is\nrequired to implement our procedures. A Monte Carlo comparative study of our\nR-estimators and other routinely-applied competitors demonstrates the benefits\nof the novel methodology, in large and small sample. Proofs, computational\naspects, and further numerical results are available in the supplementary\nmaterial.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 14:34:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 10:34:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hallin", "Marc", ""], ["La Vecchia", "Davide", ""], ["Liu", "Hang", ""]]}, {"id": "1910.08460", "submitter": "Martin Wahl", "authors": "Martin Wahl", "title": "On the perturbation series for eigenvalues and eigenprojections", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard perturbation result states that perturbed eigenvalues and\neigenprojections admit a perturbation series provided that the operator norm of\nthe perturbation is smaller than a constant times the corresponding eigenvalue\nisolation distance. In this paper, we show that the same holds true under a\nweighted condition, where the perturbation is symmetrically normalized by the\nsquare-root of the reduced resolvent. This weighted condition originates in\nrandom perturbations where it leads to significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:12:32 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Wahl", "Martin", ""]]}, {"id": "1910.08477", "submitter": "Cl\\'ement Berenfeld", "authors": "Cl\\'ement Berenfeld, Marc Hoffmann", "title": "Density estimation on an unknown submanifold", "comments": "36 pages, 21 figures. v2 : important structural modifications and\n  several minor corrections have been done, following comments from anonymous\n  peer reviewers. We also added a new result (Thm 2.6) that underlines the\n  necessity of the reach constraint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate density estimation from a $n$-sample in the Euclidean space\n$\\mathbb R^D$, when the data is supported by an unknown submanifold $M$ of\npossibly unknown dimension $d < D$ under a reach condition. We study\nnonparametric kernel methods for pointwise loss, with data-driven bandwidths\nthat incorporate some learning of the geometry via a local dimension estimator.\nWhen $f$ has H\\\"older smoothness $\\beta$ and $M$ has regularity $\\alpha$, our\nestimator achieves the rate $n^{-\\alpha \\wedge \\beta/(2\\alpha \\wedge \\beta+d)}$\nand does not depend on the ambient dimension $D$ and is asymptotically minimax\nfor $\\alpha \\geq \\beta$. Following Lepski's principle, a bandwidth selection\nrule is shown to achieve smoothness adaptation. We also investigate the case\n$\\alpha \\leq \\beta$: by estimating in some sense the underlying geometry of\n$M$, we establish in dimension $d=1$ that the minimax rate is\n$n^{-\\beta/(2\\beta+1)}$ proving in particular that it does not depend on the\nregularity of $M$. Finally, a numerical implementation is conducted on some\ncase studies in order to confirm the practical feasibility of our estimators.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:42:46 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:01:13 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Berenfeld", "Cl\u00e9ment", ""], ["Hoffmann", "Marc", ""]]}, {"id": "1910.08491", "submitter": "Amaury Durand", "authors": "Amaury Durand and Fran\\c{c}ois Roueff", "title": "Spectral analysis of weakly stationary processes valued in a separable\n  Hilbert space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review and clarify the construction of a spectral theory\nfor weakly-stationary processes valued in a separable Hilbert space. We\nintroduce the basic fundamental concepts and results of functional analysis and\noperator theory needed to follow the way paved by Payen in [52], Mandrekar and\nSalehi in [45] and Kakihara in [33]. They lead us to view the spectral\nrepresentation of a weakly stationary Hilbert valued time series as a gramian\nisometry between its time domain and its spectral domain. Time invariant linear\nfilters with Hilbert-valued inputs and outputs are then defined through their\noperator transfer functions in the spectral domain. General results on the\ncomposition and inversion of such filters follow naturally. Spectral\nrepresentations have enjoyed a renewed interest in the context of functional\ntime series. The gramian isometry between the time and spectral domains\nconstitutes an interesting and enlightening complement to recent approaches\nsuch as the one proposed in [50]. We also provide an overview of recent\nstatistical results for the spectral analysis of functional time-series.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 16:23:19 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 12:12:19 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 11:00:43 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Durand", "Amaury", ""], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "1910.08520", "submitter": "Anil Aswani", "authors": "Anil Aswani and Matt Olfat", "title": "Optimization Hierarchy for Fair Statistical Decision Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven decision-making has drawn scrutiny from policy makers due to\nfears of potential discrimination, and a growing literature has begun to\ndevelop fair statistical techniques. However, these techniques are often\nspecialized to one model context and based on ad-hoc arguments, which makes it\ndifficult to perform theoretical analysis. This paper develops an optimization\nhierarchy, which is a sequence of optimization problems with an increasing\nnumber of constraints, for fair statistical decision problems. Because our\nhierarchy is based on the framework of statistical decision problems, this\nmeans it provides a systematic approach for developing and studying fair\nversions of hypothesis testing, decision-making, estimation, regression, and\nclassification. We use the insight that qualitative definitions of fairness are\nequivalent to statistical independence between the output of a statistical\ntechnique and a random variable that measures attributes for which fairness is\ndesired. We use this insight to construct an optimization hierarchy that lends\nitself to numerical computation, and we use tools from variational analysis and\nrandom set theory to prove that higher levels of this hierarchy lead to\nconsistency in the sense that it asymptotically imposes this independence as a\nconstraint in corresponding statistical decision problems. We demonstrate\nnumerical effectiveness of our hierarchy using several data sets, and we use\nour hierarchy to fairly perform automated dosing of morphine.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:31:01 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 16:25:22 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 18:49:43 GMT"}, {"version": "v4", "created": "Tue, 29 Dec 2020 20:53:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Aswani", "Anil", ""], ["Olfat", "Matt", ""]]}, {"id": "1910.08690", "submitter": "Guy Martial Nkiet", "authors": "Ulrich Djemby Bivigou, Guy Martial Nkiet", "title": "Robustifying multiple-set linear canonical analysis with S-estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a robust version of multiple-set linear canonical analysis\nobtained by using a S-estimator of covariance operator. The related influence\nfunctions are derived. Asymptotic properties of this robust method are obtained\nand a robust test for mutual non-correlation is introduced.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:18:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bivigou", "Ulrich Djemby", ""], ["Nkiet", "Guy Martial", ""]]}, {"id": "1910.08771", "submitter": "Yuan Tian", "authors": "Yuan Tian", "title": "Convex Reconstruction of Structured Matrix Signals from Linear\n  Measurements (I): Theoretical Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reconstructing n-by-n structured matrix signal\nX via convex programming, where each column xj is a vector of s-sparsity and\nall columns have the same l1-norm. The regularizer in use is matrix norm\n|||X|||1=maxj|xj|1.The contribution in this paper has two parts. The first part\nis about conditions for stability and robustness in signal reconstruction via\nsolving the convex programming from noise-free or noisy measurements.We\nestablish uniform sufficient conditions which are very close to necessary\nconditions and non-uniform conditions are also discussed. Similar as the\ntraditional compressive sensing theory for reconstructing vector signals, a\nrelated RIP condition is established. In addition, stronger conditions are\ninvestigated to guarantee the reconstructed signal's support stability, sign\nstability and approximation-error robustness. The second part is to establish\nupper and lower bounds on number of measurements for robust reconstruction in\nnoise. We take the convex geometric approach in random measurement setting and\none of the critical ingredients in this approach is to estimate the related\nwidths bounds in case of Gaussian and non-Gaussian distributions. These bounds\nare explicitly controlled by signal's structural parameters r and s which\ndetermine matrix signal's column-wise sparsity and l1-column-flatness\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 13:44:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 10:11:09 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 14:01:36 GMT"}, {"version": "v4", "created": "Sun, 1 Dec 2019 11:01:10 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Tian", "Yuan", ""]]}, {"id": "1910.08904", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "$hv$-Block Cross Validation is not a BIBD: a Note on the Paper by Jeff\n  Racine (2000)", "comments": "Technique report. 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note corrects a mistake in the paper \"consistent cross-validatory\nmodel-selection for dependent data: $hv$-block cross-validation\" by Racine\n(2000). In his paper, he implied that the therein proposed $hv$-block\ncross-validation is consistent in the sense of Shao (1993). To get this\nintuition, he relied on the speculation that $hv$-block is a balanced\nincomplete block design (BIBD). This note demonstrates that this is not the\ncase, and thus the theoretical consistency of $hv$-block remains an open\nquestion. In addition, I also provide a Python program counting the number of\noccurrences of each sample and each pair of samples.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:27:10 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1910.08997", "submitter": "Trambak Banerjee", "authors": "Trambak Banerjee, Qiang Liu, Gourab Mukherjee and Wenguang Sun", "title": "A General Framework for Empirical Bayes Estimation in Discrete Linear\n  Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a Nonparametric Empirical Bayes (NEB) framework for compound\nestimation in the discrete linear exponential family, which includes a wide\nclass of discrete distributions frequently arising from modern big data\napplications. We propose to directly estimate the Bayes shrinkage factor in the\ngeneralized Robbins' formula via solving a scalable convex program, which is\ncarefully developed based on a RKHS representation of the Stein's discrepancy\nmeasure. The new NEB estimation framework is flexible for incorporating various\nstructural constraints into the data driven rule, and provides a unified\napproach to compound estimation with both regular and scaled squared error\nlosses. We develop theory to show that the class of NEB estimators enjoys\nstrong asymptotic properties. Comprehensive simulation studies as well as\nanalyses of real data examples are carried out to demonstrate the superiority\nof the NEB estimator over competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 14:47:48 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Banerjee", "Trambak", ""], ["Liu", "Qiang", ""], ["Mukherjee", "Gourab", ""], ["Sun", "Wenguang", ""]]}, {"id": "1910.09014", "submitter": "Daniel Irving Bernstein", "authors": "Daniel Irving Bernstein, Basil Saeed, Chandler Squires, Caroline Uhler", "title": "Ordering-Based Causal Structure Learning in the Presence of Latent\n  Variables", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a causal graph in the presence of latent\nconfounders given i.i.d.~samples from the model. While current algorithms for\ncausal structure discovery in the presence of latent confounders are\nconstraint-based, we here propose a score-based approach. We prove that under\nassumptions weaker than faithfulness, any sparsest independence map (IMAP) of\nthe distribution belongs to the Markov equivalence class of the true model.\nThis motivates the \\emph{Sparsest Poset} formulation - that posets can be\nmapped to minimal IMAPs of the true model such that the sparsest of these IMAPs\nis Markov equivalent to the true model. Motivated by this result, we propose a\ngreedy algorithm over the space of posets for causal structure discovery in the\npresence of latent confounders and compare its performance to the current\nstate-of-the-art algorithms FCI and FCI+ on synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:36:06 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 21:04:46 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Bernstein", "Daniel Irving", ""], ["Saeed", "Basil", ""], ["Squires", "Chandler", ""], ["Uhler", "Caroline", ""]]}, {"id": "1910.09083", "submitter": "Minghe Zhang", "authors": "Minghe Zhang, Liyan Xie, Yao Xie", "title": "Online Community Detection by Spectral CUSUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online community change detection algorithm called spectral\nCUSUM to detect the emergence of a community using a subspace projection\nprocedure based on a Gaussian model setting. Theoretical analysis is provided\nto characterize the average run length (ARL) and expected detection delay\n(EDD), as well as the asymptotic optimality. Simulation and real data examples\ndemonstrate the good performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 23:47:33 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:48:12 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhang", "Minghe", ""], ["Xie", "Liyan", ""], ["Xie", "Yao", ""]]}, {"id": "1910.09151", "submitter": "Georgios Rovatsos", "authors": "Georgios Rovatsos, Venugopal V. Veeravalli, Don Towsley, Ananthram\n  Swami", "title": "Quickest Detection of Growing Dynamic Anomalies in Networks", "comments": "2 figures, conference style+appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickest growing dynamic anomaly detection in sensor networks\nis studied. Initially, the observations at the sensors, which are sampled\nsequentially by the decision maker, are generated according to a pre-change\ndistribution. At some unknown but deterministic time instant, a dynamic anomaly\nemerges in the network, affecting a different set of sensors as time\nprogresses. The observations of the affected sensors are generated from a\npost-change distribution. It is assumed that the number of affected sensors\nincreases with time, and that only the initial and the final size of the\nanomaly are known by the decision maker. The goal is to detect the emergence of\nthe anomaly as quickly as possible while guaranteeing a sufficiently low\nfrequency of false alarm events. This detection problem is posed as a\nstochastic optimization problem by using a delay metric that is based on the\nworst possible path of the anomaly. A detection rule is proposed that is\nasymptotically optimal as the mean time to false alarm goes to infinity.\nFinally, numerical results are provided to validate our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 05:05:23 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 22:37:44 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Rovatsos", "Georgios", ""], ["Veeravalli", "Venugopal V.", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""]]}, {"id": "1910.09192", "submitter": "Partha Hazarika", "authors": "Sricharan Shah, Subrata Chakraborty, Partha Jyoti Hazarika and M.\n  Masoom Ali", "title": "The Generalized-Alpha-Beta-Skew-Normal Distribution: Properties and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we have introduced a generalized version of alpha beta skew\nnormal distribution in the same line of Sharafi et al. (2017) and investigated\nsome of its basic properties. The extensions of the proposed distribution have\nalso been studied. The appropriateness of the proposed distribution has been\ntested by comparing the values of Akaike Information Criterion (AIC) and\nBayesian Information Criterion (BIC) with the values of some other known\nrelated distributions for better model selection. Likelihood ratio test has\nbeen used for discriminating between nested models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:01:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Shah", "Sricharan", ""], ["Chakraborty", "Subrata", ""], ["Hazarika", "Partha Jyoti", ""], ["Ali", "M. Masoom", ""]]}, {"id": "1910.09211", "submitter": "Tchilabalo Abozou Kpanzou", "authors": "Gane Samb Lo, Tchilabalo Abozou Kpanzou and Cheikh Mohamed Haidara", "title": "Statistical tests for the Pseudo-Lindley distribution and applications", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-Lindley distribution was introduced as a useful generalization of\nthe Lindley distribution in Zeghdoudi and Nedjar (2016) who showed interesting\nproperties of their new laws and efficiencies in modeling data in Reliability\nand Survival Analysis. In this paper we study the estimators of the pair of\nparameters and determine their asymptotic law from which a chi-square law is\nderived. From both asymptotic laws, statistical tests are built. Simulation\nstudies on the tests conclude to their efficiency for data sizes generally used\nin Reliability. R codes related to statistical analysis on that law are given\nin an appropriate archive repository code paper in Arxiv.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:57:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Lo", "Gane Samb", ""], ["Kpanzou", "Tchilabalo Abozou", ""], ["Haidara", "Cheikh Mohamed", ""]]}, {"id": "1910.09219", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn", "title": "Marginally Interpretable Linear Transformation Models for Clustered\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustered observations are ubiquitous in controlled and observational studies\nand arise naturally in multicenter trials or longitudinal surveys. I present\ntwo novel models for the analysis of clustered observations where the marginal\ndistributions are described by a linear transformation model and the\ncorrelations by a joint multivariate normal distribution. Both models provide\nanalytic formulae for the marginal distributions, one of which features\ndirectly interpretable parameters. Owing to the richness of transformation\nmodels, the techniques are applicable to any type of response variable,\nincluding bounded, skewed, binary, ordinal, or survival responses. I present\nre-analyses of five applications from different domains, including models for\nnon-normal and discrete responses, and explain how specific models for the\nestimation of marginal distributions can be defined within this novel modelling\nframework and how the results can be interpreted in a marginal way.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:08:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hothorn", "Torsten", ""]]}, {"id": "1910.09227", "submitter": "Alisa Kirichenko", "authors": "Rianne de Heide and Alisa Kirichenko and Nishant Mehta and Peter\n  Gr\\\"unwald", "title": "Safe-Bayesian Generalized Linear Regression", "comments": "Final version. Accepted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalized Bayesian inference under misspecification, i.e. when the\nmodel is 'wrong but useful'. Generalized Bayes equips the likelihood with a\nlearning rate $\\eta$. We show that for generalized linear models (GLMs),\n$\\eta$-generalized Bayes concentrates around the best approximation of the\ntruth within the model for specific $\\eta \\neq 1$, even under severely\nmisspecified noise, as long as the tails of the true distribution are\nexponential. We derive MCMC samplers for generalized Bayesian lasso and\nlogistic regression and give examples of both simulated and real-world data in\nwhich generalized Bayes substantially outperforms standard Bayes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:32:26 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 09:20:14 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 21:52:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["de Heide", "Rianne", ""], ["Kirichenko", "Alisa", ""], ["Mehta", "Nishant", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "1910.09319", "submitter": "Jikai Hou", "authors": "Jikai Hou", "title": "Empirical Process of Multivariate Gaussian under General Dependence", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores certain kinds of empirical process with respect to the\ncomponents of multivariate Gaussian. We put forward some finite sample bounds\nwhich hold for multivariate Gaussian under general dependence. We give\nnecessary and sufficient condition for the convergence in probability of the\nrandom variable sequence\n$\\displaystyle\\left\\{\\sup_t\\vert\\widehat{F}_n(t)-\\mathbf{E}\\widehat{F}_n(t)\\vert\\right\\}_{n\\in\n\\mathbb{N}}$, where $\\widehat{F}_n(t)$ is the empirical distribution. Also, we\nfind a similar sufficient condition for almost surely convergence.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:51:28 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:17:29 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 12:09:30 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 11:08:00 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Hou", "Jikai", ""]]}, {"id": "1910.09391", "submitter": "Davy Paindaveine", "authors": "Christine Cutting, Davy Paindaveine, and Thomas Verdebout", "title": "On the power of axial tests of uniformity on spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing uniformity on the $p$-dimensional unit sphere is arguably the most\nfundamental problem in directional statistics. In this paper, we consider this\nproblem in the framework of axial data, that is, under the assumption that the\n$n$ observations at hand are randomly drawn from a distribution that charges\nantipodal regions equally. More precisely, we focus on axial, rotationally\nsymmetric, alternatives and first address the problem under which the direction\n$\\theta$ of the corresponding symmetry axis is specified. In this setup, we\nobtain Le Cam optimal tests of uniformity, that are based on the sample\ncovariance matrix (unlike their non-axial analogs, that are based on the sample\naverage). For the more important unspecified-$\\theta$ problem, some classical\ntests are available in the literature, but virtually nothing is known on their\nnon-null behavior. We therefore study the non-null behavior of the celebrated\nBingham test and of other tests that exploit the single-spiked nature of the\nconsidered alternatives. We perform Monte Carlo exercises to investigate the\nfinite-sample behavior of our tests and to show their agreement with our\nasymptotic results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:08:46 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cutting", "Christine", ""], ["Paindaveine", "Davy", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1910.09485", "submitter": "Jure Vogrinc", "authors": "Jure Vogrinc and Wilfrid Stephen Kendall", "title": "Counterexamples for optimal scaling of Metropolis-Hastings chains with\n  rough target densities", "comments": "44 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sufficiently smooth targets of product form it is known that the variance\nof a single coordinate of the proposal in RWM (Random walk Metropolis) and MALA\n(Metropolis adjusted Langevin algorithm) should optimally scale as $n^{-1}$ and\nas $n^{-\\frac{1}{3}}$ with dimension $n$, and that the acceptance rates should\nbe tuned to $0.234$ and $0.574$. We establish counterexamples to demonstrate\nthat smoothness assumptions of the order of $\\mathcal{C}^1(\\mathbb{R})$ for RWM\nand $\\mathcal{C}^3(\\mathbb{R})$ for MALA are indeed required if these scaling\nrates are to hold. The counterexamples identify classes of marginal targets for\nwhich these guidelines are violated, obtained by perturbing a standard Normal\ndensity (at the level of the potential for RWM and the second derivative of the\npotential for MALA) using roughness generated by a path of fractional Brownian\nmotion with Hurst exponent $H$. For such targets there is strong evidence that\nRWM and MALA proposal variances should optimally be scaled as\n$n^{-\\frac{1}{H}}$ and as $n^{-\\frac{1}{2+H}}$ and will then obey anomalous\nacceptance rate guidelines. Useful heuristics resulting from this theory are\ndiscussed. The paper develops a framework capable of tackling optimal scaling\nresults for quite general Metropolis-Hastings algorithms (possibly depending on\na random environment).\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:22:33 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:45:34 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Vogrinc", "Jure", ""], ["Kendall", "Wilfrid Stephen", ""]]}, {"id": "1910.09493", "submitter": "Bin Luo", "authors": "Bin Luo and Xiaoli Gao", "title": "High-dimensional robust approximated M-estimators for mean regression\n  with asymmetric data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetry along with heteroscedasticity or contamination often occurs with\nthe growth of data dimensionality. In ultra-high dimensional data analysis,\nsuch irregular settings are usually overlooked for both theoretical and\ncomputational convenience. In this paper, we establish a framework for\nestimation in high-dimensional regression models using Penalized Robust\nApproximated quadratic M-estimators (PRAM). This framework allows general\nsettings such as random errors lack of symmetry and homogeneity, or the\ncovariates are not sub-Gaussian. To reduce the possible bias caused by the\ndata's irregularity in mean regression, PRAM adopts a loss function with a\nflexible robustness parameter growing with the sample size. Theoretically, we\nfirst show that, in the ultra-high dimension setting, PRAM estimators have\nlocal estimation consistency at the minimax rate enjoyed by the LS-Lasso. Then\nwe show that PRAM with an appropriate non-convex penalty in fact agrees with\nthe local oracle solution, and thus obtain its oracle property.\nComputationally, we demonstrate the performances of six PRAM estimators using\nthree types of loss functions for approximation (Huber, Tukey's biweight and\nCauchy loss) combined with two types of penalty functions (Lasso and MCP). Our\nsimulation studies and real data analysis demonstrate satisfactory finite\nsample performances of the PRAM estimator under general irregular settings.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:33:46 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Luo", "Bin", ""], ["Gao", "Xiaoli", ""]]}, {"id": "1910.09499", "submitter": "Miaoyan Wang", "authors": "Zhuoyan Xu, Jiaxin Hu, and Miaoyan Wang", "title": "Generalized tensor regression with covariates on multiple modes", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of tensor-response regression given covariates on\nmultiple modes. Such data problems arise frequently in applications such as\nneuroimaging, network analysis, and spatial-temporal modeling. We propose a new\nfamily of tensor response regression models that incorporate covariates, and\nestablish the theoretical accuracy guarantees. Unlike earlier methods, our\nestimation allows high-dimensionality in both the tensor response and the\ncovariate matrices on multiple modes. An efficient alternating updating\nalgorithm is further developed. Our proposal handles a broad range of data\ntypes, including continuous, count, and binary observations. Through simulation\nand applications to two real datasets, we demonstrate the outperformance of our\napproach over the state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:43:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Zhuoyan", ""], ["Hu", "Jiaxin", ""], ["Wang", "Miaoyan", ""]]}, {"id": "1910.09502", "submitter": "Florian Gunsilius", "authors": "Florian Gunsilius", "title": "A path-sampling method to partially identify causal effects in\n  instrumental variable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial identification approaches are a flexible and robust alternative to\nstandard point-identification approaches in general instrumental variable\nmodels. However, this flexibility comes at the cost of a ``curse of\ncardinality'': the number of restrictions on the identified set grows\nexponentially with the number of points in the support of the endogenous\ntreatment. This article proposes a novel path-sampling approach to this\nchallenge. It is designed for partially identifying causal effects of interest\nin the most complex models with continuous endogenous treatments. A stochastic\nprocess representation allows to seamlessly incorporate assumptions on\nindividual behavior into the model. Some potential applications include\ndose-response estimation in randomized trials with imperfect compliance, the\nevaluation of social programs, welfare estimation in demand models, and\ncontinuous choice models. As a demonstration, the method provides informative\nnonparametric bounds on household expenditures under the assumption that\nexpenditure is continuous. The mathematical contribution is an approach to\napproximately solving infinite dimensional linear programs on path spaces via\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:44:15 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 16:00:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Gunsilius", "Florian", ""]]}, {"id": "1910.09648", "submitter": "Max Little", "authors": "Max A. Little, Reham Badawy", "title": "Causal bootstrapping", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To draw scientifically meaningful conclusions and build reliable models of\nquantitative phenomena, cause and effect must be taken into consideration\n(either implicitly or explicitly). This is particularly challenging when the\nmeasurements are not from controlled experimental (interventional) settings,\nsince cause and effect can be obscured by spurious, indirect influences. Modern\npredictive techniques from machine learning are capable of capturing\nhigh-dimensional, nonlinear relationships between variables while relying on\nfew parametric or probabilistic model assumptions. However, since these\ntechniques are associational, applied to observational data they are prone to\npicking up spurious influences from non-experimental (observational) data,\nmaking their predictions unreliable. Techniques from causal inference, such as\nprobabilistic causal diagrams and do-calculus, provide powerful (nonparametric)\ntools for drawing causal inferences from such observational data. However,\nthese techniques are often incompatible with modern, nonparametric machine\nlearning algorithms since they typically require explicit probabilistic models.\nHere, we develop causal bootstrapping for augmenting classical nonparametric\nbootstrap resampling with information on the causal relationship between\nvariables. This makes it possible to resample observational data such that, if\nit is possible to identify an interventional relationship from that data, new\ndata representing that relationship can be simulated from the original\nobservational data. In this way, we can use modern machine learning algorithms\nunaltered to make statistically powerful, yet causally-robust, predictions. We\ndevelop several causal bootstrapping algorithms for drawing interventional\ninferences from observational data, for classification and regression problems,\nand demonstrate, using synthetic and real-world examples, the value of this\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:52:56 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 20:51:57 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 00:23:44 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Little", "Max A.", ""], ["Badawy", "Reham", ""]]}, {"id": "1910.09662", "submitter": "Qiyang Han", "authors": "Qiyang Han, Kengo Kato", "title": "Berry-Esseen bounds for Chernoff-type non-standard asymptotics in\n  isotonic regression", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Chernoff-type distribution is a nonnormal distribution defined by the slope\nat zero of the greatest convex minorant of a two-sided Brownian motion with a\npolynomial drift. While a Chernoff-type distribution is known to appear as the\ndistributional limit in many non-regular statistical estimation problems, the\naccuracy of Chernoff-type approximations has remained largely unknown. In the\npresent paper, we tackle this problem and derive Berry-Esseen bounds for\nChernoff-type limit distributions in the canonical non-regular statistical\nestimation problem of isotonic (or monotone) regression. The derived\nBerry-Esseen bounds match those of the oracle local average estimator with\noptimal bandwidth in each scenario of possibly different Chernoff-type\nasymptotics, up to multiplicative logarithmic factors. Our method of proof\ndiffers from standard techniques on Berry-Esseen bounds, and relies on new\nlocalization techniques in isotonic regression and an anti-concentration\ninequality for the supremum of a Brownian motion with a Lipschitz drift.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 21:17:44 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 12:24:02 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Han", "Qiyang", ""], ["Kato", "Kengo", ""]]}, {"id": "1910.09695", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Christeen Wijethunga", "title": "Confidence intervals centred on bootstrap smoothed estimators: an\n  impossibility result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Kabaila and Wijethunga assessed the performance of a confidence\ninterval centred on a bootstrap smoothed estimator, with width proportional to\nan estimator of Efron's delta method approximation to the standard deviation of\nthis estimator. They used a testbed situation consisting of two nested linear\nregression models, with error variance assumed known, and model selection using\na preliminary hypothesis test. This assessment was in terms of coverage and\nscaled expected length, where the scaling is with respect to the expected\nlength of the usual confidence interval with the same minimum coverage\nprobability. They found that this confidence interval has scaled expected\nlength that (a) has a maximum value that may be much greater than 1 and (b) is\ngreater than a number slightly less than 1 when the simpler model is correct.\nWe therefore ask the following question. For a confidence interval, centred on\nthe bootstrap smoothed estimator, does there exist a formula for its data-based\nwidth such that, in this testbed situation, it has the desired minimum coverage\nand scaled expected length that (a) has a maximum value that is not too much\nlarger than 1 and (b) is substantially less than 1 when the simpler model is\ncorrect? Using a recent decision-theoretic performance bound due to Kabaila and\nKong, it is shown that the answer to this question is `no' for a wide range of\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:43:08 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kabaila", "Paul", ""], ["Wijethunga", "Christeen", ""]]}, {"id": "1910.10225", "submitter": "Pulong Ma", "authors": "Pulong Ma", "title": "Objective Bayesian Analysis of a Cokriging Model for Hierarchical\n  Multifidelity Codes", "comments": "26 pages, published", "journal-ref": null, "doi": "10.1137/19M1289893", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive cokriging models have been widely used to emulate multiple\ncomputer models with different levels of fidelity. The dependence structures\nare modeled via Gaussian processes at each level of fidelity, where covariance\nstructures are often parameterized up to a few parameters. The predictive\ndistributions typically require intensive Monte Carlo approximations in\nprevious works. This article derives new closed-form formulas to compute the\nmeans and variances of predictive distributions in autoregressive cokriging\nmodels that only depend on correlation parameters. For parameter estimation, we\nconsider objective Bayesian analysis of such autoregressive cokriging models.\nWe show that common choices of prior distributions, such as the constant prior\nand inverse correlation prior, typically lead to improper posteriors. We also\ndevelop several objective priors such as the independent reference prior and\nthe independent Jeffreys prior that are shown to yield proper posterior\ndistributions. This development is illustrated with a borehole function in an\neight-dimensional input space and applied to an engineering application in a\nsix-dimensional input space.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 20:50:31 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 16:21:11 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 18:22:19 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 17:56:42 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ma", "Pulong", ""]]}, {"id": "1910.10382", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu", "title": "How well can we learn large factor models without assuming strong\n  factors?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of learning models with a latent\nfactor structure. The focus is to find what is possible and what is impossible\nif the usual strong factor condition is not imposed. We study the minimax rate\nand adaptivity issues in two problems: pure factor models and panel regression\nwith interactive fixed effects. For pure factor models, if the number of\nfactors is known, we develop adaptive estimation and inference procedures that\nattain the minimax rate. However, when the number of factors is not specified a\npriori, we show that there is a tradeoff between validity and efficiency: any\nconfidence interval that has uniform validity for arbitrary factor strength has\nto be conservative; in particular its width is bounded away from zero even when\nthe factors are strong. Conversely, any data-driven confidence interval that\ndoes not require as an input the exact number of factors (including weak ones)\nand has shrinking width under strong factors does not have uniform coverage and\nthe worst-case coverage probability is at most 1/2. For panel regressions with\ninteractive fixed effects, the tradeoff is much better. We find that the\nminimax rate for learning the regression coefficient does not depend on the\nfactor strength and propose a simple estimator that achieves this rate.\nHowever, when weak factors are allowed, uncertainty in the number of factors\ncan cause a great loss of efficiency although the rate is not affected. In most\ncases, we find that the strong factor condition (and/or exact knowledge of\nnumber of factors) improves efficiency, but this condition needs to be imposed\nby faith and cannot be verified in data for inference purposes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 06:41:53 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 07:49:54 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 13:21:34 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Zhu", "Yinchu", ""]]}, {"id": "1910.10426", "submitter": "Linas Petkevicius", "authors": "Vilijandas Bagdonavicius, Linas Petkevicius", "title": "Multiple outlier detection tests for parametric models", "comments": null, "journal-ref": "Mathematics 8 (2020) 2156", "doi": "10.3390/math8122156", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple multiple outlier identification method for parametric\nlocation-scale and shape-scale models when the number of possible outliers is\nnot specified. The method is based on a result giving asymptotic properties of\nextreme z-scores. Robust estimators of model parameters are used defining\nz-scores. An extensive simulation study was done for comparing of the proposed\nmethod with existing methods. For the normal family, the method is compared\nwith the well known Davies-Gather, Rosner's, Hawking's and Bolshev's multiple\noutlier identification methods. The choice of an upper limit for the number of\npossible outliers in case of Rosner's test application is discussed. For other\nfamilies, the proposed method is compared with a method generalizing\nGather-Davies method. In most situations, the new method has the highest\noutlier identification power in terms of masking and swamping values. We also\ncreated R package outliersTests for proposed test.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:23:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bagdonavicius", "Vilijandas", ""], ["Petkevicius", "Linas", ""]]}, {"id": "1910.10513", "submitter": "Puning Zhao", "authors": "Puning Zhao, Lifeng Lai", "title": "Minimax Rate Optimal Adaptive Nearest Neighbor Classification and\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k Nearest Neighbor (kNN) method is a simple and popular statistical method\nfor classification and regression. For both classification and regression\nproblems, existing works have shown that, if the distribution of the feature\nvector has bounded support and the probability density function is bounded away\nfrom zero in its support, the convergence rate of the standard kNN method, in\nwhich k is the same for all test samples, is minimax optimal. On the contrary,\nif the distribution has unbounded support, we show that there is a gap between\nthe convergence rate achieved by the standard kNN method and the minimax bound.\nTo close this gap, we propose an adaptive kNN method, in which different k is\nselected for different samples. Our selection rule does not require precise\nknowledge of the underlying distribution of features. The new proposed method\nsignificantly outperforms the standard one. We characterize the convergence\nrate of the proposed adaptive method, and show that it matches the minimax\nlower bound.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:56:53 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Zhao", "Puning", ""], ["Lai", "Lifeng", ""]]}, {"id": "1910.10562", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Arun K. Kuchibhotla, Aaditya K. Ramdas", "title": "Nested conformal prediction and quantile out-of-bag ensemble methods", "comments": "38 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a popular tool for providing valid prediction sets\nfor classification and regression problems, without relying on any\ndistributional assumptions on the data. While the traditional description of\nconformal prediction starts with a nonconformity score, we provide an alternate\n(but equivalent) view that starts with a sequence of nested sets and calibrates\nthem to find a valid prediction set. The nested framework subsumes all\nnonconformity scores, including recent proposals based on quantile regression\nand density estimation. While these ideas were originally derived based on\nsample splitting, our framework seamlessly extends them to other aggregation\nschemes like cross-conformal, jackknife+ and out-of-bag methods. We use the\nframework to derive a new algorithm (QOOB, pronounced cube) that combines four\nideas: quantile regression, cross-conformalization, ensemble methods and\nout-of-bag predictions. We develop a computationally efficient implementation\nof cross-conformal, that is also used by QOOB. In a detailed numerical\ninvestigation, QOOB performs either the best or close to the best on all\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:44:38 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:26:16 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 03:29:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Kuchibhotla", "Arun K.", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "1910.10589", "submitter": "Guilherme Pumi", "authors": "Taiane Schaedler Prass and Guilherme Pumi", "title": "On the behavior of the DFA and DCCA in trend-stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop the asymptotic theory of the Detrended Fluctuation\nAnalysis (DFA) and Detrended Cross-Correlation Analysis (DCCA) for\ntrend-stationary stochastic processes without any assumption on the specific\nform of the underlying distribution. All results are presented and derived\nunder the general framework of potentially overlapping boxes for the polynomial\nfit. We prove the stationarity of the DFA and DCCA, viewed as stochastic\nprocesses, obtain closed forms for moments up to second order, including the\ncovariance structure for DFA and DCCA and a miscellany of law of large number\nrelated results. Our results generalize and improve several results presented\nin the literature. To verify the behavior of our theoretical results in small\nsamples, we present a Monte Carlo simulation study and an empirical application\nto econometric time series.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:43:03 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:53:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Prass", "Taiane Schaedler", ""], ["Pumi", "Guilherme", ""]]}, {"id": "1910.10669", "submitter": "Ruiyi Yang", "authors": "John Harlim, Daniel Sanz-Alonso, Ruiyi Yang", "title": "Kernel Methods for Bayesian Elliptic Inverse Problems on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the formulation and implementation of Bayesian\ninverse problems to learn input parameters of partial differential equations\n(PDEs) defined on manifolds. Specifically, we study the inverse problem of\ndetermining the diffusion coefficient of a second-order elliptic PDE on a\nclosed manifold from noisy measurements of the solution. Inspired by manifold\nlearning techniques, we approximate the elliptic differential operator with a\nkernel-based integral operator that can be discretized via Monte-Carlo without\nreference to the Riemannian metric. The resulting computational method is\nmesh-free and easy to implement, and can be applied without full knowledge of\nthe underlying manifold, provided that a point cloud of manifold samples is\navailable. We adopt a Bayesian perspective to the inverse problem, and\nestablish an upper-bound on the total variation distance between the true\nposterior and an approximate posterior defined with the kernel forward map.\nSupporting numerical results show the effectiveness of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:09:22 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Harlim", "John", ""], ["Sanz-Alonso", "Daniel", ""], ["Yang", "Ruiyi", ""]]}, {"id": "1910.10692", "submitter": "Yizhe Zhu", "authors": "Kameron Decker Harris, Yizhe Zhu", "title": "Deterministic tensor completion with hypergraph expanders", "comments": "35 pages, 4 figures. To appear in SIAM Journal on Mathematics of Data\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel analysis of low-rank tensor completion based on hypergraph\nexpanders. As a proxy for rank, we minimize the max-quasinorm of the tensor,\nwhich generalizes the max-norm for matrices. Our analysis is deterministic and\nshows that the number of samples required to approximately recover an order-$t$\ntensor with at most $n$ entries per dimension is linear in $n$, under the\nassumption that the rank and order of the tensor are $O(1)$. As steps in our\nproof, we find a new expander mixing lemma for a $t$-partite, $t$-uniform\nregular hypergraph model, and prove several new properties about tensor\nmax-quasinorm. To the best of our knowledge, this is the first deterministic\nanalysis of tensor completion. We develop a practical algorithm that solves a\nrelaxed version of the max-quasinorm minimization problem, and we demonstrate\nits efficacy with numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:51:13 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:28:33 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:48:23 GMT"}, {"version": "v4", "created": "Thu, 29 Jul 2021 16:50:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Harris", "Kameron Decker", ""], ["Zhu", "Yizhe", ""]]}, {"id": "1910.10868", "submitter": "Yueqiao Faith Zhang", "authors": "Yueqiao Faith Zhang, Xiongzhi Chen", "title": "An FDR upper bound for an adaptive one-way GBH procedure under\n  exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been some numerical evidence on the conservativeness of an adaptive\none-way GBH procedure for multiple testing the means of equally correlated\nnormal random variables. However, a theoretical investigation into this seems\nto be lacking. We provide an analytic, non-asymptotic FDR upper bound for such\na procedure under the aforementioned multiple testing scenario. The bound is\nnot tight but reasonably quantifies how bad the FDR of the procedure can be. As\nby-products, we extend two relevant existing results to the setting of p-values\nthat are not necessarily super-uniform.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 01:16:36 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Zhang", "Yueqiao Faith", ""], ["Chen", "Xiongzhi", ""]]}, {"id": "1910.10890", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, Ilias Zadik", "title": "Inference in High-Dimensional Linear Regression via Lattice Basis\n  Reduction and Integer Relation Detection", "comments": "56 pages. Parts of the material of this manuscript were presented at\n  NeurIPS 2018, and ISIT 2019. This submission subsumes the content of\n  arXiv:1803.06716", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the high-dimensional linear regression problem, where the\nalgorithmic goal is to efficiently infer an unknown feature vector\n$\\beta^*\\in\\mathbb{R}^p$ from its linear measurements, using a small number $n$\nof samples. Unlike most of the literature, we make no sparsity assumption on\n$\\beta^*$, but instead adopt a different regularization: In the noiseless\nsetting, we assume $\\beta^*$ consists of entries, which are either rational\nnumbers with a common denominator $Q\\in\\mathbb{Z}^+$ (referred to as\n$Q$-rationality); or irrational numbers supported on a rationally independent\nset of bounded cardinality, known to learner; collectively called as the\nmixed-support assumption. Using a novel combination of the PSLQ integer\nrelation detection, and LLL lattice basis reduction algorithms, we propose a\npolynomial-time algorithm which provably recovers a $\\beta^*\\in\\mathbb{R}^p$\nenjoying the mixed-support assumption, from its linear measurements\n$Y=X\\beta^*\\in\\mathbb{R}^n$ for a large class of distributions for the random\nentries of $X$, even with one measurement $(n=1)$. In the noisy setting, we\npropose a polynomial-time, lattice-based algorithm, which recovers a\n$\\beta^*\\in\\mathbb{R}^p$ enjoying $Q$-rationality, from its noisy measurements\n$Y=X\\beta^*+W\\in\\mathbb{R}^n$, even with a single sample $(n=1)$. We further\nestablish for large $Q$, and normal noise, this algorithm tolerates\ninformation-theoretically optimal level of noise. We then apply these ideas to\ndevelop a polynomial-time, single-sample algorithm for the phase retrieval\nproblem. Our methods address the single-sample $(n=1)$ regime, where the\nsparsity-based methods such as LASSO and Basis Pursuit are known to fail.\nFurthermore, our results also reveal an algorithmic connection between the\nhigh-dimensional linear regression problem, and the integer relation detection,\nrandomized subset-sum, and shortest vector problems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:41:39 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "1910.10923", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot", "title": "ERM and RERM are optimal estimators for regression problems when\n  malicious outliers corrupt the labels", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Empirical Risk Minimizers (ERM) and Regularized Empirical Risk\nMinimizers (RERM) for regression problems with convex and $L$-Lipschitz loss\nfunctions. We consider a setting where $|\\cO|$ malicious outliers contaminate\nthe labels. In that case, under a local Bernstein condition, we show that the\n$L_2$-error rate is bounded by $ r_N + AL |\\cO|/N$, where $N$ is the total\nnumber of observations, $r_N$ is the $L_2$-error rate in the non-contaminated\nsetting and $A$ is a parameter coming from the local Bernstein condition. When\n$r_N$ is minimax-rate-optimal in a non-contaminated setting, the rate $r_N +\nAL|\\cO|/N$ is also minimax-rate-optimal when $|\\cO|$ outliers contaminate the\nlabel. The main results of the paper can be used for many non-regularized and\nregularized procedures under weak assumptions on the noise. We present results\nfor Huber's M-estimators (without penalization or regularized by the\n$\\ell_1$-norm) and for general regularized learning problems in reproducible\nkernel Hilbert spaces when the noise can be heavy-tailed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 05:48:45 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:46:49 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chinot", "Geoffrey", ""]]}, {"id": "1910.10924", "submitter": "Norbert Henze", "authors": "Norbert Henze, M. Dolores Jim\\'enez--Gamero", "title": "A test for Gaussianity in Hilbert spaces via the empirical\n  characteristic functional", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,X_2, \\ldots$ be independent and identically distributed random\nelements taking values in a separable Hilbert space $\\mathbb{H}$. With\napplications for functional data in mind, $\\mathbb{H}$ may be regarded as a\nspace of square-integrable functions, defined on a compact interval. We propose\nand study a novel test of the hypothesis $H_0$ that $X_1$ has some unspecified\nnon-degenerate Gaussian distribution. The test statistic\n$T_n=T_n(X_1,\\ldots,X_n)$ is based on a measure of deviation between the\nempirical characteristic functional of $X_1,\\ldots,X_n$ and the characteristic\nfunctional of a suitable Gaussian random element of $\\mathbb{H}$. We derive the\nasymptotic distribution of $T_n$ as $n \\to \\infty$ under $H_0$ and provide a\nconsistent bootstrap approximation thereof. Moreover, we obtain an almost sure\nlimit of $T_n$ as well as a normal limit distribution of $T_n$ under\nalternatives to Gaussianity. Simulations show that the new test is competitive\nwith respect to the hitherto few competitors available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 05:52:24 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Henze", "Norbert", ""], ["Jim\u00e9nez--Gamero", "M. Dolores", ""]]}, {"id": "1910.11095", "submitter": "Mohammed Bouchouia", "authors": "Mohammed Bouchouia, Fran\\c{c}ois Portier", "title": "High dimensional regression for regenerative time-series: an application\n  to road traffic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical predictive model in which a high-dimensional time-series\nregenerates at the end of each day is used to model road traffic. Due to the\nregeneration, prediction is based on a daily modeling using a vector\nautoregressive model that combines linearly the past observations of the day.\nDue to the high-dimension, the learning algorithm follows from an\nL1-penalization of the regression coefficients. Excess risk bounds are\nestablished under the high-dimensional framework in which the number of road\nsections goes to infinity with the number of observed days. Considering\nfloating car data observed in an urban area, the approach is compared to\nstate-of-the-art methods including neural networks. In addition of being highly\ncompetitive in terms of prediction, it enables the identification of the most\ndeterminant sections of the road network.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:46:17 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 08:14:59 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 00:13:10 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 09:47:08 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 07:57:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bouchouia", "Mohammed", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1910.11142", "submitter": "Yury Maximov", "authors": "Valerii Likhosherstov, Yury Maximov, Michael Chertkov", "title": "Tractable Minor-free Generalization of Planar Zero-field Ising Models", "comments": "32 pages. arXiv admin note: substantial text overlap with\n  arXiv:1906.06431, arXiv:1812.09587", "journal-ref": null, "doi": "10.1088/1742-5468/abcaf1", "report-no": null, "categories": "cs.DS math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of zero-field Ising models over $N$ binary\nvariables/spins obtained by consecutive \"gluing\" of planar and $O(1)$-sized\ncomponents and subsets of at most three vertices into a tree. The\npolynomial-time algorithm of the dynamic programming type for solving exact\ninference (computing partition function) and exact sampling (generating i.i.d.\nsamples) consists in a sequential application of an efficient (for planar) or\nbrute-force (for $O(1)$-sized) inference and sampling to the components as a\nblack box. To illustrate the utility of the new family of tractable graphical\nmodels, we first build a polynomial algorithm for inference and sampling of\nzero-field Ising models over $K_{3,3}$-minor-free topologies and over\n$K_{5}$-minor-free topologies -- both are extensions of the planar zero-field\nIsing models -- which are neither genus - nor treewidth-bounded. Second, we\ndemonstrate empirically an improvement in the approximation quality of the\nNP-hard problem of inference over the square-grid Ising model in a\nnode-dependent non-zero \"magnetic\" field.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 10:39:48 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Likhosherstov", "Valerii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1910.11223", "submitter": "Christof Sch\\\"otz", "authors": "Christof Sch\\\"otz", "title": "Arbitrary Rates of Convergence for Projected and Extrinsic Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study central limit theorems for the projected sample mean of independent\nand identically distributed observations on subsets $\\mathcal Q \\subset \\mathbb\nR^2$ of the Euclidean plane.\n  It is well-known that two conditions suffice to obtain a parametric rate of\nconvergence for the projected sample mean: $\\mathcal Q$ is a $\\mathcal\nC^2$-manifold, and the expectation of the underlying distribution calculated in\n$\\mathbb R^2$ is bounded away from the medial axis, the set of point that do\nnot have a unique projection to $\\mathcal Q$.\n  We show that breaking one of these conditions can lead to any other rate: For\na virtually arbitrary prescribed rate, we construct $\\mathcal Q$ such that all\ndistributions with expectation at a preassigned point attain this rate.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:27:18 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 10:26:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sch\u00f6tz", "Christof", ""]]}, {"id": "1910.11248", "submitter": "Jiaxi Zhao", "authors": "Wuchen Li, Jiaxi Zhao", "title": "Wasserstein information matrix", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information matrices for statistical models by the $L^2$-Wasserstein\nmetric. We call them Wasserstein information matrices (WIMs), which are analogs\nof classical Fisher information matrices. We introduce Wasserstein score\nfunctions and study covariance operators in statistical models. Using them, we\nestablish Wasserstein-Cramer-Rao bounds for estimations and explore their\ncomparisons with classical results. We next consider the asymptotic behaviors\nand efficiency of estimators. We derive the on-line asymptotic efficiency for\nWasserstein natural gradient. Besides, we study a Poincar\\'e efficiency for\nWasserstein natural gradient of maximal likelihood estimation. Several\nanalytical examples of WIMs are presented, including location-scale families,\nindependent families, and rectified linear unit (ReLU) generative models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:49:47 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 03:02:06 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 15:47:55 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 09:09:42 GMT"}, {"version": "v5", "created": "Tue, 11 Aug 2020 08:24:50 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Wuchen", ""], ["Zhao", "Jiaxi", ""]]}, {"id": "1910.11291", "submitter": "Yuting Lan", "authors": "Ning Zhang, Wenxin Jiang and Yuting Lan", "title": "Conditional variable screening via ordinary least squares projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel variable screening method for linear\nmodels named as conditional screening via ordinary least squares projection\n(COLP). COLP can take advantage of prior knowledge concerning certain active\npredictors by eliminating the adverse impact of their coefficients in the\nestimation of remaining ones and thus significantly enhance the screening\naccuracy. We prove its sure-screening property under reasonable assumptions and\ndemonstrate its utility in an application to a leukemia dataset. Moreover,\nbased on the conditional approach, we introduce an iterative algorithm named as\nforward screening via ordinary least squares projection (FOLP), which not only\ncould exploit the prior information more effectively, but also has promising\nperformance when no prior knowledge is available using a data-driven\nconditioning set. Extensive simulation studies are carried out to demonstrate\nthe competence of both proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:08:54 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 02:03:49 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Zhang", "Ning", ""], ["Jiang", "Wenxin", ""], ["Lan", "Yuting", ""]]}, {"id": "1910.11429", "submitter": "Peter Holderrieth", "authors": "Peter Holderrieth", "title": "Cores for Piecewise-Deterministic Markov Processes used in Markov Chain\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show fundamental properties of the Markov semigroup of recently proposed\nMCMC algorithms based on piecewise-deterministic Markov processes (PDMPs) such\nas the Bouncy Particle Sampler, the Zig Zag Process or the Randomized\nHamiltonian Monte Carlo method. Under assumptions typically satisfied in MCMC\nsettings, we prove that PDMPs are Feller processes and the space of infinitely\ndifferentiable functions with compact support forms a core of their generator.\nAs we illustrate via martingale problems and a simplified proof of the\ninvariance of target distributions, these results provide a fundamental tool\nfor the rigorous analysis of these algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:15:07 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 21:07:20 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 21:34:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Holderrieth", "Peter", ""]]}, {"id": "1910.11575", "submitter": "Pierre Neuvial", "authors": "Gilles Blanchard (LMO), Pierre Neuvial (IMT), Etienne Roquain (LPSM\n  UMR 8001)", "title": "On agnostic post hoc approaches to false positive control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is a book chapter which gives a partial survey on post hoc\napproaches to false positive control.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 08:51:23 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Blanchard", "Gilles", "", "LMO"], ["Neuvial", "Pierre", "", "IMT"], ["Roquain", "Etienne", "", "LPSM\n  UMR 8001"]]}, {"id": "1910.11602", "submitter": "Chiara Amorino", "authors": "Chiara Amorino (LaMME), Arnaud Gloter (LaMME)", "title": "Joint estimation for volatility and drift parameters of ergodic jump\n  diffusion processes via contrast function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider an ergodic diffusion process with jumps whose drift\ncoefficient depends on $\\mu$ and volatility coefficient depends on $\\sigma$,\ntwo unknown parameters. We suppose that the process is discretely observed at\nthe instants (t n i)i=0,...,n with $\\Delta$n = sup i=0,...,n--1 (t n i+1 -- t n\ni) $\\rightarrow$ 0. We introduce an estimator of $\\theta$ := ($\\mu$, $\\sigma$),\nbased on a contrast function, which is asymptotically gaussian without\nrequiring any conditions on the rate at which $\\Delta$n $\\rightarrow$ 0,\nassuming a finite jump activity. This extends earlier results where a condition\non the step discretization was needed (see [13],[28]) or where only the\nestimation of the drift parameter was considered (see [2]). In general\nsituations, our contrast function is not explicit and in practise one has to\nresort to some approximation. We propose explicit approximations of the\ncontrast function, such that the estimation of $\\theta$ is feasible under the\ncondition that n$\\Delta$ k n $\\rightarrow$ 0 where k > 0 can be arbitrarily\nlarge. This extends the results obtained by Kessler [17] in the case of\ncontinuous processes. Efficient drift estimation, efficient volatility\nestimation,ergodic properties, high frequency data, L{\\'e}vy-driven SDE,\nthresholding methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 10:06:55 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 15:54:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Amorino", "Chiara", "", "LaMME"], ["Gloter", "Arnaud", "", "LaMME"]]}, {"id": "1910.11660", "submitter": "Ioannis Papastathopoulos", "authors": "Graeme Auld and Ioannis Papastathopoulos", "title": "Extremal clustering in non-stationary random sequences", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the distribution of extreme values of strictly\nstationary sequences differ from those of independent and identically\ndistributed sequences in that extremal clustering may occur. Here we consider\nnon-stationary but identically distributed sequences of random variables\nsubject to suitable long-range dependence restrictions. We find that the\nlimiting distribution of appropriately normalized sample maxima depends on a\nparameter that measures the average extremal clustering of the sequence. Based\non this new representation we derive the asymptotic distribution for the time\nbetween consecutive extreme observations and construct moment and\nlikelihood-based estimators for measures of extremal clustering. We specialize\nour results to random sequences with periodic dependence structure.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 12:42:46 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 09:52:21 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 06:43:05 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Auld", "Graeme", ""], ["Papastathopoulos", "Ioannis", ""]]}, {"id": "1910.11849", "submitter": "Rishabh Dudeja", "authors": "Rishabh Dudeja, Junjie Ma and Arian Maleki", "title": "Information Theoretic Limits for Phase Retrieval with Subsampled Haar\n  Sensing Matrices", "comments": "Some references added, reviewer comments addressed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information theoretic limits of recovering an unknown $n$\ndimensional, complex signal vector $\\mathbf{x}_\\star$ with unit norm from $m$\nmagnitude-only measurements of the form $y_i = |(\\mathbf{A}\n\\mathbf{x}_\\star)_i|^2, \\; i = 1,2 \\dots , m$, where $\\mathbf{A}$ is the\nsensing matrix. This is known as the Phase Retrieval problem and models\npractical imaging systems where measuring the phase of the observations is\ndifficult. Since in a number of applications, the sensing matrix has orthogonal\ncolumns, we model the sensing matrix as a subsampled Haar matrix formed by\npicking $n$ columns of a uniformly random $m \\times m$ unitary matrix. We study\nthis problem in the high dimensional asymptotic regime, where $m,n \\rightarrow\n\\infty$, while $m/n \\rightarrow \\delta$ with $\\delta$ being a fixed number, and\nshow that if $m < (2-o_n(1))\\cdot n$, then any estimator is asymptotically\northogonal to the true signal vector $\\mathbf{x}_\\star$. This lower bound is\nsharp since when $m > (2+o_n(1)) \\cdot n $, estimators that achieve a non\ntrivial asymptotic correlation with the signal vector are known from previous\nworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 17:10:19 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 06:41:30 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Dudeja", "Rishabh", ""], ["Ma", "Junjie", ""], ["Maleki", "Arian", ""]]}, {"id": "1910.11984", "submitter": "Ryota Yuasa", "authors": "Ryota Yuasa and Tatsuya Kubokawa", "title": "Ridge-type Linear Shrinkage Estimation of the Matrix Mean of\n  High-dimensional Normal Distribution", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2020.104608", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the mean matrix of the multivariate normal distribution is\naddressed in the high dimensional setting. Efron-Morris-type linear shrinkage\nestimators based on ridge estimators for the precision matrix instead of the\nMoore-Penrose generalized inverse are considered, and the weights in the\nridge-type linear shrinkage estimators are estimated in terms of minimizing the\nStein unbiased risk estimators under the quadratic loss. It is shown that the\nridge-type linear shrinkage estimators with the estimated weights are minimax,\nand that the estimated weights converge to the optimal weights in the Bayesian\nmodel with high dimension by using the random matrix theory. The performance of\nthe ridge-type linear shrinkage estimators is numerically compared with the\nexisting estimators including the Efron-Morris and James-Stein estimators.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 03:07:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yuasa", "Ryota", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1910.12327", "submitter": "Sourav Chatterjee", "authors": "Mona Azadkia, Sourav Chatterjee", "title": "A simple measure of conditional dependence", "comments": "41 pages, 2 tables. Final version. To appear in Ann. Statist. An R\n  package is available at https://CRAN.R-project.org/package=FOCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coefficient of conditional dependence between two random\nvariables $Y$ and $Z$ given a set of other variables $X_1,\\ldots,X_p$, based on\nan i.i.d. sample. The coefficient has a long list of desirable properties, the\nmost important of which is that under absolutely no distributional assumptions,\nit converges to a limit in $[0,1]$, where the limit is $0$ if and only if $Y$\nand $Z$ are conditionally independent given $X_1,\\ldots,X_p$, and is $1$ if and\nonly if $Y$ is equal to a measurable function of $Z$ given $X_1,\\ldots,X_p$.\nMoreover, it has a natural interpretation as a nonlinear generalization of the\nfamiliar partial $R^2$ statistic for measuring conditional dependence by\nregression. Using this statistic, we devise a new variable selection algorithm,\ncalled Feature Ordering by Conditional Independence (FOCI), which is\nmodel-free, has no tuning parameters, and is provably consistent under sparsity\nassumptions. A number of applications to synthetic and real datasets are worked\nout.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:14:39 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 22:14:11 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 07:14:17 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 20:44:42 GMT"}, {"version": "v5", "created": "Sun, 10 Jan 2021 08:57:37 GMT"}, {"version": "v6", "created": "Sun, 28 Mar 2021 05:52:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Azadkia", "Mona", ""], ["Chatterjee", "Sourav", ""]]}, {"id": "1910.12372", "submitter": "Saptarshi Roy", "authors": "Saptarshi Roy, Kaustav Chakraborty, Somnath Bhadra and Ayanendranath\n  Basu", "title": "Density Power Downweighting and Robust Inference: Some New Strategies", "comments": "33 Pages, 6 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving the robustness of the procedure has, at the present time, become\nalmost a default requirement for statistical data analysis. Since efficiency at\nthe model and robustness under misspecification of the model are often in\nconflict, it is important to choose such inference procedures which provide the\nbest compromise between these two concepts. Some minimum Bregman divergence\nestimators and related tests of hypothesis seem to be able to do well in this\nrespect, with the procedures based on the density power divergence providing\nthe existing standard. In this paper we propose a new family of Bregman\ndivergences which is a superfamily encompassing the density power divergence.\nThis paper describes the inference procedures resulting from this new family of\ndivergences, and makes a strong case for the utility of this divergence family\nin statistical inference.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 22:37:58 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Roy", "Saptarshi", ""], ["Chakraborty", "Kaustav", ""], ["Bhadra", "Somnath", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1910.12394", "submitter": "Albert Vexler", "authors": "Albert Vexler", "title": "Univariate Likelihood Projections and Characterizations of the\n  Multivariate Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of characterizing a multivariate distribution of a random vector\nusing examination of univariate combinations of vector components is an\nessential issue of multivariate analysis. The likelihood principle plays a\nprominent role in developing powerful statistical inference tools. In this\ncontext, we raise the question: can the univariate likelihood function based on\na random vector be used to provide the uniqueness in reconstructing the vector\ndistribution? In multivariate normal (MN) frameworks, this question links to a\nreverse of Cochran's theorem that concerns the distribution of quadratic forms\nin normal variables. We characterize the MN distribution through the univariate\nlikelihood type projections. The proposed principle is employed to illustrate\nsimple techniques for assessing multivariate normality via well-known tests\nthat use univariate observations. The presented testing strategy can exhibit\nhigh and stable power characteristics in comparison to the well-known\nprocedures in various scenarios when observed vectors are non-MN distributed,\nwhereas their components are normally distributed random variables. In such\ncases, the classical multivariate normality tests may break down completely.\n  KEY WORDS: Characterization, Goodness of fit, Infinity divisible, Likelihood,\nMultivariate normal distribution, Projection, Quadratic form, Test for\nmultivariate normality.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 01:11:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Vexler", "Albert", ""]]}, {"id": "1910.12428", "submitter": "Jingbo Liu", "authors": "Jingbo Liu and Philippe Rigollet", "title": "Power analysis of knockoff filters for correlated designs", "comments": "Accepted to Neurips 2019. The conference version includes the\n  contents of this version excluding the appendices. v3 on arXiv corrected some\n  typos in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knockoff filter introduced by Barber and Cand\\`es 2016 is an elegant\nframework for controlling the false discovery rate in variable selection. While\nempirical results indicate that this methodology is not too conservative, there\nis no conclusive theoretical result on its power. When the predictors are\ni.i.d. Gaussian, it is known that as the signal to noise ratio tend to\ninfinity, the knockoff filter is consistent in the sense that one can make FDR\ngo to 0 and power go to 1 simultaneously. In this work we study the case where\nthe predictors have a general covariance matrix $\\Sigma$. We introduce a simple\nfunctional called effective signal deficiency (ESD) of the covariance matrix\n$\\Sigma$ that predicts consistency of various variable selection methods. In\nparticular, ESD reveals that the structure of the precision matrix\n$\\Sigma^{-1}$ plays a central role in consistency and therefore, so does the\nconditional independence structure of the predictors. To leverage this\nconnection, we introduce Conditional Independence knockoff, a simple procedure\nthat is able to compete with the more sophisticated knockoff filters and that\nis defined when the predictors obey a Gaussian tree graphical models (or when\nthe graph is sufficiently sparse). Our theoretical results are supported by\nnumerical evidence on synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 03:48:42 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:40:50 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 21:12:13 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Liu", "Jingbo", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1910.12545", "submitter": "Timo Dimitriadis", "authors": "Timo Dimitriadis, Andrew J. Patton, Patrick W. Schmidt", "title": "Testing Forecast Rationality for Measures of Central Tendency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN math.ST q-fin.EC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rational respondents to economic surveys may report as a point forecast any\nmeasure of the central tendency of their (possibly latent) predictive\ndistribution, for example the mean, median, mode, or any convex combination\nthereof. We propose tests of forecast rationality when the measure of central\ntendency used by the respondent is unknown. We overcome an identification\nproblem that arises when the measures of central tendency are equal or in a\nlocal neighborhood of each other, as is the case for (exactly or nearly)\nsymmetric distributions. As a building block, we also present novel tests for\nthe rationality of mode forecasts. We apply our tests to survey forecasts of\nindividual income, Greenbook forecasts of U.S. GDP, and random walk forecasts\nfor exchange rates. We find that the Greenbook and random walk forecasts are\nbest rationalized as mean, or near-mean forecasts, while the income survey\nforecasts are best rationalized as mode forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 10:41:07 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 13:42:35 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 09:27:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dimitriadis", "Timo", ""], ["Patton", "Andrew J.", ""], ["Schmidt", "Patrick W.", ""]]}, {"id": "1910.12697", "submitter": "Aditya Deshmukh", "authors": "Aditya Deshmukh, Srikrishna Bhashyam, Venugopal V. Veeravalli", "title": "Sequential Controlled Sensing for Composite Multihypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multi-hypothesis testing with controlled sensing of\nobservations is considered. The distribution of observations collected under\neach control is assumed to follow a single-parameter exponential family\ndistribution. The goal is to design a policy to find the true hypothesis with\nminimum expected delay while ensuring that the probability of error is below a\ngiven constraint. The decision-maker can control the delay by intelligently\nchoosing the control for observation collection in each time slot. We derive a\npolicy that satisfies the given constraint on the error probability. We also\nshow that the policy is asymptotically optimal in the sense that it\nasymptotically achieves an information-theoretic lower bound on the expected\ndelay.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 22:21:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Deshmukh", "Aditya", ""], ["Bhashyam", "Srikrishna", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1910.12701", "submitter": "Junshan Xie", "authors": "Tiefeng Jiang and Junshan Xie", "title": "Limiting behavior of largest entry of random tensor constructed by\n  high-dimensional data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let ${X}_{k}=(x_{k1}, \\cdots, x_{kp})', k=1,\\cdots,n$, be a random sample of\nsize $n$ coming from a $p$-dimensional population. For a fixed integer $m\\geq\n2$, consider a hypercubic random tensor $\\mathbf{{T}}$ of $m$-th order and rank\n$n$ with \\begin{eqnarray*}\n  \\mathbf{{T}}= \\sum_{k=1}^{n}\\underbrace{{X}_{k}\\otimes\\cdots\\otimes\n{X}_{k}}_{m~multiple}=\\Big(\\sum_{k=1}^{n} x_{ki_{1}}x_{ki_{2}}\\cdots\nx_{ki_{m}}\\Big)_{1\\leq i_{1},\\cdots, i_{m}\\leq p}. \\end{eqnarray*} Let $W_n$ be\nthe largest off-diagonal entry of $\\mathbf{{T}}$. We derive the asymptotic\ndistribution of $W_n$ under a suitable normalization for two cases. They are\nthe ultra-high dimension case with $p\\to\\infty$ and $\\log p=o(n^{\\beta})$ and\nthe high-dimension case with $p\\to \\infty$ and $p=O(n^{\\alpha})$ where\n$\\alpha,\\beta>0$. The normalizing constant of $W_n$ depends on $m$ and the\nlimiting distribution of $W_n$ is a Gumbel-type distribution involved with\nparameter $m$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:20:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jiang", "Tiefeng", ""], ["Xie", "Junshan", ""]]}, {"id": "1910.12756", "submitter": "Nikita Zhivotovskiy", "authors": "Olivier Bousquet, Nikita Zhivotovskiy", "title": "Fast classification rates without standard margin assumptions", "comments": "29 pages, 1 figure; presentation changed according to referees\n  suggestion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of learning rates for classes with finite\nVC dimension. It is well known that fast learning rates up to\n$O\\left(\\frac{d}{n}\\right)$ are achievable by the empirical risk minimization\nalgorithm (ERM) if low noise or margin assumptions are satisfied. These usually\nrequire the optimal Bayes classifier to be in the class, and it has been shown\nthat when this is not the case, the fast rates cannot be achieved even in the\nnoise free case. In this paper, we further investigate the question of the fast\nrates under the misspecification, when the Bayes classifier is not in the class\n(also called the agnostic setting).\n  First, we consider classification with a reject option, namely Chow's reject\noption model, and show that by slightly lowering the impact of hard instances,\na learning rate of order $O\\left(\\frac{d}{n}\\log \\frac{n}{d}\\right)$ is always\nachievable in the agnostic setting by a specific learning algorithm. Similar\nresults were only known under special versions of margin assumptions. We also\nshow that the performance of the proposed algorithm is never worse than the\nperformance of ERM.\n  Based on those results, we derive the necessary and sufficient conditions for\nclassification (without a reject option) with fast rates in the agnostic\nsetting achievable by improper learners. This simultaneously extends the work\nof Massart and N\\'{e}d\\'{e}lec (Ann. of Statistics, 2006), which studied this\nquestion in the case where the Bayesian optimal rule belongs to the class, and\nthe work of Ben-David and Urner (COLT, 2014), which allows the misspecification\nbut is limited to the no noise setting. Our result also provides the first\ngeneral setup in statistical learning theory in which an improper learning\nalgorithm may significantly improve the learning rate for non-convex losses.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 15:34:57 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 17:24:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bousquet", "Olivier", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "1910.12797", "submitter": "Chao Gao", "authors": "Chao Gao and Zongming Ma", "title": "Testing Equivalence of Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we test whether two datasets share a common clustering\nstructure. As a leading example, we focus on comparing clustering structures in\ntwo independent random samples from two mixtures of multivariate normal\ndistributions. Mean parameters of these normal distributions are treated as\npotentially unknown nuisance parameters and are allowed to differ. Assuming\nknowledge of mean parameters, we first determine the phase diagram of the\ntesting problem over the entire range of signal-to-noise ratios by providing\nboth lower bounds and tests that achieve them. When nuisance parameters are\nunknown, we propose tests that achieve the detection boundary adaptively as\nlong as ambient dimensions of the datasets grow at a sub-linear rate with the\nsample size.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:48:49 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 18:52:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""]]}, {"id": "1910.12831", "submitter": "Sebastian Espinosa", "authors": "Sebastian Espinosa, Jorge F. Silva and Pablo Piantanida", "title": "On the Exponential Approximation of Type II Error Probability of\n  Distributed Test of Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed binary test of statistical independence under\ncommunication (information bits) constraints. While testing independence is\nvery relevant in various applications, distributed independence test is\nparticularly useful for event detection in sensor networks where data\ncorrelation often occurs among observations of devices in the presence of a\nsignal of interest. By focusing on the case of two devices because of their\ntractability, we begin by investigating conditions on Type I error probability\nrestrictions under which the minimum Type II error admits an exponential\nbehavior with the sample size. Then, we study the finite sample-size regime of\nthis problem. We derive new upper and lower bounds for the gap between the\nminimum Type II error and its exponential approximation under different setups,\nincluding restrictions imposed on the vanishing Type I error probability. Our\ntheoretical results shed light on the sample-size regimes at which\napproximations of the Type II error probability via error exponents became\ninformative enough in the sense of predicting well the actual error\nprobability. We finally discuss an application of our results where the gap is\nevaluated numerically, and we show that exponential approximations are not only\ntractable but also a valuable proxy for the Type II probability of error in the\nfinite-length regime.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:41:22 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 14:37:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Espinosa", "Sebastian", ""], ["Silva", "Jorge F.", ""], ["Piantanida", "Pablo", ""]]}, {"id": "1910.12871", "submitter": "Yoshiki Kinoshita", "authors": "Yoshiki Kinoshita and Nakahiro Yoshida", "title": "Penalized quasi likelihood estimation for variable selection", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized methods are applied to quasi likelihood analysis for stochastic\ndifferential equation models. In this paper, we treat the quasi likelihood\nfunction and the associated statistical random field for which a polynomial\ntype large deviation inequality holds. Then penalty terms do not disturb a\npolynomial type large deviation inequality. This property ensures the\nconvergence of moments of the associated estimator which plays an important\nrole to evaluate the upper bound of the probability that model selection is\nincorrect.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:00:01 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Kinoshita", "Yoshiki", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1910.12970", "submitter": "Lan Gao", "authors": "Lan Gao, Yingying Fan, Jinchi Lv and Qi-Man Shao", "title": "Asymptotic Distributions of High-Dimensional Distance Correlation\n  Inference", "comments": "67 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance correlation has become an increasingly popular tool for detecting\nthe nonlinear dependence between a pair of potentially high-dimensional random\nvectors. Most existing works have explored its asymptotic distributions under\nthe null hypothesis of independence between the two random vectors when only\nthe sample size or the dimensionality diverges. Yet its asymptotic null\ndistribution for the more realistic setting when both sample size and\ndimensionality diverge in the full range remains largely underdeveloped. In\nthis paper, we fill such a gap and develop central limit theorems and\nassociated rates of convergence for a rescaled test statistic based on the\nbias-corrected distance correlation in high dimensions under some mild\nregularity conditions and the null hypothesis. Our new theoretical results\nreveal an interesting phenomenon of blessing of dimensionality for\nhigh-dimensional distance correlation inference in the sense that the accuracy\nof normal approximation can increase with dimensionality. Moreover, we provide\na general theory on the power analysis under the alternative hypothesis of\ndependence, and further justify the capability of the rescaled distance\ncorrelation in capturing the pure nonlinear dependency under moderately high\ndimensionality for a certain type of alternative hypothesis. The theoretical\nresults and finite-sample performance of the rescaled statistic are illustrated\nwith several simulation examples and a blockchain application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:58:44 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 01:21:55 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 01:40:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gao", "Lan", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""], ["Shao", "Qi-Man", ""]]}, {"id": "1910.13074", "submitter": "Yumou Qiu", "authors": "Song Xi Chen, Bin Guo, Yumou Qiu", "title": "Multi-level Thresholding Test for High Dimensional Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing the equality of two high-dimensional covariance matrices\nby carrying out a multi-level thresholding procedure, which is designed to\ndetect sparse and faint differences between the covariances. A novel\nU-statistic composition is developed to establish the asymptotic distribution\nof the thresholding statistics in conjunction with the matrix blocking and the\ncoupling techniques. We propose a multi-thresholding test that is shown to be\npowerful in detecting sparse and weak differences between two covariance\nmatrices. The test is shown to have attractive detection boundary and to attain\nthe optimal minimax rate in the signal strength under different regimes of high\ndimensionality and the sparsity of the signal. Simulation studies are conducted\nto demonstrate the utility of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:58:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chen", "Song Xi", ""], ["Guo", "Bin", ""], ["Qiu", "Yumou", ""]]}, {"id": "1910.13289", "submitter": "Yi Yu", "authors": "Oscar Hernan Madrid Padilla and Yi Yu and Daren Wang and Alessandro\n  Rinaldo", "title": "Optimal nonparametric multivariate change point detection and\n  localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multivariate nonparametric change point detection problem, where\nthe data are a sequence of independent $p$-dimensional random vectors whose\ndistributions are piecewise-constant with Lipschitz densities changing at\nunknown times, called change points. We quantify the size of the distributional\nchange at any change point with the supremum norm of the difference between the\ncorresponding densities. We are concerned with the localization task of\nestimating the positions of the change points. In our analysis, we allow for\nthe model parameters to vary with the total number of time points, including\nthe minimal spacing between consecutive change points and the magnitude of the\nsmallest distributional change. We provide information-theoretic lower bounds\non both the localization rate and the minimal signal-to-noise ratio required to\nguarantee consistent localization. We formulate a novel algorithm based on\nkernel density estimation that nearly achieves the minimax lower bound, save\npossibly for logarithm factors. We have provided extensive numerical evidence\nto support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:27:40 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 09:29:55 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:37:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1910.13316", "submitter": "Jeffrey Rosenthal", "authors": "J.S. Rosenthal, A. Dote, K. Dabiri, H. Tamura, S. Chen, A.\n  Sheikholeslami", "title": "Jump Markov Chains and Rejection-Free Metropolis Algorithms", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider versions of the Metropolis algorithm which avoid the inefficiency\nof rejections. We first illustrate that a natural Uniform Selection Algorithm\nmight not converge to the correct distribution. We then analyse the use of\nMarkov jump chains which avoid successive repetitions of the same state. After\nexploring the properties of jump chains, we show how they can exploit\nparallelism in computer hardware to produce more efficient samples. We apply\nour results to the Metropolis algorithm, to Parallel Tempering, to a Bayesian\nmodel, to a two-dimensional ferromagnetic 4 x 4 Ising model, and to a\npseudo-marginal MCMC algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:20:39 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 00:27:10 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 16:27:55 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Rosenthal", "J. S.", ""], ["Dote", "A.", ""], ["Dabiri", "K.", ""], ["Tamura", "H.", ""], ["Chen", "S.", ""], ["Sheikholeslami", "A.", ""]]}, {"id": "1910.13418", "submitter": "Alexander Petersen", "authors": "Alexander Petersen, Xi Liu, and Afshin A. Divani", "title": "Wasserstein $F$-tests and Confidence Bands for the Fr\\`echet Regression\n  of Density Response Curves", "comments": "58 pages (with Appendix), 5 figures, accepted at Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data consisting of samples of probability density functions are increasingly\nprevalent, necessitating the development of methodologies for their analysis\nthat respect the inherent nonlinearities associated with densities. In many\napplications, density curves appear as functional response objects in a\nregression model with vector predictors. For such models, inference is key to\nunderstand the importance of density-predictor relationships, and the\nuncertainty associated with the estimated conditional mean densities, defined\nas conditional Fr\\'echet means under a suitable metric. Using the Wasserstein\ngeometry of optimal transport, we consider the Fr\\'echet regression of density\ncurve responses and develop tests for global and partial effects, as well as\nsimultaneous confidence bands for estimated conditional mean densities. The\nasymptotic behavior of these objects is based on underlying functional central\nlimit theorems within Wasserstein space, and we demonstrate that they are\nasymptotically of the correct size and coverage, with uniformly strong\nconsistency of the proposed tests under sequences of contiguous alternatives.\nThe accuracy of these methods, including nominal size, power, and coverage, is\nassessed through simulations, and their utility is illustrated through a\nregression analysis of post-intracerebral hemorrhage hematoma densities and\ntheir associations with a set of clinical and radiological covariates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:30:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 16:37:19 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Petersen", "Alexander", ""], ["Liu", "Xi", ""], ["Divani", "Afshin A.", ""]]}, {"id": "1910.13668", "submitter": "Ting-Kam Leonard Wong", "authors": "Peter Baxendale, Ting-Kam Leonard Wong", "title": "Random concave functions", "comments": "42 pages, 8 figures. Substantially revised. To appear in The Annals\n  of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spaces of convex and concave functions appear naturally in theory and\napplications. For example, convex regression and log-concave density estimation\nare important topics in nonparametric statistics. In stochastic portfolio\ntheory, concave functions on the unit simplex measure the concentration of\ncapital, and their gradient maps define novel investment strategies. The\ngradient maps may also be regarded as optimal transport maps on the simplex. In\nthis paper we construct and study probability measures supported on spaces of\nconcave functions. These measures may serve as prior distributions in Bayesian\nstatistics and Cover's universal portfolio, and induce distribution-valued\nrandom variables via optimal transport. The random concave functions are\nconstructed on the unit simplex by taking a suitably scaled (mollified, or\nsoft) minimum of random hyperplanes. Depending on the regime of the parameters,\nwe show that as the number of hyperplanes tends to infinity there are several\npossible limiting behaviors. In particular, there is a transition from a\ndeterministic almost sure limit to a non-trivial limiting distribution that can\nbe characterized using convex duality and Poisson point processes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 04:52:32 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 03:06:19 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 04:08:33 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Baxendale", "Peter", ""], ["Wong", "Ting-Kam Leonard", ""]]}, {"id": "1910.13972", "submitter": "Paxton Turner", "authors": "Paxton Turner, Raghu Meka, Philippe Rigollet", "title": "Balancing Gaussian vectors in high dimension", "comments": null, "journal-ref": "COLT 2020", "doi": null, "report-no": null, "categories": "cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems in controlled experiments, we study the discrepancy of\nrandom matrices with continuous entries where the number of columns $n$ is much\nlarger than the number of rows $m$. Our first result shows that if $\\omega(1) =\nm = o(n)$, a matrix with i.i.d. standard Gaussian entries has discrepancy\n$\\Theta(\\sqrt{n} \\, 2^{-n/m})$ with high probability. This provides sharp\nguarantees for Gaussian discrepancy in a regime that had not been considered\nbefore in the existing literature. Our results also apply to a more general\nfamily of random matrices with continuous i.i.d entries, assuming that $m =\nO(n/\\log{n})$. The proof is non-constructive and is an application of the\nsecond moment method. Our second result is algorithmic and applies to random\nmatrices whose entries are i.i.d. and have a Lipschitz density. We present a\nrandomized polynomial-time algorithm that achieves discrepancy\n$e^{-\\Omega(\\log^2(n)/m)}$ with high probability, provided that $m =\nO(\\sqrt{\\log{n}})$. In the one-dimensional case, this matches the best known\nalgorithmic guarantees due to Karmarkar--Karp. For higher dimensions $2 \\leq m\n= O(\\sqrt{\\log{n}})$, this establishes the first efficient algorithm achieving\ndiscrepancy smaller than $O( \\sqrt{m} )$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:41:06 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 03:55:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Turner", "Paxton", ""], ["Meka", "Raghu", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1910.13986", "submitter": "Reese Pathak", "authors": "Simon Foucart, Deanna Needell, Reese Pathak, Yaniv Plan, Mary Wootters", "title": "Weighted matrix completion from non-random, non-uniform sampling\n  patterns", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the matrix completion problem when the observation pattern is\ndeterministic and possibly non-uniform. We propose a simple and efficient\ndebiased projection scheme for recovery from noisy observations and analyze the\nerror under a suitable weighted metric. We introduce a simple function of the\nweight matrix and the sampling pattern that governs the accuracy of the\nrecovered matrix. We derive theoretical guarantees that upper bound the\nrecovery error and nearly matching lower bounds that showcase optimality in\nseveral regimes. Our numerical experiments demonstrate the computational\nefficiency and accuracy of our approach, and show that debiasing is essential\nwhen using non-uniform sampling patterns.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:08:16 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Foucart", "Simon", ""], ["Needell", "Deanna", ""], ["Pathak", "Reese", ""], ["Plan", "Yaniv", ""], ["Wootters", "Mary", ""]]}, {"id": "1910.14009", "submitter": "Paul Schneider", "authors": "Paul Schneider and Caio Almeida", "title": "Constrained Polynomial Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a non-negative polynomial minimum-norm likelihood ratio (PLR) of\ntwo distributions of which only moments are known. The PLR converges to the\ntrue, unknown, likelihood ratio. We show consistency, obtain the asymptotic\ndistribution for the PLR coefficients estimated with sample moments, and\npresent two applications. The first develops a PLR for the unknown transition\ndensity of a jump-diffusion process. The second modifies the Hansen-Jagannathan\npricing kernel framework to accommodate linear return models consistent with\nno-arbitrage.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 17:45:29 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 14:41:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Schneider", "Paul", ""], ["Almeida", "Caio", ""]]}, {"id": "1910.14067", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e and Konstantin Usevich", "title": "Spectral properties of kernel matrices in the flat limit", "comments": "40 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel matrices are of central importance to many applied fields. In this\nmanuscript, we focus on spectral properties of kernel matrices in the so-called\n\"flat limit\", which occurs when points are close together relative to the scale\nof the kernel. We establish asymptotic expressions for the determinants of the\nkernel matrices, which we then leverage to obtain asymptotic expressions for\nthe main terms of the eigenvalues. Analyticity of the eigenprojectors yields\nexpressions for limiting eigenvectors, which are strongly tied to discrete\northogonal polynomials. Both smooth and finitely smooth kernels are covered,\nwith stronger results available in the finite smoothness case.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:09:29 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 10:54:20 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Usevich", "Konstantin", ""]]}, {"id": "1910.14167", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler, Dheeraj Nagaraj", "title": "Phase Transitions for Detecting Latent Geometry in Random Graphs", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graphs with latent geometric structure are popular models of social\nand biological networks, with applications ranging from network user profiling\nto circuit design. These graphs are also of purely theoretical interest within\ncomputer science, probability and statistics. A fundamental initial question\nregarding these models is: when are these random graphs affected by their\nlatent geometry and when are they indistinguishable from simpler models without\nlatent structure, such as the Erd\\H{o}s-R\\'{e}nyi graph $\\mathcal{G}(n, p)$? We\naddress this question for two of the most well-studied models of random graphs\nwith latent geometry -- the random intersection and random geometric graph.\n  Our results are as follows: (1) we prove that the random intersection graph\nconverges in total variation to $\\mathcal{G}(n, p)$ when $d =\n\\tilde{\\omega}(n^3)$, and does not if $d = o(n^3)$, resolving an open problem\nin Fill et al. (2000), Rybarczyk (2011) and Kim et al. (2018); (2) we provide\nconditions under which the matrix of intersection sizes of random family of\nsets converges in total variation to a symmetric matrix with independent\nPoisson entries, yielding the first total variation convergence result for\n$\\tau$-random intersection graphs to $\\mathcal{G}(n, p)$; and (3) we show that\nthe random geometric graph on $\\mathbb{S}^{d - 1}$ with edge density $p$\nconverges in total variation to $\\mathcal{G}(n, p)$ when $d =\n\\tilde{\\omega}\\left(\\min\\{ pn^3, p^2 n^{7/2} \\} \\right)$, yielding the first\nprogress towards a conjecture of Bubeck et al. (2016). The first of these three\nresults was obtained simultaneously and independently by Bubeck, Racz and\nRichey.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:41:51 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 18:30:32 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 02:54:41 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""], ["Nagaraj", "Dheeraj", ""]]}, {"id": "1910.14330", "submitter": "Qing Yang", "authors": "Q. Yang, Y. Li and Y. Zhang", "title": "Change Point Detection for Nonparametric Regression under Strongly\n  Mixing Process", "comments": "34 pages, 20 figures, 1 tex file, 5 tex auxiliary files", "journal-ref": null, "doi": "10.1007/s00362-020-01196-y", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the estimation of the structural change point in\nthe nonparametric model with dependent observations. We introduce a\nmaximum-CUSUM-estimation procedure, where the CUSUM statistic is constructed\nbased on the sum-of-squares aggregation of the difference of the two\nNadaraya-Watson estimates using the observations before and after a specific\ntime point. Under some mild conditions, we prove that the statistic tends to\nzero almost surely if there is no change, and is larger than a threshold\nasymptotically almost surely otherwise, which helps us to obtain a\nthreshold-detection strategy. Furthermore, we demonstrate the strong\nconsistency of the change point estimator. In the simulation, we discuss the\nselection of the bandwidth and the threshold used in the estimation, and show\nthe robustness of our method in the long-memory scenario. We implement our\nmethod to the data of Nasdaq 100 index and find that the relation between the\nrealized volatility and the return exhibits several structural changes in\n2007-2009.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 09:33:52 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 03:47:34 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yang", "Q.", ""], ["Li", "Y.", ""], ["Zhang", "Y.", ""]]}, {"id": "1910.14458", "submitter": "Edouard Pauwels", "authors": "Mai Trang Vu, Fran\\c{c}ois Bachoc, Edouard Pauwels", "title": "Rate of convergence for geometric inference based on the empirical\n  Christoffel function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the support of a measure from a finite,\nindependent, sample. The estimators which are considered are constructed based\non the empirical Christoffel function. Such estimators have been proposed for\nthe problem of set estimation with heuristic justifications. We carry out a\ndetailed finite sample analysis, that allows us to select the threshold and\ndegree parameters as a function of the sample size. We provide a convergence\nrate analysis of the resulting support estimation procedure. Our analysis\nestablishes that we may obtain finite sample bounds which are comparable to\nexisting rates for different set estimation procedures. Our results rely on\nconcentration inequalities for the empirical Christoffel function and on\nestimates of the supremum of the Christoffel-Darboux kernel on sets with smooth\nboundaries, that can be considered of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 13:32:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:21:26 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Vu", "Mai Trang", ""], ["Bachoc", "Fran\u00e7ois", ""], ["Pauwels", "Edouard", ""]]}, {"id": "1910.14632", "submitter": "Matthew Dunlop", "authors": "Matthew M. Dunlop", "title": "Multiplicative noise in Bayesian inverse problems: Well-posedness and\n  consistency of MAP estimators", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplicative noise arises in inverse problems when, for example,\nuncertainty on measurements is proportional to the size of the measurement\nitself. The likelihood that arises is hence more complicated than that from\nadditive noise. We consider two multiplicative noise models: purely\nmultiplicative noise, and a mixture of multiplicative noise and additive noise.\nAdopting a Bayesian approach, we provide conditions for the resulting posterior\ndistributions on Banach space to be continuous with respect to perturbations in\nthe data; the inclusion of additive noise in addition to multiplicative noise\nacts as a form of regularization, allowing for milder conditions on the forward\nmap. Additionally, we show that MAP estimators exist for both the purely\nmultiplicative and mixed noise models when a Gaussian prior is employed, and\nfor the latter prove that they are consistent in the small noise limit when all\nnoise is Gaussian.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:20:26 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Dunlop", "Matthew M.", ""]]}]