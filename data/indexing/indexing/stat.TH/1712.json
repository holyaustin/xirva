[{"id": "1712.00155", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda and Kazuyuki Tanaka", "title": "Susceptibility Propagation by Using Diagonal Consistency", "comments": null, "journal-ref": "Phys. Rev. E, Vol.87, 012134, 2013", "doi": "10.1103/PhysRevE.87.012134", "report-no": null, "categories": "cond-mat.stat-mech math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A susceptibility propagation that is constructed by combining a belief\npropagation and a linear response method is used for approximate computation\nfor Markov random fields. Herein, we formulate a new, improved susceptibility\npropagation by using the concept of a diagonal matching method that is based on\nmean-field approaches to inverse Ising problems. The proposed susceptibility\npropagation is robust for various network structures, and it is reduced to the\nordinary susceptibility propagation and to the adaptive\nThouless-Anderson-Palmer equation in special cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 02:16:05 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yasuda", "Muneki", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1712.00196", "submitter": "Fangjun Xu", "authors": "Hailing Sang, Yongli Sang and Fangjun Xu", "title": "Kernel entropy estimation for linear processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{X_n: n\\in \\mathbb{N}\\}$ be a linear process with bounded probability\ndensity function $f(x)$. We study the estimation of the quadratic functional\n$\\int_{\\mathbb{R}} f^2(x)\\, dx$. With a Fourier transform on the kernel\nfunction and the projection method, it is shown that, under certain mild\nconditions, the estimator \\[ \\frac{2}{n(n-1)h_n} \\sum_{1\\le i<j\\le\nn}K\\left(\\frac{X_i-X_j}{h_n}\\right) \\] has similar asymptotical properties as\nthe i.i.d. case studied in Gin\\'{e} and Nickl (2008) if the linear process\n$\\{X_n: n\\in \\mathbb{N}\\}$ has the defined short range dependence. We also\nprovide an application to $L^2_2$ divergence and the extension to multivariate\nlinear processes. The simulation study for linear processes with Gaussian and\n$\\alpha$-stable innovations confirms our theoretical results. As an\nillustration, we estimate the $L^2_2$ divergences among the density functions\nof average annual river flows for four rivers and obtain promising results.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 05:07:13 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Sang", "Hailing", ""], ["Sang", "Yongli", ""], ["Xu", "Fangjun", ""]]}, {"id": "1712.00346", "submitter": "Ryo Imai", "authors": "Ryo Imai, Tatsuya Kubokawa and Malay Ghosh", "title": "Bayes Minimax Competitors of Preliminary Test Estimators in k Sample\n  Problems", "comments": "16 pages. arXiv admin note: text overlap with arXiv:1711.10822", "journal-ref": "Japanese Journal of Statistics and Data Science June 2018, Volume\n  1, Issue 1, pp 3-21", "doi": "10.1007/s42081-018-0002-x", "report-no": "JJSD-D-17-00012R2", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the estimation of a mean vector of a multivariate\nnormal population where the mean vector is suspected to be nearly equal to mean\nvectors of $k-1$ other populations. As an alternative to the preliminary test\nestimator based on the test statistic for testing hypothesis of equal means, we\nderive empirical and hierarchical Bayes estimators which shrink the sample mean\nvector toward a pooled mean estimator given under the hypothesis. The\nminimaxity of those Bayesian estimators are shown, and their performances are\ninvestigated by simulation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 07:10:49 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 11:45:03 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 12:17:23 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Imai", "Ryo", ""], ["Kubokawa", "Tatsuya", ""], ["Ghosh", "Malay", ""]]}, {"id": "1712.00541", "submitter": "Hailin Sang", "authors": "Janet Nakarmi and Hailin Sang", "title": "Central limit theorem for the variable bandwidth kernel density\n  estimators", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the ideal variable bandwidth kernel density estimator\nintroduced by McKay (1993) and Jones, McKay and Hu (1994) and the plug-in\npractical version of the variable bandwidth kernel estimator with two sequences\nof bandwidths as in Gin\\'e and Sang (2013). Based on the bias and variance\nanalysis of the ideal and true variable bandwidth kernel density estimators, we\nstudy the central limit theorems for each of them.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 03:47:11 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nakarmi", "Janet", ""], ["Sang", "Hailin", ""]]}, {"id": "1712.00710", "submitter": "Christina Lee Yu", "authors": "Christian Borgs, Jennifer Chayes, Devavrat Shah, and Christina Lee Yu", "title": "Iterative Collaborative Filtering for Sparse Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sparse matrix estimation where the goal is to estimate an\n$n\\times n$ matrix from noisy observations of a small subset of its entries. We\nanalyze the estimation error of the popularly utilized collaborative filtering\nalgorithm for the sparse regime. Specifically, we propose a novel iterative\nvariant of the algorithm, adapted to handle the setting of sparse observations.\nWe establish that as long as the fraction of entries observed at random scale\nas $\\frac{\\log^{1+\\kappa}(n)}{n}$ for any fixed $\\kappa > 0$, the estimation\nerror with respect to the $\\max$-norm decays to $0$ as $n\\to\\infty$ assuming\nthe underlying matrix of interest has constant rank $r$. Our result is robust\nto model mis-specification in that if the underlying matrix is approximately\nrank $r$, then the estimation error decays to the approximate error with\nrespect to the $\\max$-norm. In the process, we establish algorithm's ability to\nhandle arbitrary bounded noise in the observations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 05:39:33 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 04:23:31 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 17:11:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Shah", "Devavrat", ""], ["Yu", "Christina Lee", ""]]}, {"id": "1712.00711", "submitter": "Yuting Wei", "authors": "Yuting Wei, Martin J. Wainwright", "title": "The local geometry of testing in ellipses: Tight control via localized\n  Kolmogorov widths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the local geometry of testing a mean vector within a\nhigh-dimensional ellipse against a compound alternative. Given samples of a\nGaussian random vector, the goal is to distinguish whether the mean is equal to\na known vector within an ellipse, or equal to some other unknown vector in the\nellipse. Such ellipse testing problems lie at the heart of several\napplications, including non-parametric goodness-of-fit testing, signal\ndetection in cognitive radio, and regression function testing in reproducing\nkernel Hilbert spaces. While past work on such problems has focused on the\ndifficulty in a global sense, we study difficulty in a way that is localized to\neach vector within the ellipse. Our main result is to give sharp upper and\nlower bounds on the localized minimax testing radius in terms of an explicit\nformula involving the Kolmogorov width of the ellipse intersected with a\nEuclidean ball. When applied to particular examples, our general theorems yield\ninteresting rates that were not known before: as a particular case, for testing\nin Sobolev ellipses of smoothness $\\alpha$, we demonstrate rates that vary from\n$(\\sigma^2)^{\\frac{4 \\alpha}{4 \\alpha + 1}}$, corresponding to the classical\nglobal rate, to the faster rate $(\\sigma^2)^{\\frac{8\n  \\alpha}{8 \\alpha + 1}}$, achievable for vectors at favorable locations within\nthe ellipse. We also show that the optimal test for this problem is achieved by\na linear projection test that is based on an explicit lower-dimensional\nprojection of the observation vector.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 05:53:36 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 05:58:21 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Wei", "Yuting", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1712.00771", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Kengo Kato", "title": "Randomized incomplete $U$-statistics in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference for the mean vector of a high-dimensional\n$U$-statistic. In the era of Big Data, the dimension $d$ of the $U$-statistic\nand the sample size $n$ of the observations tend to be both large, and the\ncomputation of the $U$-statistic is prohibitively demanding. Data-dependent\ninferential procedures such as the empirical bootstrap for $U$-statistics is\neven more computationally expensive. To overcome such computational bottleneck,\nincomplete $U$-statistics obtained by sampling fewer terms of the $U$-statistic\nare attractive alternatives. In this paper, we introduce randomized incomplete\n$U$-statistics with sparse weights whose computational cost can be made\nindependent of the order of the $U$-statistic. We derive non-asymptotic\nGaussian approximation error bounds for the randomized incomplete\n$U$-statistics in high dimensions, namely in cases where the dimension $d$ is\npossibly much larger than the sample size $n$, for both non-degenerate and\ndegenerate kernels. In addition, we propose generic bootstrap methods for the\nincomplete $U$-statistics that are computationally much less-demanding than\nexisting bootstrap methods, and establish finite sample validity of the\nproposed bootstrap methods. Our methods are illustrated on the application to\nnonparametric testing for the pairwise independence of a high-dimensional\nrandom vector under weaker assumptions than those appearing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 14:01:42 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 16:34:38 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 06:26:12 GMT"}, {"version": "v4", "created": "Sun, 27 Jan 2019 20:11:03 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1712.00812", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Exact upper and lower bounds on the misclassification probability", "comments": "Version 3: exact upper bounds are added; results are compared with\n  ones by Feder and Merhav. Version 4: additional discussion presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact lower and upper bounds on the best possible misclassification\nprobability for a finite number of classes are obtained in terms of the total\nvariation norms of the differences between the sub-distributions over the\nclasses. These bounds are compared with the exact bounds in terms of the\nconditional entropy obtained by Feder and Merhav.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 18:42:40 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 16:47:32 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 02:00:29 GMT"}, {"version": "v4", "created": "Fri, 9 Feb 2018 02:12:50 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1712.00858", "submitter": "Jeremy Sumner", "authors": "Venta Terauds and Jeremy Sumner", "title": "Circular genome rearrangement models: applying representation theory to\n  evolutionary distance calculations", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.GR math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the symmetry of circular genome rearrangement models, discuss\nthe implementation of a new representation-theoretic method of calculating\nevolutionary distances between circular genomes, and give the results of some\ninitial calculations for genomes with up to 11 regions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 23:59:01 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Terauds", "Venta", ""], ["Sumner", "Jeremy", ""]]}, {"id": "1712.00892", "submitter": "Zhigang Bao", "authors": "Zhigang Bao", "title": "Tracy-Widom limit for Kendall's tau", "comments": "supplementary material is attached to the end of the paper for the\n  convenience of the reader", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a high-dimensional random matrix model from\nnonparametric statistics called the Kendall rank correlation matrix, which is a\nnatural multivariate extension of the Kendall rank correlation coefficient. We\nestablish the Tracy-Widom law for its largest eigenvalue. It is the first\nTracy-Widom law for a nonparametric random matrix model, and also the first\nTracy-Widom law for a high-dimensional U-statistic.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 03:30:24 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 08:05:22 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 02:44:55 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bao", "Zhigang", ""]]}, {"id": "1712.01158", "submitter": "Mohsen Ahmadi Fahandar", "authors": "Mohsen Ahmadi Fahandar, Eyke H\\\"ullermeier, In\\'es Couso", "title": "Statistical Inference for Incomplete Ranking Data: The Case of\n  Rank-Dependent Coarsening", "comments": "Proceedings of the 34th International Conference on Machine Learning\n  (ICML 2017), 10 pages", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning (ICML 2017), Sydney, Australia, PMLR 70:1078-1087, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of statistical inference for ranking data,\nspecifically rank aggregation, under the assumption that samples are incomplete\nin the sense of not comprising all choice alternatives. In contrast to most\nexisting methods, we explicitly model the process of turning a full ranking\ninto an incomplete one, which we call the coarsening process. To this end, we\npropose the concept of rank-dependent coarsening, which assumes that incomplete\nrankings are produced by projecting a full ranking to a random subset of ranks.\nFor a concrete instantiation of our model, in which full rankings are drawn\nfrom a Plackett-Luce distribution and observations take the form of pairwise\npreferences, we study the performance of various rank aggregation methods. In\naddition to predictive accuracy in the finite sample setting, we address the\ntheoretical question of consistency, by which we mean the ability to recover a\ntarget ranking when the sample size goes to infinity, despite a potential bias\nin the observations caused by the (unknown) coarsening.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 15:49:57 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fahandar", "Mohsen Ahmadi", ""], ["H\u00fcllermeier", "Eyke", ""], ["Couso", "In\u00e9s", ""]]}, {"id": "1712.01293", "submitter": "Thorsten Gl\\\"usenkamp", "authors": "Thorsten Gl\\\"usenkamp", "title": "Probabilistic treatment of the uncertainty from the finite size of\n  weighted Monte Carlo data", "comments": null, "journal-ref": "Eur. Phys. J. Plus (2018) 133: 218", "doi": "10.1140/epjp/i2018-12042-x", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation in HEP experiments often involves Monte-Carlo simulation\nto model the experimental response function. A typical application are\nforward-folding likelihood analyses with re-weighting, or time-consuming\nminimization schemes with a new simulation set for each parameter value.\nProblematically, the finite size of such Monte Carlo samples carries intrinsic\nuncertainty that can lead to a substantial bias in parameter estimation if it\nis neglected and the sample size is small. We introduce a probabilistic\ntreatment of this problem by replacing the usual likelihood functions with\nnovel generalized probability distributions that incorporate the finite\nstatistics via suitable marginalization. These new PDFs are analytic, and can\nbe used to replace the Poisson, multinomial, and sample-based unbinned\nlikelihoods, which covers many use cases in high-energy physics. In the limit\nof infinite statistics, they reduce to the respective standard probability\ndistributions. In the general case of arbitrary Monte Carlo weights, the\nexpressions involve the fourth Lauricella function $F_D$, for which we find a\nnew finite-sum representation in a certain parameter setting. The result also\nrepresents an exact form for Carlson's Dirichlet average $R_n$ with $n>0$, and\nthereby an efficient way to calculate the probability generating function of\nthe Dirichlet-multinomial distribution, the extended divided difference of a\nmonomial, or arbitrary moments of univariate B-splines. We demonstrate the bias\nreduction of our approach with a typical toy Monte Carlo problem, estimating\nthe normalization of a peak in a falling energy spectrum, and compare the\nresults with previously published methods from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 19:00:06 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 16:23:01 GMT"}, {"version": "v3", "created": "Sun, 10 Jun 2018 17:02:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Gl\u00fcsenkamp", "Thorsten", ""]]}, {"id": "1712.01431", "submitter": "Brendan Beare", "authors": "Brendan K. Beare and Alexis Akira Toda", "title": "Determination of Pareto exponents in economic models driven by Markov\n  multiplicative processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article contains new tools for studying the shape of the stationary\ndistribution of sizes in a dynamic economic system in which units experience\nrandom multiplicative shocks and are occasionally reset. Each unit has a\nMarkov-switching type which influences their growth rate and reset probability.\nWe show that the size distribution has a Pareto upper tail, with exponent equal\nto the unique positive solution to an equation involving the spectral radius of\na certain matrix-valued function. We illustrate the use of our results by\napplying them to the wealth distribution in a heterogeneous-agent general\nequilibrium model, in which agents engage in private enterprise and trade\nrisk-free bonds while subject to Markov-switching productivity and mortality\nrisk. A plausible numerical calibration yields a Pareto exponent of 1.39 for\nthe upper tail of the wealth distribution, similar to estimates obtained from\ncross-sectional data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 01:02:17 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 05:50:44 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 02:22:44 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 12:39:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Beare", "Brendan K.", ""], ["Toda", "Alexis Akira", ""]]}, {"id": "1712.01479", "submitter": "Simon Clinet", "authors": "Simon Clinet and Yoann Potiron", "title": "Estimation for high-frequency data under parametric market\n  microstructure noise", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general class of noise-robust estimators based on the existing\nestimators in the non-noisy high-frequency data literature. The microstructure\nnoise is a parametric function of the limit order book. The noise-robust\nestimators are constructed as plug-in versions of their counterparts, where we\nreplace the efficient price, which is non-observable, by an estimator based on\nthe raw price and limit order book data. We show that the technology can be\napplied to five leading examples where, depending on the problem, price\npossibly includes infinite jump activity and sampling times encompass\nasynchronicity and endogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 05:16:15 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 07:35:51 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Clinet", "Simon", ""], ["Potiron", "Yoann", ""]]}, {"id": "1712.01745", "submitter": "Zacharie Naulet", "authors": "Zacharie Naulet, Daniel M. Roy, Ekansh Sharma, Victor Veitch", "title": "Bootstrap estimators for the tail-index and for the count statistics of\n  graphex processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphex processes resolve some pathologies in traditional random graph\nmodels, notably, providing models that are both projective and allow sparsity.\nMost of the literature on graphex processes study them from a probabilistic\npoint of view. Techniques for inferring the parameter of these processes -- the\nso-called \\textit{graphon} -- are still marginal; exceptions are a few papers\nconsidering parametric families of graphons. Nonparametric estimation remains\nunconsidered. In this paper, we propose estimators for a selected choice of\nfunctionals of the graphon. Our estimators originate from the subsampling\ntheory for graphex processes, hence can be seen as a form of bootstrap\nprocedure.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:36:06 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 19:23:10 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 11:40:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Naulet", "Zacharie", ""], ["Roy", "Daniel M.", ""], ["Sharma", "Ekansh", ""], ["Veitch", "Victor", ""]]}, {"id": "1712.01775", "submitter": "Arnak Dalalyan S.", "authors": "Olivier Collier and Arnak Dalalyan", "title": "Estimating linear functionals of a sparse family of Poisson means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that we observe a sample of size n composed of p-dimensional signals,\neach signal having independent entries drawn from a scaled Poisson distribution\nwith an unknown intensity. We are interested in estimating the sum of the n\nunknown intensity vectors, under the assumption that most of them coincide with\na given 'background' signal. The number s of p-dimensional signals different\nfrom the background signal plays the role of sparsity and the goal is to\nleverage this sparsity assumption in order to improve the quality of estimation\nas compared to the naive estimator that computes the sum of the observed\nsignals. We first introduce the group hard thresholding estimator and analyze\nits mean squared error measured by the squared Euclidean norm. We establish a\nnonasymptotic upper bound showing that the risk is at most of the order of\n{\\sigma}^2(sp + s^2sqrt(p)) log^3/2(np). We then establish lower bounds on the\nminimax risk over a properly defined class of collections of s-sparse signals.\nThese lower bounds match with the upper bound, up to logarithmic terms, when\nthe dimension p is fixed or of larger order than s^2. In the case where the\ndimension p increases but remains of smaller order than s^2, our results show a\ngap between the lower and the upper bounds, which can be up to order sqrt(p).\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:27:20 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 22:02:49 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Collier", "Olivier", ""], ["Dalalyan", "Arnak", ""]]}, {"id": "1712.01777", "submitter": "Wei-Kuo Chen", "authors": "Wei-Kuo Chen", "title": "Phase transition in the spiked random tensor with Rademacher prior", "comments": "41 pages, 1 figure, major revision on the abstract and introduction,\n  new references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a deformation from a symmetric Gaussian\nrandom $p$-tensor $(p\\geq 3)$ with a rank-one spike sampled from the Rademacher\nprior. Recently in Lesieur et al. (2017), it was proved that there exists a\ncritical threshold $\\beta_p$ so that when the signal-to-noise ratio exceeds\n$\\beta_p$, one can distinguish the spiked and unspiked tensors and weakly\nrecover the prior via the minimal mean-square-error method. On the other side,\nPerry, Wein, and Bandeira (2017) proved that there exists a $\\beta_p'<\\beta_p$\nsuch that any statistical hypothesis test can not distinguish these two\ntensors, in the sense that their total variation distance asymptotically\nvanishes, when the signa-to-noise ratio is less than $\\beta_p'$. In this work,\nwe show that $\\beta_p$ is indeed the critical threshold that strictly separates\nthe distinguishability and indistinguishability between the two tensors under\nthe total variation distance. Our approach is based on a subtle analysis of the\nhigh temperature behavior of the pure $p$-spin model with Ising spin, arising\ninitially from the field of spin glasses. In particular, we identify the\nsignal-to-noise criticality $\\beta_p$ as the critical temperature,\ndistinguishing the high and low temperature behavior, of the Ising pure\n$p$-spin mean-field spin glass model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 17:29:06 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 18:51:05 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Wei-Kuo", ""]]}, {"id": "1712.01835", "submitter": "Michael Kane", "authors": "Michael Kane", "title": "Percolation Threshold Results on \\Erdos-\\Renyi Graphs: an Empirical\n  Process Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define a directed percolation over Erdos-Renyi graphs and\nderive weak limit results for the resulting stochastic process.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 16:25:17 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Kane", "Michael", ""]]}, {"id": "1712.01934", "submitter": "Oleksandr Zadorozhnyi", "authors": "Gilles Blanchard and Oleksandr Zadorozhnyi", "title": "Concentration of weakly dependent Banach-valued sums and applications to\n  statistical learning methods", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a Bernstein-type inequality for sums of Banach-valued random\nvariables satisfying a weak dependence assumption of general type and under\ncertain smoothness assumptions of the underlying Banach norm. We use this\ninequality in order to investigate in the asymptotical regime the error upper\nbounds for the broad family of spectral regularization methods for reproducing\nkernel decision rules, when trained on a sample coming from a $\\tau-$mixing\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:24:52 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 14:34:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Blanchard", "Gilles", ""], ["Zadorozhnyi", "Oleksandr", ""]]}, {"id": "1712.02009", "submitter": "Adityanand Guntuboyina", "authors": "Sujayam Saha and Adityanand Guntuboyina", "title": "On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:30:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 23:36:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Saha", "Sujayam", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "1712.02043", "submitter": "Kun Liang", "authors": "Peter MacDonald and Kun Liang and Arnold Janssen", "title": "Dynamic adaptive procedures that control the false discovery rate", "comments": "To appear in Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiple testing problem with independent tests, the classical linear\nstep-up procedure controls the false discovery rate (FDR) at level\n$\\pi_0\\alpha$, where $\\pi_0$ is the proportion of true null hypotheses and\n$\\alpha$ is the target FDR level. Adaptive procedures can improve power by\nincorporating estimates of $\\pi_0$, which typically rely on a tuning parameter.\nFixed adaptive procedures set their tuning parameters before seeing the data\nand can be shown to control the FDR in finite samples. We develop theoretical\nresults for dynamic adaptive procedures whose tuning parameters are determined\nby the data. We show that, if the tuning parameter is chosen according to a\nleft-to-right stopping time rule, the corresponding dynamic adaptive procedure\ncontrols the FDR in finite samples. Examples include the recently proposed\nright-boundary procedure and the widely used lowest-slope procedure, among\nothers. Simulation results show that the right-boundary procedure is more\npowerful than other dynamic adaptive procedures under independence and mild\ndependence conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 05:31:42 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 14:54:24 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 16:00:56 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["MacDonald", "Peter", ""], ["Liang", "Kun", ""], ["Janssen", "Arnold", ""]]}, {"id": "1712.02369", "submitter": "Lirong Xue", "authors": "Lirong Xue, Samory Kpotufe", "title": "Achieving the time of $1$-NN, but the accuracy of $k$-NN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple approach which, given distributed computing resources,\ncan nearly achieve the accuracy of $k$-NN prediction, while matching (or\nimproving) the faster prediction time of $1$-NN. The approach consists of\naggregating denoised $1$-NN predictors over a small number of distributed\nsubsamples. We show, both theoretically and experimentally, that small\nsubsample sizes suffice to attain similar performance as $k$-NN, without\nsacrificing the computational efficiency of $1$-NN.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:02:00 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 01:47:29 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Xue", "Lirong", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1712.02379", "submitter": "Todd Kuffner", "authors": "Liang Hong and Todd A. Kuffner and Ryan Martin", "title": "On overfitting and post-selection uncertainty assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression context, when the relevant subset of explanatory variables is\nuncertain, it is common to use a data-driven model selection procedure.\nClassical linear model theory, applied naively to the selected sub-model, may\nnot be valid because it ignores the selected sub-model's dependence on the\ndata. We provide an explanation of this phenomenon, in terms of overfitting,\nfor a class of model selection criteria.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:14:30 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Hong", "Liang", ""], ["Kuffner", "Todd A.", ""], ["Martin", "Ryan", ""]]}, {"id": "1712.02445", "submitter": "Minerva Mukhopadhyay", "authors": "Minerva Mukhopadhyay and David B. Dunson", "title": "Targeted Random Projection for Prediction from High-Dimensional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computationally-efficient prediction from high\ndimensional and highly correlated predictors in challenging settings where\naccurate variable selection is effectively impossible. Direct application of\npenalization or Bayesian methods implemented with Markov chain Monte Carlo can\nbe computationally daunting and unstable. Hence, some type of dimensionality\nreduction prior to statistical analysis is in order. Common solutions include\napplication of screening algorithms to reduce the regressors, or dimension\nreduction using projections of the design matrix. The former approach can be\nhighly sensitive to threshold choice in finite samples, while the later can\nhave poor performance in very high-dimensional settings. We propose a TArgeted\nRandom Projection (TARP) approach that combines positive aspects of both\nstrategies to boost performance. In particular, we propose to use information\nfrom independent screening to order the inclusion probabilities of the features\nin the projection matrix used for dimension reduction, leading to data-informed\nsparsity. We provide theoretical support for a Bayesian predictive algorithm\nbased on TARP, including both statistical and computational complexity\nguarantees. Examples for simulated and real data applications illustrate gains\nrelative to a variety of competitors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 23:42:51 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Dunson", "David B.", ""]]}, {"id": "1712.02469", "submitter": "Chunlin Wang", "authors": "Chunlin Wang, Paul Marriott and Pengfei Li", "title": "Asymptotic coverage probabilities of bootstrap percentile confidence\n  intervals for constrained parameters", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behaviour of the commonly used bootstrap percentile confidence\ninterval is investigated when the parameters are subject to linear inequality\nconstraints. We concentrate on the important one- and two-sample problems with\ndata generated from general parametric distributions in the natural exponential\nfamily. The focus of this paper is on quantifying the coverage probabilities of\nthe parametric bootstrap percentile confidence intervals, in particular their\nlimiting behaviour near boundaries. We propose a local asymptotic framework to\nstudy this subtle coverage behaviour. Under this framework, we discover that\nwhen the true parameters are on, or close to, the restriction boundary, the\nasymptotic coverage probabilities can always exceed the nominal level in the\none-sample case; however, they can be, remarkably, both under and over the\nnominal level in the two-sample case. Using illustrative examples, we show that\nthe results provide theoretical justification and guidance on applying the\nbootstrap percentile method to constrained inference problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 01:51:35 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Chunlin", ""], ["Marriott", "Paul", ""], ["Li", "Pengfei", ""]]}, {"id": "1712.02519", "submitter": "Chao Gao", "authors": "Fengshuo Zhang and Chao Gao", "title": "Convergence Rates of Variational Posterior Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence rates of variational posterior distributions for\nnonparametric and high-dimensional inference. We formulate general conditions\non prior, likelihood, and variational class that characterize the convergence\nrates. Under similar \"prior mass and testing\" conditions considered in the\nliterature, the rate is found to be the sum of two terms. The first term stands\nfor the convergence rate of the true posterior distribution, and the second\nterm is contributed by the variational approximation error. For a class of\npriors that admit the structure of a mixture of product measures, we propose a\nnovel prior mass condition, under which the variational approximation error of\nthe mean-field class is dominated by convergence rate of the true posterior. We\ndemonstrate the applicability of our general results for various models, prior\ndistributions and variational classes by deriving convergence rates of the\ncorresponding variational posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 07:30:16 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 02:15:27 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 17:38:55 GMT"}, {"version": "v4", "created": "Mon, 17 Jun 2019 04:44:38 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Zhang", "Fengshuo", ""], ["Gao", "Chao", ""]]}, {"id": "1712.02685", "submitter": "Natalie Neumeyer", "authors": "Natalie Neumeyer and Ingrid Van Keilegom", "title": "Bootstrap of residual processes in regression: to smooth or not to\n  smooth ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a location model of the form $Y = m(X) +\n\\varepsilon$, where $m(\\cdot)$ is the unknown regression function, the error\n$\\varepsilon$ is independent of the $p$-dimensional covariate $X$ and\n$E(\\varepsilon)=0$. Given i.i.d. data $(X_1,Y_1),\\ldots,(X_n,Y_n)$ and given an\nestimator $\\hat m(\\cdot)$ of the function $m(\\cdot)$ (which can be parametric\nor nonparametric of nature), we estimate the distribution of the error term\n$\\varepsilon$ by the empirical distribution of the residuals $Y_i-\\hat m(X_i)$,\n$i=1,\\ldots,n$. To approximate the distribution of this estimator, Koul and\nLahiri (1994) and Neumeyer (2008, 2009) proposed bootstrap procedures, based on\nsmoothing the residuals either before or after drawing bootstrap samples. So\nfar it has been an open question whether a classical non-smooth residual\nbootstrap is asymptotically valid in this context. In this paper we solve this\nopen problem, and show that the non-smooth residual bootstrap is consistent. We\nillustrate this theoretical result by means of simulations, that show the\naccuracy of this bootstrap procedure for various models, testing procedures and\nsample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:52:00 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1712.02747", "submitter": "Olivier Catoni", "authors": "Olivier Catoni, Ilaria Giulini", "title": "Dimension-free PAC-Bayesian bounds for matrices, vectors, and linear\n  least squares regression", "comments": "Version 1 needed some further proofreading", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on dimension-free PAC-Bayesian bounds, under weak\npolynomial moment assumptions, allowing for heavy tailed sample distributions.\nIt covers the estimation of the mean of a vector or a matrix, with applications\nto least squares linear regression. Special efforts are devoted to the\nestimation of Gram matrices, due to their prominent role in high-dimension data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:44:42 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 18:19:41 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Catoni", "Olivier", ""], ["Giulini", "Ilaria", ""]]}, {"id": "1712.02990", "submitter": "Veronique Maume-Deschamps", "authors": "Abdul-Fattah Abu-Awwad (ICJ), V\\'eronique Maume-Deschamps (ICJ),\n  Ribereau Pierre (ICJ)", "title": "Censored pairwise likelihood-based tests for mixing coefficient of\n  spatial max-mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-mixture processes are defined as Z = max(aX, (1 -- a)Y) with X an\nasymptotic dependent (AD) process, Y an asymptotic independent (AI) process and\na $\\in$ [0, 1]. So that, the mixing coefficient a may reveal the strength of\nthe AD part present in the max-mixture process. In this paper we focus on two\ntests based on censored pairwise likelihood estimates. We compare their\nperformance through an extensive simulation study. Monte Carlo simulation plays\na fundamental tool for asymptotic variance calculations. We apply our tests to\ndaily precipitations from the East of Australia. Drawbacks and possible\ndevelopments are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 09:29:44 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Abu-Awwad", "Abdul-Fattah", "", "ICJ"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ"], ["Pierre", "Ribereau", "", "ICJ"]]}, {"id": "1712.03058", "submitter": "Theresa Stocks", "authors": "Theresa Stocks", "title": "Iterated filtering methods for Markov process epidemic models", "comments": "This manuscript is a preprint of a chapter to appear in the Handbook\n  of Infectious Disease Data Analysis, Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman \\& Hall/CRC, 2018. Please use the book for\n  possible citations. Corrected typo in the references and modified second\n  example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic epidemic models have proven valuable for public health decision\nmakers as they provide useful insights into the understanding and prevention of\ninfectious diseases. However, inference for these types of models can be\ndifficult because the disease spread is typically only partially observed e.g.\nin form of reported incidences in given time periods. This chapter discusses\nhow to perform likelihood-based inference for partially observed Markov\nepidemic models when it is relatively easy to generate samples from the Markov\ntransmission model while the likelihood function is intractable. The first part\nof the chapter reviews the theoretical background of inference for partially\nobserved Markov processes (POMP) via iterated filtering. In the second part of\nthe chapter the performance of the method and associated practical difficulties\nare illustrated on two examples. In the first example a simulated outbreak data\nset consisting of the number of newly reported cases aggregated by week is\nfitted to a POMP where the underlying disease transmission model is assumed to\nbe a simple Markovian SIR model. The second example illustrates possible model\nextensions such as seasonal forcing and over-dispersion in both, the\ntransmission and observation model, which can be used, e.g., when analysing\nroutinely collected rotavirus surveillance data. Both examples are implemented\nusing the R-package pomp (King et al., 2016) and the code is made available\nonline.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 13:44:34 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 09:08:34 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 10:18:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Stocks", "Theresa", ""]]}, {"id": "1712.03278", "submitter": "Milad Bakhshizadeh", "authors": "Milad Bakhshizadeh, Arian Maleki, Shirin Jalali", "title": "Using Black-box Compression Algorithms for Phase Retrieval", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive phase retrieval refers to the problem of recovering a structured\n$n$-dimensional complex-valued vector from its phase-less under-determined\nlinear measurements. The non-linearity of measurements makes designing\ntheoretically-analyzable efficient phase retrieval algorithms challenging. As a\nresult, to a great extent, algorithms designed in this area are developed to\ntake advantage of simple structures such as sparsity and its convex\ngeneralizations. The goal of this paper is to move beyond simple models through\nemploying compression codes. Such codes are typically developed to take\nadvantage of complex signal models to represent the signals as efficiently as\npossible. In this work, it is shown how an existing compression code can be\ntreated as a black box and integrated into an efficient solution for phase\nretrieval. First, COmpressive PhasE Retrieval (COPER) optimization, a\ncomputationally-intensive compression-based phase retrieval method, is\nproposed. COPER provides a theoretical framework for studying compression-based\nphase retrieval. The number of measurements required by COPER is connected to\n$\\kappa$, the $\\alpha$-dimension (closely related to the rate-distortion\ndimension) of the given family of compression codes. To finds the solution of\nCOPER, an efficient iterative algorithm called gradient descent for COPER\n(GD-COPER) is proposed. It is proven that under some mild conditions on the\ninitialization, if the number of measurements is larger than $ C \\kappa^2\n\\log^2 n$, where $C$ is a constant, GD-COPER obtains an accurate estimate of\nthe input vector in polynomial time. In the simulation results, JPEG2000 is\nintegrated in GD-COPER to confirm the superb performance of the resulting\nalgorithm on real-world images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 20:46:46 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 21:33:19 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 19:28:25 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bakhshizadeh", "Milad", ""], ["Maleki", "Arian", ""], ["Jalali", "Shirin", ""]]}, {"id": "1712.03299", "submitter": "Andres Christen", "authors": "J. Andr\\'es Christen, Marcos A. Capistr\\'an, M. Luisa Daza-Torres,\n  Hugo Flores-Arg\\\"uedas and J. Cricelio Montesinos-L\\'opez", "title": "Posterior distribution existence and error control in Banach spaces in\n  the Bayesian approach to UQ in inverse problems", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the results of \\cite{Capistran2016} on expected Bayes factors\n(BF) to control the numerical error in the posterior distribution to an\ninfinite dimensional setting when considering Banach functional spaces and now\nin a prior setting. The main result is a bound on the absolute global error to\nbe tolerated by the Forward Map numerical solver, to keep the BF of the\nnumerical vs. the theoretical model near to 1, now in this more general\nsetting, possibly including a truncated, finite dimensional approximate prior\nmeasure. In so doing we found a far more general setting to define and prove\nexistence of the infinite dimensional posterior distribution than that depicted\nin, for example, \\cite{Stuart2010}. Discretization consistency and rates of\nconvergence are also investigated in this general setting for the Bayesian\ninverse problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 22:30:01 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 15:49:46 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 16:29:37 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Christen", "J. Andr\u00e9s", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Daza-Torres", "M. Luisa", ""], ["Flores-Arg\u00fcedas", "Hugo", ""], ["Montesinos-L\u00f3pez", "J. Cricelio", ""]]}, {"id": "1712.03305", "submitter": "Dennis Leung", "authors": "Weidong Liu, Dennis Leung, Qiman Shao", "title": "False Discovery Control for Pairwise Comparisons - An Asymptotic\n  Solution to Williams, Jones and Tukey's Conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under weak moment and asymptotic conditions, we offer an affirmative answer\nto whether the BH procedure (Benjamini and Hochberg, 1995) can control the\nfalse discovery rate in testing pairwise comparisons of means under a one-way\nANOVA layout. Specifically, despite the fact that the two sample t-statistics\ndo not exhibit positive regression dependency (Benjamini and Yekutieli, 2001),\nour result shows that the BH procedure can asymptotically control the\ndirectional false discovery rate as conjectured by Williams, Jones, and Tukey\n(1999). Such a result is useful for most general situations when the number of\nvariables is moderately large and/or when idealistic assumptions such as\nnormality and a balanced design are violated.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 22:56:40 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Liu", "Weidong", ""], ["Leung", "Dennis", ""], ["Shao", "Qiman", ""]]}, {"id": "1712.03317", "submitter": "Junyong Park", "authors": "Junyong Park", "title": "Testing homogeneity of proportions from sparse binomial data with a\n  large number of groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider testing the homogeneity for proportions in\nindependent binomial distributions especially when data are sparse for large\nnumber of groups. We provide broad aspects of our proposed tests such as\ntheoretical studies, simulations and real data application. We present the\nasymptotic null distributions and asymptotic powers for our proposed tests and\ncompare their performance with existing tests. Our simulation studies show that\nnone of tests dominate the others, however our proposed test and a few tests\nare expected to control given sizes and obtain significant powers. We also\npresent a real example regarding safety concerns associated with Avandiar\n(rosiglitazone) in Nissen and Wolsky (2007).\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 00:30:01 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 01:33:49 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 18:26:09 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Park", "Junyong", ""]]}, {"id": "1712.03358", "submitter": "Kayanan Manickavasagar", "authors": "Manickavasagar Kayanan and Pushpakanthie Wijekoon", "title": "Stochastic Restricted Biased Estimators in misspecified regression model\n  with incomplete prior information", "comments": "35 Pages, 6 Figures", "journal-ref": "Kayanan, M., & Wijekoon, P. (2018). Stochastic Restricted Biased\n  Estimators in Misspecified Regression Model with Incomplete Prior\n  Information. Journal of Probability and Statistics, 8.\n  doi:10.1155/2018/1452181", "doi": "10.1155/2018/1452181", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the analysis of misspecification was extended to the\nrecently introduced stochastic restricted biased estimators when\nmulticollinearity exists among the explanatory variables. The Stochastic\nRestricted Ridge Estimator (SRRE), Stochastic Restricted Almost Unbiased Ridge\nEstimator (SRAURE), Stochastic Restricted Liu Estimator (SRLE), Stochastic\nRestricted Almost Unbiased Liu Estimator (SRAULE), Stochastic Restricted\nPrincipal Component Regression Estimator (SRPCR), Stochastic Restricted r-k\nclass estimator (SRrk) and Stochastic Restricted r-d class estimator (SRrd)\nwere examined in the misspecified regression model due to missing relevant\nexplanatory variables when incomplete prior information of the regression\ncoefficients is available. Further, the superiority conditions between\nestimators and their respective predictors were obtained in the mean square\nerror matrix (MSEM) sense. Finally, a numerical example and a Monte Carlo\nsimulation study were used to illustrate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 08:59:00 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 19:40:27 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Kayanan", "Manickavasagar", ""], ["Wijekoon", "Pushpakanthie", ""]]}, {"id": "1712.03412", "submitter": "Huiming Zhang", "authors": "Huiming Zhang, Jinzhu Jia", "title": "Elastic-net Regularized High-dimensional Negative Binomial Regression:\n  Consistency and Weak Signals Detection", "comments": "27 pages", "journal-ref": "Statistica Sinica,32(1),1-27 (2022)", "doi": "10.5705/ss.202019.0315", "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a sparse negative binomial regression (NBR) for count data by\nshowing the non-asymptotic advantages of using the elastic-net estimator. Two\ntypes of oracle inequalities are derived for the NBR's elastic-net estimates by\nusing the Compatibility Factor Condition and the Stabil Condition. The second\ntype of oracle inequality is for the random design and can be extended to many\n$\\ell_1 + \\ell_2$ regularized M-estimations, with the corresponding empirical\nprocess having stochastic Lipschitz properties. We derive the concentration\ninequality for the suprema empirical processes for the weighted sum of negative\nbinomial variables to show some high--probability events. We apply the method\nby showing the sign consistency, provided that the nonzero components in the\ntrue sparse vector are larger than a proper choice of the weakest signal\ndetection threshold. In the second application, we show the grouping effect\ninequality with high probability. Third, under some assumptions for a design\nmatrix, we can recover the true variable set with a high probability if the\nweakest signal detection threshold is large than the turning parameter up to a\nknown constant. Lastly, we briefly discuss the de-biased elastic-net estimator,\nand numerical studies are given to support the proposal.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 17:08:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 17:19:01 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 17:58:50 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 02:07:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Huiming", ""], ["Jia", "Jinzhu", ""]]}, {"id": "1712.03522", "submitter": "Igor Silin", "authors": "Igor Silin", "title": "Finite sample Bernstein-von Mises theorems for functionals and spectral\n  projectors of the covariance matrix", "comments": "32 pages, submitted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a prior influence on the posterior distribution of\ncovariance matrix vanishes as sample size grows. The assumptions on a prior are\nexplicit and mild. The results are valid for a finite sample and admit the\ndimension $p$ growing with the sample size $n$. We exploit the described fact\nto derive the finite sample Bernstein-von Mises theorem for functionals of\ncovariance matrix (e.g. eigenvalues) and to find the posterior distribution of\nthe Frobenius distance between spectral projector and empirical spectral\nprojector. This can be useful for constructing sharp confidence sets for the\ntrue value of the functional or for the true spectral projector.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 13:18:59 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 21:33:11 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Silin", "Igor", ""]]}, {"id": "1712.03610", "submitter": "Ting-Kam Leonard Wong", "authors": "Ting-Kam Leonard Wong", "title": "Logarithmic divergences from optimal transport and R\\'enyi geometry", "comments": "39 pages. Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergences, also known as contrast functions, are distance-like quantities\ndefined on manifolds of non-negative or probability measures. Using the duality\nin optimal transport, we introduce and study the one-parameter family of\n$L^{(\\pm \\alpha)}$-divergences. It includes the Bregman divergence\ncorresponding to the Euclidean quadratic cost, and the $L$-divergence\nintroduced by Pal and the author in connection with portfolio theory and a\nlogarithmic cost function. They admit natural generalizations of exponential\nfamily that are closely related to the $\\alpha$-family and $q$-exponential\nfamily. In particular, the $L^{(\\pm \\alpha)}$-divergences of the corresponding\npotential functions are R\\'{e}nyi divergences. Using this unified framework we\nprove that the induced geometries are dually projectively flat with constant\nsectional curvatures, and a generalized Pythagorean theorem holds true.\nConversely, we show that if a statistical manifold is dually projectively flat\nwith constant curvature $\\pm \\alpha$ with $\\alpha > 0$, then it is locally\ninduced by an $L^{(\\mp \\alpha)}$-divergence. We define in this context a\ncanonical divergence which extends the one for dually flat manifolds.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 23:42:29 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 23:35:27 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 01:52:46 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wong", "Ting-Kam Leonard", ""]]}, {"id": "1712.03848", "submitter": "Ryan Martin", "authors": "Chang Liu and Ryan Martin and Weining Shen", "title": "Empirical priors and posterior concentration in a piecewise polynomial\n  sequence model", "comments": "32 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference on high-dimensional parameters in structured linear models is an\nimportant statistical problem. This paper focuses on the case of a piecewise\npolynomial Gaussian sequence model, and we develop a new empirical Bayes\nsolution that enjoys adaptive minimax posterior concentration rates and\nimproved structure learning properties compared to existing methods. Moreover,\nthanks to the conjugate form of the empirical prior, posterior computations are\nfast and easy. Numerical examples also highlight the method's strong\nfinite-sample performance compared to existing methods across a range of\ndifferent scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:49:42 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 15:52:12 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 14:50:16 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Liu", "Chang", ""], ["Martin", "Ryan", ""], ["Shen", "Weining", ""]]}, {"id": "1712.03852", "submitter": "Ryan Martin", "authors": "Minwoo Chae and Ryan Martin and Stephen G. Walker", "title": "Fast nonparametric near-maximum likelihood estimation of a mixing\n  density", "comments": "11 pages, 3 figures", "journal-ref": "Statistics & Probability Letters, 2018, volume 140, pages 142--146", "doi": "10.1016/j.spl.2018.05.012", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are regularly used in density estimation applications, but the\nproblem of estimating the mixing distribution remains a challenge.\nNonparametric maximum likelihood produce estimates of the mixing distribution\nthat are discrete, and these may be hard to interpret when the true mixing\ndistribution is believed to have a smooth density. In this paper, we\ninvestigate an algorithm that produces a sequence of smooth estimates that has\nbeen conjectured to converge to the nonparametric maximum likelihood estimator.\nHere we give a rigorous proof of this conjecture, and propose a new data-driven\nstopping rule that produces smooth near-maximum likelihood estimates of the\nmixing density, and simulations demonstrate the quality empirical performance\nof this estimator.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:55:18 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chae", "Minwoo", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1712.04106", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan", "title": "Sparse Phase Retrieval via Sparse PCA Despite Model Misspecification: A\n  Simplified and Extended Analysis", "comments": "Edited formatting for abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional misspecified phase retrieval.\nThis is where we have an $s$-sparse signal vector $\\mathbf{x}_*$ in\n$\\mathbb{R}^n$, which we wish to recover using sampling vectors\n$\\textbf{a}_1,\\ldots,\\textbf{a}_m$, and measurements $y_1,\\ldots,y_m$, which\nare related by the equation $f(\\left<\\textbf{a}_i,\\textbf{x}_*\\right>) = y_i$.\nHere, $f$ is an unknown link function satisfying a positive correlation with\nthe quadratic function. This problem was analyzed in a recent paper by Neykov,\nWang and Liu, who provided recovery guarantees for a two-stage algorithm with\nsample complexity $m = O(s^2\\log n)$. In this paper, we show that the first\nstage of their algorithm suffices for signal recovery with the same sample\ncomplexity, and extend the analysis to non-Gaussian measurements. Furthermore,\nwe show how the algorithm can be generalized to recover a signal vector\n$\\textbf{x}_*$ efficiently given geometric prior information other than\nsparsity.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 02:59:05 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 04:29:11 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Tan", "Yan Shuo", ""]]}, {"id": "1712.04404", "submitter": "Aline Marguet", "authors": "Marc Hoffmann and Aline Marguet", "title": "Statistical estimation in a randomly structured branching population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary branching process structured by a stochastic trait that\nevolves according to a diffusion process that triggers the branching events, in\nthe spirit of Kimmel's model of cell division with parasite infection. Based on\nthe observation of the trait at birth of the first n generations of the\nprocess, we construct nonparametric estimator of the transition of the\nassociated bifurcating chain and study the parametric estimation of the\nbranching rate. In the limit, as n tends to infinity, we obtain asymptotic\nefficiency in the parametric case and minimax optimality in the nonparametric\ncase.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 17:41:45 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 10:18:56 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 21:06:17 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Hoffmann", "Marc", ""], ["Marguet", "Aline", ""]]}, {"id": "1712.04487", "submitter": "Steve Huntsman", "authors": "Steve Huntsman", "title": "Topological mixture estimation", "comments": "10 pages, 10 figures, accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.AT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density functions that represent sample data are often multimodal, i.e. they\nexhibit more than one maximum. Typically this behavior is taken to indicate\nthat the underlying data deserves a more detailed representation as a mixture\nof densities with individually simpler structure. The usual specification of a\ncomponent density is quite restrictive, with log-concave the most general case\nconsidered in the literature, and Gaussian the overwhelmingly typical case. It\nis also necessary to determine the number of mixture components \\emph{a\npriori}, and much art is devoted to this. Here, we introduce \\emph{topological\nmixture estimation}, a completely nonparametric and computationally efficient\nsolution to the one-dimensional problem where mixture components need only be\nunimodal. We repeatedly perturb the unimodal decomposition of Baryshnikov and\nGhrist to produce a topologically and information-theoretically optimal\nunimodal mixture. We also detail a smoothing process that optimally exploits\ntopological persistence of the unimodal category in a natural way when working\ndirectly with sample data. Finally, we illustrate these techniques through\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 19:51:23 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 19:16:47 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Huntsman", "Steve", ""]]}, {"id": "1712.04730", "submitter": "Francesca Fortunato", "authors": "Francesca Fortunato", "title": "Limit theorems for the Multiplicative Binomial Distribution (MBD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sum of $n$ {non-independent} Bernoulli random variables could be modeled\nin several different ways. One of these is the Multiplicative Binomial\nDistribution (MBD), introduced by Altham (1978) and revised by Lovison (1998).\nIn this work, we focus on the distribution asymptotic behavior as its\nparameters diverge. In addition, we derive a specific property describing the\nrelationship between the joint probability of success of $n$ binary-dependent\nresponses and the individual Bernoulli one; particularly, we prove that it\ndepends on both the sign and the strength of the association between the random\nvariables.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 12:19:36 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 21:55:07 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Fortunato", "Francesca", ""]]}, {"id": "1712.04802", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Mert Demirer, Esther Duflo, and Iv\\'an\n  Fern\\'andez-Val", "title": "Generic Machine Learning Inference on Heterogenous Treatment Effects in\n  Randomized Experiments", "comments": "52 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose strategies to estimate and make inference on key features of\nheterogeneous effects in randomized experiments. These key features include\nbest linear predictors of the effects on machine learning proxies, average\neffects sorted by impact groups, and average characteristics of most and least\nimpacted units. The approach is valid in high dimensional settings, where the\neffects are proxied by machine learning methods. We post-process these proxies\ninto the estimates of the key features. Our approach is generic, it can be used\nin conjunction with penalized methods, deep and shallow neural networks,\ncanonical and new random forests, boosted trees, and ensemble methods.\nEstimation and inference are based on repeated data splitting to avoid\noverfitting and achieve validity. For inference, we take medians of p-values\nand medians of confidence intervals, resulting from many different data splits,\nand then adjust their nominal level to guarantee uniform validity. This\nvariational inference method, which quantifies the uncertainty coming from both\nparameter estimation and data splitting, is shown to be uniformly valid for a\nlarge class of data generating processes. We illustrate the use of the approach\nwith a randomized field experiment that evaluated a combination of nudges to\nstimulate demand for immunization in India.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:47:57 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 18:23:16 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 01:40:23 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 13:34:05 GMT"}, {"version": "v5", "created": "Mon, 28 Dec 2020 09:05:55 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Demirer", "Mert", ""], ["Duflo", "Esther", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "1712.04903", "submitter": "Tom Leinster", "authors": "Tom Leinster", "title": "A short characterization of relative entropy", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove characterization theorems for relative entropy (also known as\nKullback-Leibler divergence), q-logarithmic entropy (also known as Tsallis\nentropy), and q-logarithmic relative entropy. All three have been characterized\naxiomatically before, but we show that earlier proofs can be simplified\nconsiderably, at the same time relaxing some of the hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:14:52 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Leinster", "Tom", ""]]}, {"id": "1712.04912", "submitter": "Xinkun Nie", "authors": "Xinkun Nie, Stefan Wager", "title": "Quasi-Oracle Estimation of Heterogeneous Treatment Effects", "comments": "Biometrika, forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible estimation of heterogeneous treatment effects lies at the heart of\nmany statistical challenges, such as personalized medicine and optimal resource\nallocation. In this paper, we develop a general class of two-step algorithms\nfor heterogeneous treatment effect estimation in observational studies. We\nfirst estimate marginal effects and treatment propensities in order to form an\nobjective function that isolates the causal component of the signal. Then, we\noptimize this data-adaptive objective function. Our approach has several\nadvantages over existing methods. From a practical perspective, our method is\nflexible and easy to use: In both steps, we can use any loss-minimization\nmethod, e.g., penalized regression, deep neural networks, or boosting;\nmoreover, these methods can be fine-tuned by cross validation. Meanwhile, in\nthe case of penalized kernel regression, we show that our method has a\nquasi-oracle property: Even if the pilot estimates for marginal effects and\ntreatment propensities are not particularly accurate, we achieve the same error\nbounds as an oracle who has a priori knowledge of these two nuisance\ncomponents. We implement variants of our approach based on penalized\nregression, kernel ridge regression, and boosting in a variety of simulation\nsetups, and find promising performance relative to existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 18:32:13 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:11:54 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 16:51:33 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 06:31:20 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Nie", "Xinkun", ""], ["Wager", "Stefan", ""]]}, {"id": "1712.05035", "submitter": "Zvi Rosen", "authors": "Zvi Rosen, Anand Bhaskar, Sebastien Roch, Yun S. Song", "title": "Geometry of the sample frequency spectrum and the perils of demographic\n  inference", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample frequency spectrum (SFS), which describes the distribution of\nmutant alleles in a sample of DNA sequences, is a widely used summary statistic\nin population genetics. The expected SFS has a strong dependence on the\nhistorical population demography and this property is exploited by popular\nstatistical methods to infer complex demographic histories from DNA sequence\ndata. Most, if not all, of these inference methods exhibit pathological\nbehavior, however. Specifically, they often display runaway behavior in\noptimization, where the inferred population sizes and epoch durations can\ndegenerate to 0 or diverge to infinity, and show undesirable sensitivity of the\ninferred demography to perturbations in the data. The goal of this paper is to\nprovide theoretical insights into why such problems arise. To this end, we\ncharacterize the geometry of the expected SFS for piecewise-constant\ndemographic histories and use our results to show that the aforementioned\npathological behavior of popular inference methods is intrinsic to the geometry\nof the expected SFS. We provide explicit descriptions and visualizations for a\ntoy model with sample size 4, and generalize our intuition to arbitrary sample\nsizes n using tools from convex and algebraic geometry. We also develop a\nuniversal characterization result which shows that the expected SFS of a sample\nof size n under an arbitrary population history can be recapitulated by a\npiecewise-constant demography with only k(n) epochs, where k(n) is between n/2\nand 2n-1. The set of expected SFS for piecewise-constant demographies with\nfewer than k(n) epochs is open and non-convex, which causes the above phenomena\nfor inference from data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 22:52:21 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Rosen", "Zvi", ""], ["Bhaskar", "Anand", ""], ["Roch", "Sebastien", ""], ["Song", "Yun S.", ""]]}, {"id": "1712.05066", "submitter": "H\\'ector Araya", "authors": "H\\'ector Araya and Natalia Bahamonde and Tania Roa and Soledad Torres", "title": "Statistical Inference in Fractional Poisson Ornstein-Uhlenbeck Process", "comments": "18 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the problem of parameter estimation for a discrete\nOrnstein - Uhlenbeck model driven by Poisson fractional noise. Based on random\nwalk approximation for the noise, we study least squares and maximum likelihood\nestimators. Thus, asymptotic behaviours of the estimator is carried out, and a\nsimulation study is shown to illustrate our results.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 01:13:24 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Araya", "H\u00e9ctor", ""], ["Bahamonde", "Natalia", ""], ["Roa", "Tania", ""], ["Torres", "Soledad", ""]]}, {"id": "1712.05279", "submitter": "Johanna F. Ziegel", "authors": "Ingo Steinwart, Johanna F. Ziegel", "title": "Strictly proper kernel scores and characteristic kernels on compact\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strictly proper kernel scores are well-known tool in probabilistic\nforecasting, while characteristic kernels have been extensively investigated in\nthe machine learning literature. We first show that both notions coincide, so\nthat insights from one part of the literature can be used in the other. We then\nshow that the metric induced by a characteristic kernel cannot reliably\ndistinguish between distributions that are far apart in the total variation\nnorm as soon as the underlying space of measures is infinite dimensional. In\naddition, we provide a characterization of characteristic kernels in terms of\neigenvalues and -functions and apply this characterization to the case of\ncontinuous kernels on (locally) compact spaces. In the compact case we further\nshow that characteristic kernels exist if and only if the space is metrizable.\nAs special cases of our general theory we investigate translation-invariant\nkernels on compact Abelian groups and isotropic kernels on spheres. The latter\nare of particular interest for forecast evaluation of probabilistic predictions\non spherical domains as frequently encountered in meteorology and climatology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 15:18:29 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Steinwart", "Ingo", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1712.05311", "submitter": "John T. Whelan", "authors": "John T. Whelan", "title": "Prior Distributions for the Bradley-Terry Model of Paired Comparisons", "comments": "22 pages, 3 figures, formatted for submission to the Electronic\n  Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bradley-Terry model assigns probabilities for the outcome of paired\ncomparison experiments based on strength parameters associated with the objects\nbeing compared. We consider different proposed choices of prior parameter\ndistributions for Bayesian inference of the strength parameters based on the\npaired comparison results. We evaluate them according to four desiderata\nmotivated by the use of inferred Bradley-Terry parameters to rate teams on the\nbasis of outcomes of a set of games: invariance under interchange of teams,\ninvariance under interchange of winning and losing, normalizability and\ninvariance under elimination of teams. We consider various proposals which fail\nto satisfy one or more of these desiderata, and illustrate two proposals which\nsatisfy them. Both are one-parameter independent distributions for the\nlogarithms of the team strengths: 1) Gaussian and 2) Type III generalized\nlogistic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 16:08:54 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Whelan", "John T.", ""]]}, {"id": "1712.05495", "submitter": "Arnak Dalalyan S.", "authors": "Olivier Collier, Arnak S. Dalalyan", "title": "Minimax estimation of a p-dimensional linear functional in sparse\n  Gaussian models and robust estimation of the mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two problems of estimation in high-dimensional Gaussian models.\nThe first problem is that of estimating a linear functional of the means of $n$\nindependent $p$-dimensional Gaussian vectors, under the assumption that most of\nthese means are equal to zero. We show that, up to a logarithmic factor, the\nminimax rate of estimation in squared Euclidean norm is between $(s^2\\wedge n)\n+sp$ and $(s^2\\wedge np)+sp$. The estimator that attains the upper bound being\ncomputationally demanding, we investigate suitable versions of group\nthresholding estimators that are efficiently computable even when the dimension\nand the sample size are very large. An interesting new phenomenon revealed by\nthis investigation is that the group thresholding leads to a substantial\nimprovement in the rate as compared to the element-wise thresholding. Thus, the\nrate of the group thresholding is $s^2\\sqrt{p}+sp$, while the element-wise\nthresholding has an error of order $s^2p+sp$. To the best of our knowledge,\nthis is the first known setting in which leveraging the group structure leads\nto a polynomial improvement in the rate.\n  The second problem studied in this work is the estimation of the common\n$p$-dimensional mean of the inliers among $n$ independent Gaussian vectors. We\nshow that there is a strong analogy between this problem and the first one.\nExploiting it, we propose new strategies of robust estimation that are\ncomputationally tractable and have better rates of convergence than the other\ncomputationally tractable robust (with respect to the presence of the outliers\nin the data) estimators studied in the literature. However, this tractability\ncomes with a loss of the minimax-rate-optimality in some regimes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 01:16:05 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 18:35:10 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 09:42:58 GMT"}, {"version": "v4", "created": "Thu, 8 Nov 2018 20:59:49 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Collier", "Olivier", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1712.05593", "submitter": "Piet Groeneboom", "authors": "Fadoua Balabdaoui, Piet Groeneboom and Kim Hendrickx", "title": "Score estimation in the monotone single index model", "comments": "31 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1111/sjos.12361", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation in the single index model where the link function is\nmonotone. For this model a profile least squares estimator has been proposed to\nestimate the unknown link function and index. Although it is natural to propose\nthis procedure, it is still unknown whether it produces index estimates which\nconverge at the parametric rate. We show that this holds if we solve a score\nequation corresponding to this least squares problem. Using a Lagrangian\nformulation, we show how one can solve this score equation without any\nreparametrization. This makes it easy to solve the score equations in high\ndimensions. We also compare our method with the Effective Dimension Reduction\n(EDR) and the Penalized Least Squares Estimator (PLSE) methods, both available\non CRAN as R packages, and compare with link-free methods, where the covariates\nare ellipticallly symmetric.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 09:40:22 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 23:51:17 GMT"}, {"version": "v3", "created": "Wed, 9 May 2018 12:59:47 GMT"}, {"version": "v4", "created": "Tue, 23 Oct 2018 15:18:45 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Groeneboom", "Piet", ""], ["Hendrickx", "Kim", ""]]}, {"id": "1712.05630", "submitter": "Richard Samworth", "authors": "Milana Gataric, Tengyao Wang and Richard J. Samworth", "title": "Sparse principal component analysis via axis-aligned random projections", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for sparse principal component analysis, based on\nthe aggregation of eigenvector information from carefully-selected axis-aligned\nrandom projections of the sample covariance matrix. Unlike most alternative\napproaches, our algorithm is non-iterative, so is not vulnerable to a bad\nchoice of initialisation. We provide theoretical guarantees under which our\nprincipal subspace estimator can attain the minimax optimal rate of convergence\nin polynomial time. In addition, our theory provides a more refined\nunderstanding of the statistical and computational trade-off in the problem of\nsparse principal component estimation, revealing a subtle interplay between the\neffective sample size and the number of random projections that are required to\nachieve the minimax optimal rate. Numerical studies provide further insight\ninto the procedure and confirm its highly competitive finite-sample\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 11:55:39 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 17:35:54 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 10:12:09 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 16:18:07 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Gataric", "Milana", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1712.05717", "submitter": "Tim Sullivan", "authors": "H. C. Lie and T. J. Sullivan and A. L. Teckentrup", "title": "Random forward models and log-likelihoods in Bayesian inverse problems", "comments": "25 pages", "journal-ref": "ASA/SIAM Journal of Uncertainty Quantification (2018)", "doi": "10.1137/18M1166523", "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of randomised forward models and log-likelihoods within\nthe Bayesian approach to inverse problems. Such random approximations to the\nexact forward model or log-likelihood arise naturally when a computationally\nexpensive model is approximated using a cheaper stochastic surrogate, as in\nGaussian process emulation (kriging), or in the field of probabilistic\nnumerical methods. We show that the Hellinger distance between the exact and\napproximate Bayesian posteriors is bounded by moments of the difference between\nthe true and approximate log-likelihoods. Example applications of these\nstability results are given for randomised misfit models in large data\napplications and the probabilistic solution of ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 15:39:26 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 10:08:59 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 14:16:50 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 10:53:11 GMT"}, {"version": "v5", "created": "Fri, 28 Sep 2018 09:02:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "H. C.", ""], ["Sullivan", "T. J.", ""], ["Teckentrup", "A. L.", ""]]}, {"id": "1712.05731", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Wei Jin, Yanxun Xu", "title": "A Theoretical Framework for Bayesian Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unifying framework for Bayesian nonparametric regression to\nstudy the rates of contraction with respect to the integrated $L_2$-distance\nwithout assuming the regression function space to be uniformly bounded. The\nframework is very flexible and can be applied to a wide class of nonparametric\nprior models. Three non-trivial applications of the proposed framework are\nprovided: The finite random series regression of an $\\alpha$-H\\\"older function,\nwith adaptive rates of contraction up to a logarithmic factor; The un-modified\nblock prior regression of an $\\alpha$-Sobolev function, with adaptive-and-exact\nrates of contraction; The Gaussian spline regression of an $\\alpha$-H\\\"older\nfunction, with the near-optimal posterior contraction. These applications serve\nas generalization or complement of their respective results in the literature.\nExtensions to the fixed-design regression problem and sparse additive models in\nhigh dimensions are discussed as well.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 16:18:43 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 18:09:56 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 23:40:05 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Xie", "Fangzheng", ""], ["Jin", "Wei", ""], ["Xu", "Yanxun", ""]]}, {"id": "1712.05743", "submitter": "Dheeraj Nagaraj", "authors": "Guy Bresler and Dheeraj M. Nagaraj", "title": "Stein's Method for Stationary Distributions of Markov Chains and\n  Application to Ising Models", "comments": "significant reorganization of the original. Includes a picture (yay!)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new technique, based on Stein's method, for comparing two\nstationary distributions of irreducible Markov Chains whose update rules are\n`close enough'. We apply this technique to compare Ising models on $d$-regular\nexpander graphs to the Curie-Weiss model (complete graph) in terms of pairwise\ncorrelations and more generally $k$th order moments. Concretely, we show that\n$d$-regular Ramanujan graphs approximate the $k$th order moments of the\nCurie-Weiss model to within average error $k/\\sqrt{d}$ (averaged over the size\n$k$ subsets). The result applies even in the low-temperature regime; we also\nderive some simpler approximation results for functionals of Ising models that\nhold only at high enough temperatures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 16:40:29 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 18:35:19 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 19:22:08 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Bresler", "Guy", ""], ["Nagaraj", "Dheeraj M.", ""]]}, {"id": "1712.05786", "submitter": "Alex Gibberd Dr", "authors": "Alex J. Gibberd and Sandipan Roy", "title": "Multiple Changepoint Estimation in High-Dimensional Gaussian Graphical\n  Models", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the consistency properties of a regularised estimator for the\nsimultaneous identification of both changepoints and graphical dependency\nstructure in multivariate time-series. Traditionally, estimation of Gaussian\nGraphical Models (GGM) is performed in an i.i.d setting. More recently, such\nmodels have been extended to allow for changes in the distribution, but only\nwhere changepoints are known a-priori. In this work, we study the Group-Fused\nGraphical Lasso (GFGL) which penalises partial-correlations with an L1 penalty\nwhile simultaneously inducing block-wise smoothness over time to detect\nmultiple changepoints. We present a proof of consistency for the estimator,\nboth in terms of changepoints, and the structure of the graphical models in\neach segment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:43:39 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Gibberd", "Alex J.", ""], ["Roy", "Sandipan", ""]]}, {"id": "1712.05915", "submitter": "Ivan Nourdin", "authors": "Ivan Nourdin and T.T. Diu Tran", "title": "Statistical inference for Vasicek-type model driven by Hermite processes", "comments": "19 pages, revised according to referee's report", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $Z$ denote a Hermite process of order $q \\geq 1$ and self-similarity\nparameter $H \\in (\\frac{1}{2}, 1)$. This process is $H$-self-similar, has\nstationary increments and exhibits long-range dependence. When $q=1$, it\ncorresponds to the fractional Brownian motion, whereas it is not Gaussian as\nsoon as $q\\geq 2$. In this paper, we deal with a Vasicek-type model driven by\n$Z$, of the form $dX_t = a(b - X_t)dt +dZ_t$. Here, $a > 0$ and $b \\in\n\\mathbb{R}$ are considered as unknown drift parameters. We provide estimators\nfor $a$ and $b$ based on continuous-time observations. For all possible values\nof $H$ and $q$, we prove strong consistency and we analyze the asymptotic\nfluctuations.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 07:51:46 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 15:58:16 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Nourdin", "Ivan", ""], ["Tran", "T. T. Diu", ""]]}, {"id": "1712.06160", "submitter": "Yannik Pitcan", "authors": "Yannik Pitcan", "title": "A Note on Concentration Inequalities for U-Statistics", "comments": "I give the first full proofs of Hoeffding and Bernstein related\n  inequalities for U-statistics -- these are not found in the literature to my\n  knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to discuss various concentration inequalities for\nU-statistics and most recent results. A special focus will be on providing\nproofs for bounds on the U-statistics using classical concentration\ninequalities, which, although the results well known, the proofs are not found\nin the literature.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 19:25:20 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 22:42:57 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Pitcan", "Yannik", ""]]}, {"id": "1712.06335", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev and Georgy Golubev", "title": "On One Problem in Multichannel Signal Detection", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical problem of detection of a signal with unknown\nenergy in a multi-channel system, observed in a Gaussian noise. We assume that\nthe signal can appear in the $k$-th channel with a known small prior\nprobability $\\bar{\\pi}_k$. Using noisy observations from all channels we would\nlike to detect whether the signal is presented in one of the channels or we\nobserve pure noise. In our work we describe and compare statistical properties\nof maximum posterior probability test and optimal Bayes test. In particular,\nfor these tests we obtain limiting distributions of test statistics and define\nsets of their non-detectable signals.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 11:03:23 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Golubev", "Georgy", ""]]}, {"id": "1712.06454", "submitter": "Evgeny Pchelintsev", "authors": "Evgeny Pchelintsev and Serguei Pergamenshchikov", "title": "Oracle inequalities for the stochastic differential equations", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.03111,\n  arXiv:1611.07378", "journal-ref": null, "doi": "10.1007/s11203-018-9180-1", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a survey of recent results on the adaptive robust non\nparametric methods for the continuous time regression model with the semi -\nmartingale noises with jumps. The noises are modeled by the L\\'evy processes,\nthe Ornstein -- Uhlenbeck processes and semi-Markov processes. We represent the\ngeneral model selection method and the sharp oracle inequalities methods which\nprovide the robust efficient estimation in the adaptive setting. Moreover, we\npresent the recent results on the improved model selection methods for the\nnonparametric estimation problems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 15:15:27 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pchelintsev", "Evgeny", ""], ["Pergamenshchikov", "Serguei", ""]]}, {"id": "1712.06532", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Dependence and dependence structures: estimation and visualization using\n  the unifying concept of distance multivariance", "comments": "restructured; several new results", "journal-ref": "Open Statistics, Vol. 1, No. 1 (2020)", "doi": "10.1515/stat-2020-0001", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance multivariance is a multivariate dependence measure, which can detect\ndependencies between an arbitrary number of random vectors each of which can\nhave a distinct dimension. Here we discuss several new aspects, present a\nconcise overview and use it as the basis for several new results and concepts:\nIn particular, we show that distance multivariance unifies (and extends)\ndistance covariance and the Hilbert-Schmidt independence criterion HSIC,\nmoreover also the classical linear dependence measures: covariance, Pearson's\ncorrelation and the RV coefficient appear as limiting cases. Based on distance\nmultivariance several new measures are defined: a multicorrelation which\nsatisfies a natural set of multivariate dependence measure axioms and\n$m$-multivariance which is a dependence measure yielding tests for pairwise\nindependence and independence of higher order. These tests are computationally\nfeasible and under very mild moment conditions they are consistent against all\nalternatives. Moreover, a general visualization scheme for higher order\ndependencies is proposed, including consistent estimators (based on distance\nmultivariance) for the dependence structure.\n  Many illustrative examples are provided. All functions for the use of\ndistance multivariance in applications are published in the R-package\n'multivariance'.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:17:54 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 11:53:11 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 19:59:49 GMT"}, {"version": "v4", "created": "Tue, 6 Aug 2019 12:24:56 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "1712.06788", "submitter": "Shahar Mendelson", "authors": "Gabor Lugosi and Shahar Mendelson", "title": "A remark on \"Robust machine learning by median-of-means\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the recent results announced in \"Robust machine learning by\nmedian-of-means: theory and practice\" by G. Lecu\\'e and M. Lerasle. We show\nthat these results are, in fact, almost obvious outcomes of the machinery\ndeveloped in [4] for the study of tournament procedures.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 05:26:51 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Lugosi", "Gabor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1712.06983", "submitter": "Dennis Dobler", "authors": "Dennis Dobler, Sarah Friedrich, and Markus Pauly", "title": "Nonparametric MANOVA in Mann-Whitney effects", "comments": "Authors in alphabetical order, 28 pages, 3 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate analysis of variance (MANOVA) is a powerful and versatile method\nto infer and quantify main and interaction effects in metric multivariate\nmulti-factor data. It is, however, neither robust against change in units nor a\nmeaningful tool for ordinal data. Thus, we propose a novel nonparametric\nMANOVA. Contrary to existing rank-based procedures we infer hypotheses\nformulated in terms of meaningful Mann-Whitney-type effects in lieu of\ndistribution functions. The tests are based on a quadratic form in multivariate\nrank effect estimators and critical values are obtained by the bootstrap. This\nnewly developed procedure consequently provides asymptotically exact and\nconsistent inference for general models such as the nonparametric\nBehrens-Fisher problem as well as multivariate one-, two-, and higher-way\ncrossed layouts. Computer simulations in small samples confirm the reliability\nof the developed method for ordinal as well as metric data with covariance\nheterogeneity. Finally, an analysis of a real data example illustrates the\napplicability and correct interpretation of the results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 15:21:34 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 09:41:34 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Dobler", "Dennis", ""], ["Friedrich", "Sarah", ""], ["Pauly", "Markus", ""]]}, {"id": "1712.07159", "submitter": "Mathias Vetter", "authors": "Ole Martin and Mathias Vetter", "title": "The null hypothesis of common jumps in case of irregular and\n  asynchronous observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes novel tests for the absence of jumps in a univariate\nsemimartingale and for the absence of common jumps in a bivariate\nsemimartingale. Our methods rely on ratio statistics of power variations based\non irregular observations, sampled at different frequencies. We develop central\nlimit theorems for the statistics under the respective null hypotheses and\napply bootstrap procedures to assess the limiting distributions. Further we\ndefine corrected statistics to improve the finite sample performance.\nSimulations show that the test based on our corrected statistic yields good\nresults and even outperforms existing tests in the case of regular\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 19:23:30 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Martin", "Ole", ""], ["Vetter", "Mathias", ""]]}, {"id": "1712.07248", "submitter": "Demian Pouzo", "authors": "Michael Jansson and Demian Pouzo", "title": "Towards a General Large Sample Theory for Regularized Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for studying regularized estimators; such\nestimators are pervasive in estimation problems wherein \"plug-in\" type\nestimators are either ill-defined or ill-behaved. Within this framework, we\nderive, under primitive conditions, consistency and a generalization of the\nasymptotic linearity property. We also provide data-driven methods for choosing\ntuning parameters that, under some conditions, achieve the aforementioned\nproperties. We illustrate the scope of our approach by presenting a wide range\nof applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 22:46:32 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:01:48 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 18:21:21 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 04:33:59 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jansson", "Michael", ""], ["Pouzo", "Demian", ""]]}, {"id": "1712.07364", "submitter": "Martin Spindler", "authors": "Sven Klaassen and Jannis Kueck and Martin Spindler", "title": "Transformation Models in High-Dimensions", "comments": "63 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation models are a very important tool for applied statisticians and\neconometricians. In many applications, the dependent variable is transformed so\nthat homogeneity or normal distribution of the error holds. In this paper, we\nanalyze transformation models in a high-dimensional setting, where the set of\npotential covariates is large. We propose an estimator for the transformation\nparameter and we show that it is asymptotically normally distributed using an\northogonalized moment condition where the nuisance functions depend on the\ntarget parameter. In a simulation study, we show that the proposed estimator\nworks well in small samples. A common practice in labor economics is to\ntransform wage with the log-function. In this study, we test if this\ntransformation holds in CPS data from the United States.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 08:58:41 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Klaassen", "Sven", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "1712.07371", "submitter": "Jonas Krampe", "authors": "Jonas Krampe, Jens-Peter Kreiss and Efstathios Paparoditis", "title": "EstimatedWold Representation and Spectral Density-Driven Bootstrap for\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-order dependence structure of purely nondeterministic stationary\nprocess is described by the coefficients of the famous Wold representation.\nThese coefficients can be obtained by factorizing the spectral density of the\nprocess. This relation together with some spectral density estimator is used in\norder to obtain consistent estimators of these coefficients. A spectral\ndensity-driven bootstrap for time series is then developed which uses the\nentire sequence of estimated MA coefficients together with appropriately\ngenerated pseudo innovations in order to obtain a bootstrap pseudo time series.\nIt is shown that if the underlying process is linear and if the pseudo\ninnovations are generated by means of an i.i.d. wild bootstrap which mimics, to\nthe necessary extent, the moment structure of the true innovations, this\nbootstrap proposal asymptotically works for a wide range of statistics. The\nrelations of the proposed bootstrap procedure to some other bootstrap\nprocedures, including the autoregressive-sieve bootstrap, are discussed. It is\nshown that the latter is a special case of the spectral density-driven\nbootstrap, if a parametric autoregressive spectral density estimator is used.\nSimulations investigate the performance of the new bootstrap procedure in\nfinite sample situations. Furthermore, a real-life data example is presented.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 09:17:40 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Krampe", "Jonas", ""], ["Kreiss", "Jens-Peter", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "1712.07381", "submitter": "Jingjing Zou", "authors": "Jingjing Zou, Richard A. Davis, Gennady Samorodnitsky", "title": "Extreme Value Analysis Without the Largest Values: What Can Be Done?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with the analysis of heavy-tailed data when a\nportion of the extreme values is unavailable. This research was motivated by an\nanalysis of the degree distributions in a large social network. The degree\ndistributions of such networks tend to have power law behavior in the tails. We\nfocus on the Hill estimator, which plays a starring role in heavy-tailed\nmodeling. The Hill estimator for this data exhibited a smooth and increasing\n\"sample path\" as a function of the number of upper order statistics used in\nconstructing the estimator. This behavior became more apparent as we\nartificially removed more of the upper order statistics. Building on this\nobservation we introduce a new version of the Hill estimator. It is a function\nof the number of the upper order statistics used in the estimation, but also\ndepends on the number of unavailable extreme values. We establish functional\nconvergence of the normalized Hill estimator to a Gaussian process. An\nestimation procedure is developed based on the limit theory to estimate the\nnumber of missing extremes and extreme value parameters including the tail\nindex and the bias of Hill's estimator. We illustrate how this approach works\nin both simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 09:41:17 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 23:38:12 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Zou", "Jingjing", ""], ["Davis", "Richard A.", ""], ["Samorodnitsky", "Gennady", ""]]}, {"id": "1712.07519", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang and Weijie Su", "title": "Statistical Inference for the Population Landscape via Moment Adjusted\n  Stochastic Gradients", "comments": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 2019, to appear", "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 81 (2019) 431-456", "doi": "10.1111/rssb.12313", "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical inference tasks often require iterative optimization\nmethods to compute the solution. Convergence analysis from an optimization\nviewpoint only informs us how well the solution is approximated numerically but\noverlooks the sampling nature of the data. In contrast, recognizing the\nrandomness in the data, statisticians are keen to provide uncertainty\nquantification, or confidence, for the solution obtained using iterative\noptimization methods. This paper makes progress along this direction by\nintroducing the moment-adjusted stochastic gradient descents, a new stochastic\noptimization method for statistical inference. We establish non-asymptotic\ntheory that characterizes the statistical distribution for certain iterative\nmethods with optimization guarantees. On the statistical front, the theory\nallows for model mis-specification, with very mild conditions on the data. For\noptimization, the theory is flexible for both convex and non-convex cases.\nRemarkably, the moment-adjusting idea motivated from \"error standardization\" in\nstatistics achieves a similar effect as acceleration in first-order\noptimization methods used to fit generalized linear models. We also demonstrate\nthis acceleration effect in the non-convex setting through numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 15:16:53 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 23:31:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Su", "Weijie", ""]]}, {"id": "1712.07641", "submitter": "Joni Virta", "authors": "Joni Virta, Bing Li, Klaus Nordhausen, Hannu Oja", "title": "Independent component analysis for multivariate functional data", "comments": "39 pages, 3 figures", "journal-ref": "Journal of Multivariate Analysis 176: 104568 (2020)", "doi": "10.1016/j.jmva.2019.104568", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend two methods of independent component analysis, fourth order blind\nidentification and joint approximate diagonalization of eigen-matrices, to\nvector-valued functional data. Multivariate functional data occur naturally and\nfrequently in modern applications, and extending independent component analysis\nto this setting allows us to distill important information from this type of\ndata, going a step further than the functional principal component analysis. To\nallow the inversion of the covariance operator we make the assumption that the\ndependency between the component functions lies in a finite-dimensional\nsubspace. In this subspace we define fourth cross-cumulant operators and use\nthem to construct the two novel, Fisher consistent methods for solving the\nindependent component problem for vector-valued functions. Both simulations and\nan application on a hand gesture data set show the usefulness and advantages of\nthe proposed methods over functional principal component analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 18:54:27 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Virta", "Joni", ""], ["Li", "Bing", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "1712.07801", "submitter": "Chao Gao", "authors": "Haoyang Liu and Chao Gao", "title": "Density Estimation with Contaminated Data: Minimax Rates and Theory of\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies density estimation under pointwise loss in the setting of\ncontamination model. The goal is to estimate $f(x_0)$ at some\n$x_0\\in\\mathbb{R}$ with i.i.d. observations, $$ X_1,\\dots,X_n\\sim\n(1-\\epsilon)f+\\epsilon g, $$ where $g$ stands for a contamination distribution.\nIn the context of multiple testing, this can be interpreted as estimating the\nnull density at a point. We carefully study the effect of contamination on\nestimation through the following model indices: contamination proportion\n$\\epsilon$, smoothness of target density $\\beta_0$, smoothness of contamination\ndensity $\\beta_1$, and level of contamination $m$ at the point to be estimated,\ni.e. $g(x_0)\\leq m$. It is shown that the minimax rate with respect to the\nsquared error loss is of order $$\n[n^{-\\frac{2\\beta_0}{2\\beta_0+1}}]\\vee[\\epsilon^2(1\\wedge\nm)^2]\\vee[n^{-\\frac{2\\beta_1}{2\\beta_1+1}}\\epsilon^{\\frac{2}{2\\beta_1+1}}], $$\nwhich characterizes the exact influence of contamination on the difficulty of\nthe problem. We then establish the minimal cost of adaptation to contamination\nproportion, to smoothness and to both of the numbers. It is shown that some\nsmall price needs to be paid for adaptation in any of the three cases.\nVariations of Lepski's method are considered to achieve optimal adaptation.\n  The problem is also studied when there is no smoothness assumption on the\ncontamination distribution. This setting that allows for an arbitrary\ncontamination distribution is recognized as Huber's $\\epsilon$-contamination\nmodel. The minimax rate is shown to be $$\n[n^{-\\frac{2\\beta_0}{2\\beta_0+1}}]\\vee [\\epsilon^{\\frac{2\\beta_0}{\\beta_0+1}}].\n$$ The adaptation theory is also different from the smooth contamination case.\nWhile adaptation to either contamination proportion or smoothness only costs a\nlogarithmic factor, adaptation to both numbers is proved to be impossible.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:56:19 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 01:54:01 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 03:22:00 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Liu", "Haoyang", ""], ["Gao", "Chao", ""]]}, {"id": "1712.08057", "submitter": "J. Eduardo Vera-Vald\\'es", "authors": "J. Eduardo Vera-Vald\\'es", "title": "On Long Memory Origins and Forecast Horizons", "comments": null, "journal-ref": "Journal of Forecasting (2020) DOI: 10.1002/for.2651", "doi": "10.1002/for.2651", "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most long memory forecasting studies assume that the memory is generated by\nthe fractional difference operator. We argue that the most cited theoretical\narguments for the presence of long memory do not imply the fractional\ndifference operator, and assess the performance of the autoregressive\nfractionally integrated moving average $(ARFIMA)$ model when forecasting series\nwith long memory generated by nonfractional processes. We find that high-order\nautoregressive $(AR)$ models produce similar or superior forecast performance\nthan $ARFIMA$ models at short horizons. Nonetheless, as the forecast horizon\nincreases, the $ARFIMA$ models tend to dominate in forecast performance. Hence,\n$ARFIMA$ models are well suited for forecasts of long memory processes\nregardless of the long memory generating mechanism, particularly for medium and\nlong forecast horizons. Additionally, we analyse the forecasting performance of\nthe heterogeneous autoregressive ($HAR$) model which imposes restrictions on\nhigh-order $AR$ models. We find that the structure imposed by the $HAR$ model\nproduces better long horizon forecasts than $AR$ models of the same order, at\nthe price of inferior short horizon forecasts in some cases. Our results have\nimplications for, among others, Climate Econometrics and Financial Econometrics\nmodels dealing with long memory series at different forecast horizons. We show\nin an example that while a short memory autoregressive moving average $(ARMA)$\nmodel gives the best performance when forecasting the Realized Variance of the\nS\\&P 500 up to a month ahead, the $ARFIMA$ model gives the best performance for\nlonger forecast horizons.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:23:47 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Vera-Vald\u00e9s", "J. Eduardo", ""]]}, {"id": "1712.08244", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "How Well Can Generative Adversarial Networks Learn Densities: A\n  Nonparametric View", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the rate of convergence for learning densities under\nthe Generative Adversarial Networks (GAN) framework, borrowing insights from\nnonparametric statistics. We introduce an improved GAN estimator that achieves\na faster rate, through simultaneously leveraging the level of smoothness in the\ntarget density and the evaluation metric, which in theory remedies the mode\ncollapse problem reported in the literature. A minimax lower bound is\nconstructed to show that when the dimension is large, the exponent in the rate\nfor the new GAN estimator is near optimal. One can view our results as\nanswering in a quantitative way how well GAN learns a wide range of densities\nwith different smoothness properties, under a hierarchy of evaluation metrics.\nAs a byproduct, we also obtain improved generalization bounds for GAN with\ndeeper ReLU discriminator network.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 23:13:27 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 21:19:40 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liang", "Tengyuan", ""]]}, {"id": "1712.08665", "submitter": "Vicky Fasen-Hartmann", "authors": "Vicky Fasen-Hartmann and Markus Scholz", "title": "Quasi-maximum likelihood estimation for cointegrated continuous-time\n  state space models observed at low frequencies", "comments": "51 pages", "journal-ref": "Electronic Journal of Statistics 2019", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate quasi-maximum likelihood (QML) estimation for\nthe parameters of a cointegrated solution of a continuous-time linear state\nspace model observed at discrete time points. The class of cointegrated\nsolutions of continuous-time linear state space models is equivalent to the\nclass of cointegrated continuous-time ARMA (MCARMA) processes. As a start, some\npseudo-innovations are constructed to be able to define a QML-function.\nMoreover, the parameter vector is divided appropriately in long-run and\nshort-run parameters using a representation for cointegrated solutions of\ncontinuous-time linear state space models as a sum of a L\\'evy process plus a\nstationary solution of a linear state space model. Then, we establish the\nconsistency of our estimator in three steps. First, we show the consistency for\nthe QML estimator of the long-run parameters. In the next step, we calculate\nits consistency rate. Finally, we use these results to prove the consistency\nfor the QML estimator of the short-run parameters. After all, we derive the\nlimiting distributions of the estimators. The long-run parameters are\nasymptotically mixed normally distributed, whereas the short-run parameters are\nasymptotically normally distributed. The performance of the QML estimator is\ndemonstrated by a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 21:26:33 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 14:31:12 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fasen-Hartmann", "Vicky", ""], ["Scholz", "Markus", ""]]}, {"id": "1712.08669", "submitter": "Mimoza Zografi", "authors": "Mimoza Zografi and Evdokia Xekalaki", "title": "Modeling Spatial Overdispersion with the Generalized Waring Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling spatial overdispersion requires point processes models with finite\ndimensional distributions that are overdisperse relative to the Poisson.\nFitting such models usually heavily relies on the properties of stationarity,\nergodicity, and orderliness. And, though processes based on negative binomial\nfinite dimensional distributions have been widely considered, they typically\nfail to simultaneously satisfy the three required properties for fitting.\nIndeed, it has been conjectured by Diggle and Milne that no negative binomial\nmodel can satisfy all three properties. In light of this, we change\nperspective, and construct a new process based on a different overdisperse\ncount model, the Generalized Waring Distribution. While comparably tractable\nand flexible to negative binomial processes, the Generalized Waring process is\nshown to possess all required properties, and additionally span the negative\nbinomial and Poisson processes as limiting cases. In this sense, the GW process\nprovides an approximate resolution to the conundrum highlighted by Diggle and\nMilne.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 22:00:27 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zografi", "Mimoza", ""], ["Xekalaki", "Evdokia", ""]]}, {"id": "1712.08685", "submitter": "Nick Duffield", "authors": "Nesreen K. Ahmed and Nick Duffield and Liangzhen Xia", "title": "Sampling for Approximate Bipartite Network Projection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks manifest as a stream of edges that represent transactions,\ne.g., purchases by retail customers. Many machine learning applications employ\nneighborhood-based measures to characterize the similarity among the nodes,\nsuch as the pairwise number of common neighbors (CN) and related metrics. While\nthe number of node pairs that share neighbors is potentially enormous, only a\nrelatively small proportion of them have many common neighbors. This motivates\nfinding a weighted sampling approach to preferentially sample these node pairs.\nThis paper presents a new sampling algorithm that provides a fixed size\nunbiased estimate of the similarity matrix resulting from a bipartite graph\nstream projection. The algorithm has two components. First, it maintains a\nreservoir of sampled bipartite edges with sampling weights that favor selection\nof high similarity nodes. Second, arriving edges generate a stream of\n\\textsl{similarity updates} based on their adjacency with the current sample.\nThese updates are aggregated in a second reservoir sample-based stream\naggregator to yield the final unbiased estimate. Experiments on real world\ngraphs show that a 10% sample at each stage yields estimates of high similarity\nedges with weighted relative errors of about 1%.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 23:56:44 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 03:46:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Xia", "Liangzhen", ""]]}, {"id": "1712.08748", "submitter": "Won-Ki Seo", "authors": "Won-Ki Seo", "title": "Cointegration and Representation of Cointegrated Autoregressive\n  Processes in Banach Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of cointegration for time series taking values in a\npotentially infinite dimensional Banach space. Examples of such time series\ninclude stochastic processes in C[0,1] equipped with the supremum distance and\nthose in a finite dimensional vector space equipped with a non-Euclidean\ndistance. We then develop versions of the Granger-Johansen representation\ntheorems for I(1) and I(2) autoregressive (AR) processes taking values in such\na space. To achieve our goal, we first note that an AR(p) law of motion can be\ncharacterized by a linear operator pencil via the companion form\nrepresentation, and then study the spectral properties of a linear operator\npencil to obtain a necessary and sufficient condition for a given AR(p) law of\nmotion to admit I(1) or I(2) solutions. These operator-theoretic results form a\nfundamental basis for our representation theorems. Furthermore, it is shown\nthat our operator-theoretic approach is in fact a closely related extension of\nthe conventional approach taken in a Euclidean space setting. Our theoretical\nresults may be especially relevant in a recently growing literature on\nfunctional time series analysis in Banach spaces.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 09:59:29 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 13:02:33 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 09:58:28 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 03:49:36 GMT"}, {"version": "v5", "created": "Tue, 16 Mar 2021 17:00:44 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Seo", "Won-Ki", ""]]}, {"id": "1712.08754", "submitter": "Hirofumi Ohta", "authors": "Hirofumi Ohta, Satoshi Hara", "title": "On Estimation of Conditional Modes Using Multiple Quantile Regressions", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation method for the conditional mode when the\nconditioning variable is high-dimensional. In the proposed method, we first\nestimate the conditional density by solving quantile regressions multiple\ntimes. We then estimate the conditional mode by finding the maximum of the\nestimated conditional density. The proposed method has two advantages in that\nit is computationally stable because it has no initial parameter dependencies,\nand it is statistically efficient with a fast convergence rate. Synthetic and\nreal-world data experiments demonstrate the better performance of the proposed\nmethod compared to other existing ones.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 11:02:20 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ohta", "Hirofumi", ""], ["Hara", "Satoshi", ""]]}, {"id": "1712.08823", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov, Thomas Jaki and Xavier Paoletti", "title": "A Benchmark for Dose Finding Studies with Continuous Outcomes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important tool to evaluate the performance of any design is an optimal\nbenchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56)\nthat provides an upper bound on the performance of a design under a given\nscenario. The original benchmark can be applied to dose finding studies with a\nbinary endpoint only. However, there is a growing interest in dose finding\nstudies involving continuous outcomes, but no benchmark for such studies has\nbeen developed. We show that the original benchmark and its extension by Cheung\n(2014, Biometrics 70(2), 389-397), when looked at from a different perspective,\ncan be generalised to various settings with several discrete and continuous\noutcomes. We illustrate and compare the benchmark performance in the setting of\na Phase I clinical trial with continuous toxicity endpoint and in the setting\nof a Phase I/II clinical trial with continuous efficacy outcome. We show that\nthe proposed benchmark provides an accurate upper bound for model-based dose\nfinding methods and serves as a powerful tool for evaluating designs.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 19:14:21 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 10:24:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""], ["Paoletti", "Xavier", ""]]}, {"id": "1712.08837", "submitter": "Ze Jin", "authors": "Ze Jin, Benjamin B. Risk, David S. Matteson", "title": "Optimization and Testing in Linear Non-Gaussian Component Analysis", "comments": "33 pages, 3 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:37:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 20:23:30 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""]]}, {"id": "1712.08867", "submitter": "Qian Qin", "authors": "Qian Qin and James P. Hobert", "title": "Convergence complexity analysis of Albert and Chib's algorithm for\n  Bayesian probit regression", "comments": "This is a revised version of the article \"Asymptotically stable drift\n  and minorization for Markov chains with application to Albert and Chib's\n  algorithm\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of MCMC algorithms in high dimensional Bayesian problems has become\nroutine. This has spurred so-called convergence complexity analysis, the goal\nof which is to ascertain how the convergence rate of a Monte Carlo Markov chain\nscales with sample size, $n$, and/or number of covariates, $p$. This article\nprovides a thorough convergence complexity analysis of Albert and Chib's (1993)\ndata augmentation algorithm for the Bayesian probit regression model. The main\ntools used in this analysis are drift and minorization conditions. The usual\npitfalls associated with this type of analysis are avoided by utilizing\ncentered drift functions, which are minimized in high posterior probability\nregions, and by using a new technique to suppress high-dimensionality in the\nconstruction of minorization conditions. The main result is that the geometric\nconvergence rate of the underlying Markov chain is bounded below 1 both as $n\n\\rightarrow \\infty$ (with $p$ fixed), and as $p \\rightarrow \\infty$ (with $n$\nfixed). Furthermore, the first computable bounds on the total variation\ndistance to stationarity are byproducts of the asymptotic analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 02:23:51 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 17:12:18 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""]]}, {"id": "1712.08946", "submitter": "Ruobin Gong", "authors": "Ruobin Gong and Xiao-Li Meng", "title": "Judicious Judgment Meets Unsettling Updating: Dilation, Sure Loss, and\n  Simpson's Paradox", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning using imprecise probabilities is gaining more attention\nbecause it presents an alternative strategy for reducing irreplicable findings\nby freeing the user from the task of making up unwarranted high-resolution\nassumptions. However, model updating as a mathematical operation is inherently\nexact, hence updating imprecise models requires the user's judgment in choosing\namong competing updating rules. These rules often lead to incompatible\ninferences, and can exhibit unsettling phenomena like dilation, contraction and\nsure loss, which cannot occur with the Bayes rule and precise probabilities. We\nrevisit a number of famous \"paradoxes\", including the three prisoners/Monty\nHall problem, revealing that a logical fallacy arises from a set of marginally\nplausible yet jointly incommensurable assumptions when updating the underlying\nimprecise model. We establish an equivalence between Simpson's paradox and an\nimplicit adoption of a pair of aggregation rules that induce sure loss. We also\nexplore behavioral discrepancies between the generalized Bayes rule, Dempster's\nrule and the Geometric rule as alternative posterior updating rules for Choquet\ncapacities of order 2. We show that both the generalized Bayes rule and\nGeometric rule are incapable of updating without prior information regardless\nof how strong the information in our data is, and that Dempster's rule and the\nGeometric rule can mathematically contradict each other with respect to\ndilation and contraction. Our findings show that unsettling updates reflect a\ncollision between the rules' assumptions and the inexactness allowed by the\nmodel itself, highlighting the invaluable role of judicious judgment in\nhandling low-resolution information, and the care we must take when applying\nlearning rules to update imprecise probabilities.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 17:03:37 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Gong", "Ruobin", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1712.08964", "submitter": "Qifan Song", "authors": "Qifan Song and Faming Liang", "title": "Nearly optimal Bayesian Shrinkage for High Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, shrinkage priors have received much attention in\nBayesian analysis of high-dimensional data. In this paper, we study the problem\nfor high-dimensional linear regression models. We show that if the shrinkage\nprior has a heavy and flat tail, and allocates a sufficiently large probability\nmass in a very small neighborhood of zero, then its posterior properties are as\ngood as those of the spike-and-slab prior. While enjoying its efficiency in\nBayesian computation, the shrinkage prior can lead to a nearly optimal\ncontraction rate and selection consistency as the spike-and-slab prior. Our\nnumerical results show that under posterior consistency, Bayesian methods can\nyield much better results in variable selection than the regularization\nmethods, such as Lasso and SCAD. We also establish a Bernstein von-Mises type\nresults comparable to Castillo et al (2015), this result leads to a convenient\nway to quantify uncertainties of the regression coefficient estimates, which\nhas been beyond the ability of regularization methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 19:52:08 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Song", "Qifan", ""], ["Liang", "Faming", ""]]}, {"id": "1712.08977", "submitter": "Michael Levine", "authors": "Michael Levine", "title": "Robust functional estimation in the multivariate partial linear model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adaptive estimation of the functional component in\na multivariate partial linear model where the argument of the function is\ndefined on a $q$-dimensional grid. Obtaining an adaptive estimator of this\nfunctional component is an important practical problem in econometrics where\nexact distributions of random errors and the parametric component are mostly\nunknown and cannot safely assumed to be normal. An estimator of the functional\ncomponent that is adaptive in the mean squared sense over the wide range of\nmultivariate Besov classes and robust to a wide choice of distributions of the\nlinear component and random errors is constructed. It is also shown that the\nsame estimator is locally adaptive over the same range of Besov classes and\nrobust over large collections of distributions of the linear component and\nrandom errors as well. At any fixed point, this estimator also attains a local\nadaptive minimax rate. The procedure needed to obtain such an estimator turns\nout to depend on the choice of the right shrinkage approach in the wavelet\ndomain. We show that one possible approach is to use the multivariate version\nof the classical BlockJS method. The multivariate version of BlockJS is\ndeveloped in the manuscript and is shown to represent an independent interest.\nFinally, the Besov space scale over which the proposed estimator is locally\nadaptive is shown to depend on the dimensionality of the domain of the\nfunctional component; the higher the dimension, the larger the smoothness\nindicator of Besov spaces must be.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 00:45:02 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Levine", "Michael", ""]]}, {"id": "1712.08983", "submitter": "Debdeep Pati", "authors": "Debdeep Pati, Anirban Bhattacharya, Yun Yang", "title": "On Statistical Optimality of Variational Bayes", "comments": "Accepted at AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article addresses a long-standing open problem on the justification of\nusing variational Bayes methods for parameter estimation. We provide general\nconditions for obtaining optimal risk bounds for point estimates acquired from\nmean-field variational Bayesian inference. The conditions pertain to the\nexistence of certain test functions for the distance metric on the parameter\nspace and minimal assumptions on the prior. A general recipe for verification\nof the conditions is outlined which is broadly applicable to existing Bayesian\nmodels with or without latent variables. As illustrations, specific\napplications to Latent Dirichlet Allocation and Gaussian mixture models are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 01:29:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""], ["Yang", "Yun", ""]]}, {"id": "1712.09232", "submitter": "Jaron Sanders", "authors": "Jaron Sanders, Alexandre Prouti\\`ere and Se-Young Yun", "title": "Clustering in Block Markov Chains", "comments": "73 pages, 18 plots, second revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers cluster detection in Block Markov Chains (BMCs). These\nMarkov chains are characterized by a block structure in their transition\nmatrix. More precisely, the $n$ possible states are divided into a finite\nnumber of $K$ groups or clusters, such that states in the same cluster exhibit\nthe same transition rates to other states. One observes a trajectory of the\nMarkov chain, and the objective is to recover, from this observation only, the\n(initially unknown) clusters. In this paper we devise a clustering procedure\nthat accurately, efficiently, and provably detects the clusters. We first\nderive a fundamental information-theoretical lower bound on the detection error\nrate satisfied under any clustering algorithm. This bound identifies the\nparameters of the BMC, and trajectory lengths, for which it is possible to\naccurately detect the clusters. We next develop two clustering algorithms that\ncan together accurately recover the cluster structure from the shortest\npossible trajectories, whenever the parameters allow detection. These\nalgorithms thus reach the fundamental detectability limit, and are optimal in\nthat sense.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 11:00:15 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 17:44:07 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 18:03:21 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sanders", "Jaron", ""], ["Prouti\u00e8re", "Alexandre", ""], ["Yun", "Se-Young", ""]]}, {"id": "1712.09483", "submitter": "Zhao Ren", "authors": "Yu Liu and Zhao Ren", "title": "Minimax Estimation of Large Precision Matrices with Bandable Cholesky\n  Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last decade witnesses significant methodological and theoretical advances in\nestimating large precision matrices. In particular, there are scientific\napplications such as longitudinal data, meteorology and spectroscopy in which\nthe ordering of the variables can be interpreted through a bandable structure\non the Cholesky factor of the precision matrix. However, the minimax theory has\nstill been largely unknown, as opposed to the well established minimax results\nover the corresponding bandable covariance matrices. In this paper, we focus on\ntwo commonly used types of parameter spaces, and develop the optimal rates of\nconvergence under both the operator norm and the Frobenius norm. A striking\nphenomenon is found: two types of parameter spaces are fundamentally different\nunder the operator norm but enjoy the same rate optimality under the Frobenius\nnorm, which is in sharp contrast to the equivalence of corresponding two types\nof bandable covariance matrices under both norms. This fundamental difference\nis established by carefully constructing the corresponding minimax lower\nbounds. Two new estimation procedures are developed: for the operator norm, our\noptimal procedure is based on a novel local cropping estimator targeting on all\nprinciple submatrices of the precision matrix while for the Frobenius norm, our\noptimal procedure relies on a delicate regression-based thresholding rule.\nLepski's method is considered to achieve optimal adaptation. We further\nestablish rate optimality in the nonparanormal model. Numerical studies are\ncarried out to confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 03:26:25 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:35:21 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 03:47:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Liu", "Yu", ""], ["Ren", "Zhao", ""]]}, {"id": "1712.09562", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, and Fr\\'ed\\'erique\n  Letu\\'e", "title": "Spatial point processes intensity estimation with a diverging number of\n  covariates", "comments": "arXiv admin note: text overlap with arXiv:1703.02462", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection procedures for spatial point processes parametric intensity\nestimation have been recently developed since more and more applications\ninvolve a large number of covariates. In this paper, we investigate the setting\nwhere the number of covariates diverges as the domain of observation increases.\nIn particular, we consider estimating equations based on Campbell theorems\nderived from Poisson and logistic regression likelihoods regularized by a\ngeneral penalty function. We prove that, under some conditions, the\nconsistency, the sparsity, and the asymptotic normality are valid for such a\nsetting. We support the theoretical results by numerical ones obtained from\nsimulation experiments and an application to forestry datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 12:34:05 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Letu\u00e9", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1712.09661", "submitter": "Nadezhda Gribkova Dr.", "authors": "Lingzhi Chen, Youri Davydov, Nadezhda Gribkova, and Ri\\v{c}ardas\n  Zitikis", "title": "Estimating the index of increase via balancing deterministic and random\n  data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and explore an empirical index of increase that works in both\ndeterministic and random environments, thus allowing to assess monotonicity of\nfunctions that are prone to random measurement-errors. We prove consistency of\nthe index and show how its rate of convergence is influenced by deterministic\nand random parts of the data. In particular, the obtained results suggest a\nfrequency at which observations should be taken in order to reach any\npre-specified level of estimation precision. We illustrate the index using data\narising from purely deterministic and error-contaminated functions, which may\nor may not be monotonic.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 19:47:16 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 18:06:21 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Chen", "Lingzhi", ""], ["Davydov", "Youri", ""], ["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1712.09694", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "On the estimation of correlation in a binary sequence model", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary sequence generated by thresholding a hidden continuous\nsequence. The hidden variables are assumed to have a compound symmetry\ncovariance structure with a single parameter characterizing the common\ncorrelation. We study the parameter estimation problem under such one-parameter\nmodels. We demonstrate that maximizing the likelihood function does not yield\nconsistent estimates for the correlation. We then formally prove the\nnonestimability of the parameter by deriving a non-vanishing minimax lower\nbound. This counter-intuitive phenomenon provides an interesting insight that\none-bit information of each latent variable is not sufficient to consistently\nrecover their common correlation. On the other hand, we further show that\ntrinary data generated from the hidden variables can consistently estimate the\ncorrelation with parametric convergence rate. Thus we reveal a phase transition\nphenomenon regarding the discretization of latent continuous variables while\npreserving the estimability of the correlation. Numerical experiments are\nperformed to validate the conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 22:19:19 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1712.09813", "submitter": "Mansoor Sheikh", "authors": "M Sheikh and A C C Coolen", "title": "Accurate Bayesian Data Classification without Hyperparameter\n  Cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the standard Bayesian multivariate Gaussian generative data\nclassifier by considering a generalization of the conjugate, normal-Wishart\nprior distribution and by deriving the hyperparameters analytically via\nevidence maximization. The behaviour of the optimal hyperparameters is explored\nin the high-dimensional data regime. The classification accuracy of the\nresulting generalized model is competitive with state-of-the art Bayesian\ndiscriminant analysis methods, but without the usual computational burden of\ncross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:36:34 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Sheikh", "M", ""], ["Coolen", "A C C", ""]]}, {"id": "1712.09912", "submitter": "Daren Wang", "authors": "Daren Wang, Yi Yu and Alessandro Rinaldo", "title": "Optimal Covariance Change Point Localization in High Dimension", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of change point detection for covariance matrices in\nhigh dimensions. We assume that we observe a sequence {X_i}_{i=1,...,n} of\nindependent and centered p-dimensional sub-Gaussian random vectors whose\ncovariance matrices are piecewise constant. Our task is to recover with high\naccuracy the number and locations of the change points, which are assumed\nunknown. Our generic model setting allows for all the model parameters to\nchange with n, including the dimension p, the minimal spacing between\nconsecutive change points, the magnitude of smallest change size and the\nmaximal Orlicz- 2 norm of the covariance matrices of the sample points. Without\nassuming any additional structural assumption, such as low rank matrices or\nhaving sparse principle components, we set up a general framework and a\nbenchmark result for the covariance change point detection problem. We\nintroduce two procedures, one based on the binary segmentation algorithm (e.g.\nVostrikova, 1981) and the other on its extension known as wild binary\nsegmentation of Fryzlewicz (2014), and demonstrate that, under suitable\nconditions, both procedures are able to consistently es- timate the number and\nlocations of change points. Our second algorithm, called Wild Binary\nSegmentation through Independent Projection (WBSIP), is shown to be optimal in\nthe sense of allowing for the minimax scaling in all the relevant parameters.\nOur minimax analysis reveals a phase transition effect based on the problem of\nchange point localization. To the best of our knowledge, this type of results\nhas not been established elsewhere in the high-dimensional change point\ndetection literature.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:14:07 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 10:37:51 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1712.09941", "submitter": "Long Feng", "authors": "Long Feng, Cun-Hui Zhang", "title": "Sorted Concave Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is biased. Concave penalized least squares estimation (PLSE) takes\nadvantage of signal strength to reduce this bias, leading to sharper error\nbounds in prediction, coefficient estimation and variable selection. For\nprediction and estimation, the bias of the Lasso can be also reduced by taking\na smaller penalty level than what selection consistency requires, but such\nsmaller penalty level depends on the sparsity of the true coefficient vector.\nThe sorted L1 penalized estimation (Slope) was proposed for adaptation to such\nsmaller penalty levels. However, the advantages of concave PLSE and Slope do\nnot subsume each other. We propose sorted concave penalized estimation to\ncombine the advantages of concave and sorted penalizations. We prove that\nsorted concave penalties adaptively choose the smaller penalty level and at the\nsame time benefits from signal strength, especially when a significant\nproportion of signals are stronger than the corresponding adaptively selected\npenalty levels. A local convex approximation, which extends the local linear\nand quadratic approximations to sorted concave penalties, is developed to\nfacilitate the computation of sorted concave PLSE and proven to possess desired\nprediction and estimation error bounds. We carry out a unified treatment of\npenalty functions in a general optimization setting, including the penalty\nlevels and concavity of the above mentioned sorted penalties and mixed\npenalties motivated by Bayesian considerations. Our analysis of prediction and\nestimation errors requires the restricted eigenvalue condition on the design,\nnot beyond, and provides selection consistency under a required minimum signal\nstrength condition in addition. Thus, our results also sharpens existing\nresults on concave PLSE by removing the upper sparse eigenvalue component of\nthe sparse Riesz condition.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 17:16:53 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Feng", "Long", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1712.10087", "submitter": "Jason Klusowski M", "authors": "W. D. Brinda and Jason M. Klusowski", "title": "Finite-sample risk bounds for maximum likelihood estimation with\n  arbitrary penalties", "comments": "To appear in IEEE Transactions on Information Theory, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MDL two-part coding $ \\textit{index of resolvability} $ provides a\nfinite-sample upper bound on the statistical risk of penalized likelihood\nestimators over countable models. However, the bound does not apply to\nunpenalized maximum likelihood estimation or procedures with exceedingly small\npenalties. In this paper, we point out a more general inequality that holds for\narbitrary penalties. In addition, this approach makes it possible to derive\nexact risk bounds of order $1/n$ for iid parametric models, which improves on\nthe order $(\\log n)/n$ resolvability bounds. We conclude by discussing\nimplications for adaptive estimation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 01:15:49 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Brinda", "W. D.", ""], ["Klusowski", "Jason M.", ""]]}, {"id": "1712.10099", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu and Lingsong Zhang", "title": "Finite-sample bounds for the multivariate Behrens-Fisher distribution\n  with proportional covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Behrens-Fisher problem is a well-known hypothesis testing problem in\nstatistics concerning two-sample mean comparison. In this article, we confirm\none conjecture in Eaton and Olshen (1972), which provides stochastic bounds for\nthe multivariate Behrens-Fisher test statistic under the null hypothesis. We\nalso extend their results on the stochastic ordering of random quotients to the\narbitrary finite dimensional case. This work can also be seen as a\ngeneralization of Hsu (1938) that provided the bounds for the univariate\nBehrens-Fisher problem. The results obtained in this article can be used to\nderive a testing procedure for the multivariate Behrens-Fisher problem that\nstrongly controls the Type I error.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 02:34:45 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Qiu", "Yixuan", ""], ["Zhang", "Lingsong", ""]]}, {"id": "1712.10163", "submitter": "Alexander Wein", "authors": "Afonso S. Bandeira, Ben Blum-Smith, Joe Kileel, Amelia Perry, Jonathan\n  Weed, Alexander S. Wein", "title": "Estimation under group actions: recovering orbits from invariants", "comments": "54 pages. This version contains a number of new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.AC math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by geometric problems in signal processing, computer vision, and\nstructural biology, we study a class of orbit recovery problems where we\nobserve very noisy copies of an unknown signal, each acted upon by a random\nelement of some group (such as Z/p or SO(3)). The goal is to recover the orbit\nof the signal under the group action in the high-noise regime. This generalizes\nproblems of interest such as multi-reference alignment (MRA) and the\nreconstruction problem in cryo-electron microscopy (cryo-EM). We obtain\nmatching lower and upper bounds on the sample complexity of these problems in\nhigh generality, showing that the statistical difficulty is intricately\ndetermined by the invariant theory of the underlying symmetry group.\n  In particular, we determine that for cryo-EM with noise variance $\\sigma^2$\nand uniform viewing directions, the number of samples required scales as\n$\\sigma^6$. We match this bound with a novel algorithm for ab initio\nreconstruction in cryo-EM, based on invariant features of degree at most 3. We\nfurther discuss how to recover multiple molecular structures from heterogeneous\ncryo-EM samples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 09:53:24 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 17:35:19 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Blum-Smith", "Ben", ""], ["Kileel", "Joe", ""], ["Perry", "Amelia", ""], ["Weed", "Jonathan", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1712.10252", "submitter": "Adrien Meynard", "authors": "Adrien Meynard (I2M), Bruno Torr\\'esani (I2M)", "title": "Spectral analysis for nonstationary audio", "comments": "IEEE/ACM Transactions on Audio, Speech and Language Processing,\n  Institute of Electrical and Electronics Engineers, In press", "journal-ref": null, "doi": "10.1109/TASLP.2018.2862353", "report-no": null, "categories": "eess.AS cs.SD math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for the analysis of nonstationary signals is proposed, with a\nfocus on audio applications. Following earlier contributions, nonstationarity\nis modeled via stationarity-breaking operators acting on Gaussian stationary\nrandom signals. The focus is on time warping and amplitude modulation, and an\napproximate maximum-likelihood approach based on suitable approximations in the\nwavelet transform domain is developed. This paper provides theoretical analysis\nof the approximations, and introduces JEFAS, a corresponding estimation\nalgorithm. The latter is tested and validated on synthetic as well as real\naudio signal.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 15:12:28 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 12:09:35 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 13:27:26 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Meynard", "Adrien", "", "I2M"], ["Torr\u00e9sani", "Bruno", "", "I2M"]]}]