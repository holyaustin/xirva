[{"id": "1008.0054", "submitter": "Jean-Marc Bardet", "authors": "Jean-Marc Bardet (SAMM), William Chakry Kengne (SAMM), Olivier\n  Wintenberger (CEREMADE)", "title": "Detecting multiple change-points in general causal time series using\n  penalized quasi-likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the off-line multiple change-point detection in a\nsemiparametric framework. The time series is supposed to belong to a large\nclass of models including AR($\\infty$), ARCH($\\infty$), TARCH($\\infty$),...\nmodels where the coefficients change at each instant of breaks. The different\nunknown parameters (number of changes, change dates and parameters of\nsuccessive models) are estimated using a penalized contrast built on\nconditional quasi-likelihood. Under Lipshitzian conditions on the model, the\nconsistency of the estimator is proved when the moment order $r$ of the process\nsatisfies $r\\geq 2$. If $r\\geq 4$, the same convergence rates for the\nestimators than in the case of independent random variables are obtained. The\nparticular cases of AR($\\infty$), ARCH($\\infty$) and TARCH($\\infty$) show that\nour method notably improves the existing results.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 06:23:15 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Bardet", "Jean-Marc", "", "SAMM"], ["Kengne", "William Chakry", "", "SAMM"], ["Wintenberger", "Olivier", "", "CEREMADE"]]}, {"id": "1008.0055", "submitter": "Christian Sch\\\"afer", "authors": "Christian Sch\\\"afer (CREST, CEREMADE)", "title": "Parametric families on large binary spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of adaptive Monte Carlo algorithms, we cannot directly\ngenerate independent samples from the distribution of interest but use a proxy\nwhich we need to be close to the target. Generally, such a proxy distribution\nis a parametric family on the sampling spaces of the target distribution. For\ncontinuous sampling problems in high dimensions, we often use the multivariate\nnormal distribution as a proxy for we can easily parametrise it by its moments\nand quickly sample from it. Our objective is to construct similarly flexible\nparametric families on binary sampling spaces too large for exhaustive\nenumeration. The binary sampling problem is more difficult than its continuous\ncounterpart since the choice of a suitable proxy distribution is not obvious.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 06:23:55 GMT"}, {"version": "v2", "created": "Fri, 18 Feb 2011 12:34:42 GMT"}, {"version": "v3", "created": "Mon, 21 Feb 2011 10:18:26 GMT"}, {"version": "v4", "created": "Thu, 24 Feb 2011 12:29:15 GMT"}, {"version": "v5", "created": "Mon, 14 Nov 2011 18:16:55 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Sch\u00e4fer", "Christian", "", "CREST, CEREMADE"]]}, {"id": "1008.0116", "submitter": "Michael Boutsikas V", "authors": "M.V. Boutsikas, A.C. Rakitzis and D.L. Antzoulakos", "title": "On the relation between the distributions of stopping time and stopped\n  sum with applications", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T\\$ be a stopping time associated with a sequence of independent random\nvariables $Z_{1},Z_{2},...$ . By applying a suitable change in the probability\nmeasure we present relations between the moment or probability generating\nfunctions of the stopping time $T$ and the stopped sum\n$%S_{T}=Z_{1}+Z_{2}+...+Z_{T}$. These relations imply that, when the\ndistribution of $S_{T}$\\ is known, then the distribution of $T$\\ is also known\nand vice versa. Applications are offered in order to illustrate the\napplicability of the main results, which also have independent interest. In the\nfirst one we consider a random walk with exponentially distributed up and down\nsteps and derive the distribution of its first exit time from an interval\n$(-a,b).$ In the second application we consider a series of samples from a\nmanufacturing process and we let $Z_{i},i\\geq 1$, denoting the number of\nnon-conforming products in the $i$-th sample. We derive the joint distribution\nof the random vector $(T,S_{T})$, where $T$ is the waiting time until the\nsampling level of the inspection changes based on a $k$-run switching rule.\nFinally, we demonstrate how the joint distribution of $%(T,S_{T})$ can be used\nfor the estimation of the probability $p$ of an item being defective, by\nemploying an EM algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 18:02:20 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2011 09:50:29 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Boutsikas", "M. V.", ""], ["Rakitzis", "A. C.", ""], ["Antzoulakos", "D. L.", ""]]}, {"id": "1008.0121", "submitter": "Roberto Casarin", "authors": "R. Casarin and L. Dalla Valle, F. Leisen", "title": "Bayesian Model Selection for Beta Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with Bayesian inference for Beta autoregressive processes. We\nrestrict our attention to the class of conditionally linear processes. These\nprocesses are particularly suitable for forecasting purposes, but are difficult\nto estimate due to the constraints on the parameter space. We provide a full\nBayesian approach to the estimation and include the parameter restrictions in\nthe inference problem by a suitable specification of the prior distributions.\nMoreover in a Bayesian framework parameter estimation and model choice can be\nsolved simultaneously. In particular we suggest a Markov-Chain Monte Carlo\n(MCMC) procedure based on a Metropolis-Hastings within Gibbs algorithm and\nsolve the model selection problem following a reversible jump MCMC approach.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 20:30:32 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Casarin", "R.", ""], ["Valle", "L. Dalla", ""], ["Leisen", "F.", ""]]}, {"id": "1008.0127", "submitter": "Saralees Nadarajah", "authors": "C. S. Withers, S. Nadarajah", "title": "Nonparametric estimates of low bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an arbitrary smooth functional of $k\n\\geq 1 $ distribution functions (d.f.s.) in terms of random samples from them.\nThe natural estimate replaces the d.f.s by their empirical d.f.s. Its bias is\ngenerally $\\sim n^{-1}$, where $n$ is the minimum sample size, with a {\\it\n$p$th order} iterative estimate of bias $ \\sim n^{-p}$ for any $p$. For $p \\leq\n4$, we give an explicit estimate in terms of the first $2p - 2$ von Mises\nderivatives of the functional evaluated at the empirical d.f.s. These may be\nused to obtain {\\it unbiased} estimates, where these exist and are of known\nform in terms of the sample sizes; our form for such unbiased estimates is much\nsimpler than that obtained using polykays and tables of the symmetric\nfunctions. Examples include functions of a mean vector (such as the ratio of\ntwo means and the inverse of a mean), standard deviation, correlation, return\ntimes and exceedances. These $p$th order estimates require only $\\sim n $\ncalculations. This is in sharp contrast with computationally intensive bias\nreduction methods such as the $p$th order bootstrap and jackknife, which\nrequire $\\sim n^p $ calculations.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 22:12:11 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Withers", "C. S.", ""], ["Nadarajah", "S.", ""]]}, {"id": "1008.0204", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Mixture decompositions of exponential families using a decomposition of\n  their sample spaces", "comments": "17 pages, 2 figures", "journal-ref": "Kybernetika, Volume 49 (2013), Number 1", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the smallest $m$ such that every element of\nan exponential family can be written as a mixture of $m$ elements of another\nexponential family. We propose an approach based on coverings and packings of\nthe face lattice of the corresponding convex support polytopes and results from\ncoding theory. We show that $m=q^{N-1}$ is the smallest number for which any\ndistribution of $N$ $q$-ary variables can be written as mixture of $m$\nindependent $q$-ary variables. Furthermore, we show that any distribution of\n$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elements\nof the $k$-interaction exponential family.\n", "versions": [{"version": "v1", "created": "Sun, 1 Aug 2010 20:24:02 GMT"}, {"version": "v2", "created": "Sat, 11 Dec 2010 16:08:08 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2011 14:25:41 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2013 22:24:28 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1008.0446", "submitter": "Guillermo Henry", "authors": "Guillermo Henry and Daniela Rodriguez", "title": "Robust Estimators in Partly Linear Regression Models on Riemannian\n  Manifolds", "comments": "14 pages, 1 table, 3 figures, minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under a partially linear models we study a family of robust estimates for the\nregression parameter and the regression function when some of the predictor\nvariables take values on a Riemannian manifold. We obtain the consistency and\nthe asymptotic normality of the proposed estimators. Also, we consider a robust\ncross validation procedure to select the smoothing parameter. Simulations and\napplication to real data show the performance of our proposal under small\nsamples and contamination.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 03:01:59 GMT"}, {"version": "v2", "created": "Wed, 25 May 2011 18:25:33 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["Henry", "Guillermo", ""], ["Rodriguez", "Daniela", ""]]}, {"id": "1008.0526", "submitter": "Nicolas Verzelen", "authors": "Nicolas Verzelen (MISTEA)", "title": "Minimax risks for sparse regressions: Ultra-high-dimensional phenomenons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the standard Gaussian linear regression model $Y=X\\theta+\\epsilon$,\nwhere $Y\\in R^n$ is a response vector and $ X\\in R^{n*p}$ is a design matrix.\nNumerous work have been devoted to building efficient estimators of $\\theta$\nwhen $p$ is much larger than $n$. In such a situation, a classical approach\namounts to assume that $\\theta_0$ is approximately sparse. This paper studies\nthe minimax risks of estimation and testing over classes of $k$-sparse vectors\n$\\theta$. These bounds shed light on the limitations due to\nhigh-dimensionality. The results encompass the problem of prediction\n(estimation of $X\\theta$), the inverse problem (estimation of $\\theta_0$) and\nlinear testing (testing $X\\theta=0$). Interestingly, an elbow effect occurs\nwhen the number of variables $k\\log(p/k)$ becomes large compared to $n$.\nIndeed, the minimax risks and hypothesis separation distances blow up in this\nultra-high dimensional setting. We also prove that even dimension reduction\ntechniques cannot provide satisfying results in an ultra-high dimensional\nsetting. Moreover, we compute the minimax risks when the variance of the noise\nis unknown. The knowledge of this variance is shown to play a significant role\nin the optimal rates of estimation and testing. All these minimax bounds\nprovide a characterization of statistical problems that are so difficult so\nthat no procedure can provide satisfying results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 12:04:05 GMT"}, {"version": "v2", "created": "Mon, 20 Sep 2010 07:42:56 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2012 13:08:58 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["Verzelen", "Nicolas", "", "MISTEA"]]}, {"id": "1008.1290", "submitter": "Venkat Chandrasekaran", "authors": "Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky", "title": "Latent variable graphical model selection via convex optimization", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS949 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1935-1967", "doi": "10.1214/11-AOS949", "report-no": "IMS-AOS-AOS949", "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we observe samples of a subset of a collection of random variables.\nNo additional information is provided about the number of latent variables, nor\nof the relationship between the latent and observed variables. Is it possible\nto discover the number of latent components, and to learn a statistical model\nover the entire collection of variables? We address this question in the\nsetting in which the latent and observed variables are jointly Gaussian, with\nthe conditional statistics of the observed variables conditioned on the latent\nvariables being specified by a graphical model. As a first step we give natural\nconditions under which such latent-variable Gaussian graphical models are\nidentifiable given marginal statistics of only the observed variables.\nEssentially these conditions require that the conditional graphical model among\nthe observed variables is sparse, while the effect of the latent variables is\n\"spread out\" over most of the observed variables. Next we propose a tractable\nconvex program based on regularized maximum-likelihood for model selection in\nthis latent-variable setting; the regularizer uses both the $\\ell_1$ norm and\nthe nuclear norm. Our modeling framework can be viewed as a combination of\ndimensionality reduction (to identify latent variables) and graphical modeling\n(to capture remaining statistical structure not attributable to the latent\nvariables), and it consistently estimates both the number of latent components\nand the conditional graphical model structure among the observed variables.\nThese results are applicable in the high-dimensional setting in which the\nnumber of latent/observed variables grows with the number of samples of the\nobserved variables. The geometric properties of the algebraic varieties of\nsparse matrices and of low-rank matrices play an important role in our\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 6 Aug 2010 23:23:05 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 09:51:46 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Chandrasekaran", "Venkat", ""], ["Parrilo", "Pablo A.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1008.1298", "submitter": "Donald E. Ramirez", "authors": "Diarmuid O'Driscoll and Donald E. Ramirez", "title": "Geometric View of Measurement Errors", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, Volume\n  40, Issue 9 October 2011 , pages 1373 - 1382", "doi": "10.1080/03610918.2011.575503", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The slope of the best fit line from minimizing the sum of the squared oblique\nerrors is the root of a polynomial of degree four. This geometric view of\nmeasurement errors is used to give insight into the performance of various\nslope estimators for the measurement error model including an adjusted fourth\nmoment estimator introduced by Gillard and Iles (2005) to remove the jump\ndiscontinuity in the estimator of Copas (1972). The polynomial of degree four\nis associated with a minimun deviation estimator. A simulation study compares\nthese estimators showing improvement in bias and mean squared error.\n", "versions": [{"version": "v1", "created": "Sat, 7 Aug 2010 02:53:54 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["O'Driscoll", "Diarmuid", ""], ["Ramirez", "Donald E.", ""]]}, {"id": "1008.1342", "submitter": "Mohamed El Machkouri", "authors": "Mohamed El Machkouri", "title": "Asymptotic normality of the Parzen-Rosenblatt density estimator for\n  strongly mixing random fields", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the asymptotic normality of the kernel density estimator (introduced\nby Rosenblatt (1956) and Parzen (1962)) in the context of stationary strongly\nmixing random fields. Our approach is based on the Lindeberg's method rather\nthan on Bernstein's small-block-large-block technique and coupling arguments\nwidely used in previous works on nonparametric estimation for spatial\nprocesses. Our method allows us to consider only minimal conditions on the\nbandwidth parameter and provides a simple criterion on the (non-uniform) strong\nmixing coefficients which do not depend on the bandwith.\n", "versions": [{"version": "v1", "created": "Sat, 7 Aug 2010 13:25:33 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Machkouri", "Mohamed El", ""]]}, {"id": "1008.1355", "submitter": "Ioannis Kontoyiannis", "authors": "Petros Dellaportas, Ioannis Kontoyiannis", "title": "Control Variates for Reversible MCMC Samplers", "comments": "44 pages; 6 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general methodology is introduced for the construction and effective\napplication of control variates to estimation problems involving data from\nreversible MCMC samplers. We propose the use of a specific class of functions\nas control variates, and we introduce a new, consistent estimator for the\nvalues of the coefficients of the optimal linear combination of these\nfunctions. The form and proposed construction of the control variates is\nderived from our solution of the Poisson equation associated with a specific\nMCMC scenario. The new estimator, which can be applied to the same MCMC sample,\nis derived from a novel, finite-dimensional, explicit representation for the\noptimal coefficients. The resulting variance-reduction methodology is primarily\napplicable when the simulated data are generated by a conjugate random-scan\nGibbs sampler. MCMC examples of Bayesian inference problems demonstrate that\nthe corresponding reduction in the estimation variance is significant, and that\nin some cases it can be quite dramatic. Extensions of this methodology in\nseveral directions are given, including certain families of Metropolis-Hastings\nsamplers and hybrid Metropolis-within-Gibbs algorithms. Corresponding\nsimulation examples are presented illustrating the utility of the proposed\nmethods. All methodological and asymptotic arguments are rigorously justified\nunder easily verifiable and essentially minimal conditions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Aug 2010 16:58:09 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Dellaportas", "Petros", ""], ["Kontoyiannis", "Ioannis", ""]]}, {"id": "1008.1393", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo", "title": "Towards Nonstationary, Nonparametric Independent Process Analysis with\n  Unknown Source Component Dimensions", "comments": "9 pages, 3 figures", "journal-ref": "European Signal Processing Conference (EUSIPCO), 1718-1722, 2011", "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to extend independent subspace analysis (ISA) to\nthe case of (i) nonparametric, not strictly stationary source dynamics and (ii)\nunknown source component dimensions. We make use of functional autoregressive\n(fAR) processes to model the temporal evolution of the hidden sources. An\nextension of the ISA separation principle--which states that the ISA problem\ncan be solved by traditional independent component analysis (ICA) and\nclustering of the ICA elements--is derived for the solution of the defined fAR\nindependent process analysis task (fAR-IPA): applying fAR identification we\nreduce the problem to ISA. A local averaging approach, the Nadaraya-Watson\nkernel regression technique is adapted to obtain strongly consistent fAR\nestimation. We extend the Amari-index to different dimensional components and\nillustrate the efficiency of the fAR-IPA approach by numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 8 Aug 2010 09:31:57 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Szabo", "Zoltan", ""]]}, {"id": "1008.1716", "submitter": "Roman Vershynin", "authors": "Elizaveta Levina, Roman Vershynin", "title": "Partial estimation of covariance matrices", "comments": "15 pages, to appear in PTRF. Small changes in light of comments from\n  the referee", "journal-ref": "Probability Theory and Related Fields 153 (2012), 405--419", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical approach to accurately estimating the covariance matrix \\Sigma of\na p-variate normal distribution is to draw a sample of size n > p and form a\nsample covariance matrix. However, many modern applications operate with much\nsmaller sample sizes, thus calling for estimation guarantees in the regime n <<\np. We show that a sample of size n = O(m log^6 p) is sufficient to accurately\nestimate in operator norm an arbitrary symmetric part of \\Sigma consisting of m\n< n entries per row. This follows from a general result on estimating Hadamard\nproducts M.\\Sigma, where M is an arbitrary symmetric matrix.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 14:11:52 GMT"}, {"version": "v2", "created": "Fri, 11 Feb 2011 22:18:17 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Levina", "Elizaveta", ""], ["Vershynin", "Roman", ""]]}, {"id": "1008.1909", "submitter": "Djalel Eddine Meskaldji", "authors": "Djalel Eddine Meskaldji, Leila Cammoun, Patric Hagmann, Reto Meuli,\n  Jean Philippe Thiran and Stephan Morgenthaler", "title": "Efficient statistical analysis of large correlated multivariate\n  datasets: a case study on brain connectivity matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroimaging, a large number of correlated tests are routinely performed\nto detect active voxels in single-subject experiments or to detect regions that\ndiffer between individuals belonging to different groups. In order to bound the\nprobability of a false discovery of pair-wise differences, a Bonferroni or\nother correction for multiplicity is necessary. These corrections greatly\nreduce the power of the comparisons which means that small signals\n(differences) remain hidden and therefore have been more or less successful\ndepending on the application. We introduce a method that improves the power of\na family of correlated statistical tests by reducing their number in an orderly\nfashion using our a-priori understanding of the problem . The tests are grouped\nby blocks that respect the data structure and only one or a few tests per group\nare performed. For each block we construct an appropriate summary statistic\nthat characterizes a meaningful feature of the block. The comparisons are based\non these summary statistics by a block-wise approach. We contrast this method\nwith the one based on the individual measures in terms of power. Finally, we\napply the method to compare brain connectivity matrices. Although the method is\nused in this study on the particular case of imaging, the proposed strategy can\nbe applied to a large variety of problems that involves multiple comparisons\nwhen the tests can be grouped according to attributes that depend on the\nspecific problem. Keywords and phrases: Multiple comparisons ; Family-wise\nerror rate; False discovery rate; Bonferroni procedure; Human brain\nconnectivity; Brain connectivity matrices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 14:01:13 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Meskaldji", "Djalel Eddine", ""], ["Cammoun", "Leila", ""], ["Hagmann", "Patric", ""], ["Meuli", "Reto", ""], ["Thiran", "Jean Philippe", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "1008.1924", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman, Yulia Gavrilov, Robert J. Adler", "title": "Peak Detection as Multiple Testing", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detecting equal-shaped non-overlapping\nunimodal peaks in the presence of Gaussian ergodic stationary noise, where the\nnumber, location and heights of the peaks are unknown. A multiple testing\napproach is proposed in which, after kernel smoothing, the presence of a peak\nis tested at each observed local maximum. The procedure provides strong control\nof the family wise error rate and the false discovery rate asymptotically as\nboth the signal-to-noise ratio (SNR) and the search space get large, where the\nsearch space may grow exponentially as a function of SNR. Simulations assuming\na Gaussian peak shape and a Gaussian autocorrelation function show that desired\nerror levels are achieved for relatively low SNR and are robust to partial peak\noverlap. Simulations also show that detection power is maximized when the\nsmoothing bandwidth is close to the bandwidth of the signal peaks, akin to the\nwell-known matched filter theorem in signal processing. The procedure is\nillustrated in an analysis of electrical recordings of neuronal cell activity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 14:52:33 GMT"}], "update_date": "2010-08-12", "authors_parsed": [["Schwartzman", "Armin", ""], ["Gavrilov", "Yulia", ""], ["Adler", "Robert J.", ""]]}, {"id": "1008.2145", "submitter": "Enzo Orsingher", "authors": "Enzo Orsingher, Federico Polito", "title": "Fractional pure birth processes", "comments": "Published in at http://dx.doi.org/10.3150/09-BEJ235 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2010, Vol. 16, No. 3, 858-881", "doi": "10.3150/09-BEJ235", "report-no": "IMS-BEJ-BEJ235", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fractional version of the classical nonlinear birth process of\nwhich the Yule--Furry model is a particular case. Fractionality is obtained by\nreplacing the first order time derivative in the difference-differential\nequations which govern the probability law of the process with the\nDzherbashyan--Caputo fractional derivative. We derive the probability\ndistribution of the number $\\mathcal{N}_{\\nu}(t)$ of individuals at an\narbitrary time $t$. We also present an interesting representation for the\nnumber of individuals at time $t$, in the form of the subordination relation\n$\\mathcal{N}_{\\nu}(t)=\\mathcal{N}(T_{2\\nu}(t))$, where $\\mathcal{N}(t)$ is the\nclassical generalized birth process and $T_{2\\nu}(t)$ is a random time whose\ndistribution is related to the fractional diffusion equation. The fractional\nlinear birth process is examined in detail in Section 3 and various forms of\nits distribution are given and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 15:36:57 GMT"}, {"version": "v2", "created": "Mon, 14 Feb 2011 14:50:03 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Orsingher", "Enzo", ""], ["Polito", "Federico", ""]]}, {"id": "1008.2277", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Faithfulness in Chain Graphs: The Gaussian Case", "comments": null, "journal-ref": "Proceedings of the 14th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2011), 588-599", "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the classic\nLauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian\ndistributions that factorize with respect to a chain graph $G$ with $d$\nparameters have positive Lebesgue measure with respect to $\\mathbb{R}^d$,\nwhereas those that factorize with respect to $G$ but are not faithful to it\nhave zero Lebesgue measure with respect to $\\mathbb{R}^d$. This means that, in\nthe measure-theoretic sense described, almost all the regular Gaussian\ndistributions that factorize with respect to $G$ are faithful to it.\n", "versions": [{"version": "v1", "created": "Fri, 13 Aug 2010 10:03:48 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1008.2471", "submitter": "Jacques Touboul", "authors": "Jacques Touboul (LSTA)", "title": "Projection Pursuit through Relative Entropy Minimization", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection Pursuit methodology permits to solve the difficult problem of\nfinding an estimate of a density defined on a set of very large dimension. In\nhis seminal article, Huber (see \"Projection pursuit\", Annals of Statistics,\n1985) evidences the interest of the Projection Pursuit method thanks to the\nfactorisation of a density into a Gaussian component and some residual density\nin a context of Kullback-Leibler divergence maximisation. In the present\narticle, we introduce a new algorithm, and in particular a test for the\nfactorisation of a density estimated from an iid sample.\n", "versions": [{"version": "v1", "created": "Sat, 14 Aug 2010 20:20:34 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Touboul", "Jacques", "", "LSTA"]]}, {"id": "1008.2581", "submitter": "Andrea Montanari", "authors": "Mohsen Bayati and Andrea Montanari", "title": "The LASSO risk for gaussian matrices", "comments": "43 pages, 5 figures (v3 rectifies some inconsistencies in the\n  formulation of auxiliary lemmas)", "journal-ref": "IEEE Transactions on Information Theory, Vol 587, Issue 4 pp.\n  1997-2017, 2012", "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a coefficient vector x_0\\in R^N from\nnoisy linear observation y=Ax_0+w \\in R^n. In many contexts (ranging from model\nselection to image processing) it is desirable to construct a sparse estimator\nx'. In this case, a popular approach consists in solving an L1-penalized least\nsquares problem known as the LASSO or Basis Pursuit DeNoising (BPDN).\n  For sequences of matrices A of increasing dimensions, with independent\ngaussian entries, we prove that the normalized risk of the LASSO converges to a\nlimit, and we obtain an explicit expression for this limit. Our result is the\nfirst rigorous derivation of an explicit formula for the asymptotic mean square\nerror of the LASSO for random instances. The proof technique is based on the\nanalysis of AMP, a recently developed efficient algorithm, that is inspired\nfrom graphical models ideas.\n  Simulations on real data matrices suggest that our results can be relevant in\na broad array of practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 06:41:05 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2012 18:48:07 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 18:01:53 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Bayati", "Mohsen", ""], ["Montanari", "Andrea", ""]]}, {"id": "1008.2639", "submitter": "Bikramjit Das", "authors": "Bikramjit Das, Souvik Ghosh", "title": "Weak limits for exploratory plots in the analysis of extremes", "comments": "Published in at http://dx.doi.org/10.3150/11-BEJ401 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 1, 308-343", "doi": "10.3150/11-BEJ401", "report-no": "IMS-BEJ-BEJ401", "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory data analysis is often used to test the goodness-of-fit of sample\nobservations to specific target distributions. A few such graphical tools have\nbeen extensively used to detect subexponential or heavy-tailed behavior in\nobserved data. In this paper we discuss asymptotic limit behavior of two such\nplotting tools: the quantile-quantile plot and the mean excess plot. The weak\nconsistency of these plots to fixed limit sets in an appropriate topology of\n$\\mathbb{R}^2$ has been shown in Das and Resnick (Stoch. Models 24 (2008)\n103-132) and Ghosh and Resnick (Stochastic Process. Appl. 120 (2010)\n1492-1517). In this paper we find asymptotic distributional limits for these\nplots when the underlying distributions have regularly varying right-tails. As\nan application we construct confidence bounds around the plots which enable us\nto statistically test whether the underlying distribution is heavy-tailed or\nnot.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 12:53:27 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 13:24:39 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Das", "Bikramjit", ""], ["Ghosh", "Souvik", ""]]}, {"id": "1008.2697", "submitter": "James Kuelbs", "authors": "James Kuelbs, Thomas Kurtz, Joel Zinn", "title": "A CLT for empirical processes involving time-dependent data", "comments": "Published in at http://dx.doi.org/10.1214/11-AOP711 the Annals of\n  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Probability 2013, Vol. 41, No. 2, 785-816", "doi": "10.1214/11-AOP711", "report-no": "IMS-AOP-AOP711", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For stochastic processes $\\{X_t:t\\in E\\}$, we establish sufficient conditions\nfor the empirical process based on $\\{I_{X_t\\le y}-\\operatorname{Pr}(X_t\\le\ny):t\\in E,y\\in\\mathbb{R}\\}$ to satisfy the CLT uniformly in $t\\in\nE,y\\in\\mathbb{R}$. Corollaries of our main result include examples of classical\nprocesses where the CLT holds, and we also show that it fails for Brownian\nmotion tied down at zero and $E=[0,1]$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 15:52:39 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2013 11:55:22 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Kuelbs", "James", ""], ["Kurtz", "Thomas", ""], ["Zinn", "Joel", ""]]}, {"id": "1008.2732", "submitter": "Nirian Martin", "authors": "Nirian Martin and Leandro Pardo", "title": "Poisson loglinear modeling with linear constraints on the expected cell\n  frequencies", "comments": null, "journal-ref": null, "doi": "10.1007/s13571-012-0035-2", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider Poisson loglinear models with linear constraints\n(LMLC) on the expected table counts. Multinomial and product multinomial\nloglinear models can be obtained by considering that some marginal totals\n(linear constraints on the expected table counts) have been prefixed in a\nPoisson loglinear model. Therefore with the theory developed in this paper,\nmultinomial and product multinomial loglinear models can be considered as a\nparticular case. To carry out inferences on the parameters in the LMLC an\ninformation-theoretic approach is followed from which the classical maximum\nlikelihood estimators and Pearson chi-square statistics for goodness-of fit are\nobtained. In addition, nested hypotheses are proposed as a general procedure\nfor hypothesis testing. Through a simulation study the appropriateness of\nproposed inference tools is illustrated.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 18:25:17 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1008.2780", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega", "title": "Logic, Reasoning under Uncertainty and Causality", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple framework for reasoning under uncertainty and intervention is\nintroduced. This is achieved in three steps. First, logic is restated in\nset-theoretic terms to obtain a framework for reasoning under certainty.\nSecond, this framework is extended to model reasoning under uncertainty.\nFinally, causal spaces are introduced and shown how they provide enough\ninformation to model knowledge containing causal information about the world.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 21:55:37 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Ortega", "Pedro A.", ""]]}, {"id": "1008.2886", "submitter": "Jonas Str\\\"ojby", "authors": "Jimmy Olsson and Jonas Str\\\"ojby", "title": "Particle-based likelihood inference in partially observed diffusion\n  processes using generalised Poisson estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the use of the expectation-maximisation (EM) algorithm\nfor inference in partially observed diffusion processes. In this context, a\nwell known problem is that all except a few diffusion processes lack\nclosed-form expressions of the transition densities. Thus, in order to estimate\nefficiently the EM intermediate quantity we construct, using novel techniques\nfor unbiased estimation of diffusion transition densities, a random weight\nfixed-lag auxiliary particle smoother, which avoids the well known problem of\nparticle trajectory degeneracy in the smoothing mode. The estimator is\njustified theoretically and demonstrated on a simulated example.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 13:17:24 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Olsson", "Jimmy", ""], ["Str\u00f6jby", "Jonas", ""]]}, {"id": "1008.3229", "submitter": "Julien Worms", "authors": "Julien Worms (LMV), Rym Worms (LAMA)", "title": "Empirical Likelihood based Confidence Regions for first order parameters\n  of a heavy tailed distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1, \\ldots, X_n$ be some i.i.d. observations from a heavy tailed\ndistribution $F$, i.e. such that the common distribution of the excesses over a\nhigh threshold $u_n$ can be approximated by a Generalized Pareto Distribution\n$G_{\\gamma,\\sigma_n}$ with $\\gamma >0$. This work is devoted to the problem of\nfinding confidence regions for the couple $(\\gamma,\\sigma_n)$ : combining the\nempirical likelihood methodology with estimation equations (close but not\nidentical to the likelihood equations) introduced by J. Zhang (Australian and\nNew Zealand J. Stat n.49(1), 2007), asymptotically valid confidence regions for\n$(\\gamma,\\sigma_n)$ are obtained and proved to perform better than Wald-type\nconfidence regions (especially those derived from the asymptotic normality of\nthe maximum likelihood estimators). By profiling out the scale parameter,\nconfidence intervals for the tail index are also derived.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 07:06:10 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Worms", "Julien", "", "LMV"], ["Worms", "Rym", "", "LAMA"]]}, {"id": "1008.3514", "submitter": "Nawaf Bou-Rabee", "authors": "Nawaf Bou-Rabee, Martin Hairer, Eric Vanden-Eijnden", "title": "Non-asymptotic mixing of the MALA algorithm", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-Adjusted Langevin Algorithm (MALA), originally introduced to\nsample exactly the invariant measure of certain stochastic differential\nequations (SDE) on infinitely long time intervals, can also be used to\napproximate pathwise the solution of these SDEs on finite time intervals.\nHowever, when applied to an SDE with a nonglobally Lipschitz drift coefficient,\nthe algorithm may not have a spectral gap even when the SDE does. This paper\nreconciles MALA's lack of a spectral gap with its ergodicity to the invariant\nmeasure of the SDE and finite time accuracy. In particular, the paper shows\nthat its convergence to equilibrium happens at exponential rate up to terms\nexponentially small in time-stepsize. This quantification relies on MALA's\nability to exactly preserve the SDE's invariant measure and accurately\nrepresent the SDE's transition probability on finite time intervals.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 14:25:30 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Bou-Rabee", "Nawaf", ""], ["Hairer", "Martin", ""], ["Vanden-Eijnden", "Eric", ""]]}, {"id": "1008.3651", "submitter": "Anatoli Iouditski", "authors": "Anatoli Iouditski (LJK), Arkadii S. Nemirovski (ISyE)", "title": "Accuracy guarantees for L1-recovery", "comments": null, "journal-ref": "IEEE Transactions on Information Theory 57, 12 (2011) 7818 - 7839", "doi": "10.1109/TIT.2011.2162569", "report-no": null, "categories": "math.ST cs.SY math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss two new methods of recovery of sparse signals from noisy\nobservation based on $\\ell_1$- minimization. They are closely related to the\nwell-known techniques such as Lasso and Dantzig Selector. However, these\nestimators come with efficiently verifiable guaranties of performance. By\noptimizing these bounds with respect to the method parameters we are able to\nconstruct the estimators which possess better statistical properties than the\ncommonly used ones. We also show how these techniques allow to provide\nefficiently computable accuracy bounds for Lasso and Dantzig Selector. We link\nour performance estimations to the well known results of Compressive Sensing\nand justify our proposed approach with an oracle inequality which links the\nproperties of the recovery algorithms and the best estimation performance when\nthe signal support is known. We demonstrate how the estimates can be computed\nusing the Non-Euclidean Basis Pursuit algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 21 Aug 2010 18:32:03 GMT"}, {"version": "v2", "created": "Fri, 29 Oct 2010 15:03:42 GMT"}, {"version": "v3", "created": "Fri, 27 May 2011 13:43:50 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Iouditski", "Anatoli", "", "LJK"], ["Nemirovski", "Arkadii S.", "", "ISyE"]]}, {"id": "1008.3654", "submitter": "Martin Wainwright", "authors": "Garvesh Raskutti, Martin J. Wainwright, Bin Yu", "title": "Minimax-optimal rates for sparse additive models over kernel classes via\n  convex programming", "comments": "Lower bounds presented in part in Proceedings of the NIPS Conference,\n  December 2009 Revised version December 2011: new Theorem 3, showing\n  restrictiveness of global boundedness condition. Sharper version of Theorem\n  1, with rates in both empirical and population norm", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse additive models are families of $d$-variate functions that have the\nadditive decomposition $f^* = \\sum_{j \\in S} f^*_j$, where $S$ is an unknown\nsubset of cardinality $s \\ll d$. In this paper, we consider the case where each\nunivariate component function $f^*_j$ lies in a reproducing kernel Hilbert\nspace (RKHS), and analyze a method for estimating the unknown function $f^*$\nbased on kernels combined with $\\ell_1$-type convex regularization. Working\nwithin a high-dimensional framework that allows both the dimension $d$ and\nsparsity $s$ to increase with $n$, we derive convergence rates (upper bounds)\nin the $L^2(\\mathbb{P})$ and $L^2(\\mathbb{P}_n)$ norms over the class\n$\\MyBigClass$ of sparse additive models with each univariate function $f^*_j$\nin the unit ball of a univariate RKHS with bounded kernel function. We\ncomplement our upper bounds by deriving minimax lower bounds on the\n$L^2(\\mathbb{P})$ error, thereby showing the optimality of our method. Thus, we\nobtain optimal minimax rates for many interesting classes of sparse additive\nmodels, including polynomials, splines, and Sobolev classes. We also show that\nif, in contrast to our univariate conditions, the multivariate function class\nis assumed to be globally bounded, then much faster estimation rates are\npossible for any sparsity $s = \\Omega(\\sqrt{n})$, showing that global\nboundedness is a significant restriction in the high-dimensional setting.\n", "versions": [{"version": "v1", "created": "Sat, 21 Aug 2010 18:40:10 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2011 09:28:49 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Raskutti", "Garvesh", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1008.3954", "submitter": "Wang Zhou", "authors": "Guangming Pan, Qi-Man Shao, Wang Zhou", "title": "Central limit theorem of nonparametric estimate of spectral density\n  functions of sample covariance matrices", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A consistent kernel estimator of the limiting spectral distribution of\ngeneral sample covariance matrices was introduced in Jing, Pan, Shao and Zhou\n(2010). The central limit theorem of the kernel estimator is proved in this\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 03:14:37 GMT"}], "update_date": "2010-08-25", "authors_parsed": [["Pan", "Guangming", ""], ["Shao", "Qi-Man", ""], ["Zhou", "Wang", ""]]}, {"id": "1008.4059", "submitter": "Thomas Royen", "authors": "Thomas Royen", "title": "The distribution of the square sum of Dirichlet random variables and a\n  table with quantiles of Greenwood's statistic", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact distribution of the square sum of Dirichlet random variables is\ngiven by two different univariate integral representations. Alternatively,\nthree representations by orthogonal series with Jacobi or Legendre polynomials\nare derived. As a special case the distribution of the square sum of spacings -\nalso called Greenwood's statistic - is obtained. Nine quantiles of this\nstatistic are tabulated with eight digits where the number of squares ranges\nfrom 10 to 100.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 14:47:38 GMT"}], "update_date": "2010-08-25", "authors_parsed": [["Royen", "Thomas", ""]]}, {"id": "1008.4441", "submitter": "Sylvain Corlay", "authors": "Sylvain Corlay (LPMA), Gilles Pag\\`es (LPMA)", "title": "Functional quantization-based stratified sampling methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose several quantization-based stratified sampling\nmethods to reduce the variance of a Monte Carlo simulation. Theoretical aspects\nof stratification lead to a strong link between optimal quadratic quantization\nand the variance reduction that can be achieved with stratified sampling. We\nfirst put the emphasis on the consistency of quantization for partitioning the\nstate space in stratified sampling methods in both finite and infinite\ndimensional cases. We show that the proposed quantization-based strata design\nhas uniform efficiency among the class of Lipschitz continuous functionals.\nThen a stratified sampling algorithm based on product functional quantization\nis proposed for path-dependent functionals of multi-factor diffusions. The\nmethod is also available for other Gaussian processes such as Brownian bridge\nor Ornstein-Uhlenbeck processes. We derive in detail the case of\nOrnstein-Uhlenbeck processes. We also study the balance between the algorithmic\ncomplexity of the simulation and the variance reduction factor\n", "versions": [{"version": "v1", "created": "Thu, 26 Aug 2010 06:56:50 GMT"}, {"version": "v2", "created": "Sat, 4 Oct 2014 19:23:40 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Corlay", "Sylvain", "", "LPMA"], ["Pag\u00e8s", "Gilles", "", "LPMA"]]}, {"id": "1008.4831", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, John Skilling", "title": "Foundations of Inference", "comments": "This updated version of the paper has been published in the journal\n  Axioms (please see journal reference). 28 pages, 9 figures", "journal-ref": "Axioms 2012, 1(1):38-73", "doi": "10.3390/axioms1010038", "report-no": null, "categories": "math.PR cs.AI math.LO math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and clear foundation for finite inference that unites and\nsignificantly extends the approaches of Kolmogorov and Cox. Our approach is\nbased on quantifying lattices of logical statements in a way that satisfies\ngeneral lattice symmetries. With other applications such as measure theory in\nmind, our derivations assume minimal symmetries, relying on neither negation\nnor continuity nor differentiability. Each relevant symmetry corresponds to an\naxiom of quantification, and these axioms are used to derive a unique set of\nquantifying rules that form the familiar probability calculus. We also derive a\nunique quantification of divergence, entropy and information.\n", "versions": [{"version": "v1", "created": "Sat, 28 Aug 2010 04:37:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 05:11:39 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Skilling", "John", ""]]}, {"id": "1008.4877", "submitter": "Maurice de Gosson Dr", "authors": "Maurice de Gosson", "title": "On the Use of Minimum Volume Ellipsoids and Symplectic Capacities for\n  Studying Classical Uncertainties for Joint Position-Momentum Measurements", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/2010/11/P11005", "report-no": null, "categories": "math-ph math.MP math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimum volume ellipsoid estimator associates to a cloud of\npoints in phase space. Using as a natural measure of uncertainty the symplectic\ncapacity of the covariance ellipsoid we find that classical uncertainties obey\nrelations similar to those found in non-standard quantum mechanics.\n", "versions": [{"version": "v1", "created": "Sat, 28 Aug 2010 16:54:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["de Gosson", "Maurice", ""]]}, {"id": "1008.4886", "submitter": "Stephane Gaiffas", "authors": "St\\'ephane Ga\\\"iffas (LSTA), Guillaume Lecu\\'e", "title": "Sharp oracle inequalities for the prediction of a high-dimensional\n  matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe $(X_i,Y_i)_{i=1}^n$ where the $Y_i$'s are real valued outputs and\nthe $X_i$'s are $m\\times T$ matrices. We observe a new entry $X$ and we want to\npredict the output $Y$ associated with it. We focus on the high-dimensional\nsetting, where $m T \\gg n$. This includes the matrix completion problem with\nnoise, as well as other problems. We consider linear prediction procedures\nbased on different penalizations, involving a mixture of several norms: the\nnuclear norm, the Frobenius norm and the $\\ell_1$-norm. For these procedures,\nwe prove sharp oracle inequalities, using a statistical learning theory point\nof view. A surprising fact in our results is that the rates of convergence do\nnot depend on $m$ and $T$ directly. The analysis is conducted without the\nusually considered incoherency condition on the unknown matrix or restricted\nisometry condition on the sampling operator. Moreover, our results are the\nfirst to give for this problem an analysis of penalization (such nuclear norm\npenalization) as a regularization algorithm: our oracle inequalities prove that\nthese procedures have a prediction accuracy close to the deterministic oracle\none, given that the reguralization parameters are well-chosen.\n", "versions": [{"version": "v1", "created": "Sat, 28 Aug 2010 19:38:20 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Ga\u00efffas", "St\u00e9phane", "", "LSTA"], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "1008.5319", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "On two simple tests for normality with high power", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test statistics of two powerful tests for normality \\citep{lm1,mud2} are\nestimators of the correlation coefficient between certain sample moments. We\nderive new versions of the test statistics that are functions of the sample\nskewness and sample kurtosis. This sheds some light on the nature of these\ntests and leads to easier computations.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 14:01:59 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2011 12:29:42 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2011 06:20:05 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1008.5374", "submitter": "Magnus Fontes Mr.", "authors": "Magnus Fontes", "title": "Statistical and knowledge supported visualization of multivariate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work we have selected a collection of statistical and\nmathematical tools useful for the exploration of multivariate data and we\npresent them in a form that is meant to be particularly accessible to a\nclassically trained mathematician. We give self contained and streamlined\nintroductions to principal component analysis, multidimensional scaling and\nstatistical hypothesis testing. Within the presented mathematical framework we\nthen propose a general exploratory methodology for the investigation of real\nworld high dimensional datasets that builds on statistical and knowledge\nsupported visualizations. We exemplify the proposed methodology by applying it\nto several different genomewide DNA-microarray datasets. The exploratory\nmethodology should be seen as an embryo that can be expanded and developed in\nmany directions. As an example we point out some recent promising advances in\nthe theory for random matrices that, if further developed, potentially could\nprovide practically useful and theoretically well founded estimations of\ninformation content in dimension reducing visualizations. We hope that the\npresent work can serve as an introduction to, and help to stimulate more\nresearch within, the interesting and rapidly expanding field of data\nexploration.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 17:30:06 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Fontes", "Magnus", ""]]}]