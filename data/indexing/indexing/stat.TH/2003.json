[{"id": "2003.00083", "submitter": "Heejong Bong", "authors": "Heejong Bong, Wanshan Li, Shamindra Shrotriya, Alessandro Rinaldo", "title": "Nonparametric Estimation in the Dynamic Bradley-Terry Model", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a time-varying generalization of the Bradley-Terry model that\nallows for nonparametric modeling of dynamic global rankings of distinct teams.\nWe develop a novel estimator that relies on kernel smoothing to pre-process the\npairwise comparisons over time and is applicable in sparse settings where the\nBradley-Terry may not be fit. We obtain necessary and sufficient conditions for\nthe existence and uniqueness of our estimator. We also derive time-varying\noracle bounds for both the estimation error and the excess risk in the\nmodel-agnostic setting where the Bradley-Terry model is not necessarily the\ntrue data generating process. We thoroughly test the practical effectiveness of\nour model using both simulated and real world data and suggest an efficient\ndata-driven approach for bandwidth tuning.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 21:52:49 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bong", "Heejong", ""], ["Li", "Wanshan", ""], ["Shrotriya", "Shamindra", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2003.00116", "submitter": "Aliasghar Tarkhan", "authors": "Aliasghar Tarkhan and Noah Simon", "title": "BigSurvSGD: Big Survival Data Analysis via Stochastic Gradient Descent", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many biomedical applications, outcome is measured as a ``time-to-event''\n(eg. disease progression or death). To assess the connection between features\nof a patient and this outcome, it is common to assume a proportional hazards\nmodel, and fit a proportional hazards regression (or Cox regression). To fit\nthis model, a log-concave objective function known as the ``partial\nlikelihood'' is maximized. For moderate-sized datasets, an efficient\nNewton-Raphson algorithm that leverages the structure of the objective can be\nemployed. However, in large datasets this approach has two issues: 1) The\ncomputational tricks that leverage structure can also lead to computational\ninstability; 2) The objective does not naturally decouple: Thus, if the dataset\ndoes not fit in memory, the model can be very computationally expensive to fit.\nThis additionally means that the objective is not directly amenable to\nstochastic gradient-based optimization methods. To overcome these issues, we\npropose a simple, new framing of proportional hazards regression: This results\nin an objective function that is amenable to stochastic gradient descent. We\nshow that this simple modification allows us to efficiently fit survival models\nwith very large datasets. This also facilitates training complex, eg.\nneural-network-based, models with survival data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:19:47 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 00:56:57 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Tarkhan", "Aliasghar", ""], ["Simon", "Noah", ""]]}, {"id": "2003.00362", "submitter": "Johannes Ernst-Emanuel Buck", "authors": "Johannes Buck, Claudia Kl\\\"uppelberg", "title": "Recursive max-linear models with propagating noise", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive max-linear vectors model causal dependence between node variables\nby a structural equation model, expressing each node variable as a max-linear\nfunction of its parental nodes in a directed acyclic graph (DAG) and some\nexogenous innovation. For such a model, there exists a unique minimum DAG,\nrepresented by the Kleene star matrix of its edge weight matrix, which\nidentifies the model and can be estimated. For a more realistic statistical\nmodeling we introduce some random observational noise. A probabilistic analysis\nof this new noisy model reveals that the unique minimum DAG representing the\ndistribution of the non-noisy model remains unchanged and identifiable.\nMoreover, the distribution of the minimum ratio estimators of the model\nparameters at their left limits are completely determined by the distribution\nof the noise variables up to a positive constant. Under a regular variation\ncondition on the noise variables we prove that the estimated Kleene star matrix\nconverges to a matrix of independent Weibull entries after proper centering and\nscaling.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 23:18:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Buck", "Johannes", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "2003.00377", "submitter": "Yang Li", "authors": "Yang Li, Jinqiao Duan, Xianbin Liu and Yanxia Zhang", "title": "Most Probable Dynamics of Stochastic Dynamical Systems with\n  Exponentially Light Jump Fluctuations", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": "10.1063/5.0006292", "report-no": null, "categories": "math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of the exit events from a bounded domain containing a stable\nfixed point induced by non-Gaussian L\\'evy fluctuations plays a pivotal role in\npractical physical systems. In the limit of weak noise, we develop a\nHamiltonian formalism under the L\\'evy fluctuations with exponentially light\njumps for one- and two-dimensional stochastic dynamical systems. This formalism\nis based on a recently proved large deviation principle for dynamical systems\nunder non-Gaussian L\\'evy perturbations. We demonstrate how to compute the most\nprobable exit path and the quasi-potential by several examples. Meanwhile, we\nexplore the impacts of the jump measure on the quasi-potential quantitatively\nand on the most probable exit path qualitatively. Results show that the\nquasi-potential can be well estimated by an approximate analytical expression.\nMoreover, we discover that although the most probable exit paths are analogous\nto the Gaussian case for the isotropic noise, the anisotropic noise leads to\nsignificant changes of the structure of the exit paths. These findings shed\nlight on the underlying qualitative mechanism and quantitative feature of the\nexit phenomenon induced by non-Gaussian noise.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 01:40:21 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Yang", ""], ["Duan", "Jinqiao", ""], ["Liu", "Xianbin", ""], ["Zhang", "Yanxia", ""]]}, {"id": "2003.00520", "submitter": "Credo Vovor Dassu", "authors": "G.R. Ducharme, S. Kaci, C. Vovor-Dassu", "title": "Smooths Tests of Goodness-of-fit for the Newcomb-Benford distribution", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Newcomb-Benford probability distribution is becoming very popular in many\nareas using statistics, notably in fraud detection. In such contexts, it is\nimportant to be able to determine if a data set arises from this distribution\nwhile controlling the risk of a Type 1 error, i.e. falsely identifying a fraud,\nand a Type 2 error, i.e. not detecting that a fraud occurred. The statistical\ntool to do this work is a goodness-of-fit test. For the Newcomb-Benford\ndistribution, the most popular such test is Pearson's chi-square test whose\npower, related to the Type 2 error, is known to be weak. Consequently, other\ntests have been recently introduced. The goal of the present work is to build\nnew goodness-of-fit tests for this distribution, based on the smooth test\nprinciple. These tests are then compared to some of their competitors. It turns\nout that the proposals of the paper are globally preferable to existing tests\nand should be seriously considered in fraud detection contexts, among others.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 16:51:34 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ducharme", "G. R.", ""], ["Kaci", "S.", ""], ["Vovor-Dassu", "C.", ""]]}, {"id": "2003.00570", "submitter": "Subhabrata Sen", "authors": "Rajarshi Mukherjee, Subhabrata Sen", "title": "On Minimax Exponents of Sparse Testing", "comments": "53 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exact asymptotics of the minimax risk for global testing against\nsparse alternatives in the context of high dimensional linear regression. Our\nresults characterize the leading order behavior of this minimax risk in several\nregimes, uncovering new phase transitions in its behavior. This complements a\nvast literature characterizing asymptotic consistency in this problem, and\nprovides a useful benchmark, against which the performance of specific tests\nmay be compared. Finally, we provide some preliminary evidence that popular\nsparsity adaptive procedures might be sub-optimal in terms of the minimax risk.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 19:55:58 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Sen", "Subhabrata", ""]]}, {"id": "2003.00973", "submitter": "Debabrota Basu", "authors": "Ashish Dandekar, Debabrota Basu, Stephane Bressan", "title": "Differential Privacy at Risk: Bridging Randomness and Privacy Budget", "comments": "Presented in Workshop on Privacy Preserving AI (PPAI) at AAAI, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The calibration of noise for a privacy-preserving mechanism depends on the\nsensitivity of the query and the prescribed privacy level. A data steward must\nmake the non-trivial choice of a privacy level that balances the requirements\nof users and the monetary constraints of the business entity. We analyse roles\nof the sources of randomness, namely the explicit randomness induced by the\nnoise distribution and the implicit randomness induced by the data-generation\ndistribution, that are involved in the design of a privacy-preserving\nmechanism. The finer analysis enables us to provide stronger privacy guarantees\nwith quantifiable risks. Thus, we propose privacy at risk that is a\nprobabilistic calibration of privacy-preserving mechanisms. We provide a\ncomposition theorem that leverages privacy at risk. We instantiate the\nprobabilistic calibration for the Laplace mechanism by providing analytical\nresults. We also propose a cost model that bridges the gap between the privacy\nlevel and the compensation budget estimated by a GDPR compliant business\nentity. The convexity of the proposed cost model leads to a unique fine-tuning\nof privacy level that minimises the compensation budget. We show its\neffectiveness by illustrating a realistic scenario that avoids overestimation\nof the compensation budget by using privacy at risk for the Laplace mechanism.\nWe quantitatively show that composition using the cost optimal privacy at risk\nprovides stronger privacy guarantee than the classical advanced composition.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:44:14 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 23:14:27 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dandekar", "Ashish", ""], ["Basu", "Debabrota", ""], ["Bressan", "Stephane", ""]]}, {"id": "2003.01016", "submitter": "Holger Drees", "authors": "Holger Drees, Sebastian Neblung", "title": "Asymptotics for sliding blocks estimators of rare events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drees and Rootz\\'en (2010) have established limit theorems for a general\nclass of empirical processes of statistics that are useful for the extreme\nvalue analysis of time series, but do not apply to statistics of sliding\nblocks, including so-called runs estimators. We generalize these results to\nempirical processes which cover both the class considered by Drees and\nRootz\\'en (2010) and processes of sliding blocks statistics. Using this\napproach, one can analyze different types of statistics in a unified framework.\nWe show that statistics based on sliding blocks are asymptotically normal with\nan asymptotic variance which, under rather mild conditions, is smaller than or\nequal to the asymptotic variance of the corresponding estimator based on\ndisjoint blocks. Finally, the general theory is applied to three well-known\nestimators of the extremal index. It turns out that they all have the same\nlimit distribution, a fact which has so far been overlooked in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:48:41 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 08:27:13 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Drees", "Holger", ""], ["Neblung", "Sebastian", ""]]}, {"id": "2003.01027", "submitter": "Enrico Scalas", "authors": "J. Chen, A.G. Hawkes, E. Scalas", "title": "A Fractional Hawkes process", "comments": "13 pages, 3 figures, submitted to Nonlocal and Fractional Operators,\n  Proceedings of a Conference held in Rome (12-13 April 2019), Festschrift in\n  honour of Renato Spigler, published in SEMA SIMAI Springer Series and edited\n  by Roberto Garrappa (University of Bari), Francesco Mainardi (University of\n  Bologna) and Luisa Beghin (University of Rome)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We modify ETAS models by replacing the Pareto-like kernel proposed by Ogata\nwith a Mittag-Leffler type kernel. Provided that the kernel decays as a power\nlaw with exponent $\\beta + 1 \\in (1,2]$, this replacement has the advantage\nthat the Laplace transform of the Mittag-Leffler function is known explicitly,\nleading to simpler calculation of relevant quantities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:00:53 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "J.", ""], ["Hawkes", "A. G.", ""], ["Scalas", "E.", ""]]}, {"id": "2003.01291", "submitter": "Timo Welti", "authors": "Arnulf Jentzen and Timo Welti", "title": "Overall error analysis for the training of deep neural networks via\n  stochastic gradient descent with random initialisation", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the accomplishments of deep learning based algorithms in numerous\napplications and very broad corresponding research interest, at the moment\nthere is still no rigorous understanding of the reasons why such algorithms\nproduce useful results in certain situations. A thorough mathematical analysis\nof deep learning based algorithms seems to be crucial in order to improve our\nunderstanding and to make their implementation more effective and efficient. In\nthis article we provide a mathematically rigorous full error analysis of deep\nlearning based empirical risk minimisation with quadratic loss function in the\nprobabilistically strong sense, where the underlying deep neural networks are\ntrained using stochastic gradient descent with random initialisation. The\nconvergence speed we obtain is presumably far from optimal and suffers under\nthe curse of dimensionality. To the best of our knowledge, we establish,\nhowever, the first full error analysis in the scientific literature for a deep\nlearning based algorithm in the probabilistically strong sense and, moreover,\nthe first full error analysis in the scientific literature for a deep learning\nbased algorithm where stochastic gradient descent with random initialisation is\nthe employed optimisation method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:41:17 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Jentzen", "Arnulf", ""], ["Welti", "Timo", ""]]}, {"id": "2003.01350", "submitter": "Guillaume Boglioni Beaulieu", "authors": "Benjamin Avanzi, Guillaume Boglioni Beaulieu, Pierre Lafaye de\n  Micheaux, Fr\\'ed\\'eric Ouimet, Bernard Wong", "title": "A counterexample to the central limit theorem for pairwise independent\n  random variables having a common arbitrary margin", "comments": "12 pages, 2 figures", "journal-ref": "J. Math. Anal. Appl. 499 (2021), no.1, 1-13", "doi": "10.1016/j.jmaa.2021.124982", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Central Limit Theorem (CLT) is one of the most fundamental results in\nstatistics. It states that the standardized sample mean of a sequence of $n$\nmutually independent and identically distributed random variables with finite\nfirst and second moments converges in distribution to a standard Gaussian as\n$n$ goes to infinity. In particular, pairwise independence of the sequence is\ngenerally not sufficient for the theorem to hold. We construct explicitly a\nsequence of pairwise independent random variables having a common but arbitrary\nmarginal distribution $F$ (satisfying very mild conditions) for which the CLT\nis not verified. We study the extent of this 'failure' of the CLT by obtaining,\nin closed form, the asymptotic distribution of the sample mean of our sequence.\nThis is illustrated through several theoretical examples, for which we provide\nassociated computing codes in the R language.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 06:04:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 06:21:31 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Beaulieu", "Guillaume Boglioni", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Ouimet", "Fr\u00e9d\u00e9ric", ""], ["Wong", "Bernard", ""]]}, {"id": "2003.01533", "submitter": "Ivan Iudice Ph.D.", "authors": "Donatella Darsena, Giacinto Gelli, Ivan Iudice and Francesco Verde", "title": "Design and performance analysis of channel estimators under pilot\n  spoofing attacks in multiple-antenna systems", "comments": "15 pages, 13 figures, accepted for publication in IEEE Transactions\n  on Information Forensics and Security", "journal-ref": null, "doi": "10.1109/TIFS.2020.2985548", "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple antenna systems employing time-division duplexing, spatial\nprecoder design at the base station (BS) leverages channel state information\nacquired through uplink pilot transmission, under the assumption of channel\nreciprocity. Malicious eavesdroppers can start pilot spoofing attacks to alter\nsuch design, so as to improve their eavesdropping performance in downlink. The\naim of this paper is to study the effects of pilot spoofing attacks on uplink\nchannel estimation, by assuming that the BS knows the angle of arrivals (AoAs)\nof the legitimate channels. Specifically, after assessing the performance of\nthe simple least squares estimator (LSE), we consider more sophisticated\nestimators, such as the maximum likelihood estimator (MLE) and different\nversions of the minimum mean square error estimator (MMSEE), involving\ndifferent degrees of a priori information about the pilot spoofing attacks.\nTheoretical analysis and numerical simulations are used to compare the\nperformance of such estimators. In particular, we analytically demonstrate that\nthe spoofing effects in the high signal-to-noise regime can be completely\nsuppressed, under certain conditions involving the AoAs of the legitimate and\nspoofing channels. Moreover, we show that even an imperfect knowledge of the\nAoAs and of the average transmission power of the spoofing signals allows the\nMLE and MMSEE to achieve significant performance gains over the LSE.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 14:32:39 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 10:26:00 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 17:33:02 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2020 09:35:30 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Darsena", "Donatella", ""], ["Gelli", "Giacinto", ""], ["Iudice", "Ivan", ""], ["Verde", "Francesco", ""]]}, {"id": "2003.01534", "submitter": "Ivan Iudice Ph.D.", "authors": "Donatella Darsena, Giacinto Gelli, Ivan Iudice and Francesco Verde", "title": "Separable MSE-based design of two-way multiple-relay cooperative MIMO 5G\n  networks", "comments": "13 pages, 4 figures, accepted for publication in MDPI Sensors", "journal-ref": null, "doi": "10.3390/s20216284", "report-no": null, "categories": "eess.SP math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the combination of multi-antenna and relaying techniques has been\nextensively studied for Long Term Evolution Advanced (LTE-A) and Internet of\nThings (IoT) applications, it is expected to still play an important role in\n5th Generation (5G) networks. However, the expected benefits of these\ntechnologies cannot be achieved without a proper system design. In this paper,\nwe consider the problem of jointly optimizing terminal precoders/decoders and\nrelay forwarding matrices on the basis of the sum mean square error (MSE)\ncriterion in multiple-input multiple-output (MIMO) two-way relay systems, where\ntwo multi-antenna nodes mutually exchange information via multi-antenna\namplify-and-forward relays. This problem is nonconvex and a local optimal\nsolution is typically found by using iterative algorithms based on alternating\noptimization. We show how the constrained minimization of the sum-MSE can be\nrelaxed to obtain two separated subproblems which, under mild conditions, admit\na closed-form solution. Compared to iterative approaches, the proposed design\nis more suited to be integrated in 5G networks, since it is computationally\nmore convenient and its performance exhibits a better scaling in the number of\nrelays.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 14:32:54 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 16:14:30 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 09:08:28 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2020 13:44:02 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Darsena", "Donatella", ""], ["Gelli", "Giacinto", ""], ["Iudice", "Ivan", ""], ["Verde", "Francesco", ""]]}, {"id": "2003.01772", "submitter": "Agn\\`es Lagnoux", "authors": "Fabrice Gamboa, Pierre Gremaud, Thierry Klein, Agn\\`es Lagnoux", "title": "Global Sensitivity Analysis: a new generation of mighty estimators based\n  on rank statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical estimation framework for a large family of\nglobal sensitivity analysis methods. Our approach is based on rank statistics\nand uses an empirical correlation coefficient recently introduced by Sourav\nChatterjee. We show how to apply this approach to compute not only the\nCram\\'er-von-Mises indices, which are directly related to Chatterjee's notion\nof correlation, but also Sobol indices at any order, higher-order moment\nindices, and Shapley effects. We establish consistency of the resulting\nestimators and demonstrate their numerical efficiency, especially for small\nsample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 20:16:32 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Gamboa", "Fabrice", ""], ["Gremaud", "Pierre", ""], ["Klein", "Thierry", ""], ["Lagnoux", "Agn\u00e8s", ""]]}, {"id": "2003.01803", "submitter": "Quanquan Gu", "authors": "Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu", "title": "MOTS: Minimax Optimal Thompson Sampling", "comments": "27 pages, 1 table, 2 figures. This version improves the presentation\n  in V2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is one of the most widely used algorithms for many online\ndecision problems, due to its simplicity in implementation and superior\nempirical performance over other state-of-the-art methods. Despite its\npopularity and empirical success, it has remained an open problem whether\nThompson sampling can match the minimax lower bound $\\Omega(\\sqrt{KT})$ for\n$K$-armed bandit problems, where $T$ is the total time horizon. In this paper,\nwe solve this long open problem by proposing a variant of Thompson sampling\ncalled MOTS that adaptively clips the sampling instance of the chosen arm at\neach time step. We prove that this simple variant of Thompson sampling achieves\nthe minimax optimal regret bound $O(\\sqrt{KT})$ for finite time horizon $T$, as\nwell as the asymptotic optimal regret bound for Gaussian rewards when $T$\napproaches infinity. To our knowledge, MOTS is the first Thompson sampling type\nalgorithm that achieves the minimax optimality for multi-armed bandit problems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 21:24:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 03:42:26 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 06:12:11 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Jin", "Tianyuan", ""], ["Xu", "Pan", ""], ["Shi", "Jieming", ""], ["Xiao", "Xiaokui", ""], ["Gu", "Quanquan", ""]]}, {"id": "2003.01827", "submitter": "Christophe Ley", "authors": "Christophe Ley", "title": "Gauss and the identity function -- a tale of characterizations of the\n  normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normal distribution is well-known for several results that it is the only\nto fulfil. The aim of the present paper is to show that many of these\ncharacterizations actually follow from the fact that the derivative of the\nlog-density of the normal distribution is the (negative) identity function.\nThis \\emph{a priori} very simple yet surprising observation allows a deeper\nunderstanding of existing characterizations and paves the way to an immediate\nextension to a general density $x\\mapsto p(x)$ by replacing $-x$ in these\nresults with $(\\log p(x))'$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 23:04:43 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 07:49:06 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ley", "Christophe", ""]]}, {"id": "2003.01856", "submitter": "Hongxiang Qiu", "authors": "Hongxiang Qiu, Alex Luedtke, Marco Carone", "title": "Universal sieve-based strategies for efficient estimation using machine\n  learning tools", "comments": "46 pages, 6 figures, submitted to Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we wish to estimate a finite-dimensional summary of one or more\nfunction-valued features of an underlying data-generating mechanism under a\nnonparametric model. One approach to estimation is by plugging in flexible\nestimates of these features. Unfortunately, in general, such estimators may not\nbe asymptotically efficient, which often makes these estimators difficult to\nuse as a basis for inference. Though there are several existing methods to\nconstruct asymptotically efficient plug-in estimators, each such method either\ncan only be derived using knowledge of efficiency theory or is only valid under\nstringent smoothness assumptions. Among existing methods, sieve estimators\nstand out as particularly convenient because efficiency theory is not required\nin their construction, their tuning parameters can be selected data adaptively,\nand they are universal in the sense that the same fits lead to efficient\nplug-in estimators for a rich class of estimands. Inspired by these desirable\nproperties, we propose two novel universal approaches for estimating\nfunction-valued features that can be analyzed using sieve estimation theory.\nCompared to traditional sieve estimators, these approaches are valid under more\ngeneral conditions on the smoothness of the function-valued features by\nutilizing flexible estimates that can be obtained, for example, using machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:59:46 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 23:36:33 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Qiu", "Hongxiang", ""], ["Luedtke", "Alex", ""], ["Carone", "Marco", ""]]}, {"id": "2003.01897", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Prayaag Venkat, Sham Kakade, Tengyu Ma", "title": "Optimal Regularization Can Mitigate Double Descent", "comments": "v2: Accepted to ICLR 2021. Minor edits to Intro and Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical and theoretical studies have shown that many learning\nalgorithms -- from linear regression to neural networks -- can have test\nperformance that is non-monotonic in quantities such the sample size and model\nsize. This striking phenomenon, often referred to as \"double descent\", has\nraised questions of if we need to re-think our current understanding of\ngeneralization. In this work, we study whether the double-descent phenomenon\ncan be avoided by using optimal regularization. Theoretically, we prove that\nfor certain linear regression models with isotropic data distribution,\noptimally-tuned $\\ell_2$ regularization achieves monotonic test performance as\nwe grow either the sample size or the model size. We also demonstrate\nempirically that optimally-tuned $\\ell_2$ regularization can mitigate double\ndescent for more general models, including neural networks. Our results suggest\nthat it may also be informative to study the test risk scalings of various\nalgorithms in the context of appropriately tuned regularization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 05:19:09 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 04:45:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Venkat", "Prayaag", ""], ["Kakade", "Sham", ""], ["Ma", "Tengyu", ""]]}, {"id": "2003.01951", "submitter": "Felix Abramovich", "authors": "Felix Abramovich, Vadim Grinshtein and Tomer Levy", "title": "Multiclass classification by sparse multinomial logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider high-dimensional multiclass classification by\nsparse multinomial logistic regression. We propose first a feature selection\nprocedure based on penalized maximum likelihood with a complexity penalty on\nthe model size and derive the nonasymptotic bounds for misclassification excess\nrisk of the resulting classifier. We establish also their tightness by deriving\nthe corresponding minimax lower bounds. In particular, we show that there exist\ntwo regimes corresponding to small and large number of classes. The bounds can\nbe reduced under the additional low noise condition. To find a penalized\nmaximum likelihood solution with a complexity penalty requires, however, a\ncombinatorial search over all possible models. To design a feature selection\nprocedure computationally feasible for high-dimensional data, we propose\nmultinomial logistic group Lasso and Slope classifiers and show that they also\nachieve the minimax order.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:44:48 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 09:53:00 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 11:35:48 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Abramovich", "Felix", ""], ["Grinshtein", "Vadim", ""], ["Levy", "Tomer", ""]]}, {"id": "2003.02468", "submitter": "Xiaohan Wei", "authors": "Xiaohan Wei", "title": "II. High Dimensional Estimation under Weak Moment Assumptions:\n  Structured Recovery and Matrix Estimation", "comments": "PhD thesis, USC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this thesis is to develop new theories on high-dimensional\nstructured signal recovery under a rather weak assumption on the measurements\nthat only a finite number of moments exists. High-dimensional recovery has been\none of the emerging topics in the last decade partly due to the celebrated work\nof Candes, Romberg and Tao (e.g. [CRT06, CRT04]). The original analysis there\n(and the works thereafter) necessitates a strong concentration argument\n(namely, the restricted isometry property), which only holds for a rather\nrestricted class of measurements with light-tailed distributions. It had long\nbeen conjectured that high-dimensional recovery is possible even if restricted\nisometry type conditions do not hold, but the general theory was beyond the\ngrasp until very recently, when the works [Men14a, KM15] propose a new\nsmall-ball method. In these two papers, the authors initiated a new analysis\nframework for general empirical risk minimization (ERM) problems with respect\nto the square loss, which is robust and can potentially allow heavy-tailed loss\nfunctions. The materials in this thesis are partly inspired by [Men14a], but\nare of a different mindset: rather than directly analyzing the existing ERMs\nfor signal recovery for which it is difficult to avoid strong moment\nassumptions, we show that, in many circumstances, by carefully re-designing the\nERMs to start with, one can still achieve the minimax optimal statistical rate\nof signal recovery with very high probability under much weaker assumptions\nthan existing works.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 07:41:01 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Wei", "Xiaohan", ""]]}, {"id": "2003.02469", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Cumulant-free closed-form formulas for some common (dis)similarities\n  between densities of an exponential family", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler,\n$\\alpha$-divergences, and Jeffreys' divergences between densities belonging to\na same exponential family have generic closed-form formulas relying on the\nstrictly convex and real-analytic cumulant function characterizing the\nexponential family. In this work, we report (dis)similarity formulas which\nbypass the explicit use of the cumulant function and highlight the role of\nquasi-arithmetic means and their multivariate mean operator extensions. In\npractice, these cumulant-free formulas are handy when implementing these\n(dis)similarities using legacy Application Programming Interfaces (APIs) since\nour method requires only to partially factorize the densities canonically of\nthe considered exponential family.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 07:46:22 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 01:01:42 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 04:11:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "2003.02566", "submitter": "Matthieu Garcin", "authors": "Matthieu Garcin", "title": "A comparison of maximum likelihood and absolute moments for the\n  estimation of Hurst exponents in a stationary framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The absolute-moment method is widespread for estimating the Hurst exponent of\na fractional Brownian motion $X$. But this method is biased when applied to a\nstationary version of $X$, in particular an inverse Lamperti transform of $X$,\nwith a linear time contraction of parameter $\\theta$. We present an adaptation\nof the absolute-moment method to this framework and we compare it to the\nmaximum likelihood method, with simulations. While it appears that the\nmaximum-likelihood method is more accurate than the adapted absolute-moment\nestimation, this last method is not uninteresting for two reasons: it makes it\npossible to confirm visually that the model is well specified and it is\ncomputationally more performing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:34:46 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:28:13 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Garcin", "Matthieu", ""]]}, {"id": "2003.02678", "submitter": "Sara van de Geer", "authors": "Sara van de Geer", "title": "Logistic regression with total variation regularization", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study logistic regression with total variation penalty on the canonical\nparameter and show that the resulting estimator satisfies a sharp oracle\ninequality: the excess risk of the estimator is adaptive to the number of jumps\nof the underlying signal or an approximation thereof. In particular when there\nare finitely many jumps, and jumps up are sufficiently separated from jumps\ndown, then the estimator converges with a parametric rate up to a logarithmic\nterm $\\log n / n$, provided the tuning parameter is chosen appropriately of\norder $1/ \\sqrt n$. Our results extend earlier results for quadratic loss to\nlogistic loss. We do not assume any a priori known bounds on the canonical\nparameter but instead only make use of the local curvature of the theoretical\nrisk.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:45:49 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["van de Geer", "Sara", ""]]}, {"id": "2003.02805", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "A strong law of large numbers for simultaneously testing parameters of\n  Lancaster bivariate distributions", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We prove a strong law of large numbers for simultaneously testing parameters\nof a large number of dependent, Lancaster bivariate random variables with\ninfinite supports, and discuss its implications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:12:32 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "2003.02941", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of test power for Z-test and Chi-square test with\n  auxiliary information", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this article is to study how an auxiliary information can be\nused to improve the power of two famous statistical tests: the $ Z$-test and\nthe chi-square test. This information can be of any nature - probability of\nsets of partitions, expectation of a function, ... - and is not even required\nto be an exact information, it can be given by an estimate based on a larger\nsample for example. Some definitions of auxiliary information can be found in\nthe statistical literature and will be recalled. In this article, the notion of\nauxiliary information is discussed here from a very general point of view.\nThese two statistical tests are modified so that the auxiliary information is\ntaken into account. One show in particular that the power of these tests is\nincreased exponentially. Some statistical examples are treated to show the\nconcreteness of this method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:46:21 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2003.03280", "submitter": "Jos\\'e G. G\\'omez-Garc\\'ia", "authors": "Jos\\'e G. G\\'omez-Garc\\'ia", "title": "A dependent Lindeberg central limit theorem for cluster functionals on\n  stationary random fields", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a central limit theorem for the finite-dimensional\nmarginal distributions of empirical processes $(Z_n(f))_{f\\in\\mathcal{F}}$\nwhose index set $\\mathcal{F}$ is a family of cluster functionals valued on\nblocks of values of a stationary random field. The practicality and\napplicability of the result depends mainly on the usual Lindeberg condition and\na sequence $T_n$ which summarizes the dependence between the blocks of the\nrandom field values. Finally, as application, we use the previous result in\norder to show the Gaussian asymptotic behavior of the iso-extremogram estimator\nintroduced in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:30:26 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["G\u00f3mez-Garc\u00eda", "Jos\u00e9 G.", ""]]}, {"id": "2003.03615", "submitter": "Mikhail Boldin", "authors": "Michael Boldin", "title": "Local Power of Tests of Fit for Normality of Autoregression", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary $AR(p)$ model. The autoregression parameters are\nunknown as well as the distribution of innovations. Based on the residuals from\nthe parameter estimates, an analog of empirical distribution function is\ndefined and the tests of Kolmogorov's and $\\omega^2$ type is constructed for\ntesting hypotheses on the normality of innovations. We obtain the asymptotic\npower of these tests under local alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 17:32:51 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Boldin", "Michael", ""]]}, {"id": "2003.03668", "submitter": "Richard Samworth", "authors": "Yudong Chen, Tengyao Wang and Richard J. Samworth", "title": "High-dimensional, multiscale online changepoint detection", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for high-dimensional, online changepoint detection\nin settings where a $p$-variate Gaussian data stream may undergo a change in\nmean. The procedure works by performing likelihood ratio tests against simple\nalternatives of different scales in each coordinate, and then aggregating test\nstatistics across scales and coordinates. The algorithm is online in the sense\nthat both its storage requirements and worst-case computational complexity per\nnew observation are independent of the number of previous observations; in\npractice, it may even be significantly faster than this. We prove that the\npatience, or average run length under the null, of our procedure is at least at\nthe desired nominal level, and provide guarantees on its response delay under\nthe alternative that depend on the sparsity of the vector of mean change.\nSimulations confirm the practical effectiveness of our proposal, which is\nimplemented in the R package 'ocd', and we also demonstrate its utility on a\nseismology data set.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 21:54:09 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 15:46:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Yudong", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2003.03857", "submitter": "Johannes Heiny", "authors": "Johannes Heiny and Jianfeng Yao", "title": "Limiting distributions for eigenvalues of sample correlation matrices\n  from heavy-tailed populations", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a $p$-dimensional population ${\\mathbf x} \\in\\mathbb{R}^p$ with iid\ncoordinates in the domain of attraction of a stable distribution with index\n$\\alpha\\in (0,2)$. Since the variance of ${\\mathbf x}$ is infinite, the sample\ncovariance matrix ${\\mathbf S}_n=n^{-1}\\sum_{i=1}^n {{\\mathbf x}_i}{\\mathbf\nx}'_i$ based on a sample ${\\mathbf x}_1,\\ldots,{\\mathbf x}_n$ from the\npopulation is not well behaved and it is of interest to use instead the sample\ncorrelation matrix ${\\mathbf R}_n= \\{\\operatorname{diag}({\\mathbf\nS}_n)\\}^{-1/2}\\, {\\mathbf S}_n \\{\\operatorname{diag}({\\mathbf S}_n)\\}^{-1/2}$.\nThis paper finds the limiting distributions of the eigenvalues of ${\\mathbf\nR}_n$ when both the dimension $p$ and the sample size $n$ grow to infinity such\nthat $p/n\\to \\gamma \\in (0,\\infty)$. The family of limiting distributions\n$\\{H_{\\alpha,\\gamma}\\}$ is new and depends on the two parameters $\\alpha$ and\n$\\gamma$. The moments of $H_{\\alpha,\\gamma}$ are fully identified as sum of two\ncontributions: the first from the classical Mar\\v{c}enko-Pastur law and a\nsecond due to heavy tails. Moreover, the family $\\{H_{\\alpha,\\gamma}\\}$ has\ncontinuous extensions at the boundaries $\\alpha=2$ and $\\alpha=0$ leading to\nthe Mar\\v{c}enko-Pastur law and a modified Poisson distribution, respectively.\n  Our proofs use the method of moments, the path-shortening algorithm developed\nin [18] and some novel graph counting combinatorics. As a consequence, the\nmoments of $H_{\\alpha,\\gamma}$ are expressed in terms of combinatorial objects\nsuch as Stirling numbers of the second kind. A simulation study on these\nlimiting distributions $H_{\\alpha,\\gamma}$ is also provided for comparison with\nthe Mar\\v{c}enko-Pastur law.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 22:05:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Heiny", "Johannes", ""], ["Yao", "Jianfeng", ""]]}, {"id": "2003.03863", "submitter": "Philip Ernst", "authors": "F. Thomas Bruss, Philip A. Ernst, and Dongzhou Huang", "title": "The rencontre problem", "comments": "40 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $\\left\\{X^{1}_k\\right\\}_{k=1}^{\\infty},\n\\left\\{X^{2}_k\\right\\}_{k=1}^{\\infty}, \\cdots,\n\\left\\{X^{d}_k\\right\\}_{k=1}^{\\infty}$ be $d$ independent sequences of\nBernoulli random variables with success-parameters $p_1, p_2, \\cdots, p_d$\nrespectively, where $d \\geq 2$ is a positive integer, and $ 0<p_j<1$ for all\n$j=1,2,\\cdots,d.$ Let \\begin{equation*} S^{j}(n) = \\sum_{i=1}^{n} X^{j}_{i} =\nX^{j}_{1} + X^{j}_{2} + \\cdots + X^{j}_{n}, \\quad n =1,2 , \\cdots.\n\\end{equation*} We declare a \"rencontre\" at time $n$, or, equivalently, say\nthat $n$ is a \"rencontre-time,\" if \\begin{equation*} S^{1}(n) = S^{2}(n) =\n\\cdots = S^{d}(n). \\end{equation*} We motivate and study the distribution of\nthe first (provided it is finite) rencontre time.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 23:13:02 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Bruss", "F. Thomas", ""], ["Ernst", "Philip A.", ""], ["Huang", "Dongzhou", ""]]}, {"id": "2003.03880", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, Rasmus Waagepetersen", "title": "Information criteria for inhomogeneous spatial point processes", "comments": "6 figures", "journal-ref": null, "doi": "10.1088/1742-6596/1752/1/012015", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical foundation for a number of model selection criteria is\nestablished in the context of inhomogeneous point processes and under various\nasymptotic settings: infill, increasing domain, and combinations of these. For\ninhomogeneous Poisson processes we consider Akaike information criterion and\nthe Bayesian information criterion, and in particular we identify the point\nprocess analogue of sample size needed for the Bayesian information criterion.\nConsidering general inhomogeneous point processes we derive new composite\nlikelihood and composite Bayesian information criteria for selecting a\nregression model for the intensity function. The proposed model selection\ncriteria are evaluated using simulations of Poisson processes and cluster point\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 01:45:04 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "2003.03886", "submitter": "Ryan Tibshirani", "authors": "Ryan J. Tibshirani", "title": "Divided Differences, Falling Factorials, and Discrete Splines: Another\n  Look at Trend Filtering and Related Problems", "comments": "74 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews a class of univariate piecewise polynomial functions known\nas discrete splines, which share properties analogous to the better-known class\nof spline functions, but where continuity in derivatives is replaced by (a\nsuitable notion of) continuity in divided differences. As it happens, discrete\nsplines bear connections to a wide array of developments in applied mathematics\nand statistics, from divided differences and Newton interpolation (dating back\nto over 300 years ago) to trend filtering (from the last 15 years). We survey\nthese connections, and contribute some new perspectives and new results along\nthe way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:02:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 16:04:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "2003.03887", "submitter": "Oliver Cliff", "authors": "Oliver M. Cliff, Leonardo Novelli, Ben D. Fulcher, James M. Shine and\n  Joseph T. Lizier", "title": "Assessing the Significance of Directed and Multivariate Measures of\n  Linear Dependence Between Time Series", "comments": "27 pages, 14 figures, final submission to Phys Rev. Research editors\n  (before copyediting)", "journal-ref": "Phys. Rev. Research 3, 013145 (2021)", "doi": "10.1103/PhysRevResearch.3.013145", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST physics.data-an q-bio.NC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring linear dependence between time series is central to our\nunderstanding of natural and artificial systems. Unfortunately, the hypothesis\ntests that are used to determine statistically significant directed or\nmultivariate relationships from time-series data often yield spurious\nassociations (Type I errors) or omit causal relationships (Type II errors).\nThis is due to the autocorrelation present in the analysed time series -- a\nproperty that is ubiquitous across diverse applications, from brain dynamics to\nclimate change. Here we show that, for limited data, this issue cannot be\nmediated by fitting a time-series model alone (e.g., in Granger causality or\nprewhitening approaches), and instead that the degrees of freedom in\nstatistical tests should be altered to account for the effective sample size\ninduced by cross-correlations in the observations. This insight enabled us to\nderive modified hypothesis tests for any multivariate correlation-based\nmeasures of linear dependence between covariance-stationary time series,\nincluding Granger causality and mutual information with Gaussian marginals. We\nuse both numerical simulations (generated by autoregressive models and digital\nfiltering) as well as recorded fMRI-neuroimaging data to show that our tests\nare unbiased for a variety of stationary time series. Our experiments\ndemonstrate that the commonly used $F$- and $\\chi^2$-tests can induce\nsignificant false-positive rates of up to $100\\%$ for both measures, with and\nwithout prewhitening of the signals. These findings suggest that many\ndependencies reported in the scientific literature may have been, and may\ncontinue to be, spuriously reported or missed if modified hypothesis tests are\nnot used when analysing time series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:06:01 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:02:18 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 10:13:03 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cliff", "Oliver M.", ""], ["Novelli", "Leonardo", ""], ["Fulcher", "Ben D.", ""], ["Shine", "James M.", ""], ["Lizier", "Joseph T.", ""]]}, {"id": "2003.04008", "submitter": "Richard D. Gill", "authors": "R. D. Gill", "title": "Anna Karenina and The Two Envelopes Problem", "comments": "Final corrections (fingers crossed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Anna Karenina principle is named after the opening sentence in the\neponymous novel: Happy families are all alike; every unhappy family is unhappy\nin its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in\nprobability theory, mathematical economics, logic, and philosophy. Time and\nagain a new analysis is published in which an author claims finally to explain\nwhat actually goes wrong in this paradox. Each author (the present author\nincluded) emphasizes what is new in their approach and concludes that earlier\napproaches did not get to the root of the matter. We observe that though a\nlogical argument is only correct if every step is correct, an apparently\nlogical argument which goes astray can be thought of as going astray at\ndifferent places. This leads to a comparison between the literature on TEP and\na successful movie franchise: it generates a succession of sequels, and even\nprequels, each with a different director who approaches the same basic premise\nin a personal way. We survey resolutions in the literature with a view to\nsynthesis, correct common errors, and give a new theorem on order properties of\nan exchangeable pair of random variables, at the heart of most TEP variants and\ninterpretations. A theorem on asymptotic independence between the amount in\nyour envelope and the question whether it is smaller or larger shows that the\npathological situation of improper priors or infinite expectation values has\nconsequences as we merely approach such a situation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 09:54:28 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:03:23 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 10:11:04 GMT"}, {"version": "v4", "created": "Mon, 29 Mar 2021 08:50:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gill", "R. D.", ""]]}, {"id": "2003.04026", "submitter": "Oscar Oelrich", "authors": "Oscar Oelrich, Shutong Ding, M{\\aa}ns Magnusson, Aki Vehtari, Mattias\n  Villani", "title": "When are Bayesian model probabilities overconfident?", "comments": "6 pages + 4 pages appendix, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model comparison is often based on the posterior distribution over\nthe set of compared models. This distribution is often observed to concentrate\non a single model even when other measures of model fit or forecasting ability\nindicate no strong preference. Furthermore, a moderate change in the data\nsample can easily shift the posterior model probabilities to concentrate on\nanother model. We document overconfidence in two high-profile applications in\neconomics and neuroscience. To shed more light on the sources of overconfidence\nwe derive the sampling variance of the Bayes factor in univariate and\nmultivariate linear regression. The results show that overconfidence is likely\nto happen when i) the compared models give very different approximations of the\ndata-generating process, ii) the models are very flexible with large degrees of\nfreedom that are not shared between the models, and iii) the models\nunderestimate the true variability in the data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 10:30:28 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Oelrich", "Oscar", ""], ["Ding", "Shutong", ""], ["Magnusson", "M\u00e5ns", ""], ["Vehtari", "Aki", ""], ["Villani", "Mattias", ""]]}, {"id": "2003.04199", "submitter": "Niko Lietz\\'en", "authors": "Niko Lietz\\'en, Lauri Viitasaari and Pauliina Ilmonen", "title": "Modeling temporally uncorrelated components for complex-valued\n  stationary processes", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a complex-valued linear mixture model, under discrete weakly\nstationary processes. We recover latent components of interest, which have\nundergone a linear mixing. We study asymptotic properties of a classical\nunmixing estimator, that is based on simultaneous diagonalization of the\ncovariance matrix and an autocovariance matrix with lag $\\tau$. Our main\ncontribution is that our asymptotic results can be applied to a large class of\nprocesses. In related literature, the processes are typically assumed to have\nweak correlations. We extend this class and consider the unmixing estimator\nunder stronger dependency structures. In particular, we analyze the asymptotic\nbehavior of the unmixing estimator under both, long- and short-range dependent\ncomplex-valued processes. Consequently, our theory covers unmixing estimators\nthat converge slower than the usual $\\sqrt{T}$ and unmixing estimators that\nproduce non-Gaussian asymptotic distributions. The presented methodology is a\npowerful prepossessing tool and highly applicable in several fields of\nstatistics. Complex-valued processes are frequently encountered in, for\nexample, biomedical applications and signal processing. In addition, our\napproach can be applied to model real-valued problems that involve temporally\nuncorrelated pairs. These are encountered in, for example, applications in\nfinance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:23:38 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 11:46:42 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Lietz\u00e9n", "Niko", ""], ["Viitasaari", "Lauri", ""], ["Ilmonen", "Pauliina", ""]]}, {"id": "2003.04208", "submitter": "Rasmus Henningsson", "authors": "Magnus Fontes and Rasmus Henningsson", "title": "Principal Moment Analysis", "comments": "10 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Moment Analysis is a method designed for dimension reduction,\nanalysis and visualization of high dimensional multivariate data. It\ngeneralizes Principal Component Analysis and allows for significant statistical\nmodeling flexibility, when approximating an unknown underlying probability\ndistribution, by enabling direct analysis of general approximate measures.\nThrough https://principalmomentanalysis.github.io/ we provide an\nimplementation, together with a graphical user interface, of a simplex based\nversion of Principal Moment Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:48:29 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Fontes", "Magnus", ""], ["Henningsson", "Rasmus", ""]]}, {"id": "2003.04265", "submitter": "Claudia Neves", "authors": "John H.J. Einmahl, Ana Ferreira, Laurens de Haan, Claudia Neves and\n  Chen Zhou", "title": "Spatial dependence and space-time trend in extreme events", "comments": "Supporting information: the detailed proof of Theorem 6, referenced\n  in Section 4, as well as simulations showcasing finite sample performance of\n  the proposed methods are available with this paper at https://bit.ly/3aJFM6B", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical theory of extremes is extended to observations that are\nnon-stationary and not independent. The non-stationarity over time and space is\ncontrolled via the scedasis (tail scale) in the marginal distributions. Spatial\ndependence stems from multivariate extreme value theory. We establish\nasymptotic theory for both the weighted sequential tail empirical process and\nthe weighted tail quantile process based on all observations, taken over time\nand space. The results yield two statistical tests for homoscedasticity in the\ntail, one in space and one in time. Further, we show that the common extreme\nvalue index can be estimated via a pseudo-maximum likelihood procedure based on\npooling all (non-stationary and dependent) observations. Our leading example\nand application is rainfall in Northern Germany.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:16:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Einmahl", "John H. J.", ""], ["Ferreira", "Ana", ""], ["de Haan", "Laurens", ""], ["Neves", "Claudia", ""], ["Zhou", "Chen", ""]]}, {"id": "2003.04314", "submitter": "Felix Cheysson", "authors": "Felix Cheysson (1, 2 and 3), Gabriel Lang (1) ((1) Universit\\'e\n  Paris-Saclay, AgroParisTech, INRAE, UMR MIA-Paris, Paris, France, (2)\n  Epidemiology and Modeling of bacterial Evasion to Antibacterials Unit (EMEA),\n  Institut Pasteur, Paris, France, (3) Anti-infective Evasion and\n  Pharmacoepidemiology Team, Centre for Epidemiology and Public health (CESP),\n  Inserm / UVSQ, France)", "title": "Strong mixing condition for Hawkes processes and application to Whittle\n  estimation from count data", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the time series generated by the event counts of\nstationary Hawkes processes. When the exact locations of points are not\nobserved, but only counts over time intervals of fixed size, existing methods\nof estimation are not applicable. We first establish a strong mixing condition\nwith polynomial decay rate for Hawkes processes, from their Poisson cluster\nstructure. This allows us to propose a spectral approach to the estimation of\nHawkes processes, based on Whittle's method, which provides consistent and\nasymptotically normal estimates under common regularity conditions on their\nreproduction kernels. Simulated datasets and a case-study illustrate the\nperformances of the estimation, notably of the Hawkes reproduction mean and\nkernel when time intervals are relatively large.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:37:28 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Cheysson", "Felix", "", "1, 2 and 3"], ["Lang", "Gabriel", ""]]}, {"id": "2003.04406", "submitter": "Kevin Duisters", "authors": "Kevin Duisters and Johannes Schmidt-Hieber", "title": "On frequentist coverage of Bayesian credible sets for estimation of the\n  mean under constraints", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist coverage of $(1-\\alpha)$-highest posterior density (HPD) credible\nsets is studied in a signal plus noise model under a large class of noise\ndistributions. We consider a specific class of spike-and-slab prior\ndistributions. Different regimes are identified and we derive closed form\nexpressions for the $(1-\\alpha)$-HPD on each of these regimes. Similar to the\nearlier work by Marchand and Strawderman, it is shown that under suitable\nconditions, the frequentist coverage can drop to $1-3\\alpha/2.$\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 20:45:18 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Duisters", "Kevin", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "2003.04433", "submitter": "Somabha Mukherjee", "authors": "Somabha Mukherjee, Rohit K. Patra, Andrew L. Johnson, Hiroshi Morita", "title": "Least Squares Estimation of a Monotone Quasiconvex Regression Function", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for the estimation of a multivariate function based\non the economic axioms of monotonicity and quasiconvexity. We prove the\nexistence of the nonparametric least squares estimator (LSE) for a monotone and\nquasiconvex function and provide two characterizations for it. One of these\ncharacterizations is useful from the theoretical point of view, while the other\nhelps in the computation of the estimator. We show that the LSE is almost\nsurely unique and is the solution to a mixed-integer quadratic optimization\nproblem. We prove consistency and find finite sample risk bounds for the LSE\nunder both fixed lattice and random design settings for the covariates. We\nillustrate the superior performance of the LSE against existing estimators via\nsimulation. Finally, we use the LSE to estimate the production function for the\nJapanese plywood industry and the cost function for hospitals across the US.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:16:57 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Patra", "Rohit K.", ""], ["Johnson", "Andrew L.", ""], ["Morita", "Hiroshi", ""]]}, {"id": "2003.04773", "submitter": "Lukas Steinberger", "authors": "Cristina Butucea and Angelika Rohde and Lukas Steinberger", "title": "Interactive versus non-interactive locally differentially private\n  estimation: Two elbows for the quadratic functional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy has recently received increasing attention from\nthe statistics community as a valuable tool to protect the privacy of\nindividual data owners without the need of a trusted third party. Similar to\nthe classic notion of randomized response, the idea is that data owners\nrandomize their true information locally and only release the perturbed data.\nMany different protocols for such local perturbation procedures can be\ndesigned. In all the estimation problems studied in the literature so far,\nhowever, no significant difference in terms of minimax risk between purely\nnon-interactive protocols and protocols that allow for some amount of\ninteraction between individual data providers could be observed. In this paper\nwe show that for estimating the integrated square of a density, sequentially\ninteractive procedures improve substantially over the best possible\nnon-interactive procedure in terms of minimax rate of estimation. In\nparticular, in the non-interactive scenario we identify an elbow in the minimax\nrate at $s=\\frac34$, whereas in the sequentially interactive scenario the elbow\nis at $s=\\frac12$. This is markedly different from both, the case of direct\nobservations, where the elbow is well known to be at $s=\\frac14$, as well as\nfrom the case where Laplace noise is added to the original data, where an elbow\nat $s= \\frac94$ is obtained. The fact that a particular locally differentially\nprivate, but interactive, mechanism improves over the simple non-interactive\none is also of great importance for practical implementations of local\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:28:42 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 10:20:20 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Butucea", "Cristina", ""], ["Rohde", "Angelika", ""], ["Steinberger", "Lukas", ""]]}, {"id": "2003.04788", "submitter": "Timo Klock", "authors": "Timo Klock, Alessandro Lanteri, Stefano Vigogna", "title": "Estimating multi-index models with response-conditional least squares", "comments": "30 pages, 13 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-index model is a simple yet powerful high-dimensional regression\nmodel which circumvents the curse of dimensionality assuming $ \\mathbb{E} [ Y |\nX ] = g(A^\\top X) $ for some unknown index space $A$ and link function $g$. In\nthis paper we introduce a method for the estimation of the index space, and\nstudy the propagation error of an index space estimate in the regression of the\nlink function. The proposed method approximates the index space by the span of\nlinear regression slope coefficients computed over level sets of the data.\nBeing based on ordinary least squares, our approach is easy to implement and\ncomputationally efficient. We prove a tight concentration bound that shows\n$N^{-1/2}$-convergence, but also faithfully describes the dependence on the\nchosen partition of level sets, hence giving indications on the hyperparameter\ntuning. The estimator's competitiveness is confirmed by extensive comparisons\nwith state-of-the-art methods, both on synthetic and real data sets. As a\nsecond contribution, we establish minimax optimal generalization bounds for\nk-nearest neighbors and piecewise polynomial regression when trained on samples\nprojected onto any $N^{-1/2}$-consistent estimate of the index space, thus\nproviding complete and provable estimation of the multi-index model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 15:00:51 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:27:05 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Klock", "Timo", ""], ["Lanteri", "Alessandro", ""], ["Vigogna", "Stefano", ""]]}, {"id": "2003.04840", "submitter": "Kaie Kubjas", "authors": "Alexandros Grosdos, Alexander Heaton, Kaie Kubjas, Olga Kuznetsova,\n  Georgy Scholten, and Miruna-Stefana Sorea", "title": "Exact Solutions in Log-Concave Maximum Likelihood Estimation", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probability density functions that are log-concave. Despite the\nspace of all such densities being infinite-dimensional, the maximum likelihood\nestimate is the exponential of a piecewise linear function determined by\nfinitely many quantities, namely the function values, or heights, at the data\npoints. We explore in what sense exact solutions to this problem are possible.\nFirst, we show that the heights given by the maximum likelihood estimate are\ngenerically transcendental. For a cell in one dimension, the maximum likelihood\nestimator is expressed in closed form using the generalized W-Lambert function.\nEven more, we show that finding the log-concave maximum likelihood estimate is\nequivalent to solving a collection of polynomial-exponential systems of a\nspecial form. Even in the case of two equations, very little is known about\nsolutions to these systems. As an alternative, we use Smale's alpha-theory to\nrefine approximate numerical solutions and to certify solutions to log-concave\ndensity estimation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:39:21 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Grosdos", "Alexandros", ""], ["Heaton", "Alexander", ""], ["Kubjas", "Kaie", ""], ["Kuznetsova", "Olga", ""], ["Scholten", "Georgy", ""], ["Sorea", "Miruna-Stefana", ""]]}, {"id": "2003.04932", "submitter": "Antonio Di Crescenzo", "authors": "Antonio Di Crescenzo and Luca Paolillo", "title": "Analysis and applications of the residual varentropy of random lifetimes", "comments": "20 pages, 9 figures, to appear on \"Probability in the Engineering and\n  Informational Sciences\"", "journal-ref": null, "doi": "10.1017/S0269964820000133", "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reliability theory and survival analysis, the residual entropy is known as\na measure suitable to describe the dynamic information content in stochastic\nsystems conditional on survival. Aiming to analyze the variability of such\ninformation content, in this paper we introduce the variance of the residual\nlifetimes, \"residual varentropy\" in short. After a theoretical investigation of\nsome properties of the residual varentropy, we illustrate certain applications\nrelated to the proportional hazards model and the first-passage times of an\nOrnstein-Uhlenbeck jump-diffusion process.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 19:00:53 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Di Crescenzo", "Antonio", ""], ["Paolillo", "Luca", ""]]}, {"id": "2003.05006", "submitter": "Michael Levine", "authors": "Yan Cui, Michael Levine, and Zhou Zhou", "title": "Estimation and Inference of Time-Varying Auto-Covariance under Complex\n  Trend: A Difference-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a difference-based nonparametric methodology for the estimation\nand inference of the time-varying auto-covariance functions of a locally\nstationary time series when it is contaminated by a complex trend with both\nabrupt and smooth changes. Simultaneous confidence bands (SCB) with\nasymptotically correct coverage probabilities are constructed for the\nauto-covariance functions under complex trend. A simulation-assisted\nbootstrapping method is proposed for the practical construction of the SCB.\nDetailed simulation and a real data example round out our presentation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 21:22:01 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Cui", "Yan", ""], ["Levine", "Michael", ""], ["Zhou", "Zhou", ""]]}, {"id": "2003.05167", "submitter": "Fabien Panloup", "authors": "Karine Bertin (CIMFAV), Nicolas Klutchnikoff, Fabien Panloup (LAREMA),\n  Maylis Varvenne (IMT, UT1)", "title": "Adaptive estimation of the stationary density of a stochastic\n  differential equation driven by a fractional Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build and study a data-driven procedure for the estimation of the\nstationary density f of an additive fractional SDE. To this end, we also prove\nsome new concentrations bounds for discrete observations of such dynamics in\nstationary regime.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:57:21 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Bertin", "Karine", "", "CIMFAV"], ["Klutchnikoff", "Nicolas", "", "LAREMA"], ["Panloup", "Fabien", "", "LAREMA"], ["Varvenne", "Maylis", "", "IMT, UT1"]]}, {"id": "2003.05221", "submitter": "Savi Virolainen", "authors": "Savi Virolainen", "title": "A mixture autoregressive model based on Gaussian and Student's\n  $t$-distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new mixture autoregressive model which combines Gaussian and\nStudent's $t$ mixture components. The model has very attractive properties\nanalogous to the Gaussian and Student's $t$ mixture autoregressive models, but\nit is more flexible as it enables to model series which consist of both\nconditionally homoscedastic Gaussian regimes and conditionally heteroscedastic\nStudent's $t$ regimes. The usefulness of our model is demonstrated in an\nempirical application to the monthly U.S. interest rate spread between the\n3-month Treasury bill rate and the effective federal funds rate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:16:36 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 12:16:52 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 15:16:44 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Virolainen", "Savi", ""]]}, {"id": "2003.05479", "submitter": "Shunichi Amari", "authors": "Shun-ichi Amari", "title": "Wasserstein statistics in 1D location-scale model", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein geometry and information geometry are two important structures\nintroduced in a manifold of probability distributions. The former is defined by\nusing the transportation cost between two distributions, so it reflects the\nmetric structure of the base manifold on which distributions are defined.\nInformation geometry is constructed based on the invariance criterion that the\ngeometry is invariant under reversible transformations of the base space. Both\nhave their own merits for applications. Statistical inference is constructed on\ninformation geometry, where the Fisher metric plays a fundamental role, whereas\nWasserstein geometry is useful for applications to computer vision and AI. We\npropose statistical inference based on the Wasserstein geometry in the case\nthat the base space is 1-dimensional. By using the location-scale model, we\nderive the $W$-estimator explicitly and studies its asymptotic behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:48:00 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Amari", "Shun-ichi", ""]]}, {"id": "2003.05572", "submitter": "Jerome Darbon", "authors": "Jerome Darbon, Gabriel P. Langlois", "title": "On Bayesian posterior mean estimators in imaging sciences and\n  Hamilton-Jacobi Partial Differential Equations", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational and Bayesian methods are two approaches that have been widely\nused to solve image reconstruction problems. In this paper, we propose original\nconnections between Hamilton--Jacobi (HJ) partial differential equations and a\nbroad class of Bayesian methods and posterior mean estimators with Gaussian\ndata fidelity term and log-concave prior. Whereas solutions to certain\nfirst-order HJ PDEs with initial data describe maximum a posteriori estimators\nin a Bayesian setting, here we show that solutions to some viscous HJ PDEs with\ninitial data describe a broad class of posterior mean estimators. These\nconnections allow us to establish several representation formulas and optimal\nbounds involving the posterior mean estimate. In particular, we use these\nconnections to HJ PDEs to show that some Bayesian posterior mean estimators can\nbe expressed as proximal mappings of twice continuously differentiable\nfunctions, and furthermore we derive a representation formula for these\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 02:15:36 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Darbon", "Jerome", ""], ["Langlois", "Gabriel P.", ""]]}, {"id": "2003.05599", "submitter": "Minwoo Chae", "authors": "Minwoo Chae, Pierpaolo De Blasi, Stephen G. Walker", "title": "Posterior asymptotics in Wasserstein metrics on the real line", "comments": "43pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use the class of Wasserstein metrics to study asymptotic\nproperties of posterior distributions. Our first goal is to provide sufficient\nconditions for posterior consistency. In addition to the well-known Schwartz's\nKullback--Leibler condition on the prior, the true distribution and most\nprobability measures in the support of the prior are required to possess\nmoments up to an order which is determined by the order of the Wasserstein\nmetric. We further investigate convergence rates of the posterior distributions\nfor which we need stronger moment conditions. The required tail conditions are\nsharp in the sense that the posterior distribution may be inconsistent or\ncontract slowly to the true distribution without these conditions. Our study\ninvolves techniques that build on recent advances on Wasserstein convergence of\nempirical measures. We apply the results to density estimation with a Dirichlet\nprocess mixture prior and conduct a simulation study for further illustration.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 03:26:50 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:56:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chae", "Minwoo", ""], ["De Blasi", "Pierpaolo", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2003.05619", "submitter": "Mikhail  Ermakov s", "authors": "Mikhail Ermakov", "title": "On uniform consistency of nonparametric tests I", "comments": "42 pages. arXiv admin note: text overlap with arXiv:1807.09076", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out necessary and sufficient conditions of uniform consistency of\nnonparametric sets of alternatives for widespread nonparametric tests.\nNonparametric sets of alternatives can be defined both in terms of distribution\nfunction and in terms of density (or signals in the problem of signal detection\nin Gaussian white noise). In this part of paper such conditions are provided\nfor $\\chi^2-$tests with increasing number of cells, Cramer-von Mises tests,\ntests generated $\\mathbb{L}_2$- norms of kernel estimators and tests generated\nquadratic forms of estimators of Fourier coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 05:01:03 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 12:07:54 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 08:41:01 GMT"}, {"version": "v4", "created": "Sun, 24 May 2020 09:11:30 GMT"}, {"version": "v5", "created": "Mon, 3 Aug 2020 12:17:18 GMT"}, {"version": "v6", "created": "Sun, 30 Aug 2020 10:06:04 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ermakov", "Mikhail", ""]]}, {"id": "2003.05683", "submitter": "Nick Kloodt", "authors": "Nick Kloodt", "title": "Identification in a Fully Nonparametric Transformation Model with\n  Heteroscedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so far most general identification result in the context of nonparametric\ntransformation models is proven. The result is constructive in the sense that\nit provides an explicit expression of the transformation function.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 09:50:35 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Kloodt", "Nick", ""]]}, {"id": "2003.05716", "submitter": "Guy Martial Nkiet", "authors": "Armando Sosthene Kali Balogoun, Guy Martial Nkiet, Carlos Ogouyandjou", "title": "Asymptotic normality of a generalized maximum mean discrepancy estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an estimator of the generalized maximum mean\ndiscrepancy between several distributions, constructed by modifying a naive\nestimator. Asymptotic normality is obtained for this estimator both under\nequality of these distributions and under the alternative hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:46:27 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Balogoun", "Armando Sosthene Kali", ""], ["Nkiet", "Guy Martial", ""], ["Ogouyandjou", "Carlos", ""]]}, {"id": "2003.05814", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, and Leonardo Moreno", "title": "Level set and density estimation on manifolds", "comments": "21 pages, y figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of the estimation of the level sets L_f({\\lambda}) of\nthe density f of a random vector X supported on a smooth manifold M\\subsetR^d ,\nfrom an iid sample of X. To do that we introduce a kernel-based estimator f^n,h\n, which is a slightly modified version of the one proposed in [45], and proves\nits a.s. uniform convergence to f . Then, we propose two estimators of L f\n({\\lambda}), the first one is a plug-in: L f^n,h ({\\lambda}), which is proven\nto be a.s. consistent in Hausdorff distance and distance in measure, if L\nf({\\lambda}) does not meet the boundary of M . While the second one assumes\nthat L f({\\lambda}) is r-convex, and is estimated by means of the r-convex hull\nof L f^n,h({\\lambda}). The performance of our proposal is illustrated through\nsome simulated examples. In a real data example we analyze the intensity and\ndirection of strong and moderate winds.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 14:19:01 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 01:13:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Moreno", "Leonardo", ""]]}, {"id": "2003.05838", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot, Matthieu Lerasle", "title": "On the robustness of the minimum $\\ell_2$ interpolator", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the interpolator with minimal $\\ell_2$-norm $\\hat{\\beta}$ in a\ngeneral high dimensional linear regression framework where $\\mathbb Y=\\mathbb\nX\\beta^*+\\xi$ where $\\mathbb X$ is a random $n\\times p$ matrix with independent\n$\\mathcal N(0,\\Sigma)$ rows and without assumption on the noise vector $\\xi\\in\n\\mathbb R^n$. We prove that, with high probability, the prediction loss of this\nestimator is bounded from above by $(\\|\\beta^*\\|^2_2r_{cn}(\\Sigma)\\vee\n\\|\\xi\\|^2)/n$, where $r_{k}(\\Sigma)=\\sum_{i\\geq k}\\lambda_i(\\Sigma)$ are the\nrests of the sum of eigenvalues of $\\Sigma$. These bounds show a transition in\nthe rates. For high signal to noise ratios, the rates\n$\\|\\beta^*\\|^2_2r_{cn}(\\Sigma)/n$ broadly improve the existing ones. For low\nsignal to noise ratio, we also provide lower bound holding with large\nprobability. Under assumptions on the sprectrum of $\\Sigma$, this lower bound\nis of order $\\| \\xi\\|_2^2/n$, matching the upper bound. Consequently, in the\nlarge noise regime, we are able to precisely track the prediction error with\nlarge probability. This results give new insight when the interpolation can be\nharmless in high dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:12:28 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 13:48:18 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chinot", "Geoffrey", ""], ["Lerasle", "Matthieu", ""]]}, {"id": "2003.05968", "submitter": "Holger Dette", "authors": "Zhou Zhou, Holger Dette", "title": "Statistical Inference for High Dimensional Panel Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop statistical inference tools for high dimensional\nfunctional time series. We introduce a new concept of physical dependent\nprocesses in the space of square integrable functions, which adopts the idea of\nbasis decomposition of functional data in these spaces, and derive Gaussian and\nmultiplier bootstrap approximations for sums of high dimensional functional\ntime series. These results have numerous important statistical consequences.\nExemplarily, we consider the development of joint simultaneous confidence bands\nfor the mean functions and the construction of tests for the hypotheses that\nthe mean functions in the spatial dimension are parallel. The results are\nillustrated by means of a small simulation study and in the analysis of\nCanadian temperature data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 18:31:48 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhou", "Zhou", ""], ["Dette", "Holger", ""]]}, {"id": "2003.06024", "submitter": "Mathias Drton", "authors": "Mathias Drton, Satoshi Kuriki, Peter Hoff", "title": "Existence and Uniqueness of the Kronecker Covariance MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In matrix-valued datasets the sampled matrices often exhibit correlations\namong both their rows and their columns. A useful and parsimonious model of\nsuch dependence is the matrix normal model, in which the covariances among the\nelements of a random matrix are parameterized in terms of the Kronecker product\nof two covariance matrices, one representing row covariances and one\nrepresenting column covariance. An appealing feature of such a matrix normal\nmodel is that the Kronecker covariance structure allows for standard likelihood\ninference even when only a very small number of data matrices is available. For\ninstance, in some cases a likelihood ratio test of dependence may be performed\nwith a sample size of one. However, more generally the sample size required to\nensure boundedness of the matrix normal likelihood or the existence of a unique\nmaximizer depends in a complicated way on the matrix dimensions. This motivates\nthe study of how large a sample size is needed to ensure that maximum\nlikelihood estimators exist, and exist uniquely with probability one. Our main\nresult gives precise sample size thresholds in the paradigm where the number of\nrows and the number of columns of the data matrices differ by at most a factor\nof two. Our proof uses invariance properties that allow us to consider data\nmatrices in canonical form, as obtained from the Kronecker canonical form for\nmatrix pencils.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 21:19:18 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 19:51:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Drton", "Mathias", ""], ["Kuriki", "Satoshi", ""], ["Hoff", "Peter", ""]]}, {"id": "2003.06202", "submitter": "Thomas Hamm", "authors": "Thomas Hamm and Ingo Steinwart", "title": "Adaptive Learning Rates for Support Vector Machines Working on Data with\n  Low Intrinsic Dimension", "comments": "35 pages, accepted manuscript in The Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive improved regression and classification rates for support vector\nmachines using Gaussian kernels under the assumption that the data has some\nlow-dimensional intrinsic structure that is described by the box-counting\ndimension. Under some standard regularity assumptions for regression and\nclassification we prove learning rates, in which the dimension of the ambient\nspace is replaced by the box-counting dimension of the support of the data\ngenerating distribution. In the regression case our rates are in some cases\nminimax optimal up to logarithmic factors, whereas in the classification case\nour rates are minimax optimal up to logarithmic factors in a certain range of\nour assumptions and otherwise of the form of the best known rates. Furthermore,\nwe show that a training validation approach for choosing the hyperparameters of\nan SVM in a data dependent way achieves the same rates adaptively, that is\nwithout any knowledge on the data generating distribution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 11:10:16 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 10:59:06 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 11:13:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hamm", "Thomas", ""], ["Steinwart", "Ingo", ""]]}, {"id": "2003.06368", "submitter": "Naveed Merchant", "authors": "Jeffery Hart and Taeryon Choi and Naveed Merchant", "title": "Use of Cross-validation Bayes Factors to Test Equality of Two Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric, two-sample Bayesian test for checking whether or\nnot two data sets share a common distribution. The test makes use of data\nsplitting ideas and does not require priors for high-dimensional parameter\nvectors as do other nonparametric Bayesian procedures. We provide evidence that\nthe new procedure provides more stable Bayes factors than do methods based on\nP\\'olya trees. Somewhat surprisingly, the behavior of the proposed Bayes\nfactors when the two distributions are the same is usually superior to that of\nP\\'olya tree Bayes factors. We showcase the effectiveness of the test by\nproving its consistency, conducting a simulation study and applying the test to\nHiggs boson data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 16:26:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hart", "Jeffery", ""], ["Choi", "Taeryon", ""], ["Merchant", "Naveed", ""]]}, {"id": "2003.06502", "submitter": "Michele Caprio", "authors": "Michele Caprio and Sayan Mukherjee", "title": "Ergodic theorems for lower probability kinematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard Bayesian setting, there is often ambiguity in prior choice, as\none may have not sufficient information to uniquely identify a suitable prior\nprobability measure encapsulating initial beliefs. To overcome this, we specify\na set $\\mathcal{P}$ of plausible prior probability measures. As more and more\nobservations are collected, $\\mathcal{P}$ is updated using Jeffrey's rule of\nconditioning, a generalization of Bayesian updating which proves to be more\nphilosophically compelling in many situations. We build the sequence\n$(\\mathcal{P}^*_k)$ of successive updates of $\\mathcal{P}$ and we provide an\nergodic theory to analyze its limit, for both countable and uncountable sample\nspaces. A result of this ergodic theory is a strong law of large numbers in the\nuncountable setting. We also give a rule, that we call Jeffrey-Geometric rule,\nto update lower probabilities associated with the elements of\n$(\\mathcal{P}^*_k)$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 22:52:57 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 13:02:02 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 14:42:06 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 23:17:54 GMT"}, {"version": "v5", "created": "Sun, 3 May 2020 19:38:40 GMT"}, {"version": "v6", "created": "Fri, 6 Nov 2020 15:03:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Caprio", "Michele", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "2003.06511", "submitter": "Haiyun He", "authors": "Haiyun He, Qiaosheng Zhang, and Vincent Y. F. Tan", "title": "Optimal Change-Point Detection with Training Sequences in the Large and\n  Moderate Deviations Regimes", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel offline change-point detection problem from\nan information-theoretic perspective. In contrast to most related works, we\nassume that the knowledge of the underlying pre- and post-change distributions\nare not known and can only be learned from the training sequences which are\navailable. We further require the probability of the \\emph{estimation error} to\ndecay either exponentially or sub-exponentially fast (corresponding\nrespectively to the large and moderate deviations regimes in information theory\nparlance). Based on the training sequences as well as the test sequence\nconsisting of a single change-point, we design a change-point estimator and\nfurther show that this estimator is optimal by establishing matching (strong)\nconverses. This leads to a full characterization of the optimal confidence\nwidth (i.e., half the width of the confidence interval within which the true\nchange-point is located at with high probability) as a function of the\nundetected error, under both the large and moderate deviations regimes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 23:39:40 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 03:13:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 10:58:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["He", "Haiyun", ""], ["Zhang", "Qiaosheng", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2003.06725", "submitter": "Turku Ozlum Celik", "authors": "T\\\"urk\\\"u \\\"Ozl\\\"um \\c{C}elik, Asgar Jamneshan, Guido Mont\\'ufar,\n  Bernd Sturmfels, Lorenzo Venturello", "title": "Wasserstein Distance to Independence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An independence model for discrete random variables is a Segre-Veronese\nvariety in a probability simplex. Any metric on the set of joint states of the\nrandom variables induces a Wasserstein metric on the probability simplex. The\nunit ball of this polyhedral norm is dual to the Lipschitz polytope. Given any\ndata distribution, we seek to minimize its Wasserstein distance to a fixed\nindependence model. The solution to this optimization problem is a piecewise\nalgebraic function of the data. We compute this function explicitly in small\ninstances, we examine its combinatorial structure and algebraic degrees in the\ngeneral case, and we present some experimental case studies.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 00:17:24 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 04:22:11 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["\u00c7elik", "T\u00fcrk\u00fc \u00d6zl\u00fcm", ""], ["Jamneshan", "Asgar", ""], ["Mont\u00fafar", "Guido", ""], ["Sturmfels", "Bernd", ""], ["Venturello", "Lorenzo", ""]]}, {"id": "2003.06804", "submitter": "Chris U. Carmona", "authors": "Chris U. Carmona and Geoff K. Nicholls", "title": "Semi-Modular Inference: enhanced learning in multi-modular models by\n  tempering the influence of components", "comments": "for associated R package to reproduce results, see\n  https://github.com/christianu7/aistats2020smi", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian statistical inference loses predictive optimality when generative\nmodels are misspecified.\n  Working within an existing coherent loss-based generalisation of Bayesian\ninference, we show existing Modular/Cut-model inference is coherent, and write\ndown a new family of Semi-Modular Inference (SMI) schemes, indexed by an\ninfluence parameter, with Bayesian inference and Cut-models as special cases.\nWe give a meta-learning criterion and estimation procedure to choose the\ninference scheme. This returns Bayesian inference when there is no\nmisspecification.\n  The framework applies naturally to Multi-modular models. Cut-model inference\nallows directed information flow from well-specified modules to misspecified\nmodules, but not vice versa. An existing alternative power posterior method\ngives tunable but undirected control of information flow, improving prediction\nin some settings. In contrast, SMI allows tunable and directed information flow\nbetween modules.\n  We illustrate our methods on two standard test cases from the literature and\na motivating archaeological data set.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:55:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Carmona", "Chris U.", ""], ["Nicholls", "Geoff K.", ""]]}, {"id": "2003.06961", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, George Michailidis", "title": "Online detection of local abrupt changes in high-dimensional Gaussian\n  graphical models", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying change points in high-dimensional Gaussian\ngraphical models (GGMs) in an online fashion is of interest, due to new\napplications in biology, economics and social sciences. The offline version of\nthe problem, where all the data are a priori available, has led to a number of\nmethods and associated algorithms involving regularized loss functions.\nHowever, for the online version, there is currently only a single work in the\nliterature that develops a sequential testing procedure and also studies its\nasymptotic false alarm probability and power. The latter test is best suited\nfor the detection of change points driven by global changes in the structure of\nthe precision matrix of the GGM, in the sense that many edges are involved.\nNevertheless, in many practical settings the change point is driven by local\nchanges, in the sense that only a small number of edges exhibit changes. To\nthat end, we develop a novel test to address this problem that is based on the\n$\\ell_\\infty$ norm of the normalized covariance matrix of an appropriately\nselected portion of incoming data. The study of the asymptotic distribution of\nthe proposed test statistic under the null (no presence of a change point) and\nthe alternative (presence of a change point) hypotheses requires new technical\ntools that examine maxima of graph-dependent Gaussian random variables, and\nthat of independent interest. It is further shown that these tools lead to the\nimposition of mild regularity conditions for key model parameters, instead of\nmore stringent ones required by leveraging previously used tools in related\nproblems in the literature. Numerical work on synthetic data illustrates the\ngood performance of the proposed detection procedure both in terms of\ncomputational and statistical efficiency across numerous experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:41:34 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Michailidis", "George", ""]]}, {"id": "2003.07315", "submitter": "Antony Overstall", "authors": "Antony M. Overstall", "title": "Properties of using Fisher information gain for Bayesian design of\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian decision-theoretic approach to design of experiments involves\nspecifying a design (values of all controllable variables) to maximise the\nexpected utility function (expectation with respect to the distribution of\nresponses and parameters). For most common utility functions, the expected\nutility is rarely available in closed form and requires a computationally\nexpensive approximation which then needs to be maximised over the space of all\npossible designs. This hinders practical use of the Bayesian approach to find\nexperimental designs. However, recently, a new utility called Fisher\ninformation gain has been proposed. The resulting expected Fisher information\ngain reduces to the prior expectation of the trace of the Fisher information\nmatrix. Since the Fisher information is often available in closed form, this\nsignificantly simplifies approximation and subsequent identification of optimal\ndesigns. In this paper, it is shown that for exponential family models,\nmaximising the expected Fisher information gain is equivalent to maximising an\nalternative objective function over a reduced-dimension space, simplifying even\nfurther the identification of optimal designs. However, if this function does\nnot have enough global maxima, then designs that maximise the expected Fisher\ninformation gain lead to non-identifiablility.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:38:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 13:08:58 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 12:10:11 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Overstall", "Antony M.", ""]]}, {"id": "2003.07331", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum, Stefan Steinerberger", "title": "Randomly Aggregated Least Squares for Support Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of exact support recovery: given an (unknown) vector\n$\\theta \\in \\left\\{-1,0,1\\right\\}^D$, we are given access to the noisy\nmeasurement $$ y = X\\theta + \\omega,$$ where $X \\in \\mathbb{R}^{N \\times D}$ is\na (known) Gaussian matrix and the noise $\\omega \\in \\mathbb{R}^N$ is an\n(unknown) Gaussian vector. How small we can choose $N$ and still reliably\nrecover the support of $\\theta$? We present RAWLS (Randomly Aggregated\nUnWeighted Least Squares Support Recovery): the main idea is to take random\nsubsets of the $N$ equations, perform a least squares recovery over this\nreduced bit of information and then average over many random subsets. We show\nthat the proposed procedure can provably recover an approximation of $\\theta$\nand demonstrate its use in support recovery through numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:04:35 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 08:22:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "2003.07545", "submitter": "Zhaonan Qu", "authors": "Zhaonan Qu, Yinyu Ye, Zhengyuan Zhou", "title": "Diagonal Preconditioning: Theory and Algorithms", "comments": "Under review, previous version wrong draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagonal preconditioning has been a staple technique in optimization and\nmachine learning. It often reduces the condition number of the design or\nHessian matrix it is applied to, thereby speeding up convergence. However,\nrigorous analyses of how well various diagonal preconditioning procedures\nimprove the condition number of the preconditioned matrix and how that\ntranslates into improvements in optimization are rare. In this paper, we first\nprovide an analysis of a popular diagonal preconditioning technique based on\ncolumn standard deviation and its effect on the condition number using random\nmatrix theory. Then we identify a class of design matrices whose condition\nnumbers can be reduced significantly by this procedure. We then study the\nproblem of optimal diagonal preconditioning to improve the condition number of\nany full-rank matrix and provide a bisection algorithm and a potential\nreduction algorithm with $O(\\log(\\frac{1}{\\epsilon}))$ iteration complexity,\nwhere each iteration consists of an SDP feasibility problem and a Newton update\nusing the Nesterov-Todd direction, respectively. Finally, we extend the optimal\ndiagonal preconditioning algorithm to an adaptive setting and compare its\nempirical performance at reducing the condition number and speeding up\nconvergence for regression and classification problems with that of another\nadaptive preconditioning technique, namely batch normalization, that is\nessential in training machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 05:48:27 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 00:37:49 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Qu", "Zhaonan", ""], ["Ye", "Yinyu", ""], ["Zhou", "Zhengyuan", ""]]}, {"id": "2003.07878", "submitter": "Mikhail Boldin", "authors": "Michael Boldin", "title": "On Symmetrized Pearson's Type Test in Autoregression with Outliers:\n  Robust Testing of Normality", "comments": "In Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary linear AR($p$) model with observations subject to\ngross errors (outliers). The autoregression parameters are unknown as well as\nthe distribution and moments of innoovations. The distribution of outliers\n$\\Pi$ is unknown and arbitrary, their intensity is $\\gamma n^{-1/2}$ with an\nunknown $\\gamma$, $n$ is the sample size. The autoregression parameters are\nestimated by any estimator which is $n^{1/2}$-consistent uniformly in\n$\\gamma\\leq \\Gamma<\\infty$. Using the residuals from the estimated\nautoregression, we construct a kind of empirical distribution function\n(e.d.f.), which is a counterpart of the (inaccessible) e.d.f. of the\nautoregression innovations. We obtain a stochastic expansion of this e.d.f.,\nwhich enables us to construct the symmetrized test of Pearson's chi-square type\nfor the normality of distribution of innovations. We establish qualitative\nrobustness of these tests in terms of uniform equicontinuity of the limiting\nlevels (as functions of $\\gamma$ and $\\Pi$) with respect to $\\gamma$ in a\nneighborhood of $\\gamma=0$.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 18:29:06 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Boldin", "Michael", ""]]}, {"id": "2003.07880", "submitter": "Dan Li", "authors": "Dan Li and Sonia Mart\\'inez", "title": "High-Confidence Attack Detection via Wasserstein-Metric Computations", "comments": "Submitted to Control system letters", "journal-ref": null, "doi": "10.1109/LCSYS.2020.3002689", "report-no": null, "categories": "math.DS cs.SY eess.SY math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a sensor attack and fault detection problem for linear\ncyber-physical systems, which are subject to system noise that can obey an\nunknown light-tailed distribution. We propose a new threshold-based detection\nmechanism that employs the Wasserstein metric, and which guarantees system\nperformance with high confidence employing a finite number of measurements. The\nproposed detector may generate false alarms with a rate $\\Delta$ in normal\noperation, where $\\Delta$ can be tuned to be arbitrarily small by means of a\nbenchmark distribution which is part of our mechanism. Thus, the proposed\ndetector is sensitive to sensor attacks and faults which have a statistical\nbehavior that is different from that of the system's noise. We quantify the\nimpact of stealthy attacks---which aim to perturb the system operation while\nproducing false alarms that are consistent with the natural system's\nnoise---via a probabilistic reachable set. To enable tractable implementation\nof our methods, we propose a linear optimization problem that computes the\nproposed detection measure and a semidefinite program that produces the\nproposed reachable set.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 18:32:10 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 03:25:31 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Dan", ""], ["Mart\u00ednez", "Sonia", ""]]}, {"id": "2003.07898", "submitter": "Kun Chen", "authors": "Kun Chen, Ruipeng Dong, Wanwan Xu, Zemin Zheng", "title": "Statistically Guided Divide-and-Conquer for Sparse Factorization of\n  Large Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse factorization of a large matrix is fundamental in modern\nstatistical learning. In particular, the sparse singular value decomposition\nand its variants have been utilized in multivariate regression, factor\nanalysis, biclustering, vector time series modeling, among others. The appeal\nof this factorization is owing to its power in discovering a\nhighly-interpretable latent association network, either between samples and\nvariables or between responses and predictors. However, many existing methods\nare either ad hoc without a general performance guarantee, or are\ncomputationally intensive, rendering them unsuitable for large-scale studies.\nWe formulate the statistical problem as a sparse factor regression and tackle\nit with a divide-and-conquer approach. In the first stage of division, we\nconsider both sequential and parallel approaches for simplifying the task into\na set of co-sparse unit-rank estimation (CURE) problems, and establish the\nstatistical underpinnings of these commonly-adopted and yet poorly understood\ndeflation methods. In the second stage of division, we innovate a contended\nstagewise learning technique, consisting of a sequence of simple incremental\nupdates, to efficiently trace out the whole solution paths of CURE. Our\nalgorithm has a much lower computational complexity than alternating convex\nsearch, and the choice of the step size enables a flexible and principled\ntradeoff between statistical accuracy and computational efficiency. Our work is\namong the first to enable stagewise learning for non-convex problems, and the\nidea can be applicable in many multi-convex problems. Extensive simulation\nstudies and an application in genetics demonstrate the effectiveness and\nscalability of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:12:21 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chen", "Kun", ""], ["Dong", "Ruipeng", ""], ["Xu", "Wanwan", ""], ["Zheng", "Zemin", ""]]}, {"id": "2003.07937", "submitter": "Yassir Jedra", "authors": "Yassir Jedra and Alexandre Proutiere", "title": "Finite-time Identification of Stable Linear Systems: Optimality of the\n  Least-Squares Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.SY eess.SY stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new finite-time analysis of the estimation error of the Ordinary\nLeast Squares (OLS) estimator for stable linear time-invariant systems. We\ncharacterize the number of observed samples (the length of the observed\ntrajectory) sufficient for the OLS estimator to be $(\\varepsilon,\\delta)$-PAC,\ni.e., to yield an estimation error less than $\\varepsilon$ with probability at\nleast $1-\\delta$. We show that this number matches existing sample complexity\nlower bounds [1,2] up to universal multiplicative factors (independent of\n($\\varepsilon,\\delta)$ and of the system). This paper hence establishes the\noptimality of the OLS estimator for stable systems, a result conjectured in\n[1]. Our analysis of the performance of the OLS estimator is simpler, sharper,\nand easier to interpret than existing analyses. It relies on new concentration\nresults for the covariates matrix.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:59:17 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 20:50:34 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 17:54:55 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Jedra", "Yassir", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "2003.07980", "submitter": "Cecilia Mondaini", "authors": "Nathan E. Glatt-Holtz and Cecilia F. Mondaini", "title": "Mixing Rates for Hamiltonian Monte Carlo Algorithms in Finite and\n  Infinite Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the geometric ergodicity of the preconditioned Hamiltonian Monte\nCarlo (HMC) algorithm defined on an infinite-dimensional Hilbert space, as\ndeveloped in [Beskos et al., Stochastic Process. Appl., 2011]. This algorithm\ncan be used as a basis to sample from certain classes of target measures which\nare absolutely continuous with respect to a Gaussian measure. Our work\naddresses an open question posed in [Beskos et al., Stochastic Process. Appl.,\n2011], and provides an alternative to a recent proof based on exact coupling\ntechniques given in arXiv:1909.07962. The approach here establishes convergence\nin a suitable Wasserstein distance by using the weak Harris theorem together\nwith a generalized coupling argument. We also show that a law of large numbers\nand central limit theorem can be derived as a consequence of our main\nconvergence result. Moreover, our approach yields a novel proof of mixing rates\nfor the classical finite-dimensional HMC algorithm. As such, the methodology we\ndevelop provides a flexible framework to tackle the rigorous convergence of\nother Markov Chain Monte Carlo algorithms. Additionally, we show that the scope\nof our result includes certain measures that arise in the Bayesian approach to\ninverse PDE problems, cf. [Stuart, Acta Numer., 2010]. Particularly, we verify\nall of the required assumptions for a certain class of inverse problems\ninvolving the recovery of a divergence free vector field from a passive scalar,\narXiv:1808.01084v3.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 23:09:47 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Glatt-Holtz", "Nathan E.", ""], ["Mondaini", "Cecilia F.", ""]]}, {"id": "2003.08087", "submitter": "Andrew Karl", "authors": "Andrew T. Karl and Dale L. Zimmerman", "title": "A Diagnostic for Bias in Linear Mixed Model Estimators Induced by\n  Dependence Between the Random Effects and the Corresponding Model Matrix", "comments": "26 pages, 4 figures, 2 tables", "journal-ref": "Journal of Statistical Planning and Inference, 211, March 2021,\n  Pages 107-118", "doi": "10.1016/j.jspi.2020.06.004", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how violations of the often-overlooked standard assumption that\nthe random effects model matrix in a linear mixed model is fixed (and thus\nindependent of the random effects vector) can lead to bias in estimators of\nestimable functions of the fixed effects. However, if the random effects of the\noriginal mixed model are instead also treated as fixed effects, or if the fixed\nand random effects model matrices are orthogonal with respect to the inverse of\nthe error covariance matrix (with probability one), or if the random effects\nand the corresponding model matrix are independent, then these estimators are\nunbiased. The bias in the general case is quantified and compared to a\nrandomized permutation distribution of the predicted random effects, producing\nan informative summary graphic for each estimator of interest. This is\ndemonstrated through the examination of sporting outcomes used to estimate a\nhome field advantage.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 08:25:02 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 20:01:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Karl", "Andrew T.", ""], ["Zimmerman", "Dale L.", ""]]}, {"id": "2003.08109", "submitter": "Remi Jezequel", "authors": "R\\'emi J\\'ez\\'equel (SIERRA), Pierre Gaillard (SIERRA), Alessandro\n  Rudi (SIERRA)", "title": "Efficient improper learning for online logistic regression", "comments": null, "journal-ref": "Conference on Learning Theory 2020, Jul 2020, Graz, Austria", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of online logistic regression and consider the regret\nwith respect to the 2-ball of radius B. It is known (see [Hazan et al., 2014])\nthat any proper algorithm which has logarithmic regret in the number of samples\n(denoted n) necessarily suffers an exponential multiplicative constant in B. In\nthis work, we design an efficient improper algorithm that avoids this\nexponential constant while preserving a logarithmic regret. Indeed, [Foster et\nal., 2018] showed that the lower bound does not apply to improper algorithms\nand proposed a strategy based on exponential weights with prohibitive\ncomputational complexity. Our new algorithm based on regularized empirical risk\nminimization with surrogate losses satisfies a regret scaling as O(B log(Bn))\nwith a per-round time-complexity of order O(d^2).\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 09:16:14 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:00:39 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 08:01:06 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["J\u00e9z\u00e9quel", "R\u00e9mi", "", "SIERRA"], ["Gaillard", "Pierre", "", "SIERRA"], ["Rudi", "Alessandro", "", "SIERRA"]]}, {"id": "2003.08222", "submitter": "Jing Lei", "authors": "Jing Lei and Kevin Z. Lin", "title": "Bias-adjusted spectral clustering in multi-layer stochastic block models", "comments": "49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating common community structures in\nmulti-layer stochastic block models, where each single layer may not have\nsufficient signal strength to recover the full community structure. In order to\nefficiently aggregate signal across different layers, we argue that the\nsum-of-squared adjacency matrices contains sufficient signal even when\nindividual layers are very sparse. Our method features a bias-removal step that\nis necessary when the squared noise matrices may overwhelm the signal in the\nvery sparse regime. The analysis of our method uses several novel tail\nprobability bounds for matrix linear combinations with matrix-valued\ncoefficients and matrix-valued quadratic forms, which may be of independent\ninterest. The performance of our method and the necessity of bias removal is\ndemonstrated in synthetic data and in microarray analysis about gene\nco-expression networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 13:51:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 19:56:37 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 15:37:14 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Lei", "Jing", ""], ["Lin", "Kevin Z.", ""]]}, {"id": "2003.08449", "submitter": "Tyrel Stokes", "authors": "Tyrel Stokes, Russell Steele, Ian Shrier", "title": "Causal Simulation Experiments: Lessons from Bias Amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work in causal inference has explored an important class\nof variables which, when conditioned on, may further amplify existing\nunmeasured confounding bias (bias amplification). Despite this theoretical\nwork, existing simulations of bias amplification in clinical settings have\nsuggested bias amplification may not be as important in many practical cases as\nsuggested in the theoretical literature.We resolve this tension by using tools\nfrom the semi-parametric regression literature leading to a general\ncharacterization in terms of the geometry of OLS estimators which allows us to\nextend current results to a larger class of DAGs, functional forms, and\ndistributional assumptions. We further use these results to understand the\nlimitations of current simulation approaches and to propose a new framework for\nperforming causal simulation experiments to compare estimators. We then\nevaluate the challenges and benefits of extending this simulation approach to\nthe context of a real clinical data set with a binary treatment, laying the\ngroundwork for a principled approach to sensitivity analysis for bias\namplification in the presence of unmeasured confounding.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 19:33:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Stokes", "Tyrel", ""], ["Steele", "Russell", ""], ["Shrier", "Ian", ""]]}, {"id": "2003.08544", "submitter": "Masaaki Fukasawa", "authors": "Masaaki Fukasawa", "title": "EM algorithm for stochastic hybrid systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic hybrid system, also known as a switching diffusion, is a\ncontinuous-time Markov process with state space consisting of discrete and\ncontinuous parts. We consider parametric estimation of theQmatrix for the\ndiscrete state transitions and of the drift coefficient for the diffusion part.\nFirst, we derive the likelihood function under the complete observation of a\nsample path in continuous-time. Then, extending a finite-dimensional filter for\nhidden Markov models developed by Elliott et al. (Hidden Markov Models,\nSpringer, 1995) to stochastic hybrid systems, we derive the likelihood function\nand the EM algorithm under a partial observation where the continuous state is\nmonitored continuously in time, while the discrete state is unobserved.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 02:32:26 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:56:11 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fukasawa", "Masaaki", ""]]}, {"id": "2003.08571", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama and William E. Strawderman", "title": "Admissible estimators of a multivariate normal mean vector when the\n  scale is unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study admissibility of a subclass of generalized Bayes estimators of a\nmultivariate normal vector when the variance is unknown, under scaled quadratic\nloss. Minimaxity is also established for certain of these estimators.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:03:42 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "2003.08614", "submitter": "F. Richard Guo", "authors": "F. Richard Guo and Thomas S. Richardson", "title": "Chernoff-type Concentration of Empirical Probabilities in Relative\n  Entropy", "comments": "corrected a numerical error", "journal-ref": null, "doi": "10.1109/TIT.2020.3034539", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relative entropy of the empirical probability vector with\nrespect to the true probability vector in multinomial sampling of $k$\ncategories, which, when multiplied by sample size $n$, is also the\nlog-likelihood ratio statistic. We generalize a recent result and show that the\nmoment generating function of the statistic is bounded by a polynomial of\ndegree $n$ on the unit interval, uniformly over all true probability vectors.\nWe characterize the family of polynomials indexed by $(k,n)$ and obtain\nexplicit formulae. Consequently, we develop Chernoff-type tail bounds,\nincluding a closed-form version from a large sample expansion of the bound\nminimizer. Our bound dominates the classic method-of-types bound and is\ncompetitive with the state of the art. We demonstrate with an application to\nestimating the proportion of unseen butterflies.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:45:12 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 22:15:31 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 01:53:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Guo", "F. Richard", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "2003.08787", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "A comparison of Hurst exponent estimators in long-range dependent curve\n  time series", "comments": "36 pages, 4 tables", "journal-ref": "Journal of Time Series Econometrics, 2020, 12(1)", "doi": "10.1515/jtse-2019-0009", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hurst exponent is the simplest numerical summary of self-similar\nlong-range dependent stochastic processes. We consider the estimation of Hurst\nexponent in long-range dependent curve time series. Our estimation method\nbegins by constructing an estimate of the long-run covariance function, which\nwe use, via dynamic functional principal component analysis, in estimating the\northonormal functions spanning the dominant sub-space of functional time\nseries. Within the context of functional autoregressive fractionally integrated\nmoving average models, we compare finite-sample bias, variance and mean square\nerror among some time- and frequency-domain Hurst exponent estimators and make\nour recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:51:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2003.08847", "submitter": "Claudia Kirch", "authors": "Claudia Kirch and Christina Stoehr", "title": "Asymptotic delay times of sequential tests based on U-statistics for\n  early and late change points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential change point tests aim at giving an alarm as soon as possible\nafter a structural break occurs while controlling the asymptotic false alarm\nerror. For such tests it is of particular importance to understand how quickly\na break is detected. While this is often assessed by simulations only, in this\npaper, we derive the asymptotic distribution of the delay time for sequential\nchange point procedures based on U-statistics. This includes the\ndifference-of-means (DOM) sequential test, that has been discussed previously,\nbut also a new robust Wilcoxon sequential change point test. Similar to\nasymptotic relative efficiency in an a-posteriori setting, the results allow us\nto compare the detection delay of the two procedures. It is shown that the\nWilcoxon sequential procedure has a smaller detection delay for heavier tailed\ndistributions which is also confirmed by simulations. While the previous\nliterature only derives results for early change points, we obtain the\nasymptotic distribution of the delay time for both early as well as late change\npoints. Finally, we evaluate how well the asymptotic distribution approximates\nthe actual stopping times for finite samples via a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 14:57:14 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kirch", "Claudia", ""], ["Stoehr", "Christina", ""]]}, {"id": "2003.08920", "submitter": "Hyun-Jung Kim", "authors": "Igor Cialenco and Hyun-Jung Kim", "title": "Parameter estimation for discretely sampled stochastic heat equation\n  driven by space-only noise", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive consistent and asymptotically normal estimators for the drift and\nvolatility parameters of the stochastic heat equation driven by an additive\nspace-only white noise when the solution is sampled discretely in the physical\ndomain. We consider both the full space and the bounded domain. We establish\nthe exact spatial regularity of the solution, which in turn, using\npower-variation arguments, allows building the desired estimators. We show that\nnaive approximations of the derivatives appearing in the power-variation based\nestimators may create nontrivial biases, which we compute explicitly. The\nproofs are rooted in Malliavin-Stein's method.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:34:45 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 18:17:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Cialenco", "Igor", ""], ["Kim", "Hyun-Jung", ""]]}, {"id": "2003.08967", "submitter": "Alex Dytso", "authors": "Alex Dytso, Michael Fauss, and H. Vincent Poor", "title": "The Vector Poisson Channel: On the Linearity of the Conditional Mean\n  Estimator", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3025525", "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies properties of the conditional mean estimator in vector\nPoisson noise. The main emphasis is to study conditions on prior distributions\nthat induce linearity of the conditional mean estimator. The paper consists of\ntwo main results. The first result shows that the only distribution that\ninduces the linearity of the conditional mean estimator is a product gamma\ndistribution. Moreover, it is shown that the conditional mean estimator cannot\nbe linear when the dark current parameter of the Poisson noise is non-zero. The\nsecond result produces a quantitative refinement of the first result.\nSpecifically, it is shown that if the conditional mean estimator is close to\nlinear in a mean squared error sense, then the prior distribution must be close\nto a product gamma distribution in terms of their characteristic functions.\nFinally, the results are compared to their Gaussian counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:21:33 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dytso", "Alex", ""], ["Fauss", "Michael", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2003.08989", "submitter": "Minge Xie", "authors": "Min-ge Xie and Zheshi Zheng", "title": "Homeostasis phenomenon in predictive inference when using a wrong\n  learning model: a tale of random split of data into training and test sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note uses a conformal prediction procedure to provide further support on\nseveral points discussed by Professor Efron (Efron, 2020) concerning\nprediction, estimation and IID assumption. It aims to convey the following\nmessages: (1) Under the IID (e.g., random split of training and testing data\nsets) assumption, prediction is indeed an easier task than estimation, since\nprediction has a 'homeostasis property' in this case -- Even if the model used\nfor learning is completely wrong, the prediction results maintain valid. (2) If\nthe IID assumption is violated (e.g., a targeted prediction on specific\nindividuals), the homeostasis property is often disrupted and the prediction\nresults under a wrong model are usually invalid. (3) Better model estimation\ntypically leads to more accurate prediction in both IID and non-IID cases. Good\nmodeling and estimation practices are important and, in many times, crucial for\nobtaining good prediction results. The discussion also provides one explanation\nwhy the deep learning method works so well in academic exercises (with\nexperiments set up by randomly splitting the entire data into training and\ntesting data sets), but fails to deliver many `killer applications' in real\nworld applications.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 19:12:01 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xie", "Min-ge", ""], ["Zheng", "Zheshi", ""]]}, {"id": "2003.09091", "submitter": "Yanglei Song", "authors": "Yanglei Song, Xiaohui Chen, Kengo Kato", "title": "Stratified incomplete local simplex tests for curvature of nonparametric\n  multiple regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principled nonparametric tests for regression curvature in $\\mathbb{R}^{d}$\nare often statistically and computationally challenging. This paper introduces\nthe stratified incomplete local simplex (SILS) tests for joint concavity of\nnonparametric multiple regression. The SILS tests with suitable bootstrap\ncalibration are shown to achieve simultaneous guarantees on dimension-free\ncomputational complexity, polynomial decay of the uniform error-in-size, and\npower consistency for general (global and local) alternatives. To establish\nthese results, a general theory for incomplete $U$-processes with stratified\nrandom sparse weights is developed. Novel technical ingredients include maximal\ninequalities for the supremum of multiple incomplete $U$-processes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:39:52 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 04:47:06 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Song", "Yanglei", ""], ["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "2003.09126", "submitter": "Marta Ferreira", "authors": "Helena Ferreira and Marta Ferreira", "title": "The stopped clock model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme values theory presents specific tools for modeling and predicting\nextreme phenomena. In particular, risk assessment is often analyzed through\nmeasures for tail dependence and high values clustering. Despite technological\nadvances allowing an increasingly larger and more efficient data collection,\nthere are sometimes failures in the records, which causes difficulties in\nstatistical inference, especially in the tail where data are scarcer. In this\narticle we present a model with a simple and intuitive failures scheme, where\neach record failure is replaced by the last record available. We will study its\nextremal behavior with regard to local dependence and high values clustering,\nas well as the temporal dependence on the tail.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 07:13:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""]]}, {"id": "2003.09219", "submitter": "Sebastian Reich", "authors": "Sebastian Reich and Paul Rozdeba", "title": "Posterior contraction rates for non-parametric state and drift\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a combined state and drift estimation problem for the linear\nstochastic heat equation. The infinite-dimensional Bayesian inference problem\nis formulated in terms of the Kalman-Bucy filter over an extended state space,\nand its long-time asymptotic properties are studied. Asymptotic posterior\ncontraction rates in the unknown drift function are the main contribution of\nthis paper. Such rates have been studied before for stationary non-parametric\nBayesian inverse problems, and here we demonstrate the consistency of our\ntime-dependent formulation with these previous results building upon scale\nseparation and a slow manifold approximation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 12:11:29 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 14:44:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Reich", "Sebastian", ""], ["Rozdeba", "Paul", ""]]}, {"id": "2003.09281", "submitter": "C\\'eline Duval", "authors": "C\\'eline Duval and Ester Mariucci", "title": "Non-asymptotic control of the cumulative distribution function of L\\'evy\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose non-asymptotic controls of the cumulative distribution function\n$P(|X_{t}|\\ge \\varepsilon)$, for any $t>0$, $\\varepsilon>0$ and any L\\'evy\nprocess $X$ such that its L\\'evy density is bounded from above by the density\nof an $\\alpha$-stable type L\\'evy process in a neighborhood of the origin. The\nresults presented are non-asymptotic and optimal, they apply to a large class\nof L\\'evy processes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 14:03:03 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Duval", "C\u00e9line", ""], ["Mariucci", "Ester", ""]]}, {"id": "2003.09318", "submitter": "Ana Carpio", "authors": "Ana Carpio, Sergei Iakunin, Georg Stadler", "title": "Bayesian approach to inverse scattering with topological priors", "comments": null, "journal-ref": "Inverse Problems 36, 105001, 2020", "doi": "10.1088/1361-6420/abaa30", "report-no": null, "categories": "math.NA cs.NA math.OC math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian inference framework to estimate uncertainties in\ninverse scattering problems. Given the observed data, the forward model and\ntheir uncertainties, we find the posterior distribution over a finite parameter\nfield representing the objects. To construct the prior distribution we use a\ntopological sensitivity analysis. We demonstrate the approach on the Bayesian\nsolution of 2D inverse problems in light and acoustic holography with synthetic\ndata. Statistical information on objects such as their center location,\ndiameter size, orientation, as well as material properties, are extracted by\nsampling the posterior distribution. Assuming the number of objects known,\ncomparison of the results obtained by Markov Chain Monte Carlo sampling and by\nsampling a Gaussian distribution found by linearization about the maximum a\nposteriori estimate show reasonable agreement. The latter procedure has low\ncomputational cost, which makes it an interesting tool for uncertainty studies\nin 3D. However, MCMC sampling provides a more complete picture of the posterior\ndistribution and yields multi-modal posterior distributions for problems with\nlarger measurement noise. When the number of objects is unknown, we devise a\nstochastic model selection framework.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 15:08:35 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 11:12:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Carpio", "Ana", ""], ["Iakunin", "Sergei", ""], ["Stadler", "Georg", ""]]}, {"id": "2003.09389", "submitter": "Anirban Das", "authors": "Anirban Das, Manfred Denker, Anna Levina and Lucia Tabacu", "title": "Estimations by stable motions and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric parameter estimation of confidence intervals when\nthe underlying has large or infinite variance. We explain the method by a\nsimple numerical example and provide an application to estimate the coupling\nstrength in neuronal networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:57:16 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Das", "Anirban", ""], ["Denker", "Manfred", ""], ["Levina", "Anna", ""], ["Tabacu", "Lucia", ""]]}, {"id": "2003.09467", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang and Melike Oguz-Alper", "title": "BIG sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sampling is a statistical approach to study real graphs, which\nrepresent the structure of many technological, social or biological phenomena\nof interest. We develop bipartite incident graph sampling (BIGS) as a feasible\nrepresentation of graph sampling from arbitrary finite graphs. It provides also\na unified treatment of the existing unconventional sampling methods which were\nstudied separately in the past, including indirect, network and adaptive\ncluster sampling. The sufficient and necessary conditions of feasible BIGS\nrepresentation are established, given which one can apply a family of\nHansen-Hurwitz type design-unbiased estimators in addition to the standard\nHorvitz-Thompson estimator. The approach increases therefore the potentials of\nefficiency gains in graph sampling. A general result regarding the relative\nefficiency of the two types of estimators is obtained. Numerical examples are\ngiven to illustrate the versatility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 19:06:05 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhang", "Li-Chun", ""], ["Oguz-Alper", "Melike", ""]]}, {"id": "2003.09493", "submitter": "Holger Dette", "authors": "Holger Dette, Xin Liu, Rong-Xian Yue", "title": "Design admissibility and de la Garza phenomenon in multi-factor\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of an optimal design for a given regression problem is an\nintricate optimization problem, especially for models with multivariate\npredictors. Design admissibility and invariance are main tools to reduce the\ncomplexity of the optimization problem and have been successfully applied for\nmodels with univariate predictors. In particular several authors have developed\nsufficient conditions for the existence of saturated designs in univariate\nmodels, where the number of support points of the optimal design equals the\nnumber of parameters. These results generalize the celebrated de la Garza\nphenomenon (de la Garza, 1954) which states that for a polynomial regression\nmodel of degree $p-1$ any optimal design can be based on at most $p$ points.\nThis paper provides - for the first time - extensions of these results for\nmodels with a multivariate predictor. In particular we study a geometric\ncharacterization of the support points of an optimal design to provide\nsufficient conditions for the occurrence of the de la Garza phenomenon in\nmodels with multivariate predictors and characterize properties of admissible\ndesigns in terms of admissibility of designs in conditional univariate\nregression models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:45:03 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Dette", "Holger", ""], ["Liu", "Xin", ""], ["Yue", "Rong-Xian", ""]]}, {"id": "2003.09555", "submitter": "Qian Qin", "authors": "Qian Qin, James P. Hobert", "title": "On the limitations of single-step drift and minorization in Markov chain\n  convergence analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last three decades, there has been a considerable effort within the\napplied probability community to develop techniques for bounding the\nconvergence rates of general state space Markov chains. Most of these results\nassume the existence of drift and minorization (d\\&m) conditions. It has often\nbeen observed that convergence rate bounds based on single-step d\\&m tend to be\noverly conservative, especially in high-dimensional situations. This article\nbuilds a framework for studying this phenomenon. It is shown that any\nconvergence rate bound based on a set of d\\&m conditions cannot do better than\na certain unknown optimal bound. Strategies are designed to put bounds on the\noptimal bound itself, and this allows one to quantify the extent to which a\nd\\&m-based convergence rate bound can be sharp. The new theory is applied to\nseveral examples, including a Gaussian autoregressive process (whose true\nconvergence rate is known), and a Metropolis adjusted Langevin algorithm. The\nresults strongly suggest that convergence rate bounds based on single-step d\\&m\nconditions are quite inadequate in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:21:08 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 21:55:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""]]}, {"id": "2003.09636", "submitter": "Christopher Strothmann", "authors": "Karl Friedrich Siburg and Christopher Strothmann", "title": "A Markov product for tail dependence functions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmaa.2021.124942", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Markov product structure for multivariate tail dependence\nfunctions, building upon the well-known Markov product for copulas. We\ninvestigate algebraic and monotonicity properties of this new product as well\nas its role in describing the tail behaviour of the Markov product of copulas.\nFor the bivariate case, we show additional smoothing properties and derive a\ncharacterization of idempotents together with the limiting behaviour of n-fold\niterations. Finally, we establish a one-to-one correspondence between bivariate\ntail dependence functions and a class of positive, substochastic operators.\nThese operators are contractions both on $L^1(\\mathbb{R}_+)$ and\n$L^\\infty(\\mathbb{R}_+)$ and constitute a natural generalization of Markov\noperators.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 11:43:57 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 09:02:02 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Siburg", "Karl Friedrich", ""], ["Strothmann", "Christopher", ""]]}, {"id": "2003.09830", "submitter": "Ismaila Ba", "authors": "Isma\\\"ila Ba and Jean-Fran\\c{c}ois Coeurjolly", "title": "Inference for possibly high-dimensional inhomogeneous Gibbs point\n  processes", "comments": "55 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs point processes (GPPs) constitute a large and flexible class of spatial\npoint processes with explicit dependence between the points. They can model\nattractive as well as repulsive point patterns. Feature selection procedures\nare an important topic in high-dimensional statistical modeling. In this paper,\ncomposite likelihood approach regularized with convex and non-convex penalty\nfunctions is proposed to handle statistical inference for possibly\nhigh-dimensional inhomogeneous GPPs. The composite likelihood incorporates both\nthe pseudo-likelihood and the logistic composite likelihood. We particularly\ninvestigate the setting where the number of covariates diverges as the domain\nof observation increases. Under some conditions provided on the spatial GPP and\non the penalty functions, we show that the oracle property, the consistency and\nthe asymptotic normality hold. Our results also cover the low-dimensional case\nwhich fills a large gap in the literature. Through simulation experiments, we\nvalidate our theoretical results and finally, an application to a tropical\nforestry dataset illustrates the use of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 07:25:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 13:09:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ba", "Isma\u00efla", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""]]}, {"id": "2003.09915", "submitter": "Ashesh Rambachan", "authors": "Iavor Bojinov, Ashesh Rambachan, and Neil Shephard", "title": "Panel Experiments and Dynamic Causal Effects: A Finite Population\n  Perspective", "comments": "Forthcoming in Quantitative Economics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In panel experiments, we randomly assign units to different interventions,\nmeasuring their outcomes, and repeating the procedure in several periods. Using\nthe potential outcomes framework, we define finite population dynamic causal\neffects that capture the relative effectiveness of alternative treatment paths.\nFor a rich class of dynamic causal effects, we provide a nonparametric\nestimator that is unbiased over the randomization distribution and derive its\nfinite population limiting distribution as either the sample size or the\nduration of the experiment increases. We develop two methods for inference: a\nconservative test for weak null hypotheses and an exact randomization test for\nsharp null hypotheses. We further analyze the finite population probability\nlimit of linear fixed effects estimators. These commonly-used estimators do not\nrecover a causally interpretable estimand if there are dynamic causal effects\nand serial correlation in the assignments, highlighting the value of our\nproposed estimator.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 14:58:03 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 23:08:59 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 21:34:59 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 19:56:55 GMT"}, {"version": "v5", "created": "Thu, 27 May 2021 13:44:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bojinov", "Iavor", ""], ["Rambachan", "Ashesh", ""], ["Shephard", "Neil", ""]]}, {"id": "2003.09960", "submitter": "Kaizheng Wang", "authors": "Kaizheng Wang, Yuling Yan, Mateo Diaz", "title": "Efficient Clustering for Stretched Mixtures: Landscape and Optimality", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a canonical clustering problem where one receives\nunlabeled samples drawn from a balanced mixture of two elliptical distributions\nand aims for a classifier to estimate the labels. Many popular methods\nincluding PCA and k-means require individual components of the mixture to be\nsomewhat spherical, and perform poorly when they are stretched. To overcome\nthis issue, we propose a non-convex program seeking for an affine transform to\nturn the data into a one-dimensional point cloud concentrating around -1 and 1,\nafter which clustering becomes easy. Our theoretical contributions are\ntwo-fold: (1) we show that the non-convex loss function exhibits desirable\nlandscape properties as long as the sample size exceeds some constant multiple\nof the dimension, and (2) we leverage this to prove that an efficient\nfirst-order algorithm achieves near-optimal statistical precision even without\ngood initialization. We also propose a general methodology for multi-class\nclustering tasks with flexible choices of feature transforms and loss\nobjectives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 17:57:07 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:45:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kaizheng", ""], ["Yan", "Yuling", ""], ["Diaz", "Mateo", ""]]}, {"id": "2003.10021", "submitter": "Gregorio Landi", "authors": "Gregorio Landi, Giovanni E. Landi", "title": "Proofs of non-optimality of the standard least-squares method for track\n  reconstructions", "comments": "It completes the results of INSTRUMENTS 2020 4,2-- 13 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST hep-ex physics.ins-det stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a standard criterium in statistics to define an optimal estimator the\none with the minimum variance. Thus, the optimality is proved with inequality\namong variances of competing estimators. The inequalities, demonstrated here,\ndisfavor the standard least squares estimators. Inequalities among estimators\nare connected to names of Cramer, Rao and Frechet. The standard demonstrations\nof these inequalities require very special analytical properties of the\nprobability functions, globally indicated as regular models. These limiting\nconditions are too restrictive to handle realistic problems in track fitting. A\nprevious extension to heteroscedastic models of the Cramer-Rao-Frechet\ninequalities was performed with Gaussian distributions. These demonstrations\nproved beyond any possible doubts the superiority of the heteroscedastic models\ncompared to the standard least squares method. However, the Gaussian\ndistributions are typical members of the required regular models. Instead, the\nrealistic probability distributions, encountered in tracker detectors, are very\ndifferent from Gaussian distributions. Therefore, to have well grounded set of\ninequalities, the limitations to regular models must be overtaken. The aim of\nthis paper is to demonstrate the inequalities for least squares estimators for\nirregular models of probabilities, explicitly excluded by the\nCramer-Rao-Frechet demonstrations. Estimators for straight and parabolic tracks\nwill be considered. The final part deals with the form of the distributions of\nsimplified heteroscedastic track models reconstructed with optimal estimators\nand the standard (non-optimal) estimators. A comparison among the distributions\nof these different estimators shows the large loss in resolution of the\nstandard least-squares estimators.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 23:03:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Landi", "Gregorio", ""], ["Landi", "Giovanni E.", ""]]}, {"id": "2003.10115", "submitter": "Matthias  L\\\"owe", "authors": "Matthias L\\\"owe and Sara Terveer", "title": "A Central Limit Theorem for incomplete U-statistics over triangular\n  arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the fluctuations of incomplete $U$-statistics over a triangular\narray of independent random variables. We give criteria for a Central Limit\nTheorem (CLT, for short) to hold in the sense that we prove that an\nappropriately scaled and centered version of the U-statistic converges to a\nnormal random variable. Our method of proof relies on a martingale CLT. A\npossible application -- a CLT for the hitting time for random walk on random\ngraphs -- will be presented in \\cite{LoTe20b}\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 07:51:01 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["L\u00f6we", "Matthias", ""], ["Terveer", "Sara", ""]]}, {"id": "2003.10126", "submitter": "Thomas Lartigue", "authors": "Thomas Lartigue (ARAMIS, CMAP), Stanley Durrleman (ARAMIS),\n  St\\'ephanie Allassonni\\`ere (CRC)", "title": "Deterministic Approximate EM Algorithm; Application to the Riemann\n  Approximation EM and the Tempered EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation Maximisation (EM) algorithm is widely used to optimise\nnon-convex likelihood functions with hidden variables. Many authors modified\nits simple design to fit more specific situations. For instance the Expectation\n(E) step has been replaced by Monte Carlo (MC) approximations, Markov Chain\nMonte Carlo approximations, tempered approximations... Most of the well-studied\napproximations belong to the stochastic class. By comparison, the literature is\nlacking when it comes to deterministic approximations. In this paper, we\nintroduce a theoretical framework, with state-of-the-art convergence\nguarantees, for any deterministic approximation of the E step. We analyse\ntheoretically and empirically several approximations that fit into this\nframework. First, for cases with intractable E steps, we introduce a\ndeterministic alternative to the MC-EM, using Riemann sums. This method is easy\nto implement and does not require the tuning of hyper-parameters. Then, we\nconsider the tempered approximation, borrowed from the Simulated Annealing\noptimisation technique and meant to improve the EM solution. We prove that the\ntempered EM verifies the convergence guarantees for a wide range of temperature\nprofiles. We showcase empirically how it is able to escape adversarial\ninitialisations. Finally, we combine the Riemann and tempered approximations to\naccomplish both their purposes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:23:54 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 14:40:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Lartigue", "Thomas", "", "ARAMIS, CMAP"], ["Durrleman", "Stanley", "", "ARAMIS"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC"]]}, {"id": "2003.10164", "submitter": "Didier Girard", "authors": "Karim Benhenni (IPS), Didier Girard (IPS), Sana Louhichi (IPS)", "title": "On bandwidth selection problems in nonparametric trend estimation under\n  martingale difference errors", "comments": "Bernoulli journal, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in the problem of smoothing parameter\nselection in nonparametric curve estimation under dependent errors. We focus on\nkernel estimation and the case when the errors form a general stationary\nsequence of martingale difference random variables where neither linearity\nassumption nor \"all moments are finite\" are required.We compare the behaviors\nof the smoothing bandwidths obtained by minimizing either the unknown average\nsquared error, the theoretical mean average squared error, a Mallows-type\ncriterion adapted to the dependent case and the family of criteria known as\ngeneralized cross validation (GCV) extensions of the Mallows' criterion. We\nprove that these three minimizers and those based on the GCV family are\nfirst-order equivalent in probability. We give also a normal asymptotic\nbehavior of the gap between the minimizer of the average square error and that\nof the Mallows-type criterion. This is extended to the GCV family.Finally, we\napply our theoretical results to a specific case of martingale difference\nsequence, namely the Auto-Regressive Conditional Heteroscedastic (ARCH(1))\nprocess.A Monte-carlo simulation study, for this regression model with ARCH(1)\nprocess, is conducted.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 10:32:03 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:34:12 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 08:49:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Benhenni", "Karim", "", "IPS"], ["Girard", "Didier", "", "IPS"], ["Louhichi", "Sana", "", "IPS"]]}, {"id": "2003.10323", "submitter": "Adrien Mazoyer", "authors": "Jean-Fran\\c{c}ois Coeurjolly, Adrien Mazoyer and Pierre-Olivier\n  Amblard", "title": "Monte Carlo integration of non-differentiable functions on\n  $[0,1]^\\iota$, $\\iota=1,\\dots,d$, using a single determinantal point pattern\n  defined on $[0,1]^d$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.CA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the use of a particular class of determinantal point\nprocesses (DPP), a class of repulsive spatial point processes, for Monte Carlo\nintegration. Let $d\\ge 1$, $I\\subseteq \\overline d=\\{1,\\dots,d\\}$ with\n$\\iota=|I|$. Using a single set of $N$ quadrature points $\\{u_1,\\dots,u_N\\}$\ndefined, once for all, in dimension $d$ from the realization of the DPP model,\nwe investigate \"minimal\" assumptions on the integrand in order to obtain\nunbiased Monte Carlo estimates of $\\mu(f_I)=\\int_{[0,1]^\\iota} f_I(u)\n\\mathrm{d} u$ for any known $\\iota$-dimensional integrable function on\n$[0,1]^\\iota$. In particular, we show that the resulting estimator has variance\nwith order $N^{-1-(2s\\wedge 1)/d}$ when the integrand belongs to some Sobolev\nspace with regularity $s > 0$. When $s>1/2$ (which includes a large class of\nnon-differentiable functions), the variance is asymptotically explicit and the\nestimator is shown to satisfy a Central Limit Theorem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 15:07:55 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Mazoyer", "Adrien", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "2003.10330", "submitter": "Joni Virta", "authors": "Joni Virta, Niko Lietz\\'en, Lauri Viitasaari, Pauliina Ilmonen", "title": "Latent Model Extreme Value Index Estimation", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel strategy for multivariate extreme value index estimation.\nIn applications such as finance, volatility and risk present in the components\nof a multivariate time series are often driven by the same underlying factors,\nsuch as the subprime crisis in the US. To estimate the latent risk, we apply a\ntwo-stage procedure. First, a set of independent latent series is estimated\nusing a method of latent variable analysis. Then, univariate risk measures are\nestimated individually for the latent series to assess their contribution to\nthe overall risk. As our main theoretical contribution, we derive conditions\nunder which the effect of the first step to the asymptotic behavior of the risk\nestimators is negligible. Simulations demonstrate the theory under both i.i.d.\nand dependent data, and an application into financial data illustrates the\nusefulness of the method in extracting joint sources of risk in practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:28:00 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Virta", "Joni", ""], ["Lietz\u00e9n", "Niko", ""], ["Viitasaari", "Lauri", ""], ["Ilmonen", "Pauliina", ""]]}, {"id": "2003.10360", "submitter": "Jos\\'e Antonio Perusqu\\'ia Cort\\'es", "authors": "Jos\\'e A. Perusqu\\'ia, Jim E. Griffin and Cristiano Villa", "title": "Bayesian Models Applied to Cyber Security Anomaly Detection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber security is an important concern for all individuals, organisations and\ngovernments globally. Cyber attacks have become more sophisticated, frequent\nand dangerous than ever, and traditional anomaly detection methods have been\nproved to be less effective when dealing with these new classes of cyber\nthreats. In order to address this, both classical and Bayesian models offer a\nvalid and innovative alternative to the traditional signature-based methods,\nmotivating the increasing interest in statistical research that it has been\nobserved in recent years. In this review we provide a description of some\ntypical cyber security challenges, typical types of data and statistical\nmethods, paying special attention to Bayesian approaches for these problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:26:13 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:59:47 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 12:09:31 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 23:04:16 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Perusqu\u00eda", "Jos\u00e9 A.", ""], ["Griffin", "Jim E.", ""], ["Villa", "Cristiano", ""]]}, {"id": "2003.10409", "submitter": "Aukosh Jagannath", "authors": "Gerard Ben Arous, Reza Gheissari, Aukosh Jagannath", "title": "Online stochastic gradient descent on non-convex losses from\n  high-dimensional inference", "comments": "final version to appear at Jour. Mach. Learn. Res$.$", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a popular algorithm for optimization\nproblems arising in high-dimensional inference tasks. Here one produces an\nestimator of an unknown parameter from independent samples of data by\niteratively optimizing a loss function. This loss function is random and often\nnon-convex. We study the performance of the simplest version of SGD, namely\nonline SGD, from a random start in the setting where the parameter space is\nhigh-dimensional.\n  We develop nearly sharp thresholds for the number of samples needed for\nconsistent estimation as one varies the dimension. Our thresholds depend only\non an intrinsic property of the population loss which we call the information\nexponent. In particular, our results do not assume uniform control on the loss\nitself, such as convexity or uniform derivative bounds. The thresholds we\nobtain are polynomial in the dimension and the precise exponent depends\nexplicitly on the information exponent. As a consequence of our results, we\nfind that except for the simplest tasks, almost all of the data is used simply\nin the initial search phase to obtain non-trivial correlation with the ground\ntruth. Upon attaining non-trivial correlation, the descent is rapid and\nexhibits law of large numbers type behavior.\n  We illustrate our approach by applying it to a wide set of inference tasks\nsuch as phase retrieval, and parameter estimation for generalized linear\nmodels, online PCA, and spiked tensor models, as well as to supervised learning\nfor single-layer networks with general activation functions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:34:06 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 04:22:38 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 19:00:50 GMT"}, {"version": "v4", "created": "Mon, 10 May 2021 17:56:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Arous", "Gerard Ben", ""], ["Gheissari", "Reza", ""], ["Jagannath", "Aukosh", ""]]}, {"id": "2003.10431", "submitter": "Wei-Kuo Chen", "authors": "Wei-Kuo Chen, Wai-Kit Lam", "title": "Universality of Approximate Message Passing Algorithms", "comments": "43 pages, minor revision in the introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math-ph math.IT math.MP math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a broad class of Approximate Message Passing (AMP) algorithms\ndefined as a Lipschitzian functional iteration in terms of an $n\\times n$\nrandom symmetric matrix $A$. We establish universality in noise for this AMP in\nthe $n$-limit and validate this behavior in a number of AMPs popularly adapted\nin compressed sensing, statistical inferences, and optimizations in spin\nglasses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:58:46 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 15:46:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chen", "Wei-Kuo", ""], ["Lam", "Wai-Kit", ""]]}, {"id": "2003.10443", "submitter": "Subha Maity", "authors": "Subha Maity, Yuekai Sun, and Moulinath Banerjee", "title": "Minimax optimal approaches to the label shift problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax rates of convergence in the label shift problem. In addition\nto the usual setting in which the learner only has access to unlabeled examples\nfrom the target domain, we also consider the setting in which a small number of\nlabeled examples from the target domain are available to the learner. Our study\nreveals a difference in the difficulty of the label shift problem in the two\nsettings. We attribute this difference to the availability of data from the\ntarget domain to estimate the class conditional distributions in the latter\nsetting. We also show that a distributional matching approach is minimax\nrate-optimal in the former setting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:28:26 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 23:46:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Maity", "Subha", ""], ["Sun", "Yuekai", ""], ["Banerjee", "Moulinath", ""]]}, {"id": "2003.10517", "submitter": "Martin Bladt", "authors": "Hansjoerg Albrecher, Martin Bladt, Mogens Bladt", "title": "Multivariate Matrix Mittag--Leffler distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the construction principle of multivariate phase-type distributions\nto establish an analytically tractable class of heavy-tailed multivariate\nrandom variables whose marginal distributions are of Mittag-Leffler type with\narbitrary index of regular variation. The construction can essentially be seen\nas allowing a scalar parameter to become matrix-valued. The class of\ndistributions is shown to be dense among all multivariate positive random\nvariables and hence provides a versatile candidate for the modelling of\nheavy-tailed, but tail-independent, risks in various fields of application.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:03:23 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Bladt", "Martin", ""], ["Bladt", "Mogens", ""]]}, {"id": "2003.10523", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "Matt Emschwiller, David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, Ilias\n  Zadik", "title": "Neural Networks and Polynomial Regression. Demystifying the\n  Overparametrization Phenomena", "comments": "59 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of neural network models, overparametrization refers to the\nphenomena whereby these models appear to generalize well on the unseen data,\neven though the number of parameters significantly exceeds the sample sizes,\nand the model perfectly fits the in-training data. A conventional explanation\nof this phenomena is based on self-regularization properties of algorithms used\nto train the data. In this paper we prove a series of results which provide a\nsomewhat diverging explanation. Adopting a teacher/student model where the\nteacher network is used to generate the predictions and student network is\ntrained on the observed labeled data, and then tested on out-of-sample data, we\nshow that any student network interpolating the data generated by a teacher\nnetwork generalizes well, provided that the sample size is at least an explicit\nquantity controlled by data dimension and approximation guarantee alone,\nregardless of the number of internal nodes of either teacher or student\nnetwork.\n  Our claim is based on approximating both teacher and student networks by\npolynomial (tensor) regression models with degree depending on the desired\naccuracy and network depth only. Such a parametrization notably does not depend\non the number of internal nodes. Thus a message implied by our results is that\nparametrizing wide neural networks by the number of hidden nodes is misleading,\nand a more fitting measure of parametrization complexity is the number of\nregression coefficients associated with tensorized data. In particular, this\nsomewhat reconciles the generalization ability of neural networks with more\nclassical statistical notions of data complexity and generalization bounds. Our\nempirical results on MNIST and Fashion-MNIST datasets indeed confirm that\ntensorized regression achieves a good out-of-sample performance, even when the\ndegree of the tensor is at most two.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:09:31 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Emschwiller", "Matt", ""], ["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "2003.10703", "submitter": "Mathias Vetter", "authors": "Mathias Vetter", "title": "A universal approach to estimate the conditional variance in\n  semimartingale limit theorems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical central limit theorems in high-frequency asymptotics for\nsemimartingales are results on stable convergence to a mixed normal limit with\nan unknown conditional variance. Estimating this conditional variance usually\nis a hard task, in particular when the underlying process contains jumps. For\nthis reason, several authors have recently discussed methods to automatically\nestimate the conditional variance, i.e. they build a consistent estimator from\nthe original statistics, but computed at various different time scales. Their\nmethods work in several situations, but are essentially restricted to the case\nof continuous paths always. The aim of this work is to present a new method to\nconsistently estimate the conditional variance which works regardless of\nwhether the underlying process is continuous or has jumps. We will discuss the\ncase of power variations in detail and give insight to the heuristics behind\nthe approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:53:09 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Vetter", "Mathias", ""]]}, {"id": "2003.10844", "submitter": "Lixing Zhu", "authors": "Ran Liu, Yun Fang and Lixing Zhu", "title": "Model Checking for Parametric Ordinary Differential Equations System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations have been used to model dynamical systems in\na broad range. Model checking for parametric ordinary differential equations is\na necessary step to check whether the assumed models are plausible. In this\npaper we introduce three test statistics for their different purposes. We first\ngive a trajectory matching-based test for the whole system. To further identify\nwhich component function(s) would be wrongly modelled, we introduce two test\nstatistics that are based on integral matching and gradient matching\nrespectively. We investigate the asymptotic properties of the three test\nstatistics under the null, global and local alternative hypothesis. To achieve\nthese purposes, we also investigate the asymptotic properties of nonlinear\nleast squares estimation and two-step collocation estimation under both the\nnull and alternatives. The results about the estimations are also new in the\nliterature. To examine the performances of the tests, we conduct several\nnumerical simulations. A real data example about immune cell kinetics and\ntrafficking for influenza infection is analyzed for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:43:21 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 02:04:50 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Ran", ""], ["Fang", "Yun", ""], ["Zhu", "Lixing", ""]]}, {"id": "2003.10878", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini", "title": "The Gauss' Bayes Factor", "comments": "13 pages, 2 figures (Version 2 benefits of editorial improvements)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO math.ST physics.data-an physics.hist-ph stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 'Theoria motus corporum coelestium in sectionibus conicis solem ambientum'\nGauss presents, as a theorem and with emphasis, the rule to update the ratio of\nprobabilities of complementary hypotheses, in the light of an observed event\nwhich could be due to either of them. Although he focused on a priori equally\nprobable hypotheses, in order to solve the problem on which he was interested\nin, the theorem can be easily extended to the general case. But, curiously, I\nhave not been able to find references to his result in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 10:22:39 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 13:19:17 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["D'Agostini", "Giulio", ""]]}, {"id": "2003.11086", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Jerry Li and Anastasia Voloshinov", "title": "Efficient Algorithms for Multidimensional Segmented Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problem of fixed design {\\em multidimensional\nsegmented regression}: Given noisy samples from a function $f$, promised to be\npiecewise linear on an unknown set of $k$ rectangles, we want to recover $f$ up\nto a desired accuracy in mean-squared error. We provide the first sample and\ncomputationally efficient algorithm for this problem in any fixed dimension.\nOur algorithm relies on a simple iterative merging approach, which is novel in\nthe multidimensional setting. Our experimental evaluation on both synthetic and\nreal datasets shows that our algorithm is competitive and in some cases\noutperforms state-of-the-art heuristics. Code of our implementation is\navailable at\n\\url{https://github.com/avoloshinov/multidimensional-segmented-regression}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:39:34 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Voloshinov", "Anastasia", ""]]}, {"id": "2003.11122", "submitter": "Martin Bladt", "authors": "Hansjoerg Albrecher, Martin Bladt, and Mogens Bladt", "title": "Multivariate fractional phase--type distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Kulkarni class of multivariate phase--type distributions in a\nnatural time--fractional way to construct a new class of multivariate\ndistributions with heavy-tailed Mittag-Leffler(ML)-distributed marginals. The\napproach relies on assigning rewards to a non--Mar\\-ko\\-vi\\-an jump process\nwith ML sojourn times. This new class complements an earlier multivariate ML\nconstruction \\cite{multiml} and in contrast to the former also allows for tail\ndependence. We derive properties and characterizations of this class, and work\nout some special cases that lead to explicit density representations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 21:30:07 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Bladt", "Martin", ""], ["Bladt", "Mogens", ""]]}, {"id": "2003.11196", "submitter": "Xin Tong Thomson", "authors": "Xi Chen and Qiang Liu and Xin T. Tong", "title": "Dimension Independent Generalization Error by Stochastic Gradient\n  Descent", "comments": "60 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One classical canon of statistics is that large models are prone to\noverfitting, and model selection procedures are necessary for high dimensional\ndata. However, many overparameterized models, such as neural networks, perform\nvery well in practice, although they are often trained with simple online\nmethods and regularization. The empirical success of overparameterized models,\nwhich is often known as benign overfitting, motivates us to have a new look at\nthe statistical generalization theory for online optimization. In particular,\nwe present a general theory on the generalization error of stochastic gradient\ndescent (SGD) solutions for both convex and locally convex loss functions. We\nfurther discuss data and model conditions that lead to a ``low effective\ndimension\". Under these conditions, we show that the generalization error\neither does not depend on the ambient dimension $p$ or depends on $p$ via a\npoly-logarithmic factor. We also demonstrate that in several widely used\nstatistical models, the ``low effective dimension'' arises naturally in\noverparameterized settings. The studied statistical applications include both\nconvex models such as linear regression and logistic regression and non-convex\nmodels such as $M$-estimator and two-layer neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 03:08:41 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 06:13:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Qiang", ""], ["Tong", "Xin T.", ""]]}, {"id": "2003.11238", "submitter": "Krishnakumar Balasubramanian", "authors": "Jiaxiang Li, Krishnakumar Balasubramanian, Shiqian Ma", "title": "Stochastic Zeroth-order Riemannian Derivative Estimation and\n  Optimization", "comments": "Additional experimental results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic zeroth-order optimization over Riemannian submanifolds\nembedded in Euclidean space, where the task is to solve Riemannian optimization\nproblem with only noisy objective function evaluations. Towards this, our main\ncontribution is to propose estimators of the Riemannian gradient and Hessian\nfrom noisy objective function evaluations, based on a Riemannian version of the\nGaussian smoothing technique. The proposed estimators overcome the difficulty\nof the non-linearity of the manifold constraint and the issues that arise in\nusing Euclidean Gaussian smoothing techniques when the function is defined only\nover the manifold. We use the proposed estimators to solve Riemannian\noptimization problems in the following settings for the objective function: (i)\nstochastic and gradient-Lipschitz (in both nonconvex and geodesic convex\nsettings), (ii) sum of gradient-Lipschitz and non-smooth functions, and (iii)\nHessian-Lipschitz. For these settings, we analyze the oracle complexity of our\nalgorithms to obtain appropriately defined notions of $\\epsilon$-stationary\npoint or $\\epsilon$-approximate local minimizer. Notably, our complexities are\nindependent of the dimension of the ambient Euclidean space and depend only on\nthe intrinsic dimension of the manifold under consideration. We demonstrate the\napplicability of our algorithms by simulation results and real-world\napplications on black-box stiffness control for robotics and black-box attacks\nto neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 06:58:19 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 06:07:06 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 04:05:52 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Li", "Jiaxiang", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ma", "Shiqian", ""]]}, {"id": "2003.11373", "submitter": "Qiuping Wang", "authors": "Qiuping Wang and Xiao Zhang and Jing Luo and Yang Ouyang and Qian Wang", "title": "Weighted directed networks with a differentially private bi-degree\n  sequence", "comments": "19 pages. arXiv admin note: text overlap with arXiv:1705.01715 and\n  arXiv:1408.1156 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The $p_0$ model is an exponential random graph model for directed networks\nwith the bi-degree sequence as the exclusively sufficient statistic. It\ncaptures the network feature of degree heterogeneity. The consistency and\nasymptotic normality of a differentially private estimator of the parameter in\nthe private $p_0$ model has been established. However, the $p_0$ model only\nfocuses on binary edges. In many realistic networks, edges could be weighted,\ntaking a set of finite discrete values. In this paper, we further show that the\nmoment estimators of the parameters based on the differentially private\nbi-degree sequence in the weighted $p_0$ model are consistent and\nasymptotically normal. Numerical studies demonstrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:59:50 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wang", "Qiuping", ""], ["Zhang", "Xiao", ""], ["Luo", "Jing", ""], ["Ouyang", "Yang", ""], ["Wang", "Qian", ""]]}, {"id": "2003.11423", "submitter": "Li-Chun Zhang", "authors": "Luis Sanguiao Sande and Li-Chun Zhang", "title": "Design-unbiased statistical learning in survey sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design-consistent model-assisted estimation has become the standard practice\nin survey sampling. However, a general theory is lacking so far, which allows\none to incorporate modern machine-learning techniques that can lead to\npotentially much more powerful assisting models. We propose a subsampling\nRao-Blackwell method, and develop a statistical learning theory for exactly\ndesign-unbiased estimation with the help of linear or non-linear prediction\nmodels. Our approach makes use of classic ideas from Statistical Science as\nwell as the rapidly growing field of Machine Learning. Provided rich auxiliary\ninformation, it can yield considerable efficiency gains over standard linear\nmodel-assisted methods, while ensuring valid estimation for the given target\npopulation, which is robust against potential mis-specifications of the\nassisting model at the individual level.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 14:27:39 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Sande", "Luis Sanguiao", ""], ["Zhang", "Li-Chun", ""]]}, {"id": "2003.11462", "submitter": "Xinghao Qiao", "authors": "Shaojun Guo and Xinghao Qiao", "title": "On Consistency and Sparsity for High-Dimensional Functional Time Series\n  with Application to Autoregressions", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.07619", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling a large collection of functional time series arises in a broad\nspectral of real applications. Under such a scenario, not only the number of\nfunctional variables can be diverging with, or even larger than the number of\ntemporally dependent functional observations, but each function itself is an\ninfinite-dimensional object, posing a challenging task. In this paper, a\nstandard three-step procedure is proposed to address such large-scale problems.\nTo provide theoretical guarantees for the three-step procedure, we focus on\nmultivariate stationary processes and propose a novel {\\it functional stability\nmeasure} based on their spectral properties. Such stability measure facilitates\nthe development of some useful concentration bounds on sample (auto)covariance\nfunctions, which serve as a fundamental tool for further consistency analysis,\ne.g., for deriving rates of convergence on the regularized estimates in\nhigh-dimensional settings. As {\\it functional principal component analysis}\n(FPCA) is one of the key dimension reduction techniques in the first step, we\nalso investigate the consistency properties of the relevant estimated terms\nunder a FPCA framework. To illustrate with an important application, we\nconsider vector functional autoregressive models and develop a regularization\napproach to estimate autoregressive coefficient functions under the sparsity\nconstraint. Using our derived convergence results, we investigate the\ntheoretical properties of the regularized estimate under high-dimensional\nscaling. Finally, the finite-sample performance of the proposed method is\nexamined through both simulations and a public financial dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 23:18:43 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Guo", "Shaojun", ""], ["Qiao", "Xinghao", ""]]}, {"id": "2003.11497", "submitter": "Karthik Bharath", "authors": "Huiling Le, Alexander Lewis, Karthik Bharath and Christopher Fallaize", "title": "A diffusion approach to Stein's method on Riemannian manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We detail an approach to develop Stein's method for bounding integral metrics\non probability measures defined on a Riemannian manifold $\\mathbf{M}$. Our\napproach exploits the relationship between the generator of a diffusion on\n$\\mathbf{M}$ with target invariant measure and its characterising Stein\noperator. We consider a pair of such diffusions with different starting points,\nand investigate properties of solution to the Stein equation based on analysis\nof the distance process between the pair. Several examples elucidating the role\nof geometry of $\\mathbf{M}$ in these developments are presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:03:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Le", "Huiling", ""], ["Lewis", "Alexander", ""], ["Bharath", "Karthik", ""], ["Fallaize", "Christopher", ""]]}, {"id": "2003.11769", "submitter": "Ilsang Ohn", "authors": "Ilsang Ohn, Yongdai Kim", "title": "Nonconvex sparse regularization for deep neural networks and its\n  optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical studies proved that deep neural network (DNN) estimators\nobtained by minimizing empirical risk with a certain sparsity constraint can\nattain optimal convergence rates for regression and classification problems.\nHowever, the sparsity constraint requires to know certain properties of the\ntrue model, which are not available in practice. Moreover, computation is\ndifficult due to the discrete nature of the sparsity constraint. In this paper,\nwe propose a novel penalized estimation method for sparse DNNs, which resolves\nthe aforementioned problems existing in the sparsity constraint. We establish\nan oracle inequality for the excess risk of the proposed sparse-penalized DNN\nestimator and derive convergence rates for several learning tasks. In\nparticular, we prove that the sparse-penalized estimator can adaptively attain\nminimax convergence rates for various nonparametric regression problems. For\ncomputation, we develop an efficient gradient-based optimization algorithm that\nguarantees the monotonic reduction of the objective function.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:15:28 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ohn", "Ilsang", ""], ["Kim", "Yongdai", ""]]}, {"id": "2003.11771", "submitter": "Tadeusz Inglot", "authors": "Tadeusz Inglot", "title": "Moderate deviation theorem for the Neyman-Pearson statistic in testing\n  uniformity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for local alternatives to uniformity which are determined by a\nsequence of square integrable densities the moderate deviation (MD) theorem for\nthe corresponding Neyman-Pearson statistic does not hold in the full range for\nall unbounded densities. We give a sufficient condition under which MD theorem\nholds. The proof is based on Mogulskii's inequality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:23:11 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Inglot", "Tadeusz", ""]]}, {"id": "2003.11830", "submitter": "Robert Sicks", "authors": "Robert Sicks, Ralf Korn, Stefanie Schwaar", "title": "A lower bound for the ELBO of the Bernoulli Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variational autoencoder (VAE) for binary data. Our main\ninnovations are an interpretable lower bound for its training objective, a\nmodified initialization and architecture of such a VAE that leads to faster\ntraining, and a decision support for finding the appropriate dimension of the\nlatent space via using a PCA. Numerical examples illustrate our theoretical\nresult and the performance of the new architecture.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 10:59:53 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Sicks", "Robert", ""], ["Korn", "Ralf", ""], ["Schwaar", "Stefanie", ""]]}, {"id": "2003.11869", "submitter": "Fr\u00e9d\u00e9ric Pro\u00efa", "authors": "Eunice Okome Obiang, Pascal J\\'ez\\'equel, Fr\\'ed\\'eric Pro\\\"ia", "title": "A partial graphical model with a structural prior on the direct links\n  between predictors and responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the estimation of a partial graphical model with a\nstructural Bayesian penalization. Precisely, we are interested in the linear\nregression setting where the estimation is made through the direct links\nbetween potentially high-dimensional predictors and multiple responses, since\nit is known that Gaussian graphical models enable to exhibit direct links only,\nwhereas coefficients in linear regressions contain both direct and indirect\nrelations (due \\textit{e.g.} to strong correlations among the variables). A\nsmooth penalty reflecting a generalized Gaussian Bayesian prior on the\ncovariates is added, either enforcing patterns (like row structures) in the\ndirect links or regulating the joint influence of predictors. We give a\ntheoretical guarantee for our method, taking the form of an upper bound on the\nestimation error arising with high probability, provided that the model is\nsuitably regularized. Empirical studies on synthetic data and a real dataset\nare conducted.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:43:13 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 11:08:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Obiang", "Eunice Okome", ""], ["J\u00e9z\u00e9quel", "Pascal", ""], ["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2003.12126", "submitter": "Holger Dette", "authors": "Holger Dette, Gauthier Dierickx, Tim Kutta", "title": "Quantifying deviations from separability in space-time functional\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of covariance operators of spatio-temporal data is in many\napplications only computationally feasible under simplifying assumptions, such\nas separability of the covariance into strictly temporal and spatial\nfactors.Powerful tests for this assumption have been proposed in the\nliterature. However, as real world systems, such as climate data are\nnotoriously inseparable, validating this assumption by statistical tests, seems\ninherently questionable. In this paper we present an alternative approach: By\nvirtue of separability measures, we quantify how strongly the data's covariance\noperator diverges from a separable approximation. Confidence intervals localize\nthese measures with statistical guarantees. This method provides users with a\nflexible tool, to weigh the computational gains of a separable model against\nthe associated increase in bias. As separable approximations we consider the\nestablished methods of partial traces and partial products, and develop weak\nconvergence principles for the corresponding estimators. Moreover, we also\nprove such results for estimators of optimal, separable approximations, which\nare arguably of most interest in applications. In particular we present for the\nfirst time statistical inference for this object, which has been confined to\nestimation previously. Besides confidence intervals, our results encompass\ntests for approximate separability. All methods proposed in this paper are free\nof nuisance parameters and do neither require computationally expensive\nresampling procedures nor the estimation of nuisance parameters. A simulation\nstudy underlines the advantages of our approach and its applicability is\ndemonstrated by the investigation of German annual temperature data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:49:58 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dette", "Holger", ""], ["Dierickx", "Gauthier", ""], ["Kutta", "Tim", ""]]}, {"id": "2003.12200", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon, Bruno Scarpa and Sonia Petrone", "title": "Enriched Pitman-Yor processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian nonparametrics there exists a rich variety of discrete priors,\nincluding the Dirichlet process and its generalizations, which are nowadays\nwell-established tools. Despite the remarkable advances, few proposals are\ntailored for modeling observations lying on product spaces, such as\n$\\mathbb{R}^p$. In this setting, most of the available priors lack of\nflexibility and they do not allow for separate partition structures among the\nspaces. We address these issues by introducing a discrete nonparametric prior\ntermed enriched Pitman-Yor process (EPY). Theoretical properties of this novel\nprior are extensively investigated. Specifically, we discuss its formal link\nwith the enriched Dirichlet process and normalized random measures, we describe\na square-breaking representation and we obtain closed form expressions for the\nposterior law and the involved urn schemes. In second place, we show that\nseveral existing approaches, including Dirichlet processes with a spike and\nslab base measure and mixture of mixtures models, implicitly rely on special\ncases of the EPY, which therefore constitutes a unified probabilistic framework\nfor many Bayesian nonparametric priors. Interestingly, our unifying formulation\nwill allow to naturally extend these models, while preserving their analytical\ntractability. As an illustration, we employ the EPY for a species sampling\nproblem in ecology.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 01:34:27 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Rigon", "Tommaso", ""], ["Scarpa", "Bruno", ""], ["Petrone", "Sonia", ""]]}, {"id": "2003.12321", "submitter": "Harry Haupt", "authors": "Harry Haupt", "title": "Seemingly unrelated and fixed-effect panel regressions: collinearity and\n  singular dispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper discusses identification conditions, representations and relations\nof generalized least squares estimators of regression parameters in\nmultivariate linear regression models such as seemingly unrelated and fixed\neffect panel models. Results are presented on identification for unrestricted\ndispersion structure and general heteroskedasticity and cross-equation\ndependence, considering explicit and implicit restrictions, singularity of the\ndispersion and multicollinearity in the design matrix.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 10:39:50 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 06:34:20 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Haupt", "Harry", ""]]}, {"id": "2003.12544", "submitter": "Yannick Baraud", "authors": "Yannick Baraud", "title": "Tests and estimation strategies associated to some loss functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the joint distribution of $n$\nindependent random variables. Our approach is based on a family of candidate\nprobabilities that we shall call a model and which is chosen to either contain\nthe true distribution of the data or at least to provide a good approximation\nof it with respect to some loss function. The aim of the present paper is to\ndescribe a general estimation strategy that allows to adapt to both the\nspecific features of the model and the choice of the loss function in view of\ndesigning an estimator with good estimation properties. The losses we have in\nmind are based on the total variation, Hellinger, Wasserstein and\n$\\mathbb{L}_p$-distances to name a few. We show that the risk of the resulting\nestimator with respect to the loss function can be bounded by the sum of an\napproximation term accounting for the loss between the true distribution and\nthe model and a complexity term that corresponds to the bound we would get if\nthis distribution did belong to the model. Our results hold under mild\nassumptions on the true distribution of the data and are based on exponential\ndeviation inequalities that are non-asymptotic and involve explicit constants.\nWhen the model reduces to two distinct probabilities, we show how our\nestimation strategy leads to a robust test whose errors of first and second\nkinds only depend on the losses between the true distribution and the two\ntested probabilities.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:26:01 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:08:40 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 09:32:51 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 07:37:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Baraud", "Yannick", ""]]}, {"id": "2003.12573", "submitter": "Kata Vuk", "authors": "Herold Dehling, Kata Vuk, Martin Wendler", "title": "Change-Point Detection based on Weighted Two-Sample U-Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the large-sample behavior of change-point tests based on\nweighted two-sample U-statistics, in the case of short-range dependent data.\nUnder some mild mixing conditions, we establish convergence of the test\nstatistic to an extreme value distribution. A simulation study shows that the\nweighted tests are superior to the non-weighted versions when the change-point\noccurs near the boundary of the time interval, while they loose power in the\ncenter.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:51:24 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 08:51:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Dehling", "Herold", ""], ["Vuk", "Kata", ""], ["Wendler", "Martin", ""]]}, {"id": "2003.12699", "submitter": "Yunzong Xu", "authors": "David Simchi-Levi, Yunzong Xu", "title": "Bypassing the Monster: A Faster and Simpler Optimal Algorithm for\n  Contextual Bandits under Realizability", "comments": "Forthcoming in Mathematics of Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general (stochastic) contextual bandit problem under the\nrealizability assumption, i.e., the expected reward, as a function of contexts\nand actions, belongs to a general function class $\\mathcal{F}$. We design a\nfast and simple algorithm that achieves the statistically optimal regret with\nonly ${O}(\\log T)$ calls to an offline regression oracle across all $T$ rounds.\nThe number of oracle calls can be further reduced to $O(\\log\\log T)$ if $T$ is\nknown in advance. Our results provide the first universal and optimal reduction\nfrom contextual bandits to offline regression, solving an important open\nproblem in the contextual bandit literature. A direct consequence of our\nresults is that any advances in offline regression immediately translate to\ncontextual bandits, statistically and computationally. This leads to faster\nalgorithms and improved regret guarantees for broader classes of contextual\nbandit problems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 04:16:52 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 01:46:50 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 05:59:42 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 16:47:44 GMT"}, {"version": "v5", "created": "Sat, 10 Jul 2021 04:40:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Simchi-Levi", "David", ""], ["Xu", "Yunzong", ""]]}, {"id": "2003.12838", "submitter": "Botond Szabo", "authors": "Botond Szabo and Harry van Zanten", "title": "Distributed function estimation: adaptation using minimal communication", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether in a distributed setting, adaptive estimation of a\nsmooth function at the optimal rate is possible under minimal communication. It\nturns out that the answer depends on the risk considered and on the number of\nservers over which the procedure is distributed. We show that for the\n$L_\\infty$-risk, adaptively obtaining optimal rates under minimal communication\nis not possible. For the $L_2$-risk, it is possible over a range of\nregularities that depends on the relation between the number of local servers\nand the total sample size.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 16:44:06 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Szabo", "Botond", ""], ["van Zanten", "Harry", ""]]}, {"id": "2003.12935", "submitter": "Liyan Xie", "authors": "Anatoli Juditsky, Arkadi Nemirovski, Liyan Xie, Yao Xie", "title": "Convex Parameter Recovery for Interacting Marked Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new general modeling approach for multivariate discrete event\ndata with categorical interacting marks, which we refer to as marked Bernoulli\nprocesses. In the proposed model, the probability of an event of a specific\ncategory to occur in a location may be influenced by past events at this and\nother locations. We do not restrict interactions to be positive or decaying\nover time as it is commonly adopted, allowing us to capture an arbitrary shape\nof influence from historical events, locations, and events of different\ncategories. In our modeling, prior knowledge is incorporated by allowing\ngeneral convex constraints on model parameters. We develop two parameter\nestimation procedures utilizing the constrained Least Squares (LS) and Maximum\nLikelihood (ML) estimation, which are solved using variational inequalities\nwith monotone operators. We discuss different applications of our approach and\nillustrate the performance of proposed recovery routines on synthetic examples\nand a real-world police dataset.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 03:23:30 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 02:17:32 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 17:53:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""], ["Xie", "Liyan", ""], ["Xie", "Yao", ""]]}, {"id": "2003.12968", "submitter": "D. H. S. Maithripala", "authors": "Pathmanathan Pankayaraj, D. H. S. Maithripala, and J. M. Berg", "title": "A Decentralized Policy with Logarithmic Regret for a Class of\n  Multi-Agent Multi-Armed Bandit Problems with Option Unavailability\n  Constraints and Stochastic Communication Protocols", "comments": "Pre-print submitted for review to the 2020 CDC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multi-armed bandit (MAB) problem in which multiple\nmobile agents receive rewards by sampling from a collection of spatially\ndispersed stochastic processes, called bandits. The goal is to formulate a\ndecentralized policy for each agent, in order to maximize the total cumulative\nreward over all agents, subject to option availability and inter-agent\ncommunication constraints. The problem formulation is motivated by applications\nin which a team of autonomous mobile robots cooperates to accomplish an\nexploration and exploitation task in an uncertain environment. Bandit locations\nare represented by vertices of the spatial graph. At any time, an agent's\noption consist of sampling the bandit at its current location, or traveling\nalong an edge of the spatial graph to a new bandit location. Communication\nconstraints are described by a directed, non-stationary, stochastic\ncommunication graph. At any time, agents may receive data only from their\ncommunication graph in-neighbors. For the case of a single agent on a fully\nconnected spatial graph, it is known that the expected regret for any optimal\npolicy is necessarily bounded below by a function that grows as the logarithm\nof time. A class of policies called upper confidence bound (UCB) algorithms\nasymptotically achieve logarithmic regret for the classical MAB problem. In\nthis paper, we propose a UCB-based decentralized motion and option selection\npolicy and a non-stationary stochastic communication protocol that guarantee\nlogarithmic regret. To our knowledge, this is the first such decentralized\npolicy for non-fully connected spatial graphs with communication constraints.\nWhen the spatial graph is fully connected and the communication graph is\nstationary, our decentralized algorithm matches or exceeds the best reported\nprior results from the literature.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 08:12:49 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 04:29:22 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Pankayaraj", "Pathmanathan", ""], ["Maithripala", "D. H. S.", ""], ["Berg", "J. M.", ""]]}, {"id": "2003.13072", "submitter": "Mikhail Boldin", "authors": "Michael Boldin", "title": "On the Power of Symmetrized Pearson's Type Test under Local Alternatives\n  in Autoregression with Outliers", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary linear AR($p$) model with observations subject to\ngross errors (outliers). The autoregression parameters are unknown as well as\nthe distribution function $G$ of innovations. The distribution of outliers\n$\\Pi$ is unknown and arbitrary, their intensity is $\\gamma n^{-1/2}$ with an\nunknown $\\gamma$, $n$ is the sample size. We test the hypothesis $H_0\\colon\nG=G_0$ with simmetric $G_0$. We find the power of the test under local\nalternatives $H_{1n}(\\rho)\\colon G=(1-\\rho n^{-1/2})G_0+\\rho n^{-1/2}H$. Our\ntest is the special symmetrized Pearson's type test. Namely, first of all we\nestimate the autoregression parameters and then using the residuals from the\nestimated autoregression we construct a kind of empirical distribution function\n(e.d.f.), which is a counterpart of the (inaccessible) e.d.f. of the\nautoregression innovations. We obtain a stochastic expansion of this e.d.f. and\nits symmetrized variant under $H_{1n}(\\rho)$ , which enables us to construct\nand investigate our symmetrized test of Pearson's type for $H_0$. We establish\nqualitative robustness of this test in terms of uniform equicontinuity of the\nlimiting power (as functions of $\\gamma,\\rho$ and $\\Pi$) with respect to\n$\\gamma$ in a neighborhood of $\\gamma=0$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 16:53:40 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Boldin", "Michael", ""]]}, {"id": "2003.13076", "submitter": "Adelchi Azzalini", "authors": "Reinaldo B. Arellano-Valle and Adelchi Azzalini", "title": "A formulation for continuous mixtures of multivariate normal\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several formulations have long existed in the literature in the form of\ncontinuous mixtures of normal variables where a mixing variable operates on the\nmean or on the variance or on both the mean and the variance of a multivariate\nnormal variable, by changing the nature of these basic constituents from\nconstants to random quantities. More recently, other mixture-type constructions\nhave been introduced, where the core random component, on which the mixing\noperation operates, is not necessarily normal. The main aim of the present work\nis to show that many existing constructions can be encompassed by a formulation\nwhere normal variables are mixed using two univariate random variables. For\nthis formulation, we derive various general properties. Within the proposed\nframework, it is also simpler to formulate new proposals of parametric families\nand we provide a few such instances. At the same time, the exposition provides\na review of the theme of normal mixtures.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 16:59:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Arellano-Valle", "Reinaldo B.", ""], ["Azzalini", "Adelchi", ""]]}, {"id": "2003.13126", "submitter": "Lasse Petersen", "authors": "Lasse Petersen and Niels Richard Hansen", "title": "Testing Conditional Independence via Quantile Regression Based Partial\n  Copulas", "comments": null, "journal-ref": "Journal of Machine Learning Research 22(70):1-47, 2021", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The partial copula provides a method for describing the dependence between\ntwo random variables $X$ and $Y$ conditional on a third random vector $Z$ in\nterms of nonparametric residuals $U_1$ and $U_2$. This paper develops a\nnonparametric test for conditional independence by combining the partial copula\nwith a quantile regression based method for estimating the nonparametric\nresiduals. We consider a test statistic based on generalized correlation\nbetween $U_1$ and $U_2$ and derive its large sample properties under\nconsistency assumptions on the quantile regression procedure. We demonstrate\nthrough a simulation study that the resulting test is sound under complicated\ndata generating distributions. Moreover, in the examples considered the test is\ncompetitive to other state-of-the-art conditional independence tests in terms\nof level and power, and it has superior power in cases with conditional\nvariance heterogeneity of $X$ and $Y$ given $Z$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:56:47 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 13:28:30 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 16:49:01 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 07:42:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Petersen", "Lasse", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "2003.13153", "submitter": "Ji Chen", "authors": "Ji Chen, Xiaodong Li, Zongming Ma", "title": "Nonconvex Matrix Completion with Linearly Parameterized Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques of matrix completion aim to impute a large portion of missing\nentries in a data matrix through a small portion of observed ones, with broad\nmachine learning applications including collaborative filtering, pairwise\nranking, etc. In practice, additional structures are usually employed in order\nto improve the accuracy of matrix completion. Examples include subspace\nconstraints formed by side information in collaborative filtering, and skew\nsymmetry in pairwise ranking. This paper performs a unified analysis of\nnonconvex matrix completion with linearly parameterized factorization, which\ncovers the aforementioned examples as special cases. Importantly, uniform upper\nbounds for estimation errors are established for all local minima, provided\nthat the sampling rate satisfies certain conditions determined by the rank,\ncondition number, and incoherence parameter of the ground-truth low rank\nmatrix. Empirical efficiency of the proposed method is further illustrated by\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 22:40:47 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Ji", ""], ["Li", "Xiaodong", ""], ["Ma", "Zongming", ""]]}, {"id": "2003.13162", "submitter": "Andrey A Popov", "authors": "Andrey A Popov, Adrian Sandu", "title": "An Explicit Probabilistic Derivation of Inflation in a Scalar Ensemble\n  Kalman Filter for Finite Step, Finite Ensemble Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": "CSL-TR-20-1", "categories": "math.OC cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a probabilistic approach to analyze the converge of an\nensemble Kalman filter solution to an exact Kalman filter solution in the\nsimplest possible setting, the scalar case, as it allows us to build upon a\nrich literature of scalar probability distributions and non-elementary\nfunctions. To this end we introduce the bare-bones Scalar Pedagogical Ensemble\nKalman Filter (SPEnKF). We show that in the asymptotic case of ensemble size,\nthe expected value of both the analysis mean and variance estimate of the\nSPEnKF converges to that of the true Kalman filter, and that the variances of\nboth tend towards zero, at each time moment. We also show that the ensemble\nconverges in probability in the complementary case, when the ensemble is\nfinite, and time is taken to infinity. Moreover, we show that in the\nfinite-ensemble, finite-time case, variance inflation and mean correction can\nbe leveraged to coerce the SPEnKF converge to its scalar Kalman filter\ncounterpart. We then apply this framework to analyze perturbed observations and\nexplain why perturbed observations ensemble Kalman filters underperform their\ndeterministic counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 23:53:49 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Popov", "Andrey A", ""], ["Sandu", "Adrian", ""]]}, {"id": "2003.13208", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Sivaraman Balakrishnan, Larry Wasserman", "title": "Minimax optimality of permutation tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation tests are widely used in statistics, providing a finite-sample\nguarantee on the type I error rate whenever the distribution of the samples\nunder the null hypothesis is invariant to some rearrangement. Despite its\nincreasing popularity and empirical success, theoretical properties of the\npermutation test, especially its power, have not been fully explored beyond\nsimple cases. In this paper, we attempt to fill this gap by presenting a\ngeneral non-asymptotic framework for analyzing the power of the permutation\ntest. The utility of our proposed framework is illustrated in the context of\ntwo-sample and independence testing under both discrete and continuous\nsettings. In each setting, we introduce permutation tests based on U-statistics\nand study their minimax performance. We also develop exponential concentration\nbounds for permuted U-statistics based on a novel coupling idea, which may be\nof independent interest. Building on these exponential bounds, we introduce\npermutation tests which are adaptive to unknown smoothness parameters without\nlosing much power. The proposed framework is further illustrated using more\nsophisticated test statistics including weighted U-statistics for multinomial\ntesting and Gaussian kernel-based statistics for density testing. Finally, we\nprovide some simulation results that further justify the permutation approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 04:05:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kim", "Ilmun", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2003.13277", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and {\\L}ukas Smaga", "title": "Permutation test for the multivariate coefficient of variation in\n  factorial designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New inference methods for the multivariate coefficient of variation and its\nreciprocal, the standardized mean, are presented. While there are various\ntesting procedures for both parameters in the univariate case, it is less known\nhow to do inference in the multivariate setting appropriately. There are some\nexisting procedures but they rely on restrictive assumptions on the underlying\ndistributions. We tackle this problem by applying Wald-type statistics in the\ncontext of general, potentially heteroscedastic factorial designs. In addition\nto the $k$-sample case, higher-way layouts can be incorporated into this\nframework allowing the discussion of main and interaction effects. The\nresulting procedures are shown to be asymptotically valid under the null\nhypothesis and consistent under general alternatives. To improve the finite\nsample performance, we suggest permutation versions of the tests and shown that\nthe tests' asymptotic properties can be transferred to them. An exhaustive\nsimulation study compares the new tests, their permutation counterparts and\nexisting methods. To further analyse the differences between the tests, we\nconduct two illustrative real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:56:47 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Smaga", "\u0141ukas", ""]]}, {"id": "2003.13351", "submitter": "Chunhao Cai", "authors": "Chunhao Cai, Yinzhong Huang and Weilin Xiao", "title": "Maximum likelihood estimation for mixed fractional Vasicek processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixed fractional Vasicek model, which is an extended model of the\ntraditional Vasicek model, has been widely used in modelling volatility,\ninterest rate and exchange rate. Obviously, if some phenomenon are modeled by\nthe mixed fractional Vasicek model, statistical inference for this process is\nof great interest. Based on continuous time observations, this paper considers\nthe problem of estimating the drift parameters in the mixed fractional Vasicek\nmodel. We will propose the maximum likelihood estimators of the drift\nparameters in the mixed fractional Vasicek model with the Radon-Nikodym\nderivative for a mixed fractional Brownian motion. Using the fundamental\nmartingale and the Laplace transform, both the strong consistency and the\nasymptotic normality of the maximum likelihood estimators have been established\nfor all $H\\in(0,1)$, $H\\neq 1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 11:33:40 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:35:37 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 02:43:34 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Cai", "Chunhao", ""], ["Huang", "Yinzhong", ""], ["Xiao", "Weilin", ""]]}, {"id": "2003.13435", "submitter": "Yue Ju", "authors": "Yue Ju, Tianshi Chen, Biqiang Mu and Lennart Ljung", "title": "Supplementary Material for CDC Submission No. 1461", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY eess.SY stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the influences of the condition number of the\nregression matrix upon the comparison between two hyper-parameter estimation\nmethods: the empirical Bayes (EB) and the Stein's unbiased estimator with\nrespect to the mean square error (MSE) related to output prediction (SUREy). We\nfirstly show that the greatest power of the condition number of the regression\nmatrix of SUREy cost function convergence rate upper bound is always one larger\nthan that of EB cost function convergence rate upper bound. Meanwhile, EB and\nSUREy hyper-parameter estimators are both proved to be asymptotically normally\ndistributed under suitable conditions. In addition, one ridge regression case\nis further investigated to show that when the condition number of the\nregression matrix goes to infinity, the asymptotic variance of SUREy estimator\ntends to be larger than that of EB estimator.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:01:58 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:59:15 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 07:24:02 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ju", "Yue", ""], ["Chen", "Tianshi", ""], ["Mu", "Biqiang", ""], ["Ljung", "Lennart", ""]]}, {"id": "2003.13478", "submitter": "Andrii Babii", "authors": "Andrii Babii", "title": "High-dimensional mixed-frequency IV regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a high-dimensional linear IV regression for the data\nsampled at mixed frequencies. We show that the high-dimensional slope parameter\nof a high-frequency covariate can be identified and accurately estimated\nleveraging on a low-frequency instrumental variable. The distinguishing feature\nof the model is that it allows handing high-dimensional datasets without\nimposing the approximate sparsity restrictions. We propose a\nTikhonov-regularized estimator and derive the convergence rate of its\nmean-integrated squared error for time series data. The estimator has a\nclosed-form expression that is easy to compute and demonstrates excellent\nperformance in our Monte Carlo experiments. We estimate the real-time price\nelasticity of supply on the Australian electricity spot market. Our estimates\nsuggest that the supply is relatively inelastic and that its elasticity is\nheterogeneous throughout the day.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:41:02 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Babii", "Andrii", ""]]}, {"id": "2003.13486", "submitter": "Alfredo Alegr\\'ia", "authors": "Alfredo Alegr\\'ia, Xavier Emery, Christian Lantu\\'ejoul", "title": "The Turning Arcs: a Computationally Efficient Algorithm to Simulate\n  Isotropic Vector-Valued Gaussian Random Fields on the $d$-Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random fields on the sphere play a fundamental role in the natural sciences.\nThis paper presents a simulation algorithm parenthetical to the spectral\nturning bands method used in Euclidean spaces, for simulating scalar- or\nvector-valued Gaussian random fields on the $d$-dimensional unit sphere. The\nsimulated random field is obtained by a sum of Gegenbauer waves, each of which\nis variable along a randomly oriented arc and constant along the parallels\northogonal to the arc. Convergence criteria based on the Berry-Ess\\'een\ninequality are proposed to choose suitable parameters for the implementation of\nthe algorithm, which is illustrated through numerical experiments. A by-product\nof this work is a closed-form expression of the Schoenberg coefficients\nassociated with the Chentsov and exponential covariance models on spheres of\ndimensions greater than or equal to 2.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:06:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Alegr\u00eda", "Alfredo", ""], ["Emery", "Xavier", ""], ["Lantu\u00e9joul", "Christian", ""]]}, {"id": "2003.13490", "submitter": "Johannes Krebs", "authors": "Johannes Krebs and Christian Hirsch", "title": "Functional central limit theorems for persistent Betti numbers on\n  cylindrical networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study functional central limit theorems for persistent Betti numbers\nobtained from networks defined on a Poisson point process. The limit is formed\nin large volumes of cylindrical shape stretching only in one dimension. The\nresults cover a directed sublevel-filtration for stabilizing networks and the\nCech and Vietoris-Rips complex on the random geometric graph.\n  The presented functional central limit theorems open the door to a variety of\nstatistical applications in topological data analysis and we consider\ngoodness-of-fit tests in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:14:23 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:39:33 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Krebs", "Johannes", ""], ["Hirsch", "Christian", ""]]}, {"id": "2003.13530", "submitter": "Nicolas Schreuder", "authors": "Nicolas Schreuder", "title": "Bounding the expectation of the supremum of empirical processes indexed\n  by H\\\"older classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide upper bounds on the expectation of the supremum of\nempirical processes indexed by H\\\"older classes of any smoothness and for any\ndistribution supported on a bounded set in $\\mathbb R^d$. These results can be\nalternatively seen as non-asymptotic risk bounds, when the unknown distribution\nis estimated by its empirical counterpart, based on $n$ independent\nobservations, and the error of estimation is quantified by the integral\nprobability metrics (IPM). In particular, the IPM indexed by a H\\\"older class\nis considered and the corresponding rates are derived. These results\ninterpolate between the two well-known extreme cases: the rate $n^{-1/d}$\ncorresponding to the Wassertein-1 distance (the least smooth case) and the fast\nrate $n^{-1/2}$ corresponding to very smooth functions (for instance, functions\nfrom an RKHS defined by a bounded kernel).\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:58:42 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 17:48:21 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 09:34:56 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Schreuder", "Nicolas", ""]]}, {"id": "2003.13531", "submitter": "Reinhard  H\\\"opfner", "authors": "Reinhard H\\\"opfner", "title": "Polynomials under Ornstein-Uhlenbeck noise and an application to\n  inference in stochastic Hodgkin-Huxley systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss estimation problems where a polynomial is observed under Ornstein\nUhlenbeck noise over a long time interval. We prove local asymptotic normality\n(LAN) and specify asymptotically efficient estimators. We apply this to the\nfollowing problem: feeding noise into the classical (deterministic) Hodgkin\nHuxley model of neuroscience, we are interested in asymptotically efficient\nestimation of the parameters of the noise process.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:00:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["H\u00f6pfner", "Reinhard", ""]]}, {"id": "2003.13662", "submitter": "Philipp Reichenbach", "authors": "Carlos Am\\'endola, Kathl\\'en Kohn, Philipp Reichenbach, Anna Seigal", "title": "Invariant theory and scaling algorithms for maximum likelihood\n  estimation", "comments": "34 pages; minor changes in comparison to version 2. The discrete part\n  on log-linear models from version 1 is contained in the companion paper\n  arXiv:2012.07793", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We uncover connections between maximum likelihood estimation in statistics\nand norm minimization over a group orbit in invariant theory. We focus on\nGaussian transformation families, which include matrix normal models and\nGaussian graphical models given by transitive directed acyclic graphs. We use\nstability under group actions to characterize boundedness of the likelihood,\nand existence and uniqueness of the maximum likelihood estimate. Our approach\nreveals promising consequences of the interplay between invariant theory and\nstatistics. In particular, existing scaling algorithms from statistics can be\nused in invariant theory, and vice versa.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:47:41 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:18:41 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 17:29:19 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Kohn", "Kathl\u00e9n", ""], ["Reichenbach", "Philipp", ""], ["Seigal", "Anna", ""]]}, {"id": "2003.13723", "submitter": "Panagiotis Lolas", "authors": "Panagiotis Lolas", "title": "Regularization in High-Dimensional Regression and Classification via\n  Random Matrix Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study general singular value shrinkage estimators in high-dimensional\nregression and classification, when the number of features and the sample size\nboth grow proportionally to infinity. We allow models with general covariance\nmatrices that include a large class of data generating distributions. As far as\nthe implications of our results are concerned, we find exact asymptotic\nformulas for both the training and test errors in regression models fitted by\ngradient descent, which provides theoretical insights for early stopping as a\nregularization method. In addition, we propose a numerical method based on the\nempirical spectra of covariance matrices for the optimal eigenvalue shrinkage\nclassifier in linear discriminant analysis. Finally, we derive optimal\nestimators for the dense mean vectors of high-dimensional distributions.\nThroughout our analysis we rely on recent advances in random matrix theory and\ndevelop further results of independent mathematical interest.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:18:04 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lolas", "Panagiotis", ""]]}, {"id": "2003.13819", "submitter": "Milad Bakhshizadeh", "authors": "Milad Bakhshizadeh, Arian Maleki, Victor H. de la Pena", "title": "Sharp Concentration Results for Heavy-Tailed Distributions", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain concentration and large deviation for the sums of independent and\nidentically distributed random variables with heavy-tailed distributions. Our\nconcentration results are concerned with random variables whose distributions\nsatisfy $P(X>t) \\leq {\\rm e}^{- I(t)}$, where $I: \\mathbb{R} \\rightarrow\n\\mathbb{R}$ is an increasing function and $I(t)/t \\rightarrow \\alpha \\in [0,\n\\infty)$ as $t \\rightarrow \\infty$. Our main theorem can not only recover some\nof the existing results, such as the concentration of the sum of subWeibull\nrandom variables, but it can also produce new results for the sum of random\nvariables with heavier tails. We show that the concentration inequalities we\nobtain are sharp enough to offer large deviation results for the sums of\nindependent random variables as well. Our analyses which are based on standard\ntruncation arguments simplify, unify and generalize the existing results on the\nconcentration and large deviation of heavy-tailed random variables.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:05:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:12:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bakhshizadeh", "Milad", ""], ["Maleki", "Arian", ""], ["de la Pena", "Victor H.", ""]]}, {"id": "2003.13844", "submitter": "Xin Bing", "authors": "Xin Bing and Yang Ning and Yaosheng Xu", "title": "Adaptive Estimation in Multivariate Response Regression with Hidden\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of the coefficient matrix $\\Ttheta$ in\nmultivariate regression with hidden variables, $Y = (\\Ttheta)^TX + (B^*)^TZ +\nE$, where $Y$ is a $m$-dimensional response vector, $X$ is a $p$-dimensional\nvector of observable features, $Z$ represents a $K$-dimensional vector of\nunobserved hidden variables, possibly correlated with $X$, and $E$ is an\nindependent error. The number of hidden variables $K$ is unknown and both $m$\nand $p$ are allowed but not required to grow with the sample size $n$. Since\nonly $Y$ and $X$ are observable, we provide necessary conditions for the\nidentifiability of $\\Ttheta$. The same set of conditions are shown to be\nsufficient when the error $E$ is homoscedastic. Our identifiability proof is\nconstructive and leads to a novel and computationally efficient estimation\nalgorithm, called HIVE. The first step of the algorithm is to estimate the best\nlinear prediction of $Y$ given $X$ in which the unknown coefficient matrix\nexhibits an additive decomposition of $\\Ttheta$ and a dense matrix originated\nfrom the correlation between $X$ and the hidden variable $Z$. Under the row\nsparsity assumption on $\\Ttheta$, we propose to minimize a penalized least\nsquares loss by regularizing $\\Ttheta$ via a group-lasso penalty and\nregularizing the dense matrix via a multivariate ridge penalty. Non-asymptotic\ndeviation bounds of the in-sample prediction error are established. Our second\nstep is to estimate the row space of $B^*$ by leveraging the covariance\nstructure of the residual vector from the first step. In the last step, we\nremove the effect of hidden variable by projecting $Y$ onto the complement of\nthe estimated row space of $B^*$. Non-asymptotic error bounds of our final\nestimator are established. The model identifiability, parameter estimation and\nstatistical guarantees are further extended to the setting with heteroscedastic\nerrors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:17:19 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 19:17:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bing", "Xin", ""], ["Ning", "Yang", ""], ["Xu", "Yaosheng", ""]]}, {"id": "2003.13849", "submitter": "Ad Ridder", "authors": "Shaul K. Bar-Lev and Ad Ridder", "title": "New exponential dispersion models for count data -- the ABM and LM\n  classes", "comments": "27 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their fundamental paper on cubic variance functions, Letac and Mora (The\nAnnals of Statistics,1990) presented a systematic, rigorous and comprehensive\nstudy of natural exponential families on the real line, their characterization\nthrough their variance functions and mean value parameterization. They\npresented a section that for some reason has been left unnoticed. This section\ndeals with the construction of variance functions associated with natural\nexponential families of counting distributions on the set of nonnegative\nintegers and allows to find the corresponding generating measures. As\nexponential dispersion models are based on natural exponential families, we\nintroduce in this paper two new classes of exponential dispersion models based\non their results. For these classes, which are associated with simple variance\nfunctions, we derive their mean value parameterization and their associated\ngenerating measures. We also prove that they have some desirable properties.\nBoth classes are shown to be overdispersed and zero-inflated in ascending\norder, making them as competitive statistical models for those in use in both,\nstatistical and actuarial modeling. To our best knowledge, the classes of\ncounting distributions we present in this paper, have not been introduced or\ndiscussed before in the literature. To show that our classes can serve as\ncompetitive statistical models for those in use (e.g., Poisson, Negative\nbinomial), we include a numerical example of real data. In this example, we\ncompare the performance of our classes with relevant competitive models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:38:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 21:45:14 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bar-Lev", "Shaul K.", ""], ["Ridder", "Ad", ""]]}, {"id": "2003.13938", "submitter": "Goubi Mouloud", "authors": "Mouloud Goubi", "title": "On the the linear processes of a stationary time series AR(2)", "comments": "5p", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim in this work is to give explicit formula of the linear processes\nsolution of autoregressive time series AR(2) with hint of generating functions\ntheory by using the Horadam numbers and polynomials.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:34:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Goubi", "Mouloud", ""]]}, {"id": "2003.14250", "submitter": "Joshua Agterberg", "authors": "Joshua Agterberg, Minh Tang, Carey E. Priebe", "title": "On Two Distinct Sources of Nonidentifiability in Latent Position Random\n  Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two separate and distinct sources of nonidentifiability arise naturally in\nthe context of latent position random graph models, though neither are unique\nto this setting. In this paper we define and examine these two\nnonidentifiabilities, dubbed subspace nonidentifiability and model-based\nnonidentifiability, in the context of random graph inference. We give examples\nwhere each type of nonidentifiability comes into play, and we show how in\ncertain settings one need worry about one or the other type of\nnonidentifiability. Then, we characterize the limit for model-based\nnonidentifiability both with and without subspace nonidentifiability. We\nfurther obtain additional limiting results for covariances and $U$-statistics\nof stochastic block models and generalized random dot product graphs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:41:02 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Agterberg", "Joshua", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}]