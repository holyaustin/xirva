[{"id": "1904.00070", "submitter": "Yi Hao", "authors": "Yi Hao, Alon Orlitsky, Ananda T. Suresh, Yihong Wu", "title": "Data Amplification: A Unified and Competitive Approach to Property\n  Estimation", "comments": "In NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating properties of discrete distributions is a fundamental problem in\nstatistical learning. We design the first unified, linear-time, competitive,\nproperty estimator that for a wide class of properties and for all underlying\ndistributions uses just $2n$ samples to achieve the performance attained by the\nempirical estimator with $n\\sqrt{\\log n}$ samples. This provides off-the-shelf,\ndistribution-independent, \"amplification\" of the amount of data available\nrelative to common-practice estimators.\n  We illustrate the estimator's practical advantages by comparing it to\nexisting estimators for a wide variety of properties and distributions. In most\ncases, its performance with $n$ samples is even as good as that of the\nempirical estimator with $n\\log n$ samples, and for essentially all properties,\nits performance is comparable to that of the best existing estimator designed\nspecifically for that property.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 19:49:01 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hao", "Yi", ""], ["Orlitsky", "Alon", ""], ["Suresh", "Ananda T.", ""], ["Wu", "Yihong", ""]]}, {"id": "1904.00173", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Asymptotic nonparametric statistical analysis of stationary time series", "comments": "This is the author's version of the homonymous volume published by\n  Springer. The final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-030-12564-6 Further updates and corrections may\n  be made here", "journal-ref": null, "doi": "10.1007/978-3-030-12564-6", "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationarity is a very general, qualitative assumption, that can be assessed\non the basis of application specifics. It is thus a rather attractive\nassumption to base statistical analysis on, especially for problems for which\nless general qualitative assumptions, such as independence or finite memory,\nclearly fail. However, it has long been considered too general to allow for\nstatistical inference to be made. One of the reasons for this is that rates of\nconvergence, even of frequencies to the mean, are not available under this\nassumption alone. Recently, it has been shown that, while some natural and\nsimple problems such as homogeneity, are indeed provably impossible to solve if\none only assumes that the data is stationary (or stationary ergodic), many\nothers can be solved using rather simple and intuitive algorithms. The latter\nproblems include clustering and change point estimation. In this volume I\nsummarize these results. The emphasis is on asymptotic consistency, since this\nthe strongest property one can obtain assuming stationarity alone. While for\nmost of the problems for which a solution is found this solution is\nalgorithmically realizable, the main objective in this area of research, the\nobjective which is only partially attained, is to understand what is possible\nand what is not possible to do for stationary time series. The considered\nproblems include homogeneity testing, clustering with respect to distribution,\nclustering with respect to independence, change-point estimation, identity\ntesting, and the general question of composite hypotheses testing. For the\nlatter problem, a topological criterion for the existence of a consistent test\nis presented. In addition, several open questions are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 08:47:46 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1904.00245", "submitter": "Simone Padoan PhD", "authors": "Simone A. Padoan, Stefano Rizzelli", "title": "Consistency of Bayesian Inference for Multivariate Max-Stable\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting extreme events is important in many applications in risk analysis.\nThe extreme-value theory suggests modelling extremes by max-stable\ndistributions. The Bayesian approach provides a natural framework for\nstatistical prediction. Although various Bayesian inferential procedures have\nbeen proposed in the literature of univariate extremes and some for\nmultivariate extremes, the study of their asymptotic properties has been left\nlargely untouched. In this paper we focus on a semiparatric Bayesian method for\nestimating max-stable distributions in arbitrary dimension. We establish\nconsistency of the pertaining posterior distributions for fairly general,\nwell-specified max-stable models, whose margins can be short-, light- or\nheavy-tailed. We then extend our consistency results to the case where the data\ncome from a distribution lying in a neighbourhood of a max-stable one, which\nrepresents the most realistic inferential setting.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 16:37:49 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 18:14:06 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 13:27:04 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Padoan", "Simone A.", ""], ["Rizzelli", "Stefano", ""]]}, {"id": "1904.00459", "submitter": "Jordan Awan", "authors": "Jordan Awan and Aleksandra Slavkovic", "title": "Differentially Private Inference for Binomial Data", "comments": "25 pages before references; 39 pages total. 8 figures. arXiv admin\n  note: text overlap with arXiv:1805.09236", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive uniformly most powerful (UMP) tests for simple and one-sided\nhypotheses for a population proportion within the framework of Differential\nPrivacy (DP), optimizing finite sample performance. We show that in general, DP\nhypothesis tests can be written in terms of linear constraints, and for\nexchangeable data can always be expressed as a function of the empirical\ndistribution. Using this structure, we prove a 'Neyman-Pearson lemma' for\nbinomial data under DP, where the DP-UMP only depends on the sample sum. Our\ntests can also be stated as a post-processing of a random variable, whose\ndistribution we coin ''Truncated-Uniform-Laplace'' (Tulap), a generalization of\nthe Staircase and discrete Laplace distributions. Furthermore, we obtain exact\n$p$-values, which are easily computed in terms of the Tulap random variable.\n  Using the above techniques, we show that our tests can be applied to give\nuniformly most accurate one-sided confidence intervals and optimal confidence\ndistributions. We also derive uniformly most powerful unbiased (UMPU) two-sided\ntests, which lead to uniformly most accurate unbiased (UMAU) two-sided\nconfidence intervals. We show that our results can be applied to\ndistribution-free hypothesis tests for continuous data. Our simulation results\ndemonstrate that all our tests have exact type I error, and are more powerful\nthan current techniques.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 18:27:53 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Awan", "Jordan", ""], ["Slavkovic", "Aleksandra", ""]]}, {"id": "1904.00555", "submitter": "Abhijit Mandal", "authors": "Anirban Mondal, Abhijit Mandal", "title": "Stratified Random Sampling for Dependent Inputs", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.jspi.2019.08.001", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach of obtaining stratified random samples from statistically\ndependent random variables is described. The proposed method can be used to\nobtain samples from the input space of a computer forward model in estimating\nexpectations of functions of the corresponding output variables. The advantage\nof the proposed method over the existing methods is that it preserves the exact\nform of the joint distribution on the input variables. The asymptotic\ndistribution of the new estimator is derived. Asymptotically, the variance of\nthe estimator using the proposed method is less than that obtained using the\nsimple random sampling, with the degree of variance reduction depending on the\ndegree of additivity in the function being integrated. This technique is\napplied to a practical example related to the performance of the river flood\ninundation model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:17:12 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Mondal", "Anirban", ""], ["Mandal", "Abhijit", ""]]}, {"id": "1904.00763", "submitter": "Samy Blusseau", "authors": "Bastien Ponchon (CMM, LTCI), Santiago Velasco-Forero (CMM), Samy\n  Blusseau (CMM), Jesus Angulo (CMM), Isabelle Bloch (LTCI)", "title": "Part-based approximations for morphological operators using asymmetric\n  auto-encoders", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology, Jul 2019,\n  Saarbr{\\\"u}cken, Germany", "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of building a part-based representation of a\ndataset of images. More precisely, we look for a non-negative, sparse\ndecomposition of the images on a reduced set of atoms, in order to unveil a\nmorphological and interpretable structure of the data. Additionally, we want\nthis decomposition to be computed online for any new sample that is not part of\nthe initial dataset. Therefore, our solution relies on a sparse, non-negative\nauto-encoder where the encoder is deep (for accuracy) and the decoder shallow\n(for interpretability). This method compares favorably to the state-of-the-art\nonline methods on two datasets (MNIST and Fashion MNIST), according to\nclassical metrics and to a new one we introduce, based on the invariance of the\nrepresentation to morphological dilation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 08:16:48 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 12:03:34 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ponchon", "Bastien", "", "CMM, LTCI"], ["Velasco-Forero", "Santiago", "", "CMM"], ["Blusseau", "Samy", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Bloch", "Isabelle", "", "LTCI"]]}, {"id": "1904.00840", "submitter": "Marija Cupari\\'c", "authors": "Marija Cupari\\'c, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "New consistent exponentiality tests based on $V$-empirical Laplace\n  transforms with comparison of efficiencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new consistent goodness-of-fit tests for exponential distribution,\nbased on the Desu characterization. The test statistics represent the weighted\n$L^2$ and $L^{\\infty}$ distances between appropriate V-empirical Laplace\ntransforms of random variables that appear in the characterization. In\naddition, we perform an extensive comparison of Bahadur efficiencies of\ndifferent recent and classical exponentiality tests. We also present the\nempirical powers of new tests.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:36:51 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 07:30:51 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Cupari\u0107", "Marija", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1904.00891", "submitter": "Nazar Buzun", "authors": "Nazar Buzun", "title": "Gaussian approximation for empirical barycenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider Wasserstein barycenters (average in Wasserstein\ndistance) in Fourier basis. We prove that random Fourier parameters of the\nbarycenter converge to Gaussian random vector by distribution. The convergence\napproximation has been done in finite-sample condition with rate\n$O(p/\\sqrt{n})$ depending on measures count ($n$) and the dimension of\nparameters ($p$).\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 14:49:25 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 02:26:30 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Buzun", "Nazar", ""]]}, {"id": "1904.00917", "submitter": "Sidney Resnick", "authors": "Jaakko Lehtomaa, Sidney Resnick", "title": "Asymptotic independence and support detection techniques for\n  heavy-tailed multivariate data", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central objectives of modern risk management is to find a set of\nrisks where the probability of multiple simultaneous catastrophic events is\nnegligible. That is, risks are taken only when their joint behavior seems\nsufficiently independent. This paper aims to help to identify asymptotically\nindependent risks by providing additional tools for describing dependence\nstructures of multiple risks when the individual risks can obtain very large\nvalues.\n  The study is performed in the setting of multivariate regular variation. We\nshow how asymptotic independence is connected to properties of the support of\nthe angular measure and present an asymptotically consistent estimator of the\nsupport. The estimator generalizes to any dimension $N\\geq 2$ and requires no\nprior knowledge of the support. The validity of the support estimate can be\nrigorously tested under mild assumptions by an asymptotically normal test\nstatistic.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 15:45:39 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Lehtomaa", "Jaakko", ""], ["Resnick", "Sidney", ""]]}, {"id": "1904.01003", "submitter": "Eduard Belitser", "authors": "Eduard Belitser and Nurzhan Nurushev", "title": "General framework for projection structures", "comments": "89 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part, we develop a general framework for projection structures\nand study several inference problems within this framework. We propose\nprocedures based on data dependent measures (DDM) and make connections with\nempirical Bayes and penalization methods. The main inference problem is the\nuncertainty quantification (UQ), but on the way we solve the estimation,\nDDM-contraction problems, and a weak version of the structure recovery problem.\nThe approach is local in that the quality of the inference procedures is\nmeasured by the local quantity, the oracle rate, which is the best trade-off\nbetween the approximation error by a projection structure and the complexity of\nthat approximating projection structure. Like in statistical learning settings,\nwe develop distribution-free theory as no particular model is imposed, we only\nassume certain mild condition on the stochastic part of the projection\npredictor. We introduce the excessive bias restriction (EBR) under which we\nestablish the local confidence optimality of the constructed confidence ball.\n  The proposed general framework unifies a very broad class of high-dimensional\nmodels and structures, interesting and important on their own right. In the\nsecond part, we apply the developed theory and demonstrate how the general\nresults deliver a whole avenue of local and global minimax results (many new\nones, some known results from the literature are improved) for particular\nmodels and structures as consequences, including white noise model and density\nestimation with smoothness structure, linear regression and dictionary learning\nwith sparsity structures, biclustering and stochastic block models with\nclustering structure, covariance matrix estimation with banding and sparsity\nstructures, and many others. Various adaptive minimax results over various\nscales follow also from our local results.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 11:56:33 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 13:18:28 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Belitser", "Eduard", ""], ["Nurushev", "Nurzhan", ""]]}, {"id": "1904.01320", "submitter": "Michael Messer", "authors": "Michael Messer", "title": "Bivariate change point detection: joint detection of changes in\n  expectation and variance", "comments": "32 pages, 14 figures, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for change point detection is proposed. We consider a univariate\nsequence of independent random variables with piecewise constant expectation\nand variance, apart from which the distribution may vary periodically. We aim\nto detect change points in both expectation and variance. For that, we propose\na statistical test for the null hypothesis of no change points and an algorithm\nfor change point detection. Both are based on a bivariate moving sum approach\nthat jointly evaluates the mean and the empirical variance. The joint\nconsideration helps improve inference as compared to separate univariate\napproaches. We infer on the strength and the type of changes with confidence.\nNonparametric methodology supports the analysis of diverse data. Additionally,\na multi-scale approach addresses complex patterns in change points and effects.\nWe demonstrate the performance through theoretical results and simulation\nstudies. A companion R-package jcp (available on CRAN) is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:30:33 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 17:19:46 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 18:35:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Messer", "Michael", ""]]}, {"id": "1904.01383", "submitter": "Amine Hadji", "authors": "Amine Hadji and Botond Sz\\'abo", "title": "Can we trust Bayesian uncertainty quantification from Gaussian process\n  priors with squared exponential covariance kernel?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the frequentist coverage properties of credible sets resulting\nin from Gaussian process priors with squared exponential covariance kernel.\nFirst we show that by selecting the scaling hyper-parameter using the maximum\nmarginal likelihood estimator in the (slightly modified) squared exponential\ncovariance kernel the corresponding credible sets will provide overconfident,\nmisleading uncertainty statements for a large, representative subclass of the\nfunctional parameters in context of the Gaussian white noise model. Then we\nshow that by either blowing up the credible sets with a logarithmic factor or\nmodifying the maximum marginal likelihood estimator with a logarithmic term one\ncan get reliable uncertainty statement and adaptive size of the credible sets\nunder some additional restriction. Finally we demonstrate on a numerical study\nthat the derived negative and positive results extend beyond the Gaussian white\nnoise model to the nonparametric regression and classification models for small\nsample sizes as well.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:00:10 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hadji", "Amine", ""], ["Sz\u00e1bo", "Botond", ""]]}, {"id": "1904.01494", "submitter": "Christopher Rembold", "authors": "Christopher M. Rembold", "title": "Linearity of Data and Linear Probability Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some data is linearly additive, other data is not. In this paper, I discuss\ntypes of data based on the boundedness of the data and their linearity. 1)\nUnbounded data can be linear. 2) One-side bounded data is usually log\ntransformed to be linear. 3) Two-side bounded data is not linear. 4) Untidy\ndata do not fit in these categories. An example of two-sided bounded data is\nprobabilities which should be transformed into a linear probability space by\ntaking the logarithm of the odds ratio (log10 odds) which is termed Weight (W).\nCalculations of means and standard deviation is more accurate when calculated\nas W values than when calculated as probabilities. A methods to analyze untidy\ndata is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:33:08 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Rembold", "Christopher M.", ""]]}, {"id": "1904.01729", "submitter": "Koji Tsukuda", "authors": "Koji Tsukuda", "title": "Error bounds for the normal approximation to the length of a Ewens\n  partition", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $K(=K_{n,\\theta})$ be a positive integer-valued random variable whose\ndistribution is given by ${\\rm P}(K = x) = \\bar{s}(n,x) \\theta^x/(\\theta)_n$\n$(x=1,\\ldots,n) $, where $\\theta$ is a positive number, $n$ is a positive\ninteger, $(\\theta)_n=\\theta(\\theta+1)\\cdots(\\theta+n-1)$ and $\\bar{s}(n,x)$ is\nthe coefficient of $\\theta^x$ in $(\\theta)_n$ for $x=1,\\ldots,n$. This formula\ndescribes the distribution of the length of a Ewens partition, which is a\nstandard model of random partitions. As $n$ tends to infinity, $K$\nasymptotically follows a normal distribution. Moreover, as $n$ and $\\theta$\nsimultaneously tend to infinity, if $n^2/\\theta\\to\\infty$, $K$ also\nasymptotically follows a normal distribution. In this paper, error bounds for\nthe normal approximation are provided. The result shows that the decay rate of\nthe error changes due to asymptotic regimes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:10:26 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 11:29:42 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Tsukuda", "Koji", ""]]}, {"id": "1904.01820", "submitter": "Giulio Biroli", "authors": "Giulio Biroli and Alice Guionnet", "title": "Large deviations for the largest eigenvalues and eigenvectors of spiked\n  random matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.dis-nn cond-mat.stat-mech math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider matrices formed by a random $N\\times N$ matrix drawn from the\nGaussian Orthogonal Ensemble (or Gaussian Unitary Ensemble) plus a rank-one\nperturbation of strength $\\theta$, and focus on the largest eigenvalue, $x$,\nand the component, $u$, of the corresponding eigenvector in the direction\nassociated to the rank-one perturbation. We obtain the large deviation\nprinciple governing the atypical joint fluctuations of $x$ and $u$.\nInterestingly, for $\\theta>1$, in large deviations characterized by a small\nvalue of $u$, i.e. $u<1-1/\\theta$, the second-largest eigenvalue pops out from\nthe Wigner semi-circle and the associated eigenvector orients in the direction\ncorresponding to the rank-one perturbation. We generalize these results to the\nWishart Ensemble, and we extend them to the first $n$ eigenvalues and the\nassociated eigenvectors.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 07:53:33 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Biroli", "Giulio", ""], ["Guionnet", "Alice", ""]]}, {"id": "1904.01859", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Creating new distributions using integration and summation by parts", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for generating new distributions from old can be thought of as\ntechniques for simplifying integrals used in reverse. Hence integrating a\nprobability density function (pdf) by parts provides a new way of modifying\ndistributions; the resulting pdfs are integrals that sometimes require\ncomputation as special functions. Summation by parts can be used similarly for\ndiscrete distributions. The general methodology is given, with some examples of\ndistribution classes and of specific distributions, and fits to data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 09:02:21 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1904.02130", "submitter": "Krishnakumar Balasubramanian", "authors": "Andreas Anastasiou, Krishnakumar Balasubramanian, Murat A. Erdogdu", "title": "Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic\n  Rates of Martingale CLT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged\nstochastic gradient descent (SGD) to a normal random vector for a class of\ntwice-differentiable test functions. A crucial intermediate step is proving a\nnon-asymptotic martingale central limit theorem (CLT), i.e., establishing the\nrates of convergence of a multivariate martingale difference sequence to a\nnormal random vector, which might be of independent interest. We obtain the\nexplicit rates for the multivariate martingale CLT using a combination of\nStein's method and Lindeberg's argument, which is then used in conjunction with\na non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our results have\npotentially interesting consequences for computing confidence intervals for\nparameter estimation with SGD and constructing hypothesis tests with SGD that\nare valid in a non-asymptotic sense.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:47:46 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Balasubramanian", "Krishnakumar", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "1904.02182", "submitter": "Hyun-Jung Kim", "authors": "Igor Cialenco, Hyun-Jung Kim and Sergey V. Lototsky", "title": "Statistical Analysis of Some Evolution Equations Driven by Space-only\n  Noise", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical properties of stochastic evolution equations driven\nby space-only noise, either additive or multiplicative. While forward problems,\nsuch as existence, uniqueness, and regularity of the solution, for such\nequations have been studied, little is known about inverse problems for these\nequations. We exploit the somewhat unusual structure of the observations coming\nfrom these equations that leads to an interesting interplay between classical\nand non-traditional statistical models. We derive several types of estimators\nfor the drift and/or diffusion coefficients of these equations, and prove their\nrelevant properties.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:07:28 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Cialenco", "Igor", ""], ["Kim", "Hyun-Jung", ""], ["Lototsky", "Sergey V.", ""]]}, {"id": "1904.02219", "submitter": "Abhik Ghosh PhD", "authors": "Elena Castilla, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "Robust semiparametric inference for polytomous logistic regression with\n  complex survey design", "comments": "Preprint; Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing polytomous response from a complex survey scheme, like stratified\nor cluster sampling is very crucial in several socio-economics applications. We\npresent a class of minimum quasi weighted density power divergence estimators\nfor the polytomous logistic regression model with such a complex survey. This\nfamily of semiparametric estimators is a robust generalization of the maximum\nquasi weighted likelihood estimator exploiting the advantages of the popular\ndensity power divergence measure. Accordingly robust estimators for the design\neffects are also derived. Robust testing of general linear hypotheses on the\nregression coefficients are proposed using the new estimators. Their asymptotic\ndistributions and robustness properties are theoretically studied and also\nempirically validated through a numerical example and an extensive Monte Carlo\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:47:17 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Castilla", "Elena", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1904.02250", "submitter": "Curtis Miller", "authors": "Lajos Horv\\'ath and Curtis Miller and Gregory Rice", "title": "A new class of change point test statistics of R\\'enyi type", "comments": "Accepted for publication by the Journal of Business and Economic\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of change point test statistics is proposed that utilizes a\nweighting and trimming scheme for the cumulative sum (CUSUM) process inspired\nby R\\'enyi (1953). A thorough asymptotic analysis and simulations both\ndemonstrate that this new class of statistics possess superior power compared\nto traditional change point statistics based on the CUSUM process when the\nchange point is near the beginning or end of the sample. Generalizations of\nthese \"R\\'enyi\" statistics are also developed to test for changes in the\nparameters in linear and non-linear regression models, and in generalized\nmethod of moments estimation. In these contexts we applied the proposed\nstatistics, as well as several others, to test for changes in the coefficients\nof Fama-French factor models. We observed that the R\\'enyi statistic was the\nmost effective in terms of retrospectively detecting change points that occur\nnear the endpoints of the sample.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 21:56:25 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Horv\u00e1th", "Lajos", ""], ["Miller", "Curtis", ""], ["Rice", "Gregory", ""]]}, {"id": "1904.02291", "submitter": "Rohit Agrawal", "authors": "Rohit Agrawal", "title": "Finite-Sample Concentration of the Multinomial in Relative Entropy", "comments": null, "journal-ref": "IEEE Trans. Inform. Theory 66(10):6297-6302, 2020", "doi": "10.1109/TIT.2020.2996134", "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the moment generating function of the Kullback-Leibler\ndivergence (relative entropy) between the empirical distribution of $n$\nindependent samples from a distribution $P$ over a finite alphabet of size $k$\n(i.e. a multinomial distribution) and $P$ itself is no more than that of a\ngamma distribution with shape $k - 1$ and rate $n$. The resulting exponential\nconcentration inequality becomes meaningful (less than 1) when the divergence\n$\\varepsilon$ is larger than $(k-1)/n$, whereas the standard method of types\nbound requires $\\varepsilon > \\frac{1}{n} \\cdot \\log{\\binom{n+k-1}{k-1}} \\geq\n(k-1)/n \\cdot \\log(1 + n/(k-1))$, thus saving a factor of order $\\log(n/k)$ in\nthe standard regime of parameters where $n\\gg k$. As a consequence, we also\nobtain finite-sample bounds on all the moments of the empirical divergence\n(equivalently, the discrete likelihood-ratio statistic), which are within\nconstant factors (depending on the moment) of their asymptotic values. Our\nproof proceeds via a simple reduction to the case $k = 2$ of a binary alphabet\n(i.e. a binomial distribution), and has the property that improvements in the\ncase of $k = 2$ directly translate to improvements for general $k$. In\nparticular, we conjecture a bound on the binomial moment generating function\nthat would almost close the quadratic gap between our finite-sample bound and\nthe asymptotic moment generating function bound from Wilks' theorem (which does\nnot hold for finite samples).\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 01:03:19 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 16:25:55 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 17:43:27 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 01:07:04 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Agrawal", "Rohit", ""]]}, {"id": "1904.02415", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl and Zahra Saberi", "title": "A Bayesian Nonparametric Test for Assessing Multivariate Normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel Bayesian nonparametric test for assessing multivariate\nnormal models is presented. While there are extensive frequentist and graphical\nmethods for testing multivariate normality, it is challenging to find Bayesian\ncounterparts. The proposed approach is based on the use of the Dirichlet\nprocess and Mahalanobis distance. More precisely, the Mahalanobis distance is\nemployed as a good technique to transform the $m$-variate problem into a\nunivariate problem. Then the Dirichlet process is used as a prior on the\ndistribution of the Mahalanobis distance. The concentration of the distribution\nof the distance between the posterior process and the chi-square distribution\nwith $m$ degrees of freedom is compared to the concentration of the\ndistribution of the distance between the prior process and the chi-square\ndistribution with $m$ degrees of freedom via a relative belief ratio. The\ndistance between the Dirichlet process and the chi-square distribution is\nestablished based on the Anderson-Darling distance. Key theoretical results of\nthe approach are derived. The procedure is illustrated through several\nexamples, in which the proposed approach shows excellent performance.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:00:10 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 14:37:04 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 09:44:06 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 03:53:30 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "1904.02505", "submitter": "Guillaume Dehaene P.", "authors": "Guillaume P. Dehaene", "title": "A deterministic and computable Bernstein-von Mises theorem", "comments": "The first version contained an incorrect claim in section 5.1 : in\n  general the KL divergence does not bound the difference of the moments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bernstein-von Mises results (BvM) establish that the Laplace approximation is\nasymptotically correct in the large-data limit. However, these results are\ninappropriate for computational purposes since they only hold over most, and\nnot all, datasets and involve hard-to-estimate constants. In this article, I\npresent a new BvM theorem which bounds the Kullback-Leibler (KL) divergence\nbetween a fixed log-concave density $f\\left(\\boldsymbol{\\theta}\\right)$ and its\nLaplace approximation. The bound goes to $0$ as the higher-derivatives of\n$f\\left(\\boldsymbol{\\theta}\\right)$ tend to $0$ and\n$f\\left(\\boldsymbol{\\theta}\\right)$ becomes increasingly Gaussian. The\nclassical BvM theorem in the IID large-data asymptote is recovered as a\ncorollary.\n  Critically, this theorem further suggests a number of computable\napproximations of the KL divergence with the most promising being: \\[\nKL\\left(g_{LAP},f\\right)\\approx\\frac{1}{2}\\text{Var}_{\\boldsymbol{\\theta}\\sim\ng\\left(\\boldsymbol{\\theta}\\right)}\\left(\\log\\left[f\\left(\\boldsymbol{\\theta}\\right)\\right]-\\log\\left[g_{LAP}\\left(\\boldsymbol{\\theta}\\right)\\right]\\right)\n\\] An empirical investigation of these bounds in the logistic classification\nmodel reveals that these approximations are great surrogates for the KL\ndivergence. This result, and future results of a similar nature, could provide\na path towards rigorously controlling the error due to the Laplace\napproximation and more modern approximation methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:06:34 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 12:56:40 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Dehaene", "Guillaume P.", ""]]}, {"id": "1904.02624", "submitter": "Michael Kosorok", "authors": "Pourab Roy, Jason P. Fine, and Michael R. Kosorok", "title": "Efficient estimation of accelerated lifetime models under length-biased\n  sampling", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In prevalent cohort studies where subjects are recruited at a cross-section,\nthe time to an event may be subject to length-biased sampling, with the\nobserved data being either the forward recurrence time, or the backward\nrecurrence time, or their sum. In the regression setting, it has been shown\nthat the accelerated failure time model for the underlying event time is\ninvariant under these observed data set-ups and can be fitted using standard\nmethodology for accelerated failure time model estimation, ignoring the\nlength-bias. However, the efficiency of these estimators is unclear, owing to\nthe fact that the observed covariate distribution, which is also length-biased,\nmay contain information about the regression parameter in the accelerated life\nmodel. We demonstrate that if the true covariate distribution is completely\nunspecified, then the naive estimator based on the conditional likelihood given\nthe covariates is fully efficient.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:53:06 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Roy", "Pourab", ""], ["Fine", "Jason P.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1904.02826", "submitter": "Oliver Maclaren", "authors": "Oliver J. Maclaren and Ruanui Nicholson", "title": "What can be estimated? Identifiability, estimability, causal inference\n  and ill-posed inverse problems", "comments": "41 pages, 5 figures. Fixed typos, added references, added examples.\n  New examples (updated again) introduce explicit 'view' and 'undo' operations\n  to complement 'do' operation as part of the translation between structural\n  causal models and our abstract statistical formalism", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider basic conceptual questions concerning the relationship between\nstatistical estimation and causal inference. Firstly, we show how to translate\ncausal inference problems into an abstract statistical formalism without\nrequiring any structure beyond an arbitrarily-indexed family of probability\nmodels. The formalism is simple but can incorporate a variety of causal\nmodelling frameworks, including 'structural causal models', but also models\nexpressed in terms of, e.g., differential equations. We focus primarily on the\nstructural/graphical causal modelling literature, however. Secondly, we\nconsider the extent to which causal and statistical concerns can be cleanly\nseparated, examining the fundamental question: 'What can be estimated from\ndata?'. We call this the problem of estimability. We approach this by analysing\na standard formal definition of 'can be estimated' commonly adopted in the\ncausal inference literature -- identifiability -- in our abstract statistical\nformalism. We use elementary category theory to show that identifiability\nimplies the existence of a Fisher-consistent estimator, but also show that this\nestimator may be discontinuous, and thus unstable, in general. This difficulty\narises because the causal inference problem is, in general, an ill-posed\ninverse problem. Inverse problems have three conditions which must be satisfied\nto be considered well-posed: existence, uniqueness, and stability of solutions.\nHere identifiability corresponds to the question of uniqueness; in contrast, we\ntake estimability to mean satisfaction of all three conditions, i.e.\nwell-posedness. Lack of stability implies that naive translation of a causally\nidentifiable quantity into an achievable statistical estimation target may\nprove impossible. Our article is primarily expository and aimed at unifying\nideas from multiple fields, though we provide new constructions and proofs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 23:46:44 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 06:36:23 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 04:54:06 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 01:48:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Maclaren", "Oliver J.", ""], ["Nicholson", "Ruanui", ""]]}, {"id": "1904.02880", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda and William E. Strawderman", "title": "Predictive density estimation under the Wasserstein loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate predictive density estimation under the $L^2$ Wasserstein loss\nfor location families and location-scale families. We show that plug-in\ndensities form a complete class and that the Bayesian predictive density is\ngiven by the plug-in density with the posterior mean of the location and scale\nparameters. We provide Bayesian predictive densities that dominate the best\nequivariant one in normal models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 05:56:01 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Matsuda", "Takeru", ""], ["Strawderman", "William E.", ""]]}, {"id": "1904.02883", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock and Geoffrey J. McLachlan", "title": "On missing label patterns in semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate model based classification with partially labelled training\ndata. In many biostatistical applications, labels are manually assigned by\nexperts, who may leave some observations unlabelled due to class uncertainty.\nWe analyse semi-supervised learning as a missing data problem and identify\nsituations where the missing label pattern is non-ignorable for the purposes of\nmaximum likelihood estimation. In particular, we find that a relationship\nbetween classification difficulty and the missing label pattern implies a\nnon-ignorable missingness mechanism. We examine a number of real datasets and\nconclude the pattern of missing labels is related to the difficulty of\nclassification. We propose a joint modelling strategy involving the observed\ndata and the missing label mechanism to account for the systematic missing\nlabels. Full likelihood inference including the missing label mechanism can\nimprove the efficiency of parameter estimation, and increase classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:07:14 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ahfock", "Daniel", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1904.02893", "submitter": "Francois Roueff", "authors": "Fran\\c{c}ois Roueff (IDS, S2A), Randal Douc (TIPIC-SAMOVAR, CITI), Ois\n  Roueff, Tepmony Sim (ITC)", "title": "Necessary and sufficient conditions for the identifiability of\n  observation-driven models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we are interested in proving that a given\nobservation-driven model is identifiable. In the case of a GARCH(p, q) model, a\nsimple sufficient condition has been established in [1] for showing the\nconsistency of the quasi-maximum likelihood estimator. It turns out that this\ncondition applies for a much larger class of observation-driven models, that we\ncall the class of linearly observation-driven models. This class includes\nstandard integer valued observation-driven time series, such as the log-linear\nPoisson GARCH or the NBIN-GARCH models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:53:46 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 16:33:23 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Roueff", "Fran\u00e7ois", "", "IDS, S2A"], ["Douc", "Randal", "", "TIPIC-SAMOVAR, CITI"], ["Roueff", "Ois", "", "IDS, S2A"], ["Sim", "Tepmony", "", "ITC"]]}, {"id": "1904.02963", "submitter": "Vincenzo Matta", "authors": "Vincenzo Matta, Augusto Santos, Ali H. Sayed", "title": "Graph Learning over Partially Observed Diffusion Networks: Role of\n  Degree Concentration", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.MA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines the problem of graph learning over a diffusion network\nwhen data can be collected from a limited portion of the network (partial\nobservability). The main question is to establish technical guarantees of\nconsistent recovery of the subgraph of probed network nodes, i) despite the\npresence of unobserved nodes; and ii) under different connectivity regimes,\nincluding the dense regime where the probed nodes are influenced by many\nconnections coming from the unobserved ones. We ascertain that suitable\nestimators of the combination matrix (i.e., the matrix that quantifies the\npairwise interaction between nodes) possess an identifiability gap that enables\nthe discrimination between connected and disconnected nodes. Fundamental\nconditions are established under which the subgraph of monitored nodes can be\nrecovered, with high probability as the network size increases, through\nuniversal clustering algorithms. This claim is proved for three matrix\nestimators: i) the Granger estimator that adapts to the partial observability\nsetting the solution that is exact under full observability ; ii) the one-lag\ncorrelation matrix; and iii) the residual estimator based on the difference\nbetween two consecutive time samples. A detailed characterization of the\nasymptotic behavior of these estimators is established in terms of an error\nbias and of the identifiability gap, and a sample complexity analysis is\nperformed to establish how the number of samples scales with the network size\nto achieve consistent learning. Comparison among the estimators is performed\nthrough illustrative examples that show how estimators that are not optimal in\nthe full observability regime can outperform the Granger estimator in the\npartial observability regime. The analysis reveals that the fundamental\nproperty enabling consistent graph learning is the statistical concentration of\nnode degrees.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:48:21 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 15:44:19 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Matta", "Vincenzo", ""], ["Santos", "Augusto", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1904.02965", "submitter": "Trang Bui Thi Thien", "authors": "Thi Thien Trang Bui", "title": "Aggregated kernel based tests for signal detection in a regression model", "comments": "31 pages, 5 tables", "journal-ref": null, "doi": "10.13140/RG.2.2.22895.28325", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering a regression model, we address the question of testing the\nnullity of the regression function. The testing procedure is available when the\nvariance of the observations is unknown and does not depend on any prior\ninformation on the alternative. We first propose a single testing procedure\nbased on a general symmetrickernel and an estimation of the variance of the\nobservations. The corresponding critical values are constructed to obtain non\nasymptotic level-? tests. We then introduce an aggregation procedure to avoid\nthe difficult choice of the kernel and of the parameters of the kernel. The\nmultiple tests satisfy non-asymptotic properties and are adaptive in the\nminimax sense over several classes of regular alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 09:49:49 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Bui", "Thi Thien Trang", ""]]}, {"id": "1904.03028", "submitter": "Abdellatif Zaidi", "authors": "Abdellatif Zaidi and Inaki Estella Aguerri", "title": "Optimal Rate-Exponent Region for a Class of Hypothesis Testing Against\n  Conditional Independence Problems", "comments": "Submitted for publication to the IEEE Information Theory Workshop,\n  ITW 2019. arXiv admin note: substantial text overlap with arXiv:1811.03933", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of distributed hypothesis testing against conditional\nindependence problems. Under the criterion that stipulates minimization of the\nType II error rate subject to a (constant) upper bound $\\epsilon$ on the Type I\nerror rate, we characterize the set of encoding rates and exponent for both\ndiscrete memoryless and memoryless vector Gaussian settings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:03:21 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Zaidi", "Abdellatif", ""], ["Aguerri", "Inaki Estella", ""]]}, {"id": "1904.03136", "submitter": "Cheng Mao", "authors": "Jan-Christian H\\\"utter, Cheng Mao, Philippe Rigollet and Elina Robeva", "title": "Estimation of Monge Matrices", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monge matrices and their permuted versions known as pre-Monge matrices\nnaturally appear in many domains across science and engineering. While the rich\nstructural properties of such matrices have long been leveraged for algorithmic\npurposes, little is known about their impact on statistical estimation. In this\nwork, we propose to view this structure as a shape constraint and study the\nproblem of estimating a Monge matrix subject to additive random noise. More\nspecifically, we establish the minimax rates of estimation of Monge and\npre-Monge matrices. In the case of pre-Monge matrices, the minimax-optimal\nleast-squares estimator is not efficiently computable, and we propose two\nefficient estimators and establish their rates of convergence. Our theoretical\nfindings are supported by numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 16:01:52 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["H\u00fctter", "Jan-Christian", ""], ["Mao", "Cheng", ""], ["Rigollet", "Philippe", ""], ["Robeva", "Elina", ""]]}, {"id": "1904.03171", "submitter": "Eric Marchand", "authors": "\\'Eric Marchand and William E. Strawderman", "title": "On shrinkage estimation for balanced loss functions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of a multivariate mean $\\theta$ is considered under natural\nmodifications of balanced loss function of the form: (i) $\\omega \\,\n\\rho(\\|\\delta-\\delta_0\\|^2) + (1-\\omega) \\, \\rho(\\|\\delta-\\theta\\|^2) $, and\n(ii) $\\ell \\left( \\omega \\, \\|\\delta-\\delta_0\\|^2 + (1-\\omega) \\,\n\\|\\delta-\\theta\\|^2 \\right)\\,$, where $\\delta_0$ is a target estimator of\n$\\gamma(\\theta)$. After briefly reviewing known results for original balanced\nloss with identity $\\rho$ or $\\ell$, we provide, for increasing and concave\n$\\rho$ and $\\ell$ which also satisfy a completely monotone property,\nBaranchik-type estimators of $\\theta$ which dominate the benchmark\n$\\delta_0(X)=X$ for $X$ either distributed as multivariate normal or as a scale\nmixture of normals. Implications are given with respect to model robustness and\nsimultaneous dominance with respect to either $\\rho$ or $\\ell\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:39:16 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Marchand", "\u00c9ric", ""], ["Strawderman", "William E.", ""]]}, {"id": "1904.03313", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Andrea Montanari", "title": "On the computational tractability of statistical estimation on amenable\n  graphs", "comments": "Stronger results, improved presentation. The transitivity assumption\n  on the limiting graph is removed. Instead, we introduce and use the notion of\n  a `tame' random rooted graph. 40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a vector of discrete variables\n$(\\theta_1,\\cdots,\\theta_n)$, based on noisy observations $Y_{uv}$ of the pairs\n$(\\theta_u,\\theta_v)$ on the edges of a graph $G=([n],E)$. This setting\ncomprises a broad family of statistical estimation problems, including group\nsynchronization on graphs, community detection, and low-rank matrix estimation.\n  A large body of theoretical work has established sharp thresholds for weak\nand exact recovery, and sharp characterizations of the optimal reconstruction\naccuracy in such models, focusing however on the special case of\nErd\\\"os--R\\'enyi-type random graphs. The single most important finding of this\nline of work is the ubiquity of an information-computation gap. Namely, for\nmany models of interest, a large gap is found between the optimal accuracy\nachievable by any statistical method, and the optimal accuracy achieved by\nknown polynomial-time algorithms. Moreover, this gap is generally believed to\nbe robust to small amounts of additional side information revealed about the\n$\\theta_i$'s.\n  How does the structure of the graph $G$ affect this picture? Is the\ninformation-computation gap a general phenomenon or does it only apply to\nspecific families of graphs?\n  We prove that the picture is dramatically different for graph sequences\nconverging to amenable graphs (including, for instance, $d$-dimensional grids).\nWe consider a model in which an arbitrarily small fraction of the vertex labels\nis revealed, and show that a linear-time local algorithm can achieve\nreconstruction accuracy that is arbitrarily close to the information-theoretic\noptimum. We contrast this to the case of random graphs. Indeed, focusing on\ngroup synchronization on random regular graphs, we prove that the\ninformation-computation gap still persists even when a small amount of side\ninformation is revealed.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 22:48:23 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 20:40:52 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Montanari", "Andrea", ""]]}, {"id": "1904.03372", "submitter": "Mengjia Yu", "authors": "Mengjia Yu and Xiaohui Chen", "title": "A robust bootstrap change point test for high-dimensional location\n  parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of change point detection for high-dimensional\ndistributions in a location family when the dimension can be much larger than\nthe sample size. In change point analysis, the widely used cumulative sum\n(CUSUM) statistics are sensitive to outliers and heavy-tailed distributions. In\nthis paper, we propose a robust, tuning-free (i.e., fully data-dependent), and\neasy-to-implement change point test that enjoys strong theoretical guarantees.\nTo achieve the robust purpose in a nonparametric setting, we formulate the\nchange point detection in the multivariate $U$-statistics framework with\nanti-symmetric and nonlinear kernels. Specifically, the within-sample noise is\ncancelled out by anti-symmetry of the kernel, while the signal distortion under\ncertain nonlinear kernels can be controlled such that the between-sample change\npoint signal is magnitude preserving. A (half) jackknife multiplier bootstrap\n(JMB) tailored to the change point detection setting is proposed to calibrate\nthe distribution of our $\\ell^{\\infty}$-norm aggregated test statistic. Subject\nto mild moment conditions on kernels, we derive the uniform rates of\nconvergence for the JMB to approximate the sampling distribution of the test\nstatistic, and analyze its size and power properties. Extensions to multiple\nchange point testing and estimation are discussed with illustration from\nnumeric studies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:16:01 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 04:46:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Yu", "Mengjia", ""], ["Chen", "Xiaohui", ""]]}, {"id": "1904.03530", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Prudhvi Gurram, and Gene Whipps", "title": "A Bayesian Theory of Change Detection in Statistically Periodic Random\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of stochastic processes called independent and periodically\nidentically distributed (i.p.i.d.) processes is defined to capture periodically\nvarying statistical behavior. A novel Bayesian theory is developed for\ndetecting a change in the distribution of an i.p.i.d. process. It is shown that\nthe Bayesian change point problem can be expressed as a problem of optimal\ncontrol of a Markov decision process (MDP) with periodic transition and cost\nstructures. Optimal control theory is developed for periodic MDPs for\ndiscounted and undiscounted total cost criteria. A fixed-point equation is\nobtained that is satisfied by the optimal cost function. It is shown that the\noptimal policy for the MDP is nonstationary but periodic in nature. A value\niteration algorithm is obtained to compute the optimal cost function. The\nresults from the MDP theory are then applied to detect changes in i.p.i.d.\nprocesses. It is shown that while the optimal change point algorithm is a\nstopping rule based on a periodic sequence of thresholds, a single-threshold\npolicy is asymptotically optimal, as the probability of false alarm goes to\nzero. Numerical results are provided to demonstrate that the asymptotically\noptimal policy is not strictly optimal.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 20:57:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Banerjee", "Taposh", ""], ["Gurram", "Prudhvi", ""], ["Whipps", "Gene", ""]]}, {"id": "1904.03559", "submitter": "Abram Kagan", "authors": "Abram M. Kagan and Paul J. Smith", "title": "Statistical Meaning of Mean Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic properties of the Fisher information allow to reveal the\nstatistical meaning of classical inequalities between mean functions. The\nproperties applied to scale mixtures of Gaussian distributions lead to a new\nmean function of purely statistical origin, unrelated to the classical\narithmetic, geometric, and harmonic means. We call it the informational mean\nand show that when the arguments of the mean functions are Hermitian positive\ndefinite matrices, not necessarily commuting, the informational mean lies\nbetween the arithmetic and harmonic means, playing, in a sense, the role of the\ngeometric mean that cannot be correctly defined in case of non-commuting\nmatrices.\\\\ Surprisingly the monotonicity and additivity properties of the\nFisher information lead to a new generalization of the classical inequality\nbetween the arithmetic and harmonic means.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:53:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kagan", "Abram M.", ""], ["Smith", "Paul J.", ""]]}, {"id": "1904.03725", "submitter": "Ezequiel Smucler", "authors": "Andrea Rotnitzky, Ezequiel Smucler and James M. Robins", "title": "Characterization of parameters with a mixed bias property", "comments": "minor revisions, added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study a class of parameters with the so-called `mixed bias\nproperty'. For parameters with this property, the bias of the semiparametric\nefficient one step estimator is equal to the mean of the product of the\nestimation errors of two nuisance functions. In non-parametric models,\nparameters with the mixed bias property admit so-called rate doubly robust\nestimators, i.e. estimators that are consistent and asymptotically normal when\none succeeds in estimating both nuisance functions at sufficiently fast rates,\nwith the possibility of trading off slower rates of convergence for the\nestimator of one of the nuisance functions with faster rates for the estimator\nof the other nuisance. We show that the class of parameters with the mixed bias\nproperty strictly includes two recently studied classes of parameters which, in\nturn, include many parameters of interest in causal inference. We characterize\nthe form of parameters with the mixed bias property and of their influence\nfunctions. Furthermore, we derive two functional moment equations, each being\nsolved at one of the two nuisance functions, as well as, two functional loss\nfunctions, each being minimized at one of the two nuisance functions. These\nloss functions can be used to derive loss based penalized estimators of the\nnuisance functions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:56:59 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 15:24:32 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Rotnitzky", "Andrea", ""], ["Smucler", "Ezequiel", ""], ["Robins", "James M.", ""]]}, {"id": "1904.03737", "submitter": "Ezequiel Smucler", "authors": "Ezequiel Smucler, Andrea Rotnitzky, James M. Robins", "title": "A unifying approach for doubly-robust $\\ell_1$ regularized estimation of\n  causal contrasts", "comments": "fixed example 11, added example 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference about a scalar parameter under a non-parametric model\nbased on a one-step estimator computed as a plug in estimator plus the\nempirical mean of an estimator of the parameter's influence function. We focus\non a class of parameters that have influence function which depends on two\ninfinite dimensional nuisance functions and such that the bias of the one-step\nestimator of the parameter of interest is the expectation of the product of the\nestimation errors of the two nuisance functions. Our class includes many\nimportant treatment effect contrasts of interest in causal inference and\neconometrics, such as ATE, ATT, an integrated causal contrast with a continuous\ntreatment, and the mean of an outcome missing not at random. We propose\nestimators of the target parameter that entertain approximately sparse\nregression models for the nuisance functions allowing for the number of\npotential confounders to be even larger than the sample size. By employing\nsample splitting, cross-fitting and $\\ell_1$-regularized regression estimators\nof the nuisance functions based on objective functions whose directional\nderivatives agree with those of the parameter's influence function, we obtain\nestimators of the target parameter with two desirable robustness properties:\n(1) they are rate doubly-robust in that they are root-n consistent and\nasymptotically normal when both nuisance functions follow approximately sparse\nmodels, even if one function has a very non-sparse regression coefficient, so\nlong as the other has a sufficiently sparse regression coefficient, and (2)\nthey are model doubly-robust in that they are root-n consistent and\nasymptotically normal even if one of the nuisance functions does not follow an\napproximately sparse model so long as the other nuisance function follows an\napproximately sparse model with a sufficiently sparse regression coefficient.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:34:26 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:07:12 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 21:59:14 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Smucler", "Ezequiel", ""], ["Rotnitzky", "Andrea", ""], ["Robins", "James M.", ""]]}, {"id": "1904.03752", "submitter": "Hanshen Xiao", "authors": "Hanshen Xiao, Guoqiang Xiao", "title": "From Co-prime to the Diophantine Equation Based Sparse Sensing", "comments": "Sparse Sensing; Co-prime Sampling; Co-prime Array", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a careful design of sample spacings either in temporal and spatial\ndomain, co-prime sensing can reconstruct the autocorrelation at a significantly\ndenser set of points based on Bazout theorem. However, still restricted from\nBazout theorem, it is required O(M1 + M2) samples to estimate frequencies in\nthe case of co-prime sampling, where M1 and M2 are co-prime down-sampling\nrates. Besides, for Direction-of-arrival (DOA) estimation, the sensors can not\nbe arbitrarily sparse in co-prime arrays. In this letter, we restrain our focus\non complex waveforms and present a framework under multiple samplers/sensors\nfor both frequency and DOA estimation based on Diophantine equation, which is\nessentially to estimate the autocorrelation with higher order statistics\ninstead of the second order one. We prove that, given arbitrarily high\ndown-sampling rates, there exist sampling schemes with samples to estimate\nautocorrelation only proportional to the sum of degrees of freedom (DOF) and\nthe number of snapshots required. In the scenario of DOA estimation, we show\nthere exist arrays of N sensors with O(N^3) DOF and O(N) minimal distance\nbetween sensors.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:54:32 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 14:16:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Xiao", "Hanshen", ""], ["Xiao", "Guoqiang", ""]]}, {"id": "1904.03817", "submitter": "Reza Hosseini", "authors": "Reza Hosseini, Amir Najmi", "title": "Unbiased variance reduction in randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a flexible method for decreasing the variance of\nestimators for complex experiment effect metrics (e.g. ratio metrics) while\nretaining asymptotic unbiasedness. This method uses the auxiliary information\nabout the experiment units to decrease the variance. The method can incorporate\nalmost any arbitrary predictive model (e.g. linear regression, regularization,\nneural networks) to adjust the estimators. The adjustment involves some free\nparameters which can be optimized to achieve the smallest variance reduction\ngiven the predictive model performance. Also we approximate the achievable\nreduction in variance in fairly general settings mathematically. Finally, we\nuse simulations to show the method works.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:22:16 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Hosseini", "Reza", ""], ["Najmi", "Amir", ""]]}, {"id": "1904.03858", "submitter": "Alexander Wein", "authors": "Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore", "title": "The Kikuchi Hierarchy and Tensor PCA", "comments": "42 pages. This version adds results on odd-order tensor PCA and\n  even-arity XOR refutation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the tensor PCA (principal component analysis) problem, we propose a new\nhierarchy of increasingly powerful algorithms with increasing runtime. Our\nhierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead\ninspired by statistical physics and related algorithms such as belief\npropagation and AMP (approximate message passing). Our level-$\\ell$ algorithm\ncan be thought of as a linearized message-passing algorithm that keeps track of\n$\\ell$-wise dependencies among the hidden variables. Specifically, our\nalgorithms are spectral methods based on the Kikuchi Hessian, which generalizes\nthe well-studied Bethe Hessian to the higher-order Kikuchi free energies.\n  It is known that AMP, the flagship algorithm of statistical physics, has\nsubstantially worse performance than SOS for tensor PCA. In this work we\n'redeem' the statistical physics approach by showing that our hierarchy gives a\npolynomial-time algorithm matching the performance of SOS. Our hierarchy also\nyields a continuum of subexponential-time algorithms, and we prove that these\nachieve the same (conjecturally optimal) tradeoff between runtime and\nstatistical power as SOS. Our proofs are much simpler than prior work, and also\napply to the related problem of refuting random $k$-XOR formulas. The results\nwe present here apply to tensor PCA for tensors of all orders, and to $k$-XOR\nwhen $k$ is even.\n  Our methods suggest a new avenue for systematically obtaining optimal\nalgorithms for Bayesian inference problems, and our results constitute a step\ntoward unifying the statistical physics and sum-of-squares approaches to\nalgorithm design.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 06:26:35 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:35:50 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wein", "Alexander S.", ""], ["Alaoui", "Ahmed El", ""], ["Moore", "Cristopher", ""]]}, {"id": "1904.03859", "submitter": "Anne Eggels", "authors": "A.W. Eggels, D.T. Crommelin", "title": "Efficient estimation of divergence-based sensitivity indices with\n  Gaussian process surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of sensitivity indices based on divergence\nmeasures such as Hellinger distance. For sensitivity analysis of complex\nmodels, these divergence-based indices can be estimated by Monte-Carlo sampling\n(MCS) in combination with kernel density estimation (KDE). In a direct\napproach, the complex model must be evaluated at every input point generated by\nMCS, resulting in samples in the input-output space that can be used for\ndensity estimation. However, if the computational cost of the complex model\nstrongly limits the number of model evaluations, this direct method gives large\nerrors. We propose to use Gaussian process (GP) surrogates to increase the\nnumber of samples in the combined input-output space. By enlarging this sample\nset, the KDE becomes more accurate, leading to improved estimates. To compare\nthe GP surrogates, we use a surrogate constructed by samples obtained with\nstochastic collocation, combined with Lagrange interpolation. Furthermore, we\npropose a new estimation method for these sensitivity indices based on minimum\nspanning trees. Finally, we also propose a new type of sensitivity indices\nbased on divergence measures, namely direct sensitivity indices. These are\nuseful when the input data is dependent.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 06:29:28 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:29:22 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Eggels", "A. W.", ""], ["Crommelin", "D. T.", ""]]}, {"id": "1904.03920", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier, Mohammad Emtiyaz\n  Khan", "title": "A Generalization Bound for Online Variational Inference", "comments": "Published in the proceedings of ACML 2019", "journal-ref": "Proceedings in Machine Learning Research, 2019, vol. 101, pp.\n  662-677", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference provides an attractive online-learning framework to\nanalyze sequential data, and offers generalization guarantees which hold even\nwith model mismatch and adversaries. Unfortunately, exact Bayesian inference is\nrarely feasible in practice and approximation methods are usually employed, but\ndo such methods preserve the generalization properties of Bayesian inference ?\nIn this paper, we show that this is indeed the case for some variational\ninference (VI) algorithms. We consider a few existing online, tempered VI\nalgorithms, as well as a new algorithm, and derive their generalization bounds.\nOur theoretical result relies on the convexity of the variational objective,\nbut we argue that the result should hold more generally and present empirical\nevidence in support of this. Our work in this paper presents theoretical\njustifications in favor of online algorithms relying on approximate Bayesian\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 09:53:25 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:32:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""], ["Khan", "Mohammad Emtiyaz", ""]]}, {"id": "1904.04052", "submitter": "Wesley Pegden", "authors": "Maria Chikina, Alan Frieze, Jonathan Mattingly, Wesley Pegden", "title": "Separating effect from significance in Markov chain tests", "comments": "title changed. (was: \"Practical tests for significance in Markov\n  chains\"). 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give qualitative and quantitative improvements to theorems which enable\nsignificance testing in Markov Chains, with a particular eye toward the goal of\nenabling strong, interpretable, and statistically rigorous claims of political\ngerrymandering. Our results can be used to demonstrate at a desired\nsignificance level that a given Markov Chain state (e.g., a districting) is\nextremely unusual (rather than just atypical) with respect to the fragility of\nits characteristics in the chain. We also provide theorems specialized to\nleverage quantitative improvements when there is a product structure in the\nunderlying probability space, as can occur due to geographical constraints on\ndistrictings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 13:36:28 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 02:43:30 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 22:48:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Chikina", "Maria", ""], ["Frieze", "Alan", ""], ["Mattingly", "Jonathan", ""], ["Pegden", "Wesley", ""]]}, {"id": "1904.04239", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Prudhvi Gurram, and Gene Whipps", "title": "Minimax-Optimal Algorithms for Detecting Changes in Statistically\n  Periodic Random Processes", "comments": "arXiv admin note: text overlap with arXiv:1810.12760,\n  arXiv:1807.06945", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory and algorithms are developed for detecting changes in the distribution\nof statistically periodic random processes. The statistical periodicity is\nmodeled using independent and periodically identically distributed processes, a\nnew class of stochastic processes proposed by us. An algorithm is developed\nthat is minimax asymptotically optimal as the false alarm rate goes to zero.\nAlgorithms are also developed for the cases when the post-change distribution\nis not known or when there are multiple streams of observations. The modeling\nis inspired by real datasets encountered in cyber-physical systems, biology,\nand medicine. The developed algorithms are applied to sequences of Instagram\ncounts collected around a 5K run in New York City to detect the run.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 21:06:23 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 04:22:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Banerjee", "Taposh", ""], ["Gurram", "Prudhvi", ""], ["Whipps", "Gene", ""]]}, {"id": "1904.04276", "submitter": "Lin Liu L", "authors": "Lin Liu, Rajarshi Mukherjee, James M. Robins", "title": "On nearly assumption-free tests of nominal confidence interval coverage\n  for causal parameters estimated by machine learning", "comments": "Significant updates from the previous version. In press in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many causal effect parameters of interest, doubly robust machine learning\n(DRML) estimators $\\hat{\\psi}_{1}$ are the state-of-the-art, incorporating the\ngood prediction performance of machine learning; the decreased bias of doubly\nrobust estimators; and the analytic tractability and bias reduction of sample\nsplitting with cross fitting. Nonetheless, even in the absence of confounding\nby unmeasured factors, the nominal $(1 - \\alpha)$ Wald confidence interval\n$\\hat{\\psi}_{1} \\pm z_{\\alpha / 2} \\widehat{\\mathsf{se}} [\\hat{\\psi}_{1}]$ may\nstill undercover even in large samples, because the bias of $\\hat{\\psi}_{1}$\nmay be of the same or even larger order than its standard error of order\n$n^{-1/2}$.\n  In this paper, we introduce essentially assumption-free tests that (i) can\nfalsify the null hypothesis that the bias of $\\hat{\\psi}_{1}$ is of smaller\norder than its standard error, (ii) can provide an upper confidence bound on\nthe true coverage of the Wald interval, and (iii) are valid under the null\nunder no smoothness/sparsity assumptions on the nuisance parameters. The tests,\nwhich we refer to as \\underline{A}ssumption \\underline{F}ree\n\\underline{E}mpirical \\underline{C}overage \\underline{T}ests (AFECTs), are\nbased on a U-statistic that estimates part of the bias of $\\hat{\\psi}_{1}$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:09:45 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:47:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Lin", ""], ["Mukherjee", "Rajarshi", ""], ["Robins", "James M.", ""]]}, {"id": "1904.04287", "submitter": "Ali Dolati", "authors": "S. M. Mirhoseini, A. Dolati and M. Amini", "title": "On a class of distributions generated by stochastic mixture of the\n  extreme order statistics of a sample of size two", "comments": null, "journal-ref": "J. J. Stat. Theory Appl, 10, 455-468 (2011)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper considers a family of distributions constructed by a stochastic\nmixture of the order statistics of a sample of size two. Various properties of\nthe proposed model are studied. We apply the model to extend the exponential\nand symmetric Laplace distributions. An extension to the bivariate case is\nconsidered.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:23:29 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Mirhoseini", "S. M.", ""], ["Dolati", "A.", ""], ["Amini", "M.", ""]]}, {"id": "1904.04417", "submitter": "Ruoyang Zhang", "authors": "Ruoyang Zhang, Malay Ghosh", "title": "Ultra High-dimensional Multivariate Posterior Contraction Rate Under\n  Shrinkage Priors", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, shrinkage priors have received much attention in\nhigh-dimensional data analysis from a Bayesian perspective. Compared with\nwidely used spike-and-slab priors, shrinkage priors have better computational\nefficiency. But the theoretical properties, especially posterior contraction\nrate, which is important in uncertainty quantification, are not established in\nmany cases. In this paper, we apply global-local shrinkage priors to\nhigh-dimensional multivariate linear regression with unknown covariance matrix.\nWe show that when the prior is highly concentrated near zero and has heavy\ntail, the posterior contraction rates for both coefficients matrix and\ncovariance matrix are nearly optimal. Our results hold when number of features\np grows much faster than the sample size n, which is of great interest in\nmodern data analysis. We show that a class of readily implementable scale\nmixture of normal priors satisfies the conditions of the main theorem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 01:53:31 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:40:00 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zhang", "Ruoyang", ""], ["Ghosh", "Malay", ""]]}, {"id": "1904.04525", "submitter": "Gianluca Finocchio", "authors": "Gianluca Finocchio, Johannes Schmidt-Hieber", "title": "Bayesian variance estimation in the Gaussian sequence model with partial\n  information on the means", "comments": "33 pages, 1 table, corrected typos, improved proofs, expanded\n  sections, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the Gaussian sequence model under the additional assumption that a\nfixed fraction of the means is known. We study the problem of variance\nestimation from a frequentist Bayesian perspective. The maximum likelihood\nestimator (MLE) for $\\sigma^2$ is biased and inconsistent. This raises the\nquestion whether the posterior is able to correct the MLE in this case. By\ndeveloping a new proving strategy that uses refined properties of the posterior\ndistribution, we find that the marginal posterior is inconsistent for any\ni.i.d. prior on the mean parameters. In particular, no assumption on the decay\nof the prior needs to be imposed. Surprisingly, we also find that consistency\ncan be retained for a hierarchical prior based on Gaussian mixtures. In this\ncase we also establish a limiting shape result and determine the limit\ndistribution. In contrast to the classical Bernstein-von Mises theorem, the\nlimit is non-Gaussian. We show that the Bayesian analysis leads to new\nstatistical estimators outperforming the correctly calibrated MLE in a\nnumerical simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 08:30:42 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 09:28:26 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Finocchio", "Gianluca", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1904.04556", "submitter": "Markus Bibinger", "authors": "Markus Bibinger", "title": "Cusum tests for changes in the Hurst exponent and volatility of\n  fractional Brownian motion", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we construct cusum change-point tests for the Hurst exponent\nand the volatility of a discretely observed fractional Brownian motion. As a\nstatistical application of the functional Breuer-Major theorems by B\\'egyn\n(2007) and Nourdin and Nualart (2019), we show under infill asymptotics\nconsistency of the tests and weak convergence to the Kolmogorov-Smirnov law\nunder the no-change hypothesis. The test is feasible and pivotal in the sense\nthat it is based on a statistic and critical values which do not require\nknowledge of any parameter values. Consistent estimation of the break date\nunder the alternative hypothesis is established. We demonstrate the\nfinite-sample properties in simulations and a data example.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:20:33 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 10:44:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bibinger", "Markus", ""]]}, {"id": "1904.04761", "submitter": "Alexander Jordan", "authors": "Alexander I. Jordan, Anja M\\\"uhlemann, Johanna F. Ziegel", "title": "Optimal solutions to the isotonic regression problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, the solution to a regression problem is the minimizer of a given\nloss criterion, and depends on the specified loss function. The nonparametric\nisotonic regression problem is special, in that optimal solutions can be found\nby solely specifying a functional. These solutions will then be minimizers\nunder all loss functions simultaneously as long as the loss functions have the\nrequested functional as the Bayes act. For the functional, the only requirement\nis that it can be defined via an identification function, with examples\nincluding the expectation, quantile, and expectile functionals. Generalizing\nclassical results, we characterize the optimal solutions to the isotonic\nregression problem for such functionals, and extend the results from the case\nof totally ordered explanatory variables to partial orders. For total orders,\nwe show that any solution resulting from the pool-adjacent-violators algorithm\nis optimal. It is noteworthy, that simultaneous optimality is unattainable in\nthe unimodal regression problem, despite its close connection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:15:26 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 14:02:14 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Jordan", "Alexander I.", ""], ["M\u00fchlemann", "Anja", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1904.04765", "submitter": "Song Fang", "authors": "Song Fang, Mikael Skoglund, Karl Henrik Johansson, Hideaki Ishii,\n  Quanyan Zhu", "title": "Generic Variance Bounds on Estimation and Prediction Errors in Time\n  Series Analysis: An Entropy Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we obtain generic bounds on the variances of estimation and\nprediction errors in time series analysis via an information-theoretic\napproach. It is seen in general that the error bounds are determined by the\nconditional entropy of the data point to be estimated or predicted given the\nside information or past observations. Additionally, we discover that in order\nto achieve the prediction error bounds asymptotically, the necessary and\nsufficient condition is that the \"innovation\" is asymptotically white Gaussian.\nWhen restricted to Gaussian processes and 1-step prediction, our bounds are\nshown to reduce to the Kolmogorov-Szeg\\\"o formula and Wiener-Masani formula\nknown from linear prediction theory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:22:14 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 09:17:02 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 15:57:47 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 23:12:57 GMT"}, {"version": "v5", "created": "Tue, 11 May 2021 14:48:31 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fang", "Song", ""], ["Skoglund", "Mikael", ""], ["Johansson", "Karl Henrik", ""], ["Ishii", "Hideaki", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1904.04774", "submitter": "Gregor Pasemann", "authors": "Gregor Pasemann and Wilhelm Stannat", "title": "Drift Estimation for Stochastic Reaction-Diffusion Systems", "comments": "see journal webpage for published version", "journal-ref": "Electron. J. Stat. 14 (2020), no. 1, 547-579", "doi": "10.1214/19-EJS1665", "report-no": null, "categories": "math.ST math.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parameter estimation problem for a class of semilinear stochastic evolution\nequations is considered. Conditions for consistency and asymptotic normality\nare given in terms of growth and continuity properties of the nonlinear part.\nEmphasis is put on the case of stochastic reaction-diffusion systems.\nRobustness results for statistical inference under model uncertainty are\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:37:25 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 17:11:37 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 22:34:59 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Pasemann", "Gregor", ""], ["Stannat", "Wilhelm", ""]]}, {"id": "1904.04813", "submitter": "Luc Berthouze", "authors": "Antoine Messager, Nicos Georgiou, Luc Berthouze", "title": "A new method for the robust characterisation of pairwise statistical\n  dependency between point processes", "comments": "Fixed error with the formulation of the variance term. Updated all\n  results accordingly. Removed the derivation of conditional expected value and\n  standard deviation (this will be covered elsewhere)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust detection of statistical dependencies between the components of a\ncomplex system is a key step in gaining a network-based understanding of the\nsystem. Because of their simplicity and low computation cost, pairwise\nstatistics are commonly used in a variety of fields. Those approaches, however,\ntypically suffer from one or more limitations such as lack of confidence\nintervals requiring reliance on surrogate data, sensitivity to binning,\nsparsity of the signals, or short duration of the records. In this paper we\ndevelop a method for assessing pairwise dependencies in point processes that\novercomes these challenges. Given two point processes $X$ and $Y$ each emitting\na given number of events $m$ and $n$ in a fixed period of time $T$, we derive\nexact analytical expressions for the expected value and standard deviation of\nthe number of pairs events $X_i,Y_j$ separated by a delay of less than $\\tau$\none should expect to observe if $X$ and $Y$ were i.i.d. uniform random\nvariables. We prove that this statistic is normally distributed in the limit of\nlarge $T$, which enables the definition of a Z-score characterising the\nlikelihood of the observed number of coincident events happening by chance. We\nnumerically confirm the analytical results and show that the property of\nnormality is robust in a wide range of experimental conditions. We then\nexperimentally demonstrate the predictive power of the method using a noisy\nversion of the common shock model. Our results show that our approach has\nexcellent behaviour even in scenarios with low event density and/or when the\nrecordings are short.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:55:53 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 19:58:04 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Messager", "Antoine", ""], ["Georgiou", "Nicos", ""], ["Berthouze", "Luc", ""]]}, {"id": "1904.04863", "submitter": "Djamel Meraghni", "authors": "Djamel Meraghni, Louiza Soltane", "title": "Extreme value theory based confidence intervals for the parameters of a\n  symmetric L\\'evy-stable distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the asymptotic normality of the extreme value theory (EVT) based\nestimators of the parameters of a symmetric L\\'evy-stable distribution, to\nconstruct confidence intervals. The accuracy of these intervals is evaluated\nthrough a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 18:47:30 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Meraghni", "Djamel", ""], ["Soltane", "Louiza", ""]]}, {"id": "1904.05095", "submitter": "Marie-Colette van Lieshout", "authors": "M.N.M. van Lieshout", "title": "Infill asymptotics and bandwidth selection for kernel estimators of\n  spatial intensity functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic mean squared error of kernel estimators of the\nintensity function of a spatial point process. We show that when $n$\nindependent copies of a point process in $\\mathbb R^d$ are superposed, the\noptimal bandwidth $h_n$ is of the order $n^{-1/(d+4)}$ under appropriate\nsmoothness conditions on the kernel and true intensity function. We apply the\nAbramson principle to define adaptive kernel estimators and show that\nasymptotically the optimal adaptive bandwidth is of the order $n^{-1/(d+8)}$\nunder appropriate smoothness conditions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 10:15:41 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["van Lieshout", "M. N. M.", ""]]}, {"id": "1904.05209", "submitter": "Dennis Kristensen", "authors": "Dennis Kristensen and Young Jun Lee", "title": "Local Polynomial Estimation of Time-Varying Parameters in Nonlinear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel asymptotic theory for local polynomial (quasi-)\nmaximum-likelihood estimators of time-varying parameters in a broad class of\nnonlinear time series models. Under weak regularity conditions, we show the\nproposed estimators are consistent and follow normal distributions in large\nsamples. Our conditions impose weaker smoothness and moment conditions on the\ndata-generating process and its likelihood compared to existing theories.\nFurthermore, the bias terms of the estimators take a simpler form. We\ndemonstrate the usefulness of our general results by applying our theory to\nlocal (quasi-)maximum-likelihood estimators of a time-varying VAR's, ARCH and\nGARCH, and Poisson autogressions. For the first three models, we are able to\nsubstantially weaken the conditions found in the existing literature. For the\nPoisson autogression, existing theories cannot be be applied while our novel\napproach allows us to analyze it.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 14:26:16 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kristensen", "Dennis", ""], ["Lee", "Young Jun", ""]]}, {"id": "1904.05232", "submitter": "Dennis Kristensen", "authors": "Dennis Kristensen and Patrick K. Mogensen and Jong Myun Moon and\n  Bertel Schjerning", "title": "Solving Dynamic Discrete Choice Models Using Smoothing and Sieve Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to combine smoothing, simulations and sieve approximations to\nsolve for either the integrated or expected value function in a general class\nof dynamic discrete choice (DDC) models. We use importance sampling to\napproximate the Bellman operators defining the two functions. The random\nBellman operators, and therefore also the corresponding solutions, are\ngenerally non-smooth which is undesirable. To circumvent this issue, we\nintroduce a smoothed version of the random Bellman operator and solve for the\ncorresponding smoothed value function using sieve methods. We show that one can\navoid using sieves by generalizing and adapting the `self-approximating' method\nof Rust (1997) to our setting. We provide an asymptotic theory for the\napproximate solutions and show that they converge with root-N-rate, where $N$\nis number of Monte Carlo draws, towards Gaussian processes. We examine their\nperformance in practice through a set of numerical experiments and find that\nboth methods perform well with the sieve method being particularly attractive\nin terms of computational speed and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:05:22 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 11:57:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Kristensen", "Dennis", ""], ["Mogensen", "Patrick K.", ""], ["Moon", "Jong Myun", ""], ["Schjerning", "Bertel", ""]]}, {"id": "1904.05289", "submitter": "Hao Chen", "authors": "Hao Chen and Yin Xia", "title": "A Normality Test for High-dimensional Data based on a Nearest Neighbor\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical methodologies for high-dimensional data assume the\npopulation is normal. Although a few multivariate normality tests have been\nproposed, to the best of our knowledge, none of them can properly control the\ntype I error when the dimension is larger than the number of observations. In\nthis work, we propose a novel nonparametric test that utilizes the nearest\nneighbor information. The proposed method guarantees the asymptotic type I\nerror control under the high-dimensional setting. Simulation studies verify the\nempirical size performance of the proposed test when the dimension grows with\nthe sample size and at the same time exhibit a superior power performance of\nthe new test compared with alternative methods. We also illustrate our approach\nthrough two popularly used data sets in high-dimensional classification and\nclustering literatures where deviation from the normality assumption may lead\nto invalid conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:47:12 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 19:59:53 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 04:15:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Hao", ""], ["Xia", "Yin", ""]]}, {"id": "1904.05338", "submitter": "Amin Jalali", "authors": "Amin Jalali, Adel Javanmard and Maryam Fazel", "title": "New Computational and Statistical Aspects of Regularized Regression with\n  Application to Rare Feature Selection and Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior knowledge on properties of a target model often come as discrete or\ncombinatorial descriptions. This work provides a unified computational\nframework for defining norms that promote such structures. More specifically,\nwe develop associated tools for optimization involving such norms given only\nthe orthogonal projection oracle onto the non-convex set of desired models. As\nan example, we study a norm, which we term the doubly-sparse norm, for\npromoting vectors with few nonzero entries taking only a few distinct values.\nWe further discuss how the K-means algorithm can serve as the underlying\nprojection oracle in this case and how it can be efficiently represented as a\nquadratically constrained quadratic program. Our motivation for the study of\nthis norm is regularized regression in the presence of rare features which\nposes a challenge to various methods within high-dimensional statistics, and in\nmachine learning in general. The proposed estimation procedure is designed to\nperform automatic feature selection and aggregation for which we develop\nstatistical bounds. The bounds are general and offer a statistical framework\nfor norm-based regularization. The bounds rely on novel geometric quantities on\nwhich we attempt to elaborate as well.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:44:25 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Jalali", "Amin", ""], ["Javanmard", "Adel", ""], ["Fazel", "Maryam", ""]]}, {"id": "1904.05526", "submitter": "Cong Ma", "authors": "Jianqing Fan, Cong Ma, Yiqiao Zhong", "title": "A Selective Overview of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:53:15 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 13:59:45 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1904.05741", "submitter": "Ilmun Kim", "authors": "Ilmun Kim", "title": "Comparing a Large Number of Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a test for the equality of multiple distributions\nbased on kernel mean embeddings. Our framework provides a flexible way to\nhandle multivariate or even high-dimensional data by virtue of kernel methods\nand allows the number of distributions to increase with the sample size. This\nis in contrast to previous studies that have been mostly restricted to\nclassical low-dimensional settings with a fixed number of distributions. By\nbuilding on Cramer-type moderate deviation for degenerate two-sample\nV-statistics, we derive the limiting null distribution of the test statistic\nand show that it converges to a Gumbel distribution. The limiting distribution,\nhowever, depends on an infinite number of nuisance parameters, which makes it\ninfeasible for use in practice. To address this issue, the proposed test is\nimplemented via the permutation procedure and is shown to be minimax rate\noptimal against sparse alternatives. During our analysis, an exponential\nconcentration inequality for the permuted test statistic is developed which may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:54:13 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 16:12:59 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kim", "Ilmun", ""]]}, {"id": "1904.05850", "submitter": "Alexander L Young", "authors": "Alexander L Young and David B Dunson", "title": "Consistent Entropy Estimation for Stationary Time Series", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy estimation, due in part to its connection with mutual information,\nhas seen considerable use in the study of time series data including causality\ndetection and information flow. In many cases, the entropy is estimated using\n$k$-nearest neighbor (Kozachenko-Leonenko) based methods. However, analytic\nresults on this estimator are limited to independent data. In the article, we\nshow rigorous bounds on the rate of decay of the bias in the number of samples,\n$N$, assuming they are drawn from a stationary process which satisfies a\nsuitable mixing condition. Numerical examples are presented which demonstrate\nthe efficiency of the estimator when applied to a Markov process with\nstationary Gaussian density. These results support the asymptotic rates derived\nin the theoretical work.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:08:20 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 16:00:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Young", "Alexander L", ""], ["Dunson", "David B", ""]]}, {"id": "1904.06288", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan and Philip Thompson", "title": "Outlier-robust estimation of a sparse linear model using\n  $\\ell_1$-penalized Huber's $M$-estimator", "comments": "This is a follow up paper of arXiv:1805.08020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a $p$-dimensional $s$-sparse vector in a\nlinear model with Gaussian design and additive noise. In the case where the\nlabels are contaminated by at most $o$ adversarial outliers, we prove that the\n$\\ell_1$-penalized Huber's $M$-estimator based on $n$ samples attains the\noptimal rate of convergence $(s/n)^{1/2} + (o/n)$, up to a logarithmic factor.\nFor more general design matrices, our results highlight the importance of two\nproperties: the transfer principle and the incoherence property. These\nproperties with suitable constants are shown to yield the optimal rates, up to\nlog-factors, of robust estimation with adversarial contamination.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:52:04 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 14:54:33 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 15:52:50 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Thompson", "Philip", ""]]}, {"id": "1904.06412", "submitter": "Michael Levine", "authors": "Michael Levine, Donald Richards, and Jianxi Su", "title": "Independence Properties of the Truncated Multivariate Elliptical\n  Distributions", "comments": "15 pages; updated abstract, additional references and an additional\n  Section #5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated multivariate distributions arise extensively in econometric\nmodelling when non-negative random variables are intrinsic to the\ndata-generation process. More broadly, truncated multivariate distributions\nhave appeared in censored and truncated regression models, simultaneous\nequations modelling, multivariate regression, and applications going back to\nthe now-classic papers of Amemiya (1974) and Heckman (1976). In some\napplications of truncated multivariate distributions, there arises the problem\nof characterizing the distribution through correlation and independence\nproperties of sub-vectors. In this paper, we characterize the truncated\nmultivariate normal random vectors for which two complementary sub-vectors are\nmutually independent. Further, we characterize the multivariate truncated\nelliptical distributions, proving that if two complementary sub-vectors are\nmutually independent then the distribution of the joint vector is truncated\nmultivariate normal, as is the distribution of each sub-vector. As an\napplication, we apply the independence criterion to test the hypothesis of\nindependence of the entrance examination scores and subsequent course averages\nachieved by a sample of university students; to do so, we verify the regularity\nconditions underpinning a classical theorem of Wilks on the asymptotic null\ndistribution of the likelihood ratio test statistic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 20:43:56 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 17:51:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Levine", "Michael", ""], ["Richards", "Donald", ""], ["Su", "Jianxi", ""]]}, {"id": "1904.06632", "submitter": "Mansoor Sheikh", "authors": "M Sheikh, A.C.C. Coolen", "title": "Analysis of overfitting in the regularized Cox model", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8121/ab375c", "report-no": null, "categories": "stat.ME cond-mat.dis-nn cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox proportional hazards model is ubiquitous in the analysis of\ntime-to-event data. However, when the data dimension p is comparable to the\nsample size $N$, maximum likelihood estimates for its regression parameters are\nknown to be biased or break down entirely due to overfitting. This prompted the\nintroduction of the so-called regularized Cox model. In this paper we use the\nreplica method from statistical physics to investigate the relationship between\nthe true and inferred regression parameters in regularized multivariate Cox\nregression with L2 regularization, in the regime where both p and N are large\nbut with p/N ~ O(1). We thereby generalize a recent study from maximum\nlikelihood to maximum a posteriori inference. We also establish a relationship\nbetween the optimal regularization parameter and p/N, allowing for\nstraightforward overfitting corrections in time-to-event analysis.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 05:48:02 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:15:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sheikh", "M", ""], ["Coolen", "A. C. C.", ""]]}, {"id": "1904.06675", "submitter": "Yousri Slaoui", "authors": "Yousri SLAOUI and Asma JMAEI", "title": "Recursive density estimators based on Robbins-Monro's scheme and using\n  Bernstein polynomials", "comments": null, "journal-ref": "Statistics and Its Interface, 2019", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the alleviation of the boundary problem when the\nprobability density function has bounded support. We apply Robbins-Monro's\nalgorithm and Bernstein polynomials to construct a recursive density estimator.\nWe study the asymptotic properties of the proposed recursive estimator. We then\ncompared our proposed recursive estimator with many others estimators. Finally,\nwe confirm our theoretical result through a simulation study and then using two\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 10:52:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["SLAOUI", "Yousri", ""], ["JMAEI", "Asma", ""]]}, {"id": "1904.06721", "submitter": "Martin Wendler", "authors": "Olimjon Sh. Sharipov, Martin Wendler", "title": "Bootstrapping Covariance Operators of Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing hypothesis on the covariance operator of functional time series,\nwe suggest to use the full functional information and to avoid dimension\nreduction techniques. The limit distribution follows from the central limit\ntheorem of the weak convergence of the partial sum process in general Hilbert\nspace applied to the product space. In order to obtain critical values for\ntests, we generalize bootstrap results from the independent to the dependent\ncase. This results can be applied to covariance operators, autocovariance\noperators and cross covariance operators. We discuss one sample and changepoint\ntests and give some simulation results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 16:39:16 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 18:30:31 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 09:32:30 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Sharipov", "Olimjon Sh.", ""], ["Wendler", "Martin", ""]]}, {"id": "1904.06826", "submitter": "Yo Sheena", "authors": "Yo Sheena", "title": "Asymptotic efficiency of M.L.E. using prior survey in multinomial\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating information from a prior survey is generally supposed to\ndecrease the estimation risk of the present survey. This paper aims to show how\nthe risk changes by incorporating the information of a prior survey through\nwatching the first and the second-order terms of the asymptotic expansion of\nthe risk. We recognize that the prior information is of some help for risk\nreduction when we can acquire samples of a sufficient size for both surveys.\nInterestingly, when the sample size of the present survey is small, the use of\nthe prior survey can increase the risk. In other words, blending information\nfrom both surveys can have a negative effect on the risk. Based on these\nobservations, we give some suggestions on whether or not to use the results of\nthe prior survey and the sample size to use in the surveys for a reliable\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 03:20:18 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Sheena", "Yo", ""]]}, {"id": "1904.07089", "submitter": "Mika Meitz", "authors": "Mika Meitz and Pentti Saikkonen", "title": "Subgeometrically ergodic autoregressions", "comments": "Feb 2020: Major revision (most significant changes in Section 5)", "journal-ref": null, "doi": "10.1017/S0266466620000419", "report-no": null, "categories": "econ.EM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss how the notion of subgeometric ergodicity in Markov\nchain theory can be exploited to study stationarity and ergodicity of nonlinear\ntime series models. Subgeometric ergodicity means that the transition\nprobability measures converge to the stationary measure at a rate slower than\ngeometric. Specifically, we consider suitably defined higher-order nonlinear\nautoregressions that behave similarly to a unit root process for large values\nof the observed series but we place almost no restrictions on their dynamics\nfor moderate values of the observed series. Results on the subgeometric\nergodicity of nonlinear autoregressions have previously appeared only in the\nfirst-order case. We provide an extension to the higher-order case and show\nthat the autoregressions we consider are, under appropriate conditions,\nsubgeometrically ergodic. As useful implications we also obtain stationarity\nand $\\beta$-mixing with subgeometrically decaying mixing coefficients.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:50:46 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 10:55:01 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 14:36:25 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Meitz", "Mika", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1904.07103", "submitter": "Mika Meitz", "authors": "Mika Meitz and Pentti Saikkonen", "title": "Subgeometric ergodicity and $\\beta$-mixing", "comments": "v2 updated reference to Meitz and Saikkonen (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that stationary geometrically ergodic Markov chains are\n$\\beta$-mixing (absolutely regular) with geometrically decaying mixing\ncoefficients. Furthermore, for initial distributions other than the stationary\none, geometric ergodicity implies $\\beta$-mixing under suitable moment\nassumptions. In this note we show that similar results hold also for\nsubgeometrically ergodic Markov chains. In particular, for both stationary and\nother initial distributions, subgeometric ergodicity implies $\\beta$-mixing\nwith subgeometrically decaying mixing coefficients. Although this result is\nsimple it should prove very useful in obtaining rates of mixing in situations\nwhere geometric ergodicity can not be established. To illustrate our results we\nderive new subgeometric ergodicity and $\\beta$-mixing results for the\nself-exciting threshold autoregressive model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:11:01 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 11:01:08 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Meitz", "Mika", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1904.07111", "submitter": "Yannick Guyonvarch", "authors": "Alexis Derumigny, Lucas Girard and Yannick Guyonvarch", "title": "On the construction of confidence intervals for ratios of expectations", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econometrics, many parameters of interest can be written as ratios of\nexpectations. The main approach to construct confidence intervals for such\nparameters is the delta method. However, this asymptotic procedure yields\nintervals that may not be relevant for small sample sizes or, more generally,\nin a sequence-of-model framework that allows the expectation in the denominator\nto decrease to $0$ with the sample size. In this setting, we prove a\ngeneralization of the delta method for ratios of expectations and the\nconsistency of the nonparametric percentile bootstrap. We also investigate\nfinite-sample inference and show a partial impossibility result: nonasymptotic\nuniform confidence intervals can be built for ratios of expectations but not at\nevery level. Based on this, we propose an easy-to-compute index to appraise the\nreliability of the intervals based on the delta method. Simulations and an\napplication illustrate our results and the practical usefulness of our rule of\nthumb.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:26:23 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Derumigny", "Alexis", ""], ["Girard", "Lucas", ""], ["Guyonvarch", "Yannick", ""]]}, {"id": "1904.07150", "submitter": "Kolyan Ray", "authors": "Kolyan Ray and Botond Szabo", "title": "Variational Bayes for high-dimensional linear regression with sparse\n  priors", "comments": "42 pages. To appear in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2020.1847121", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mean-field spike and slab variational Bayes (VB) approximation to\nBayesian model selection priors in sparse high-dimensional linear regression.\nUnder compatibility conditions on the design matrix, oracle inequalities are\nderived for the mean-field VB approximation, implying that it converges to the\nsparse truth at the optimal rate and gives optimal prediction of the response\nvector. The empirical performance of our algorithm is studied, showing that it\nworks comparably well as other state-of-the-art Bayesian variable selection\nmethods. We also numerically demonstrate that the widely used coordinate-ascent\nvariational inference (CAVI) algorithm can be highly sensitive to the parameter\nupdating order, leading to potentially poor performance. To mitigate this, we\npropose a novel prioritized updating scheme that uses a data-driven updating\norder and performs better in simulations. The variational algorithm is\nimplemented in the R package 'sparsevb'.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:58:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:33:39 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 15:06:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""]]}, {"id": "1904.07174", "submitter": "Ilias Zadik", "authors": "David Gamarnik and Ilias Zadik", "title": "The Landscape of the Planted Clique Problem: Dense subgraphs and the\n  Overlap Gap Property", "comments": "70 pages, 3 Figures. Added Figure 1 (phase diagram), and a new result\n  proving that the OGP implies the failure of an MCMC family to recover the\n  planted clique", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG math.OC math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the computational-statistical gap of the planted\nclique problem, where a clique of size $k$ is planted in an Erdos Renyi graph\n$G(n,\\frac{1}{2})$ resulting in a graph $G\\left(n,\\frac{1}{2},k\\right)$. The\ngoal is to recover the planted clique vertices by observing\n$G\\left(n,\\frac{1}{2},k\\right)$ . It is known that the clique can be recovered\nas long as $k \\geq \\left(2+\\epsilon\\right)\\log n $ for any $\\epsilon>0$, but no\npolynomial-time algorithm is known for this task unless $k=\\Omega\\left(\\sqrt{n}\n\\right)$. Following a statistical-physics inspired point of view as an attempt\nto understand this computational-statistical gap, we study the landscape of the\n\"sufficiently dense\" subgraphs of $G$ and their overlap with the planted\nclique.\n  Using the first moment method, we study the densest subgraph problems for\nsubgraphs with fixed, but arbitrary, overlap size with the planted clique, and\nprovide evidence of a phase transition for the presence of Overlap Gap Property\n(OGP) at $k=\\Theta\\left(\\sqrt{n}\\right)$. OGP is a concept introduced\noriginally in spin glass theory and known to suggest algorithmic hardness when\nit appears. We establish the presence of OGP when $k$ is a small positive power\nof $n$ by using a conditional second moment method. As our main technical tool,\nwe establish the first, to the best of our knowledge, concentration results for\nthe $K$-densest subgraph problem for the Erdos-Renyi model\n$G\\left(n,\\frac{1}{2}\\right)$ when $K=n^{0.5-\\epsilon}$ for arbitrary\n$\\epsilon>0$. Finally, to study the OGP we employ a certain form of\noverparametrization, which is conceptually aligned with a large body of recent\nwork in learning theory and optimization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 16:44:00 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 10:56:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gamarnik", "David", ""], ["Zadik", "Ilias", ""]]}, {"id": "1904.07227", "submitter": "Jack Noonan", "authors": "Jack Noonan and Anatoly Zhigljavsky", "title": "First passage time for Slepian process with linear barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend results of L.A. Shepp by finding explicit formulas\nfor the first passage probability $F_{a,b}(T\\, |\\, x)={\\rm Pr}(S(t)<a+bt \\text{\nfor all } t\\in[0,T]\\,\\, | \\,\\,S(0)=x)$, for all $T>0$, where $S(t)$ is a\nGaussian process with mean 0 and covariance $\\mathbb{E}\nS(t)S(t')=\\max\\{0,1-|t-t'|\\}\\,.$ We then extend the results to the case of\npiecewise-linear barriers and outline applications to change-point detection\nproblems. Previously, explicit formulas for $F_{a,b}(T\\, |\\, x)$ were known\nonly for the cases $b=0$ (constant barrier) or $T\\leq 1$ (short interval).\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 13:35:23 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1904.07296", "submitter": "Davide Giraudo", "authors": "Davide Giraudo", "title": "Limit theorems for U-statistics of Bernoulli data", "comments": "30 pages", "journal-ref": "ALEA, Lat. Am. J. Probab. Math. Stat. 18, 793-828 (2021)", "doi": "10.30757/ALEA.v18-29", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider U-statistics whose data is a strictly stationary\nsequence which can be expressed as a functional of an i.i.d. one. We establish\na strong law of large numbers, a bounded law of the iterated logarithms and a\ncentral limit theorem under a dependence condition. The main ingredients for\nthe proof are an approximation by U-statistics whose data is a functional of\n$\\ell$ i.i.d. random variables and an analogue of the Hoeffding's decomposition\nfor U-statistics of this type.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:26:02 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 08:36:49 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Giraudo", "Davide", ""]]}, {"id": "1904.07416", "submitter": "Kaijie Xue", "authors": "Kaijie Xue, Fang Yao", "title": "Distribution and correlation free two-sample test of high-dimensional\n  means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample test for high-dimensional means that requires neither\ndistributional nor correlational assumptions, besides some weak conditions on\nthe moments and tail properties of the elements in the random vectors. This\ntwo-sample test based on a nontrivial extension of the one-sample central limit\ntheorem (Chernozhukov et al., 2017) provides a practically useful procedure\nwith rigorous theoretical guarantees on its size and power assessment. In\nparticular, the proposed test is easy to compute and does not require the\nindependently and identically distributed assumption, which is allowed to have\ndifferent distributions and arbitrary correlation structures. Further desired\nfeatures include weaker moments and tail conditions than existing methods,\nallowance for highly unequal sample sizes, consistent power behavior under\nfairly general alternative, data dimension allowed to be exponentially high\nunder the umbrella of such general conditions. Simulated and real data examples\nare used to demonstrate the favorable numerical performance over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:44:43 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Xue", "Kaijie", ""], ["Yao", "Fang", ""]]}, {"id": "1904.07477", "submitter": "Feng Li", "authors": "Lu Lin and Feng Li", "title": "A Global Bias-Correction DC Method for Biased Estimation under Memory\n  Constraint", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a global bias-correction divide-and-conquer (GBC-DC)\nrule for biased estimation under the case of memory constraint. In order to\nintroduce the new estimation, a closed representation of the local estimators\nobtained by the data in each batch is adopted, aiming to formulate a pro forma\nlinear regression between the local estimators and the true parameter of\ninterest. Least square method is then used within this framework to composite a\nglobal estimator of the parameter. Thus, the main advantage over the classical\nDC method is that the new GBC-DC method can absorb the information hidden in\nthe statistical structure and the variables in each batch of data.\nConsequently, the resulting global estimator is strictly unbiased even if the\nlocal estimator has a non-negligible bias. Moreover, the global estimator is\nconsistent, and even can achieve root-$n$ consistency, without the constraint\non the number of batches. Another attractive feature of the new method is\ncomputationally simple and efficient, without use of any iterative algorithm\nand local bias-correction. Specifically, the proposed GBC-DC method applies to\nvarious biased estimations such as shrinkage-type estimation and nonparametric\nregression estimation. Detailed simulation studies demonstrate that the\nproposed GBC-DC approach is significantly bias-corrected, and the behavior is\ncomparable with the full data estimation and is much better than the\ncompetitors.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:57:36 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 06:13:10 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lin", "Lu", ""], ["Li", "Feng", ""]]}, {"id": "1904.07763", "submitter": "Lara Kassab", "authors": "Lara Kassab", "title": "Multidimensional Scaling: Infinite Metric Measure Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling (MDS) is a popular technique for mapping a finite\nmetric space into a low-dimensional Euclidean space in a way that best\npreserves pairwise distances. We study a notion of MDS on infinite metric\nmeasure spaces, along with its optimality properties and goodness of fit. This\nallows us to study the MDS embeddings of the geodesic circle $S^1$ into\n$\\mathbb{R}^m$ for all $m$, and to ask questions about the MDS embeddings of\nthe geodesic $n$-spheres $S^n$ into $\\mathbb{R}^m$. Furthermore, we address\nquestions on convergence of MDS. For instance, if a sequence of metric measure\nspaces converges to a fixed metric measure space $X$, then in what sense do the\nMDS embeddings of these spaces converge to the MDS embedding of $X$?\nConvergence is understood when each metric space in the sequence has the same\nfinite number of points, or when each metric space has a finite number of\npoints tending to infinity. We are also interested in notions of convergence\nwhen each metric space in the sequence has an arbitrary (possibly infinite)\nnumber of points.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:31:37 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kassab", "Lara", ""]]}, {"id": "1904.07770", "submitter": "Pavlina Jordanova", "authors": "Pavlina K. Jordanova, Milan Stehl\\'ik", "title": "Logarithm of ratios of two order statistics and regularly varying tails", "comments": "Eleventh Conference of the Euro-American Consortium for Promoting the\n  Application of Mathematics in Technical and Natural Sciences, Albena,\n  Bulgaria, June 20-25, 2019", "journal-ref": null, "doi": "10.1063/1.5130791", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we suppose that the observed random variable has cumulative distribution\nfunction $F$ with regularly varying tail, i.e. $1-F \\in RV_{-\\alpha}$, $\\alpha\n> 0$. Using the results about exponential order statistics we investigate\nlogarithms of ratios of two order statistics of a sample of independent\nobservations on Pareto distributed random variable with parameter $\\alpha$.\nShort explicit formulae for its mean and variance are obtained. Then we\ntransform this function in such a way that to obtain unbiased, asymptotically\nefficient, and asymptotically normal estimator for $\\alpha$. Finally we\nsimulate Pareto samples and show that in the considered cases the proposed\nestimator outperforms the well known Hill, t-Hill, Pickands and\nDeckers-Einmahl-de Haan estimators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:40:03 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Jordanova", "Pavlina K.", ""], ["Stehl\u00edk", "Milan", ""]]}, {"id": "1904.07908", "submitter": "Antoine Godichon-Baggioni", "authors": "Bernard Bercu, Antoine Godichon-Baggioni and Bruno Portier", "title": "An efficient stochastic Newton algorithm for parameter estimation in\n  logistic regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is a well-known statistical model which is commonly used\nin the situation where the output is a binary random variable. It has a wide\nrange of applications including machine learning, public health, social\nsciences, ecology and econometry. In order to estimate the unknown parameters\nof logistic regression with data streams arriving sequentially and at high\nspeed, we focus our attention on a recursive stochastic algorithm. More\nprecisely, we investigate the asymptotic behavior of a new stochastic Newton\nalgorithm. It enables to easily update the estimates when the data arrive\nsequentially and to have research steps in all directions. We establish the\nalmost sure convergence of our stochastic Newton algorithm as well as its\nasymptotic normality. All our theoretical results are illustrated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:19:32 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Bercu", "Bernard", ""], ["Godichon-Baggioni", "Antoine", ""], ["Portier", "Bruno", ""]]}, {"id": "1904.08232", "submitter": "Sarah Lemler", "authors": "Charlotte Dion (SU, LPSM UMR 8001), Sarah Lemler (MICS)", "title": "Nonparametric drift estimation for diffusions with jumps driven by a\n  Hawkes process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a 1-dimensional diffusion process X with jumps. The particularity\nof this model relies in the jumps which are driven by a multidimensional Hawkes\nprocess denoted N. This article is dedicated to the study of a nonparametric\nestimator of the drift coefficient of this original process. We construct\nestimators based on discrete observations of the process X in a high frequency\nframework with a large horizon time and on the observations of the process N.\nThe proposed nonparametric estimator is built from a least squares contrast\nprocedure on subspace spanned by trigonometric basis vectors. We obtain\nadaptive results that are comparable with the one obtained in the nonparametric\nregression context. We finally conduct a simulation study in which we first\nfocus on the implementation of the process and then on showing the good\nbehavior of the estimator.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:44:39 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 11:03:52 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Dion", "Charlotte", "", "SU, LPSM UMR 8001"], ["Lemler", "Sarah", "", "MICS"]]}, {"id": "1904.08247", "submitter": "Alice Le Brigant", "authors": "Alice Le Brigant (ENAC), St\\'ephane Puechmorel (ENAC)", "title": "The Fisher-Rao geometry of beta distributions applied to the study of\n  canonical moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Fisher-Rao geometry on the parameter space of beta\ndistributions. We derive the geodesic equations and the sectional curvature,\nand prove that it is negative. This leads to uniqueness for the Riemannian\ncentroid in that space. We use this Riemannian structure to study canonical\nmoments, an intrinsic representation of the moments of a probability\ndistribution. Drawing on the fact that a uniform distribution in the regular\nmoment space corresponds to a product of beta distributions in the canonical\nmoment space, we propose a mapping from the space of canonical moments to the\nproduct beta manifold, allowing us to use the Fisher-Rao geometry of beta\ndistributions to compare and analyze canonical moments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:56:46 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Brigant", "Alice Le", "", "ENAC"], ["Puechmorel", "St\u00e9phane", "", "ENAC"]]}, {"id": "1904.08276", "submitter": "Thiago do R\\^ego Sousa", "authors": "Richard A. Davis, Thiago do R\\^ego Sousa and Claudia Kl\\\"uppelberg", "title": "Indirect Inference for Time Series Using the Empirical Characteristic\n  Function and Control Variates", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We estimate the parameter of a stationary time series process by minimizing\nthe integrated weighted mean squared error between the empirical and simulated\ncharacteristic function, when the true characteristic functions cannot be\nexplicitly computed. Motivated by Indirect Inference, we use a Monte Carlo\napproximation of the characteristic function based on iid simulated blocks. As\na classical variance reduction technique, we propose the use of control\nvariates for reducing the variance of this Monte Carlo approximation. These two\napproximations yield two new estimators that are applicable to a large class of\ntime series processes. We show consistency and asymptotic normality of the\nparameter estimators under strong mixing, moment conditions, and smoothness of\nthe simulated blocks with respect to its parameter. In a simulation study we\nshow the good performance of these new simulation based estimators, and the\nsuperiority of the control variates based estimator for Poisson driven time\nseries of counts.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 13:59:14 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 12:20:09 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 14:27:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Davis", "Richard A.", ""], ["Sousa", "Thiago do R\u00eago", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "1904.08532", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson and Grigoris Paouris", "title": "Stable recovery and the coordinate small-ball behaviour of random\n  vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery procedures in various application in Data Science are based on\n\\emph{stable point separation}. In its simplest form, stable point separation\nimplies that if $f$ is \"far away\" from $0$, and one is given a random sample\n$(f(Z_i))_{i=1}^m$ where a proportional number of the sample points may be\ncorrupted by noise, that information is still enough to exhibit that $f$ is far\nfrom $0$.\n  Stable point separation is well understood in the context of iid sampling,\nand to explore it for general sampling methods we introduce a new notion---the\n\\emph{coordinate small-ball} of a random vector $X$. Roughly put, this feature\ncaptures the number of \"relatively large coordinates\" of\n$(|<TX,u_i>|)_{i=1}^m$, where $T:\\mathbb{R}^n \\to \\mathbb{R}^m$ is an arbitrary\nlinear operator and $(u_i)_{i=1}^m$ is any fixed orthonormal basis of\n$\\mathbb{R}^m$.\n  We show that under the bare-minimum assumptions on $X$, and with high\nprobability, many of the values $|<TX,u_i>|$ are at least of the order\n$\\|T\\|_{S_2}/\\sqrt{m}$. As a result, the \"coordinate structure\" of $TX$\nexhibits the typical Euclidean norm of $TX$ and does so in a stable way.\n  One outcome of our analysis is that random sub-sampled convolutions satisfy\nstable point separation under minimal assumptions on the generating random\nvector---a fact that was known previously only in a highly restrictive setup,\nnamely, for random vectors with iid subgaussian coordinates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:17:48 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Mendelson", "Shahar", ""], ["Paouris", "Grigoris", ""]]}, {"id": "1904.08551", "submitter": "Demian Pouzo", "authors": "Ignacio Esponda and Demian Pouzo and Yuichi Yamamoto", "title": "Asymptotic Behavior of Bayesian Learners with Misspecified Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an agent who represents uncertainty about the environment via a\npossibly misspecified model. Each period, the agent takes an action, observes a\nconsequence, and uses Bayes' rule to update her belief about the environment.\nThis framework has become increasingly popular in economics to study behavior\ndriven by incorrect or biased beliefs. Current literature has characterized\nasymptotic behavior under fairly specific assumptions. By first showing that\nthe key element to predict the agent's behavior is the frequency of her past\nactions, we are able to characterize asymptotic behavior in general settings in\nterms of the solutions of a generalization of a differential equation that\ndescribes the evolution of the frequency of actions. We then present a series\nof implications that can be readily applied to economic applications, thus\nproviding off-the-shelf tools that can be used to characterize behavior under\nmisspecified learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:58:32 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 00:32:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Esponda", "Ignacio", ""], ["Pouzo", "Demian", ""], ["Yamamoto", "Yuichi", ""]]}, {"id": "1904.08575", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu, Peter Davies, Aldo Glielmo, and Hemant Tyagi", "title": "SPONGE: A generalized eigenproblem for clustering signed networks", "comments": "33 pages, 18 figures", "journal-ref": "AISTATS 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a principled and theoretically sound spectral method for $k$-way\nclustering in signed graphs, where the affinity measure between nodes takes\neither positive or negative values. Our approach is motivated by social balance\ntheory, where the task of clustering aims to decompose the network into\ndisjoint groups, such that individuals within the same group are connected by\nas many positive edges as possible, while individuals from different groups are\nconnected by as many negative edges as possible. Our algorithm relies on a\ngeneralized eigenproblem formulation inspired by recent work on constrained\nclustering. We provide theoretical guarantees for our approach in the setting\nof a signed stochastic block model, by leveraging tools from matrix\nperturbation theory and random matrix theory. An extensive set of numerical\nexperiments on both synthetic and real data shows that our approach compares\nfavorably with state-of-the-art methods for signed clustering, especially for\nlarge number of clusters and sparse measurement graphs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:55:22 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 12:22:00 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Davies", "Peter", ""], ["Glielmo", "Aldo", ""], ["Tyagi", "Hemant", ""]]}, {"id": "1904.08580", "submitter": "Karthik Rajkumar", "authors": "Karthik Rajkumar", "title": "Ridge regularization for Mean Squared Error Reduction in Regression with\n  Weak Instruments", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I show that classic two-stage least squares (2SLS) estimates\nare highly unstable with weak instruments. I propose a ridge estimator (ridge\nIV) and show that it is asymptotically normal even with weak instruments,\nwhereas 2SLS is severely distorted and un-bounded. I motivate the ridge IV\nestimator as a convex optimization problem with a GMM objective function and an\nL2 penalty. I show that ridge IV leads to sizable mean squared error reductions\ntheoretically and validate these results in a simulation study inspired by data\ndesigns of papers published in the American Economic Review.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 03:25:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Rajkumar", "Karthik", ""]]}, {"id": "1904.08625", "submitter": "Kristi Kuljus", "authors": "Kristi Kuljus and Bo Ranneby", "title": "Asymptotic normality of generalized maximum spacing estimators for\n  multivariate observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the maximum spacing method is considered for multivariate\nobservations. Nearest neighbour balls are used as a multidimensional analogue\nto univariate spacings. A class of information-type measures is used to\ngeneralize the concept of maximum spacing estimators. Asymptotic normality of\nthese generalized maximum spacing estimators is proved when the assigned model\nclass is correct, that is the true density is a member of the model class.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:51:32 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Kuljus", "Kristi", ""], ["Ranneby", "Bo", ""]]}, {"id": "1904.08730", "submitter": "Surojit Biswas", "authors": "Surojit Biswas and Nitin Gupta", "title": "Some ordering properties of highest and lowest order statistics with\n  exponentiated Gumble type-II distributed components", "comments": "16 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we have studied the stochastic comparisons of the highest and\nlowest order statistics of exponentiated Gumble type-II distribution with three\nparameters. We have compared both the statistics by using three different\nstochastic ordering. First, we consider a system with different scale and outer\nshape parameters and then we study the usual stochastic ordering of the lowest\nand highest order statistics in the sense of multivariate chain majorization.\nIn addition, we construct two examples to support our results. Second, by using\nthe vector majorization technique, we study the usual stochastic ordering, the\nreversed failure rate ordering and the likelihood ratio ordering with respect\nto different outer shape parameters, next, by varying the inner shape\nparameter, we discuss the usual stochastic order of the lowest order statistics\nand we have shown that the highest order statistics are not comparable in the\nusual stochastic ordering by an example.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:32:45 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Biswas", "Surojit", ""], ["Gupta", "Nitin", ""]]}, {"id": "1904.09017", "submitter": "Gr\u00e9goire Sergeant-Perthuis", "authors": "Gr\\'egoire Sergeant-Perthuis", "title": "Intersection property and interaction decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decomposition into interaction subspaces is a hierarchical decomposition\nof the spaces of cylindrical functions of a finite product space, also called\nfactor spaces. It is an important construction in graphical models and a\nstandard way to prove the Hammersley-Clifford theorem that relates Markov\nfields to Gibbs fields and plays a central role in Kellerer's result for the\nlinearized marginal problem. We define an intersection of sum property, or\nsimply intersection property, and show that it characterizes collections of\nvector subspaces over a poset that can be hierarchically decomposed into direct\nsums, giving therefore a general setting for such construction to hold. We will\ncall this generalization the interaction decomposition. The intersection\nproperty is the Bayesian intersection property when specified to factor spaces\nwhich, under this new perspective on the interaction decomposition, appears to\nbe a structure property. An application is the extension of the decomposition\ninto interaction subspaces for any product of any set.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 21:22:25 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 15:48:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sergeant-Perthuis", "Gr\u00e9goire", ""]]}, {"id": "1904.09027", "submitter": "Bai Jiang", "authors": "Jianqing Fan and Yongyi Guo and Bai Jiang", "title": "Adaptive Huber Regression on Markov-dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear regression has been intensively studied in the\ncommunity of statistics in the last two decades. For the convenience of\ntheoretical analyses, classical methods usually assume independent observations\nand sub-Gaussian-tailed errors. However, neither of them hold in many real\nhigh-dimensional time-series data. Recently [Sun, Zhou, Fan, 2019, J. Amer.\nStat. Assoc., in press] proposed Adaptive Huber Regression (AHR) to address the\nissue of heavy-tailed errors. They discover that the robustification parameter\nof the Huber loss should adapt to the sample size, the dimensionality, and the\nmoments of the heavy-tailed errors. We progress in a vertical direction and\njustify AHR on dependent observations. Specifically, we consider an important\ndependence structure -- Markov dependence. Our results show that the Markov\ndependence impacts on the adaption of the robustification parameter and the\nestimation of regression coefficients in the way that the sample size should be\ndiscounted by a factor depending on the spectral gap of the underlying Markov\nchain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:31:22 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 22:31:47 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Yongyi", ""], ["Jiang", "Bai", ""]]}, {"id": "1904.09192", "submitter": "Eric Gautier", "authors": "Jad Beyhum (TSE, UT1), Eric Gautier (TSE, UT1)", "title": "Square-root nuclear norm penalized estimator for panel data models with\n  approximately low-rank unobserved heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a nuclear norm penalized estimator for panel data models\nwith interactive effects. The low-rank interactive effects can be an\napproximate model and the rank of the best approximation unknown and grow with\nsample size. The estimator is solution of a well-structured convex optimization\nproblem and can be solved in polynomial-time. We derive rates of convergence,\nstudy the low-rank properties of the estimator, estimation of the rank and of\nannihilator matrices when the number of time periods grows with the sample\nsize. Two-stage estimators can be asymptotically normal. None of the procedures\nrequire knowledge of the variance of the errors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 13:21:19 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 14:06:30 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Beyhum", "Jad", "", "TSE, UT1"], ["Gautier", "Eric", "", "TSE, UT1"]]}, {"id": "1904.09204", "submitter": "Hau-tieng Wu", "authors": "Matan Gavish, Ronen Talmon, Pei-Chun Su, Hau-Tieng Wu", "title": "Optimal Recovery of Mahalanobis Distance in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of Mahalanobis distance (MD) estimation\nfrom high-dimensional noisy data. By relying on recent transformative results\nin covariance matrix estimation, we demonstrate the sensitivity of MD to\nmeasurement noise, determining the exact asymptotic signal-to-noise ratio at\nwhich MD fails, and quantifying its performance otherwise. In addition, for an\nappropriate loss function, we propose an asymptotically optimal shrinker, which\nis shown to be beneficial over the classical implementation of the MD, both\nanalytically and in simulations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:20:42 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Gavish", "Matan", ""], ["Talmon", "Ronen", ""], ["Su", "Pei-Chun", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1904.09232", "submitter": "Osama Idais", "authors": "Osama Idais and Rainer Schwabe", "title": "Analytic solutions for locally optimal designs for gamma models having\n  linear predictor without intercept", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gamma model is a generalized linear model for gamma-distributed outcomes.\nThe model is widely applied in psychology, ecology or medicine. In this paper\nwe focus on gamma models having a linear predictor without intercept. For a\nspecific scenario sets of locally D- and A-optimal designs are to be developed.\nRecently, Gaffke et al. (2018) established a complete class and an essentially\ncomplete class of designs for gamma models to obtain locally D-optimal designs.\nHowever to extend this approach to gamma model without an intercept term is\ncomplicated. To solve that further techniques have to be developed in the\ncurrent work. Further, by a suitable transformation between gamma models with\nand without intercept optimality results may be transferred from one model to\nthe other. Additionally by means of The General Equivalence Theorem optimality\ncan be characterized for multiple regression by a system of polynomial\ninequalities which can be solved analytically or by computer algebra. By this\nnecessary and sufficient conditions on the parameter values can be obtained for\nthe local D-optimality of particular designs. The robustness of the derived\ndesigns with respect to misspecifications of the initial parameter values is\nexamined by means of their local D-efficiencies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 22:06:46 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Idais", "Osama", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1904.09236", "submitter": "Dandan Jiang", "authors": "Dandan Jiang, Zhiqiang Hou and Zhidong Bai", "title": "Generalized Four Moment Theorem with an application to the CLT for the\n  spiked eigenvalues of high-dimensional general Fisher-matrices", "comments": "29pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The universality for the local spiked eigenvalues is a powerful tool to deal\nwith the problems of the asymptotic law for the bulks of spiked eigenvalues of\nhigh-dimensional generalized Fisher matrices. In this paper, we focus on a more\ngeneralized spiked Fisher matrix, where $\\Sigma_1\\Sigma_2^{-1}$ is free of the\nrestriction of diagonal independence, and both of the spiked eigenvalues and\nthe population 4th moments are not necessary required to be bounded. By\nreducing the matching four moments constraint to a tail probability, we propose\na Generalized Four Moment Theorem (G4MT) for the bulks of spiked eigenvalues of\nhigh-dimensional generalized Fisher matrices, which shows that the limiting\ndistribution of the spiked eigenvalues of a generalized spiked Fisher matrix is\nindependent of the actual distributions of the samples provided to satisfy the\nour relaxed assumptions. Furthermore, as an illustration, we also apply the\nG4MT to the Central Limit Theorem for the spiked eigenvalues of generalized\nspiked Fisher matrix, which removes the strict condition of the diagonal block\nindependence given in Wang and Yao (2017) and extends their result to a wider\nusage without the requirements of the bounded 4th moments and the diagonal\nblock independent structure, meeting the actual cases better.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 07:05:47 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Jiang", "Dandan", ""], ["Hou", "Zhiqiang", ""], ["Bai", "Zhidong", ""]]}, {"id": "1904.09318", "submitter": "Emil Aas Stoltenberg", "authors": "Emil Aas Stoltenberg and Nils Lid Hjort", "title": "Multivariate Estimation of Poisson Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the multivariate estimation of a vector of Poisson\nmeans. A novel loss function that penalises bad estimates of each of the\nparameters and the sum (or equivalently the mean) of the parameters is\nintroduced. Under this loss function, a class of minimax estimators that\nuniformly dominate the maximum likelihood estimator is derived. Crucially,\nthese methods have the property that for estimating a given component\nparameter, the full data vector is utilised. Estimators in this class can be\nfine-tuned to limit shrinkage away from the maximum likelihood estimator,\nthereby avoiding implausible estimates of the sum of the parameters. Further\nlight is shed on this new class of estimators by showing that it can be derived\nby Bayesian and empirical Bayesian methods. In particular, we exhibit a\ngeneralisation of the Clevenson-Zidek estimator, and prove its admissibility.\nMoreover, a class of prior distributions for which the Bayes estimators\nuniformly dominate the maximum likelihood estimator under the new loss function\nis derived. A section is included involving weighted loss functions, notably\nalso leading to a procedure improving uniformly on the maximum likelihood\nmethod in an infinite-dimensional setup. Importantly, some of our methods lead\nto constructions of new multivariate models for both rate parameters and count\nobservations. Finally, estimators that shrink the usual estimators towards a\ndata based point in the parameter space are derived and compared.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 19:10:17 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 21:12:04 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Stoltenberg", "Emil Aas", ""], ["Hjort", "Nils Lid", ""]]}, {"id": "1904.09344", "submitter": "Deepak Nag Ayyala", "authors": "Seonghun Cho, Johan Lim, Deepak Nag Ayyala, Junyong Park, Anindya Roy", "title": "Note on Mean Vector Testing for High-Dimensional Dependent Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the mean vector test in high dimension, Ayyala et al.(2017,153:136-155)\nproposed new test statistics when the observational vectors are M dependent.\nUnder certain conditions, the test statistics for one-same and two-sample cases\nwere shown to be asymptotically normal. While the test statistics and the\nasymptotic results are valid, some parts of the proof of asymptotic normality\nneed to be corrected. In this work, we provide corrections to the proofs of\ntheir main theorems. We also note a few minor discrepancies in calculations in\nthe publication.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 21:05:58 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cho", "Seonghun", ""], ["Lim", "Johan", ""], ["Ayyala", "Deepak Nag", ""], ["Park", "Junyong", ""], ["Roy", "Anindya", ""]]}, {"id": "1904.09347", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Efficient two-sample functional estimation and the super-oracle\n  phenomenon", "comments": "82 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of two-sample integral functionals, of the type\nthat occur naturally, for example, when the object of interest is a divergence\nbetween unknown probability densities. Our first main result is that, in wide\ngenerality, a weighted nearest neighbour estimator is efficient, in the sense\nof achieving the local asymptotic minimax lower bound. Moreover, we also prove\na corresponding central limit theorem, which facilitates the construction of\nasymptotically valid confidence intervals for the functional, having\nasymptotically minimal width. One interesting consequence of our results is the\ndiscovery that, for certain functionals, the worst-case performance of our\nestimator may improve on that of the natural `oracle' estimator, which is given\naccess to the values of the unknown densities at the observations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:22:51 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1904.09372", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo and Michael Jansson", "title": "Average Density Estimators: Efficiency and Bootstrap Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights a tension between semiparametric efficiency and\nbootstrap consistency in the context of a canonical semiparametric estimation\nproblem, namely the problem of estimating the average density. It is shown that\nalthough simple plug-in estimators suffer from bias problems preventing them\nfrom achieving semiparametric efficiency under minimal smoothness conditions,\nthe nonparametric bootstrap automatically corrects for this bias and that, as a\nresult, these seemingly inferior estimators achieve bootstrap consistency under\nminimal smoothness conditions. In contrast, several \"debiased\" estimators that\nachieve semiparametric efficiency under minimal smoothness conditions do not\nachieve bootstrap consistency under those same conditions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:34:42 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 23:21:19 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""]]}, {"id": "1904.09394", "submitter": "Abbas Khalili Professor", "authors": "Annaliza McGillivray, Abbas Khalili, and David A. Stephens", "title": "Estimating Sparse Networks with Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical modelling techniques based on sparse selection have been applied to\ninfer complex networks in many fields, including biology and medicine,\nengineering, finance, and social sciences. One structural feature of some of\nthe networks in such applications that poses a challenge for statistical\ninference is the presence of a small number of strongly interconnected nodes in\na network which are called hubs. For example, in microbiome research hubs or\nmicrobial taxa play a significant role in maintaining stability of the\nmicrobial community structure. In this paper, we investigate the problem of\nestimating sparse networks in which there are a few highly connected hub nodes.\nMethods based on L1-regularization have been widely used for performing sparse\nselection in the graphical modelling context. However, while these methods\nencourage sparsity, they do not take into account structural information of the\nnetwork. We introduce a new method for estimating networks with hubs that\nexploits the ability of (inverse) covariance selection methods to include\nstructural information about the underlying network. Our proposed method is a\nweighted lasso approach with novel row/column sum weights, which we refer to as\nthe hubs weighted graphical lasso. We establish large sample properties of the\nmethod when the number of parameters diverges with the sample size, and\nevaluate its finite sample performance via extensive simulations. We illustrate\nthe method with an application to microbiome data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 03:32:12 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 01:56:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["McGillivray", "Annaliza", ""], ["Khalili", "Abbas", ""], ["Stephens", "David A.", ""]]}, {"id": "1904.09420", "submitter": "Raanju Ragavendar Sundararajan", "authors": "Raanju Ragavendar Sundararajan, Vladas Pipiras and Mohsen Pourahmadi", "title": "Stationary subspace analysis of nonstationary covariance processes:\n  eigenstructure description and testing", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary subspace analysis (SSA) searches for linear combinations of the\ncomponents of nonstationary vector time series that are stationary. These\nlinear combinations and their number defne an associated stationary subspace\nand its dimension. SSA is studied here for zero mean nonstationary covariance\nprocesses. We characterize stationary subspaces and their dimensions in terms\nof eigenvalues and eigenvectors of certain symmetric matrices. This\ncharacterization is then used to derive formal statistical tests for estimating\ndimensions of stationary subspaces. Eigenstructure-based techniques are also\nproposed to estimate stationary subspaces, without relying on previously used\ncomputationally intensive optimization-based methods. Finally, the introduced\nmethodologies are examined on simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 08:22:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sundararajan", "Raanju Ragavendar", ""], ["Pipiras", "Vladas", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "1904.09532", "submitter": "Shengming Luo", "authors": "Jiashun Jin, Zheng Tracy Ke, Shengming Luo", "title": "Optimal Adaptivity of Signed-Polygon Statistics for Network Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric social network, we are interested in testing whether it has\nonly one community or multiple communities. The desired tests should (a)\naccommodate severe degree heterogeneity, (b) accommodate mixed-memberships, (c)\nhave a tractable null distribution, and (d) adapt automatically to different\nlevels of sparsity, and achieve the optimal phase diagram. How to find such a\ntest is a challenging problem.\n  We propose the Signed Polygon as a class of new tests. Fixing $m \\geq 3$, for\neach $m$-gon in the network, define a score using the centered adjacency\nmatrix. The sum of such scores is then the $m$-th order Signed Polygon\nstatistic. The Signed Triangle (SgnT) and the Signed Quadrilateral (SgnQ) are\nspecial examples of the Signed Polygon.\n  We show that both the SgnT and SgnQ tests satisfy (a)-(d), and especially,\nthey work well for both very sparse and less sparse networks. Our proposed\ntests compare favorably with the existing tests. For example, the EZ and GC\ntests behave unsatisfactorily in the less sparse case and do not achieve the\noptimal phase diagram. Also, many existing tests do not allow for severe\nheterogeneity or mixed-memberships, and they behave unsatisfactorily in our\nsettings.\n  The analysis of the SgnT and SgnQ tests is delicate and extremely tedious,\nand the main reason is that we need a unified proof that covers a wide range of\nsparsity levels and a wide range of degree heterogeneity. For lower bound\ntheory, we use a phase transition framework, which includes the standard\nminimax argument, but is more informative. The proof uses classical theorems on\nmatrix scaling.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:41:07 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 20:04:31 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""]]}, {"id": "1904.09534", "submitter": "Hau-tieng Wu", "authors": "Matt Sourisseau, Hau-Tieng Wu, Zhou Zhou", "title": "Inference of synchrosqueezing transform -- toward a unified statistical\n  analysis of nonlinear-type time-frequency analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a statistical analysis of a tool in nonlinear-type time-frequency\nanalysis, the synchrosqueezing transform (SST), for both the null and non-null\ncases. The intricate nonlinear interaction of different quantities in SST is\nquantified by carefully analyzing relevant multivariate complex Gaussian random\nvariables. Specifically, we provide the quotient distribution of dependent and\nimproper complex Gaussian random variables. Then, a central limit theorem\nresult for SST is established. Furthermore, we provide a block bootstrap scheme\nbased on the established SST theory to test if a given time series contains\noscillatory components.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:57:28 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 22:16:38 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 10:44:20 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sourisseau", "Matt", ""], ["Wu", "Hau-Tieng", ""], ["Zhou", "Zhou", ""]]}, {"id": "1904.09623", "submitter": "Deborshee Sen", "authors": "Deborshee Sen and Alexandre H Thiery", "title": "Particle filter efficiency under limited communication", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods are typically not straightforward to\nimplement on parallel architectures. This is because standard resampling\nschemes involve communication between all particles in the system. In this\narticle, we consider the $\\alpha$-SMC algorithm, a generalised particle filter\nalgorithm with limited communications between particles. The communication\nbetween different particles is controlled through the so-called\n$\\alpha$-matrices. We study the influence of the communication structure on the\nconvergence and stability properties of the resulting algorithms. We prove that\nunder standard assumptions, it is possible to use randomised communication\nstructures where each particle only communicates with a small number of\nneighbouring particles while still having good mixing properties, and this\nensures that the resulting algorithms are stable in time and converge at the\nusual Monte Carlo rate. A particularly simple approach to implement these ideas\nconsists of choosing the $\\alpha$-matrices as the Markov transition matrices of\nrandom walks on Ramanujan graphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 16:49:02 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sen", "Deborshee", ""], ["Thiery", "Alexandre H", ""]]}, {"id": "1904.09632", "submitter": "Jeremiah Zhe Liu", "authors": "Jeremiah Zhe Liu", "title": "Gaussian Process Regression and Classification under Mathematical\n  Constraints with Learning Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce constrained Gaussian process (CGP), a Gaussian process model for\nrandom functions that allows easy placement of mathematical constrains (e.g.,\nnon-negativity, monotonicity, etc) on its sample functions. CGP comes with\nclosed-form probability density function (PDF), and has the attractive feature\nthat its posterior distributions for regression and classification are again\nCGPs with closed-form expressions. Furthermore, we show that CGP inherents the\noptimal theoretical properties of the Gaussian process, e.g. rates of posterior\ncontraction, due to the fact that CGP is an Gaussian process with a more\nefficient model space.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:19:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Jeremiah Zhe", ""]]}, {"id": "1904.09635", "submitter": "Yingjie Fei", "authors": "Yingjie Fei and Yudong Chen", "title": "Achieving the Bayes Error Rate in Synchronization and Block Models by\n  SDP, Robustly", "comments": "Partial preliminary results to appear in the Conference on Learning\n  Theory (COLT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical performance of semidefinite programming (SDP)\nrelaxations for clustering under random graph models. Under the\n$\\mathbb{Z}_{2}$ Synchronization model, Censored Block Model and Stochastic\nBlock Model, we show that SDP achieves an error rate of the form \\[\n\\exp\\Big[-\\big(1-o(1)\\big)\\bar{n} I^* \\Big]. \\] Here $\\bar{n}$ is an\nappropriate multiple of the number of nodes and $I^*$ is an\ninformation-theoretic measure of the signal-to-noise ratio. We provide matching\nlower bounds on the Bayes error for each model and therefore demonstrate that\nthe SDP approach is Bayes optimal. As a corollary, our results imply that SDP\nachieves the optimal exact recovery threshold under each model. Furthermore, we\nshow that SDP is robust: the above bound remains valid under semirandom\nversions of the models in which the observed graph is modified by a monotone\nadversary. Our proof is based on a novel primal-dual analysis of SDP under a\nunified framework for all three models, and the analysis shows that SDP tightly\napproximates a joint majority voting procedure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:44:48 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fei", "Yingjie", ""], ["Chen", "Yudong", ""]]}, {"id": "1904.09647", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Hans-Georg M\\\"uller", "title": "Total Variation Regularized Fr\\'echet Regression for Metric-Space Valued\n  Data", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Euclidean data that are indexed with a scalar predictor such as time are\nincreasingly encountered in data applications, while statistical methodology\nand theory for such random objects are not well developed yet. To address the\nneed for new methodology in this area, we develop a total variation\nregularization technique for nonparametric Fr\\'echet regression, which refers\nto a regression setting where a response residing in a metric space is paired\nwith a scalar predictor and the target is a conditional Fr\\'echet mean.\nSpecifically, we seek to approximate an unknown metric-space valued function by\nan estimator that minimizes the Fr\\'echet version of least squares and at the\nsame time has small total variation, appropriately defined for metric-space\nvalued objects. We show that the resulting estimator is representable by a\npiece-wise constant function and establish the minimax convergence rate of the\nproposed estimator for metric data objects that reside in Hadamard spaces. We\nillustrate the numerical performance of the proposed method for both simulated\nand real data, including metric spaces of symmetric positive-definite matrices\nwith the affine-invariant distance, of probability distributions on the real\nline with the Wasserstein distance, and of phylogenetic trees with the\nBillera--Holmes--Vogtmann metric.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 19:18:24 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 10:14:03 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 11:18:50 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 09:21:10 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lin", "Zhenhua", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1904.09733", "submitter": "Raffaele Argiento", "authors": "Raffaele Argiento and Maria De Iorio", "title": "Is infinity that far? A Bayesian nonparametric perspective of finite\n  mixture models", "comments": "46 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mixture models are one of the most widely used statistical tools when dealing\nwith data from heterogeneous populations. This paper considers the\nlong-standing debate over finite mixture and infinite mixtures and brings the\ntwo modelling strategies together, by showing that a finite mixture is simply a\nrealization of a point process. Following a Bayesian nonparametric perspective,\nwe introduce a new class of prior: the Normalized Independent Point Processes.\nWe investigate the probabilistic properties of this new class. Moreover, we\ndesign a conditional algorithm for finite mixture models with a random number\nof components overcoming the challenges associated with the Reversible Jump\nscheme and the recently proposed marginal algorithms. We illustrate our model\non real data and discuss an important application in population genetics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:59:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Argiento", "Raffaele", ""], ["De Iorio", "Maria", ""]]}, {"id": "1904.09750", "submitter": "Yury Kutoyants", "authors": "Yury A. Kutoyants and Li Zhou", "title": "On Parameter Estimation of the Hidden Gaussian Process in perturbed SDE", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results on parameter estimation and non-parameter estimation of\nthe linear partially observed Gaussian system of stochastic differential\nequations. We propose new one-step estimators which have the same asymptotic\nproperties as the MLE, but much more simple to calculate, the estimators are\nso-called \"estimator-processes\". The construction of the estimators is based on\nthe equations of Kalman-Bucy filtration and the asymptotic corresponds to the\nsmall noises in the observations and state (hidden process) equations. We\npropose conditions which provide the consistency and asymptotic normality and\nasymptotic efficiency of the estimators.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 07:13:38 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kutoyants", "Yury A.", ""], ["Zhou", "Li", ""]]}, {"id": "1904.09929", "submitter": "Yanan Pei", "authors": "Jose H. Blanchet, Peter W. Glynn, Yanan Pei", "title": "Unbiased Multilevel Monte Carlo: Stochastic Optimization, Steady-state\n  Simulation, Quantiles, and Other Applications", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present general principles for the design and analysis of unbiased Monte\nCarlo estimators in a wide range of settings. Our estimators posses finite\nwork-normalized variance under mild regularity conditions. We apply our\nestimators to various settings of interest, including unbiased optimization in\nSample Average Approximations, unbiased steady-state simulation of regenerative\nprocesses, quantile estimation and nested simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 15:44:14 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Blanchet", "Jose H.", ""], ["Glynn", "Peter W.", ""], ["Pei", "Yanan", ""]]}, {"id": "1904.10114", "submitter": "Taiane Prass", "authors": "S\\'ilvia Regina Costa Lopes and Taiane Schaedler Prass", "title": "Seasonal FIEGARCH Processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis. 68:262-295", "doi": "10.1016/j.csda.2013.07.001", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we develop the theory of seasonal FIEGARCH processes, denoted by\nSFIEGARCH, establishing conditions for the existence, the invertibility, the\nstationarity and the ergodicity of these processes. We analyze their asymptotic\ndependence structure by means of the autocovariance and autocorrelation\nfunctions. We also present some properties regarding their spectral\nrepresentation. All properties are illustrated through graphical examples and\nan application of SFIEGARCH models to describe the volatility of the S&P500 US\nstock index log-return time series in the period from December 13, 2004 to\nOctober 10, 2009 is provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 01:32:43 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Lopes", "S\u00edlvia Regina Costa", ""], ["Prass", "Taiane Schaedler", ""]]}, {"id": "1904.10428", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "On the Kullback-Leibler divergence between location-scale densities", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the $f$-divergence between any two densities of potentially\ndifferent location-scale families can be reduced to the calculation of the\n$f$-divergence between one standard density with another location-scale\ndensity. It follows that the $f$-divergence between two scale densities depends\nonly on the scale ratio. We then report conditions on the standard distribution\nto get symmetric $f$-divergences: First, we prove that all $f$-divergences\nbetween densities of a location family are symmetric whenever the standard\ndensity is even, and second, we illustrate a generic symmetric property with\nthe calculation of the Kullback-Leibler divergence between scale Cauchy\ndistributions. Finally, we show that the minimum $f$-divergence of any query\ndensity of a location-scale family to\n  another location-scale family is independent of the query location-scale\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 17:16:31 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 03:57:44 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 09:49:02 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1904.10495", "submitter": "Matija Vidmar", "authors": "Matija Vidmar", "title": "On laws exhibiting universal ordering under stochastic restart", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For each of (i) arbitrary stochastic reset, (ii) deterministic reset with\narbitrary period, (iii) reset at arbitrary constant rate, and then in the sense\nof either (a) first-order stochastic dominance or (b) expectation (i.e. for\neach of the six possible combinations of the preceding), those laws of random\ntimes are precisely characterized that are rendered no bigger [rendered no\nsmaller; left invariant] by all possible restart laws (within the classes (i),\n(ii), (iii), as the case may be). Partial results in the same vein for reset\nwith branching are obtained. In particular it is found that deterministic and\narbitrary stochastic restart lead to the same characterizations, but this\nequivalence fails to persist for exponential (constant-rate) reset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 18:57:17 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 10:08:34 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 08:59:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Vidmar", "Matija", ""]]}, {"id": "1904.10660", "submitter": "Chiara Amorino", "authors": "Chiara Amorino (LaMME), Arnaud Gloter (LaMME)", "title": "Unbiased truncated quadratic variation for volatility estimation in jump\n  diffusion processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of integrated volatility estimation for the solution X of a\nstochastic differential equation with L{\\'e}vy-type jumps is considered under\ndiscrete high-frequency observations in both short and long time horizon. We\nprovide an asymptotic expansion for the integrated volatility that gives us, in\ndetail, the contribution deriving from the jump part. The knowledge of such a\ncontribution allows us to build an unbiased version of the truncated quadratic\nvariation, in which the bias is visibly reduced. In earlier results the\ncondition $\\beta$ > 1 2(2--$\\alpha$) on $\\beta$ (that is such that (1/n)\n$\\beta$ is the threshold of the truncated quadratic variation) and on the\ndegree of jump activity $\\alpha$ was needed to have the original truncated\nrealized volatility well-performed (see [22], [13]). In this paper we\ntheoretically relax this condition and we show that our unbiased estimator\nachieves excellent numerical results for any couple ($\\alpha$, $\\beta$).\nL{\\'e}vy-driven SDE, integrated variance, threshold estimator, convergence\nspeed, high frequency data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 07:14:02 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:02:58 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Amorino", "Chiara", "", "LaMME"], ["Gloter", "Arnaud", "", "LaMME"]]}, {"id": "1904.10871", "submitter": "Francesco Ortelli", "authors": "Francesco Ortelli and Sara van de Geer", "title": "Prediction bounds for higher order total variation regularized least\n  squares", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish adaptive results for trend filtering: least squares estimation\nwith a penalty on the total variation of $(k-1)^{\\rm th}$ order differences.\nOur approach is based on combining a general oracle inequality for the\n$\\ell_1$-penalized least squares estimator with \"interpolating vectors\" to\nupper-bound the \"effective sparsity\". This allows one to show that the\n$\\ell_1$-penalty on the $k^{\\text{th}}$ order differences leads to an estimator\nthat can adapt to the number of jumps in the $(k-1)^{\\text{th}}$ order\ndifferences of the underlying signal or an approximation thereof. We show the\nresult for $k \\in \\{1,2,3,4\\}$ and indicate how it could be derived for general\n$k\\in \\mathbb{N}$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:26:48 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 06:18:35 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 19:26:40 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 08:08:01 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ortelli", "Francesco", ""], ["van de Geer", "Sara", ""]]}, {"id": "1904.10884", "submitter": "Hyun-Jung Kim", "authors": "Igor Cialenco, Francisco Delgado-Vences and Hyun-Jung Kim", "title": "Drift Estimation for Discretely Sampled SPDEs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to study the asymptotic properties of the maximum\nlikelihood estimator (MLE) of the drift coefficient for fractional stochastic\nheat equation driven by an additive space-time noise. We consider the\ntraditional for stochastic partial differential equations statistical\nexperiment when the measurements are performed in the spectral domain, and in\ncontrast to the existing literature, we study the asymptotic properties of the\nmaximum likelihood (type) estimators (MLE) when both, the number of Fourier\nmodes and the time go to infinity. In the first part of the paper we consider\nthe usual setup of continuous time observations of the Fourier coefficients of\nthe solutions, and show that the MLE is consistent, asymptotically normal and\noptimal in the mean-square sense. In the second part of the paper we\ninvestigate the natural time discretization of the MLE, by assuming that the\nfirst N Fourier modes are measured at M time grid points, uniformly spaced over\nthe time interval [0,T]. We provide a rigorous asymptotic analysis of the\nproposed estimators when N goes to infinity and/or T, M go to infinity. We\nestablish sufficient conditions on the growth rates of N, M and T, that\nguarantee consistency and asymptotic normality of these estimators.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:53:06 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Cialenco", "Igor", ""], ["Delgado-Vences", "Francisco", ""], ["Kim", "Hyun-Jung", ""]]}, {"id": "1904.11060", "submitter": "Michael Leung", "authors": "Michael P. Leung, Hyungsik Roger Moon", "title": "Normal Approximation in Large Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a methodology for proving central limit theorems in network models\nwith strategic interactions and homophilous agents. Since data often consists\nof observations on a single large network, we consider an asymptotic framework\nin which the network size tends to infinity. In the presence of strategic\ninteractions, network moments are generally complex functions of components,\nwhere a node's component consists of all alters to which it is directly or\nindirectly connected. We find that a modification of \"exponential\nstabilization\" conditions from the stochastic geometry literature provides a\nuseful formulation of weak dependence for moments of this type. We establish a\nCLT for a network moments satisfying stabilization and provide a methodology\nfor deriving primitive sufficient conditions for stabilization using results in\nbranching process theory. We apply the methodology to static and dynamic models\nof network formation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 20:42:11 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 02:03:13 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 04:05:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Leung", "Michael P.", ""], ["Moon", "Hyungsik Roger", ""]]}, {"id": "1904.11075", "submitter": "Jun Yang", "authors": "Yan Cui, Jun Yang, Zhou Zhou", "title": "State-domain Change Point Detection for Nonlinear Time Series Regression", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection in time series has attracted substantial interest, but\nmost of the existing results have been focused on detecting change points in\nthe time domain. This paper considers the situation where nonlinear time series\nhave potential change points in the state domain. We apply a density-weighted\nanti-symmetric kernel function to the state domain and therefore propose a\nnonparametric procedure to test the existence of change points. When the\nexistence of change points is affirmative, we further introduce an algorithm to\nestimate their number together with locations and show the convergence result\non the estimation procedure. A real dataset of German daily confirmed cases of\nCOVID-19 is used to illustrate our results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:31:12 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:49:41 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 19:52:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Yan", ""], ["Yang", "Jun", ""], ["Zhou", "Zhou", ""]]}, {"id": "1904.11085", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Mauricio Sadinle", "title": "Nonparametric Pattern-Mixture Models for Inference with Missing Data", "comments": "65 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-mixture models provide a transparent approach for handling missing\ndata, where the full-data distribution is factorized in a way that explicitly\nshows the parts that can be estimated from observed data alone, and the parts\nthat require identifying restrictions. We introduce a nonparametric estimator\nof the full-data distribution based on the pattern-mixture model factorization.\nOur approach uses the empirical observed-data distribution and augments it with\na nonparametric estimator of the missing-data distributions under a given\nidentifying restriction. Our results apply to a large class of donor-based\nidentifying restrictions that encompasses commonly used ones and can handle\nboth monotone and nonmonotone missingness. We propose a Monte Carlo procedure\nto derive point estimates of functionals of interest, and the bootstrap to\nconstruct confidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:54:14 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Sadinle", "Mauricio", ""]]}, {"id": "1904.11098", "submitter": "Indrajit Jana", "authors": "Indrajit Jana", "title": "CLT for non-Hermitian random band matrices with variance profiles", "comments": "Typos corrected; a few more explanations and a couple of pictures\n  have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the fluctuations of the linear eigenvalue statistics of a\nnon-Hermitian random band matrix of increasing bandwidth $b_{n}$ with a\ncontinuous variance profile $w_{\\nu}(x)$ converges to a\n$N(0,\\sigma_{f}^{2}(\\nu))$, where $\\nu=\\lim_{n\\to\\infty}(2b_{n}/n)\\in [0,1]$\nand $f$ is the test function. When $\\nu\\in (0,1]$, we obtain an explicit\nformula for $\\sigma_{f}^{2}(\\nu)$, which depends on $f$, and variance profile\n$w_{\\nu}$. When $\\nu=1$, the formula is consistent with Rider and Silverstein\n(2006) \\cite{rider2006gaussian}. We also independently compute an explicit\nformula for $\\sigma_{f}^{2}(0)$ i.e., when the bandwidth $b_{n}$ grows slower\ncompared to $n$. In addition, we show that $\\sigma_{f}^{2}(\\nu)\\to\n\\sigma_{f}^{2}(0)$ as $\\nu\\downarrow 0$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 23:44:40 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 14:46:26 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 13:26:54 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Jana", "Indrajit", ""]]}, {"id": "1904.11101", "submitter": "Monika Bhattacharjee", "authors": "Monika Bhattacharjee, Moulinath Banerjee and George Michailidis", "title": "Change Point Estimation in Panel Data with Temporal and Cross-sectional\n  Dependence", "comments": "57 pages, 1 figure, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting a common change point in large panel data\nbased on a mean shift model, wherein the errors exhibit both temporal and\ncross-sectional dependence. A least squares based procedure is used to estimate\nthe location of the change point. Further, we establish the convergence rate\nand obtain the asymptotic distribution of the least squares estimator. The form\nof the distribution is determined by the behavior of the norm difference of the\nmeans before and after the change point. Since the behavior of this norm\ndifference is, a priori, unknown to the practitioner, we also develop a novel\ndata driven adaptive procedure that provides valid confidence intervals for the\ncommon change point, without requiring any such knowledge. Numerical work based\non synthetic data illustrates the performance of the estimator in finite\nsamples under different settings of temporal and cross-sectional dependence,\nsample size and number of panels. Finally, we examine an application to\nfinancial stock data and discuss the identified change points.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 00:03:07 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Bhattacharjee", "Monika", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1904.11316", "submitter": "Yiming Ying", "authors": "Wei Shen, Zhenhuan Yang, Yiming Ying and Xiaoming Yuan", "title": "Stability and Optimization Error of Stochastic Gradient Descent for\n  Pairwise Learning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the stability and its trade-off with optimization\nerror for stochastic gradient descent (SGD) algorithms in the pairwise learning\nsetting. Pairwise learning refers to a learning task which involves a loss\nfunction depending on pairs of instances among which notable examples are\nbipartite ranking, metric learning, area under ROC (AUC) maximization and\nminimum error entropy (MEE) principle. Our contribution is twofold. Firstly, we\nestablish the stability results of SGD for pairwise learning in the convex,\nstrongly convex and non-convex settings, from which generalization bounds can\nbe naturally derived. Secondly, we establish the trade-off between stability\nand optimization error of SGD algorithms for pairwise learning. This is\nachieved by lower-bounding the sum of stability and optimization error by the\nminimax statistical error over a prescribed class of pairwise loss functions.\nFrom this fundamental trade-off, we obtain lower bounds for the optimization\nerror of SGD algorithms and the excess expected risk over a class of pairwise\nlosses. In addition, we illustrate our stability results by giving some\nspecific examples of AUC maximization, metric learning and MEE.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:07:37 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 14:39:35 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Shen", "Wei", ""], ["Yang", "Zhenhuan", ""], ["Ying", "Yiming", ""], ["Yuan", "Xiaoming", ""]]}, {"id": "1904.11609", "submitter": "Heudson Mirandola", "authors": "Tha\\'is C. O. Fonseca, Helio S. Migon and Heudson Mirandola", "title": "Reference Bayesian analysis for hierarchical models", "comments": "17 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternative approach for constructing invariant\nJeffreys prior distributions tailored for hierarchical or multilevel models. In\nparticular, our proposal is based on a flexible decomposition of the Fisher\ninformation for hierarchical models which overcomes the marginalization step of\nthe likelihood of model parameters. The Fisher information matrix for the\nhierarchical model is derived from the Hessian of the Kullback-Liebler (KL)\ndivergence for the model in a neighborhood of the parameter value of interest.\nProperties of the KL divergence are used to prove the proposed decomposition.\nOur proposal takes advantage of the hierarchy and leads to an alternative way\nof computing Jeffreys priors for the hyperparameters and an upper bound for the\nprior information. While the Jeffreys prior gives the minimum information about\nparameters, the proposed bound gives an upper limit for the information put in\nany prior distribution. A prior with information above that limit may be\nconsidered too informative. From a practical point of view, the proposed prior\nmay be evaluated computationally as part of a MCMC algorithm. This property\nmight be essential for modeling setups with many levels in which analytic\nmarginalization is not feasible. We illustrate the usefulness of our proposal\nwith examples in mixture models, in model selection priors such as lasso and in\nthe Student-t model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:12:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Fonseca", "Tha\u00eds C. O.", ""], ["Migon", "Helio S.", ""], ["Mirandola", "Heudson", ""]]}, {"id": "1904.11626", "submitter": "Henry Lam", "authors": "Henry Lam and Fengpei Li", "title": "Parametric Scenario Optimization under Limited Data: A Distributionally\n  Robust Optimization View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimization problems with uncertain constraints that need to be\nsatisfied probabilistically. When data are available, a common method to obtain\nfeasible solutions for such problems is to impose sampled constraints,\nfollowing the so-called scenario optimization approach. However, when the data\nsize is small, the sampled constraints may not statistically support a\nfeasibility guarantee on the obtained solution. This paper studies how to\nleverage parametric information and the power of Monte Carlo simulation to\nobtain feasible solutions for small-data situations. Our approach makes use of\na distributionally robust optimization (DRO) formulation that translates the\ndata size requirement into a Monte Carlo sample size requirement drawn from\nwhat we call a generating distribution. We show that, while the optimal choice\nof this generating distribution is the one eliciting the data or the baseline\ndistribution in a nonparametric divergence-based DRO, it is not necessarily so\nin the parametric case. Correspondingly, we develop procedures to obtain\ngenerating distributions that improve upon these basic choices. We support our\nfindings with several numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:54:15 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 14:25:57 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lam", "Henry", ""], ["Li", "Fengpei", ""]]}, {"id": "1904.11665", "submitter": "William Leeb", "authors": "William Leeb", "title": "Rapid evaluation of the spectral signal detection threshold and\n  Stieltjes transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of signal components is a frequently-encountered challenge\nin statistical applications with low signal-to-noise ratio. This problem is\nparticularly challenging in settings with heteroscedastic noise. In certain\nsignal-plus-noise models of data, such as the classical spiked covariance model\nand its variants, there are closed formulas for the spectral signal detection\nthreshold (the largest sample eigenvalue attributable solely to noise) in the\nisotropic noise setting. However, existing methods for numerically evaluating\nthe threshold for more general noise models remain unsatisfactory.\n  In this work, we introduce a rapid algorithm for evaluating the spectral\nsignal detection threshold. We consider noise matrices with a separable\nvariance profile, as these arise often in applications. The solution is based\non nested applications of Newton's method. We also devise a new algorithm for\nevaluating the Stieltjes transform of the spectral distribution at real values\nexceeding the threshold. The Stieltjes transform on this domain is known to be\na key quantity in parameter estimation for spectral denoising methods. The\ncorrectness of both algorithms is proven from a detailed analysis of the master\nequations characterizing the Stieltjes transform, and their performance is\ndemonstrated in numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 04:14:41 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:44:33 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "1904.11823", "submitter": "Michel Broniatowski", "authors": "Jana Jureckova, Amor Keziou (LMR), Michel Broniatowski (LPSM UMR\n  8001), Jana Ckov\\^A\\'a, Ckov\\^A\\' Ckov\\^A\\'a, Amor and", "title": "Uniform minimum risk equivariant estimates for moment condition models", "comments": "arXiv admin note: text overlap with arXiv:1002.0730", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider semiparametric moment condition models invariant to\ntransformation groups. The parameter of interest is estimated by minimum\nempirical divergence approach, introduced by Broniatowski and Keziou (2012). It\nis shown that the minimum empirical divergence estimates, including the\nempirical likelihood one, are equivariants. The minimum risk equivariant\nestimate is then identied to be any one of the minimum empirical divergence\nestimates minus its expectation conditionally to maximal invariant statistic of\nthe considered group of transformations. An asymptotic approximation to the\nconditional expectation, is obtained, using the result of Jureckov{\\'a} and\nPicek (2009).\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:06:46 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Jureckova", "Jana", "", "LMR"], ["Keziou", "Amor", "", "LMR"], ["Broniatowski", "Michel", "", "LPSM UMR\n  8001"], ["Ckov\u00c2\u00e1", "Jana", ""], ["Ckov\u00c2\u00e1", "Ckov\u00c2\\'", ""], ["and", "Amor", ""]]}, {"id": "1904.11871", "submitter": "Marcel Br\\\"autigam", "authors": "Marcel Br\\\"autigam and Marie Kratz", "title": "On the Dependence between Functions of Quantile and Dispersion\n  Estimators", "comments": "33 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive the joint asymptotic distributions of functions of\nquantile estimators (the non-parametric sample quantile and the parametric\nlocation-scale quantile estimator) with functions of measure of dispersion\nestimators (the sample variance, sample mean absolute deviation, sample median\nabsolute deviation) - assuming an underlying identically and independently\ndistributed sample. We also discuss the conditions required by the use of such\nestimators. Further, we show that these results can be extended to any higher\norder absolute central sample moment as measure of dispersion. Aware of the\ndifference in speed of convergence of the two quantile estimators, we compare\nthe impact of the choice of the quantile estimator (and measure of dispersion)\non the asymptotic correlations. Then we prove a scaling law for the asymptotic\ndependence of quantile estimators with measure of dispersion estimators.\nFinally, we show a good finite sample performance of the asymptotics in\nsimulations for elliptical distributions. All the results should constitute an\nimportant and useful complement in the statistical literature as those\nestimators are either of standard use in statistics and application fields, or\nshould become as such because of weaker conditions in the asymptotic theorems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 14:31:29 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Br\u00e4utigam", "Marcel", ""], ["Kratz", "Marie", ""]]}, {"id": "1904.12053", "submitter": "Shivam Garg", "authors": "Brian Axelrod, Shivam Garg, Vatsal Sharan, Gregory Valiant", "title": "Sample Amplification: Increasing Dataset Size even when Learning is\n  Impossible", "comments": "Added discussion about potential applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data drawn from an unknown distribution, $D$, to what extent is it\npossible to ``amplify'' this dataset and output an even larger set of samples\nthat appear to have been drawn from $D$? We formalize this question as follows:\nan $(n,m)$ $\\text{amplification procedure}$ takes as input $n$ independent\ndraws from an unknown distribution $D$, and outputs a set of $m > n$\n``samples''. An amplification procedure is valid if no algorithm can\ndistinguish the set of $m$ samples produced by the amplifier from a set of $m$\nindependent draws from $D$, with probability greater than $2/3$. Perhaps\nsurprisingly, in many settings, a valid amplification procedure exists, even\nwhen the size of the input dataset, $n$, is significantly less than what would\nbe necessary to learn $D$ to non-trivial accuracy. Specifically we consider two\nfundamental settings: the case where $D$ is an arbitrary discrete distribution\nsupported on $\\le k$ elements, and the case where $D$ is a $d$-dimensional\nGaussian with unknown mean, and fixed covariance. In the first case, we show\nthat an $\\left(n, n + \\Theta(\\frac{n}{\\sqrt{k}})\\right)$ amplifier exists. In\nparticular, given $n=O(\\sqrt{k})$ samples from $D$, one can output a set of\n$m=n+1$ datapoints, whose total variation distance from the distribution of $m$\ni.i.d. draws from $D$ is a small constant, despite the fact that one would need\nquadratically more data, $n=\\Theta(k)$, to learn $D$ up to small constant total\nvariation distance. In the Gaussian case, we show that an\n$\\left(n,n+\\Theta(\\frac{n}{\\sqrt{d}} )\\right)$ amplifier exists, even though\nlearning the distribution to small constant total variation distance requires\n$\\Theta(d)$ samples. In both the discrete and Gaussian settings, we show that\nthese results are tight, to constant factors. Beyond these results, we\nformalize a number of curious directions for future research along this vein.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:42:44 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 01:40:28 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Axelrod", "Brian", ""], ["Garg", "Shivam", ""], ["Sharan", "Vatsal", ""], ["Valiant", "Gregory", ""]]}, {"id": "1904.12070", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Optimal Bayesian Estimation for Random Dot Product Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach, called the posterior spectral embedding, for\nestimating the latent positions in random dot product graphs, and prove its\noptimality. Unlike the classical spectral-based adjacency/Laplacian spectral\nembedding, the posterior spectral embedding is a fully-likelihood based graph\nestimation method taking advantage of the Bernoulli likelihood information of\nthe observed adjacency matrix. We develop a minimax-lower bound for estimating\nthe latent positions, and show that the posterior spectral embedding achieves\nthis lower bound since it both results in a minimax-optimal posterior\ncontraction rate, and yields a point estimator achieving the minimax risk\nasymptotically. The convergence results are subsequently applied to clustering\nin stochastic block models, the result of which strengthens an existing result\nconcerning the number of mis-clustered vertices. We also study a spectral-based\nGaussian spectral embedding as a natural Bayesian analogy of the adjacency\nspectral embedding, but the resulting posterior contraction rate is sub-optimal\nwith an extra logarithmic factor. The practical performance of the proposed\nmethodology is illustrated through extensive synthetic examples and the\nanalysis of a Wikipedia graph data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 23:10:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1904.12154", "submitter": "Daniel H\\\"agele", "authors": "Fabian Schefczik and Daniel H\\\"agele", "title": "Ready-to-Use Unbiased Estimators for Multivariate Cumulants Including\n  One That Outperforms $\\overline{x^3}$", "comments": "submitted to IEEE Transactions on Signal Processing \\c{opyright} 2019\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for all other uses, in any current or future media", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present multivariate unbiased estimators for second, third, and fourth\norder cumulants $C_2(x,y)$, $C_3(x,y,z)$, and $C_4(x,y,z,w)$. Many relevant new\nestimators are derived for cases where some variables are average-free or pairs\nof variables have a vanishing second order cumulant. The well-know Fisher\nk-statistics is recovered for the single variable case. The variances of\nseveral estimators are explicitly given in terms of higher order cumulants and\ndiscussed with respect to random processes that are predominately Gaussian. We\nsurprisingly find that the frequently used third order estimator\n$\\overline{x^3}$ for $C_3(x,x,x)$ of a process $x$ with zero average is\noutperformed by alternative estimators. The new (Gauss-optimal) estimator\n$\\overline{x^3} - 3 \\overline{x^2}\\overline{x}(m-1)/(m+1)$ improves the\nvariance by a factor of up to $5/2$. Similarly, the estimator $\\overline{x^2\nz}$ for $C_3(x,x,z)$ can be replaced by another Gauss-optimal estimator. The\nknown estimator $\\overline{xyz}$ for $C_3(x,y,z)$ as well as previously known\nestimators for $C_2$ and $C_4$ of one average-free variable are shown to be\nGauss-optimal. As a side result of our work we present two simple recursive\nformulas for finding multivariate cumulants from moments and vice versa.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 13:09:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Schefczik", "Fabian", ""], ["H\u00e4gele", "Daniel", ""]]}, {"id": "1904.12191", "submitter": "Andrea Montanari", "authors": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Linearized two-layers neural networks in high dimension", "comments": "65 pages; 17 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning an unknown function $f_{\\star}$ on the\n$d$-dimensional sphere with respect to the square loss, given i.i.d. samples\n$\\{(y_i,{\\boldsymbol x}_i)\\}_{i\\le n}$ where ${\\boldsymbol x}_i$ is a feature\nvector uniformly distributed on the sphere and $y_i=f_{\\star}({\\boldsymbol\nx}_i)+\\varepsilon_i$. We study two popular classes of models that can be\nregarded as linearizations of two-layers neural networks around a random\ninitialization: the random features model of Rahimi-Recht (RF); the neural\ntangent kernel model of Jacot-Gabriel-Hongler (NT). Both these approaches can\nalso be regarded as randomized approximations of kernel ridge regression (with\nrespect to different kernels), and enjoy universal approximation properties\nwhen the number of neurons $N$ diverges, for a fixed dimension $d$.\n  We consider two specific regimes: the approximation-limited regime, in which\n$n=\\infty$ while $d$ and $N$ are large but finite; and the sample size-limited\nregime in which $N=\\infty$ while $d$ and $n$ are large but finite. In the first\nregime we prove that if $d^{\\ell + \\delta} \\le N\\le d^{\\ell+1-\\delta}$ for\nsmall $\\delta > 0$, then \\RF\\, effectively fits a degree-$\\ell$ polynomial in\nthe raw features, and \\NT\\, fits a degree-$(\\ell+1)$ polynomial. In the second\nregime, both RF and NT reduce to kernel methods with rotationally invariant\nkernels. We prove that, if the number of samples is $d^{\\ell + \\delta} \\le n\n\\le d^{\\ell +1-\\delta}$, then kernel methods can fit at most a a degree-$\\ell$\npolynomial in the raw features. This lower bound is achieved by kernel ridge\nregression. Optimal prediction error is achieved for vanishing ridge\nregularization.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 18:42:02 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 18:19:55 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 01:27:56 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ghorbani", "Behrooz", ""], ["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "1904.12321", "submitter": "Ted Westling", "authors": "Ted Westling, Kevin J. Downes, Dylan S. Small", "title": "Nonparametric maximum likelihood estimation under a likelihood ratio\n  order", "comments": "Revised paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of two univariate distributions based on independent samples from\nthem is a fundamental problem in statistics, with applications in a wide\nvariety of scientific disciplines. In many situations, we might hypothesize\nthat the two distributions are stochastically ordered, meaning intuitively that\nsamples from one distribution tend to be larger than those from the other. One\ntype of stochastic order that arises in economics, biomedicine, and elsewhere\nis the likelihood ratio order, also known as the density ratio order, in which\nthe ratio of the density functions of the two distributions is monotone\nnon-decreasing. In this article, we derive and study the nonparametric maximum\nlikelihood estimator of the individual distributions and the ratio of their\ndensities under the likelihood ratio order. Our work applies to discrete\ndistributions, continuous distributions, and mixed continuous-discrete\ndistributions. We demonstrate convergence in distribution of the estimator in\ncertain cases, and we illustrate our results using numerical experiments and an\nanalysis of a biomarker for predicting bacterial infection in children with\nsystemic inflammatory response syndrome.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:34:31 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 12:41:40 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 18:46:02 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 14:42:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Westling", "Ted", ""], ["Downes", "Kevin J.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1904.12398", "submitter": "Shoichi Eguchi", "authors": "Shoichi Eguchi and Yuma Uehara", "title": "Schwartz type model selection for ergodic stochastic differential\n  equation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the construction of the theoretical foundation of model comparison\nfor ergodic stochastic differential equation (SDE) models and an extension of\nthe applicable scope of the conventional Bayesian information criterion.\nDifferent from previous studies, we suppose that the candidate models are\npossibly misspecified models, and we consider both Wiener and a pure-jump\nL\\'{e}vy noise driven SDE. Based on the asymptotic behavior of the marginal\nquasi-log likelihood, the Schwarz type statistics and stepwise model selection\nprocedure are proposed. We also prove the model selection consistency of the\nproposed statistics with respect to an optimal model. We conduct some numerical\nexperiments and they support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:42:11 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 06:58:58 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 07:47:43 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 03:42:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Eguchi", "Shoichi", ""], ["Uehara", "Yuma", ""]]}, {"id": "1904.12459", "submitter": "Sudip Roy", "authors": "Sudip Roy, Ram C. Tripathi, N. Balakrishnan", "title": "A Closed Form Approximation of Moments of New Generalization of Negative\n  Binomial Distribution", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a closed form approximation to the mean and\nvariance of a new generalization of negative binomial (NGNB) distribution\narising from the Extended COM-Poisson (ECOMP) distribution developed by\nChakraborty and Imoto (2016)(see [4]). The NGNB is a special case of the ECOMP\ndistribution and was named so by these authors. This distribution is more\nflexible in terms of the dispersion index as compared to its ordinary\ncounterparts. It approaches the COM-Poisson distribution (Shmueli et al. 2005)\n[11] under suitable limiting conditions. The NGNB can also be obtained from the\nCOM-Negative Hypergeometric distribution (Roy et al. 2019)[10] as a limiting\ndistribution. In this paper, we present closed-form approximations for the mean\nand variance of the NGNB distribution. These approximations can be viewed as\nthe mean and variance of convolution of independent and identically distributed\nnegative binomial populations. The proposed closed-form approximations of the\nmean and variance will be helpful in building the link function for the\ngeneralized negative binomial regression model based on the NGNB distribution\nand other extended applications, hence resulting in enhanced applicability of\nthis model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:03:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Roy", "Sudip", ""], ["Tripathi", "Ram C.", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1904.12527", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen (LAMA), Christophe Denis (LAMA), Mohamed Hebiri (LAMA)", "title": "Minimax semi-supervised confidence sets for multi-class classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the semi-supervised framework of confidence set\nclassification with controlled expected size in minimax settings. We obtain\nsemi-supervised minimax rates of convergence under the margin assumption and a\nH{\\\"o}lder condition on the regression function. Besides, we show that if no\nfurther assumptions are made, there is no supervised method that outperforms\nthe semi-supervised estimator proposed in this work. We establish that the best\nachievable rate for any supervised method is n^{--1/2} , even if the margin\nassumption is extremely favorable. On the contrary, semi-supervised estimators\ncan achieve faster rates of convergence provided that sufficiently many\nunlabeled samples are available. We additionally perform numerical evaluation\nof the proposed algorithms empirically confirming our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:23:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chzhen", "Evgenii", "", "LAMA"], ["Denis", "Christophe", "", "LAMA"], ["Hebiri", "Mohamed", "", "LAMA"]]}, {"id": "1904.12701", "submitter": "Weiyin Fei", "authors": "Chen Fei, Weiyin Fei", "title": "Consistency of least squares estimation to the parameter for stochastic\n  differential equations under distribution uncertainty", "comments": "17 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under distribution uncertainty, on the basis of discrete data we investigate\nthe consistency of the least squares estimator (LSE) of the parameter for the\nstochastic differential equation (SDE) where the noise are characterized by\n$G$-Brownian motion. In order to obtain our main result of consistency of\nparameter estimation, we provide some lemmas by the theory of stochastic\ncalculus of sublinear expectation. The result shows that under some regularity\nconditions, the least squares estimator is strong consistent uniformly on the\nprior set. An illustrative example is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 13:34:07 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fei", "Chen", ""], ["Fei", "Weiyin", ""]]}, {"id": "1904.12704", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Cram\\'{e}r-Rao-type Bound and Stam's Inequality for Discrete Random\n  Variables", "comments": "6pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variance and the entropy power of a continuous random variable are\nbounded from below by the reciprocal of its Fisher information through the\nCram\\'{e}r-Rao bound and the Stam's inequality respectively. In this note, we\nintroduce the Fisher information for discrete random variables and derive the\ndiscrete Cram\\'{e}r-Rao-type bound and the discrete Stam's inequality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 13:41:24 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 05:49:02 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1904.12775", "submitter": "Nick Koning", "authors": "Nick Koning and Paul Bekker", "title": "Exact Testing of Many Moment Inequalities Against Multiple Violations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of testing many moment inequalities, where\nthe number of moment inequalities ($p$) is possibly larger than the sample size\n($n$). Chernozhukov et al. (2019) proposed asymptotic tests for this problem\nusing the maximum $t$ statistic. We observe that such tests can have low power\nif multiple inequalities are violated. As an alternative, we propose novel\nrandomization tests based on a maximum non-negatively weighted combination of\n$t$ statistics. We provide a condition guaranteeing size control in large\nsamples. Simulations show that the tests control size in small samples ($n =\n30$, $p = 1000$), and often has substantially higher power against alternatives\nwith multiple violations than tests based on the maximum $t$ statistic.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:33:12 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 09:30:39 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 12:40:21 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Koning", "Nick", ""], ["Bekker", "Paul", ""]]}, {"id": "1904.12810", "submitter": "Florent Chatelain", "authors": "Florent Chatelain and Nicolas Le Bihan and Jonathan H. Manton", "title": "Asymptotic regime for impropriety tests of complex random vectors", "comments": "11 pages, 8 figures, submitted to IEEE TSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impropriety testing for complex-valued vector has been considered lately due\nto potential applications ranging from digital communications to complex media\nimaging. This paper provides new results for such tests in the asymptotic\nregime, i.e. when the vector dimension and sample size grow commensurately to\ninfinity. The studied tests are based on invariant statistics named impropriety\ncoefficients. Limiting distributions for these statistics are derived, together\nwith those of the Generalized Likelihood Ratio Test (GLRT) and Roy's test, in\nthe Gaussian case. This characterization in the asymptotic regime allows also\nto identify a phase transition in Roy's test with potential application in\ndetection of complex-valued low-rank subspace corrupted by proper noise in\nlarge datasets. Simulations illustrate the accuracy of the proposed asymptotic\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:56:33 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 11:46:15 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 10:31:35 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chatelain", "Florent", ""], ["Bihan", "Nicolas Le", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "1904.12891", "submitter": "Zijian Guo", "authors": "Tianxi Cai, Tony Cai, Zijian Guo", "title": "Optimal Statistical Inference for Individualized Treatment Effects in\n  High-dimensional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict individualized treatment effects (ITEs) based on a\ngiven patient's profile is essential for personalized medicine. We propose a\nhypothesis testing approach to choosing between two potential treatments for a\ngiven individual in the framework of high-dimensional linear models. The\nmethodological novelty lies in the construction of a debiased estimator of the\nITE and establishment of its asymptotic normality uniformly for an arbitrary\nfuture high-dimensional observation, while the existing methods can only handle\ncertain specific forms of observations. We introduce a testing procedure with\nthe type-I error controlled and establish its asymptotic power. The proposed\nmethod can be extended to making inference for general linear contrasts,\nincluding both the average treatment effect and outcome prediction. We\nintroduce the optimality framework for hypothesis testing from both the\nminimaxity and adaptivity perspectives and establish the optimality of the\nproposed procedure. An extension to high-dimensional approximate linear models\nis also considered. The finite sample performance of the procedure is\ndemonstrated in simulation studies and further illustrated through an analysis\nof electronic health records data from patients with rheumatoid arthritis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:20:15 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 22:49:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cai", "Tianxi", ""], ["Cai", "Tony", ""], ["Guo", "Zijian", ""]]}, {"id": "1904.12897", "submitter": "Zijian Guo", "authors": "Zijian Guo, Cun-Hui Zhang", "title": "Extreme Eigenvalues of Nonlinear Correlation Matrices with Applications\n  to Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum correlation of functions of a pair of random variables is an\nimportant measure of stochastic dependence. It is known that this maximum\nnonlinear correlation is identical to the absolute value of the Pearson\ncorrelation for a pair of Gaussian random variables or a pair of finite sums of\niid random variables. This paper extends these results to pairwise Gaussian\nvectors and processes, nested sums of iid random variables, and permutation\nsymmetric functions of sub-groups of iid random variables. It also discusses\napplications to additive regression models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:26:37 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 13:58:19 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Guo", "Zijian", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1904.12994", "submitter": "Lalit Jain", "authors": "Jordan S. Ellenberg and Lalit Jain", "title": "Convergence rates for ordinal embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove optimal bounds for the convergence rate of ordinal embedding (also\nknown as non-metric multidimensional scaling) in the 1-dimensional case. The\nexamples witnessing optimality of our bounds arise from a result in additive\nnumber theory on sets of integers with no three-term arithmetic progressions.\nWe also carry out some computational experiments aimed at developing a sense of\nwhat the convergence rate for ordinal embedding might look like in higher\ndimensions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 00:01:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Ellenberg", "Jordan S.", ""], ["Jain", "Lalit", ""]]}, {"id": "1904.13147", "submitter": "William Dunsmuir", "authors": "Simon Clinet, William T.M. Dunsmuir, Gareth W. Peters and Kylie-Anne\n  Richards", "title": "Asymptotic Distribution of the Score Test for Detecting Marks in Hawkes\n  Processes", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic distribution of the score test of the null hypothesis that\nmarks do not impact the intensity of a Hawkes marked self-exciting point\nprocess is shown to be chi-squared. For local asymptotic power, the\ndistribution against local alternatives is also established as non-central\nchi-squared. These asymptotic results are derived using existing asymptotic\nresults for likelihood estimates of the unmarked Hawkes process model together\nwith mild additional conditions on the moments and ergodicity of the marks\nprocess and an additional uniform boundedness assumption, shown to be true for\nthe exponential decay Hawkes process.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 10:38:04 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Clinet", "Simon", ""], ["Dunsmuir", "William T. M.", ""], ["Peters", "Gareth W.", ""], ["Richards", "Kylie-Anne", ""]]}, {"id": "1904.13282", "submitter": "Aniket Biswas", "authors": "Aniket Biswas", "title": "Estimating the proportion of true null hypotheses with application in\n  microarray data", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new formulation for the proportion of true null hypotheses $(\\pi_0)$, based\non the sum of all $p$-values and the average of expected $p$-value under the\nfalse null hypotheses has been proposed in the current work. This formulation\nof the parameter of interest $\\pi_0$ has also been used to construct a new\nestimator for the same. The proposed estimator removes the problem of choosing\ntuning parameters in the existing estimators. Though the formulation is quite\ngeneral, computation of the new estimator demands use of an initial estimate of\n$\\pi_0$. The issue of choosing an appropriate initial estimator is also\ndiscussed in this work. The current work assumes normality of each gene\nexpression level and also assumes similar tests for all the hypotheses.\nExtensive simulation study shows that, the proposed estimator performs better\nthan its closest competitor, the estimator proposed in Cheng et al., 2015 over\na substantial continuous subinterval of the parameter space, under independence\nand weak dependence among the gene expression levels. The proposed method of\nestimation is applied to two real gene expression level data-sets and the\nresults are in line with what is obtained by the competing method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 14:44:56 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 13:08:52 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Biswas", "Aniket", ""]]}]