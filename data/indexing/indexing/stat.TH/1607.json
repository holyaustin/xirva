[{"id": "1607.00071", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen and Clayton D. Scott", "title": "An Operator Theoretic Approach to Nonparametric Mixture Models", "comments": "Contains and greatly extends the results from our previous work,\n  arXiv:1502.06644, and thus contains some overlap with that work. This version\n  contains some small grammatical and technical corrections as well as some\n  changes for improved clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating finite mixture models, it is common to make assumptions on\nthe mixture components, such as parametric assumptions. In this work, we make\nno distributional assumptions on the mixture components and instead assume that\nobservations from the mixture model are grouped, such that observations in the\nsame group are known to be drawn from the same mixture component. We precisely\ncharacterize the number of observations $n$ per group needed for the mixture\nmodel to be identifiable, as a function of the number $m$ of mixture\ncomponents. In addition to our assumption-free analysis, we also study the\nsettings where the mixture components are either linearly independent or\njointly irreducible. Furthermore, our analysis considers two kinds of\nidentifiability -- where the mixture model is the simplest one explaining the\ndata, and where it is the only one. As an application of these results, we\nprecisely characterize identifiability of multinomial mixture models. Our\nanalysis relies on an operator-theoretic framework that associates mixture\nmodels in the grouped-sample setting with certain infinite-dimensional tensors.\nBased on this framework, we introduce general spectral algorithms for\nrecovering the mixture components and illustrate their use on a synthetic data\nset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 23:01:37 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 02:59:07 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1607.00274", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Ryan Murray", "title": "A new analytical approach to consistency and overfitting in regularized\n  empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of binary classification: given training data\n$x_1, \\dots, x_n$ from a certain population, together with associated labels\n$y_1,\\dots, y_n \\in \\left\\{0,1 \\right\\}$, determine the best label for an\nelement $x$ not among the training data. More specifically, this work considers\na variant of the regularized empirical risk functional which is defined\nintrinsically to the observed data and does not depend on the underlying\npopulation. Tools from modern analysis are used to obtain a concise proof of\nasymptotic consistency as regularization parameters are taken to zero at rates\nrelated to the size of the sample. These analytical tools give a new framework\nfor understanding overfitting and underfitting, and rigorously connect the\nnotion of overfitting with a loss of compactness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:03:05 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Murray", "Ryan", ""]]}, {"id": "1607.00286", "submitter": "Mingli Chen", "authors": "Alexandre Belloni, Mingli Chen and Victor Chernozhukov", "title": "Quantile Graphical Models: Prediction and Conditional Independence with\n  Applications to Systemic Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two types of Quantile Graphical Models (QGMs) --- Conditional\nIndependence Quantile Graphical Models (CIQGMs) and Prediction Quantile\nGraphical Models (PQGMs). CIQGMs characterize the conditional independence of\ndistributions by evaluating the distributional dependence structure at each\nquantile index. As such, CIQGMs can be used for validation of the graph\nstructure in the causal graphical models (\\cite{pearl2009causality,\nrobins1986new, heckman2015causal}). One main advantage of these models is that\nwe can apply them to large collections of variables driven by non-Gaussian and\nnon-separable shocks. PQGMs characterize the statistical dependencies through\nthe graphs of the best linear predictors under asymmetric loss functions. PQGMs\nmake weaker assumptions than CIQGMs as they allow for misspecification. Because\nof QGMs' ability to handle large collections of variables and focus on specific\nparts of the distributions, we could apply them to quantify tail\ninterdependence. The resulting tail risk network can be used for measuring\nsystemic risk contributions that help make inroads in understanding\ninternational financial contagion and dependence structures of returns under\ndownside market movements.\n  We develop estimation and inference methods for QGMs focusing on the\nhigh-dimensional case, where the number of variables in the graph is large\ncompared to the number of observations. For CIQGMs, these methods and results\ninclude valid simultaneous choices of penalty functions, uniform rates of\nconvergence, and confidence regions that are simultaneously valid. We also\nderive analogous results for PQGMs, which include new results for penalized\nquantile regressions in high-dimensional settings to handle misspecification,\nmany controls, and a continuum of additional conditioning events.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:19:25 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 04:38:26 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 17:46:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chen", "Mingli", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1607.00292", "submitter": "Todd Kuffner", "authors": "Siddhartha Chib and Todd A. Kuffner", "title": "Bayes factor consistency", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good large sample performance is typically a minimum requirement of any model\nselection criterion. This article focuses on the consistency property of the\nBayes factor, a commonly used model comparison tool, which has experienced a\nrecent surge of attention in the literature. We thoroughly review existing\nresults. As there exists such a wide variety of settings to be considered, e.g.\nparametric vs. nonparametric, nested vs. non-nested, etc., we adopt the view\nthat a unified framework has didactic value. Using the basic marginal\nlikelihood identity of Chib (1995), we study Bayes factor asymptotics by\ndecomposing the natural logarithm of the ratio of marginal likelihoods into\nthree components. These are, respectively, log ratios of likelihoods, prior\ndensities, and posterior densities. This yields an interpretation of the log\nratio of posteriors as a penalty term, and emphasizes that to understand Bayes\nfactor consistency, the prior support conditions driving posterior consistency\nin each respective model under comparison should be contrasted in terms of the\nrates of posterior contraction they imply.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:41:40 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Chib", "Siddhartha", ""], ["Kuffner", "Todd A.", ""]]}, {"id": "1607.00393", "submitter": "David Kaplan", "authors": "David M. Kaplan (University of Missouri), Longhao Zhuo (Bank of\n  America)", "title": "Frequentist size of Bayesian inequality tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian and frequentist criteria are fundamentally different, but often\nposterior and sampling distributions are asymptotically equivalent (e.g.,\nGaussian). For the corresponding limit experiment, we characterize the\nfrequentist size of a certain Bayesian hypothesis test of (possibly nonlinear)\ninequalities. If the null hypothesis is that the (possibly\ninfinite-dimensional) parameter lies in a certain half-space, then the Bayesian\ntest's size is $\\alpha$; if the null hypothesis is a subset of a half-space,\nthen size is above $\\alpha$ (sometimes strictly); and in other cases, size may\nbe above, below, or equal to $\\alpha$. Two examples illustrate our results:\ntesting stochastic dominance and testing curvature of a translog cost function.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 20:02:05 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 20:40:33 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 16:44:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kaplan", "David M.", "", "University of Missouri"], ["Zhuo", "Longhao", "", "Bank of\n  America"]]}, {"id": "1607.00550", "submitter": "Maxim Raginsky", "authors": "Aolin Xu and Maxim Raginsky", "title": "Information-Theoretic Lower Bounds on Bayes Risk in Decentralized\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive lower bounds on the Bayes risk in decentralized estimation, where\nthe estimator does not have direct access to the random samples generated\nconditionally on the random parameter of interest, but only to the data\nreceived from local processors that observe the samples. The received data are\nsubject to communication constraints, due to quantization and the noise in the\ncommunication channels from the processors to the estimator. We first derive\ngeneral lower bounds on the Bayes risk using information-theoretic quantities,\nsuch as mutual information, information density, small ball probability, and\ndifferential entropy. We then apply these lower bounds to the decentralized\ncase, using strong data processing inequalities to quantify the contraction of\ninformation due to communication constraints. We treat the cases of a single\nprocessor and of multiple processors, where the samples observed by different\nprocessors may be conditionally dependent given the parameter, for\nnoninteractive and interactive communication protocols. Our results recover and\nimprove recent lower bounds on the Bayes risk and the minimax risk for certain\ndecentralized estimation problems, where previously only conditionally\nindependent sample sets and noiseless channels have been considered. Moreover,\nour results provide a general way to quantify the degradation of estimation\nperformance caused by distributing resources to multiple processors, which is\nonly discussed for specific examples in existing works.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 18:45:27 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Xu", "Aolin", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1607.00624", "submitter": "Alexander Tartakovsky", "authors": "Chen-Der Fuh and Alexander G. Tartakovsky", "title": "Asymptotic Bayesian Theory of Quickest Change Detection for Hidden\n  Markov Models", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 1960s, Shiryaev developed a Bayesian theory of change-point detection\nin the i.i.d. case, which was generalized in the beginning of the 2000s by\nTartakovsky and Veeravalli for general stochastic models assuming a certain\nstability of the log-likelihood ratio process. Hidden Markov models represent a\nwide class of stochastic processes that are very useful in a variety of\napplications. In this paper, we investigate the performance of the Bayesian\nShiryaev change-point detection rule for hidden Markov models. We propose a set\nof regularity conditions under which the Shiryaev procedure is first-order\nasymptotically optimal in a Bayesian context, minimizing moments of the\ndetection delay up to certain order asymptotically as the probability of false\nalarm goes to zero. The developed theory for hidden Markov models is based on\nMarkov chain representation for the likelihood ratio and r-quick convergence\nfor Markov random walks. In addition, applying Markov nonlinear renewal theory,\nwe present a high-order asymptotic approximation for the expected delay to\ndetection of the Shiryaev detection rule. Asymptotic properties of another\npopular change detection rule, the Shiryaev{Roberts rule, is studied as well.\nSome interesting examples are given for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 11:19:39 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Fuh", "Chen-Der", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1607.00673", "submitter": "Marianna Pensky", "authors": "Marianna Pensky", "title": "Dynamic network models and graphon estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider a dynamic stochastic network model. The\nobjective is estimation of the tensor of connection probabilities $\\Lambda$\nwhen it is generated by a Dynamic Stochastic Block Model (DSBM) or a dynamic\ngraphon. In particular, in the context of the DSBM, we derive a penalized least\nsquares estimator $\\widehat{\\Lambda}$ of $\\Lambda$ and show that\n$\\widehat{\\Lambda}$ satisfies an oracle inequality and also attains minimax\nlower bounds for the risk. We extend those results to estimation of $\\Lambda$\nwhen it is generated by a dynamic graphon function. The estimators constructed\nin the paper are adaptive to the unknown number of blocks in the context of the\nDSBM or to the smoothness of the graphon function. The technique relies on the\nvectorization of the model and leads to much simpler mathematical arguments\nthan the ones used previously in the stationary set up. In addition, all\nresults in the paper are non-asymptotic and allow a variety of extensions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 19:30:56 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 00:32:59 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Pensky", "Marianna", ""]]}, {"id": "1607.00696", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos", "title": "Variational limits of k-NN graph based functionals on data clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the large sample asymptotics of data analysis procedures\nbased on the optimization of functionals defined on $k$-NN graphs on point\nclouds. The paper is framed in the context of minimization of balanced cut\nfunctionals, but our techniques, ideas and results can be adapted to other\nfunctionals of relevance. We rigorously show that provided the number of\nneighbors in the graph $k:=k_n$ scales with the number of points in the cloud\nas $n \\gg k_n \\gg \\log(n)$, then with probability one, the solution to the\ngraph cut optimization problem converges towards the solution of an analogue\nvariational problem at the continuum level.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 22:51:48 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 16:18:44 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 20:45:05 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Trillos", "Nicolas Garcia", ""]]}, {"id": "1607.00743", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank\n  Designs", "comments": "The main text of this paper was published at NIPS 2014. Proofs are\n  included here in the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the residual bootstrap (RB) method in the context of\nhigh-dimensional linear regression. Specifically, we analyze the distributional\napproximation of linear contrasts $c^{\\top} (\\hat{\\beta}_{\\rho}-\\beta)$, where\n$\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression\ncoefficients are estimated via least squares, classical results show that RB\nconsistently approximates the laws of contrasts, provided that $p\\ll n$, where\nthe design matrix is of size $n\\times p$. Up to now, relatively little work has\nconsidered how additional structure in the linear model may extend the validity\nof RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version\nof RB that resamples residuals obtained from ridge regression. Our main\nstructural assumption on the design matrix is that it is nearly low rank --- in\nthe sense that its singular values decay according to a power-law profile.\nUnder a few extra technical assumptions, we derive a simple criterion for\nensuring that RB consistently approximates the law of a given contrast. We then\nspecialize this result to study confidence intervals for mean response values\n$X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More\nprecisely, we show that conditionally on a Gaussian design with near low-rank\nstructure, RB simultaneously approximates all of the laws\n$X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also\nnotable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since\nour consistency results are formulated in terms of the Mallows (Kantorovich)\nmetric, the existence of a limiting distribution is not required.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 05:50:19 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1607.00806", "submitter": "Sergey Dovgal", "authors": "Sergey Dovgal", "title": "Towards Model Selection for Local Log-Density Estimation. Fisher and\n  Wilks-type theorems", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this research is to make a step towards providing a tool for model\nselection for log-density estimation. The author revisits the procedure for\nlocal log-density estimation suggested by Clive Loader (1996) and extends the\ntheoretical results to finite-sample framework with the help of machinery of\nSpokoiny (2012). The results include bias expression from \"deterministic\"\ncounterpart and Fisher and Wilks-type theorems from \"stochastic\". We elaborate\non bandwidth trade-off $ h(n) = \\arg\\min O(h^p) + O_p(1/\\sqrt{nh^d}) $ with\nexplicit constants at big O notation.\n  Explicit expressions involve (i) true density function and (ii) model that is\nselected (dimension, bandwidth, kernel and basis, e.g. polynomial). Existing\nasymptotic properties directly follow from our results. From the expressions\nobtained it is possible to control \"the curse of dimension\" both from the side\nof log-density smoothness and the inner space dimension.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 10:04:26 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Dovgal", "Sergey", ""]]}, {"id": "1607.00864", "submitter": "Paul Rochet", "authors": "Fr\\'ed\\'eric Lavancier (SERPICO, LMJL), P Rochet (LMJL)", "title": "A tutorial on estimator averaging in spatial point process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that several competing methods are available to estimate a parameter\nin a given statistical model. The aim of estimator averaging is to provide a\nnew estimator, built as a linear combination of the initial estimators, that\nachieves better properties, under the quadratic loss, than each individual\ninitial estimator. This contribution provides an accessible and clear overview\nof the method, and investigates its performances on standard spatial point\nprocess models. It is demonstrated that the average estimator clearly improves\non standard procedures for the considered models. For each example, the code to\nimplement the method with the R software (which only consists of few lines) is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 12:47:13 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:59:41 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 12:28:46 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", "", "SERPICO, LMJL"], ["Rochet", "P", "", "LMJL"]]}, {"id": "1607.00896", "submitter": "Vladimir Panov", "authors": "Denis Belomestny, Vladimir Panov, and Jeannette Woerner", "title": "Low frequency estimation of continuous-time moving average L\\'evy\n  processes", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of statistical inference for a\ncontinuous-time moving average L\\'evy process of the form $$Z_{t} =\n\\int_{\\mathbb{R}}\\mathcal{K}(t-s)\\, dL_{s},\\quad t\\in\\mathbb{R}$$ with a\ndeterministic kernel (\\K\\) and a L{\\'e}vy process (L\\). Especially the\nestimation of the L\\'evy measure (\\nu\\) of $L$ from low-frequency observations\nof the process $Z$ is considered. We construct a consistent estimator, derive\nits convergence rates and illustrate its performance by a numerical example. On\nthe technical level, the main challenge is to establish a kind of exponential\nmixing for continuous-time moving average L\\'evy processes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 14:11:14 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 08:28:29 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Belomestny", "Denis", ""], ["Panov", "Vladimir", ""], ["Woerner", "Jeannette", ""]]}, {"id": "1607.00963", "submitter": "Yousri Slaoui", "authors": "Yousri Slaoui", "title": "Optimal bandwidth selection for semi-recursive kernel regression\n  estimators", "comments": "arXiv admin note: text overlap with arXiv:1606.06988,\n  arXiv:1606.07948", "journal-ref": "Statistics and Its Interface Volume 9 (2016), Number 3, Pages\n  375-388", "doi": "10.4310/SII.2016.v9.n3.a11", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an automatic selection of the bandwidth of the\nsemi-recursive kernel estimators of a regression function defined by the\nstochastic approximation algorithm. We showed that, using the selected\nbandwidth and some special stepsizes, the proposed semi-recursive estimators\nwill be very competitive to the nonrecursive one in terms of estimation error\nbut much better in terms of computational costs. We corroborated these\ntheoretical results through simulation study and a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 17:12:00 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Slaoui", "Yousri", ""]]}, {"id": "1607.01251", "submitter": "Jiahua Chen", "authors": "Jiahua Chen", "title": "Consistency of the MLE under mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large-sample properties of likelihood-based statistical inference under\nmixture models have received much attention from statisticians. Although the\nconsistency of the nonparametric MLE is regarded as a standard conclusion, many\nresearchers ignore the precise conditions required on the mixture model. An\nincorrect claim of consistency can lead to false conclusions even if the\nmixture model under investigation seems well behaved. Under a finite normal\nmixture model, for instance, the consistency of the plain MLE is often\nerroneously assumed in spite of recent research breakthroughs. This paper\nstreamlines the consistency results for the nonparametric MLE in general, and\nin particular for the penalized MLE under finite normal mixture models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 13:59:40 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Chen", "Jiahua", ""]]}, {"id": "1607.01434", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Andrew R. Barron", "title": "Risk Bounds for High-dimensional Ridge Function Combinations Including\n  Neural Networks", "comments": "Submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $ f^{\\star} $ be a function on $ \\mathbb{R}^d $ with an assumption of a\nspectral norm $ v_{f^{\\star}} $. For various noise settings, we show that $\n\\mathbb{E}\\|\\hat{f} - f^{\\star} \\|^2 \\leq \\left(v^4_{f^{\\star}}\\frac{\\log\nd}{n}\\right)^{1/3} $, where $ n $ is the sample size and $ \\hat{f} $ is either\na penalized least squares estimator or a greedily obtained version of such\nusing linear combinations of sinusoidal, sigmoidal, ramp, ramp-squared or other\nsmooth ridge functions. The candidate fits may be chosen from a continuum of\nfunctions, thus avoiding the rigidity of discretizations of the parameter\nspace. On the other hand, if the candidate fits are chosen from a\ndiscretization, we show that $ \\mathbb{E}\\|\\hat{f} - f^{\\star} \\|^2 \\leq\n\\left(v^3_{f^{\\star}}\\frac{\\log d}{n}\\right)^{2/5} $. This work bridges\nnon-linear and non-parametric function estimation and includes single-hidden\nlayer nets. Unlike past theory for such settings, our bound shows that the risk\nis small even when the input dimension $ d $ of an infinite-dimensional\nparameterized dictionary is much larger than the available sample size. When\nthe dimension is larger than the cube root of the sample size, this quantity is\nseen to improve the more familiar risk bound of $\nv_{f^{\\star}}\\left(\\frac{d\\log (n/d)}{n}\\right)^{1/2} $, also investigated\nhere.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 22:41:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 03:15:22 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 18:27:58 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 19:53:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1607.01516", "submitter": "Jean-Michel Loubes", "authors": "A-C Brunet (IMT), J-M Azais (IMT), J-M Loubes (IMT), J Amar (I2MC), R\n  Burcelin (I2MC)", "title": "A new gene co-expression network analysis based on Core Structure\n  Detection (CSD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to cluster gene networks. Based on a dissimilarity\nbuilt using correlation structures, we consider networks that connect all the\ngenes based on the strength of their dissimilarity. The large number of genes\nrequire the use of the threshold to find sparse structures in the graph. in\nthis work, using the notion of graph coreness, we identify clusters of genes\nwhich are central in the network. Then we estimate a network that has these\ngenes as main hubs. We use this new representation to identify biologically\nmeaningful clusters, and to highlight the importance of the nodes that compose\nthe core structures based on biological interpretations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 08:23:59 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Brunet", "A-C", "", "IMT"], ["Azais", "J-M", "", "IMT"], ["Loubes", "J-M", "", "IMT"], ["Amar", "J", "", "I2MC"], ["Burcelin", "R", "", "I2MC"]]}, {"id": "1607.01718", "submitter": "Justin Eldridge", "authors": "Justin Eldridge, Mikhail Belkin, Yusu Wang", "title": "Graphons, mergeons, and so on!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a theory of hierarchical clustering for graphs. Our\nmodeling assumption is that graphs are sampled from a graphon, which is a\npowerful and general model for generating graphs and analyzing large networks.\nGraphons are a far richer class of graph models than stochastic blockmodels,\nthe primary setting for recent progress in the statistical theory of graph\nclustering. We define what it means for an algorithm to produce the \"correct\"\nclustering, give sufficient conditions in which a method is statistically\nconsistent, and provide an explicit algorithm satisfying these properties.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 01:05:41 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 01:03:26 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 20:07:27 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eldridge", "Justin", ""], ["Belkin", "Mikhail", ""], ["Wang", "Yusu", ""]]}, {"id": "1607.01853", "submitter": "Fang Han", "authors": "Fang Han, Sheng Xu, and Wen-Xin Zhou", "title": "On Gaussian Comparison Inequality and Its Application to Spectral\n  Analysis of Large Random Matrices", "comments": "to appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chernozhukov, Chetverikov, and Kato [Ann. Statist. 42 (2014)\n1564--1597] developed a new Gaussian comparison inequality for approximating\nthe suprema of empirical processes. This paper exploits this technique to\ndevise sharp inference on spectra of large random matrices. In particular, we\nshow that two long-standing problems in random matrix theory can be solved: (i)\nsimple bootstrap inference on sample eigenvalues when true eigenvalues are\ntied; (ii) conducting two-sample Roy's covariance test in high dimensions. To\nestablish the asymptotic results, a generalized $\\epsilon$-net argument\nregarding the matrix rescaled spectral norm and several new empirical process\nbounds are developed and of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 02:09:30 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 18:54:26 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:39:43 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Han", "Fang", ""], ["Xu", "Sheng", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1607.01892", "submitter": "St\\'ephanie van der Pas", "authors": "St\\'ephanie van der Pas and Botond Szab\\'o and Aad van der Vaart", "title": "Uncertainty quantification for the horseshoe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the credible sets and marginal credible intervals resulting\nfrom the horseshoe prior in the sparse multivariate normal means model. We do\nso in an adaptive setting without assuming knowledge of the sparsity level\n(number of signals). We consider both the hierarchical Bayes method of putting\na prior on the unknown sparsity level and the empirical Bayes method with the\nsparsity level estimated by maximum marginal likelihood. We show that credible\nballs and marginal credible intervals have good frequentist coverage and\noptimal size if the sparsity level of the prior is set correctly. By general\ntheory honest confidence sets cannot adapt in size to an unknown sparsity\nlevel. Accordingly the hierarchical and empirical Bayes credible sets based on\nthe horseshoe prior are not honest over the full parameter space. We show that\nthis is due to over-shrinkage for certain parameters and characterise the set\nof parameters for which credible balls and marginal credible intervals do give\ncorrect uncertainty quantification. In particular we show that the fraction of\nfalse discoveries by the marginal Bayesian procedure is controlled by a correct\nchoice of cut-off.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 07:23:35 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 10:01:21 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["van der Pas", "St\u00e9phanie", ""], ["Szab\u00f3", "Botond", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1607.02066", "submitter": "Stefano Favaro", "authors": "Marco Battiston, Stefano Favaro, Daniel M. Roy, Yee Whye Teh", "title": "A characterization of product-form exchangeable feature probability\n  functions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the class of exchangeable feature allocations assigning\nprobability $V_{n,k}\\prod_{l=1}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature\nallocation of $n$ individuals, displaying $k$ features with counts\n$(m_{1},\\ldots,m_{k})$ for these features. Each element of this class is\nparametrized by a countable matrix $V$ and two sequences $U$ and $W$ of\nnon-negative weights. Moreover, a consistency condition is imposed to guarantee\nthat the distribution for feature allocations of $n-1$ individuals is recovered\nfrom that of $n$ individuals, when the last individual is integrated out. In\nTheorem 1.1, we prove that the only members of this class satisfying the\nconsistency condition are mixtures of the Indian Buffet Process over its mass\nparameter $\\gamma$ and mixtures of the Beta--Bernoulli model over its\ndimensionality parameter $N$. Hence, we provide a characterization of these two\nmodels as the only, up to randomization of the parameters, consistent\nexchangeable feature allocations having the required product form.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 16:07:09 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Battiston", "Marco", ""], ["Favaro", "Stefano", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1607.02201", "submitter": "Zhou Fan", "authors": "Zhou Fan and Iain M. Johnstone", "title": "Eigenvalue distributions of variance components estimators in\n  high-dimensional random effects models", "comments": "v2: clarify exposition, theorem statements, and proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the spectra of MANOVA estimators for variance component covariance\nmatrices in multivariate random effects models. When the dimensionality of the\nobservations is large and comparable to the number of realizations of each\nrandom effect, we show that the empirical spectra of such estimators are\nwell-approximated by deterministic laws. The Stieltjes transforms of these laws\nare characterized by systems of fixed-point equations, which are numerically\nsolvable by a simple iterative procedure. Our proof uses operator-valued free\nprobability theory, and we establish a general asymptotic freeness result for\nfamilies of rectangular orthogonally-invariant random matrices, which is of\nindependent interest. Our work is motivated by the estimation of components of\ncovariance between multiple phenotypic traits in quantitative genetics, and we\nspecialize our results to common experimental designs that arise in this\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 00:10:16 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 17:06:37 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fan", "Zhou", ""], ["Johnstone", "Iain M.", ""]]}, {"id": "1607.02245", "submitter": "Tingting Li", "authors": "Tingting Li, Zuoxiang Peng", "title": "Moments convergence of powered normal extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, convergence for moments of powered normal extremes is\nconsidered under an optimal choice of normalizing constants. It is shown that\nthe rates of convergence for normalized powered normal extremes depend on the\npower index. However, the dependence disappears for higher-order expansions of\nmoments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:15:06 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Li", "Tingting", ""], ["Peng", "Zuoxiang", ""]]}, {"id": "1607.02248", "submitter": "Oliver Lang", "authors": "Oliver Lang, Mario Huemer and Christian Hofbauer", "title": "On the Log-Likelihood Ratio Evaluation of CWCU Linear and Widely Linear\n  MMSE Data Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In soft decoding of data bits, the log-likelihood ratios are evaluated from\nthe estimated data symbols. For proper constellation diagrams such as QPSK or\n16-QAM, these data symbols are often estimated using the linear minimum mean\nsquare error (LMMSE) estimator. The LMMSE estimator only fulfills the weak\nBayesian unbiasedness constraint. Recently, estimators fulfilling the more\nstringent component-wise conditionally unbiased (CWCU) constraints have been\ninvestigated, such as the CWCU LMMSE estimator. In this paper, we prove that\nthe CWCU LMMSE estimates result in the very same log-likelihood ratios as the\nLMMSE estimates. For improper constellation diagrams such as 8-QAM, widely\nlinear estimators are used. For this case, we show that the widely linear\nversions of the LMMSE estimator and the CWCU LMMSE estimator also yield\nidentical log-likelihood ratios. Finally, we give a simulation example which\nillustrates a number of interesting properties of the discussed widely linear\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 06:38:09 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 08:22:32 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Lang", "Oliver", ""], ["Huemer", "Mario", ""], ["Hofbauer", "Christian", ""]]}, {"id": "1607.02387", "submitter": "Gilles Blanchard", "authors": "Gilles Blanchard, Nicole Kr\\\"amer", "title": "Convergence rates of Kernel Conjugate Gradient for random design\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove statistical rates of convergence for kernel-based least squares\nregression from i.i.d. data using a conjugate gradient algorithm, where\nregularization against overfitting is obtained by early stopping. This method\nis related to Kernel Partial Least Squares, a regression method that combines\nsupervised dimensionality reduction with least squares projection. Following\nthe setting introduced in earlier related literature, we study so-called \"fast\nconvergence rates\" depending on the regularity of the target regression\nfunction (measured by a source condition in terms of the kernel integral\noperator) and on the effective dimensionality of the data mapped into the\nkernel space. We obtain upper bounds, essentially matching known minimax lower\nbounds, for the $\\mathcal{L}^2$ (prediction) norm as well as for the stronger\nHilbert norm, if the true regression function belongs to the reproducing kernel\nHilbert space. If the latter assumption is not fulfilled, we obtain similar\nconvergence rates for appropriate norms, provided additional unlabeled data are\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 14:41:17 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kr\u00e4mer", "Nicole", ""]]}, {"id": "1607.02435", "submitter": "Philippe Rigollet", "authors": "Nicolas Flammarion, Cheng Mao and Philippe Rigollet", "title": "Optimal Rates of Statistical Seriation", "comments": "V2 corrects an error in Lemma A.1, v3 corrects appendix F on unimodal\n  regression where the bounds now hold with polynomial probability rather than\n  exponential", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix the seriation problem consists in permuting its rows in such\nway that all its columns have the same shape, for example, they are monotone\nincreasing. We propose a statistical approach to this problem where the matrix\nof interest is observed with noise and study the corresponding minimax rate of\nestimation of the matrices. Specifically, when the columns are either unimodal\nor monotone, we show that the least squares estimator is optimal up to\nlogarithmic factors and adapts to matrices with a certain natural structure.\nFinally, we propose a computationally efficient estimator in the monotonic case\nand study its performance both theoretically and experimentally. Our work is at\nthe intersection of shape constrained estimation and recent work that involves\npermutation learning, such as graph denoising and ranking.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:12:47 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 19:59:56 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 14:24:26 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Flammarion", "Nicolas", ""], ["Mao", "Cheng", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1607.02538", "submitter": "John Harlim", "authors": "Mich\\`ele De La Chevroti\\`ere and John Harlim", "title": "A data-driven method for improving the correlation estimation in serial\n  ensemble Kalman filters", "comments": "12 figures", "journal-ref": null, "doi": "10.1175/MWR-D-16-0109.1", "report-no": null, "categories": "math.ST math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data-driven method for improving the correlation estimation in serial\nensemble Kalman filters is introduced. The method finds a linear map that\ntransforms, at each assimilation cycle, the poorly estimated sample correlation\ninto an improved correlation. This map is obtained from an offline training\nprocedure without any tuning as the solution of a linear regression problem\nthat uses appropriate sample correlation statistics obtained from historical\ndata assimilation products. In an idealized OSSE with the Lorenz-96 model and\nfor a range of cases of linear and nonlinear observation models, the proposed\nscheme improves the filter estimates, especially when the ensemble size is\nsmall relative to the dimension of the state space.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 22:19:14 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 16:15:31 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["De La Chevroti\u00e8re", "Mich\u00e8le", ""], ["Harlim", "John", ""]]}, {"id": "1607.02604", "submitter": "Adil Ahidar-Coutrix", "authors": "Adil Ahidar-Coutrix, Philippe Berthet", "title": "Convergence of Multivariate Quantile Surfaces", "comments": "version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the quantile set of order $\\alpha \\in \\left[ 1/2,1\\right) $\nassociated to a law $P$ on $\\mathbb{R}^{d}$ to be the collection of its\ndirectional quantiles seen from an observer $O\\in \\mathbb{R}^{d}$. Under\nminimal assumptions these star-shaped sets are closed surfaces, continuous in\n$(O,\\alpha )$ and the collection of empirical quantile surfaces is uniformly\nconsistent.\\ Under mild assumptions -- no density or symmetry is required for\n$P$ -- our uniform central limit theorem reveals the correlations between\nquantile points and a non asymptotic Gaussian approximation provides joint\nconfident enlarged quantile surfaces. Our main result is a dimension free rate\n$n^{-1/4} (\\log n)^{1/2}(\\log\\log n) ^{1/4} $ of Bahadur-Kiefer embedding by\nthe empirical process indexed by half-spaces. These limit theorems sharply\ngeneralize the univariate quantile convergences and fully characterize the\njoint behavior of Tukey half-spaces.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 12:36:54 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 13:39:46 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ahidar-Coutrix", "Adil", ""], ["Berthet", "Philippe", ""]]}, {"id": "1607.02613", "submitter": "Shirin Jalali", "authors": "Shirin Jalali, Arian Maleki", "title": "New approach to Bayesian high-dimensional linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating parameters $X^n \\in \\mathbb{R}^n $,\ngenerated by a stationary process, from $m$ response variables $Y^m =\nAX^n+Z^m$, under the assumption that the distribution of $X^n$ is known. This\nis the most general version of the Bayesian linear regression problem. The lack\nof computationally feasible algorithms that can employ generic prior\ndistributions and provide a good estimate of $X^n$ has limited the set of\ndistributions researchers use to model the data. In this paper, a new scheme\ncalled Q-MAP is proposed. The new method has the following properties: (i) It\nhas similarities to the popular MAP estimation under the noiseless setting.\n(ii) In the noiseless setting, it achieves the \"asymptotically optimal\nperformance\" when $X^n$ has independent and identically distributed components.\n(iii) It scales favorably with the dimensions of the problem and therefore is\napplicable to high-dimensional setups. (iv) The solution of the Q-MAP\noptimization can be found via a proposed iterative algorithm which is provably\nrobust to the error (noise) in the response variables.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 13:51:01 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 19:58:22 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Jalali", "Shirin", ""], ["Maleki", "Arian", ""]]}, {"id": "1607.02623", "submitter": "Ed Furman Eduard Furman", "authors": "Edward Furman and Ricardas Zitikis", "title": "Beyond the Pearson correlation: heavy-tailed risks, weighted Gini\n  correlations, and a Gini-type weighted insurance pricing model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gini-type correlation coefficients have become increasingly important in a\nvariety of research areas, including economics, insurance and finance, where\nmodelling with heavy-tailed distributions is of pivotal importance. In such\nsituations, naturally, the classical Pearson correlation coefficient is of\nlittle use. On the other hand, it has been observed that when light-tailed\nsituations are of interest, and hence when both the Gini-type and Pearson\ncorrelation coefficients are well-defined and finite, then these coefficients\nare related and sometimes even coincide. In general, understanding how the\ncorrelation coefficients above are related has been an illusive task. In this\npaper we put forward arguments that establish such a connection via certain\nregression-type equations. This, in turn, allows us to introduce a Gini-type\nWeighted Insurance Pricing Model that works in heavy-tailed situation and thus\nprovides a natural alternative to the classical Capital Asset Pricing Model. We\nillustrate our theoretical considerations using several bivariate\ndistributions, such as elliptical and those with heavy-tailed Pareto margins.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 15:11:16 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Furman", "Edward", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1607.02630", "submitter": "Xiaoying Tian", "authors": "Xiaoying Tian, Nan Bi and Jonathan Taylor", "title": "MAGIC: a general, powerful and tractable method for selective inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective inference is a recent research topic that tries to perform valid\ninference after using the data to select a reasonable statistical model. We\npropose MAGIC, a new method for selective inference that is general, powerful\nand tractable. MAGIC is a method for selective inference after solving a convex\noptimization problem with smooth loss and $\\ell_1$ penalty. Randomization is\nincorporated into the optimization problem to boost statistical power. Through\nreparametrization, MAGIC reduces the problem into a sampling problem with\nsimple constraints. MAGIC applies to many $\\ell_1$ penalized optimization\nproblem including the Lasso, logistic Lasso and neighborhood selection in\ngraphical models, all of which we consider in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 16:13:17 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Tian", "Xiaoying", ""], ["Bi", "Nan", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1607.02689", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "On the Unique Crossing Conjecture of Diaconis and Perlman on\n  Convolutions of Gamma Random Variables", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diaconis and Perlman (1990) conjecture that the distribution functions of two\nweighted sums of iid gamma random variables cross exactly once if one weight\nvector majorizes the other. We disprove this conjecture when the shape\nparameter of the gamma variates is $\\alpha <1$ and prove it when $\\alpha\\geq\n1$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 03:56:15 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "1607.02727", "submitter": "Tibor Pog\\'any K.", "authors": "Tibor K. Pog\\'any", "title": "Integral form of the COM-Poisson normalization constant", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief note an integral expression is presented for the COM-Poisson\nrenormalization constant $Z(\\lambda, \\nu)$ on the real axis.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 10:03:44 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Pog\u00e1ny", "Tibor K.", ""]]}, {"id": "1607.02833", "submitter": "Xavier Pennec", "authors": "Xavier Pennec (ASCLEPIOS)", "title": "Barycentric Subspace Analysis on Manifolds", "comments": "Annals of Statistics, Institute of Mathematical Statistics, A\n  Para\\^itre", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the generalization of Principal Component Analysis\n(PCA) to Riemannian manifolds. We first propose a new and general type of\nfamily of subspaces in manifolds that we call barycentric subspaces. They are\nimplicitly defined as the locus of points which are weighted means of $k+1$\nreference points. As this definition relies on points and not on tangent\nvectors, it can also be extended to geodesic spaces which are not Riemannian.\nFor instance, in stratified spaces, it naturally allows principal subspaces\nthat span several strata, which is impossible in previous generalizations of\nPCA. We show that barycentric subspaces locally define a submanifold of\ndimension k which generalizes geodesic subspaces.Second, we rephrase PCA in\nEuclidean spaces as an optimization on flags of linear subspaces (a hierarchy\nof properly embedded linear subspaces of increasing dimension). We show that\nthe Euclidean PCA minimizes the Accumulated Unexplained Variances by all the\nsubspaces of the flag (AUV). Barycentric subspaces are naturally nested,\nallowing the construction of hierarchically nested subspaces. Optimizing the\nAUV criterion to optimally approximate data points with flags of affine spans\nin Riemannian manifolds lead to a particularly appealing generalization of PCA\non manifolds called Barycentric Subspaces Analysis (BSA).\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 06:44:18 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 11:59:03 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Pennec", "Xavier", "", "ASCLEPIOS"]]}, {"id": "1607.02896", "submitter": "Matteo Ruggiero", "authors": "Omiros Papaspiliopoulos, Matteo Ruggiero and Dario Span\\`o", "title": "Conjugacy properties of time-evolving Dirichlet and gamma random\n  measures", "comments": "To appear on the Electronic Journal of Statistics. arXiv admin note:\n  text overlap with arXiv:1411.4944", "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 3452-3489", "doi": "10.1214/16-EJS1194", "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend classic characterisations of posterior distributions under\nDirichlet process and gamma random measures priors to a dynamic framework. We\nconsider the problem of learning, from indirect observations, two families of\ntime-dependent processes of interest in Bayesian nonparametrics: the first is a\ndependent Dirichlet process driven by a Fleming-Viot model, and the data are\nrandom samples from the process state at discrete times; the second is a\ncollection of dependent gamma random measures driven by a Dawson-Watanabe\nmodel, and the data are collected according to a Poisson point process with\nintensity given by the process state at discrete times. Both driving processes\nare diffusions taking values in the space of discrete measures whose support\nvaries with time, and are stationary and reversible with respect to Dirichlet\nand gamma priors respectively. A common methodology is developed to obtain in\nclosed form the time-marginal posteriors given past and present data. These are\nshown to belong to classes of finite mixtures of Dirichlet processes and gamma\nrandom measures for the two models respectively, yielding conjugacy of these\nclasses to the type of data we consider. We provide explicit results on the\nparameters of the mixture components and on the mixing weights, which are\ntime-varying and drive the mixtures towards the respective priors in absence of\nfurther data. Explicit algorithms are provided to recursively compute the\nparameters of the mixtures. Our results are based on the projective properties\nof the signals and on certain duality properties of their projections.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 10:49:28 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:02:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Ruggiero", "Matteo", ""], ["Span\u00f2", "Dario", ""]]}, {"id": "1607.03173", "submitter": "Lulu Fang", "authors": "Lulu Fang and Min Wu", "title": "A note on R\\'enyi's \"record\" problem and Engel's series", "comments": "7 pages. Some conclusions are added in this newest version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1973, Williams introduced two interesting discrete Markov processes,\nnamely $C$-processes and $A$-processes, which are related to record times in\nstatistics and Engel's series in number theory respectively. Moreover, he\nshowed that these two processes share the same classical limit theorems, such\nas the law of large numbers, central limit theorem and law of the iterated\nlogarithm. In this paper, we consider the large deviations for these two Markov\nprocesses, which indicate that there is a difference between $C$-processes and\n$A$-processes in the context of large deviations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 21:33:05 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 23:50:49 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 17:53:54 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Fang", "Lulu", ""], ["Wu", "Min", ""]]}, {"id": "1607.03294", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Asymptotic Near-Minimaxity of the Randomized Shiryaev-Roberts-Pollak\n  Change-Point Detection Procedure in Continuous Time", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the classical continuous-time quickest change-point detection problem it\nis shown that the randomized Shiryaev-Roberts-Pollak procedure is\nasymptotically nearly minimax-optimal (in the sense of Pollak 1985) in the\nclass of randomized procedures with vanishingly small false alarm risk. The\nproof is explicit in that all of the relevant performance characteristics are\nfound analytically and in a closed form. The rate of convergence to the\n(unknown) optimum is elucidated as well. The obtained optimality result is a\none-order improvement of that previously obtained by Burnaev et al. (2009) for\nthe very same problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:08:55 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 21:03:51 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 15:49:39 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 21:26:22 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1607.03569", "submitter": "Shuhei Mano", "authors": "Shuhei Mano", "title": "Partition structure and the A-hypergeometric distribution associated\n  with the rational normal curve", "comments": "36 pages, 2 figures", "journal-ref": "Electron J Statist (2017) 11: 4452-4487", "doi": "10.1214/17-EJS1361", "report-no": null, "categories": "math.ST math.AC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distribution whose normalization constant is an A-hypergeometric polynomial\nis called an A-hypergeometric distribution. Such a distribution is in turn a\ngeneralization of the generalized hypergeometric distribution on the\ncontingency tables with fixed marginal sums. In this paper, we will see that an\nA-hypergeometric distribution with a homogeneous matrix of two rows,\nespecially, that associated with the rational normal curve, appears in\ninferences involving exchangeable partition structures. An exact sampling\nalgorithm is presented for the general (any number of rows) A-hypergeometric\ndistributions. Then, the maximum likelihood estimation of the A-hypergeometric\ndistribution associated with the rational normal curve, which is an algebraic\nexponential family, is discussed. The information geometry of the Newton\npolytope is useful for analyzing the full and the curved exponential family.\nAlgebraic methods are provided for evaluating the A-hypergeometric polynomials.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 02:21:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 08:27:16 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 05:36:22 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 12:23:52 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Mano", "Shuhei", ""]]}, {"id": "1607.03990", "submitter": "Jerry Li", "authors": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt", "title": "Fast Algorithms for Segmented Regression", "comments": "27 pages, appeared in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fixed design segmented regression problem: Given noisy samples\nfrom a piecewise linear function $f$, we want to recover $f$ up to a desired\naccuracy in mean-squared error.\n  Previous rigorous approaches for this problem rely on dynamic programming\n(DP) and, while sample efficient, have running time quadratic in the sample\nsize. As our main contribution, we provide new sample near-linear time\nalgorithms for the problem that -- while not being minimax optimal -- achieve a\nsignificantly better sample-time tradeoff on large datasets compared to the DP\napproach. Our experimental evaluation shows that, compared with the DP\napproach, our algorithms provide a convergence rate that is only off by a\nfactor of $2$ to $4$, while achieving speedups of three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 04:52:53 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Acharya", "Jayadev", ""], ["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1607.04327", "submitter": "Georg Hahn", "authors": "Georg Hahn", "title": "Closure properties of classes of multiple testing procedures", "comments": null, "journal-ref": "AStA Adv Stat Anal (2018), 102(2):167--178", "doi": "10.1007/s10182-017-0297-0", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical discoveries are often obtained through multiple hypothesis\ntesting. A variety of procedures exists to evaluate multiple hypotheses, for\ninstance the ones of Benjamini-Hochberg, Bonferroni, Holm or Sidak. We are\nparticularly interested in multiple testing procedures with two desired\nproperties: (solely) monotonic and well-behaved procedures. This article\ninvestigates to which extent the classes of (monotonic or well-behaved)\nmultiple testing procedures, in particular the subclasses of so-called step-up\nand step-down procedures, are closed under basic set operations, specifically\nthe union, intersection, difference and the complement of sets of rejected or\nnon-rejected hypotheses. The present article proves two main results: First,\ntaking the union or intersection of arbitrary (monotonic or well-behaved)\nmultiple testing procedures results in new procedures which are monotonic but\nnot well-behaved, whereas the complement or difference generally preserves\nneither property. Second, the two classes of (solely monotonic or well-behaved)\nstep-up and step-down procedures are closed under taking the union or\nintersection, but not the complement or difference.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 21:28:09 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 19:24:11 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 04:08:57 GMT"}, {"version": "v4", "created": "Sun, 30 Apr 2017 00:31:07 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hahn", "Georg", ""]]}, {"id": "1607.04367", "submitter": "Minwoo Chae", "authors": "Minwoo Chae, Yongdai Kim, Bas Kleijn", "title": "The semi-parametric Bernstein-von Mises theorem for regression models\n  with symmetric errors", "comments": "46 pages, 1 figure", "journal-ref": null, "doi": "10.5705/ss.202017.0074", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a smooth semi-parametric model, the marginal posterior distribution for a\nfinite dimensional parameter of interest is expected to be asymptotically\nequivalent to the sampling distribution of any efficient point-estimator. The\nassertion leads to asymptotic equivalence of credible and confidence sets for\nthe parameter of interest and is known as the semi-parametric Bernstein-von\nMises theorem. In recent years, it has received much attention and has been\napplied in many examples. We consider models in which errors with symmetric\ndensities play a role; more specifically, it is shown that the marginal\nposterior distributions of regression coefficients in the linear regression and\nlinear mixed effect models satisfy the semi-parametric Bernstein-von Mises\nassertion. As a consequence, Bayes estimators in these models achieve\nfrequentist inferential optimality, as expressed e.g. through Hajek's\nconvolution and asymptotic minimax theorems. Conditions for the prior on the\nspace of error densities are relatively mild and well-known constructions like\nthe Dirichlet process mixture of normal densities and random series priors\nconstitute valid choices. Particularly, the result provides an efficient\nestimate of regression coefficients in the linear mixed effect model, for which\nno other efficient point-estimator was known previously.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 03:08:49 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 13:04:57 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Chae", "Minwoo", ""], ["Kim", "Yongdai", ""], ["Kleijn", "Bas", ""]]}, {"id": "1607.04384", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "The Derivative of Influence Function, Location Breakdown Point, Group\n  Leverage and Regression Residuals' Plots", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several linear regression data sets, $Y (\\in R)$ on ${\\bf X} (\\in R^p),$\nvisual comparisons of $L_1$ and $L_2$-residuals' plots indicate bad leverage\ncases. The phenomenon is confirmed theoretically by introducing Location\nBreakdown Point (LBP) of a functional $T$: any point where the derivative of\n$T$'s Influence Function either takes values at infinities or does not exist.\nGuidelines for the plots' visual comparisons as diagnostic are provided. The\nnew tools used include E-matrix and suggest influence diagnostic RINFIN which\nmeasures the distance in the derivatives of $L_2$-residuals} at $({\\bf x},y)$\nfrom model $F$ and from gross-error model $F_{\\epsilon, {\\bf x},y}.$ The larger\nRINFIN$({\\bf x},y)$ is, the larger $({\\bf x},y)$'s influence in\n$L_2$-regression residual is. RINFIN allows measuring group influence of $k$\n${\\bf x}$-neighboring data cases in a size $n$ sample using their average,\n$(\\bar {\\bf x}_k,\\bar y_k),$ as one case with weight $\\epsilon=k/n.$ For high\ndimensional, simulated data, the misclassification proportion of bad leverage\ncases in data's RINFIN-ordering decreases to zero as $p$ increases, thus\nreconfirming the blessing of high dimensionality in the detection of remote\nclusters. The visual diagnostic and RINFIN are successful in applications and\ncomplement each other.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 05:14:11 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 19:45:12 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 21:57:11 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "1607.04407", "submitter": "Masayo Hirose", "authors": "Masayo Y. Hirose", "title": "Non-area-specific adjustment factor for second-order efficient empirical\n  Bayes confidence interval", "comments": "18 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical Bayes confidence interval has high user demand in many\napplications. In particular, the second-order empirical Bayes confidence\ninterval, the coverage error of which is of the third order for large number of\nareas, is widely used in small area estimation when the sample size within each\narea is not large enough to make reliable direct estimates based on a\ndesign-based approach. Yoshimori and Lahiri (2014a) proposed a new type of\nconfidence interval, called the second-order efficient empirical Bayes\nconfidence interval, whose length is less than that of the direct confidence\ninterval based on the design-based approach. However, this interval still has\nsome disadvantages: (i) it is hard to use when at least one leverage value is\nhigh; (ii) many iterations tend to be required to obtain the estimators of one\nglobal model variance parameter as the number of areas getting larger, due to\nthe area-specific adjustment factor. To prevent such issues, this paper\nproposes, as never done before, a more efficient confidence interval to allow\nfor high leverage and reduce the number of iterations for large number of\nareas, by adopting a non-area-specific adjustment factor, maintaining the\nexisting desired properties. We also reveal the relationship between the\ngeneral adjustment factor and the measure of uncertainty of the empirical Bayes\nestimator to create a second-order confidence interval. Moreover, we present\ntwo simulation studies and a real data analysis to show the efficiency of this\nconfidence interval.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 07:52:57 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 14:19:11 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Hirose", "Masayo Y.", ""]]}, {"id": "1607.04430", "submitter": "Hideatsu Tsukahara", "authors": "Johan Segers, Masaaki Sibuya and Hideatsu Tsukahara", "title": "The Empirical Beta Copula", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample from a multivariate distribution $F$, the uniform random\nvariates generated independently and rearranged in the order specified by the\ncomponentwise ranks of the original sample look like a sample from the copula\nof $F$. This idea can be regarded as a variant on Baker's [J. Multivariate\nAnal. 99 (2008) 2312--2327] copula construction and leads to the definition of\nthe empirical beta copula. The latter turns out to be a particular case of the\nempirical Bernstein copula, the degrees of all Bernstein polynomials being\nequal to the sample size.\n  Necessary and sufficient conditions are given for a Bernstein polynomial to\nbe a copula. These imply that the empirical beta copula is a genuine copula.\nFurthermore, the empirical process based on the empirical Bernstein copula is\nshown to be asymptotically the same as the ordinary empirical copula process\nunder assumptions which are significantly weaker than those given in Janssen,\nSwanepoel and Veraverbeke [J. Stat. Plan. Infer. 142 (2012) 1189--1197].\n  A Monte Carlo simulation study shows that the empirical beta copula\noutperforms the empirical copula and the empirical checkerboard copula in terms\nof both bias and variance. Compared with the empirical Bernstein copula with\nthe smoothing rate suggested by Janssen et al., its finite-sample performance\nis still significantly better in several cases, especially in terms of bias.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 09:31:18 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 06:07:26 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Segers", "Johan", ""], ["Sibuya", "Masaaki", ""], ["Tsukahara", "Hideatsu", ""]]}, {"id": "1607.04807", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan", "title": "Progress on a Conjecture Regarding the Triangular Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangular distributions are a well-known class of distributions that are\noften used as an elementary example of a probability model. Maximum likelihood\nestimation of the mode parameter of the triangular distribution over the unit\ninterval can be performed via an order statistics-based method. It had been\nconjectured that such a method can be conducted using only a constant number of\nlikelihood function evaluations, on average, as the sample size becomes large.\nWe prove two theorems that validate this conjecture. Graphical and numerical\nresults are presented to supplement our proofs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 23:18:24 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 04:39:25 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1607.04912", "submitter": "Igor Cialenco", "authors": "Igor Cialenco, Ruoting Gong and Yicong Huang", "title": "Trajectory Fitting Estimators for SPDEs Driven by Additive Noise", "comments": "Forthcoming in Statistical Inference for Stochastic Processes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of estimating the drift/viscosity\ncoefficient for a large class of linear, parabolic stochastic partial\ndifferential equations (SPDEs) driven by an additive space-time noise. We\npropose a new class of estimators, called trajectory fitting estimators (TFEs).\nThe estimators are constructed by fitting the observed trajectory with an\nartificial one, and can be viewed as an analog to the classical least squares\nestimators from the time-series analysis. As in the existing literature on\nstatistical inference for SPDEs, we take a spectral approach, and assume that\nwe observe the first $N$ Fourier modes of the solution, and we study the\nconsistency and the asymptotic normality of the TFE, as $N\\to\\infty$.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 19:55:58 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 16:01:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cialenco", "Igor", ""], ["Gong", "Ruoting", ""], ["Huang", "Yicong", ""]]}, {"id": "1607.05051", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "False confidence, non-additive beliefs, and valid statistical inference", "comments": "60 pages, 12 figures. Comments welcome at\n  https://www.researchers.one/article/2019-02-1", "journal-ref": "International Journal of Approximate Reasoning, 2019, volume 113,\n  pages 39--73", "doi": "10.1016/j.ijar.2019.06.005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics has made tremendous advances since the times of Fisher, Neyman,\nJeffreys, and others, but the fundamental and practically relevant questions\nabout probability and inference that puzzled our founding fathers remain\nunanswered. To bridge this gap, I propose to look beyond the two dominating\nschools of thought and ask the following three questions: what do scientists\nneed out of statistics, do the existing frameworks meet these needs, and, if\nnot, how to fill the void? To the first question, I contend that scientists\nseek to convert their data, posited statistical model, etc., into calibrated\ndegrees of belief about quantities of interest. To the second question, I argue\nthat any framework that returns additive beliefs, i.e., probabilities,\nnecessarily suffers from {\\em false confidence}---certain false hypotheses tend\nto be assigned high probability---and, therefore, risks systematic bias. This\nreveals the fundamental importance of {\\em non-additive beliefs} in the context\nof statistical inference. But non-additivity alone is not enough so, to the\nthird question, I offer a sufficient condition, called {\\em validity}, for\navoiding false confidence, and present a framework, based on random sets and\nbelief functions, that provably meets this condition. Finally, I discuss\ncharacterizations of p-values and confidence intervals in terms of valid\nnon-additive beliefs, which imply that users of these classical procedures are\nalready following the proposed framework without knowing it.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:56:13 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 03:53:41 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 20:43:33 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1607.05091", "submitter": "Claire Lacour", "authors": "Claire Lacour (1), Pascal Massart (1), Vincent Rivoirard (2) ((1)\n  SELECT, LM-Orsay, (2) CEREMADE)", "title": "Estimator selection: a new method with applications to kernel density\n  estimation", "comments": null, "journal-ref": "Sankhya A, Springer Verlag, 2017, 79 (2), pp.298 - 335", "doi": "10.1007/s13171-017-0107-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimator selection has become a crucial issue in non parametric estimation.\nTwo widely used methods are penalized empirical risk minimization (such as\npenalized log-likelihood estimation) or pairwise comparison (such as Lepski's\nmethod). Our aim in this paper is twofold. First we explain some general ideas\nabout the calibration issue of estimator selection methods. We review some\nknown results, putting the emphasis on the concept of minimal penalty which is\nhelpful to design data-driven selection criteria. Secondly we present a new\nmethod for bandwidth selection within the framework of kernel density density\nestimation which is in some sense intermediate between these two main methods\nmentioned above. We provide some theoretical results which lead to some fully\ndata-driven selection strategy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 14:13:05 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 12:38:56 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Lacour", "Claire", ""], ["Massart", "Pascal", ""], ["Rivoirard", "Vincent", ""]]}, {"id": "1607.05136", "submitter": "Pierpaolo De Blasi", "authors": "Pierpaolo De Blasi and Tore Schweder", "title": "Confidence distributions from likelihoods by median bias correction", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the modified directed likelihood, higher order accurate confidence limits\nfor a scalar parameter are obtained from the likelihood. They are conveniently\ndescribed in terms of a confidence distribution, that is a sample dependent\ndistribution function on the parameter space. In this paper we explore a\ndifferent route to accurate confidence limits via tail-symmetric confidence\ncurves, that is curves that describe equal tailed intervals at any level.\nInstead of modifying the directed likelihood, we consider inversion of the\nlog-likelihood ratio when evaluated at the median of the maximum likelihood\nestimator. This is shown to provide equal tailed intervals, and thus an exact\nconfidence distribution, to the third-order of approximation in regular\none-dimensional models. Median bias correction also provides an alternative\napproximation to the modified directed likelihood which holds up to the second\norder in exponential families.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 15:43:51 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["De Blasi", "Pierpaolo", ""], ["Schweder", "Tore", ""]]}, {"id": "1607.05167", "submitter": "Gustavo Didier", "authors": "Patrice Abry, Gustavo Didier and Hui Li", "title": "Two-step wavelet-based estimation for mixed Gaussian fractional\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed Gaussian fractional process $\\{Y(t)\\}_{t \\in {\\Bbb R}} = \\{PX(t)\\}_{t\n\\in {\\Bbb R}}$ is a multivariate stochastic process obtained by pre-multiplying\na vector of independent, Gaussian fractional process entries $X$ by a\nnonsingular matrix $P$. It is interpreted that $Y$ is observable, while $X$ is\na hidden process occurring in an (unknown) system of coordinates $P$. Mixed\nprocesses naturally arise as approximations to solutions of physically relevant\nclasses of multivariate fractional SDEs under aggregation. We propose a\nsemiparametric two-step wavelet-based method for estimating both the demixing\nmatrix $P^{-1}$ and the memory parameters of $X$. The asymptotic normality of\nthe estimators is established both in continuous and discrete time. Monte Carlo\nexperiments show that the finite sample estimation performance is comparable to\nthat of parametric methods, while being very computationally efficient. As\napplications, we model a bivariate time series of annual tree ring width\nmeasurements, and establish the asymptotic normality of the eigenstructure of\nsample wavelet matrices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:31:58 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 19:36:04 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Abry", "Patrice", ""], ["Didier", "Gustavo", ""], ["Li", "Hui", ""]]}, {"id": "1607.05222", "submitter": "Jess Banks", "authors": "Jess Banks, Cristopher Moore, Nicolas Verzelen, Roman Vershynin,\n  Jiaming Xu", "title": "Information-theoretic bounds and phase transitions in clustering, sparse\n  PCA, and submatrix localization", "comments": "For sparse PCA and submatrix localization, we determine the\n  information-theoretic threshold exactly in the limit where the number of\n  blocks is large or the signal matrix is very sparse based on a conditional\n  second moment method, closing the factor of root two gap in the first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting a structured, low-rank signal matrix\ncorrupted with additive Gaussian noise. This includes clustering in a Gaussian\nmixture model, sparse PCA, and submatrix localization. Each of these problems\nis conjectured to exhibit a sharp information-theoretic threshold, below which\nthe signal is too weak for any algorithm to detect. We derive upper and lower\nbounds on these thresholds by applying the first and second moment methods to\nthe likelihood ratio between these \"planted models\" and null models where the\nsignal matrix is zero. Our bounds differ by at most a factor of root two when\nthe rank is large (in the clustering and submatrix localization problems, when\nthe number of clusters or blocks is large) or the signal matrix is very sparse.\nMoreover, our upper bounds show that for each of these problems there is a\nsignificant regime where reliable detection is information- theoretically\npossible but where known algorithms such as PCA fail completely, since the\nspectrum of the observed matrix is uninformative. This regime is analogous to\nthe conjectured 'hard but detectable' regime for community detection in sparse\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 18:09:53 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 06:20:25 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Banks", "Jess", ""], ["Moore", "Cristopher", ""], ["Verzelen", "Nicolas", ""], ["Vershynin", "Roman", ""], ["Xu", "Jiaming", ""]]}, {"id": "1607.05415", "submitter": "Ryota Yabe", "authors": "Toshio Honda and Ryota Yabe", "title": "Variable selection and structure identification for varying coefficient\n  Cox models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider varying coefficient Cox models with high-dimensional covariates.\nWe apply the group Lasso method to these models and propose a variable\nselection procedure. Our procedure copes with variable selection and structure\nidentification from a high dimensional varying coefficient model to a\nsemivarying coefficient model simultaneously. We derive an oracle inequality\nand closely examine restrictive eigenvalue conditions, too. In this paper, we\ngive the details for Cox models with time-varying coefficients. The theoretical\nresults on variable selection can be easily extended to some other important\nmodels and we briefly mention those models since those models can be treated in\nthe same way. The models considered in this paper are the most popular models\namong structured nonparametric regression models. The results of a small\nnumerical study are also given.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:20:41 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Honda", "Toshio", ""], ["Yabe", "Ryota", ""]]}, {"id": "1607.05421", "submitter": "Emilien Joly", "authors": "Emilien Joly (MODAL'X), G\\'abor Lugosi (ICREA), Roberto I. Oliveira\n  (IMPA)", "title": "On the estimation of the mean of a random vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mean of a multivariatedistribution\nbased on independent samples. The main result is the proof of existence of an\nestimator with a non-asymptotic sub-Gaussian performance for all distributions\nsatisfying some mild moment assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:30:55 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Joly", "Emilien", "", "MODAL'X"], ["Lugosi", "G\u00e1bor", "", "ICREA"], ["Oliveira", "Roberto I.", "", "IMPA"]]}, {"id": "1607.05430", "submitter": "Elodie Vernet", "authors": "Elisabeth Gassiat (LM-Orsay), Judith Rousseau (CEREMADE), Elodie\n  Vernet (CMAP)", "title": "Efficient semiparametric estimation and model selection for\n  multidimensional mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider nonparametric multidimensional finite mixture\nmodels and we are interested in the semiparametric estimation of the population\nweights. Here, the i.i.d. observations are assumed to have at least three\ncomponents which are independent given the population. We approximate the\nsemiparametric model by projecting the conditional distributions on step\nfunctions associated to some partition. Our first main result is that if we\nrefine the partition slowly enough, the associated sequence of maximum\nlikelihood estimators of the weights is asymptotically efficient, and the\nposterior distribution of the weights, when using a Bayesian procedure,\nsatisfies a semiparametric Bernstein von Mises theorem. We then propose a\ncross-validation like procedure to select the partition in a finite horizon.\nOur second main result is that the proposed procedure satisfies an oracle\ninequality. Numerical experiments on simulated data illustrate our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 07:21:00 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 10:38:49 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Gassiat", "Elisabeth", "", "LM-Orsay"], ["Rousseau", "Judith", "", "CEREMADE"], ["Vernet", "Elodie", "", "CMAP"]]}, {"id": "1607.05454", "submitter": "Yunjian Yin", "authors": "Yunjian Yin, Lan Liu, Zhi Geng, and Peng Luo", "title": "Novel Criteria to Exclude the Surrogate Paradox and Their Optimalities", "comments": "28pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the primary outcome is hard to collect, surrogate endpoint is typically\nused as a substitute. However, even when the treatment has a positive average\ncausal effect (ACE) on the surrogate endpoint, which also has a positive ACE on\nthe primary outcome, it is still possible that the treatment has a negative ACE\non the primary outcome. Such a phenomenon is called the surrogate paradox and\ngreatly challenges the use of surrogate. In this paper, we provide novel\ncriteria to exclude the surrogate paradox. Unlike other conditions previously\nproposed, our conditions are testable since they only involve observed data.\nFurthermore, our criteria are optimal in the sense that they are sufficient and\n\"almost necessary\" to exclude the paradox: if the conditions are satisfied, the\nsurrogate paradox is guaranteed to be absent while if the conditions fail,\nthere exists a data generating process with surrogate paradox that can generate\nthe same observed data. That is, our criteria capture all the information in\nthe observed data to exclude the surrogate paradox rather than relying on\nunverifiable distributional assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 08:26:11 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Yin", "Yunjian", ""], ["Liu", "Lan", ""], ["Geng", "Zhi", ""], ["Luo", "Peng", ""]]}, {"id": "1607.05536", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Adaptive Fused LASSO in Grouped Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers quantile model with grouped explanatory variables. In\norder to have the sparsity of the parameter groups but also the sparsity\nbetween two successive groups of variables, we propose and study an adaptive\nfused group LASSO quantile estimator. The number of variable groups can be\nfixed or divergent. We find the convergence rate under classical assumptions\nand we show that the proposed estimator satisfies the oracle properties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:10:34 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1607.05636", "submitter": "Patrick Breheny", "authors": "Patrick Breheny", "title": "Marginal false discovery rates for penalized regression models", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods are an attractive tool for high-dimensional data\nanalysis, but their widespread adoption has been hampered by the difficulty of\napplying inferential tools. In particular, the question \"How reliable is the\nselection of those features?\" has proved difficult to address. In part, this\ndifficulty arises from defining false discoveries in the classical, fully\nconditional sense, which is possible in low dimensions but does not scale well\nto high-dimensional settings. Here, we consider the analysis of marginal false\ndiscovery rates for penalized regression methods. Restricting attention to the\nmarginal FDR permits straightforward estimation of the number of selections\nthat would likely have occurred by chance alone, and therefore provides a\nuseful summary of selection reliability. Theoretical analysis and simulation\nstudies demonstrate that this approach is quite accurate when the correlation\namong predictors is mild, and only slightly conservative when the correlation\nis stronger. Finally, the practical utility of the proposed method and its\nconsiderable advantages over other approaches are illustrated using gene\nexpression data from The Cancer Genome Atlas and GWAS data from the Myocardial\nApplied Genomics Network.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:37:25 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 14:31:58 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Breheny", "Patrick", ""]]}, {"id": "1607.05709", "submitter": "Guo Xian Yau", "authors": "Guo Xian Yau, Chong Zhang", "title": "Multi-category Angle-based Classifier Refit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important statistical learning tool. In real\napplication, besides high prediction accuracy, it is often desirable to\nestimate class conditional probabilities for new observations. For traditional\nproblems where the number of observations is large, there exist many well\ndeveloped approaches. Recently, high dimensional low sample size problems are\nbecoming increasingly popular. Margin-based classifiers, such as logistic\nregression, are well established methods in the literature. On the other hand,\nin terms of probability estimation, it is known that for binary classifiers,\nthe commonly used methods tend to under-estimate the norm of the classification\nfunction. This can lead to biased probability estimation. Remedy approaches\nhave been proposed in the literature. However, for the simultaneous\nmulticategory classification framework, much less work has been done. We fill\nthe gap in this paper. In particular, we give theoretical insights on why heavy\nregularization terms are often needed in high dimensional applications, and how\nthis can lead to bias in probability estimation. To overcome this difficulty,\nwe propose a new refit strategy for multicategory angle-based classifiers. Our\nnew method only adds a small computation cost to the problem, and is able to\nattain prediction accuracy that is as good as the regular margin-based\nclassifiers. On the other hand, the improvement of probability estimation can\nbe very significant. Numerical results suggest that the new refit approach is\nhighly competitive.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 19:49:55 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Yau", "Guo Xian", ""], ["Zhang", "Chong", ""]]}, {"id": "1607.05712", "submitter": "Zaid Harchaoui", "authors": "Dmitry Ostrovsky, Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski", "title": "Structure-Blind Signal Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a signal observed in Gaussian noise. If\nthe set of signals is convex and compact, and can be specified beforehand, one\ncan use classical linear estimators that achieve a risk within a constant\nfactor of the minimax risk. However, when the set is unspecified, designing an\nestimator that is blind to the hidden structure of the signal remains a\nchallenging problem. We propose a new family of estimators to recover signals\nobserved in Gaussian noise. Instead of specifying the set where the signal\nlives, we assume the existence of a well-performing linear estimator. Proposed\nestimators enjoy exact oracle inequalities and can be efficiently computed\nthrough convex optimization. We present several numerical illustrations that\nshow the potential of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 19:55:43 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 21:16:06 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Ostrovsky", "Dmitry", ""], ["Harchaoui", "Zaid", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1607.05735", "submitter": "Francesco Buscemi", "authors": "Francesco Buscemi, Gilad Gour", "title": "Quantum Relative Lorenz Curves", "comments": "ver2: 14 pages, 3 figures, published version; ver1: 5 pages + 2\n  figures + 15 more pages + 1 more figure", "journal-ref": "Physical Review A 95, 012110 (9 January 2017)", "doi": "10.1103/PhysRevA.95.012110", "report-no": null, "categories": "quant-ph cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of majorization and its variants, including thermomajorization,\nhave been found to play a central role in the formulation of many physical\nresource theories, ranging from entanglement theory to quantum thermodynamics.\nHere we formulate the framework of quantum relative Lorenz curves, and show how\nit is able to unify majorization, thermomajorization, and their noncommutative\nanalogues. In doing so, we define the family of Hilbert $\\alpha$-divergences\nand show how it relates with other divergences used in quantum information\ntheory. We then apply these tools to the problem of deciding the existence of a\nsuitable transformation from an initial pair of quantum states to a final one,\nfocusing in particular on applications to the resource theory of athermality, a\nprecursor of quantum thermodynamics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 20:01:01 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 03:21:49 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Buscemi", "Francesco", ""], ["Gour", "Gilad", ""]]}, {"id": "1607.05831", "submitter": "Simon Clinet", "authors": "Simon Clinet and Yoann Potiron", "title": "Statistical inference for the doubly stochastic self-exciting process", "comments": "47 pages, 4 figures, 4 tables. Under revision for Bernoulli Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and show the existence of a Hawkes self-exciting point process\nwith exponentially-decreasing kernel and where parameters are time-varying. The\nquantity of interest is defined as the integrated parameter\n$T^{-1}\\int_0^T\\theta_t^*dt$, where $\\theta_t^*$ is the time-varying parameter,\nand we consider the high-frequency asymptotics. To estimate it na\\\"ively, we\nchop the data into several blocks, compute the maximum likelihood estimator\n(MLE) on each block, and take the average of the local estimates. The\nasymptotic bias explodes asymptotically, thus we provide a non-na\\\"ive\nestimator which is constructed as the na\\\"ive one when applying a first-order\nbias reduction to the local MLE. We show the associated central limit theorem.\nMonte Carlo simulations show the importance of the bias correction and that the\nmethod performs well in finite sample, whereas the empirical study discusses\nthe implementation in practice and documents the stochastic behavior of the\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 06:27:40 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 19:14:47 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 01:14:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Clinet", "Simon", ""], ["Potiron", "Yoann", ""]]}, {"id": "1607.05842", "submitter": "Marie du Roy de Chaumaray", "authors": "Marie du Roy de Chaumaray", "title": "Moderate deviations for parameters estimation in a geometrically ergodic\n  Heston process", "comments": null, "journal-ref": "Statistical inference for stochastic processes, 2017", "doi": "10.1007/s11203-017-9158-4", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a moderate deviation principle for the maximum likelihood\nestimator of the four parameters of a geometrically ergodic Heston process. We\nalso obtain moderate deviations for the maximum likelihood estimator of the\ncouple of dimensional and drift parameters of a generalized squared radial\nOrnstein-Uhlenbeck process. We restrict ourselves to the most tractable case\nwhere the dimensional parameter satisfies $a>2$ and the drift coefficient is\nsuch that $b<0$. In contrast to the previous literature, parameters are\nestimated simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:37:10 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["de Chaumaray", "Marie du Roy", ""]]}, {"id": "1607.05884", "submitter": "Roberto Molinari Mr", "authors": "St\\'ephane Guerrier and Roberto Molinari", "title": "On the Identifiability of Latent Models for Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition of parameter identifiability is essential for the consistency\nof all estimators and is often challenging to prove. As a consequence, this\ncondition is often assumed for simplicity although this may not be\nstraightforward to assume for a variety of model settings. In this paper we\ndeal with a particular class of models that we refer to as \"latent\" models\nwhich can be defined as models made by the sum of underlying models, such as a\nvariety of linear state-space models for time series. These models are of great\nimportance in many fields, from ecology to engineering, and in this paper we\nprove the identifiability of a wide class of (second-order stationary) latent\ntime series and spatial models and discuss what this implies for some extremum\nestimators, thereby reducing the conditions for their consistency to some very\nbasic regularity conditions. Finally, a specific focus is given to the\nGeneralized Method of Wavelet Moments estimator which is also able to estimate\nintrinsically second-order stationary models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 09:28:07 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""]]}, {"id": "1607.05896", "submitter": "Florian Wisheckel", "authors": "Michael Falk and Florian Wisheckel", "title": "Multivariate Order Statistics: The Intermediate Case", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic normality of intermediate order statistics taken from univariate\niid random variables is well-known. We generalize this result to random vectors\nin arbitrary dimension, where the order statistics are taken componentwise.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:02:55 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Falk", "Michael", ""], ["Wisheckel", "Florian", ""]]}, {"id": "1607.05980", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler and Jan Ernest and Peter B\\\"uhlmann", "title": "Causal inference in partially linear structural equation models", "comments": "D.R. and J.E. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider identifiability of partially linear additive structural equation\nmodels with Gaussian noise (PLSEMs) and estimation of distributionally\nequivalent models to a given PLSEM. Thereby, we also include robustness results\nfor errors in the neighborhood of Gaussian distributions. Existing\nidentifiability results in the framework of additive SEMs with Gaussian noise\nare limited to linear and nonlinear SEMs, which can be considered as special\ncases of PLSEMs with vanishing nonparametric or parametric part, respectively.\nWe close the wide gap between these two special cases by providing a\ncomprehensive theory of the identifiability of PLSEMs by means of (A) a\ngraphical, (B) a transformational, (C) a functional and (D) a causal ordering\ncharacterization of PLSEMs that generate a given distribution P. In particular,\nthe characterizations (C) and (D) answer the fundamental question to which\nextent nonlinear functions in additive SEMs with Gaussian noise restrict the\nset of potential causal models and hence influence the identifiability. On the\nbasis of the transformational characterization (B) we provide a score-based\nestimation procedure that outputs the graphical representation (A) of the\ndistribution equivalence class of a given PLSEM. We derive its\n(high-dimensional) consistency and demonstrate its performance on simulated\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:48:19 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 19:45:33 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 16:45:07 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Ernest", "Jan", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1607.06158", "submitter": "Konstantinos Spiliopoulos", "authors": "Andrew Papanicolaou, Konstantinos Spiliopoulos", "title": "Dimension Reduction in Statistical Estimation of Partially Observed\n  Multiscale Processes", "comments": "SIAM Journal of Uncertainty Quantification, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider partially observed multiscale diffusion models that are specified\nup to an unknown vector parameter. We establish for a very general class of\ntest functions that the filter of the original model converges to a filter of\nreduced dimension. Then, this result is used to justify statistical estimation\nfor the unknown parameters of interest based on the model of reduced dimension\nbut using the original available data. This allows to learn the unknown\nparameters of interest while working in lower dimensions, as opposed to working\nwith the original high dimensional system. Simulation studies support and\nillustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:14:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 20:06:55 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 18:02:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Papanicolaou", "Andrew", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1607.06163", "submitter": "David Frazier", "authors": "David T. Frazier and Eric Renault", "title": "Indirect Inference With(Out) Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Inference (I-I) estimation of structural parameters $\\theta$\n{{requires matching observed and simulated statistics, which are most often\ngenerated using an auxiliary model that depends on instrumental parameters\n$\\beta$.}} {The estimators of the instrumental parameters will encapsulate} the\nstatistical information used for inference about the structural parameters. As\nsuch, artificially constraining these parameters may restrict the ability of\nthe auxiliary model to accurately replicate features in the structural data,\nwhich may lead to a range of issues, such as, a loss of identification.\nHowever, in certain situations the parameters $\\beta$ naturally come with a set\nof $q$ restrictions. Examples include settings where $\\beta$ must be estimated\nsubject to $q$ possibly strict inequality constraints $g(\\beta) > 0$, such as,\nwhen I-I is based on GARCH auxiliary models. In these settings we propose a\nnovel I-I approach that uses appropriately modified unconstrained auxiliary\nstatistics, which are simple to compute and always exists. We state the\nrelevant asymptotic theory for this I-I approach without constraints and show\nthat it can be reinterpreted as a standard implementation of I-I through a\nproperly modified binding function. Several examples that have featured in the\nliterature illustrate our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:57:10 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:18:18 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 10:11:32 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Frazier", "David T.", ""], ["Renault", "Eric", ""]]}, {"id": "1607.06407", "submitter": "Julian Straub", "authors": "Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III", "title": "Small-Variance Nonparametric Clustering on the Hypersphere", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  (pp. 334-342). (2015)", "doi": "10.1109/CVPR.2015.7298630", "report-no": null, "categories": "cs.CV math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural regularities in man-made environments reflect in the distribution\nof their surface normals. Describing these surface normal distributions is\nimportant in many computer vision applications, such as scene understanding,\nplane segmentation, and regularization of 3D reconstructions. Based on the\nsmall-variance limit of Bayesian nonparametric von-Mises-Fisher (vMF) mixture\ndistributions, we propose two new flexible and efficient k-means-like\nclustering algorithms for directional data such as surface normals. The first,\nDP-vMF-means, is a batch clustering algorithm derived from the Dirichlet\nprocess (DP) vMF mixture. Recognizing the sequential nature of data collection\nin many applications, we extend this algorithm to DDP-vMF-means, which infers\ntemporally evolving cluster structure from streaming data. Both algorithms\nnaturally respect the geometry of directional data, which lies on the unit\nsphere. We demonstrate their performance on synthetic directional data and real\n3D surface normals from RGB-D sensors. While our experiments focus on 3D data,\nboth algorithms generalize to high dimensional directional data such as protein\nbackbone configurations and semantic word vectors.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 17:52:08 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Straub", "Julian", ""], ["Campbell", "Trevor", ""], ["How", "Jonathan P.", ""], ["Fisher", "John W.", "III"]]}, {"id": "1607.06409", "submitter": "Ricardo Moura", "authors": "Ricardo Moura, Martin Klein, Carlos A. Coelho and Bimal Sinha", "title": "Inference for Multivariate Regression Model based on synthetic data\n  generated under Fixed-Posterior Predictive Sampling: comparison with Plug-in\n  Sampling", "comments": null, "journal-ref": "Revstat 15(2-2017), 155-186", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors derive likelihood-based exact inference methods for the\nmultivariate regression model, for singly imputed synthetic data generated via\nPosterior Predictive Sampling (PPS) and for multiply imputed synthetic data\ngenerated via a newly proposed sampling method, which the authors call\nFixed-Posterior Predictive Sampling (FPPS). In the single imputation case, our\nproposed FPPS method concurs with the usual Posterior Predictive Sampling (PPS)\nmethod, thus filling the gap in the existing literature where inferential\nmethods are only available for multiple imputation. Simulation studies compare\nthe results obtained with those for the exact test procedures under the Plug-in\nSampling method, obtained by the same authors. Measures of privacy are\ndiscussed and compared with the measures derived for the Plug-in Sampling\nmethod. An application using U.S.\\ 2000 Current Population Survey data is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 18:06:29 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Moura", "Ricardo", ""], ["Klein", "Martin", ""], ["Coelho", "Carlos A.", ""], ["Sinha", "Bimal", ""]]}, {"id": "1607.06564", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "On Stochastic Comparisons of Order Statistics from Heterogeneous\n  Exponential Samples", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the $k$th order statistic from a heterogeneous sample of $n\\geq\nk$ exponential random variables is larger than that from a homogeneous\nexponential sample in the sense of star ordering, as conjectured by Xu and\nBalakrishnan (2012). As a consequence, we establish hazard rate ordering for\norder statistics between heterogeneous and homogeneous exponential samples,\nresolving an open problem of P\\v{a}lt\\v{a}nea (2008). Extensions to general\nspacings are also presented.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 06:08:28 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "1607.06699", "submitter": "Miljenko Huzak", "authors": "Miljenko Huzak (Department of Mathematics, Faculty of Science,\n  University of Zagreb)", "title": "Estimating a class of diffusions from discrete observations via\n  approximate maximum likelihood method", "comments": "Title changed, and in Section 5 one more example added", "journal-ref": "Statistics, 52:2, 239-272 (2018)", "doi": "10.1080/02331888.2017.1382496", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate maximum likelihood method of estimation of diffusion\nparameters $(\\vartheta,\\sigma)$ based on discrete observations of a diffusion\n$X$ along fixed time-interval $[0,T]$ and Euler approximation of integrals is\nanalyzed. We assume that $X$ satisfies a SDE of form $dX_t =\\mu (X_t ,\\vartheta\n)\\, dt+\\sqrt{\\sigma} b(X_t )\\, dW_t$, with non-random initial condition. SDE is\nnonlinear in $\\vartheta$ generally. Based on assumption that maximum likelihood\nestimator $\\hat{\\vartheta}_T$ of the drift parameter based on continuous\nobservation of a path over $[0,T]$ exists we prove that measurable estimator\n$(\\hat{\\vartheta}_{n,T},\\hat{\\sigma}_{n,T})$ of the parameters obtained from\ndiscrete observations of $X$ along $[0,T]$ by maximization of the approximate\nlog-likelihood function exists, $\\hat{\\sigma}_{n,T}$ being consistent and\nasymptotically normal, and $\\hat{\\vartheta}_{n,T}-\\hat{\\vartheta}_T$ tends to\nzero with rate $\\sqrt{\\delta}_{n,T}$ in probability when $\\delta_{n,T}\n=\\max_{0\\leq i<n}(t_{i+1}-t_i )$ tends to zero with $T$ fixed. The same holds\nin case of an ergodic diffusion when $T$ goes to infinity in a way that\n$T\\delta_n$ goes to zero with equidistant sampling, and we applied these to\nshow consistency and asymptotical normality of $\\hat{\\vartheta}_{n,T}$,\n$\\hat{\\sigma}_{n,T}$ and asymptotic efficiency of $\\hat{\\vartheta}_{n,T}$ in\nthis case.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 15:00:26 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 12:54:07 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Huzak", "Miljenko", "", "Department of Mathematics, Faculty of Science,\n  University of Zagreb"]]}, {"id": "1607.06762", "submitter": "Walter Dempsey", "authors": "Harry Crane and Walter Dempsey", "title": "Relational exchangeability", "comments": null, "journal-ref": "J. Appl. Probab. 56 (2019) 192-208", "doi": "10.1017/jpr.2019.13", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relationally exchangeable structure is a random combinatorial structure\nwhose law is invariant with respect to relabeling its relations, as opposed to\nits elements. Aside from exchangeable random partitions, examples include edge\nexchangeable random graphs and hypergraphs, path exchangeable processes, and a\nrange of other network-like structures that arise in statistical applications.\nWe prove a de Finetti-type structure theorem for the general class of\nrelationally exchangeable structures.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:43:50 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 12:42:48 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Crane", "Harry", ""], ["Dempsey", "Walter", ""]]}, {"id": "1607.06903", "submitter": "Christian P. Robert", "authors": "David T. Frazier (Monash University), Gael M. Martin (Monash\n  University), Christian P. Robert (Universit\\'e Paris-Dauphine PSL and\n  University of Warwick, UK), and Judith Rousseau (University of Oxford, UK)", "title": "Asymptotic Properties of Approximate Bayesian Computation", "comments": "This 31 pages paper is a revised version of the paper, including\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation allows for statistical analysis in models\nwith intractable likelihoods. In this paper we consider the asymptotic\nbehaviour of the posterior distribution obtained by this method. We give\ngeneral results on the rate at which the posterior distribution concentrates on\nsets containing the true parameter, its limiting shape, and the asymptotic\ndistribution of the posterior mean. These results hold under given rates for\nthe tolerance used within the method, mild regularity conditions on the summary\nstatistics, and a condition linked to identification of the true parameters.\nImplications for practitioners are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 09:05:00 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:37:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 09:18:42 GMT"}, {"version": "v4", "created": "Tue, 8 May 2018 15:22:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Frazier", "David T.", "", "Monash University"], ["Martin", "Gael M.", "", "Monash\n  University"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine PSL and\n  University of Warwick, UK"], ["Rousseau", "Judith", "", "University of Oxford, UK"]]}, {"id": "1607.06921", "submitter": "Moreno Bevilacqua", "authors": "M. Bevilacqua, T. Faouzi, R. Furrer, E. Porcu", "title": "Estimation and Prediction using generalized Wendland Covariance\n  Functions under fixed domain asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation and prediction of Gaussian random fields with covariance\nmodels belonging to the generalized Wendland (GW) class, under fixed domain\nasymptotics. As the Mat\\'ern case, this class allows a continuous\nparameterization of smoothness of the underlying Gaussian random field, being\nadditionally compactly supported. The paper is divided into two parts: First,\nwe characterize the equivalence of two Gaussian measures with GW covariance\nfunction, and we provide sufficient conditions for the equivalence of two\nGaussian measures with Mat\\'ern and GW covariance functions. We elucidate the\nconsequences of these facts in terms of (misspecified) best linear unbiased\npredictors. In the second part, we establish strong consistency and asymptotic\ndistribution of the maximum likelihood estimator of the microergodic parameter\nassociated to GW covariance model, under fixed domain asymptotics. Our findings\nare illustrated through a simulation study: The first compares the finite\nsample behavior of the maximum likelihood estimation of the microergodic\nparameter with the given asymptotic distribution. We then compare the\nfinite-sample behavior of the prediction and its associated mean square error\nwhen using two equivalent Gaussian measures with Mat\\'ern and GW covariance\nmodel, using covariance tapering as benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 11:45:38 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 15:18:11 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 12:21:00 GMT"}, {"version": "v4", "created": "Thu, 16 Nov 2017 03:09:18 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Bevilacqua", "M.", ""], ["Faouzi", "T.", ""], ["Furrer", "R.", ""], ["Porcu", "E.", ""]]}, {"id": "1607.06929", "submitter": "Salem Said", "authors": "Salem Said, Hatem Hajri, Lionel Bombrun, Baba C. Vemuri", "title": "Gaussian distributions on Riemannian symmetric spaces: statistical\n  learning with structured covariance matrices", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Riemannian geometry of covariance matrices has been essential to several\nsuccessful applications, in computer vision, biomedical signal and image\nprocessing, and radar data processing. For these applications, an important\nongoing challenge is to develop Riemannian-geometric tools which are adapted to\nstructured covariance matrices. The present paper proposes to meet this\nchallenge by introducing a new class of probability distributions, Gaussian\ndistributions of structured covariance matrices. These are Riemannian analogs\nof Gaussian distributions, which only sample from covariance matrices having a\npreassigned structure, such as complex, Toeplitz, or block-Toeplitz. The\nusefulness of these distributions stems from three features: (1) they are\ncompletely tractable, analytically or numerically, when dealing with large\ncovariance matrices, (2) they provide a statistical foundation to the concept\nof structured Riemannian barycentre (i.e. Fr\\'echet or geometric mean), (3)\nthey lead to efficient statistical learning algorithms, which realise, among\nothers, density estimation and classification of structured covariance\nmatrices. The paper starts from the observation that several spaces of\nstructured covariance matrices, considered from a geometric point of view, are\nRiemannian symmetric spaces. Accordingly, it develops an original theory of\nGaussian distributions on Riemannian symmetric spaces, of their statistical\ninference, and of their relationship to the concept of Riemannian barycentre.\nThen, it uses this original theory to give a detailed description of Gaussian\ndistributions of three kinds of structured covariance matrices, complex,\nToeplitz, and block-Toeplitz. Finally, it describes algorithms for density\nestimation and classification of structured covariance matrices, based on\nGaussian distribution mixture models.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 12:56:19 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 12:43:09 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Said", "Salem", ""], ["Hajri", "Hatem", ""], ["Bombrun", "Lionel", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "1607.06993", "submitter": "Zongming Ma", "authors": "Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou", "title": "Community Detection in Degree-Corrected Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a central problem of network data analysis. Given a\nnetwork, the goal of community detection is to partition the network nodes into\na small number of clusters, which could often help reveal interesting\nstructures. The present paper studies community detection in Degree-Corrected\nBlock Models (DCBMs). We first derive asymptotic minimax risks of the problem\nfor a misclassification proportion loss under appropriate conditions. The\nminimax risks are shown to depend on degree-correction parameters, community\nsizes, and average within and between community connectivities in an intuitive\nand interpretable way. In addition, we propose a polynomial time algorithm to\nadaptively perform consistent and even asymptotically optimal community\ndetection in DCBMs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 02:53:38 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1607.07260", "submitter": "Wolfgang Bischoff Dr.", "authors": "Wolfgang Bischoff and Andreas Gegg", "title": "Boundary crossing probabilities for $(q,d)$-Slepian-processes", "comments": null, "journal-ref": "Statistics and Probability Letters 118 (2016), 139 - 144", "doi": "10.1016/j.spl.2016.06.023", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $0<q< d$ fixed let $W^{[q,d]}=(W^{[q,d]}_t)_{t\\in {[q,d]}}$ be a\n$(q,d)$-Slepian-process defined as centered, stationary Gaussian process with\ncontinuous sample paths and covariance\n  \\begin{align*} C_{W^{[q,d]}}(s,s+t) = (1-\\frac{t}{q})^+, \\quad q\\leq s\\leq\ns+t\\leq d.\n  \\end{align*} Note that\n  \\begin{align*} \\frac{1}{\\sqrt{q}}(B_t-B_{t-q})_{t\\in [q,d]}, \\end{align*}\nwhere $B_t$ is standard Brownian motion, is a $(q,d)$-Slepian-process. In this\npaper we prove an analytical formula for the boundary crossing probability\n$\\mathbb{P}\\left(W^{[q,d]}_t > g(t) \\; \\text{for some } t\\in[q,d]\\right)$, $q<\nd\\leq 2q$, in the case $g$ is a piecewise affine function. This formula can be\nused as approximation for the boundary crossing probability of an arbitrary\nboundary by approximating the boundary function by piecewise affine functions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:29:34 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Bischoff", "Wolfgang", ""], ["Gegg", "Andreas", ""]]}, {"id": "1607.07343", "submitter": "Anna Simoni", "authors": "Jean-Pierre Florens and Anna Simoni", "title": "Gaussian processes and Bayesian moment estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of moment restrictions (MRs) that overidentify a parameter\n$\\theta$, we investigate a semiparametric Bayesian approach for inference on\n$\\theta$ that does not restrict the data distribution $F$ apart from the MRs.\nAs main contribution, we construct a degenerate Gaussian process prior that,\nconditionally on $\\theta$, restricts the $F$ generated by this prior to satisfy\nthe MRs with probability one. Our prior works even in the more involved case\nwhere the number of MRs is larger than the dimension of $\\theta$. We\ndemonstrate that the corresponding posterior for $\\theta$ is computationally\nconvenient. Moreover, we show that there exists a link between our procedure,\nthe Generalized Empirical Likelihood with quadratic criterion and the limited\ninformation likelihood-based procedures. We provide a frequentist validation of\nour procedure by showing consistency and asymptotic normality of the posterior\ndistribution of $\\theta$. The finite sample properties of our method are\nillustrated through Monte Carlo experiments and we provide an application to\ndemand estimation in the airline market.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 16:23:24 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 13:15:26 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Florens", "Jean-Pierre", ""], ["Simoni", "Anna", ""]]}, {"id": "1607.07549", "submitter": "Xiao Pu", "authors": "Ery Arias-Castro and Xiao Pu", "title": "Concentration of Measure for Radial Distributions and Consequences for\n  Statistical Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems in high-dimensional statistics such as mixture modeling\nfor classification and clustering, we consider the behavior of radial densities\nas the dimension increases. We establish a form of concentration of measure,\nand even a convergence in distribution, under additional assumptions. This\nextends the well-known behavior of the normal distribution (its concentration\naround the sphere of radius square-root of the dimension) to other radial\ndensities. We draw some possible consequences for statistical modeling in\nhigh-dimensions, including a possible universality property of Gaussian\nmixtures.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 05:55:19 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 17:49:23 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pu", "Xiao", ""]]}, {"id": "1607.07600", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki", "title": "An introduction to computational algebraic statistics", "comments": "Kobe-Lyon Summer School (2015), Lecture Notes, Algorithms and\n  Computation in Mathematics, 28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the fundamental notion of a Markov basis, which\nis one of the first connections between commutative algebra and statistics. The\nnotion of a Markov basis is first introduced by Diaconis and Sturmfels (1998)\nfor conditional testing problems on contingency tables by Markov chain Monte\nCarlo methods. In this method, we make use of a connected Markov chain over the\ngiven conditional sample space to estimate the P-values numerically for various\nconditional tests. A Markov basis plays an importance role in this arguments,\nbecause it guarantees the connectivity of the chain, which is needed for\nunbiasedness of the estimate, for arbitrary conditional sample space. As\nanother important point, a Markov basis is characterized as generators of the\nwell-specified toric ideals of polynomial rings. This connection between\ncommutative algebra and statistics is the main result of Diaconis and Sturmfels\n(1998). After this first paper, a Markov basis is studied intensively by many\nresearchers both in commutative algebra and statistics, which yields an\nattractive field called computational algebraic statistics. In this paper, we\ngive a review of the Markov chain Monte Carlo methods for contingency tables\nand Markov bases, with some fundamental examples. We also give some\ncomputational examples by algebraic software Macaulay2 and statistical software\nR. Readers can also find theoretical details of the problems considered in this\npaper and various results on the structure and examples of Markov bases in\nAoki, Hara and Takemura (2012).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:14:50 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Aoki", "Satoshi", ""]]}, {"id": "1607.07705", "submitter": "Dhruva V. Raman Mr", "authors": "Dhruva V. Raman and James Anderson and Antonis Papachristodoulou", "title": "Delineating Parameter Unidentifiabilities in Complex Models", "comments": null, "journal-ref": "Phys. Rev. E 95, 032314 (2017)", "doi": "10.1103/PhysRevE.95.032314", "report-no": null, "categories": "physics.data-an math.DS math.ST q-bio.MN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists use mathematical modelling to understand and predict the\nproperties of complex physical systems. In highly parameterised models there\noften exist relationships between parameters over which model predictions are\nidentical, or nearly so. These are known as structural or practical\nunidentifiabilities, respectively. They are hard to diagnose and make reliable\nparameter estimation from data impossible. They furthermore imply the existence\nof an underlying model simplification. We describe a scalable method for\ndetecting unidentifiabilities, and the functional relations defining them, for\ngeneric models. This allows for model simplification, and appreciation of which\nparameters (or functions thereof) cannot be estimated from data. Our algorithm\ncan identify features such as redundant mechanisms and fast timescale\nsubsystems, as well as the regimes in which such approximations are valid. We\nbase our algorithm on a novel quantification of regional parametric\nsensitivity: multiscale sloppiness. Traditionally, the link between parametric\nsensitivity and the conditioning of the parameter estimation problem is made\nlocally, through the Fisher Information Matrix. This is valid in the regime of\ninfinitesimal measurement uncertainty. We demonstrate the duality between\nmultiscale sloppiness and the geometry of confidence regions surrounding\nparameter estimates made where measurement uncertainty is non-negligible.\nFurther theoretical relationships are provided linking multiscale sloppiness to\nthe Likelihood-ratio test. From this, we show that a local sensitivity analysis\n(as typically done) is insufficient for determining the reliability of\nparameter estimation, even with simple (non)linear systems. Our algorithm\nprovides a tractable alternative. We finally apply our methods to a\nlarge-scale, benchmark Systems Biology model of NF-$\\kappa$B, uncovering\npreviously unknown unidentifiabilities.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 14:17:38 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 18:18:53 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Raman", "Dhruva V.", ""], ["Anderson", "James", ""], ["Papachristodoulou", "Antonis", ""]]}, {"id": "1607.07771", "submitter": "Hyunphil Choi", "authors": "Hyunphil Choi, Matthew Reimherr", "title": "A Geometric Approach to Confidence Regions and Bands for Functional\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis, FDA, is now a well established discipline of\nstatistics, with its core concepts and perspectives in place. Despite this,\nthere are still fundamental statistical questions which have received\nrelatively little attention. One of these is the systematic construction of\nconfidence regions for functional parameters. This work is concerned with\ndeveloping, understanding, and visualizing such regions. We provide a general\nstrategy for constructing confidence regions in a real separable Hilbert space\nusing hyper-ellipsoids and hyper-rectangles. We then propose specific\nimplementations which work especially well in practice. They provide powerful\nhypothesis tests and useful visualization tools without using any simulation.\nWe also demonstrate the negative result that nearly all regions, including our\nown, have zero-coverage when working with empirical covariances. To overcome\nthis challenge we propose a new paradigm for evaluating confidence regions by\nshowing that the distance between an estimated region and the desired region\n(with proper coverage) tends to zero faster than the regions shrink to a point.\nWe call this phenomena ghosting and refer to the empirical regions as ghost\nregions. We illustrate the proposed methods in a simulation study and an\napplication to fractional anisotropy tract profile data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:59:50 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 15:51:27 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Choi", "Hyunphil", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1607.07819", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Andrew R. Barron", "title": "Approximation by Combinations of ReLU and Squared ReLU Ridge Functions\n  with $ \\ell^1 $ and $ \\ell^0 $ Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish $ L^{\\infty} $ and $ L^2 $ error bounds for functions of many\nvariables that are approximated by linear combinations of ReLU (rectified\nlinear unit) and squared ReLU ridge functions with $ \\ell^1 $ and $ \\ell^0 $\ncontrols on their inner and outer parameters. With the squared ReLU ridge\nfunction, we show that the $ L^2 $ approximation error is inversely\nproportional to the inner layer $ \\ell^0 $ sparsity and it need only be\nsublinear in the outer layer $ \\ell^0 $ sparsity. Our constructions are\nobtained using a variant of the Jones-Barron probabilistic method, which can be\ninterpreted as either stratified sampling with proportionate allocation or\ntwo-stage cluster sampling. We also provide companion error lower bounds that\nreveal near optimality of our constructions. Despite the sparsity assumptions,\nwe showcase the richness and flexibility of these ridge combinations by\ndefining a large family of functions, in terms of certain spectral conditions,\nthat are particularly well approximated by them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 17:52:00 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:41:26 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 22:02:46 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1607.07844", "submitter": "Vahid Fakoor", "authors": "Vahid Fakoor, Raheleh. Zamini", "title": "Uniform limit theorems under random truncation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study uniform versions of two limit theorems in random left\ntruncation model (RLTM). The law of large numbers (LLN) and the central limit\ntheorem (CLT) have been obtained under the bracketing entropy conditions in\nthis setting. The uniform LLN and the uniform CLT of the present paper extend\nthe one dimensional LLN and the one dimensional CLT under RLTM respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 19:08:11 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Fakoor", "Vahid", ""], ["Zamini", "Raheleh.", ""]]}, {"id": "1607.07981", "submitter": "Claudio Durastanti Dr.", "authors": "Solesne Bourguin (Boston University) and Claudio Durastanti (Ruhr\n  Universit\\\"at)", "title": "On high-frequency limits of $U$-statistics in Besov spaces over compact\n  manifolds", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, quantitative bounds in high-frequency central limit theorems\nare derived for Poisson based $U$-statistics of arbitrary degree built by means\nof wavelet coefficients over compact Riemannian manifolds. The wavelets\nconsidered here are the so-called needlets, characterized by strong\nconcentration properties and by an exact reconstruction formula. Furthermore,\nwe consider Poisson point processes over the manifold such that the density\nfunction associated to its control measure lives in a Besov space. The main\nfindings of this paper include new rates of convergence that depend strongly on\nthe degree of regularity of the control measure of the underlying Poisson point\nprocess, providing a refined understanding of the connection between regularity\nand speed of convergence in this framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 06:57:39 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bourguin", "Solesne", "", "Boston University"], ["Durastanti", "Claudio", "", "Ruhr\n  Universit\u00e4t"]]}, {"id": "1607.08077", "submitter": "Nikolay Vereshchagin", "authors": "Nikolai Vereshchagin, Alexander Shen", "title": "Algorithmic statistics: forty years later", "comments": "Missing proofs added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic statistics has two different (and almost orthogonal) motivations.\nFrom the philosophical point of view, it tries to formalize how the statistics\nworks and why some statistical models are better than others. After this notion\nof a \"good model\" is introduced, a natural question arises: it is possible that\nfor some piece of data there is no good model? If yes, how often these bad\n(\"non-stochastic\") data appear \"in real life\"?\n  Another, more technical motivation comes from algorithmic information theory.\nIn this theory a notion of complexity of a finite object (=amount of\ninformation in this object) is introduced; it assigns to every object some\nnumber, called its algorithmic complexity (or Kolmogorov complexity).\nAlgorithmic statistic provides a more fine-grained classification: for each\nfinite object some curve is defined that characterizes its behavior. It turns\nout that several different definitions give (approximately) the same curve.\n  In this survey we try to provide an exposition of the main results in the\nfield (including full proofs for the most important ones), as well as some\nhistorical comments. We assume that the reader is familiar with the main\nnotions of algorithmic information (Kolmogorov complexity) theory.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:16:35 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 09:52:39 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 17:46:51 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Vereshchagin", "Nikolai", ""], ["Shen", "Alexander", ""]]}, {"id": "1607.08085", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Improving Semantic Embedding Consistency by Metric Learning for\n  Zero-Shot Classification", "comments": "in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of zero-shot image classification. The key\ncontribution of the proposed approach is to control the semantic embedding of\nimages -- one of the main ingredients of zero-shot learning -- by formulating\nit as a metric learning problem. The optimized empirical criterion associates\ntwo types of sub-task constraints: metric discriminating capacity and accurate\nattribute prediction. This results in a novel expression of zero-shot learning\nnot requiring the notion of class in the training phase: only pairs of\nimage/attributes, augmented with a consistency indicator, are given as ground\ntruth. At test time, the learned model can predict the consistency of a test\nimage with a given set of attributes , allowing flexible ways to produce\nrecognition inferences. Despite its simplicity, the proposed approach gives\nstate-of-the-art results on four challenging datasets used for zero-shot\nrecognition evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:35:16 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1607.08156", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Bruno Pelletier, Venkatesh Saligrama", "title": "Remember the Curse of Dimensionality: The Case of Goodness-of-Fit\n  Testing in Arbitrary Dimension", "comments": "This version comes after the publication of the paper in the Journal\n  of Nonparametric Statistics. The main change is to cite the work of Ramdas et\n  al. Some very minor typos were also corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a substantial literature on nonparametric two-sample goodness-of-fit\ntesting in arbitrary dimensions spanning decades, there is no mention there of\nany curse of dimensionality. Only more recently Ramdas et al. (2015) have\ndiscussed this issue in the context of kernel methods by showing that their\nperformance degrades with the dimension even when the underlying distributions\nare isotropic Gaussians. We take a minimax perspective and follow in the\nfootsteps of Ingster (1987) to derive the minimax rate in arbitrary dimension\nwhen the discrepancy is measured in the L2 metric. That rate is revealed to be\nnonparametric and exhibit a prototypical curse of dimensionality. We further\nextend Ingster's work to show that the chi-squared test achieves the minimax\nrate. Moreover, we show that the test can be made to work when the\ndistributions have support of low intrinsic dimension. Finally, inspired by\nIngster (2000), we consider a multiscale version of the chi-square test which\ncan adapt to unknown smoothness and/or unknown intrinsic dimensionality without\nmuch loss in power.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:41:10 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 18:55:47 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 01:23:44 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pelletier", "Bruno", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1607.08318", "submitter": "Zachary Kilpatrick PhD", "authors": "Adrian E Radillo, Alan Veliz-Cuba, Kresimir Josic, and Zachary P\n  Kilpatrick", "title": "Evidence accumulation and change rate inference in dynamic environments", "comments": "43 pages, 8 figures, in press", "journal-ref": "Neural Computation (2017)", "doi": null, "report-no": null, "categories": "q-bio.NC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a constantly changing world, animals must account for environmental\nvolatility when making decisions. To appropriately discount older, irrelevant\ninformation, they need to learn the rate at which the environment changes. We\ndevelop an ideal observer model capable of inferring the present state of the\nenvironment along with its rate of change. Key to this computation is an update\nof the posterior probability of all possible changepoint counts. This\ncomputation can be challenging, as the number of possibilities grows rapidly\nwith time. However, we show how the computations can be simplified in the\ncontinuum limit by a moment closure approximation. The resulting\nlow-dimensional system can be used to infer the environmental state and change\nrate with accuracy comparable to the ideal observer. The approximate\ncomputations can be performed by a neural network model via a rate-correlation\nbased plasticity rule. We thus show how optimal observers accumulate evidence\nin changing environments, and map this computation to reduced models which\nperform inference using plausible neural mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:24:55 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 17:46:26 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Radillo", "Adrian E", ""], ["Veliz-Cuba", "Alan", ""], ["Josic", "Kresimir", ""], ["Kilpatrick", "Zachary P", ""]]}, {"id": "1607.08647", "submitter": "Rounak Dey", "authors": "Rounak Dey and Seunggeun Lee", "title": "Asymptotic properties of Principal Component Analysis and shrinkage-bias\n  adjustment under the Generalized Spiked Population model", "comments": null, "journal-ref": "J. Multivariate Anal., 173 (2019), pp. 145-164", "doi": "10.1016/j.jmva.2019.02.007", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of high-throughput technologies, principal component\nanalysis (PCA) in the high-dimensional regime is of great interest. Most of the\nexisting theoretical and methodological results for high-dimensional PCA are\nbased on the spiked population model in which all the population eigenvalues\nare equal except for a few large ones. Due to the presence of local correlation\namong features, however, this assumption may not be satisfied in many\nreal-world datasets. To address this issue, we investigated the asymptotic\nbehaviors of PCA under the generalized spiked population model. Based on the\ntheoretical results, we proposed a series of methods for the consistent\nestimation of population eigenvalues, angles between the sample and population\neigenvectors, correlation coefficients between the sample and population\nprincipal component (PC) scores, and the shrinkage bias adjustment for the\npredicted PC scores. Using numerical experiments and real data examples from\nthe genetics literature, we showed that our methods can greatly reduce bias and\nimprove prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 21:45:58 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Dey", "Rounak", ""], ["Lee", "Seunggeun", ""]]}, {"id": "1607.08826", "submitter": "Hao Luo", "authors": "Hao Luo, Alexandre Bouchard-C\\^ot\\'e, Gabriela Cohen Freue, Paul\n  Gustafson", "title": "The Constrained Maximum Likelihood Estimation For Parameters Arising\n  From Partially Identified Models", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the constrained maximum likelihood estimation theory for parameters\nof a completely identified model, proposed by Aitchison and Silvey (1958), to\nparameters arising from a partially identified model. With a partially\nidentified model, some parameters of the model may only be identified through\nconstraints imposed by additional assumptions. We show that, under certain\nconditions, the constrained maximum likelihood estimator exists and locally\nmaximize the likelihood function subject to constraints. We then study the\nasymptotic distribution of the estimator and propose a numerical algorithm for\nestimating parameters. We also discuss a special situation where exploiting\nadditional assumptions does not improve estimation efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:28:56 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Luo", "Hao", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Freue", "Gabriela Cohen", ""], ["Gustafson", "Paul", ""]]}]