[{"id": "2103.00060", "submitter": "Alessandro Casini", "authors": "Federico Belotti, Alessandro Casini, Leopoldo Catania, Stefano Grassi\n  and Pierre Perron", "title": "Simultaneous Bandwidths Determination for DK-HAC Estimators and Long-Run\n  Variance Estimation in Nonparametric Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the derivation of data-dependent simultaneous bandwidths for\ndouble kernel heteroskedasticity and autocorrelation consistent (DK-HAC)\nestimators. In addition to the usual smoothing over lagged autocovariances for\nclassical HAC estimators, the DK-HAC estimator also applies smoothing over the\ntime direction. We obtain the optimal bandwidths that jointly minimize the\nglobal asymptotic MSE criterion and discuss the trade-off between bias and\nvariance with respect to smoothing over lagged autocovariances and over time.\nUnlike the MSE results of Andrews (1991), we establish how nonstationarity\naffects the bias-variance trade-o?. We use the plug-in approach to construct\ndata-dependent bandwidths for the DK-HAC estimators and compare them with the\nDK-HAC estimators from Casini (2021) that use data-dependent bandwidths\nobtained from a sequential MSE criterion. The former performs better in terms\nof size control, especially with stationary and close to stationary data.\nFinally, we consider long-run variance estimation under the assumption that the\nseries is a function of a nonparametric estimator rather than of a\nsemiparametric estimator that enjoys the usual T^(1/2) rate of convergence.\nThus, we also establish the validity of consistent long-run variance estimation\nin nonparametric parameter estimation settings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 21:38:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Belotti", "Federico", ""], ["Casini", "Alessandro", ""], ["Catania", "Leopoldo", ""], ["Grassi", "Stefano", ""], ["Perron", "Pierre", ""]]}, {"id": "2103.00469", "submitter": "Benjamin Eltzner", "authors": "Do Tran, Benjamin Eltzner, Stephan Huckemann", "title": "Smeariness Begets Finite Sample Smeariness", "comments": "8 pages, 1 figure, conference submission to GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fr\\'echet means are indispensable for nonparametric statistics on\nnon-Euclidean spaces. For suitable random variables, in some sense, they\n\"sense\" topological and geometric structure. In particular, smeariness seems to\nindicate the presence of positive curvature. While smeariness may be considered\nmore as an academical curiosity, occurring rarely, it has been recently\ndemonstrated that finite sample smeariness (FSS) occurs regularly on circles,\ntori and spheres and affects a large class of typical probability\ndistributions. FSS can be well described by the modulation measuring the\nquotient of rescaled expected sample mean variance and population variance.\nUnder FSS it is larger than one - that is its value on Euclidean spaces - and\nthis makes quantile based tests using tangent space approximations\ninapplicable. We show here that near smeary probability distributions there are\nalways FSS probability distributions and as a first step towards the conjecture\nthat all compact spaces feature smeary distributions, we establish directional\nsmeariness under curvature bounds.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:43:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tran", "Do", ""], ["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan", ""]]}, {"id": "2103.00500", "submitter": "Masaaki Imaizumi", "authors": "Ryumei Nakada, Masaaki Imaizumi", "title": "Asymptotic Risk of Overparameterized Likelihood Models: Double Descent\n  Theory for Deep Neural Networks", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the asymptotic risk of a general class of overparameterized\nlikelihood models, including deep models. The recent empirical success of\nlarge-scale models has motivated several theoretical studies to investigate a\nscenario wherein both the number of samples, $n$, and parameters, $p$, diverge\nto infinity and derive an asymptotic risk at the limit. However, these theorems\nare only valid for linear-in-feature models, such as generalized linear\nregression, kernel regression, and shallow neural networks. Hence, it is\ndifficult to investigate a wider class of nonlinear models, including deep\nneural networks with three or more layers. In this study, we consider a\nlikelihood maximization problem without the model constraints and analyze the\nupper bound of an asymptotic risk of an estimator with penalization.\nTechnically, we combine a property of the Fisher information matrix with an\nextended Marchenko-Pastur law and associate the combination with empirical\nprocess techniques. The derived bound is general, as it describes both the\ndouble descent and the regularized risk curves, depending on the penalization.\nOur results are valid without the linear-in-feature constraints on models and\nallow us to derive the general spectral distributions of a Fisher information\nmatrix from the likelihood. We demonstrate that several explicit models, such\nas parallel deep neural networks, ensemble learning, and residual networks, are\nin agreement with our theory. This result indicates that even large and deep\nmodels have a small asymptotic risk if they exhibit a specific structure, such\nas divisibility. To verify this finding, we conduct a real-data experiment with\nparallel deep neural networks. Our results expand the applicability of the\nasymptotic risk analysis, and may also contribute to the understanding and\napplication of deep learning.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:02:08 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:25:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nakada", "Ryumei", ""], ["Imaizumi", "Masaaki", ""]]}, {"id": "2103.00512", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner, Shayan Hundrieser, Stephan F. Huckemann", "title": "Finite Sample Smeariness on Spheres", "comments": "8 pages, 4 figures, conference paper, GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite Sample Smeariness (FSS) has been recently discovered. It means that\nthe distribution of sample Fr\\'echet means of underlying rather unsuspicious\nrandom variables can behave as if it were smeary for quite large regimes of\nfinite sample sizes. In effect classical quantile-based statistical testing\nprocedures do not preserve nominal size, they reject too often under the null\nhypothesis. Suitably designed bootstrap tests, however, amend for FSS. On the\ncircle it has been known that arbitrarily sized FSS is possible, and that all\ndistributions with a nonvanishing density feature FSS. These results are\nextended to spheres of arbitrary dimension. In particular all rotationally\nsymmetric distributions, not necessarily supported on the entire sphere feature\nFSS of Type I. While on the circle there is also FSS of Type II it is\nconjectured that this is not possible on higher-dimensional spheres.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:42:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Eltzner", "Benjamin", ""], ["Hundrieser", "Shayan", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "2103.00518", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura", "title": "Bayesian Point Estimation and Predictive Density Estimation for the\n  Binomial Distribution with a Restricted Probability Parameter", "comments": "29 pages, 4 figures; Theorem 3.3 and Sections 4 and 6 have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian point estimation and predictive density\nestimation in the binomial case. After presenting preliminary results on these\nproblems, we compare the risk functions of the Bayes estimators based on the\ntruncated and untruncated beta priors and obtain dominance conditions when the\nprobability parameter is less than or equal to a known constant. The case where\nthere are both a lower bound restriction and an upper bound restriction is also\ntreated. Then our problems are shown to be related to similar problems in the\nPoisson case. Finally, numerical studies are presented.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:08:41 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 10:27:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hamura", "Yasuyuki", ""]]}, {"id": "2103.00527", "submitter": "Xavier de Luna", "authors": "Seong-ho Lee, Yanyuan Ma and Xavier de Luna", "title": "Covariate balancing for causal inference on categorical and continuous\n  treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose novel estimators for categorical and continuous treatments by\nusing an optimal covariate balancing strategy for inverse probability\nweighting. The resulting estimators are shown to be consistent and\nasymptotically normal for causal contrasts of interest, either when the model\nexplaining treatment assignment is correctly specified, or when the correct set\nof bases for the outcome models has been chosen and the assignment model is\nsufficiently rich. For the categorical treatment case, we show that the\nestimator attains the semiparametric efficiency bound when all models are\ncorrectly specified. For the continuous case, the causal parameter of interest\nis a function of the treatment dose. The latter is not parametrized and the\nestimators proposed are shown to have bias and variance of the classical\nnonparametric rate. Asymptotic results are complemented with simulations\nillustrating the finite sample properties. Our analysis of a data set suggests\na nonlinear effect of BMI on the decline in self reported health.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:52:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lee", "Seong-ho", ""], ["Ma", "Yanyuan", ""], ["de Luna", "Xavier", ""]]}, {"id": "2103.00553", "submitter": "Guillaume Basse", "authors": "Kevin Wu Han, Iavor Bojinov and Guillaume Basse", "title": "Population Interference in Panel Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of population interference, where a treatment assigned to one\nexperimental unit affects another experimental unit's outcome, has received\nconsiderable attention in standard randomized experiments. The complications\nproduced by population interference in this setting are now readily recognized,\nand partial remedies are well known. Much less understood is the impact of\npopulation interference in panel experiments where treatment is sequentially\nrandomized in the population, and the outcomes are observed at each time step.\nThis paper proposes a general framework for studying population interference in\npanel experiments and presents new finite population estimation and inference\nresults. Our findings suggest that, under mild assumptions, the addition of a\ntemporal dimension to an experiment alleviates some of the challenges of\npopulation interference for certain estimands. In contrast, we show that the\npresence of carryover effects -- that is, when past treatments may affect\nfuture outcomes -- exacerbates the problem. Revisiting the special case of\nstandard experiments with population interference, we prove a central limit\ntheorem under weaker conditions than previous results in the literature and\nhighlight the trade-off between flexibility in the design and the interference\nstructure.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:24:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Han", "Kevin Wu", ""], ["Bojinov", "Iavor", ""], ["Basse", "Guillaume", ""]]}, {"id": "2103.00567", "submitter": "Guillaume Basse", "authors": "Hui Xu and Guillaume Basse", "title": "Randomization Inference for Composite Experiments with Spillovers and\n  Peer Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-formation experiments, in which experimental units are randomly\nassigned to groups, are a powerful tool for studying peer effects in the social\nsciences. Existing design and analysis approaches allow researchers to draw\ninference from such experiments without relying on parametric assumptions. In\npractice, however, group-formation experiments are often coupled with a second,\nexternal intervention, that is not accounted for by standard nonparametric\napproaches. This note shows how to construct Fisherian randomization tests and\nNeymanian asymptotic confidence intervals for such composite experiments,\nincluding in settings where the second intervention exhibits spillovers. We\nalso propose an approach for designing optimal composite experiments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:20:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Hui", ""], ["Basse", "Guillaume", ""]]}, {"id": "2103.00574", "submitter": "Hanne Kekkonen", "authors": "Hanne Kekkonen, Matti Lassas, Eero Saksman, and Samuli Siltanen", "title": "Random tree Besov priors -- Towards fractal imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose alternatives to Bayesian a priori distributions that are\nfrequently used in the study of inverse problems. Our aim is to construct\npriors that have similar good edge-preserving properties as total variation or\nMumford-Shah priors but correspond to well defined infinite-dimensional random\nvariables, and can be approximated by finite-dimensional random variables. We\nintroduce a new wavelet-based model, where the non zero coefficient are chosen\nin a systematic way so that prior draws have certain fractal behaviour. We show\nthat realisations of this new prior take values in some Besov spaces and have\nsingularities only on a small set $\\tau$ that has a certain Hausdorff\ndimension. We also introduce an efficient algorithm for calculating the MAP\nestimator, arising from the the new prior, in denoising problem.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:47:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kekkonen", "Hanne", ""], ["Lassas", "Matti", ""], ["Saksman", "Eero", ""], ["Siltanen", "Samuli", ""]]}, {"id": "2103.00588", "submitter": "Benjamin Eltzner", "authors": "Pernille Hansen, Benjamin Eltzner, Stefan Sommer", "title": "Diffusion Means and Heat Kernel on Manifolds", "comments": "8 pages, 1 figure, conference paper submitted to GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce diffusion means as location statistics on manifold data spaces.\nA diffusion mean is defined as the starting point of an isotropic diffusion\nwith a given diffusivity. They can therefore be defined on all spaces on which\na Brownian motion can be defined and numerical calculation of sample diffusion\nmeans is possible on a variety of spaces using the heat kernel expansion. We\npresent several classes of spaces, for which the heat kernel is known and\nsample diffusion means can therefore be calculated. As an example, we\ninvestigate a classic data set from directional statistics, for which the\nsample Fr\\'echet mean exhibits finite sample smeariness.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 19:01:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hansen", "Pernille", ""], ["Eltzner", "Benjamin", ""], ["Sommer", "Stefan", ""]]}, {"id": "2103.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Zhigen Zhao, Wen Zhou", "title": "BEAUTY Powered BEAST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study inference about the uniform distribution with the proposed binary\nexpansion approximation of uniformity (BEAUTY) approach. Through an extension\nof the celebrated Euler's formula, we approximate the characteristic function\nof any copula distribution with a linear combination of means of binary\ninteractions from marginal binary expansions. This novel characterization\nenables a unification of many important existing tests through an approximation\nfrom some quadratic form of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a uniformly\nhigh power, we study test statistics with data-adaptive weights through an\noracle approach, referred to as the binary expansion adaptive symmetry test\n(BEAST). By utilizing the properties of the binary expansion filtration, we\nshow that the Neyman-Pearson test of uniformity can be approximated by an\noracle weighted sum of symmetry statistics. The BEAST with this oracle leads\nall existing tests we considered in empirical power against all complex forms\nof alternatives. This oracle therefore sheds light on the potential of\nsubstantial improvements in power and on the form of optimal weights under each\nalternative. By approximating this oracle with data-adaptive weights, we\ndevelop the BEAST that improves the empirical power of many existing tests\nagainst a wide spectrum of common alternatives while providing clear\ninterpretation of the form of non-uniformity upon rejection. We illustrate the\nBEAST with a study of the relationship between the location and brightness of\nstars.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:36:15 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:08:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Kai", ""], ["Zhao", "Zhigen", ""], ["Zhou", "Wen", ""]]}, {"id": "2103.00763", "submitter": "Shovan Chowdhury", "authors": "Shovan Chowdhury, Amarjit Kundu, Surja Kanta Mishra", "title": "Comparisons of Order Statistics from Some Heterogeneous Discrete\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare extreme order statistics through vector\nmajorization arising from heterogeneous Poisson and geometric random variables.\nThese comparisons are carried out with respect to usual stochastic ordering.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:36:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chowdhury", "Shovan", ""], ["Kundu", "Amarjit", ""], ["Mishra", "Surja Kanta", ""]]}, {"id": "2103.00817", "submitter": "Mario Kieburg Dr. habil.", "authors": "Mario Kieburg and Adam Monteleone", "title": "Local Tail Statistics of Heavy-Tailed Random Matrix Ensembles with\n  Unitary Invariance", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.dis-nn math.MP math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study heavy-tailed Hermitian random matrices that are unitarily invariant.\nThe invariance implies that the eigenvalue and eigenvector statistics are\ndecoupled. The motivating question has been whether a freely stable random\nmatrix has stable eigenvalue statistics for the largest eigenvalues in the\ntail. We investigate this question through the use of both numerical and\nanalytical means, the latter of which makes use of the supersymmetry method. A\nsurprising behaviour is uncovered in that a freely stable random matrix does\nnot necessarily yield stable statistics and if it does then it might exhibit\nPoisson or Poisson-like statistics. The Poisson statistics have been already\nobserved for heavy-tailed Wigner matrices. We conclude with two conjectures on\nthis peculiar behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:34:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kieburg", "Mario", ""], ["Monteleone", "Adam", ""]]}, {"id": "2103.01126", "submitter": "Andr\\'e Bodmer Dr.", "authors": "Michael Freunek and Andr\\'e Bodmer", "title": "BERT based patent novelty search by training claims to their own\n  description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method to concatenate patent claims to their own\ndescription. By applying this method, BERT trains suitable descriptions for\nclaims. Such a trained BERT (claim-to-description- BERT) could be able to\nidentify novelty relevant descriptions for patents. In addition, we introduce a\nnew scoring scheme, relevance scoring or novelty scoring, to process the output\nof BERT in a meaningful way. We tested the method on patent applications by\ntraining BERT on the first claims of patents and corresponding descriptions.\nBERT's output has been processed according to the relevance score and the\nresults compared with the cited X documents in the search reports. The test\nshowed that BERT has scored some of the cited X documents as highly relevant.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:54:50 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:56:25 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 13:39:23 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 07:15:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Freunek", "Michael", ""], ["Bodmer", "Andr\u00e9", ""]]}, {"id": "2103.01160", "submitter": "Francesca Boso", "authors": "Francesca Boso and Daniel M. Tartakovsky", "title": "Information-geometry of physics-informed statistical manifolds and its\n  use in data assimilation", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The data-aware method of distributions (DA-MD) is a low-dimension data\nassimilation procedure to forecast the behavior of dynamical systems described\nby differential equations. It combines sequential Bayesian update with the MD,\nsuch that the former utilizes available observations while the latter\npropagates the (joint) probability distribution of the uncertain system\nstate(s). The core of DA-MD is the minimization of a distance between an\nobservation and a prediction in distributional terms, with prior and posterior\ndistributions constrained on a statistical manifold defined by the MD. We\nleverage the information-geometric properties of the statistical manifold to\nreduce predictive uncertainty via data assimilation. Specifically, we exploit\nthe information geometric structures induced by two discrepancy metrics, the\nKullback-Leibler divergence and the Wasserstein distance, which explicitly\nyield natural gradient descent. To further accelerate optimization, we build a\ndeep neural network as a surrogate model for the MD that enables automatic\ndifferentiation. The manifold's geometry is quantified without sampling,\nyielding an accurate approximation of the gradient descent direction. Our\nnumerical experiments demonstrate that accounting for the information-geometry\nof the manifold significantly reduces the computational cost of data\nassimilation by facilitating the calculation of gradients and by reducing the\nnumber of required iterations. Both storage needs and computational cost depend\non the dimensionality of a statistical manifold, which is typically small by MD\nconstruction. When convergence is achieved, the Kullback-Leibler and $L_2$\nWasserstein metrics have similar performances, with the former being more\nsensitive to poor choices of the prior.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:50:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Boso", "Francesca", ""], ["Tartakovsky", "Daniel M.", ""]]}, {"id": "2103.01218", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis Nieto-Barajas and Eduardo Guti\\'errez-Pe\\~na", "title": "General dependence structures for some models based on exponential\n  families with quadratic variance functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a procedure to introduce general dependence structures on a set\nof random variables. These include order-$q$ moving average-type structures, as\nwell as seasonal, periodic and spatial dependences. The invariant marginal\ndistribution can be in any family that is conjugate to an exponential family\nwith quadratic variance functions. Dependence is induced via latent variables\nwhose conditional distribution mirrors the sampling distribution in a Bayesian\nconjugate analysis of such exponential families. We obtain strict stationarity\nas a special case.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 20:00:40 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Nieto-Barajas", "Luis", ""], ["Guti\u00e9rrez-Pe\u00f1a", "Eduardo", ""]]}, {"id": "2103.01280", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano, Jelena Bradic", "title": "Dynamic covariate balancing: estimating treatment effects over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of estimation and inference on the effects\nof time-varying treatment. We propose a method for inference on the effects\ntreatment histories, introducing a dynamic covariate balancing method combined\nwith penalized regression. Our approach allows for (i) treatments to be\nassigned based on arbitrary past information, with the propensity score being\nunknown; (ii) outcomes and time-varying covariates to depend on treatment\ntrajectories; (iii) high-dimensional covariates; (iv) heterogeneity of\ntreatment effects. We study the asymptotic properties of the estimator, and we\nderive the parametric convergence rate of the proposed procedure. Simulations\nand an empirical application illustrate the advantage of the method over\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:53:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:54:38 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "2103.01337", "submitter": "Sidney Resnick", "authors": "Ross Maller, Sidney Resnick and Soudabeh Shemehsavar", "title": "Splitting the Sample at the Largest Uncensored Observation", "comments": "33 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We calculate finite sample and asymptotic distributions for the largest\ncensored and uncensored survival times, and some related statistics, from a\nsample of survival data generated according to an iid censoring model. These\nstatistics are important for assessing whether there is sufficient followup in\nthe sample to be confident of the presence of immune or cured individuals in\nthe population. A key structural result obtained is that, conditional on the\nvalue of the largest uncensored survival time, and knowing the number of\ncensored observations exceeding this time, the sample partitions into two\nindependent subsamples, each subsample having the distribution of an iid sample\nof censored survival times, of reduced size, from truncated random variables.\nThis result provides valuable insight into the construction of censored\nsurvival data, and facilitates the calculation of explicit finite sample\nformulae. We illustrate for distributions of statistics useful for testing for\nsufficient followup in a sample, and apply extreme value methods to derive\nasymptotic distributions for some of those.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:52:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Maller", "Ross", ""], ["Resnick", "Sidney", ""], ["Shemehsavar", "Soudabeh", ""]]}, {"id": "2103.01356", "submitter": "Ottmar Cronie", "authors": "Ottmar Cronie, Mehdi Moradi, Christophe A.N. Biscio", "title": "Statistical learning and cross-validation for point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first general (supervised) statistical learning\nframework for point processes in general spaces. Our approach is based on the\ncombination of two new concepts, which we define in the paper: i) bivariate\ninnovations, which are measures of discrepancy/prediction-accuracy between two\npoint processes, and ii) point process cross-validation (CV), which we here\ndefine through point process thinning. The general idea is to carry out the\nfitting by predicting CV-generated validation sets using the corresponding\ntraining sets; the prediction error, which we minimise, is measured by means of\nbivariate innovations. Having established various theoretical properties of our\nbivariate innovations, we study in detail the case where the CV procedure is\nobtained through independent thinning and we apply our statistical learning\nmethodology to three typical spatial statistical settings, namely parametric\nintensity estimation, non-parametric intensity estimation and Papangelou\nconditional intensity fitting. Aside from deriving theoretical properties\nrelated to these cases, in each of them we numerically show that our\nstatistical learning approach outperforms the state of the art in terms of mean\n(integrated) squared error.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:47:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cronie", "Ottmar", ""], ["Moradi", "Mehdi", ""], ["Biscio", "Christophe A. N.", ""]]}, {"id": "2103.01357", "submitter": "Yifu Tang", "authors": "Yifu Tang, Claudia Kirch, Jeong Eun Lee and Renate Meyer", "title": "Posterior consistency for the spectral density of non-Gaussian\n  stationary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various nonparametric approaches for Bayesian spectral density estimation of\nstationary time series have been suggested in the literature, mostly based on\nthe Whittle likelihood approximation. A generalization of this approximation\nhas been proposed in Kirch et al. who prove posterior consistency for spectral\ndensity estimation in combination with the Bernstein-Dirichlet process prior\nfor Gaussian time series. In this paper, we will extend the posterior\nconsistency result to non-Gaussian time series by employing a general\nconsistency theorem of Shalizi for dependent data and misspecified models. As a\nspecial case, posterior consistency for the spectral density under the Whittle\nlikelihood as proposed by Choudhuri, Ghosal and Roy is also extended to\nnon-Gaussian time series. Small sample properties of this approach are\nillustrated with several examples of non-Gaussian time series.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:48:11 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tang", "Yifu", ""], ["Kirch", "Claudia", ""], ["Lee", "Jeong Eun", ""], ["Meyer", "Renate", ""]]}, {"id": "2103.01369", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik and Eren C. K{\\i}z{\\i}lda\\u{g}", "title": "Algorithmic Obstructions in the Random Number Partitioning Problem", "comments": "84 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the algorithmic problem of finding a near-optimal solution for\nthe number partitioning problem (NPP). The NPP appears in many applications,\nincluding the design of randomized controlled trials, multiprocessor\nscheduling, and cryptography; and is also of theoretical significance. It\npossesses a so-called statistical-to-computational gap: when its input $X$ has\ndistribution $\\mathcal{N}(0,I_n)$, its optimal value is\n$\\Theta(\\sqrt{n}2^{-n})$ w.h.p.; whereas the best polynomial-time algorithm\nachieves an objective value of only $2^{-\\Theta(\\log^2 n)}$, w.h.p.\n  In this paper, we initiate the study of the nature of this gap. Inspired by\ninsights from statistical physics, we study the landscape of NPP and establish\nthe presence of the Overlap Gap Property (OGP), an intricate geometric property\nwhich is known to be a rigorous evidence of an algorithmic hardness for large\nclasses of algorithms. By leveraging the OGP, we establish that (a) any\nsufficiently stable algorithm, appropriately defined, fails to find a\nnear-optimal solution with energy below $2^{-\\omega(n \\log^{-1/5} n)}$; and (b)\na very natural MCMC dynamics fails to find near-optimal solutions. Our\nsimulations suggest that the state of the art algorithm achieving\n$2^{-\\Theta(\\log^2 n)}$ is indeed stable, but formally verifying this is left\nas an open problem.\n  OGP regards the overlap structure of $m-$tuples of solutions achieving a\ncertain objective value. When $m$ is constant we prove the presence of OGP in\nthe regime $2^{-\\Theta(n)}$, and the absence of it in the regime $2^{-o(n)}$.\nInterestingly, though, by considering overlaps with growing values of $m$ we\nprove the presence of the OGP up to the level $2^{-\\omega(\\sqrt{n\\log n})}$.\nOur proof of the failure of stable algorithms at values $2^{-\\omega(n\n\\log^{-1/5} n)}$ employs methods from Ramsey Theory from the extremal\ncombinatorics, and is of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:09:16 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""]]}, {"id": "2103.01604", "submitter": "Alessandro Casini", "authors": "Alessandro Casini, Taosong Deng and Pierre Perron", "title": "Theory of Low Frequency Contamination from Nonstationarity and\n  Misspecification: Consequences for HAR Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish theoretical and analytical results about the low frequency\ncontamination induced by general nonstationarity for estimates such as the\nsample autocovariance and the periodogram, and deduce consequences for\nheteroskedasticity and autocorrelation robust (HAR) inference. We show that for\nshort memory nonstationarity data these estimates exhibit features akin to long\nmemory. We present explicit expressions for the asymptotic bias of these\nestimates. This bias increases with the degree of heterogeneity. in the data\nand is responsible for generating low frequency contamination or simply making\nthe time series exhibiting long memory features. The sample autocovariances\ndisplay hyperbolic rather than exponential decay while the periodogram becomes\nunbounded near the origin. We distinguish cases where this contamination only\noccurs as a small-sample problem and cases where the contamination continues to\nhold asymptotically. We show theoretically that nonparametric smoothing over\ntime is robust to low frequency contamination.in that the sample local\nautocovariance and the local periodogram are unlikely to exhibit long memory\nfeatures. Simulations confirm that our theory provides useful approximations.\nSince the autocovariances and the periodogram are key elements for HAR\ninference, our results provide new insights on the debate between consistent\nversus inconsistent small versus long/fixed-b bandwidths for long-run variance\n(LRV) estimation-based inference.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:58:43 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Casini", "Alessandro", ""], ["Deng", "Taosong", ""], ["Perron", "Pierre", ""]]}, {"id": "2103.01650", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Conditional Precedence Orders for Stochastic Comparison of Random\n  Variables", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the stochastic orders for comparing random variables, considered in\nthe literature, are afflicted with two main drawbacks: (i) lack of connex\nproperty and (ii) lack of consideration of any dependence structure between the\nrandom variables. Both these drawbacks can be overcome at the cost of\ntransitivity with the stochastic precedence order, which may seem to be a good\nchoice in particular when only two random variables are under consideration, a\nsituation where the question of transitivity does not arise. In this paper, we\nshow that even under such favorable conditions, stochastic precedence order may\ndirect to misleading conclusion in certain situations and develop variations of\nthe order to address the phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:23:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01720", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Asymptotic Stochastic Comparison of Random Processes", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods are available in the literature to stochastically compare\nrandom variables and random vectors. We introduce the notion of asymptotic\nstochastic order for random processes and define four such orders. Various\nproperties and interrelations of the orders are discussed. Sufficient\nconditions for these orders to hold for certain stochastic processes, evolving\nfrom some statistical entities of interest, are derived.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:48:14 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01727", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Departure-based Asymptotic Stochastic Order for Random Processes", "comments": "33 pages, reference hyperlink added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a specific asymptotic stochastic order for random\nprocesses based on the measure of departure discussed in the literature. As\napplications, we stochastically compare mixtures of order statistics and record\nvalues coming from two different homogeneous samples, as the sample size\nbecomes large.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:59:30 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:26:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01887", "submitter": "Eren Can K{\\i}z{\\i}lda\\u{g}", "authors": "David Gamarnik, Eren C. K{\\i}z{\\i}lda\\u{g}, and Ilias Zadik", "title": "Self-Regularity of Non-Negative Output Weights for Overparameterized\n  Two-Layer Neural Networks", "comments": "34 pages. Some of the results in the present paper are significantly\n  strengthened versions of certain results appearing in arXiv:2003.10523", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a two-layer neural network with sigmoid,\nrectified linear unit (ReLU), or binary step activation functions that \"fits\" a\ntraining data set as accurately as possible as quantified by the training\nerror; and study the following question: \\emph{does a low training error\nguarantee that the norm of the output layer (outer norm) itself is small?} We\nanswer affirmatively this question for the case of non-negative output weights.\nUsing a simple covering number argument, we establish that under quite mild\ndistributional assumptions on the input/label pairs; any such network achieving\na small training error on polynomially many data necessarily has a\nwell-controlled outer norm. Notably, our results (a) have a polynomial (in $d$)\nsample complexity, (b) are independent of the number of hidden units (which can\npotentially be very high), (c) are oblivious to the training algorithm; and (d)\nrequire quite mild assumptions on the data (in particular the input vector\n$X\\in\\mathbb{R}^d$ need not have independent coordinates). We then leverage our\nbounds to establish generalization guarantees for such networks through\n\\emph{fat-shattering dimension}, a scale-sensitive measure of the complexity\nclass that the network architectures we investigate belong to. Notably, our\ngeneralization bounds also have good sample complexity (polynomials in $d$ with\na low degree), and are in fact near-linear for some important cases of\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:36:03 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Gamarnik", "David", ""], ["K\u0131z\u0131lda\u011f", "Eren C.", ""], ["Zadik", "Ilias", ""]]}, {"id": "2103.02235", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Minimax MSE Bounds and Nonlinear VAR Prewhitening for Long-Run Variance\n  Estimation Under Nonstationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish new mean-squared error (MSE) bounds for long-run variance (LRV)\nestimation, valid for both stationary and nonstationary sequences that are\nsharper than previously established. The key element to construct such bounds\nis to use restrictions onthe degree of nonstationarity. Unlike previous bounds,\nthey show how nonstationarity influences the bias-variance trade-off. Unlike\npreviously established bounds, either under stationarity or nonstationarity,\nthe new bounds depends on the form of nonstationarity. The bounds are\nestablished for double kernel long-run variance estimators. The corresponding\nbounds for classical long-run variance estimators follow as a special case. We\nuse them to construct new data-dependent methods for the selection of\nbandwidths for (double) kernel heteroskedasticity autocorrelation consistent\n(DK-HAC) estimators. These account more flexibly for nonstationarity and lead\nto tests with The new MSE bounds and associated bandwidths help to to improve\ngood finite-sample performance, especially good power when existing LRV\nestimators lead to tests having little or no or no power. The second\ncontribution is to introduce a nonparametric nonlinear VAR prewhitened LRV\nestimator. This accounts explicitly for nonstationarity unlike previous\nprewhitened procedures which are known to be unstable. Its consistency, rate of\nconvergence and MSE bounds are established. The prewhitened DK-HAC estimators\nlead to tests with good finite-sample size while maintaining good monotonic\npower.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 07:59:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "2103.02310", "submitter": "Arnaud Poinas", "authors": "Arnaud Poinas and Fr\\'ed\\'eric Lavancier", "title": "Asymptotic approximation of the likelihood of stationary determinantal\n  point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous determinantal point processes (DPPs) are a class of repulsive\npoint processes on $\\mathbb{R}^d$ with many statistical applications. Although\nan explicit expression of their density is known, this expression is too\ncomplicated to be used directly for maximum likelihood estimation. In the\nstationary case, an approximation using Fourier series has been suggested, but\nit is limited to rectangular observation windows and no theoretical results\nsupport it. In this contribution, we investigate a different way to approximate\nthe likelihood by looking at its asymptotic behaviour when the observation\nwindow grows towards $\\mathbb{R}^d$. This new approximation is not limited to\nrectangular windows, is faster to compute than the previous one, does not\nrequire any tuning parameter, and some theoretical justifications are provided.\nThe performances of the associated estimator are assessed in a simulation study\non standard parametric models on $\\mathbb{R}^d$ and compare favourably to\ncommon alternative estimation methods for continuous DPPs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:39:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Poinas", "Arnaud", ""], ["Lavancier", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2103.02366", "submitter": "Mads Lindskou", "authors": "Mads Lindskou, Torben Tvedebrink, Poul Svante Eriksen and Niels\n  Morling", "title": "Detecting Outliers in High-dimensional Data with Mixed Variable Types\n  using Conditional Gaussian Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Outlier detection has gained increasing interest in recent years, due to\nnewly emerging technologies and the huge amount of high-dimensional data that\nare now available. Outlier detection can help practitioners to identify\nunwanted noise and/or locate interesting abnormal observations. To address\nthis, we developed a novel method for outlier detection for use in, possibly\nhigh-dimensional, datasets with both discrete and continuous variables. We\nexploit the family of decomposable graphical models in order to model the\nrelationship between the variables and use this to form an exact likelihood\nratio test for an observation that is considered an outlier. We show that our\nmethod outperforms the state-of-the-art Isolation Forest algorithm on a real\ndata example.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:40:12 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 13:21:40 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:14:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lindskou", "Mads", ""], ["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""], ["Morling", "Niels", ""]]}, {"id": "2103.02457", "submitter": "Jorge Yslas Altamirano", "authors": "Hansjoerg Albrecher, Martin Bladt, Mogens Bladt and Jorge Yslas", "title": "Continuous scaled phase-type distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study random variables arising as the product of phase-type distributions\nand continuous random variables, revisiting their asymptotic properties, and\ndeveloping an expectation-maximization algorithm for their statistical\nanalysis. Throughout, emphasis is made on closed-form formulas for various\nfunctionals of the resulting mixed distributions, which facilitates their\nanalysis and implementation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:03:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Bladt", "Martin", ""], ["Bladt", "Mogens", ""], ["Yslas", "Jorge", ""]]}, {"id": "2103.02589", "submitter": "Tobias Boege", "authors": "Tobias Boege", "title": "Incidence geometry in the projective plane via almost-principal minors\n  of symmetric matrices", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an encoding of a polynomial system into vanishing and\nnon-vanishing constraints on almost-principal minors of a symmetric,\nprincipally regular matrix, such that the solvability of the system over some\nfield is equivalent to the satisfiability of the constraints over that field.\nThis implies two complexity results about Gaussian conditional independence\nstructures. First, all real algebraic numbers are necessary to construct\ninhabitants of non-empty Gaussian statistical models defined by conditional\nindependence and dependence constraints. This gives a negative answer to a\nquestion of Petr \\v{S}ime\\v{c}ek. Second, we prove that the implication problem\nfor Gaussian CI is polynomial-time equivalent to the existential theory of the\nreals.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:39:55 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Boege", "Tobias", ""]]}, {"id": "2103.02643", "submitter": "David Benkeser", "authors": "David Benkeser, Iv\\'an D\\'iaz, Jialu Ran", "title": "Inference for natural mediation effects under case-cohort sampling with\n  applications in identifying COVID-19 vaccine correlates of protection", "comments": "26 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Combating the SARS-CoV2 pandemic will require the fast development of\neffective preventive vaccines. Regulatory agencies may open accelerated\napproval pathways for vaccines if an immunological marker can be established as\na mediator of a vaccine's protection. A rich source of information for\nidentifying such correlates are large-scale efficacy trials of COVID-19\nvaccines, where immune responses are measured subject to a case-cohort sampling\ndesign. We propose two approaches to estimation of mediation parameters in the\ncontext of case-cohort sampling designs. We establish the theoretical\nlarge-sample efficiency of our proposed estimators and evaluate them in a\nrealistic simulation to understand whether they can be employed in the analysis\nof COVID-19 vaccine efficacy trials.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:15:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Benkeser", "David", ""], ["D\u00edaz", "Iv\u00e1n", ""], ["Ran", "Jialu", ""]]}, {"id": "2103.02853", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "A multivariate normal approximation for the Dirichlet density and some\n  applications", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we prove an asymptotic expansion for the ratio of the\nDirichlet density to the multivariate normal density with the same mean and\ncovariance matrix. The expansion is then used to derive an upper bound on the\ntotal variation between the corresponding probability measures and rederive the\nasymptotic variance of the Dirichlet kernel estimators introduced by Aitchison\n& Lauder (1985) and studied theoretically in Ouimet (2020). Another potential\napplication related to the asymptotic equivalence between the Gaussian variance\nregression problem and the Gaussian white noise problem is briefly mentioned\nbut left open for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:31:45 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 06:25:58 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 00:53:09 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2103.02922", "submitter": "Kazunori Nakamoto", "authors": "Hisashi Johno and Kazunori Nakamoto", "title": "Decision tree-based estimation of the overlap of two probability\n  distributions", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new nonparametric approach, based on a decision tree algorithm, is proposed\nto calculate the overlap between two probability distributions. The devised\nframework is described analytically and numerically. The convergence of the\nestimated overlap to the true value is proved along with some experimental\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:08:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Johno", "Hisashi", ""], ["Nakamoto", "Kazunori", ""]]}, {"id": "2103.02981", "submitter": "Alessandro Casini", "authors": "Alessandro Casini", "title": "Theory of Evolutionary Spectra for Heteroskedasticity and\n  Autocorrelation Robust Inference in Possibly Misspecified and Nonstationary\n  Models", "comments": "arXiv admin note: text overlap with arXiv:2103.00060", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a theory of evolutionary spectra for heteroskedasticity and\nautocorrelation robust (HAR) inference when the data may not satisfy\nsecond-order stationarity. Nonstationarity is a common feature of economic time\nseries which may arise either from parameter variation or model\nmisspecification. In such a context, the theories that support HAR inference\nare either not applicable or do not provide accurate approximations. HAR tests\nstandardized by existing long-run variance estimators then may display size\ndistortions and little or no power. This issue can be more severe for methods\nthat use long bandwidths (i.e., fixed-b HAR tests). We introduce a class of\nnonstationary processes that have a time-varying spectral representation which\nevolves continuously except at a finite number of time points. We present an\nextension of the classical heteroskedasticity and autocorrelation consistent\n(HAC) estimators that applies two smoothing procedures. One is over the lagged\nautocovariances, akin to classical HAC estimators, and the other is over time.\nThe latter element is important to flexibly account for nonstationarity. We\nname them double kernel HAC (DK-HAC) estimators. We show the consistency of the\nestimators and obtain an optimal DK-HAC estimator under the mean squared error\n(MSE) criterion. Overall, HAR tests standardized by the proposed DK-HAC\nestimators are competitive with fixed-b HAR tests, when the latter work well,\nwith regards to size control even when there is strong dependence. Notably, in\nthose empirically relevant situations in which previous HAR tests are\nundersized and have little or no power, the DK-HAC estimator leads to tests\nthat have good size and power.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:00:46 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Casini", "Alessandro", ""]]}, {"id": "2103.02989", "submitter": "Werner M\\\"uller", "authors": "Andrej P\\'azman, Markus Hainy, Werner G. M\\\"uller", "title": "A convex approach to optimum design of experiments with correlated\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal design of experiments for correlated processes is an increasingly\nrelevant and active research topic. Until now only heuristic methods were\navailable without a possibility to judge their quality. In this work we\ncomplement the virtual noise approach by a convex formulation and an\nequivalence theorem comparable to the uncorrelated case. Hence, it is now\npossible to provide an upper performance bound against which alternative design\nmethods can be judged. We provide a comparison on some classical examples from\nthe literature.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:29:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["P\u00e1zman", "Andrej", ""], ["Hainy", "Markus", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "2103.03169", "submitter": "Toni Karvonen", "authors": "Toni Karvonen", "title": "Small Sample Spaces for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the membership in a given reproducing kernel Hilbert space\n(RKHS) of the samples of a Gaussian process $X$ is controlled by a certain\nnuclear dominance condition. However, it is less clear how to identify a\n\"small\" set of functions (not necessarily a vector space) that contains the\nsamples. This article presents a general approach for identifying such sets. We\nuse scaled RKHSs, which can be viewed as a generalisation of Hilbert scales, to\ndefine the sample support set as the largest set which is contained in every\nelement of full measure under the law of $X$ in the $\\sigma$-algebra induced by\nthe collection of scaled RKHS. This potentially non-measurable set is then\nshown to consist of those functions that can be expanded in terms of an\northonormal basis of the RKHS of the covariance kernel of $X$ and have their\nsquared basis coefficients bounded away from zero and infinity, a result\nsuggested by the Karhunen-Lo\\`{e}ve theorem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:23:28 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:49:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Karvonen", "Toni", ""]]}, {"id": "2103.03200", "submitter": "Oskar Laverny", "authors": "Oskar Laverny, Esterina Masiello, V\\'eronique Maume-Deschamps and\n  Didier Rulli\\`ere", "title": "Estimation of multivariate generalized gamma convolutions through\n  Laguerre expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalized gamma convolutions class of distributions appeared in\nThorin's work while looking for the infinite divisibility of the log-Normal and\nPareto distributions. Although these distributions have been extensively\nstudied in the univariate case, the multivariate case and the dependence\nstructures that can arise from it have received little interest in the\nliterature. Furthermore, only one projection procedure for the univariate case\nwas recently constructed, and no estimation procedures are available. By\nexpanding the densities of multivariate generalized gamma convolutions into a\ntensorized Laguerre basis, we bridge the gap and provide performant estimation\nprocedures for both the univariate and multivariate cases. We provide some\ninsights about performance of these procedures, and a convergent series for the\ndensity of multivariate gamma convolutions, which is shown to be more stable\nthan Moschopoulos's and Mathai's univariate series. We furthermore discuss some\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:06:18 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 10:01:21 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 08:56:30 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Laverny", "Oskar", ""], ["Masiello", "Esterina", ""], ["Maume-Deschamps", "V\u00e9ronique", ""], ["Rulli\u00e8re", "Didier", ""]]}, {"id": "2103.03218", "submitter": "Alon Kipnis", "authors": "Alon Kipnis and David L. Donoho", "title": "On the Inability of the Higher Criticism to Detect Rare/Weak Departures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a multiple hypothesis testing setting involving rare/weak features:\nonly few features, out of possibly many, deviate from their null hypothesis\nbehavior. Summarizing the significance of each feature by a P-value, we\nconstruct a global test against the null using the Higher Criticism (HC)\nstatistics of these P-values. We calibrate the rare/weak model using parameters\ncontrolling the asymptotic behavior of these P-values in their near-zero\n\"tail\". We derive a region in the parameter space where the HC test is\nasymptotically powerless. Our derivation involves very different tools than\npreviously used to show the powerlessness of HC, relying on properties of the\nempirical processes underlying HC. In particular, our result applies to\nsituations where HC is not asymptotically optimal, or when the asymptotically\ndetectable region of the parameter space is unknown.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:56:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kipnis", "Alon", ""], ["Donoho", "David L.", ""]]}, {"id": "2103.03235", "submitter": "Guillaume Braun", "authors": "Guillaume Braun, Hemant Tyagi, Christophe Biernacki", "title": "Clustering multilayer graphs with missing nodes", "comments": "27 pages, 7 figures, accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationship between agents can be conveniently represented by graphs. When\nthese relationships have different modalities, they are better modelled by\nmultilayer graphs where each layer is associated with one modality. Such graphs\narise naturally in many contexts including biological and social networks.\nClustering is a fundamental problem in network analysis where the goal is to\nregroup nodes with similar connectivity profiles. In the past decade, various\nclustering methods have been extended from the unilayer setting to multilayer\ngraphs in order to incorporate the information provided by each layer. While\nmost existing works assume - rather restrictively - that all layers share the\nsame set of nodes, we propose a new framework that allows for layers to be\ndefined on different sets of nodes. In particular, the nodes not recorded in a\nlayer are treated as missing. Within this paradigm, we investigate several\ngeneralizations of well-known clustering methods in the complete setting to the\nincomplete one and prove some consistency results under the Multi-Layer\nStochastic Block Model assumption. Our theoretical results are complemented by\nthorough numerical comparisons between our proposed algorithms on synthetic\ndata, and also on real datasets, thus highlighting the promising behaviour of\nour methods in various settings.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:56:59 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Braun", "Guillaume", ""], ["Tyagi", "Hemant", ""], ["Biernacki", "Christophe", ""]]}, {"id": "2103.03471", "submitter": "Yanli Yuan", "authors": "Yanli Yuan, De Wen Soh, Xiao Yang, Kun Guo, Tony Q. S. Quek", "title": "Joint Network Topology Inference via Structured Fusion Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint network topology inference represents a canonical problem of jointly\nlearning multiple graph Laplacian matrices from heterogeneous graph signals. In\nsuch a problem, a widely employed assumption is that of a simple common\ncomponent shared among multiple networks. However, in practice, a more\nintricate topological pattern, comprising simultaneously of sparse, homogeneity\nand heterogeneity components, would exhibit in multiple networks. In this\npaper, we propose a general graph estimator based on a novel structured fusion\nregularization that enables us to jointly learn multiple graph Laplacian\nmatrices with such complex topological patterns, and enjoys both high\ncomputational efficiency and rigorous theoretical guarantee. Moreover, in the\nproposed regularization term, the topological pattern among networks is\ncharacterized by a Gram matrix, endowing our graph estimator with the ability\nof flexible modelling different types of topological patterns by different\nchoices of the Gram matrix. Computationally, the regularization term, coupling\nthe parameters together, makes the formulated optimization problem intractable\nand thus, we develop a computationally-scalable algorithm based on the\nalternating direction method of multipliers (ADMM) to solve it efficiently.\nTheoretically, we provide a theoretical analysis of the proposed graph\nestimator, which establishes a non-asymptotic bound of the estimation error\nunder the high-dimensional setting and reflects the effect of several key\nfactors on the convergence rate of our algorithm. Finally, the superior\nperformance of the proposed method is illustrated through simulated and real\ndata examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:42:32 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 06:21:00 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 05:38:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yuan", "Yanli", ""], ["Soh", "De Wen", ""], ["Yang", "Xiao", ""], ["Guo", "Kun", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2103.03606", "submitter": "Kilian Fatras", "authors": "Kilian Fatras, Thibault S\\'ejourn\\'e, Nicolas Courty, R\\'emi Flamary", "title": "Unbalanced minibatch Optimal Transport; applications to Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport distances have found many applications in machine learning\nfor their capacity to compare non-parametric probability distributions. Yet\ntheir algorithmic complexity generally prevents their direct use on large scale\ndatasets. Among the possible strategies to alleviate this issue, practitioners\ncan rely on computing estimates of these distances over subsets of data, {\\em\ni.e.} minibatches. While computationally appealing, we highlight in this paper\nsome limits of this strategy, arguing it can lead to undesirable smoothing\neffects. As an alternative, we suggest that the same minibatch strategy coupled\nwith unbalanced optimal transport can yield more robust behavior. We discuss\nthe associated theoretical properties, such as unbiased estimators, existence\nof gradients and concentration bounds. Our experimental study shows that in\nchallenging problems associated to domain adaptation, the use of unbalanced\noptimal transport leads to significantly better results, competing with or\nsurpassing recent baselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:15:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Fatras", "Kilian", ""], ["S\u00e9journ\u00e9", "Thibault", ""], ["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""]]}, {"id": "2103.03702", "submitter": "Nicy Sebastian", "authors": "G S Deepthy, Nicy Sebastian and Reshma Rison", "title": "Some Properties and Applications of Burr III-Weibull Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new distribution called Burr III-Weibull(BW)\ndistribution using the concept of competing risk. We derive moments,\nconditional moments, mean deviation and quantiles of the proposed distribution.\nAlso the Renyi's entropy and order statistics of the distribution are obtained.\nEstimation of parameters of the distribution is performed via maximum\nlikelihood method. A simulation study is performed to validate the maximum\nlikelihood estimator (MLE). A real practical data set is analyzed for\nillustration.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:25:53 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Deepthy", "G S", ""], ["Sebastian", "Nicy", ""], ["Rison", "Reshma", ""]]}, {"id": "2103.03723", "submitter": "Nicy Sebastian", "authors": "Nicy Sebastian and V R Rajitha", "title": "Different Estimation Procedures For Topp Leone Exponential And Topp\n  Leone q Exponential Distruibution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topp Leone q Exponential Distruibution is a continuous model distribution\nused for modelling lifetime phenomena. In this study, we introduce different\nestimation methods for the unknown parameters of Topp Leone Exponential(TLE)\ndistribution and Topp Leone q Exponential(TLqE) distribution.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:52:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Sebastian", "Nicy", ""], ["Rajitha", "V R", ""]]}, {"id": "2103.03771", "submitter": "Petter Restadh", "authors": "Svante Linusson, Petter Restadh, Liam Solus", "title": "Greedy Causal Discovery is Geometric", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding a directed acyclic graph (DAG) that best encodes the conditional\nindependence statements observable from data is a central question within\ncausality. Algorithms that greedily transform one candidate DAG into another\ngiven a fixed set of moves have been particularly successful, for example the\nGES, GIES, and MMHC algorithms. In 2010, Studen\\'y, Hemmecke and Lindner\nintroduced the characteristic imset polytope, $\\textrm{CIM}_p$, whose vertices\ncorrespond to Markov equivalence classes, as a way of transforming causal\ndiscovery into a linear optimization problem. We show that the moves of the\naforementioned algorithms are included within classes of edges of\n$\\textrm{CIM}_p$ and that restrictions placed on the skeleton of the candidate\nDAGs correspond to faces of $\\textrm{CIM}_p$. Thus, we observe that GES, GIES,\nand MMHC all have geometric realizations as greedy edge-walks along\n$\\textrm{CIM}_p$. Furthermore, the identified edges of $\\textrm{CIM}_p$\nstrictly generalize the moves of these algorithms. Exploiting this\ngeneralization, we introduce a greedy simplex-type algorithm called greedy CIM,\nand a hybrid variant, skeletal greedy CIM, that outperforms current competitors\namong hybrid and constraint-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:00:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Linusson", "Svante", ""], ["Restadh", "Petter", ""], ["Solus", "Liam", ""]]}, {"id": "2103.03999", "submitter": "Alon Kipnis", "authors": "Alon Kipnis", "title": "Unification of Rare/Weak Detection Models using Moderate Deviations\n  Analysis and Log-Chisquared P-values", "comments": "32 pages, 2 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rare/Weak models for multiple hypothesis testing assume that only a small\nproportion of the tested hypotheses concern non-null effects and the individual\neffects are only moderately large, so that they generally do not stand out\nindividually, for example in a Bonferroni analysis. Such rare/weak models have\nbeen studied in quite a few settings, for example in some cases studies focused\non underlying Gaussian means model for the hypotheses being tested; in some\nothers, Poisson. It seems not to have been noticed before that such seemingly\ndifferent models have asymptotically the following common structure:\nSummarizing the evidence each test provides by the negative logarithm of its\nP-value, previous rare/weak model settings are asymptotically equivalent to\ndetection where most negative log P-values have a standard exponential\ndistribution but a small fraction of the P-values might have an alternative\ndistribution which is moderately larger; we do not know which individual tests\nthose might be, or even if there are any such. Moreover, the alternative\ndistribution is noncentral chisquared on one degree of freedom. We characterize\nthe asymptotic performance of global tests combining these P-values in terms of\nthe chisquared mixture parameters: the scaling parameters controlling\nheteroscedasticity, the non-centrality parameter describing the effect size\nwhenever it exists, and the parameter controlling the rarity of the non-null\neffects. Specifically, in a phase space involving the last two parameters, we\nderive a region where all tests are asymptotically powerless. Outside of this\nregion, the Berk-Jones and the Higher Criticism tests have maximal power.\nInference techniques based on the minimal P-value, false-discovery rate\ncontrolling, and Fisher's test have sub-optimal asymptotic phase diagrams. We\nprovide various examples for multiple testing problems of the said common\nstructure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 01:06:50 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 19:36:29 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 00:25:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kipnis", "Alon", ""]]}, {"id": "2103.04014", "submitter": "Chuan-Zheng Lee", "authors": "Chuan-Zheng Lee, Leighton Pate Barnes and Ayfer Ozgur", "title": "Over-the-Air Statistical Estimation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study schemes and lower bounds for distributed minimax statistical\nestimation over a Gaussian multiple-access channel (MAC) under squared error\nloss, in a framework combining statistical estimation and wireless\ncommunication. First, we develop \"analog\" joint estimation-communication\nschemes that exploit the superposition property of the Gaussian MAC and we\ncharacterize their risk in terms of the number of nodes and dimension of the\nparameter space. Then, we derive information-theoretic lower bounds on the\nminimax risk of any estimation scheme restricted to communicate the samples\nover a given number of uses of the channel and show that the risk achieved by\nour proposed schemes is within a logarithmic factor of these lower bounds. We\ncompare both achievability and lower bound results to previous \"digital\" lower\nbounds, where nodes transmit errorless bits at the Shannon capacity of the MAC,\nshowing that estimation schemes that leverage the physical layer offer a\ndrastic reduction in estimation error over digital schemes relying on a\nphysical-layer abstraction.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:07:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lee", "Chuan-Zheng", ""], ["Barnes", "Leighton Pate", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2103.04177", "submitter": "Veronika Rockova", "authors": "Tetsuya Kaji and Veronika Rockova", "title": "Metropolis-Hastings via Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Bayesian computational platform at the interface\nbetween posterior sampling and optimization in models whose marginal\nlikelihoods are difficult to evaluate. Inspired by adversarial optimization,\nnamely Generative Adversarial Networks (GAN), we reframe the likelihood\nfunction estimation problem as a classification problem. Pitting a Generator,\nwho simulates fake data, against a Classifier, who tries to distinguish them\nfrom the real data, one obtains likelihood (ratio) estimators which can be\nplugged into the Metropolis-Hastings algorithm. The resulting Markov chains\ngenerate, at a steady state, samples from an approximate posterior whose\nasymptotic properties we characterize. Drawing upon connections with empirical\nBayes and Bayesian mis-specification, we quantify the convergence rate in terms\nof the contraction speed of the actual posterior and the convergence rate of\nthe Classifier. Asymptotic normality results are also provided which justify\ninferential potential of our approach. We illustrate the usefulness of our\napproach on examples which have posed a challenge for existing Bayesian\nlikelihood-free approaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:07:33 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:42:27 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaji", "Tetsuya", ""], ["Rockova", "Veronika", ""]]}, {"id": "2103.04211", "submitter": "Igor Cialenco", "authors": "Igor Cialenco, Hyun-Jung Kim, Gregor Pasemann", "title": "Statistical analysis of discretely sampled semilinear SPDEs: a power\n  variation approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motivated by problems from statistical analysis for discretely sampled SPDEs,\nfirst we derive central limit theorems for higher order finite differences\napplied to stochastic process with arbitrary finitely regular paths. These\nresults are proved by using the notion of $\\Delta$-power variations, introduced\nherein, along with the H\\\"older-Zygmund norms. Consequently, we prove a new\ncentral limit theorem for $\\Delta$-power variations of the iterated integrals\nof a fractional Brownian motion (fBm). These abstract results, besides being of\nindependent interest, in the second part of the paper are applied to estimation\nof the drift and volatility coefficients of semilinear stochastic partial\ndifferential equations in dimension one, driven by an additive Gaussian noise\nwhite in time and possibly colored in space. In particular, we solve the\nearlier conjecture from Cialenco, Kim, Lototsky (2019) about existence of a\nnontrivial bias in the estimators derived by naive approximations of\nderivatives by finite differences. We give an explicit formula for the bias and\nderive the convergence rates of the corresponding estimators. Theoretical\nresults are illustrated by numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 23:31:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cialenco", "Igor", ""], ["Kim", "Hyun-Jung", ""], ["Pasemann", "Gregor", ""]]}, {"id": "2103.04220", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie", "title": "Euclidean Representation of Low-Rank Matrices and Its Statistical\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrices are pervasive throughout statistics, machine learning,\nsignal processing, optimization, and applied mathematics. In this paper, we\npropose a novel and user-friendly Euclidean representation framework for\nlow-rank matrices. Correspondingly, we establish a collection of technical and\ntheoretical tools for analyzing the intrinsic perturbation of low-rank matrices\nin which the underlying referential matrix and the perturbed matrix both live\non the same low-rank matrix manifold. Our analyses show that, locally around\nthe referential matrix, the sine-theta distance between subspaces is equivalent\nto the Euclidean distance between two appropriately selected orthonormal basis,\ncircumventing the orthogonal Procrustes analysis. We also establish the\nregularity of the proposed Euclidean representation function, which has a\nprofound statistical impact and a meaningful geometric interpretation. These\ntechnical devices are applicable to a broad range of statistical problems.\nSpecific applications considered in detail include Bayesian sparse spiked\ncovariance model with non-intrinsic loss, efficient estimation in stochastic\nblock models where the block probability matrix may be degenerate, and\nleast-squares estimation in biclustering problems. Both the intrinsic\nperturbation analysis of low-rank matrices and the regularity theorem may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 00:46:48 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 22:26:24 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Xie", "Fangzheng", ""]]}, {"id": "2103.04424", "submitter": "Kristin Kirchner", "authors": "Helmut Harbrecht, Lukas Herrmann, Kristin Kirchner and Christoph\n  Schwab", "title": "Multilevel approximation of Gaussian random fields: Covariance\n  compression, estimation and spatial prediction", "comments": "48 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "SAM Report 2021-?09", "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centered Gaussian random fields (GRFs) indexed by compacta such as smooth,\nbounded Euclidean domains or smooth, compact and orientable manifolds are\ndetermined by their covariance operators. We consider centered GRFs given as\nvariational solutions to coloring operator equations driven by spatial white\nnoise, with an elliptic self-adjoint pseudodifferential coloring operator from\nthe H\\\"ormander class. This includes the Mat\\'ern class of GRFs as a special\ncase. Using biorthogonal multiresolution analyses on the manifold, we prove\nthat the precision and covariance operators, respectively, may be identified\nwith bi-infinite matrices and finite sections may be diagonally preconditioned\nrendering the condition number independent of the dimension $p$ of this\nsection. We prove that a tapering strategy by thresholding applied on finite\nsections of the bi-infinite precision and covariance matrices results in\noptimally numerically sparse approximations. That is, asymptotically only\nlinearly many nonzero matrix entries are sufficient to approximate the original\nsection of the bi-infinite covariance or precision matrix using this tapering\nstrategy to arbitrary precision. The locations of these nonzero matrix entries\nare known a priori. The tapered covariance or precision matrices may also be\noptimally diagonally preconditioned. Analysis of the relative size of the\nentries of the tapered covariance matrices motivates novel, multilevel Monte\nCarlo (MLMC) oracles for covariance estimation, in sample complexity that\nscales log-linearly with respect to the number $p$ of parameters. In addition,\nwe propose and analyze a novel compressive algorithm for simulating and kriging\nof GRFs. The complexity (work and memory vs. accuracy) of these three\nalgorithms scales near-optimally in terms of the number of parameters $p$ of\nthe sample-wise approximation of the GRF in Sobolev scales.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:45:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Harbrecht", "Helmut", ""], ["Herrmann", "Lukas", ""], ["Kirchner", "Kristin", ""], ["Schwab", "Christoph", ""]]}, {"id": "2103.04554", "submitter": "Zitong Yang", "authors": "Zitong Yang, Yu Bai, Song Mei", "title": "Exact Gap between Generalization Error and Uniform Convergence in Random\n  Feature Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work showed that there could be a large gap between the classical\nuniform convergence bound and the actual test error of zero-training-error\npredictors (interpolators) such as deep neural networks. To better understand\nthis gap, we study the uniform convergence in the nonlinear random feature\nmodel and perform a precise theoretical analysis on how uniform convergence\ndepends on the sample size and the number of parameters. We derive and prove\nanalytical expressions for three quantities in this model: 1) classical uniform\nconvergence over norm balls, 2) uniform convergence over interpolators in the\nnorm ball (recently proposed by Zhou et al. (2020)), and 3) the risk of minimum\nnorm interpolator. We show that, in the setting where the classical uniform\nconvergence bound is vacuous (diverges to $\\infty$), uniform convergence over\nthe interpolators still gives a non-trivial bound of the test error of\ninterpolating solutions. We also showcase a different setting where classical\nuniform convergence bound is non-vacuous, but uniform convergence over\ninterpolators can give an improved sample complexity guarantee. Our result\nprovides a first exact comparison between the test errors and uniform\nconvergence bounds for interpolators beyond simple linear models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:20:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yang", "Zitong", ""], ["Bai", "Yu", ""], ["Mei", "Song", ""]]}, {"id": "2103.04613", "submitter": "Clement Benesse", "authors": "Cl\\'ement B\\'enesse (IMT), Fabrice Gamboa (IMT), Jean-Michel Loubes\n  (IMT), Thibaut Boissin", "title": "Fairness seen as Global Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring that a predictor is not biased against a sensible feature is the key\nof Fairness learning. Conversely, Global Sensitivity Analysis is used in\nnumerous contexts to monitor the influence of any feature on an output\nvariable. We reconcile these two domains by showing how Fairness can be seen as\na special framework of Global Sensitivity Analysis and how various usual\nindicators are common between these two fields. We also present new Global\nSensitivity Analysis indices, as well as rates of convergence, that are useful\nas fairness proxies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:04:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["B\u00e9nesse", "Cl\u00e9ment", "", "IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Boissin", "Thibaut", ""]]}, {"id": "2103.04678", "submitter": "Una Radoji\\v{c}i\\'c", "authors": "Una Radojicic, Klaus Nordhausen and Joni Virta", "title": "Large-Sample Properties of Blind Estimation of the Linear Discriminant\n  Using Projection Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the linear discriminant with projection pursuit, a\nmethod that is blind in the sense that it does not use the class labels in the\nestimation. Our viewpoint is asymptotic and, as our main contribution, we\nderive central limit theorems for estimators based on three different\nprojection indices, skewness, kurtosis and their convex combination. The\nresults show that in each case the limiting covariance matrix is proportional\nto that of linear discriminant analysis (LDA), an unblind estimator of the\ndiscriminant. An extensive comparative study between the asymptotic variances\nreveals that projection pursuit is able to achieve efficiency equal to LDA when\nthe groups are arbitrarily well-separated and their sizes are reasonably\nbalanced. We conclude with a real data example and a simulation study\ninvestigating the validity of the obtained asymptotic formulas for finite\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:36:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Radojicic", "Una", ""], ["Nordhausen", "Klaus", ""], ["Virta", "Joni", ""]]}, {"id": "2103.04715", "submitter": "Remi Laumont", "authors": "R\\'emi Laumont, Valentin de Bortoli, Andr\\'es Almansa, Julie Delon,\n  Alain Durmus and Marcelo Pereyra", "title": "Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Venkatakrishnan et al. (2013), Plug & Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:46:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:01:21 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 16:24:03 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 12:11:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Laumont", "R\u00e9mi", ""], ["de Bortoli", "Valentin", ""], ["Almansa", "Andr\u00e9s", ""], ["Delon", "Julie", ""], ["Durmus", "Alain", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2103.04902", "submitter": "Francesca Mignacco", "authors": "Francesca Mignacco, Pierfrancesco Urbani, Lenka Zdeborov\\'a", "title": "Stochasticity helps to navigate rough landscapes: comparing\n  gradient-descent-based algorithms in the phase retrieval problem", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate how gradient-based algorithms such as gradient\ndescent, (multi-pass) stochastic gradient descent, its persistent variant, and\nthe Langevin algorithm navigate non-convex loss-landscapes and which of them is\nable to reach the best generalization error at limited sample complexity. We\nconsider the loss landscape of the high-dimensional phase retrieval problem as\na prototypical highly non-convex example. We observe that for phase retrieval\nthe stochastic variants of gradient descent are able to reach perfect\ngeneralization for regions of control parameters where the gradient descent\nalgorithm is not. We apply dynamical mean-field theory from statistical physics\nto characterize analytically the full trajectories of these algorithms in their\ncontinuous-time limit, with a warm start, and for large system sizes. We\nfurther unveil several intriguing properties of the landscape and the\nalgorithms such as that the gradient descent can obtain better generalization\nproperties from less informed initializations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:06:18 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:39:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mignacco", "Francesca", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2103.05134", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Santiago Paternain and Miguel Calvo-Fullana and\n  Alejandro Ribeiro", "title": "Constrained Learning with Non-Convex Losses", "comments": "arXiv admin note: text overlap with arXiv:2006.05487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though learning has become a core technology of modern information\nprocessing, there is now ample evidence that it can lead to biased, unsafe, and\nprejudiced solutions. The need to impose requirements on learning is therefore\nparamount, especially as it reaches critical applications in social,\nindustrial, and medical domains. However, the non-convexity of most modern\nlearning problems is only exacerbated by the introduction of constraints.\nWhereas good unconstrained solutions can often be learned using empirical risk\nminimization (ERM), even obtaining a model that satisfies statistical\nconstraints can be challenging, all the more so a good one. In this paper, we\novercome this issue by learning in the empirical dual domain, where constrained\nstatistical learning problems become unconstrained, finite dimensional, and\ndeterministic. We analyze the generalization properties of this approach by\nbounding the empirical duality gap, i.e., the difference between our\napproximate, tractable solution and the solution of the original\n(non-convex)~statistical problem, and provide a practical constrained learning\nalgorithm. These results establish a constrained counterpart of classical\nlearning theory and enable the explicit use of constraints in learning. We\nillustrate this algorithm and theory in rate-constrained learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:10:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Paternain", "Santiago", ""], ["Calvo-Fullana", "Miguel", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "2103.05197", "submitter": "Chuancun Yin", "authors": "Tong Pu, Narayanaswamy Balakrishnan, Chuancun Yin", "title": "An Identity for Expectations and Characteristic Function of Matrix\n  Variate Skew-normalDistribution with Applications to Associated Stochastic\n  Orderings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish an identity for E f (Y) -E f (X), when X and Y both have matrix\nvariateskew-normal distributions and the function f fulfills some weak\nconditions. Thecharacteristic function of matrix variate skew normal\ndistribution is then derived. Finally,we make use of it to derive some\nnecessary and sucient conditions for the comparisonof matrix variate\nskew-normal distributions under six di erent orders, such as usualstochastic\norder, convex order, increasing convex order, upper orthant order,directionally\nconvex order and supermodular order.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 03:21:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Pu", "Tong", ""], ["Balakrishnan", "Narayanaswamy", ""], ["Yin", "Chuancun", ""]]}, {"id": "2103.05201", "submitter": "Chuancun Yin", "authors": "Baishuai Zuo, Chuancun Yin", "title": "Multivariate tail covariance for generalized skew-elliptical\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the multivariate tail covariance (MTCov) for generalized\nskew-elliptical distributions is considered. Some special cases for this\ndistribution, such as generalized skew-normal, generalized skew student-t,\ngeneralized skew-logistic and generalized skew-Laplace distributions, are also\nconsidered. In order to test the theoretical feasibility of our results, the\nMTCov for skewed and non skewed normal distributions are computed and compared.\nFinally, we give a special formula of the MTCov for generalized skew-elliptical\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 03:33:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zuo", "Baishuai", ""], ["Yin", "Chuancun", ""]]}, {"id": "2103.05237", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Column randomization and almost-isometric embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix $A:\\mathbb{R}^n \\to \\mathbb{R}^m$ is $(\\delta,k)$-regular if for\nany $k$-sparse vector $x$, $$ \\left| \\|Ax\\|_2^2-\\|x\\|_2^2\\right| \\leq \\delta\n\\sqrt{k} \\|x\\|_2^2. $$ We show that if $A$ is $(\\delta,k)$-regular for $1 \\leq\nk \\leq 1/\\delta^2$, then by multiplying the columns of $A$ by independent\nrandom signs, the resulting random ensemble $A_\\epsilon$ acts on an arbitrary\nsubset $T \\subset \\mathbb{R}^n$ (almost) as if it were gaussian, and with the\noptimal probability estimate: if $\\ell_*(T)$ is the gaussian mean-width of $T$\nand $d_T=\\sup_{t \\in T} \\|t\\|_2$, then with probability at least\n$1-2\\exp(-c(\\ell_*(T)/d_T)^2)$, $$ \\sup_{t \\in T} \\left| \\|A_\\epsilon\nt\\|_2^2-\\|t\\|_2^2 \\right| \\leq C\\left(\\Lambda d_T \\delta\\ell_*(T)+(\\delta\n\\ell_*(T))^2 \\right), $$ where $\\Lambda=\\max\\{1,\\delta^2\\log(n\\delta^2)\\}$.\nThis estimate is optimal for $0<\\delta \\leq 1/\\sqrt{\\log n}$.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:55:41 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "2103.05243", "submitter": "Peizhong Ju", "authors": "Peizhong Ju, Xiaojun Lin, Ness B. Shroff", "title": "On the Generalization Power of Overfitted Two-Layer Neural Tangent\n  Kernel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization performance of min $\\ell_2$-norm\noverfitting solutions for the neural tangent kernel (NTK) model of a two-layer\nneural network. We show that, depending on the ground-truth function, the test\nerror of overfitted NTK models exhibits characteristics that are different from\nthe \"double-descent\" of other overparameterized linear models with simple\nFourier or Gaussian features. Specifically, for a class of learnable functions,\nwe provide a new upper bound of the generalization error that approaches a\nsmall limiting value, even when the number of neurons $p$ approaches infinity.\nThis limiting value further decreases with the number of training samples $n$.\nFor functions outside of this class, we provide a lower bound on the\ngeneralization error that does not diminish to zero even when $n$ and $p$ are\nboth large.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:24:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ju", "Peizhong", ""], ["Lin", "Xiaojun", ""], ["Shroff", "Ness B.", ""]]}, {"id": "2103.05264", "submitter": "B.L.S. Prakasa Rao", "authors": "B.L.S. Prakasa Rao", "title": "Parametric Estimation for Processes Driven by Infinite Dimensional Mixed\n  Fractional Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parametric and nonparametric inference for stochastic processes driven by a\nfractional Brownian motion were investigated in Mishura (2008) and Prakasa\nRao(2010) among others. Similar problems for processes driven by an infinite\ndimensional fractional Brownian motion were studied in Prakasa Rao (2004,2013),\nCialenco (2009) and others. Parametric estimation for processes driven by\ninfinite dimensional mixed fractional Brownian motion is discussed in this\narticle.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:25:26 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Rao", "B. L. S. Prakasa", ""]]}, {"id": "2103.05299", "submitter": "Maxime Sangnier", "authors": "Miguel Martinez Herrera (LPSM (UMR\\_8001)), Anna Bonnet (LPSM\n  (UMR\\_8001)), Miguel Herrera, Maxime Sangnier (LPSM (UMR\\_8001))", "title": "Maximum Likelihood Estimation for Hawkes Processes with self-excitation\n  or inhibition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a maximum likelihood method for estimating the\nparameters of a univariate Hawkes process with self-excitation or inhibition.\nOur work generalizes techniques and results that were restricted to the\nself-exciting scenario. The proposed estimator is implemented for the classical\nexponential kernel and we show that, in the inhibition context, our procedure\nprovides more accurate estimations than current alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:56:58 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:29:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Herrera", "Miguel Martinez", "", "LPSM"], ["Bonnet", "Anna", "", "LPSM"], ["Herrera", "Miguel", "", "LPSM"], ["Sangnier", "Maxime", "", "LPSM"]]}, {"id": "2103.05402", "submitter": "Yukun He", "authors": "Zhigang Bao, Yukun He", "title": "Quantitative CLT for linear eigenvalue statistics of Wigner matrices", "comments": "Minor updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we establish a near-optimal convergence rate for the CLT of\nlinear eigenvalue statistics of Wigner matrices, in Kolmogorov-Smirnov\ndistance. For all test functions $f\\in C^5(\\mathbb R)$, we show that the\nconvergence rate is either $N^{-1/2+\\varepsilon}$ or $N^{-1+\\varepsilon}$,\ndepending on the first Chebyshev coefficient of $f$ and the third moment of the\ndiagonal matrix entries. The condition that distinguishes these two rates is\nnecessary and sufficient. For a general class of test functions, we further\nidentify matching lower bounds for the convergence rates. In addition, we\nidentify an explicit, non-universal contribution in the linear eigenvalue\nstatistics, which is responsible for the slow rate $N^{-1/2+\\varepsilon}$ for\nnon-Gaussian ensembles. By removing this non-universal part, we show that the\nshifted linear eigenvalue statistics have the unified convergence rate\n$N^{-1+\\varepsilon}$ for all test functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:53:12 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:49:39 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Bao", "Zhigang", ""], ["He", "Yukun", ""]]}, {"id": "2103.05574", "submitter": "Qingyuan Zhao", "authors": "Etaash Katiyar and Qingyuan Zhao", "title": "On testing mean proportionality of multivariate normal variables", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note considers the problem of testing the null hypothesis that the\nmean values of two multivariate normal variables are proportional. We show that\nthe usual likelihood ratio $\\chi^2$-test is valid non-asymptotically. Our proof\nrelies on expressing the test statistic as the minimum eigenvalue of a Wishart\nvariable and using a representation of its distribution using Legendre\npolynomials.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:29:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Katiyar", "Etaash", ""], ["Zhao", "Qingyuan", ""]]}, {"id": "2103.05684", "submitter": "Kam\\'elia Daudel", "authors": "Kam\\'elia Daudel, Randal Douc and Fran\\c{c}ois Roueff", "title": "Monotonic Alpha-divergence Minimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel iterative algorithm which carries out\n$\\alpha$-divergence minimisation by ensuring a systematic decrease in the\n$\\alpha$-divergence at each step. In its most general form, our framework\nallows us to simultaneously optimise the weights and components parameters of a\ngiven mixture model. Notably, our approach permits to build on various methods\npreviously proposed for $\\alpha$-divergence minimisation such as gradient or\npower descent schemes. Furthermore, we shed a new light on an integrated\nExpectation Maximization algorithm. We provide empirical evidence that our\nmethodology yields improved results, all the while illustrating the numerical\nbenefits of having introduced some flexibility through the parameter $\\alpha$\nof the $\\alpha$-divergence.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:41:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Daudel", "Kam\u00e9lia", ""], ["Douc", "Randal", ""], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "2103.05747", "submitter": "Likun Zhang", "authors": "Likun Zhang and Benjamin A. Shaby", "title": "Asymptotic posterior normality of the generalized extreme value\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The univariate generalized extreme value (GEV) distribution is the most\ncommonly used tool for analysing the properties of rare events. The ever\ngreater utilization of Bayesian methods for extreme value analysis warrants\ndetailed theoretical investigation, which has thus far been underdeveloped.\nEven the most basic asymptotic results are difficult to obtain because the GEV\nfails to satisfy standard regularity conditions. Here, we prove that the\nposterior distribution of the GEV parameter vector, given an independent and\nidentically distributed sequence of observations, converges to a normal\ndistribution centred at the true parameter. The proof necessitates analysing\nintegrals of the GEV likelihood function over the entire parameter space, which\nrequires considerable care because the support of the GEV density depends on\nthe parameters in complicated ways.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:40:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin A.", ""]]}, {"id": "2103.05766", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj", "title": "Interpretable Machines: Constructing Valid Prediction Intervals with\n  Random Forests", "comments": "20 pages including four figures in the main article. Supplementary\n  material available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important issue when using Machine Learning algorithms in recent research\nis the lack of interpretability. Although these algorithms provide accurate\npoint predictions for various learning problems, uncertainty estimates\nconnected with point predictions are rather sparse. A contribution to this gap\nfor the Random Forest Regression Learner is presented here. Based on its\nOut-of-Bag procedure, several parametric and non-parametric prediction\nintervals are provided for Random Forest point predictions and theoretical\nguarantees for its correct coverage probability is delivered. In a second part,\na thorough investigation through Monte-Carlo simulation is conducted evaluating\nthe performance of the proposed methods from three aspects: (i) Analyzing the\ncorrect coverage rate of the proposed prediction intervals, (ii) Inspecting\ninterval width and (iii) Verifying the competitiveness of the proposed\nintervals with existing methods. The simulation yields that the proposed\nprediction intervals are robust towards non-normal residual distributions and\nare competitive by providing correct coverage rates and comparably narrow\ninterval lengths, even for comparably small samples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 23:05:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ramosaj", "Burim", ""]]}, {"id": "2103.06176", "submitter": "Philip Ernst", "authors": "Philip A. Ernst, Dongzhou Huang, and Frederi G. Viens", "title": "Yule's \"nonsense correlation\" for Gaussian random walks", "comments": "17 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to provide an exact formula for the second\nmoment of the empirical correlation of two independent Gaussian random walks,\nas well as implicit formulas for higher moments. The proofs are based on a\nsymbolically tractable integro-differential representation formula for the\nmoments of any order in a class of empirical correlations, first established by\n\\cite{ernst2019distribution} and investigated previously in\n\\cite{ernst2017yule}.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:48:24 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:48:51 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ernst", "Philip A.", ""], ["Huang", "Dongzhou", ""], ["Viens", "Frederi G.", ""]]}, {"id": "2103.06323", "submitter": "Dmytro Ivanenko Olexandrovych", "authors": "Dmytro Ivanenko, Rostyslav Pogorielov", "title": "Parameter estimation in diffusion models with low regularity\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The article considers parameter estimation constructing such as quasi-maximum\nlikelyhood estimation and one step estimation in statistical models generated\nby solution of stochastic differential equation. It has been developed a\nsoftware for parameter estimating and has been presented correspondent testing\nand comparing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:04:42 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ivanenko", "Dmytro", ""], ["Pogorielov", "Rostyslav", ""]]}, {"id": "2103.06324", "submitter": "Zhumengmeng Jin", "authors": "Zhumengmeng Jin and James P. Hobert", "title": "Dimension free convergence rates for Gibbs samplers for Bayesian linear\n  mixed models", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of big data has led to a growing interest in so-called\nconvergence complexity analysis, which is the study of how the convergence rate\nof a Monte Carlo Markov chain (for an intractable Bayesian posterior\ndistribution) scales as the underlying data set grows in size. Convergence\ncomplexity analysis of practical Monte Carlo Markov chains on continuous state\nspaces is quite challenging, and there have been very few successful analyses\nof such chains. One fruitful analysis was recently presented by Qin and Hobert\n(2021b), who studied a Gibbs sampler for a simple Bayesian random effects\nmodel. These authors showed that, under regularity conditions, the geometric\nconvergence rate of this Gibbs sampler converges to zero (indicating immediate\nconvergence) as the data set grows in size. It is shown herein that similar\nbehavior is exhibited by Gibbs samplers for more general Bayesian models that\npossess both random effects and traditional continuous covariates, the\nso-called mixed models. The analysis employs the Wasserstein-based techniques\nintroduced by Qin and Hobert (2021b).\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:06:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jin", "Zhumengmeng", ""], ["Hobert", "James P.", ""]]}, {"id": "2103.06392", "submitter": "Christopher Harshaw", "authors": "Christopher Harshaw, David Eisenstat, Vahab Mirrokni, Jean\n  Pouget-Abadie", "title": "Design and Analysis of Bipartite Experiments under a Linear\n  Exposure-Response Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bipartite experimental framework is a recently proposed causal setting,\nwhere a bipartite graph links two distinct types of units: units that receive\ntreatment and units whose outcomes are of interest to the experimenter. Often\nmotivated by market experiments, the bipartite experimental framework has been\nused for example to investigate the causal effects of supply-side changes on\ndemand-side behavior. Similar to settings with interference and other\nviolations of the stable unit treatment value assumption (SUTVA), additional\nassumptions on potential outcomes must be made for valid inference. In this\npaper, we consider the problem of estimating the average treatment effect in\nthe bipartite experimental framework under a linear exposure-response model. We\npropose the Exposure Reweighted Linear (ERL) Estimator, an unbiased linear\nestimator of the average treatment effect in this setting. Furthermore, we\npresent Exposure-Design, a cluster-based design which aims to increase the\nprecision of the ERL estimator by realizing desirable exposure distributions.\nFinally, we demonstrate the effectiveness of the proposed estimator and design\non a publicly available Amazon user-item review graph.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 00:12:57 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Harshaw", "Christopher", ""], ["Eisenstat", "David", ""], ["Mirrokni", "Vahab", ""], ["Pouget-Abadie", "Jean", ""]]}, {"id": "2103.06420", "submitter": "Kwangmin Lee", "authors": "Kwangmin Lee, Kyoungjae Lee and Jaeyong Lee", "title": "Estimation of Conditional Mean Operator under the Bandable Covariance\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional multivariate linear regression models, where the\njoint distribution of covariates and response variables is a multivariate\nnormal distribution with a bandable covariance matrix. The main goal of this\npaper is to estimate the regression coefficient matrix, which is a function of\nthe bandable covariance matrix. Although the tapering estimator of covariance\nhas the minimax optimal convergence rate for the class of bandable covariances,\nwe show that it has a sub-optimal convergence rate for the regression\ncoefficient; that is, a minimax estimator for the class of bandable covariances\nmay not be a minimax estimator for its functionals. We propose the blockwise\ntapering estimator of the regression coefficient, which has the minimax optimal\nconvergence rate for the regression coefficient under the bandable covariance\nassumption. We also propose a Bayesian procedure called the blockwise tapering\npost-processed posterior of the regression coefficient and show that the\nproposed Bayesian procedure has the minimax optimal convergence rate for the\nregression coefficient under the bandable covariance assumption. We show that\nthe proposed methods outperform the existing methods via numerical studies.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:48:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lee", "Kwangmin", ""], ["Lee", "Kyoungjae", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2103.06428", "submitter": "Will Wei Sun", "authors": "Hilda S Ibriga and Will Wei Sun", "title": "Covariate-assisted Sparse Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to provably complete a sparse and highly-missing tensor in the\npresence of covariate information along tensor modes. Our motivation comes from\nonline advertising where users click-through-rates (CTR) on ads over various\ndevices form a CTR tensor that has about 96% missing entries and has many zeros\non non-missing entries, which makes the standalone tensor completion method\nunsatisfactory. Beside the CTR tensor, additional ad features or user\ncharacteristics are often available. In this paper, we propose\nCovariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate\ninformation for the recovery of the sparse tensor. The key idea is to jointly\nextract latent components from both the tensor and the covariate matrix to\nlearn a synthetic representation. Theoretically, we derive the error bound for\nthe recovered tensor components and explicitly quantify the improvements on\nboth the reveal probability condition and the tensor recovery accuracy due to\ncovariates. Finally, we apply COSTCO to an advertisement dataset consisting of\na CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over\nthe baseline. An important by-product is that ad latent components from COSTCO\nreveal interesting ad clusters, which are useful for better ad targeting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:13:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ibriga", "Hilda S", ""], ["Sun", "Will Wei", ""]]}, {"id": "2103.06471", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje", "title": "Causal inference with misspecified exposure mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure mappings facilitate investigations of complex causal effects when\nunits interact in experiments. Current methods assume that the exposures are\ncorrectly specified, but such an assumption cannot be verified, and its\nvalidity is often questionable. This paper describes conditions under which one\ncan draw inferences about exposure effects when the exposures are misspecified.\nThe main result is a proof of consistency under mild conditions on the errors\nintroduced by the misspecification. The rate of convergence is determined by\nthe dependence between units' specification errors, and consistency is achieved\neven if the errors are large as long as they are sufficiently weakly dependent.\nIn other words, exposure effects can be precisely estimated also under\nmisspecification as long as the units' exposures are not misspecified in the\nsame way. The limiting distribution of the estimator is discussed. Asymptotic\nnormality is achieved under stronger conditions than those needed for\nconsistency. Similar conditions also facilitate conservative variance\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:35:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["S\u00e4vje", "Fredrik", ""]]}, {"id": "2103.06476", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith, David Arbour, Ritwik Sinha, Edward H. Kennedy, and\n  Aaditya Ramdas", "title": "Doubly robust confidence sequences for sequential causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives time-uniform confidence sequences (CS) for causal effects\nin experimental and observational settings. A confidence sequence for a target\nparameter $\\psi$ is a sequence of confidence intervals $(C_t)_{t=1}^\\infty$\nsuch that every one of these intervals simultaneously captures $\\psi$ with high\nprobability. Such CSs provide valid statistical inference for $\\psi$ at\narbitrary stopping times, unlike classical fixed-time confidence intervals\nwhich require the sample size to be fixed in advance. Existing methods for\nconstructing CSs focus on the nonasymptotic regime where certain assumptions\n(such as known bounds on the random variables) are imposed, while doubly robust\nestimators of causal effects rely on (asymptotic) semiparametric theory. We use\nsequential versions of central limit theorem arguments to construct\nlarge-sample CSs for causal estimands, with a particular focus on the average\ntreatment effect (ATE) under nonparametric conditions. These CSs allow analysts\nto update inferences about the ATE in lieu of new data, and experiments can be\ncontinuously monitored, stopped, or continued for any data-dependent reason,\nall while controlling the type-I error. Finally, we describe how these CSs\nreadily extend to other causal estimands and estimators, providing a new\nframework for sequential causal inference in a wide array of problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:45:35 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:50:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Arbour", "David", ""], ["Sinha", "Ritwik", ""], ["Kennedy", "Edward H.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.06483", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn and Kosaku Takanashi", "title": "Convergence of Computed Dynamic Models with Unbounded Shock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the asymptotic convergence of computed dynamic models when\nthe shock is unbounded. Most dynamic economic models lack a closed-form\nsolution. As such, approximate solutions by numerical methods are utilized.\nSince the researcher cannot directly evaluate the exact policy function and the\nassociated exact likelihood, it is imperative that the approximate likelihood\nasymptotically converges -- as well as to know the conditions of convergence --\nto the exact likelihood, in order to justify and validate its usage. In this\nregard, Fernandez-Villaverde, Rubio-Ramirez, and Santos (2006) show convergence\nof the likelihood, when the shock has compact support. However, compact support\nimplies that the shock is bounded, which is not an assumption met in most\ndynamic economic models, e.g., with normally distributed shocks. This paper\nprovides theoretical justification for most dynamic models used in the\nliterature by showing the conditions for convergence of the approximate\ninvariant measure obtained from numerical simulations to the exact invariant\nmeasure, thus providing the conditions for convergence of the likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:12:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Takanashi", "Kosaku", ""]]}, {"id": "2103.06691", "submitter": "Jan O. Bauer", "authors": "J. O. Bauer and B. Drabant", "title": "Overlap of OLS Regression and Principal Loading Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal loading analysis is a dimension reduction method that discards\nvariables which have only a small distorting effect on the covariance matrix.\nPotentially, principal loading analysis and ordinary least squares regression\ncoincide by construction. We contribute conditions under which both methods\nintersect. Further, we provide bounds for the cut-off value in principal\nloading analysis for the case of intersection. This gives a choice for such a\nthreshold based on the perturbation matrices.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:25:44 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 09:51:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bauer", "J. O.", ""], ["Drabant", "B.", ""]]}, {"id": "2103.06923", "submitter": "Sreejith Sreekumar Dr", "authors": "Sreejith Sreekumar, Zhengxin Zhang, Ziv Goldfeld", "title": "Non-Asymptotic Performance Guarantees for Neural Estimation of\n  $\\mathsf{f}$-Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical distances (SDs), which quantify the dissimilarity between\nprobability distributions, are central to machine learning and statistics. A\nmodern method for estimating such distances from data relies on parametrizing a\nvariational form by a neural network (NN) and optimizing it. These estimators\nare abundantly used in practice, but corresponding performance guarantees are\npartial and call for further exploration. In particular, there seems to be a\nfundamental tradeoff between the two sources of error involved: approximation\nand estimation. While the former needs the NN class to be rich and expressive,\nthe latter relies on controlling complexity. This paper explores this tradeoff\nby means of non-asymptotic error bounds, focusing on three popular choices of\nSDs -- Kullback-Leibler divergence, chi-squared divergence, and squared\nHellinger distance. Our analysis relies on non-asymptotic function\napproximation theorems and tools from empirical process theory. Numerical\nresults validating the theory are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:47:30 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Sreekumar", "Sreejith", ""], ["Zhang", "Zhengxin", ""], ["Goldfeld", "Ziv", ""]]}, {"id": "2103.07020", "submitter": "Seonho Kim", "authors": "Seonho Kim, Sohail Bahmani, and Kiryung Lee", "title": "Max-Linear Regression by Scalable and Guaranteed Convex Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multivariate max-linear regression problem where the model\nparameters\n$\\boldsymbol{\\beta}_{1},\\dotsc,\\boldsymbol{\\beta}_{k}\\in\\mathbb{R}^{p}$ need to\nbe estimated from $n$ independent samples of the (noisy) observations $y =\n\\max_{1\\leq j \\leq k} \\boldsymbol{\\beta}_{j}^{\\mathsf{T}} \\boldsymbol{x} +\n\\mathrm{noise}$. The max-linear model vastly generalizes the conventional\nlinear model, and it can approximate any convex function to an arbitrary\naccuracy when the number of linear models $k$ is large enough. However, the\ninherent nonlinearity of the max-linear model renders the estimation of the\nregression parameters computationally challenging. Particularly, no estimator\nbased on convex programming is known in the literature. We formulate and\nanalyze a scalable convex program as the estimator for the max-linear\nregression problem. Under the standard Gaussian observation setting, we present\na non-asymptotic performance guarantee showing that the convex program recovers\nthe parameters with high probability. When the $k$ linear components are\nequally likely to achieve the maximum, our result shows that a sufficient\nnumber of observations scales as $k^{2}p$ up to a logarithmic factor. This\nsignificantly improves on the analogous prior result based on alternating\nminimization (Ghosh et al., 2019). Finally, through a set of Monte Carlo\nsimulations, we illustrate that our theoretical result is consistent with\nempirical behavior, and the convex estimator for max-linear regression is as\ncompetitive as the alternating minimization algorithm in practice.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:55:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kim", "Seonho", ""], ["Bahmani", "Sohail", ""], ["Lee", "Kiryung", ""]]}, {"id": "2103.07039", "submitter": "Christian Caama\\~no", "authors": "Diego I. Gallardo, Marcelo Bourguignon, Yolanda M. G\\'omez, Christian\n  Caama\\~no-Carrillo", "title": "Parametric quantile regression models for fitting double bounded\n  response with application to COVID-19 mortality rate data", "comments": "17 pag", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop two fully parametric quantile regression models,\nbased on power Johnson SB distribution Cancho et al. (2020), for modeling unit\ninterval response at different quantiles. In particular, the conditional\ndistribution is modelled by the power Johnson SB distribution. The maximum\nlikelihood method is employed to estimate the model parameters. Simulation\nstudies are conducted to evaluate the performance of the maximum likelihood\nestimators in finite samples. Furthermore, we discuss residuals and influence\ndiagnostic tools. The effectiveness of our proposals is illustrated with two\ndata set given by the mortality rate of COVID-19 in different countries.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:00:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gallardo", "Diego I.", ""], ["Bourguignon", "Marcelo", ""], ["G\u00f3mez", "Yolanda M.", ""], ["Caama\u00f1o-Carrillo", "Christian", ""]]}, {"id": "2103.07095", "submitter": "Matey Neykov", "authors": "Michael Li, Matey Neykov, Sivaraman Balakrishnan", "title": "Minimax Optimal Conditional Density Estimation under Total Variation\n  Smoothness", "comments": "42 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the minimax rate of nonparametric conditional density\nestimation under a weighted absolute value loss function in a multivariate\nsetting. We first demonstrate that conditional density estimation is impossible\nif one only requires that $p_{X|Z}$ is smooth in $x$ for all values of $z$.\nThis motivates us to consider a sub-class of absolutely continuous\ndistributions, restricting the conditional density $p_{X|Z}(x|z)$ to not only\nbe H\\\"older smooth in $x$, but also be total variation smooth in $z$. We\npropose a corresponding kernel-based estimator and prove that it achieves the\nminimax rate. We give some simple examples of densities satisfying our\nassumptions which imply that our results are not vacuous. Finally, we propose\nan estimator which achieves the minimax optimal rate adaptively, i.e., without\nthe need to know the smoothness parameter values in advance. Crucially, both of\nour estimators (the adaptive and non-adaptive ones) impose no assumptions on\nthe marginal density $p_Z$, and are not obtained as a ratio between two kernel\nsmoothing estimators which may sound like a go to approach in this problem.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:35:02 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Li", "Michael", ""], ["Neykov", "Matey", ""], ["Balakrishnan", "Sivaraman", ""]]}, {"id": "2103.07198", "submitter": "Tino Werner", "authors": "Tino Werner", "title": "Quantitative robustness of instance ranking problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance ranking problems intend to recover the true ordering of the\ninstances in a data set with a variety of applications in for example\nscientific, social and financial contexts. Robust statistics studies the\nbehaviour of estimators in the presence of perturbations of the data resp. the\nunderlying distribution and provides different concepts to characterize local\nand global robustness. In this work, we concentrate on the global robustness of\nparametric ranking problems in terms of the breakdown point which measures the\nfraction of samples that need to be perturbed in order to let the estimator\ntake unreasonable values. However, existing breakdown point notions do not\ncover ranking problems so far. We propose to define a breakdown of the\nestimator as a sign-reversal of all components which causes the predicted\nranking to be inverted, therefore we call our concept the order-inversal\nbreakdown point (OIBDP). We will study the OIBDP, based on a linear model, for\nseveral different ranking problems that we carefully distinguish and provide\nleast favorable outlier configurations, characterizations of the order-inversal\nbreakdown point as well as sharp asymptotic upper bounds. We also outline the\ncase of SVM-type ranking estimators.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:41:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Werner", "Tino", ""]]}, {"id": "2103.07318", "submitter": "Claire Lacour", "authors": "Claire Lacour (LAMA), Thanh Mai Pham Ngoc (LMO)", "title": "Semiparametric inference for mixtures of circular data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider X 1 ,. .. , X n a sample of data on the circle S 1 , whose\ndistribution is a twocomponent mixture. Denoting R and Q two rotations on S 1 ,\nthe density of the X i 's is assumed to be g(x) = pf (R --1 x) + (1 -- p)f (Q\n--1 x), where p $\\in$ (0, 1) and f is an unknown density on the circle. In this\npaper we estimate both the parametric part $\\theta$ = (p, R, Q) and the\nnonparametric part f. The specific problems of identifiability on the circle\nare studied. A consistent estimator of $\\theta$ is introduced and its\nasymptotic normality is proved. We propose a Fourier-based estimator of f with\na penalized criterion to choose the resolution level. We show that our adaptive\nestimator is optimal from the oracle and minimax points of view when the\ndensity belongs to a Sobolev ball. Our method is illustrated by numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:42:00 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Lacour", "Claire", "", "LAMA"], ["Ngoc", "Thanh Mai Pham", "", "LMO"]]}, {"id": "2103.07553", "submitter": "Darinka Dentcheva", "authors": "Darinka Dentcheva and Yang Lin", "title": "Bias Reduction in Sample-Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider stochastic optimization problems which use observed data to\nestimate essential characteristics of the random quantities involved. Sample\naverage approximation (SAA) or empirical (plug-in) estimation are very popular\nways to use data in optimization. It is well known that sample average\noptimization suffers from downward bias. We propose to use smooth estimators\nrather than empirical ones in optimization problems. We establish consistency\nresults for the optimal value and the set of optimal solutions of the new\nproblem formulation. The performance of the proposed approach is compared to\nSAA theoretically and numerically. We analyze the bias of the new problems and\nidentify sufficient conditions for ensuring less biased estimation of the\noptimal value of the true problem. At the same time, the error of the new\nestimator remains controlled. We show that those conditions are satisfied for\nmany popular statistical problems such as regression models, classification\nproblems, and optimization problems with Average (Conditional) Value-at-Risk.\nWe have observed that smoothing the least-squares objective in a regression\nproblem by a normal kernel leads to a ridge regression. Our numerical\nexperience shows that the new estimators frequently exhibit also smaller\nvariance and smaller mean-square error than those of SAA.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 22:13:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Dentcheva", "Darinka", ""], ["Lin", "Yang", ""]]}, {"id": "2103.07608", "submitter": "Salah Abid", "authors": "Salah Hamza Abid and Uday J. Quaez", "title": "Renyi Entropy of Multivariate Autoregressive Moving Average Control\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Renyi entropy is an important measure of the information, it is proposed\nby Renyi in the context of entropy theory as a generalization of Shannon\nentropy. We study in detail this measure of multivariate autoregressive moving\naverage (ARMA) control systems. The characteristic function of output process\nis represented from the terms of its residual characteristic functions. Simple\nexpression to compute the Renyi entropy for the output process of these systems\nis derived. In addition, we investigate the covariance matrix for finding the\nupper bound of the entropy. Finally, we present three separate examples that\nserve to illustrate the behavior of information in a multivariate ARMA control\nsystem where the control and noise distributed as Gaussian, Cauchy and Laplace\nprocesses.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:29:32 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abid", "Salah Hamza", ""], ["Quaez", "Uday J.", ""]]}, {"id": "2103.07682", "submitter": "Antonio Di Crescenzo", "authors": "Antonio Di Crescenzo, Abdolsaeed Toomaj", "title": "Weighted mean inactivity time function with applications", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of mean inactivity time plays a crucial role in reliability, risk\ntheory and life testing. In this regard, we introduce a weighted mean\ninactivity time function by considering a non-negative weight function. Based\non this function, we provide expressions for the variance of transformed random\nvariable and the weighted generalized cumulative entropy. The latter concept is\nan important measure of uncertainty which is shift-dependent and is of interest\nin certain applied contexts, such as reliability or mathematical neurobiology.\nMoreover, based on the comparison of mean inactivity times of a certain\nfunction of two lifetime random variables, we introduce and study a new\nstochastic order in terms of the weighted mean inactivity time function.\nSeveral characterizations and preservation properties of the new order under\nshock models, random maxima and renewal theory are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:47:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Di Crescenzo", "Antonio", ""], ["Toomaj", "Abdolsaeed", ""]]}, {"id": "2103.07689", "submitter": "Vladimir Panov", "authors": "Vladimir Panov and Ekaterina Morozova", "title": "Extreme value analysis for mixture models with heavy-tailed impurity", "comments": "25 pages,6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the extreme value analysis for the triangular arrays,\nwhich appear when some parameters of the mixture model vary as the number of\nobservations grow. When the mixing parameter is small, it is natural to\nassociate one of the components with \"an impurity\" (in case of regularly\nvarying distribution, \"heavy-tailed impurity\"), which \"pollutes\" another\ncomponent. We show that the set of possible limit distributions is much more\ndiverse than in the classical Fisher-Tippett-Gnedenko theorem, and provide the\nnumerical examples showing the efficiency of the proposed model for studying\nthe maximal values of the stock returns.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 11:28:06 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 11:49:32 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Panov", "Vladimir", ""], ["Morozova", "Ekaterina", ""]]}, {"id": "2103.08092", "submitter": "Biraj Guha", "authors": "Biraj Subhra Guha and Debdeep Pati", "title": "Adaptive posterior convergence in sparse high dimensional clipped\n  generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a framework to study posterior contraction rates in sparse high\ndimensional generalized linear models (GLM). We introduce a new family of GLMs,\ndenoted by clipped GLM, which subsumes many standard GLMs and makes minor\nmodification of the rest. With a sparsity inducing prior on the regression\ncoefficients, we delineate sufficient conditions on true data generating\ndensity that leads to minimax optimal rates of posterior contraction of the\ncoefficients in $\\ell_1$ norm. Our key contribution is to develop sufficient\nconditions commensurate with the geometry of the clipped GLM family, propose\nprior distributions which do not require any knowledge of the true parameters\nand avoid any assumption on the growth rate of the true coefficient vector.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 01:34:28 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guha", "Biraj Subhra", ""], ["Pati", "Debdeep", ""]]}, {"id": "2103.08148", "submitter": "Andrey Pak", "authors": "Mohamed Abdelghani, Alexander Melnikov and Andrey Pak", "title": "On statistical estimation and inferences in optional regression models", "comments": "to be published in Statistics: A Journal of Theoretical and Applied\n  Statistics", "journal-ref": null, "doi": "10.1080/02331888.2021.1900186", "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main object of investigation in this paper is a very general regression\nmodel in optional setting - when an observed process is an optional\nsemimartingale depending on an unknown parameter. It is well-known that\nstatistical data may present an information flow/filtration without usual\nconditions. The estimation problem is achieved by means of structural least\nsquares (LS) estimates and their sequential versions. The main results of the\npaper are devoted to the strong consistency of such LS-estimates. For\nsequential LS-estimates the property of fixed accuracy is proved.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:50:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdelghani", "Mohamed", ""], ["Melnikov", "Alexander", ""], ["Pak", "Andrey", ""]]}, {"id": "2103.08402", "submitter": "Alexander Henzi", "authors": "Alexander Henzi and Johanna F. Ziegel", "title": "Valid sequential inference on probability forecast performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability forecasts for binary events play a central role in many\napplications. Their quality is commonly assessed with proper scoring rules,\nwhich assign forecasts a numerical score such that a correct forecast achieves\na minimal expected score. In this paper, we construct e-values for testing the\nstatistical significance of score differences of competing forecasts in\nsequential settings. E-values have been proposed as an alternative to p-values\nfor hypothesis testing, and they can easily be transformed into conservative\np-values by taking the multiplicative inverse. The e-values proposed in this\narticle are valid in finite samples without any assumptions on the data\ngenerating processes. They also allow optional stopping, so a forecast user may\ndecide to interrupt evaluation taking into account the available data at any\ntime and still draw statistically valid inference, which is generally not true\nfor classical p-value based tests. In a case study on postprocessing of\nprecipitation forecasts, state-of-the-art forecasts dominance tests and\ne-values lead to the same conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:18:03 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:22:24 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Henzi", "Alexander", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "2103.08512", "submitter": "Holger Drees", "authors": "Holger Drees, Anja Jan{\\ss}en, Sebastian Neblung", "title": "Cluster based inference for extremes of time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of estimator for the spectral tail process of a\nregularly varying time series. The approach is based on a characterizing\ninvariance property of the spectral tail process, which is incorporated into\nthe new estimator via a projection technique. We show uniform asymptotic\nnormality of this estimator, both in the case of known and of unknown index of\nregular variation. In a simulation study the new procedure shows a more stable\nperformance than previously proposed estimators.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:36:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Drees", "Holger", ""], ["Jan\u00dfen", "Anja", ""], ["Neblung", "Sebastian", ""]]}, {"id": "2103.08691", "submitter": "Wagner Barreto-Souza", "authors": "Gabriela Oliveira, Wagner Barreto-Souza and Roger W.C. Silva", "title": "Fractional Poisson random sum and its associated normal variance mixture", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the partial sums of independent and identically\ndistributed random variables with the number of terms following a fractional\nPoisson (FP) distribution. The FP sum contains the Poisson and geometric\nsummations as particular cases. We show that the weak limit of the FP\nsummation, when properly normalized, is a mixture between the normal and\nMittag-Leffler distributions, which we call by Normal-Mittag-Leffler (NML) law.\nA parameter estimation procedure for the NML distribution is developed and the\nassociated asymptotic distribution is derived. Simulations are performed to\ncheck the performance of the proposed estimators under finite samples. An\nempirical illustration on the daily log-returns of the Brazilian stock exchange\nindex (IBOVESPA) shows that the NML distribution captures better the tails than\nsome of its competitors. Related problems such as a mixed Poisson\nrepresentation for the FP law and the weak convergence for the\nConway-Maxwell-Poisson random sum are also addressed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:24:38 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 12:52:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Oliveira", "Gabriela", ""], ["Barreto-Souza", "Wagner", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2103.08721", "submitter": "Jinshuo Dong", "authors": "Jinshuo Dong, Weijie J. Su, Linjun Zhang", "title": "A Central Limit Theorem for Differentially Private Query Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perhaps the single most important use case for differential privacy is to\nprivately answer numerical queries, which is usually achieved by adding noise\nto the answer vector. The central question, therefore, is to understand which\nnoise distribution optimizes the privacy-accuracy trade-off, especially when\nthe dimension of the answer vector is high. Accordingly, extensive literature\nhas been dedicated to the question and the upper and lower bounds have been\nmatched up to constant factors [BUV18, SU17]. In this paper, we take a novel\napproach to address this important optimality question. We first demonstrate an\nintriguing central limit theorem phenomenon in the high-dimensional regime.\nMore precisely, we prove that a mechanism is approximately Gaussian\nDifferentially Private [DRS21] if the added noise satisfies certain conditions.\nIn particular, densities proportional to $\\mathrm{e}^{-\\|x\\|_p^\\alpha}$, where\n$\\|x\\|_p$ is the standard $\\ell_p$-norm, satisfies the conditions. Taking this\nperspective, we make use of the Cramer--Rao inequality and show an \"uncertainty\nprinciple\"-style result: the product of the privacy parameter and the\n$\\ell_2$-loss of the mechanism is lower bounded by the dimension. Furthermore,\nthe Gaussian mechanism achieves the constant-sharp optimal privacy-accuracy\ntrade-off among all such noises. Our findings are corroborated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:06:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dong", "Jinshuo", ""], ["Su", "Weijie J.", ""], ["Zhang", "Linjun", ""]]}, {"id": "2103.08846", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "A refined continuity correction for the negative binomial distribution\n  and asymptotics of the median", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove a local limit theorem and a refined continuity\ncorrection for the negative binomial distribution. We present two applications\nof the results. First, we find the asymptotics of the median for a Negative\nBinomial (r,p) random variable jittered by a Uniform (0,1), which answers a\nproblem left open in Coeurjolly \\& Tr\\'epanier (2020). This is used to\nconstruct a simple, robust and consistent estimator of the parameter $p$, when\n$r > 0$ is known. Second, we find an upper bound on the Le Cam distance between\nnegative binomial and normal experiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 04:25:50 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2103.08895", "submitter": "Dong Xia", "authors": "Jian-Feng Cai and Jingyang Li and Dong Xia", "title": "Generalized Low-rank plus Sparse Tensor Estimation by Fast Riemannian\n  Optimization", "comments": "corrected typos - Mar. 23", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a generalized framework to estimate a latent low-rank plus\nsparse tensor, where the low-rank tensor often captures the multi-way principal\ncomponents and the sparse tensor accounts for potential model\nmis-specifications or heterogeneous signals that are unexplainable by the\nlow-rank part. The framework is flexible covering both linear and non-linear\nmodels, and can easily handle continuous or categorical variables. We propose a\nfast algorithm by integrating the Riemannian gradient descent and a novel\ngradient pruning procedure. Under suitable conditions, the algorithm converges\nlinearly and can simultaneously estimate both the low-rank and sparse tensors.\nThe statistical error bounds of final estimates are established in terms of the\ngradient of loss function. The error bounds are generally sharp under specific\nstatistical models, e.g., the robust tensor PCA and the community detection in\nhypergraph networks with outlier vertices. Moreover, our method achieves\nnon-trivial error bounds for heavy-tailed tensor PCA whenever the noise has a\nfinite $2+\\varepsilon$ moment. We apply our method to analyze the international\ntrade flow dataset and the statistician hypergraph co-authorship network, both\nyielding new and interesting findings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:38:54 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:33:59 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Li", "Jingyang", ""], ["Xia", "Dong", ""]]}, {"id": "2103.09177", "submitter": "Alexander Rakhlin", "authors": "Peter L. Bartlett and Andrea Montanari and Alexander Rakhlin", "title": "Deep learning: a statistical viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The remarkable practical success of deep learning has revealed some major\nsurprises from a theoretical perspective. In particular, simple gradient\nmethods easily find near-optimal solutions to non-convex optimization problems,\nand despite giving a near-perfect fit to training data without any explicit\neffort to control model complexity, these methods exhibit excellent predictive\naccuracy. We conjecture that specific principles underlie these phenomena: that\noverparametrization allows gradient methods to find interpolating solutions,\nthat these methods implicitly impose regularization, and that\noverparametrization leads to benign overfitting. We survey recent theoretical\nprogress that provides examples illustrating these principles in simpler\nsettings. We first review classical uniform convergence results and why they\nfall short of explaining aspects of the behavior of deep learning methods. We\ngive examples of implicit regularization in simple settings, where gradient\nmethods lead to minimal norm functions that perfectly fit the training data.\nThen we review prediction methods that exhibit benign overfitting, focusing on\nregression problems with quadratic loss. For these methods, we can decompose\nthe prediction rule into a simple component that is useful for prediction and a\nspiky component that is useful for overfitting but, in a favorable setting,\ndoes not harm prediction accuracy. We focus specifically on the linear regime\nfor neural networks, where the network can be approximated by a linear model.\nIn this regime, we demonstrate the success of gradient flow, and we consider\nbenign overfitting with two-layer networks, giving an exact asymptotic analysis\nthat precisely demonstrates the impact of overparametrization. We conclude by\nhighlighting the key challenges that arise in extending these insights to\nrealistic deep learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:26:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Montanari", "Andrea", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "2103.09267", "submitter": "Tudor Manole", "authors": "Tudor Manole, Aaditya Ramdas", "title": "Sequential Estimation of Convex Divergences using Reverse Submartingales\n  and Exchangeable Filtrations", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified technique for sequential estimation of convex\ndivergences between distributions, including integral probability metrics like\nthe kernel maximum mean discrepancy, $\\varphi$-divergences like the\nKullback-Leibler divergence, and optimal transport costs, such as powers of\nWasserstein distances. The technical underpinnings of our approach lie in the\nobservation that empirical convex divergences are (partially ordered) reverse\nsubmartingales with respect to the exchangeable filtration, coupled with\nmaximal inequalities for such processes. These techniques appear to be powerful\nadditions to the existing literature on both confidence sequences and convex\ndivergences. We construct an offline-to-sequential device that converts a wide\narray of existing offline concentration inequalities into time-uniform\nconfidence sequences that can be continuously monitored, providing valid\ninference at arbitrary stopping times. The resulting sequential bounds pay only\nan iterated logarithmic price over the corresponding fixed-time bounds,\nretaining the same dependence on problem parameters (like dimension or alphabet\nsize if applicable).\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:22:14 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Manole", "Tudor", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.09324", "submitter": "Theodore Hill", "authors": "Ronald F. Fox and Theodore P. Hill", "title": "A Note on Over- and Under-Representation Among Populations with\n  Normally-Distributed Traits", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among several subpopulations of a given species with a normally-distributed\ntrait, such as height, blood pressure, or various test scores in humans, one\nand only one of the subpopulations will always strongly dominate all the others\nin the right tail, that is, in the high-end values of that trait. Examples are\ngiven to show that this is not true in general for other common classes of\ncontinuous centrally-symmetric unimodal distributions such as Laplace, nor even\nfor other bell-shaped distributions such as Cauchy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:01:00 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Fox", "Ronald F.", ""], ["Hill", "Theodore P.", ""]]}, {"id": "2103.09383", "submitter": "Dana Yang", "authors": "Jian Ding, Yihong Wu, Jiaming Xu, Dana Yang", "title": "The planted matching problem: Sharp threshold and infinite-order phase\n  transition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.CO math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of reconstructing a perfect matching $M^*$ hidden in a\nrandomly weighted $n\\times n$ bipartite graph. The edge set includes every node\npair in $M^*$ and each of the $n(n-1)$ node pairs not in $M^*$ independently\nwith probability $d/n$. The weight of each edge $e$ is independently drawn from\nthe distribution $\\mathcal{P}$ if $e \\in M^*$ and from $\\mathcal{Q}$ if $e\n\\notin M^*$. We show that if $\\sqrt{d} B(\\mathcal{P},\\mathcal{Q}) \\le 1$, where\n$B(\\mathcal{P},\\mathcal{Q})$ stands for the Bhattacharyya coefficient, the\nreconstruction error (average fraction of misclassified edges) of the maximum\nlikelihood estimator of $M^*$ converges to $0$ as $n\\to \\infty$. Conversely, if\n$\\sqrt{d} B(\\mathcal{P},\\mathcal{Q}) \\ge 1+\\epsilon$ for an arbitrarily small\nconstant $\\epsilon>0$, the reconstruction error for any estimator is shown to\nbe bounded away from $0$ under both the sparse and dense model, resolving the\nconjecture in [Moharrami et al. 2019, Semerjian et al. 2020]. Furthermore, in\nthe special case of complete exponentially weighted graph with $d=n$,\n$\\mathcal{P}=\\exp(\\lambda)$, and $\\mathcal{Q}=\\exp(1/n)$, for which the sharp\nthreshold simplifies to $\\lambda=4$, we prove that when $\\lambda \\le\n4-\\epsilon$, the optimal reconstruction error is $\\exp\\left( -\n\\Theta(1/\\sqrt{\\epsilon}) \\right)$, confirming the conjectured infinite-order\nphase transition in [Semerjian et al. 2020].\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:59:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ding", "Jian", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""], ["Yang", "Dana", ""]]}, {"id": "2103.09718", "submitter": "Zhaoxia Yu", "authors": "Dustin Pluta, Xiangmin Xu, Daniel L. Gillen, Zhaoxia Yu", "title": "A Measurement of In-Betweenness and Inference Based on Shape Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a statistical framework to investigate whether a given\nsubpopulation lies between two other subpopulations in a multivariate feature\nspace. This methodology is motivated by a biological question from a\ncollaborator: Is a newly discovered cell type between two known types in\nseveral given features? We propose two in-betweenness indices (IBI) to quantify\nthe in-betweenness exhibited by a random triangle formed by the summary\nstatistics of the three subpopulations. Statistical inference methods are\nprovided for triangle shape and IBI metrics. The application of our methods is\ndemonstrated in three examples: the classic Iris data set, a study of risk of\nrelapse across three breast cancer subtypes, and the motivating neuronal cell\ndata with measured electrophysiological features.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:22:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pluta", "Dustin", ""], ["Xu", "Xiangmin", ""], ["Gillen", "Daniel L.", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.09794", "submitter": "Philip Ernst", "authors": "Philip A. Ernst, Abram M. Kagan, and L.C.G. Rogers", "title": "The least favorable noise", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Suppose that a random variable $X$ of interest is observed perturbed by\nindependent additive noise $Y$. This paper concerns the \"the least favorable\nperturbation\" $\\hat Y_\\ep$, which maximizes the prediction error\n$E(X-E(X|X+Y))^2$ in the class of $Y$ with $ \\var (Y)\\leq \\ep$. We find a\ncharacterization of the answer to this question, and show by example that it\ncan be surprisingly complicated. However, in the special case where $X$ is\ninfinitely divisible, the solution is complete and simple. We also explore the\nconjecture that noisier $Y$ makes prediction worse.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:30:31 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ernst", "Philip A.", ""], ["Kagan", "Abram M.", ""], ["Rogers", "L. C. G.", ""]]}, {"id": "2103.10058", "submitter": "Rolf Larsson", "authors": "Rolf Larsson", "title": "Bartlett correction of an independence test in a multivariate Poisson\n  model", "comments": "66 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a system of dependent Poisson variables, where each variable is\nthe sum of an independent variate and a common variate. It is the common\nvariate that creates the dependence. Within this system, a test of independence\nmay be constructed where the null hypothesis is that the common variate is\nidentically zero. In the present paper, we consider the maximum log likelihood\nratio test. For this test, it is well-known that the asymptotic distribution of\nthe test statistic is an equal mixture of zero and a chi square distribution\nwith one degree of freedom. We examine a Bartlett correction of this test, in\nthe hope that we will get better approximation of the nominal size for\nmoderately large sample sizes. This correction is explicitly derived, and its\nusefulness is explored in a simulation study. For practical purposes, the\ncorrection is found to be useful in dimension two, but not in higher\ndimensions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:39:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Larsson", "Rolf", ""]]}, {"id": "2103.10070", "submitter": "Clemence Reda", "authors": "Cl\\'emence R\\'eda (UP M\\'edecine Paris Nord, INSERM), Emilie Kaufmann\n  (CNRS, Lille DECCID SID), Andr\\'ee Delahaye-Duriez (UP M\\'edecine Paris Nord,\n  INSERM)", "title": "Top-m identification for linear bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application to drug repurposing, we propose the first\nalgorithms to tackle the identification of the m $\\ge$ 1 arms with largest\nmeans in a linear bandit model, in the fixed-confidence setting. These\nalgorithms belong to the generic family of Gap-Index Focused Algorithms (GIFA)\nthat we introduce for Top-m identification in linear bandits. We propose a\nunified analysis of these algorithms, which shows how the use of features might\ndecrease the sample complexity. We further validate these algorithms\nempirically on simulated data and on a simple drug repurposing task.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:04:45 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["R\u00e9da", "Cl\u00e9mence", "", "UP M\u00e9decine Paris Nord, INSERM"], ["Kaufmann", "Emilie", "", "CNRS, Lille DECCID SID"], ["Delahaye-Duriez", "Andr\u00e9e", "", "UP M\u00e9decine Paris Nord,\n  INSERM"]]}, {"id": "2103.10236", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Matteo Bottai", "title": "Confidence Regions Near Singular Information and Boundary Points With\n  Applications to Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose confidence regions with asymptotically correct uniform coverage\nprobability of parameters whose Fisher information matrix can be singular at\nimportant points of the parameter set. Our work is motivated by the need for\nreliable inference on scale parameters close or equal to zero in mixed models,\nwhich is obtained as a special case. The confidence regions are constructed by\ninverting a continuous extension of the score test statistic standardized by\nexpected information, which we show exists at points of singular information\nunder regularity conditions. Similar results have previously only been obtained\nfor scalar parameters, under conditions stronger than ours, and applications to\nmixed models have not been considered. In simulations our confidence regions\nhave near-nominal coverage with as few as $n = 20$ independent observations,\nregardless of how close to the boundary the true parameter is. It is a\ncorollary of our main results that the proposed test statistic has an\nasymptotic chi-square distribution with degrees of freedom equal to the number\nof tested parameters, even if they are on the boundary of the parameter set.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:17:26 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 12:29:47 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Bottai", "Matteo", ""]]}, {"id": "2103.10420", "submitter": "Gianluca Finocchio", "authors": "G. Finocchio, A. Derumigny and K. Proksch", "title": "Robust-to-outliers square-root LASSO, simultaneous inference with a MOM\n  approach", "comments": "70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the least-squares regression problem with unknown noise variance,\nwhere the observed data points are allowed to be corrupted by outliers.\nBuilding on the median-of-means (MOM) method introduced by Lecue and Lerasle\nAnn.Statist.48(2):906-931(April 2020) in the case of known noise variance, we\npropose a general MOM approach for simultaneous inference of both the\nregression function and the noise variance, requiring only an upper bound on\nthe noise level. Interestingly, this generalization requires care due to\nregularity issues that are intrinsic to the underlying convex-concave\noptimization problem. In the general case where the regression function belongs\nto a convex class, we show that our simultaneous estimator achieves with high\nprobability the same convergence rates and a similar risk bound as if the noise\nlevel was unknown, as well as convergence rates for the estimated noise\nstandard deviation.\n  In the high-dimensional sparse linear setting, our estimator yields a robust\nanalog of the square-root LASSO. Under weak moment conditions, it jointly\nachieves with high probability the minimax rates of estimation $s^{1/p}\n\\sqrt{(1/n) \\log(p/s)}$ for the $\\ell_p$-norm of the coefficient vector, and\nthe rate $\\sqrt{(s/n) \\log(p/s)}$ for the estimation of the noise standard\ndeviation. Here $n$ denotes the sample size, $p$ the dimension and $s$ the\nsparsity level. We finally propose an extension to the case of unknown sparsity\nlevel $s$, providing a jointly adaptive estimator $(\\widetilde \\beta,\n\\widetilde \\sigma, \\widetilde s)$. It simultaneously estimates the coefficient\nvector, the noise level and the sparsity level, with proven bounds on each of\nthese three components that hold with high probability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:55:18 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Finocchio", "G.", ""], ["Derumigny", "A.", ""], ["Proksch", "K.", ""]]}, {"id": "2103.10471", "submitter": "Nadjib Bouzar", "authors": "Emad-Eldin AA Aly and Nadjib Bouzar", "title": "Stationary underdispersed INAR(1) models based on the backward approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of the stationary first-order autoregressive integer-valued (INAR(1))\nmodels were developed for a given thinning operator using either the forward\napproach or the backward approach. In the forward approach the marginal\ndistribution of the time series is specified and an appropriate distribution\nfor the innovation sequence is sought. Whereas in the backward setting, the\nroles are reversed. The common distribution of the innovation sequence is\nspecified and the distributional properties of the marginal distribution of the\ntime series are studied. In this article we focus on the backward approach in\npresence of the Binomial thinning operator. We establish a number of\ntheoretical results which we proceed to use to develop stationary INAR(1)\nmodels with finite mean. We illustrate our results by presenting some new\nINAR(1) models that show underdispersion.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:36:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Aly", "Emad-Eldin AA", ""], ["Bouzar", "Nadjib", ""]]}, {"id": "2103.10540", "submitter": "Daniel Bonn\\'ery", "authors": "Daniel Bonnery, Francesco Pantalone and M. Giovanna Ranalli", "title": "The effect of Informative Selection on the estimation of parameters\n  related to Spatial Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the concept of informative selection, population\ndistribution and sample distribution to a spatial process context. These\nnotions were first defined in a context where the output of the random process\nof interest consists of independent and identically distributed realisations\nfor each individual of a population. It has been showed that informative\nselection was inducing a stochastic dependence among realisations on the\nselected units. In the context of spatial processes, the \"population\" is a\ncontinuous space and realisations for two different elements of the population\nare not independent. We show how informative selection may induce a different\ndependence among selected units and how the sample distribution differs from\nthe population distribution.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:49:44 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Bonnery", "Daniel", ""], ["Pantalone", "Francesco", ""], ["Ranalli", "M. Giovanna", ""]]}, {"id": "2103.10549", "submitter": "Ali Amiryousefi", "authors": "Ali Amiryousefi", "title": "Inductive Inference in Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inductive inference in supervised classification context constitutes to\nmethods and approaches to assign some objects or items into different\npredefined classes using a formal rule that is derived from training data and\npossibly some additional auxiliary information. The optimality of such an\nassignment varies under different conditions due to intrinsic attributes of the\nobjects being considered for such a task. One of these cases is when all the\nobjects' features are discrete variables with a priori known categories. As\nanother example, one can consider a modification of this case with a priori\nunknown categories. These two cases are the main focus of this thesis and based\non Bayesian inductive theories, de Finetti type exchangeability is a suitable\nassumption that facilitates the derivation of classifiers in the former\nscenario. On the contrary, this type of exchangeability is not applicable in\nthe latter case, instead, it is possible to utilise the partition\nexchangeability due to John Kingman. These two types of exchangeabilities are\ndiscussed and furthermore here I investigate inductive supervised classifiers\nbased on both types of exchangeabilities. I further demonstrate that the\nclassifiers based on de Finetti type exchangeability can optimally handle test\nitems independently of each other in the presence of infinite amounts of\ntraining data while on the other hand, classifiers based on partition\nexchangeability still continue to benefit from joint labelling of all the test\nitems. Additionally, it is shown that the inductive learning process for the\nsimultaneous classifier saturates when the amount of test data tends to\ninfinity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:25:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Amiryousefi", "Ali", ""]]}, {"id": "2103.10568", "submitter": "Holger Dette", "authors": "Eftychia Solea and Holger Dette", "title": "Nonparametric and high-dimensional functional graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing nonparametric undirected graphical\nmodels for high-dimensional functional data. Most existing statistical methods\nin this context assume either a Gaussian distribution on the vertices or linear\nconditional means. In this article we provide a more flexible model which\nrelaxes the linearity assumption by replacing it by an arbitrary additive form.\nThe use of functional principal components offers an estimation strategy that\nuses a group lasso penalty to estimate the relevant edges of the graph. We\nestablish statistical guarantees for the resulting estimators, which can be\nused to prove consistency if the dimension and the number of functional\nprincipal components diverge to infinity with the sample size. We also\ninvestigate the empirical performance of our method through simulation studies\nand a real data application.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:41:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Solea", "Eftychia", ""], ["Dette", "Holger", ""]]}, {"id": "2103.10606", "submitter": "Zejian Liu", "authors": "Meng Li, Zejian Liu, Cheng-Han Yu, Marina Vannucci", "title": "Semiparametric Bayesian Inference for Local Extrema of Functions in the\n  Presence of Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a wide range of applications where the local extrema of a function\nare the key quantity of interest. However, there is surprisingly little work on\nmethods to infer local extrema with uncertainty quantification in the presence\nof noise. By viewing the function as an infinite-dimensional nuisance\nparameter, a semiparametric formulation of this problem poses daunting\nchallenges, both methodologically and theoretically, as (i) the number of local\nextrema may be unknown, and (ii) the induced shape constraints associated with\nlocal extrema are highly irregular. In this article, we address these\nchallenges by suggesting an encompassing strategy that eliminates the need to\nspecify the number of local extrema, which leads to a remarkably simple, fast\nsemiparametric Bayesian approach for inference on local extrema. We provide\nclosed-form characterization of the posterior distribution and study its large\nsample behaviors under this encompassing regime. We show a multi-modal\nBernstein-von Mises phenomenon in which the posterior measure converges to a\nmixture of Gaussians with the number of components matching the underlying\ntruth, leading to posterior exploration that accounts for multi-modality. We\nillustrate the method through simulations and a real data application to\nevent-related potential analysis.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:58:07 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Meng", ""], ["Liu", "Zejian", ""], ["Yu", "Cheng-Han", ""], ["Vannucci", "Marina", ""]]}, {"id": "2103.10660", "submitter": "Atsushi Iwasaki", "authors": "Yasunari Hikima, Atsushi Iwasaki, and Ken Umeno", "title": "The reference distributions of Maurer's universal statistical test and\n  its improved tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maurer's universal statistical test can widely detect non-randomness of given\nsequences. Coron proposed an improved test, and further Yamamoto and Liu\nproposed a new test based on Coron's test. These tests use normal distributions\nas their reference distributions, but the soundness has not been theoretically\ndiscussed so far. Additionally, Yamamoto and Liu's test uses an experimental\nvalue as the variance of its reference distribution. In this paper, we\ntheoretically derive the variance of the reference distribution of Yamamoto and\nLiu's test and prove that the true reference distribution of Coron's test\nconverges to a normal distribution in some sense. We can apply the proof to the\nother tests with small changes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:58:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Hikima", "Yasunari", ""], ["Iwasaki", "Atsushi", ""], ["Umeno", "Ken", ""]]}, {"id": "2103.10720", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu, Kengo Kato, Xiaofeng Shao", "title": "Gaussian approximation and spatially dependent wild bootstrap for\n  high-dimensional spatial data", "comments": "63pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we establish a high-dimensional CLT for the sample mean of\n$p$-dimensional spatial data observed over irregularly spaced sampling sites in\n$\\mathbb{R}^d$, allowing the dimension $p$ to be much larger than the sample\nsize $n$. We adopt a stochastic sampling scheme that can generate irregularly\nspaced sampling sites in a flexible manner and include both pure increasing\ndomain and mixed increasing domain frameworks. To facilitate statistical\ninference, we develop the spatially dependent wild bootstrap (SDWB) and justify\nits asymptotic validity in high dimensions by deriving error bounds that hold\nalmost surely conditionally on the stochastic sampling sites. Our dependence\nconditions on the underlying random field cover a wide class of random fields\nsuch as Gaussian random fields and continuous autoregressive moving average\nrandom fields. Through numerical simulations and a real data analysis, we\ndemonstrate the usefulness of our bootstrap-based inference in several\napplications, including joint confidence interval construction for\nhigh-dimensional spatial data and change-point detection for spatio-temporal\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 10:27:36 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:11:26 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kurisu", "Daisuke", ""], ["Kato", "Kengo", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2103.10739", "submitter": "Veronique Maume-Deschamps", "authors": "Manaf Ahmed, V\\'eronique Maume-Deschamps (ICJ, PSPM), Pierre Ribereau\n  (ICJ, PSPM)", "title": "Recognizing a Spatial Extreme dependence structure: A Deep Learning\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviour of environmental extreme events is crucial for\nevaluating economic losses, assessing risks, health care and many other\naspects. In the spatial context, relevant for environmental events, the\ndependence structure plays a central rule, as it influence joined extreme\nevents and extrapolation on them. So that, recognising or at least having\npreliminary informations on patterns of these dependence structures is a\nvaluable knowledge for understanding extreme events. In this study, we address\nthe question of automatic recognition of spatial Asymptotic Dependence (AD)\nversus Asymptotic independence (AI), using Convolutional Neural Network (CNN).\nWe have designed an architecture of Convolutional Neural Network to be an\nefficient classifier of the dependence structure. Upper and lower tail\ndependence measures are used to train the CNN. We have tested our methodology\non simulated and real data sets: air temperature data at two meter over Iraq\nland and Rainfall data in the east cost of Australia.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:22:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ahmed", "Manaf", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"], ["Ribereau", "Pierre", "", "ICJ, PSPM"]]}, {"id": "2103.10949", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum and Stefan Steinerberger", "title": "Refined Least Squares for Support Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of exact support recovery based on noisy observations\nand present Refined Least Squares (RLS). Given a set of noisy measurement $$\n\\myvec{y} = \\myvec{X}\\myvec{\\theta}^* + \\myvec{\\omega},$$ and $\\myvec{X} \\in\n\\mathbb{R}^{N \\times D}$ which is a (known) Gaussian matrix and $\\myvec{\\omega}\n\\in \\mathbb{R}^N$ is an (unknown) Gaussian noise vector, our goal is to recover\nthe support of the (unknown) sparse vector $\\myvec{\\theta}^* \\in\n\\left\\{-1,0,1\\right\\}^D$. To recover the support of the $\\myvec{\\theta}^*$ we\nuse an average of multiple least squares solutions, each computed based on a\nsubset of the full set of equations. The support is estimated by identifying\nthe most significant coefficients of the average least squares solution. We\ndemonstrate that in a wide variety of settings our method outperforms\nstate-of-the-art support recovery algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:56:43 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "2103.11003", "submitter": "Po-Ling Loh", "authors": "Marco Avella-Medina, Casey Bradshaw, Po-Ling Loh", "title": "Differentially private inference via noisy optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general optimization-based framework for computing\ndifferentially private M-estimators and a new method for constructing\ndifferentially private confidence regions. Firstly, we show that robust\nstatistics can be used in conjunction with noisy gradient descent or noisy\nNewton methods in order to obtain optimal private estimators with global linear\nor quadratic convergence, respectively. We establish local and global\nconvergence guarantees, under both local strong convexity and self-concordance,\nshowing that our private estimators converge with high probability to a nearly\noptimal neighborhood of the non-private M-estimators. Secondly, we tackle the\nproblem of parametric inference by constructing differentially private\nestimators of the asymptotic variance of our private M-estimators. This\nnaturally leads to approximate pivotal statistics for constructing confidence\nregions and conducting hypothesis testing. We demonstrate the effectiveness of\na bias correction that leads to enhanced small-sample empirical performance in\nsimulations. We illustrate the benefits of our methods in several numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:55:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Avella-Medina", "Marco", ""], ["Bradshaw", "Casey", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2103.11021", "submitter": "Antonio Di Crescenzo", "authors": "Chanchal Kundu, Antonio Di Crescenzo, Maria Longobardi", "title": "On cumulative residual (past) inaccuracy for truncated random variables", "comments": "19 pages, 3 figures", "journal-ref": "Metrika, 79 (2016), pp. 335-356", "doi": "10.1007/s00184-015-0557-5", "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the drawbacks of Shannon's entropy, the concept of cumulative\nresidual and past entropy has been proposed in the information theoretic\nliterature. Furthermore, the Shannon entropy has been generalized in a number\nof different ways by many researchers. One important extension is Kerridge\ninaccuracy measure. In the present communication we study the cumulative\nresidual and past inaccuracy measures, which are extensions of the\ncorresponding cumulative entropies. Several properties, including monotonicity\nand bounds, are obtained for left, right and doubly truncated random variables.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:54:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kundu", "Chanchal", ""], ["Di Crescenzo", "Antonio", ""], ["Longobardi", "Maria", ""]]}, {"id": "2103.11038", "submitter": "Antonio Di Crescenzo", "authors": "Antonio Di Crescenzo, Luca Paolillo, Alfonso Suarez-Llorens", "title": "Stochastic comparisons, differential entropy and varentropy for\n  distributions induced by probability density functions", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stimulated by the need of describing useful notions related to information\nmeasures, we introduce the `pdf-related distributions'. These are defined in\nterms of transformation of absolutely continuous random variables through their\nown probability density functions. We investigate their main characteristics,\nwith reference to the general form of the distribution, the quantiles, and some\nrelated notions of reliability theory. This allows us to obtain a\ncharacterization of the uniform distribution based on pdf-related distributions\nof exponential and Laplace type as well. We also face the problem of stochastic\ncomparing the pdf-related distributions by resorting to suitable stochastic\norders. Finally, the given results are used to analyse properties and to\ncompare some useful information measures, such as the differential entropy and\nthe varentropy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 22:01:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Di Crescenzo", "Antonio", ""], ["Paolillo", "Luca", ""], ["Suarez-Llorens", "Alfonso", ""]]}, {"id": "2103.11147", "submitter": "Anis Mohamed Haddouche", "authors": "Anis M. Haddouche and Wei Lu", "title": "A unified approach for covariance matrix estimation under Stein loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating a covariance matrix of a\nmultivariate Gaussian distribution, relative to a Stein loss function, from a\ndecision theoretic point of view. We investigate the case where the covariance\nmatrix is invertible and the case when it is non--invertible in a unified\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 10:01:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Haddouche", "Anis M.", ""], ["Lu", "Wei", ""]]}, {"id": "2103.11201", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and David Preinerstorfer", "title": "Consistency of $p$-norm based tests in high dimensions:\n  characterization, monotonicity, domination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many commonly used test statistics are based on a norm measuring the evidence\nagainst the null hypothesis. To understand how the choice of a norm affects\npower properties of tests in high dimensions, we study the consistency sets of\n$p$-norm based tests in the prototypical framework of sequence models with\nunrestricted parameter spaces, the null hypothesis being that all observations\nhave zero mean. The consistency set of a test is here defined as the set of all\narrays of alternatives the test is consistent against as the dimension of the\nparameter space diverges. We characterize the consistency sets of $p$-norm\nbased tests and find, in particular, that the consistency against an array of\nalternatives cannot be determined solely in terms of the $p$-norm of the\nalternative. Our characterization also reveals an unexpected monotonicity\nresult: namely that the consistency set is strictly increasing in $p \\in (0,\n\\infty)$, such that tests based on higher $p$ strictly dominate those based on\nlower $p$ in terms of consistency. This monotonicity allows us to construct\nnovel tests that dominate, with respect to their consistency behavior, all\n$p$-norm based tests without sacrificing size.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 16:04:19 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 15:49:11 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 18:30:43 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2103.11205", "submitter": "Royi Jacobovic", "authors": "Royi Jacobovic", "title": "Simple sufficient condition for inadmissibility of Moran's single-split\n  test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that a statistician observes two independent variates $X_1$ and $X_2$\nhaving densities $f_i(\\cdot;\\theta)\\equiv f_i(\\cdot-\\theta)\\ ,\\ i=1,2$ ,\n$\\theta\\in\\mathbb{R}$. His purpose is to conduct a test for\n  \\begin{equation*}\n  H:\\theta=0 \\ \\ \\text{vs.}\\ \\ K:\\theta\\in\\mathbb{R}\\setminus\\{0\\}\n  \\end{equation*}\n  with a pre-defined significance level $\\alpha\\in(0,1)$.\n  Moran (1973) suggested a test which is based on a single split of the data,\n\\textit{i.e.,} to use $X_2$ in order to conduct a one-sided test in the\ndirection of $X_1$. Specifically, if $b_1$ and $b_2$ are the $(1-\\alpha)$'th\nand $\\alpha$'th quantiles associated with the distribution of $X_2$ under $H$,\nthen Moran's test has a rejection zone\n  \\begin{equation*}\n  (a,\\infty)\\times(b_1,\\infty)\\cup(-\\infty,a)\\times(-\\infty,b_2)\n  \\end{equation*}\n  where $a\\in\\mathbb{R}$ is a design parameter.\n  Motivated by this issue, the current work includes an analysis of a new\nnotion, \\textit{regular admissibility} of tests. It turns out that the theory\nregarding this kind of admissibility leads to a simple sufficient condition on\n$f_1(\\cdot)$ and $f_2(\\cdot)$ under which Moran's test is inadmissible.\nFurthermore, the same approach leads to a formal proof for the conjecture of\nDiCiccio (2018) addressing that the multi-dimensional version of Moran's test\nis inadmissible when the observations are $d$-dimensional Gaussians.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 16:35:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jacobovic", "Royi", ""]]}, {"id": "2103.11280", "submitter": "Myung Geun Kim", "authors": "Myung Geun Kim", "title": "Asymptotic distribution for the proportional covariance model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Asymptotic distribution for the proportional covariance model under\nmultivariate normal distributions is derived. To this end, the parametrization\nof the common covariance matrix by its Cholesky root is adopted. The\nderivations are made in three steps. First, the asymptotic distribution of the\nmaximum likelihood estimators of the proportionality coefficients and the\nCholesky inverse root of the common covariance matrix is derived by finding the\ninformation matrix and its inverse. Next, the asymptotic distributions for the\ncase of the Cholesky root of the common covariance matrix and finally for the\ncase of the common covariance matrix itself are derived using the multivariate\n$\\delta$-method. As an application of the asymptotic distribution derived here,\na hypothesis for homogeneity of covariance matrices is considered.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 01:37:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kim", "Myung Geun", ""]]}, {"id": "2103.11407", "submitter": "Lancelot F. James", "authors": "Lancelot F. James, Juho Lee and Abhinav Pandey", "title": "Posterior distributions for Hierarchical Spike and Slab Indian Buffet\n  processes", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian nonparametric hierarchical priors are highly effective in providing\nflexible models for latent data structures exhibiting sharing of information\nbetween and across groups. Most prominent is the Hierarchical Dirichlet Process\n(HDP), and its subsequent variants, which model latent clustering between and\nacross groups. The HDP, may be viewed as a more flexible extension of Latent\nDirichlet Allocation models (LDA), and has been applied to, for example, topic\nmodelling, natural language processing, and datasets arising in health-care. We\nfocus on analogous latent feature allocation models, where the data structures\ncorrespond to multisets or unbounded sparse matrices. The fundamental\ndevelopment in this regard is the Hierarchical Indian Buffet process (HIBP),\nwhich utilizes a hierarchy of Beta processes over J groups, where each group\ngenerates binary random matrices, reflecting within group sharing of features,\naccording to beta-Bernoulli IBP priors. To encompass HIBP versions of\nnon-Bernoulli extensions of the IBP, we introduce hierarchical versions of\ngeneral spike and slab IBP. We provide explicit novel descriptions of the\nmarginal, posterior and predictive distributions of the HIBP and its\ngeneralizations which allow for exact sampling and simpler practical\nimplementation. We highlight common structural properties of these processes\nand establish relationships to existing IBP type and related models arising in\nthe literature. Examples of potential applications may involve topic models,\nPoisson factorization models, random count matrix priors and neural network\nmodels\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:16:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["James", "Lancelot F.", ""], ["Lee", "Juho", ""], ["Pandey", "Abhinav", ""]]}, {"id": "2103.11493", "submitter": "Yiou Li", "authors": "Yiou Li, Xinwei Deng", "title": "On Efficient Design of Pilot Experiment for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental design for a generalized linear model (GLM) is important but\nchallenging since the design criterion often depends on model specification\nincluding the link function, the linear predictor, and the unknown regression\ncoefficients. Prior to constructing locally or globally optimal designs, a\npilot experiment is usually conducted to provide some insights on the model\nspecifications. In pilot experiments, little information on the model\nspecification of GLM is available. Surprisingly, there is very limited research\non the design of pilot experiments for GLMs. In this work, we obtain some\ntheoretical understanding of the design efficiency in pilot experiments for\nGLMs. Guided by the theory, we propose to adopt a low-discrepancy design with\nrespect to some target distribution for pilot experiments. The performance of\nthe proposed design is assessed through several numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 21:42:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Yiou", ""], ["Deng", "Xinwei", ""]]}, {"id": "2103.11586", "submitter": "Santhosh Karnik", "authors": "Santhosh Karnik, Justin Romberg, Mark A. Davenport", "title": "Thomson's Multitaper Method Revisited", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thomson's multitaper method estimates the power spectrum of a signal from $N$\nequally spaced samples by averaging $K$ tapered periodograms. Discrete prolate\nspheroidal sequences (DPSS) are used as tapers since they provide excellent\nprotection against spectral leakage. Thomson's multitaper method is widely used\nin applications, but most of the existing theory is qualitative or asymptotic.\nFurthermore, many practitioners use a DPSS bandwidth $W$ and number of tapers\nthat are smaller than what the theory suggests is optimal because the\ncomputational requirements increase with the number of tapers. We revisit\nThomson's multitaper method from a linear algebra perspective involving\nsubspace projections. This provides additional insight and helps us establish\nnonasymptotic bounds on some statistical properties of the multitaper spectral\nestimate, which are similar to existing asymptotic results. We show using\n$K=2NW-O(\\log(NW))$ tapers instead of the traditional $2NW-O(1)$ tapers better\nprotects against spectral leakage, especially when the power spectrum has a\nhigh dynamic range. Our perspective also allows us to derive an\n$\\epsilon$-approximation to the multitaper spectral estimate which can be\nevaluated on a grid of frequencies using $O(\\log(NW)\\log\\tfrac{1}{\\epsilon})$\nFFTs instead of $K=O(NW)$ FFTs. This is useful in problems where many samples\nare taken, and thus, using many tapers is desirable.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 05:17:50 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Karnik", "Santhosh", ""], ["Romberg", "Justin", ""], ["Davenport", "Mark A.", ""]]}, {"id": "2103.11712", "submitter": "Shigekazu Nakagawa", "authors": "Shigekazu Nakagawa, Hiroki Hashiguchi and Yoko Ono", "title": "Approximation to probability density functions in sampling distributions\n  based on Fourier cosine series", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a simple and precise approximation to probability density functions\nin sampling distributions based on the Fourier cosine series. After clarifying\nthe required conditions, we illustrate the approximation on two examples: the\ndistribution of the sum of uniformly distributed random variables, and the\ndistribution of sample skewness drawn from a normal population. The probability\ndensity function of the first example can be explicitly expressed, but that of\nthe second example has no explicit expression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:32:31 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 05:00:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nakagawa", "Shigekazu", ""], ["Hashiguchi", "Hiroki", ""], ["Ono", "Yoko", ""]]}, {"id": "2103.11821", "submitter": "Joonas Sova", "authors": "J\\\"uri Lember, Joonas Sova", "title": "Regenerativity of Viterbi process for pairwise Markov models", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.03799", "journal-ref": "Journal of Theoretical Probability volume 34 (2021)", "doi": "10.1007/s10959-020-01022-z", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hidden Markov models one of the most popular estimates of the hidden\nchain is the Viterbi path -- the path maximising the posterior probability. We\nconsider a more general setting, called the pairwise Markov model (PMM), where\nthe joint process consisting of finite-state hidden process and observation\nprocess is assumed to be a Markov chain. It has been recently proven that under\nsome conditions the Viterbi path of the PMM can almost surely be extended to\ninfinity, thereby defining the infinite Viterbi decoding of the observation\nsequence, called the Viterbi process. This was done by constructing a block of\nobservations, called a barrier, which ensures that the Viterbi path goes trough\na given state whenever this block occurs in the observation sequence. In this\npaper we prove that the joint process consisting of Viterbi process and PMM is\nregenerative. The proof involves a delicate construction of regeneration times\nwhich coincide with the occurrences of barriers. As one possible application of\nour theory, some results on the asymptotics of the Viterbi training algorithm\nare derived.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:01:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Sova", "Joonas", ""]]}, {"id": "2103.11830", "submitter": "Benjamin Robinson", "authors": "Benjamin Robinson, Robert Malinas", "title": "Optimal Linear Classification via Eigenvalue Shrinkage: The Case of\n  Additive Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the general problem of testing the mean of two\nhigh-dimensional distributions with a common, unknown covariance using a linear\nclassifier. Traditionally such a classifier is formed from the sample\ncovariance matrix of some given training data, but, as is well-known, the\nperformance of this classifier is poor when the number of training data $n$ is\nnot much larger than the data dimension $p$. We thus seek a covariance\nestimator to replace sample covariance. To account for the fact that $n$ and\n$p$ may be of comparable size, we adopt the \"large-dimensional asymptotic\nmodel\" in which $n$ and $p$ go to infinity in a fixed ratio. Under this\nassumption, we identify a covariance estimator that is detection-theoretic\noptimal within the general shrinkage class of C. Stein, and we give consistent\nestimates for the corresponding classifier's type-I and type-II errors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:26:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Robinson", "Benjamin", ""], ["Malinas", "Robert", ""]]}, {"id": "2103.11884", "submitter": "Jonas Brehmer", "authors": "Jonas Brehmer, Tilmann Gneiting, Martin Schlather, Kirstin Strokorb", "title": "Using scoring functions to evaluate point process forecasts", "comments": "49 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point process models are widely used tools to issue forecasts or assess\nrisks. In order to check which models are useful in practice, they are examined\nby a variety of statistical methods. We transfer the concept of consistent\nscoring functions, which are principled statistical tools to compare forecasts,\nto the point process setting. The results provide a novel approach for the\ncomparative assessment of forecasts and models and encompass some existing\ntesting procedures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:24:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Brehmer", "Jonas", ""], ["Gneiting", "Tilmann", ""], ["Schlather", "Martin", ""], ["Strokorb", "Kirstin", ""]]}, {"id": "2103.12021", "submitter": "Paria Rashidinejad", "authors": "Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell", "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale\n  of Pessimism", "comments": "84 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline (or batch) reinforcement learning (RL) algorithms seek to learn an\noptimal policy from a fixed dataset without active data collection. Based on\nthe composition of the offline dataset, two main categories of methods are\nused: imitation learning which is suitable for expert datasets and vanilla\noffline RL which often requires uniform coverage datasets. From a practical\nstandpoint, datasets often deviate from these two extremes and the exact data\ncomposition is usually unknown a priori. To bridge this gap, we present a new\noffline RL framework that smoothly interpolates between the two extremes of\ndata composition, hence unifying imitation learning and vanilla offline RL. The\nnew framework is centered around a weak version of the concentrability\ncoefficient that measures the deviation from the behavior policy to the expert\npolicy alone.\n  Under this new framework, we further investigate the question on algorithm\ndesign: can one develop an algorithm that achieves a minimax optimal rate and\nalso adapts to unknown data composition? To address this question, we consider\na lower confidence bound (LCB) algorithm developed based on pessimism in the\nface of uncertainty in offline RL. We study finite-sample properties of LCB as\nwell as information-theoretic limits in multi-armed bandits, contextual\nbandits, and Markov decision processes (MDPs). Our analysis reveals surprising\nfacts about optimality rates. In particular, in all three settings, LCB\nachieves a faster rate of $1/N$ for nearly-expert datasets compared to the\nusual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the number of samples in\nthe batch dataset. In the case of contextual bandits with at least two\ncontexts, we prove that LCB is adaptively optimal for the entire data\ncomposition range, achieving a smooth transition from imitation learning to\noffline RL. We further show that LCB is almost adaptively optimal in MDPs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:27:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rashidinejad", "Paria", ""], ["Zhu", "Banghua", ""], ["Ma", "Cong", ""], ["Jiao", "Jiantao", ""], ["Russell", "Stuart", ""]]}, {"id": "2103.12597", "submitter": "T\\^am Le Minh", "authors": "T\\^am Le Minh", "title": "Weak convergence of $U$-statistics on a row-column exchangeable matrix", "comments": "20 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $U$-statistics are used to estimate a population parameter by averaging a\nfunction on a subsample over all the subsamples of the population. In this\npaper, the population we are interested in is formed by the entries of a\nrow-column exchangeable matrix. We consider $U$-statistics derived from\nfunctions of quadruplets, i.e. submatrices of size $2 \\times 2$. We prove a\nweak convergence result for these $U$-statistics in the general case and we\nestablish a Central Limit Theorem when the matrix is also dissociated. We shed\nfurther light on these results using the Aldous-Hoover representation theorem\nfor row-column exchangeable random variables. Finally, to illustrate these\nresults, we give examples of hypothesis testing for bipartite networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:51:44 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 15:02:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Minh", "T\u00e2m Le", ""]]}, {"id": "2103.12725", "submitter": "Steve Yadlowsky", "authors": "Steve Yadlowsky, Taedong Yun, Cory McLean, Alexander D'Amour", "title": "SLOE: A Faster Method for Statistical Inference in High-Dimensional\n  Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression remains one of the most widely used tools in applied\nstatistics, machine learning and data science. However, in moderately\nhigh-dimensional problems, where the number of features $d$ is a non-negligible\nfraction of the sample size $n$, the logistic regression maximum likelihood\nestimator (MLE), and statistical procedures based the large-sample\napproximation of its distribution, behave poorly. Recently, Sur and Cand\\`es\n(2019) showed that these issues can be corrected by applying a new\napproximation of the MLE's sampling distribution in this high-dimensional\nregime. Unfortunately, these corrections are difficult to implement in\npractice, because they require an estimate of the \\emph{signal strength}, which\nis a function of the underlying parameters $\\beta$ of the logistic regression.\nTo address this issue, we propose SLOE, a fast and straightforward approach to\nestimate the signal strength in logistic regression. The key insight of SLOE is\nthat the Sur and Cand\\`es (2019) correction can be reparameterized in terms of\nthe \\emph{corrupted signal strength}, which is only a function of the estimated\nparameters $\\widehat \\beta$. We propose an estimator for this quantity, prove\nthat it is consistent in the relevant high-dimensional regime, and show that\ndimensionality correction using SLOE is accurate in finite samples. Compared to\nthe existing ProbeFrontier heuristic, SLOE is conceptually simpler and orders\nof magnitude faster, making it suitable for routine use. We demonstrate the\nimportance of routine dimensionality correction in the Heart Disease dataset\nfrom the UCI repository, and a genomics application using data from the UK\nBiobank. We provide an open source package for this method, available at\n\\url{https://github.com/google-research/sloe-logistic}.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:48:56 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:50:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yadlowsky", "Steve", ""], ["Yun", "Taedong", ""], ["McLean", "Cory", ""], ["D'Amour", "Alexander", ""]]}, {"id": "2103.12846", "submitter": "Rui Duan", "authors": "Rui Duan, Yang Ning, Jiasheng Shi, Raymond J Carroll, Tianxi Cai and\n  Yong Chen", "title": "On the global identifiability of logistic regression models with\n  misclassified outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the secondary use of large data from health systems, such\nas electronic health records, has demonstrated great promise in advancing\nbiomedical discoveries and improving clinical decision making. However, there\nis an increasing concern about biases in association studies caused by\nmisclassification in the binary outcomes derived from electronic health\nrecords. We revisit the classical logistic regression model with misclassified\noutcomes. Despite that local identification conditions in some related settings\nhave been previously established, the global identification of such models\nremains largely unknown and is an important question yet to be answered. We\nderive necessary and sufficient conditions for global identifiability of\nlogistic regression models with misclassified outcomes, using a novel approach\ntermed as the submodel analysis, and a technique adapted from the\nPicard-Lindel\\\"{o}f existence theorem in ordinary differential equations. In\nparticular, our results are applicable to logistic models with discrete\ncovariates, which is a common situation in biomedical studies, The conditions\nare easy to verify in practice. In addition to model identifiability, we\npropose a hypothesis testing procedure for regression coefficients in the\nmisclassified logistic regression model when the model is not identifiable\nunder the null.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:12:52 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duan", "Rui", ""], ["Ning", "Yang", ""], ["Shi", "Jiasheng", ""], ["Carroll", "Raymond J", ""], ["Cai", "Tianxi", ""], ["Chen", "Yong", ""]]}, {"id": "2103.12946", "submitter": "Linquan Ma", "authors": "Linquan Ma, Lan Liu, Wei Yang", "title": "Envelope Methods with Ignorable Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Envelope method was recently proposed as a method to reduce the dimension of\nresponses in multivariate regressions. However, when there exists missing data,\nthe envelope method using the complete case observations may lead to biased and\ninefficient results. In this paper, we generalize the envelope estimation when\nthe predictors and/or the responses are missing at random. Specifically, we\nincorporate the envelope structure in the expectation-maximization (EM)\nalgorithm. As the parameters under the envelope method are not pointwise\nidentifiable, the EM algorithm for the envelope method was not straightforward\nand requires a special decomposition. Our method is guaranteed to be more\nefficient, or at least as efficient as, the standard EM algorithm. Moreover,\nour method has the potential to outperform the full data MLE. We give\nasymptotic properties of our method under both normal and non-normal cases. The\nefficiency gain over the standard EM is confirmed in simulation studies and in\nan application to the Chronic Renal Insufficiency Cohort (CRIC) study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:48:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ma", "Linquan", ""], ["Liu", "Lan", ""], ["Yang", "Wei", ""]]}, {"id": "2103.13142", "submitter": "Jorge Yslas Altamirano", "authors": "Jorge Yslas", "title": "Fitting phase-type frailty models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frailty models are survival analysis models which account for heterogeneity\nand random effects in the data. In these models, the random effect (the\nfrailty) is assumed to have a multiplicative effect on the hazard. In this\npaper, we present frailty models using phase-type distributions as the\nfrailties. We explore the properties of the proposed frailty models and derive\nexpectation-maximization algorithms for maximum-likelihood estimation. The\nalgorithms' performance is illustrated in several numerical examples of\npractical significance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:38:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yslas", "Jorge", ""]]}, {"id": "2103.13202", "submitter": "Martin Bilodeau", "authors": "Martin Bilodeau", "title": "Anova of Balanced Variance Component Models", "comments": "8 pages, education", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Balanced linear models with fixed effects are taught in undergraduate\nprograms of all universities. These occur in experimental designs such as\none-way and two-way Anova, randomized complete block designs (RCBD) and split\nplot designs. The distributional theory for sums of squares in fixed effects\nmodels can be taught using the simplest form of Cochran's Theorem. The\ncontribution provided here allows for an easy extension of the distributional\ntheory to corresponding models with random effects. The main tool used is a\nsimple result on noncentral chi-square distribution overlooked in textbooks at\nundergraduate and graduate levels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:00:32 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:35:35 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bilodeau", "Martin", ""]]}, {"id": "2103.13213", "submitter": "Hanne Kekkonen", "authors": "Hanne Kekkonen", "title": "Consistency of Bayesian inference with Gaussian process priors for a\n  parabolic inverse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the statistical nonlinear inverse problem of recovering the\nabsorption term $f>0$ in the heat equation $$ \\partial_tu-\\frac{1}{2}\\Delta\nu+fu=0 \\quad \\text{on $\\mathcal{O}\\times(0,\\textbf{T})$}\\quad u = g \\quad\n\\text{on $\\partial\\mathcal{O}\\times(0,\\textbf{T})$}\\quad u(\\cdot,0)=u_0 \\quad\n\\text{on $\\mathcal{O}$}, $$ where $\\mathcal{O}\\in\\mathbb{R}^d$ is a bounded\ndomain, $\\textbf{T}<\\infty$ is a fixed time, and $g,u_0$ are given sufficiently\nsmooth functions describing boundary and initial values respectively. The data\nconsists of $N$ discrete noisy point evaluations of the solution $u_f$ on\n$\\mathcal{O}\\times(0,\\textbf{T})$. We study the statistical performance of\nBayesian nonparametric procedures based on a large class of Gaussian process\npriors. We show that, as the number of measurements increases, the resulting\nposterior distributions concentrate around the true parameter generating the\ndata, and derive a convergence rate for the reconstruction error of the\nassociated posterior means. We also consider the optimality of the contraction\nrates and prove a lower bound for the minimax convergence rate for inferring\n$f$ from the data, and show that optimal rates can be achieved with truncated\nGaussian priors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:18:35 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kekkonen", "Hanne", ""]]}, {"id": "2103.13248", "submitter": "Stavros Nikolakopoulos", "authors": "Stavros Nikolakopoulos, Eric Cator and Mart P. Janssen", "title": "Extending the Mann-Kendall test to allow for measurement uncertainty", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Mann-Kendall test for trend has gained a lot of attention in a range of\ndisciplines, especially in the environmental sciences. One of the drawbacks of\nthe Mann-Kendall test when applied to real data is that no distinction can be\nmade between meaningful and non-meaningful differences in subsequent\nobservations. We introduce the concept of partial ties, which allows inferences\nwhile accounting for (non)meaningful difference. We introduce the modified\nstatistic that accounts for such a concept and derive its variance estimator.\nWe also present analytical results for the behavior of the test in a class of\ncontiguous alternatives. Simulation results which illustrate the added value of\nthe test are presented. We apply our extended version of the test to some real\ndata concerning blood donation in Europe.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:04:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nikolakopoulos", "Stavros", ""], ["Cator", "Eric", ""], ["Janssen", "Mart P.", ""]]}, {"id": "2103.13249", "submitter": "Uwe Hassler", "authors": "Uwe Hassler and Mehdi Hosseinkouchack", "title": "New Proofs of the Basel Problem using Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number $\\frac{\\pi ^{2}}{6}$ is involved in the variance of several\ndistributions in statistics. At the same time it holds\n$\\sum\\nolimits_{k=1}^{\\infty }k^{-2}= \\frac{\\pi ^{2}}{6}$, which solves the\nfamous Basel problem. We first provide a historical perspective on the Basel\nproblem, and second show how to generate further proofs building on stochastic\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:08:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hassler", "Uwe", ""], ["Hosseinkouchack", "Mehdi", ""]]}, {"id": "2103.13336", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop and William Kengne", "title": "Epidemic change-point detection in general integer-valued time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the structural change in a class of discrete\nvalued time series, which the true conditional distribution of the observations\nis assumed to be unknown.\n  The conditional mean of the process depends on a parameter $\\theta^*$ which\nmay change over time.\n  We provide sufficient conditions for the consistency and the asymptotic\nnormality of the Poisson quasi-maximum likelihood estimator (QMLE) of the\nmodel.\n  We consider an epidemic change-point detection and propose a test statistic\nbased on the QMLE of the parameter. Under the null hypothesis of a constant\nparameter (no change), the test statistic converges to a distribution obtained\nfrom a difference of two Brownian bridge. The test statistic diverges to\ninfinity under the epidemic alternative, which establishes that the proposed\nprocedure is consistent in power. The effectiveness of the proposed procedure\nis illustrated by simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:46:19 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 09:25:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2103.13369", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu", "title": "Phase transition of the monotonicity assumption in learning local\n  average treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting in which a strong binary instrument is available for\na binary treatment. The traditional LATE approach assumes the monotonicity\ncondition stating that there are no defiers (or compliers). Since this\ncondition is not always obvious, we investigate the sensitivity and testability\nof this condition. In particular, we focus on the question: does a slight\nviolation of monotonicity lead to a small problem or a big problem? We find a\nphase transition for the monotonicity condition. On one of the boundary of the\nphase transition, it is easy to learn the sign of LATE and on the other side of\nthe boundary, it is impossible to learn the sign of LATE. Unfortunately, the\nimpossible side of the phase transition includes data-generating processes\nunder which the proportion of defiers tends to zero. This boundary of phase\ntransition is explicitly characterized in the case of binary outcomes. Outside\na special case, it is impossible to test whether the data-generating process is\non the nice side of the boundary. However, in the special case that the\nnon-compliance is almost one-sided, such a test is possible. We also provide\nsimple alternatives to monotonicity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:45:27 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhu", "Yinchu", ""]]}, {"id": "2103.13435", "submitter": "Pengfei Li", "authors": "Tao Yu, Pengfei Li, Baojiang Chen, Ao Yuan, Jing Qin", "title": "Maximum pairwise-rank-likelihood-based inference for the semiparametric\n  transformation model", "comments": "6 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the linear transformation model in the most general\nsetup. This model includes many important and popular models in statistics and\neconometrics as special cases. Although it has been studied for many years, the\nmethods in the literature are based on kernel-smoothing techniques or make use\nof only the ranks of the responses in the estimation of the parametric\ncomponents. The former approach needs a tuning parameter, which is not easily\noptimally specified in practice; and the latter is computationally expensive\nand may not make full use of the information in the data. In this paper, we\npropose two methods: a pairwise rank likelihood method and a\nscore-function-based method based on this pairwise rank likelihood. We also\nexplore the theoretical properties of the proposed estimators. Via extensive\nnumerical studies, we demonstrate that our methods are appealing in that the\nestimators are not only robust to the distribution of the random errors but\nalso lead to mean square errors that are in many cases comparable to or smaller\nthan those of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:27:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yu", "Tao", ""], ["Li", "Pengfei", ""], ["Chen", "Baojiang", ""], ["Yuan", "Ao", ""], ["Qin", "Jing", ""]]}, {"id": "2103.13521", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi and Terry Soo", "title": "Conditions and Assumptions for Constraint-based Causal Structure\n  Learning", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper formalizes constraint-based structure learning of the \"true\" causal\ngraph from observed data when unobserved variables are also existent. We define\na \"generic\" structure learning algorithm, which provides conditions that, under\nthe faithfulness assumption, the output of all known exact algorithms in the\nliterature must satisfy, and which outputs graphs that are Markov equivalent to\nthe causal graph. More importantly, we provide clear assumptions, weaker than\nfaithfulness, under which the same generic algorithm outputs Markov equivalent\ngraphs to the causal graph. We provide the theory for the general class of\nmodels under the assumption that the distribution is Markovian to the true\ncausal graph, and we specialize the definitions and results for structural\ncausal models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:08:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sadeghi", "Kayvan", ""], ["Soo", "Terry", ""]]}, {"id": "2103.13840", "submitter": "Boris Landa", "authors": "Boris Landa, Thomas T.C.K. Zhang, Yuval Kluger", "title": "Biwhitening Reveals the Rank of a Count Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the rank of a corrupted data matrix is an important task in data\nscience, most notably for choosing the number of components in principal\ncomponent analysis. Significant progress on this task has been made using\nrandom matrix theory by characterizing the spectral properties of large noise\nmatrices. However, utilizing such tools is not straightforward when the data\nmatrix consists of count random variables, such as Poisson or binomial, in\nwhich case the noise can be heteroskedastic with an unknown variance in each\nentry. In this work, focusing on a Poisson random matrix with independent\nentries, we propose a simple procedure termed \\textit{biwhitening} that makes\nit possible to estimate the rank of the underlying data matrix (i.e., the\nPoisson parameter matrix) without any prior knowledge on its structure. Our\napproach is based on the key observation that one can scale the rows and\ncolumns of the data matrix simultaneously so that the spectrum of the\ncorresponding noise agrees with the standard Marchenko-Pastur (MP) law,\njustifying the use of the MP upper edge as a threshold for rank selection.\nImportantly, the required scaling factors can be estimated directly from the\nobservations by solving a matrix scaling problem via the Sinkhorn-Knopp\nalgorithm. Aside from the Poisson distribution, we extend our biwhitening\napproach to other discrete distributions, such as the generalized Poisson,\nbinomial, multinomial, and negative binomial. We conduct numerical experiments\nthat corroborate our theoretical findings, and demonstrate our approach on real\nsingle-cell RNA sequencing (scRNA-seq) data, where we show that our results\nagree with a slightly overdispersed generalized Poisson model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:48:42 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Landa", "Boris", ""], ["Zhang", "Thomas T. C. K.", ""], ["Kluger", "Yuval", ""]]}, {"id": "2103.13900", "submitter": "Nestor Parolya Dr.", "authors": "Nestor Parolya, Johannes Heiny, Dorota Kurowicka", "title": "Logarithmic law of large random correlation matrix", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector $\\mathbf{y}=\\mathbf{\\Sigma}^{1/2}\\mathbf{x}$, where\nthe $p$ elements of the vector $\\mathbf{x}$ are i.i.d. real-valued random\nvariables with zero mean and finite fourth moment, and $\\mathbf{\\Sigma}^{1/2}$\nis a deterministic $p\\times p$ matrix such that the spectral norm of the\npopulation correlation matrix $\\mathbf{R}$ of $\\mathbf{y}$ is uniformly\nbounded. In this paper, we find that the log determinant of the sample\ncorrelation matrix $\\hat{\\mathbf{R}}$ based on a sample of size $n$ from the\ndistribution of $\\mathbf{y}$ satisfies a CLT (central limit theorem) for\n$p/n\\to \\gamma\\in (0, 1]$ and $p\\leq n$. Explicit formulas for the asymptotic\nmean and variance are provided. In case the mean of $\\mathbf{y}$ is unknown, we\nshow that after recentering by the empirical mean the obtained CLT holds with a\nshift in the asymptotic mean. This result is of independent interest in both\nlarge dimensional random matrix theory and high-dimensional statistical\nliterature of large sample correlation matrices for non-normal data. At last,\nthe obtained findings are applied for testing of uncorrelatedness of $p$ random\nvariables. Surprisingly, in the null case $\\mathbf{R}=\\mathbf{I}$, the test\nstatistic becomes completely pivotal and the extensive simulations show that\nthe obtained CLT also holds if the moments of order four do not exist at all,\nwhich conjectures a promising and robust test statistic for heavy-tailed\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:09:19 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 19:36:55 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Parolya", "Nestor", ""], ["Heiny", "Johannes", ""], ["Kurowicka", "Dorota", ""]]}, {"id": "2103.13977", "submitter": "Simone Giannerini", "authors": "Greta Goracci, Simone Giannerini, Kung-Sik Chan, Howell Tong", "title": "Testing for threshold effects in the TARMA framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present supremum Lagrange Multiplier tests to compare a linear ARMA\nspecification against its threshold ARMA extension. We derive the asymptotic\ndistribution of the test statistics both under the null hypothesis and\ncontiguous local alternatives. Moreover, we prove the consistency of the tests.\nThe Monte Carlo study shows that the tests enjoy good finite-sample properties,\nare robust against model mis-specification and their performance is not\naffected if the order of the model is unknown. The tests present a low\ncomputational burden and do not suffer from some of the drawbacks that affect\nthe quasi-likelihood ratio setting. Lastly, we apply our tests to a time series\nof standardized tree-ring growth indexes and this can lead to new research in\nclimate studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:11:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Goracci", "Greta", ""], ["Giannerini", "Simone", ""], ["Chan", "Kung-Sik", ""], ["Tong", "Howell", ""]]}, {"id": "2103.14011", "submitter": "Brice Huang", "authors": "Matthew Brennan, Guy Bresler, Brice Huang", "title": "De Finetti-Style Results for Wishart Matrices: Combinatorial Structure\n  and Phase Transitions", "comments": "115 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work has studied the relationship between the Wishart matrix\n$X^\\top X$, where $X\\in \\mathbb{R}^{d\\times n}$ has i.i.d. standard Gaussian\nentries, and the corresponding Gaussian matrix with independent entries above\nthe diagonal. Jiang and Li (2015) and Bubeck et al. (2016) showed that these\ntwo matrix ensembles converge in total variation whenever $d/n^3\\to \\infty$,\nand Bubeck et al. (2016) showed this to be sharp. In this paper we aim to\nidentify the precise threshold for $d$ in terms of $n$ for subsets of Wishart\nmatrices to converge in total variation to independent Gaussians. It turns out\nthat the combinatorial structure of the revealed entries, viewed as the\nadjacency matrix of a graph $G$, characterizes the distance from fully\nindependent. Specifically, we show that the threshold for $d$ depends on the\nnumber of various small subgraphs in $G$. So, even when the number of revealed\nentries is fixed, the threshold can vary wildly depending on their\nconfiguration. Convergence of masked Wishart to independent Gaussians thus\ninherently involves an interplay between both probabilistic and combinatorial\nphenomena. Our results determine the sharp threshold for a large family of $G$,\nincluding Erd\\H{o}s-R\\'enyi $G\\sim \\mathcal{G}(n,p)$ at all values $p\\gtrsim\nn^{-2}\\mathrm{polylog}(n)$. Our proof techniques are both combinatorial and\ninformation theoretic, which together allow us to carefully unravel the\ndependencies in the masked Wishart ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:45:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""], ["Huang", "Brice", ""]]}, {"id": "2103.14389", "submitter": "Quentin Paris", "authors": "Quentin Paris", "title": "Online learning with exponential weights in metric spaces", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.MG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of online learning in metric spaces using\nexponential weights. We extend the analysis of the exponentially weighted\naverage forecaster, traditionally studied in a Euclidean settings, to a more\nabstract framework. Our results rely on the notion of barycenters, a suitable\nversion of Jensen's inequality and a synthetic notion of lower curvature bound\nin metric spaces known as the measure contraction property. We also adapt the\nonline-to-batch conversion principle to apply our results to a statistical\nlearning framework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:46:10 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Paris", "Quentin", ""]]}, {"id": "2103.14390", "submitter": "Uwe Saint-Mont", "authors": "Uwe Saint-Mont", "title": "Toward a deeper understanding of a basic cascade", "comments": "arXiv admin note: substantial text overlap with arXiv:1709.10281", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards the end of the last century, B. Mandelbrot saw the importance,\nrevealed the beauty, and robustly promoted (multi-)fractals. Multiplicative\ncascades are closely related and provide simple models for the study of\nturbulence and chaos.\n  For pedagogical reasons, but also due to technical difficulties, continuous\nstochastic models have been favoured over discrete cascades. Particularly\nimportant are the $\\alpha$ and the $p$ model. It is the aim of this\ncontribution to introduce original concepts that shed new light on a variant of\nthe latter paradigmatic cascade and allow key features to be derived in a\nrather elementary fashion.\n  To this end, we introduce and study a discrete version of the $p$ model which\nis based on a new kind of sampling. Technical machinery can be kept simple,\ntherefore proofs are straightforward and formulas are explicit. It is hoped\nthat the proposed line of investigation may enhance understanding and simplify\nreceived multifractal analyses.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:46:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Saint-Mont", "Uwe", ""]]}, {"id": "2103.14445", "submitter": "Emilia Pompe", "authors": "Emilia Pompe", "title": "Introducing prior information in Weighted Likelihood Bootstrap with\n  applications to model misspecification", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Posterior Bootstrap, a set of algorithms extending Weighted\nLikelihood Bootstrap, to properly incorporate prior information and address the\nproblem of model misspecification in Bayesian inference. We consider two\napproaches to incorporating prior knowledge: the first is based on penalization\nof the Weighted Likelihood Bootstrap objective function, and the second uses\npseudo-samples from the prior predictive distribution. We also propose\nmethodology for hierarchical models, which was not previously known for methods\nbased on Weighted Likelihood Bootstrap. Edgeworth expansions guide the\ndevelopment of our methodology and allow us to provide greater insight on\nproperties of Weighted Likelihood Bootstrap than were previously known. Our\nexperiments confirm the theoretical results and show a reduction in the impact\nof model misspecification against Bayesian inference in the misspecified\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:55:35 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:24:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Pompe", "Emilia", ""]]}, {"id": "2103.14668", "submitter": "Alexander Kreiss", "authors": "Alexander Kreiss and Enno Mammen and Wolfgang Polonik", "title": "Testing For a Parametric Baseline-Intensity in Dynamic Interaction\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical network analysis it is common to observe so called interaction\ndata. Such data is characterized by the actors who form the vertices of a\nnetwork. These are able to interact with each other along the edges of the\nnetwork. One usually assumes that the edges in the network are randomly formed\nand dissolved over the observation horizon. In addition covariates are observed\nand the interest is to model the impact of the covariates on the interactions.\nIn this paper we develop a framework to test if a non-parametric form of the\nbaseline intensity allows for more flexibility than a baseline which is\nparametrically dependent on system-wide covariates (i.e. covariates which take\nthe same value for all individuals, e.g. time). This allows to test if certain\nseasonality effects can be explained by simple covariates like the time. The\nprocedure is applied to modeling the baseline intensity in a bike-sharing\nnetwork by using weather and time information.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:18:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kreiss", "Alexander", ""], ["Mammen", "Enno", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2103.15080", "submitter": "Min Dai", "authors": "Min Dai, Jinqiao Duan, Jianyu Hu, Xiangjun Wang", "title": "Variational inference of the drift function for stochastic differential\n  equations driven by L\\'{e}vy processes", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the nonparametric estimation problem of the drift\nfunction of stochastic differential equations driven by $\\alpha$-stable\nL\\'{e}vy motion. First, the Kullback-Leibler divergence between the path\nprobabilities of two stochastic differential equations with different drift\nfunctions is optimized. By using the Lagrangian multiplier, the variational\nformula based on the stationary Fokker-Planck equation is constructed. Then\ncombined with the data information, the empirical distribution is used to\nreplace the stationary density, and the drift function is estimated\nnon-parametrically from the perspective of the process. In the numerical\nexperiment, the different amounts of data and different $\\alpha$ values are\nstudied. The experimental results show that the estimation result of the drift\nfunction is related to both. When the amount of data increases, the estimation\nresult will be better, and when the $\\alpha$ value increases, the estimation\nresult is also better.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 08:24:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dai", "Min", ""], ["Duan", "Jinqiao", ""], ["Hu", "Jianyu", ""], ["Wang", "Xiangjun", ""]]}, {"id": "2103.15095", "submitter": "ChihHao Chang", "authors": "Chih-Hao Chang, Hsin-Cheng Huang, Ching-Kang Ing", "title": "Inference of Random Effects for Linear Mixed-Effects Models with a Fixed\n  Number of Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear mixed-effects model with a clustered structure, where\nthe parameters are estimated using maximum likelihood (ML) based on possibly\nunbalanced data. Inference with this model is typically done based on\nasymptotic theory, assuming that the number of clusters tends to infinity with\nthe sample size. However, when the number of clusters is fixed, classical\nasymptotic theory developed under a divergent number of clusters is no longer\nvalid and can lead to erroneous conclusions. In this paper, we establish the\nasymptotic properties of the ML estimators of random-effects parameters under a\ngeneral setting, which can be applied to conduct valid statistical inference\nwith fixed numbers of clusters. Our asymptotic theorems allow both fixed\neffects and random effects to be misspecified, and the dimensions of both\neffects to go to infinity with the sample size.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:52:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chang", "Chih-Hao", ""], ["Huang", "Hsin-Cheng", ""], ["Ing", "Ching-Kang", ""]]}, {"id": "2103.15249", "submitter": "Suqi Liu", "authors": "Suqi Liu and Miklos Z. Racz", "title": "Phase transition in noisy high-dimensional random geometric graphs", "comments": "50 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting latent geometric structure in random\ngraphs. To this end, we consider the soft high-dimensional random geometric\ngraph $\\mathcal{G}(n,p,d,q)$, where each of the $n$ vertices corresponds to an\nindependent random point distributed uniformly on the sphere\n$\\mathbb{S}^{d-1}$, and the probability that two vertices are connected by an\nedge is a decreasing function of the Euclidean distance between the points. The\nprobability of connection is parametrized by $q \\in [0,1]$, with smaller $q$\ncorresponding to weaker dependence on the geometry; this can also be\ninterpreted as the level of noise in the geometric graph. In particular, the\nmodel smoothly interpolates between the spherical hard random geometric graph\n$\\mathcal{G}(n,p,d)$ (corresponding to $q = 1$) and the Erd\\H{o}s-R\\'enyi model\n$\\mathcal{G}(n,p)$ (corresponding to $q = 0$). We focus on the dense regime\n(i.e., $p$ is a constant).\n  We show that if $nq \\to 0$ or $d \\gg n^{3} q^{2}$, then geometry is lost:\n$\\mathcal{G}(n,p,d,q)$ is asymptotically indistinguishable from\n$\\mathcal{G}(n,p)$. On the other hand, if $d \\ll n^{3} q^{6}$, then the signed\ntriangle statistic provides an asymptotically powerful test for detecting\ngeometry. These results generalize those of Bubeck, Ding, Eldan, and R\\'acz\n(2016) for $\\mathcal{G}(n,p,d)$, and give quantitative bounds on how the noise\nlevel affects the dimension threshold for losing geometry. We also prove\nanalogous results under a related but different distributional assumption, and\nwe further explore generalizations of signed triangles in order to understand\nthe intermediate regime left open by our results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 23:57:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Suqi", ""], ["Racz", "Miklos Z.", ""]]}, {"id": "2103.15457", "submitter": "Nicole Hufnagel", "authors": "Yuzhong Cheng, Nicole Hufnagel, Hiroki Masuda", "title": "Estimation of ergodic square-root diffusion under high-frequency\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Gaussian quasi-likelihood estimation of the parameter\n$\\theta:=(\\alpha,\\beta,\\gamma)$ of the square-root diffusion process, also\nknown as the Cox-Ingersoll-Ross (CIR) model, observed at high frequency.\nDifferent from the previous study [1] under low-frequency sampling,\nhigh-frequency of data provides us with very simple form of the asymptotic\ncovariance matrix. Through easy-to-compute preliminary contrast functions, we\nformulate a practical two-stage manner without numerical optimization in order\nto conduct not only an asymptotically efficient estimation of the drift\nparameters, but also high-precision estimator of the diffusion parameter.\nSimulation experiments are given to illustrate the results.\n  [1] L. Overbeck and T. Ryd\\'en. Estimation in the Cox-Ingersoll-Ross model.\nEconometric Theory, 13(3): 430-461, 1997.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:42:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cheng", "Yuzhong", ""], ["Hufnagel", "Nicole", ""], ["Masuda", "Hiroki", ""]]}, {"id": "2103.15540", "submitter": "Johan Pensar", "authors": "Johan Pensar and Henrik Nyman and Jukka Corander", "title": "Structure Learning of Contextual Markov Networks using Marginal\n  Pseudo-likelihood", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, Vol. 44: 455-479, 2017", "doi": "10.1111/sjos.12260", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are popular models for discrete multivariate systems where\nthe dependence structure of the variables is specified by an undirected graph.\nTo allow for more expressive dependence structures, several generalizations of\nMarkov networks have been proposed. Here we consider the class of contextual\nMarkov networks which takes into account possible context-specific\nindependences among pairs of variables. Structure learning of contextual Markov\nnetworks is very challenging due to the extremely large number of possible\nstructures. One of the main challenges has been to design a score, by which a\nstructure can be assessed in terms of model fit related to complexity, without\nassuming chordality. Here we introduce the marginal pseudo-likelihood as an\nanalytically tractable criterion for general contextual Markov networks. Our\ncriterion is shown to yield a consistent structure estimator. Experiments\ndemonstrate the favorable properties of our method in terms of predictive\naccuracy of the inferred models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:13:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Corander", "Jukka", ""]]}, {"id": "2103.15653", "submitter": "Nir Weinberger", "authors": "Nir Weinberger and Guy Bresler", "title": "The EM Algorithm is Adaptively-Optimal for Unbalanced Symmetric Gaussian\n  Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating the means\n$\\pm\\theta_{*}\\in\\mathbb{R}^{d}$ of a symmetric two-component Gaussian mixture\n$\\delta_{*}\\cdot N(\\theta_{*},I)+(1-\\delta_{*})\\cdot N(-\\theta_{*},I)$ where\nthe weights $\\delta_{*}$ and $1-\\delta_{*}$ are unequal. Assuming that\n$\\delta_{*}$ is known, we show that the population version of the EM algorithm\nglobally converges if the initial estimate has non-negative inner product with\nthe mean of the larger weight component. This can be achieved by the trivial\ninitialization $\\theta_{0}=0$. For the empirical iteration based on $n$\nsamples, we show that when initialized at $\\theta_{0}=0$, the EM algorithm\nadaptively achieves the minimax error rate\n$\\tilde{O}\\Big(\\min\\Big\\{\\frac{1}{(1-2\\delta_{*})}\\sqrt{\\frac{d}{n}},\\frac{1}{\\|\\theta_{*}\\|}\\sqrt{\\frac{d}{n}},\\left(\\frac{d}{n}\\right)^{1/4}\\Big\\}\\Big)$\nin no more than $O\\Big(\\frac{1}{\\|\\theta_{*}\\|(1-2\\delta_{*})}\\Big)$ iterations\n(with high probability). We also consider the EM iteration for estimating the\nweight $\\delta_{*}$, assuming a fixed mean $\\theta$ (which is possibly\nmismatched to $\\theta_{*}$). For the empirical iteration of $n$ samples, we\nshow that the minimax error rate\n$\\tilde{O}\\Big(\\frac{1}{\\|\\theta_{*}\\|}\\sqrt{\\frac{d}{n}}\\Big)$ is achieved in\nno more than $O\\Big(\\frac{1}{\\|\\theta_{*}\\|^{2}}\\Big)$ iterations. These\nresults robustify and complement recent results of Wu and Zhou obtained for the\nequal weights case $\\delta_{*}=1/2$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:28:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weinberger", "Nir", ""], ["Bresler", "Guy", ""]]}, {"id": "2103.15671", "submitter": "Edwin Fong", "authors": "Edwin Fong, Chris Holmes, Stephen G. Walker", "title": "Martingale Posterior Distributions", "comments": "60 pages, 22 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prior distribution on parameters of a likelihood is the usual starting\npoint for Bayesian uncertainty quantification. In this paper, we present a\ndifferent perspective. Given a finite data sample $Y_{1:n}$ of size $n$ from an\ninfinite population, we focus on the missing $Y_{n+1:\\infty}$ as the source of\nstatistical uncertainty, with the parameter of interest being known precisely\ngiven $Y_{1:\\infty}$. We argue that the foundation of Bayesian inference is to\nassign a predictive distribution on $Y_{n+1:\\infty}$ conditional on $Y_{1:n}$,\nwhich then induces a distribution on the parameter of interest. Demonstrating\nan application of martingales, Doob shows that choosing the Bayesian predictive\ndistribution returns the conventional posterior as the distribution of the\nparameter. Taking this as our cue, we relax the predictive machine, avoiding\nthe need for the predictive to be derived solely from the usual prior to\nposterior to predictive density formula. We introduce the martingale posterior\ndistribution, which returns Bayesian uncertainty directly on any statistic of\ninterest without the need for the likelihood and prior, and this distribution\ncan be sampled through a computational scheme we name predictive resampling. To\nthat end, we introduce new predictive methodologies for multivariate density\nestimation, regression and classification that build upon recent work on\nbivariate copulas.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:49:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fong", "Edwin", ""], ["Holmes", "Chris", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2103.15672", "submitter": "Vu Dinh", "authors": "Vu Dinh, Ann E. Rundell and Gregery T. Buzzard", "title": "Convergence of Griddy Gibbs Sampling and other perturbed Markov chains", "comments": "28 pages, 5 figures", "journal-ref": "Journal of Statistical Computation and Simulation 87.7 (2017):\n  1379-1400", "doi": "10.1080/00949655.2016.1264399", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Griddy Gibbs sampling was proposed by Ritter and Tanner (1992) as a\ncomputationally efficient approximation of the well-known Gibbs sampling\nmethod. The algorithm is simple and effective and has been used successfully to\naddress problems in various fields of applied science. However, the approximate\nnature of the algorithm has prevented it from being widely used: the Markov\nchains generated by the Griddy Gibbs sampling method are not reversible in\ngeneral, so the existence and uniqueness of its invariant measure is not\nguaranteed. Even when such an invariant measure uniquely exists, there was no\nestimate of the distance between it and the probability distribution of\ninterest, hence no means to ensure the validity of the algorithm as a means to\nsample from the true distribution.\n  In this paper, we show, subject to some fairly natural conditions, that the\nGriddy Gibbs method has a unique, invariant measure. Moreover, we provide $L^p$\nestimates on the distance between this invariant measure and the corresponding\nmeasure obtained from Gibbs sampling. These results provide a theoretical\nfoundation for the use of the Griddy Gibbs sampling method. We also address a\nmore general result about the sensitivity of invariant measures under small\nperturbations on the transition probability. That is, if we replace the\ntransition probability $P$ of any Monte Carlo Markov chain by another\ntransition probability $Q$ where $Q$ is close to $P$, we can still estimate the\ndistance between the two invariant measures. The distinguishing feature between\nour approach and previous work on convergence of perturbed Markov chain is that\nby considering the invariant measures as fixed points of linear operators on\nfunction spaces, we don't need to impose any further conditions on the rate of\nconvergence of the Markov chain.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:51:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dinh", "Vu", ""], ["Rundell", "Ann E.", ""], ["Buzzard", "Gregery T.", ""]]}, {"id": "2103.15740", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Exact converses to a reverse AM--GM inequality, with applications to\n  sums of independent random variables and (super)martingales", "comments": "16 pages. To appear in Mathematical Inequalities & Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For every given real value of the ratio $\\mu:=A_X/G_X>1$ of the arithmetic\nand geometric means of a positive random variable $X$ and every real $v>0$,\nexact upper bounds on the right- and left-tail probabilities\n$\\mathsf{P}(X/G_X\\ge v)$ and $\\mathsf{P}(X/G_X\\le v)$ are obtained, in terms of\n$\\mu$ and $v$. In particular, these bounds imply that $X/G_X\\to1$ in\nprobability as $A_X/G_X\\downarrow1$. Such a result may be viewed as a converse\nto a reverse Jensen inequality for the strictly concave function $f=\\ln$,\nwhereas the well-known Cantelli and Chebyshev inequalities may be viewed as\nconverses to a reverse Jensen inequality for the strictly concave quadratic\nfunction $f(x) \\equiv -x^2$. As applications of the mentioned new results,\nimprovements of the Markov, Bernstein--Chernoff, sub-Gaussian, and\nBennett--Hoeffding probability inequalities are given.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:26:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "2103.15996", "submitter": "Theodor Misiakiewicz Mr.", "authors": "Michael Celentano, Theodor Misiakiewicz, Andrea Montanari", "title": "Minimum complexity interpolation in random features models", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their many appealing properties, kernel methods are heavily affected\nby the curse of dimensionality. For instance, in the case of inner product\nkernels in $\\mathbb{R}^d$, the Reproducing Kernel Hilbert Space (RKHS) norm is\noften very large for functions that depend strongly on a small subset of\ndirections (ridge functions). Correspondingly, such functions are difficult to\nlearn using kernel methods.\n  This observation has motivated the study of generalizations of kernel\nmethods, whereby the RKHS norm -- which is equivalent to a weighted $\\ell_2$\nnorm -- is replaced by a weighted functional $\\ell_p$ norm, which we refer to\nas $\\mathcal{F}_p$ norm. Unfortunately, tractability of these approaches is\nunclear. The kernel trick is not available and minimizing these norms requires\nto solve an infinite-dimensional convex problem.\n  We study random features approximations to these norms and show that, for\n$p>1$, the number of random features required to approximate the original\nlearning problem is upper bounded by a polynomial in the sample size. Hence,\nlearning with $\\mathcal{F}_p$ norms is tractable in these cases. We introduce a\nproof technique based on uniform concentration in the dual, which can be of\nbroader interest in the study of overparametrized models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:00:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Celentano", "Michael", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2103.16035", "submitter": "Hanwen Huang", "authors": "Hanwen Huang", "title": "LASSO risk and phase transition under dependence", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a $k$-sparse signal\n${\\mbox{$\\beta$}}_0\\in\\mathbb{R}^p$ from noisy observations $\\bf y={\\bf\nX}\\mbox{$\\beta$}_0+{\\bf w}\\in\\mathbb{R}^n$. One of the most popular approaches\nis the $l_1$-regularized least squares, also known as LASSO. We analyze the\nmean square error of LASSO in the case of random designs in which each row of\n${\\bf X}$ is drawn from distribution $N(0,{\\mbox{$\\Sigma$}})$ with general\n${\\mbox{$\\Sigma$}}$. We first derive the asymptotic risk of LASSO in the limit\nof $n,p\\rightarrow\\infty$ with $n/p\\rightarrow\\delta$. We then examine\nconditions on $n$, $p$, and $k$ for LASSO to exactly reconstruct\n${\\mbox{$\\beta$}}_0$ in the noiseless case ${\\bf w}=0$. A phase boundary\n$\\delta_c=\\delta(\\epsilon)$ is precisely established in the phase space defined\nby $0\\le\\delta,\\epsilon\\le 1$, where $\\epsilon=k/p$. Above this boundary, LASSO\nperfectly recovers ${\\mbox{$\\beta$}}_0$ with high probability. Below this\nboundary, LASSO fails to recover $\\mbox{$\\beta$}_0$ with high probability.\nWhile the values of the non-zero elements of ${\\mbox{$\\beta$}}_0$ do not have\nany effect on the phase transition curve, our analysis shows that $\\delta_c$\ndoes depend on the signed pattern of the nonzero values of $\\mbox{$\\beta$}_0$\nfor general ${\\mbox{$\\Sigma$}}\\ne{\\bf I}_p$. This is in sharp contrast to the\nprevious phase transition results derived in i.i.d. case with\n$\\mbox{$\\Sigma$}={\\bf I}_p$ where $\\delta_c$ is completely determined by\n$\\epsilon$ regardless of the distribution of $\\mbox{$\\beta$}_0$. Underlying our\nformalism is a recently developed efficient algorithm called approximate\nmessage passing (AMP) algorithm. We generalize the state evolution of AMP from\ni.i.d. case to general case with ${\\mbox{$\\Sigma$}}\\ne{\\bf I}_p$. Extensive\ncomputational experiments confirm that our theoretical predictions are\nconsistent with simulation results on moderate size system.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:43:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Hanwen", ""]]}, {"id": "2103.16227", "submitter": "Tong Pu", "authors": "Tong Pu, Chuancun Yin", "title": "Stochastic Orderings of the Location-Scale Mixture of Elliptical\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the general familiy of multivariate elliptical location-scale\nmixture model. This class of distributions presents a mathematically tractable\nextension of the multivariate elliptical distribution. We give some sufficient\nand/or necessary conditions for various of integral stochastic orders. The\nintegral orders considered here are the usual, upper orthant, supermodular,\nconvex, increasing convex and directionally convex stochastic orders.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:17:03 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 02:55:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pu", "Tong", ""], ["Yin", "Chuancun", ""]]}, {"id": "2103.16860", "submitter": "Palash Sarkar", "authors": "Palash Sarkar and Prasanta S. Bandyopadhyay", "title": "Simpson's Paradox: A Singularity of Statistical and Inductive Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The occurrence of Simpson's paradox (SP) in $2\\times 2$ contingency tables\nhas been well studied in the literature. The first contribution of the present\nwork is to comprehensively revisit this problem. We provide a threadbare\nanalysis of SP in $2\\times 2$ contingency tables and present new results,\ndetailed proofs of previous results and a unifying view of the important\nexamples of SP that have been reported in the literature. The second\ncontribution of the paper suggests a new perspective on the surprise element of\nSP, raises some critical questions for the causal analysis of SP and provides a\nbroad perspective on logic, probability and statistics with SP at its centre.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:22:34 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sarkar", "Palash", ""], ["Bandyopadhyay", "Prasanta S.", ""]]}, {"id": "2103.16908", "submitter": "Wenyang Huang", "authors": "Wenyang Huang, Huiwen Wang, Shanshan Wang", "title": "Dimension reduction of open-high-low-close data in candlestick chart\n  based on pseudo-PCA", "comments": "33 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (open-high-low-close) OHLC data is the most common data form in the field\nof finance and the investigate object of various technical analysis. With\nincreasing features of OHLC data being collected, the issue of extracting their\nuseful information in a comprehensible way for visualization and easy\ninterpretation must be resolved. The inherent constraints of OHLC data also\npose a challenge for this issue. This paper proposes a novel approach to\ncharacterize the features of OHLC data in a dataset and then performs dimension\nreduction, which integrates the feature information extraction method and\nprincipal component analysis. We refer to it as the pseudo-PCA method.\nSpecifically, we first propose a new way to represent the OHLC data, which will\nfree the inherent constraints and provide convenience for further analysis.\nMoreover, there is a one-to-one match between the original OHLC data and its\nfeature-based representations, which means that the analysis of the\nfeature-based data can be reversed to the original OHLC data. Next, we develop\nthe pseudo-PCA procedure for OHLC data, which can effectively identify\nimportant information and perform dimension reduction. Finally, the\neffectiveness and interpretability of the proposed method are investigated\nthrough finite simulations and the spot data of China's agricultural product\nmarket.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:56:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Huang", "Wenyang", ""], ["Wang", "Huiwen", ""], ["Wang", "Shanshan", ""]]}, {"id": "2103.17027", "submitter": "Thomas Dybdahl Ahle", "authors": "Thomas D. Ahle", "title": "Sharp and Simple Bounds for the raw Moments of the Binomial and Poisson\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We prove the inequality $E[(X/\\mu)^k]\n  \\le\n  (\\frac{k/\\mu}{\\log(k/\\mu+1)})^k\n  \\le\n  \\exp(k^2/(2\\mu))$ for sub-Poissonian random variables, such as Binomially or\nPoisson distributed random variables with mean $\\mu$. The asymptotics\n$1+O(k^2/\\mu)$ can be shown to be tight for small $k$.\n  This improves over previous uniform bounds for the raw moments of those\ndistributions by a factor exponential in $k$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:23:01 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 16:10:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ahle", "Thomas D.", ""]]}, {"id": "2103.17060", "submitter": "Masanari Kimura", "authors": "Masanari Kimura and Hideitsu Hino", "title": "$\\alpha$-Geodesical Skew Divergence", "comments": null, "journal-ref": "Entropy. 2021; 23(5):528", "doi": "10.3390/e23050528", "report-no": null, "categories": "cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The asymmetric skew divergence smooths one of the distributions by mixing it,\nto a degree determined by the parameter $\\lambda$, with the other distribution.\nSuch divergence is an approximation of the KL divergence that does not require\nthe target distribution to be absolutely continuous with respect to the source\ndistribution. In this paper, an information geometric generalization of the\nskew divergence called the $\\alpha$-geodesical skew divergence is proposed, and\nits properties are studied.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:27:58 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 05:40:47 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 11:16:15 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 03:08:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kimura", "Masanari", ""], ["Hino", "Hideitsu", ""]]}, {"id": "2103.17164", "submitter": "Deborah Sulem", "authors": "Deborah Sulem, Vincent Rivoirard and Judith Rousseau", "title": "Bayesian estimation of nonlinear Hawkes process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate point processes are widely applied to model event-type data such\nas natural disasters, online message exchanges, financial transactions or\nneuronal spike trains. One very popular point process model in which the\nprobability of occurrences of new events depend on the past of the process is\nthe Hawkes process. In this work we consider the nonlinear Hawkes process,\nwhich notably models excitation and inhibition phenomena between dimensions of\nthe process. In a nonparametric Bayesian estimation framework, we obtain\nconcentration rates of the posterior distribution on the parameters, under mild\nassumptions on the prior distribution and the model. These results also lead to\nconvergence rates of Bayesian estimators. Another object of interest in\nevent-data modelling is to recover the graph of interaction - or Granger\nconnectivity graph - of the phenomenon. We provide consistency guarantees on\nBayesian methods for estimating this quantity; in particular, we prove that the\nposterior distribution is consistent on the graph adjacency matrix of the\nprocess, as well as a Bayesian estimator based on an adequate loss function.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:28:51 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 16:31:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sulem", "Deborah", ""], ["Rivoirard", "Vincent", ""], ["Rousseau", "Judith", ""]]}, {"id": "2103.17203", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "Universal Prediction Band via Semi-Definite Programming", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient method to construct nonparametric,\nheteroskedastic prediction bands for uncertainty quantification, with or\nwithout any user-specified predictive model. The data-adaptive prediction band\nis universally applicable with minimal distributional assumptions, with strong\nnon-asymptotic coverage properties, and easy to implement using standard convex\nprograms. Our approach can be viewed as a novel variance interpolation with\nconfidence and further leverages techniques from semi-definite programming and\nsum-of-squares optimization. Theoretical and numerical performances for the\nproposed approach for uncertainty quantification are analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:30:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Liang", "Tengyuan", ""]]}]