[{"id": "2105.00049", "submitter": "Shayan Hundrieser", "authors": "Shayan Hundrieser, Marcel Klatt, Axel Munk", "title": "Limit Distributions and Sensitivity Analysis for Entropic Optimal\n  Transport on Countable Spaces", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For probability measures supported on countable spaces we derive limit\ndistributions for empirical entropic optimal transport quantities. In\nparticular, we prove that the corresponding plan converges weakly to a centered\nGaussian process. Furthermore, its optimal value is shown to be asymptotically\nnormal. The results are valid for a large class of ground cost functions and\ngeneralize recently obtained limit laws for empirical entropic optimal\ntransport quantities on finite spaces. Our proofs are based on a sensitivity\nanalysis with respect to a weighted $\\ell^1$-norm relying on the dual\nformulation of entropic optimal transport as well as necessary and sufficient\noptimality conditions for the entropic transport plan. This can be used to\nderive weak convergence of the empirical entropic optimal transport plan and\nvalue that results in weighted Borisov-Dudley-Durst conditions on the\nunderlying probability measures. The weights are linked to an exponential\npenalty term for dual entropic optimal transport and the underlying ground cost\nfunction under consideration. Finally, statistical applications, such as\nbootstrap, are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:15:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hundrieser", "Shayan", ""], ["Klatt", "Marcel", ""], ["Munk", "Axel", ""]]}, {"id": "2105.00223", "submitter": "Bennet Str\\\"oh", "authors": "Robert Stelzer and Bennet Str\\\"oh", "title": "Approximations and asymptotics of continuous-time locally stationary\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general theory on stationary approximations for locally\nstationary continuous-time processes. Based on the stationary approximation, we\nuse $\\theta$-weak dependence to establish laws of large numbers and central\nlimit type results under different observation schemes. Hereditary properties\nfor a large class of finite and infinite memory transformations show the\nflexibility of the developed theory. Sufficient conditions for the existence of\nstationary approximations for time-varying L\\'evy-driven state space models are\nderived and compared to existing results. We conclude with comprehensive\nresults on the asymptotic behavior of the first and second order localized\nsample moments of time-varying L\\'evy-driven state space models.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:38:24 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Stelzer", "Robert", ""], ["Str\u00f6h", "Bennet", ""]]}, {"id": "2105.00284", "submitter": "Teppei Ogihara", "authors": "Teppei Ogihara and Yuma Uehara", "title": "Local Asymptotic Mixed Normality via Transition Density Approximation\n  and an Application to Ergodic Jump-Diffusion Processes", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sufficient conditions for local asymptotic mixed normality. We\nweaken the sufficient conditions in Theorem 1 of Jeganathan (Sankhya Ser. A\n1982) so that they can be applied to a wider class of statistical models\nincluding a jump-diffusion model. Moreover, we show that local asymptotic mixed\nnormality of a statistical model generated by approximated transition density\nfunctions is implied for the original model. Together with density\napproximation by means of thresholding techniques, we show local asymptotic\nnormality for a statistical model of discretely observed jump-diffusion\nprocesses where the drift coefficient, diffusion coefficient, and jump\nstructure are parametrized. As a consequence, the quasi-maximum-likelihood and\nBayes-type estimators proposed in Shimizu and Yoshida (Stat. Inference Stoch.\nProcess. 2006) and Ogihara and Yoshida (Stat. Inference Stoch. Process. 2011)\nare shown to be asymptotically efficient in this model. Moreover, we can\nconstruct asymptotically uniformly most powerful tests for the parameters.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:32:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ogihara", "Teppei", ""], ["Uehara", "Yuma", ""]]}, {"id": "2105.00292", "submitter": "Jian Huang", "authors": "Guohao Shen, Yuling Jiao, Yuanyuan Lin and Jian Huang", "title": "Non-asymptotic Excess Risk Bounds for Classification with Deep\n  Convolutional Neural Networks", "comments": "Guohao Shen and Yuling Jiao contributed equally to this work.\n  Co-corresponding authors: Yuanyuan Lin (Email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (Email: jian-huang@uiowa.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the problem of binary classification with a class\nof general deep convolutional neural networks, which includes fully-connected\nneural networks and fully convolutional neural networks as special cases. We\nestablish non-asymptotic excess risk bounds for a class of convex surrogate\nlosses and target functions with different modulus of continuity. An important\nfeature of our results is that we clearly define the prefactors of the risk\nbounds in terms of the input data dimension and other model parameters and show\nthat they depend polynomially on the dimensionality in some important models.\nWe also show that the classification methods with CNNs can circumvent the curse\nof dimensionality if the input data is supported on an approximate\nlow-dimensional manifold. To establish these results, we derive an upper bound\nfor the covering number for the class of general convolutional neural networks\nwith a bias term in each convolutional layer, and derive new results on the\napproximation power of CNNs for any uniformly-continuous target functions.\nThese results provide further insights into the complexity and the\napproximation power of general convolutional neural networks, which are of\nindependent interest and may have other applications. Finally, we apply our\ngeneral results to analyze the non-asymptotic excess risk bounds for four\nwidely used methods with different loss functions using CNNs, including the\nleast squares, the logistic, the exponential and the SVM hinge losses.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:55:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shen", "Guohao", ""], ["Jiao", "Yuling", ""], ["Lin", "Yuanyuan", ""], ["Huang", "Jian", ""]]}, {"id": "2105.00393", "submitter": "Huiming Zhang", "authors": "Chang Cui, Jinzhu Jia, Yijun Xiao, Huiming Zhang", "title": "Directional FDR Control for Sub-Gaussian Sparse GLMs", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse generalized linear models (GLMs) have emerged in the\nsetting that the number of samples and the dimension of variables are large,\nand even the dimension of variables grows faster than the number of samples.\nFalse discovery rate (FDR) control aims to identify some small number of\nstatistically significantly nonzero results after getting the sparse penalized\nestimation of GLMs. Using the CLIME method for precision matrix estimations, we\nconstruct the debiased-Lasso estimator and prove the asymptotical normality by\nminimax-rate oracle inequalities for sparse GLMs. In practice, it is often\nneeded to accurately judge each regression coefficient's positivity and\nnegativity, which determines whether the predictor variable is positively or\nnegatively related to the response variable conditionally on the rest\nvariables. Using the debiased estimator, we establish multiple testing\nprocedures. Under mild conditions, we show that the proposed debiased\nstatistics can asymptotically control the directional (sign) FDR and\ndirectional false discovery variables at a pre-specified significance level.\nMoreover, it can be shown that our multiple testing procedure can approximately\nachieve a statistical power of 1. We also extend our methods to the two-sample\nproblems and propose the two-sample test statistics. Under suitable conditions,\nwe can asymptotically achieve directional FDR control and directional FDV\ncontrol at the specified significance level for two-sample problems. Some\nnumerical simulations have successfully verified the FDR control effects of our\nproposed testing procedures, which sometimes outperforms the classical knockoff\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 05:34:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cui", "Chang", ""], ["Jia", "Jinzhu", ""], ["Xiao", "Yijun", ""], ["Zhang", "Huiming", ""]]}, {"id": "2105.00483", "submitter": "Aba Diop", "authors": "Aba Diop, Demba Bocar Ba, Fatimata Lo", "title": "Asymptotic properties in the Probit-Zero-inflated Binomial regression\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-inflated regression models have had wide application recently and have\nprovenuseful in modeling data with many zeros. Zero-inflated Binomial (ZIB)\nregression model is an extension of the ordinary binomial distribution that\ntakes into account the excess of zeros. In comparing the probit model to the\nlogistic model, many authors believe that there is little theoretical\njustification in choosing one formulation over the other in most circumstances\ninvolving binary responses. The logit model is considered to be computationally\nsimpler but it is based on a more restrictive assumption of error independence,\nalthough many other generalizations have dealt with that assumption as well. By\ncontrast, the probit model assumes that random errors have a multivariate\nnormal distribution. This assumption makes the probit model attractive because\nthe normal distribution provides a good approximation to many other\ndistributions. In this paper, we develop a maximum likelihood estimation\nprocedure for the parameters of a zero-inflated Binomial regression model with\nprobit link function for both component of the model. We establish the\nexistency, consistency and asymptotic normality of the proposed estimator.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:24:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Diop", "Aba", ""], ["Ba", "Demba Bocar", ""], ["Lo", "Fatimata", ""]]}, {"id": "2105.00504", "submitter": "Laura Dumitrescu", "authors": "Laura Dumitrescu, Wei Qian and J. N. K. Rao", "title": "Variable selection for longitudinal survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a new variable selection method for analyzing data\ncollected from longitudinal sample surveys. The procedure is based on the\nsurvey-weighted quadratic inference function, which was recently introduced as\nan alternative to the survey-weighted generalized estimating function. Under\nthe joint model-design framework, we introduce the penalized survey-weighted\nquadratic inference estimator and obtain sufficient conditions for the\nexistence, weak consistency, sparsity and asymptotic normality. To illustrate\nthe finite sample performance of the model selection procedure, we include a\nlimited simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:29:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dumitrescu", "Laura", ""], ["Qian", "Wei", ""], ["Rao", "J. N. K.", ""]]}, {"id": "2105.00724", "submitter": "Ines Nuessgen", "authors": "Ines N\\\"u{\\ss}gen and Alexander Schnurr", "title": "Ordinal Pattern Dependence in the Context of Long-Range Dependence", "comments": null, "journal-ref": null, "doi": "10.3390/e23060670", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal pattern dependence is a multivariate dependence measure based on the\nco-movement of two time series. In strong connection to ordinal time series\nanalysis, the ordinal information is taken into account to derive robust\nresults on the dependence between the two processes. This article deals with\nordinal pattern dependence for long-range dependent time series including mixed\ncases of short- and long-range dependence. We investigate the limit\ndistributions for estimators of ordinal pattern dependence. In doing so we\npoint out the differences that arise for the underlying time series having\ndifferent dependence structures. Depending on these assumptions, central and\nnon-central limit theorems are proven. The limit distributions for the latter\nones can be included in the class of multivariate Rosenblatt processes.\nFinally, a simulation study is provided to illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 10:13:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["N\u00fc\u00dfgen", "Ines", ""], ["Schnurr", "Alexander", ""]]}, {"id": "2105.00833", "submitter": "Sara Salvador", "authors": "Sara Salvador, Riccardo Gatto", "title": "Bayesian tests of symmetry for the generalized von Mises distribution", "comments": "25 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian tests on the symmetry of the generalized von Mises model for planar\ndirections (Gatto and Jammalamadaka, 2007) are introduced. The generalized von\nMises distribution is a flexible model that can be axially symmetric or\nasymmetric, unimodal or bimodal. A characterization of axial symmetry is\nprovided and taken as null hypothesis for one of the proposed Bayesian tests.\nThe Bayesian tests are obtained by the technique of probability perturbation.\nThe prior probability measure is perturbed so to give a positive prior\nprobability to the null hypothesis, which would be null otherwise. This allows\nfor the derivation of simple computational formulae for the Bayes factors.\nNumerical results reveal that, whenever the simulation scheme of the samples\nsupports the null hypothesis, the null posterior probabilities appear\nsystematically larger than their prior counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:22:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Salvador", "Sara", ""], ["Gatto", "Riccardo", ""]]}, {"id": "2105.00946", "submitter": "Jad Beyhum", "authors": "Jad Beyhum, Jean-Pierre Florens, Ingrid Van Keilegom", "title": "A nonparametric instrumental approach to endogeneity in competing risks\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses endogenous treatment models with duration outcomes,\ncompeting risks and random right censoring. The endogeneity issue is solved\nusing a discrete instrumental variable. We show that the competing risks model\ngenerates a non-parametric quantile instrumental regression problem. The\ncause-specific cumulative incidence, the cause-specific hazard and the\nsubdistribution hazard can be recovered from the regression function. A\ndistinguishing feature of the model is that censoring and competing risks\nprevent identification at some quantiles. We characterize the set of quantiles\nfor which exact identification is possible and give partial identification\nresults for other quantiles. We outline an estimation procedure and discuss its\nproperties. The finite sample performance of the estimator is evaluated through\nsimulations. We apply the proposed method to the Health Insurance Plan of\nGreater New York experiment.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:22:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Beyhum", "Jad", ""], ["Florens", "Jean-Pierre", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2105.01108", "submitter": "Alex Dytso", "authors": "Luc Devroye and Alex Dytso", "title": "Consistent Density Estimation Under Discrete Mixture Models", "comments": "Reason for withdrawal: There is an issue with the proof of Theorem~1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work considers a problem of estimating a mixing probability density $f$\nin the setting of discrete mixture models. The paper consists of three parts.\n  The first part focuses on the construction of an $L_1$ consistent estimator\nof $f$. In particular, under the assumptions that the probability measure $\\mu$\nof the observation is atomic, and the map from $f$ to $\\mu$ is bijective, it is\nshown that there exists an estimator $f_n$ such that for every density $f$\n$\\lim_{n\\to \\infty} \\mathbb{E} \\left[ \\int |f_n -f | \\right]=0$.\n  The second part discusses the implementation details. Specifically, it is\nshown that the consistency for every $f$ can be attained with a computationally\nfeasible estimator.\n  The third part, as a study case, considers a Poisson mixture model. In\nparticular, it is shown that in the Poisson noise setting, the bijection\ncondition holds and, hence, estimation can be performed consistently for every\n$f$.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:30:02 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 12:49:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Devroye", "Luc", ""], ["Dytso", "Alex", ""]]}, {"id": "2105.01184", "submitter": "Peng Ding", "authors": "Anqi Zhao and Peng Ding", "title": "Reconciling design-based and model-based causal inferences for\n  split-plot experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The split-plot design assigns different interventions at the whole-plot and\nsub-plot levels, respectively, and induces a group structure on the final\ntreatment assignments. A common strategy is to use the OLS fit of the outcome\non the treatment indicators coupled with the robust standard errors clustered\nat the whole-plot level. It does not give consistent estimator for the causal\neffects of interest when the whole-plot sizes vary. Another common strategy is\nto fit the linear mixed-effects model of the outcome with Normal random effects\nand errors. It is a purely model-based approach and can be sensitive to\nviolations of parametric assumptions. In contrast, the design-based inference\nassumes no outcome models and relies solely on the controllable randomization\nmechanism determined by the physical experiment. We first extend the existing\ndesign-based inference based on the {\\htf} estimator to the Hajek estimator,\nand establish the finite-population central limit theorem for both under\nsplit-plot randomization. We then reconcile the results with those under the\nmodel-based approach, and propose two regression strategies, namely (i) the WLS\nfit of the unit-level data based on the inverse probability weighting and (ii)\nthe OLS fit of the aggregate data based on whole-plot total outcomes, to\nreproduce the Hajek and {\\htf} estimators from least squares, respectively.\nThis, together with the asymptotic conservativeness of the corresponding\ncluster-robust covariances for estimating the true design-based covariances as\nwe establish in the process, justifies the validity of regression-based\nestimators for design-based inference. In light of the flexibility of\nregression formulation with covariate adjustment, we further extend the theory\nto the case with covariates and demonstrate the efficiency gain by\nregression-based covariate adjustment via both asymptotic theory and\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:38:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2105.01264", "submitter": "Jue Hou", "authors": "Jue Hou, Zijian Guo and Tianxi Cai", "title": "Surrogate Assisted Semi-supervised Inference for High Dimensional Risk\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Risk modeling with EHR data is challenging due to a lack of direct\nobservations on the disease outcome, and the high dimensionality of the\ncandidate predictors. In this paper, we develop a surrogate assisted\nsemi-supervised-learning (SAS) approach to risk modeling with high dimensional\npredictors, leveraging a large unlabeled data on candidate predictors and\nsurrogates of outcome, as well as a small labeled data with annotated outcomes.\nThe SAS procedure borrows information from surrogates along with candidate\npredictors to impute the unobserved outcomes via a sparse working imputation\nmodel with moment conditions to achieve robustness against mis-specification in\nthe imputation model and a one-step bias correction to enable interval\nestimation for the predicted risk. We demonstrate that the SAS procedure\nprovides valid inference for the predicted risk derived from a high dimensional\nworking model, even when the underlying risk prediction model is dense and the\nrisk model is mis-specified. We present an extensive simulation study to\ndemonstrate the superiority of our SSL approach compared to existing supervised\nmethods. We apply the method to derive genetic risk prediction of type-2\ndiabetes mellitus using a EHR biobank cohort.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:08:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hou", "Jue", ""], ["Guo", "Zijian", ""], ["Cai", "Tianxi", ""]]}, {"id": "2105.01348", "submitter": "Pierre Bertrand", "authors": "Pierre Bertrand (LPSM), Michel Broniatowski (LPSM), Jean-Fran\\c{c}ois\n  Marcotorchino", "title": "Continuous indetermination and average likelihood minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors transpose a discrete notion of indetermination coupling in the\ncase of continuous probabilities. They show that this coupling, expressed on\ndensities, cannot be captured by a specific copula which acts on cumulative\ndistribution functions without a high dependence on the margins. Furthermore,\nthey define a notion of average likelihood which extends the discrete notion of\ncouple matchings and demonstrate it is minimal under indetermination.\nEventually, they leverage this property to build up a statistical test to\ndistinguish indetermination and estimate its efficiency using the Bahadur's\nslope.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:56:14 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Bertrand", "Pierre", "", "LPSM"], ["Broniatowski", "Michel", "", "LPSM"], ["Marcotorchino", "Jean-Fran\u00e7ois", ""]]}, {"id": "2105.01412", "submitter": "Siegfried H\\\"ormann", "authors": "Siegfried H\\\"ormann, Thomas Kuenzer, and Gregory Rice", "title": "Estimating the conditional distribution in functional regression\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of consistently estimating the conditional\ndistribution $P(Y \\in A |X)$ of a functional data object $Y=(Y(t): t\\in[0,1])$\ngiven covariates $X$ in a general space, assuming that $Y$ and $X$ are related\nby a functional linear regression model. Two natural estimation methods are\nproposed, based on either bootstrapping the estimated model residuals, or\nfitting functional parametric models to the model residuals and estimating $P(Y\n\\in A |X)$ via simulation. Whether either of these methods lead to consistent\nestimation depends on the consistency properties of the regression operator\nestimator, and the space within which $Y$ is viewed. We show that under general\nconsistency conditions on the regression operator estimator, which hold for\ncertain functional principal component based estimators, consistent estimation\nof the conditional distribution can be achieved, both when $Y$ is an element of\na separable Hilbert space, and when $Y$ is an element of the Banach space of\ncontinuous functions. The latter results imply that sets $A$ that specify path\nproperties of $Y$, which are of interest in applications, can be considered.\nThe proposed methods are studied in several simulation experiments, and data\nanalyses of electricity price and pollution curves.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 10:54:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Kuenzer", "Thomas", ""], ["Rice", "Gregory", ""]]}, {"id": "2105.01422", "submitter": "Kinga Tikosi", "authors": "Mikl\\'os R\\'asonyi and Kinga Tikosi", "title": "On the stability of the stochastic gradient Langevin algorithm with\n  dependent data stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove, under mild conditions, that the stochastic gradient Langevin\ndynamics converges to a limiting law as time tends to infinity, even in the\ncase where the driving data sequence is dependent.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 11:16:52 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["R\u00e1sonyi", "Mikl\u00f3s", ""], ["Tikosi", "Kinga", ""]]}, {"id": "2105.01723", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho and Vu Dinh", "title": "Convergence of maximum likelihood supertree reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supertree methods are tree reconstruction techniques that combine several\nsmaller gene trees (possibly on different sets of species) to build a larger\nspecies tree. The question of interest is whether the reconstructed supertree\nconverges to the true species tree as the number of gene trees increases (that\nis, the consistency of supertree methods). In this paper, we are particularly\ninterested in the convergence rate of the maximum likelihood supertree.\nPrevious studies on the maximum likelihood supertree approach often formulate\nthe question of interest as a discrete problem and focus on reconstructing the\ncorrect topology of the species tree. Aiming to reconstruct both the topology\nand the branch lengths of the species tree, we propose an analytic approach for\nanalyzing the convergence of the maximum likelihood supertree method.\nSpecifically, we consider each tree as one point of a metric space and prove\nthat the distance between the maximum likelihood supertree and the species tree\nconverges to zero at a polynomial rate under some mild conditions. We further\nverify these conditions for the popular exponential error model of gene trees.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:44:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Dinh", "Vu", ""]]}, {"id": "2105.01769", "submitter": "Chengcheng Li", "authors": "Yunxiao Chen, Chengcheng Li, Gongjun Xu", "title": "A Note on Statistical Inference for Noisy Incomplete 1-Bit Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical inference for noisy incomplete 1-bit matrix.\nInstead of observing a subset of real-valued entries of a matrix M, we only\nhave one binary (1-bit) measurement for each entry in this subset, where the\nbinary measurement follows a Bernoulli distribution whose success probability\nis determined by the value of the entry. Despite the importance of uncertainty\nquantification to matrix completion, most of the categorical matrix completion\nliterature focus on point estimation and prediction. This paper moves one step\nfurther towards the statistical inference for 1-bit matrix completion. Under a\npopular nonlinear factor analysis model, we obtain a point estimator and derive\nits asymptotic distribution for any linear form of M and latent factor scores.\nMoreover, our analysis adopts a flexible missing-entry design that does not\nrequire a random sampling scheme as required by most of the existing asymptotic\nresults for matrix completion. The proposed estimator is statistically\nefficient and optimal, in the sense that the Cramer-Rao lower bound is achieved\nasymptotically for the model parameters. Two applications are considered,\nincluding (1) linking two forms of an educational test and (2) linking the roll\ncall voting records from multiple years in the United States senate. The first\napplication enables the comparison between examinees who took different test\nforms, and the second application allows us to compare the\nliberal-conservativeness of senators who did not serve in the senate at the\nsame time.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:23:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Chengcheng", ""], ["Xu", "Gongjun", ""]]}, {"id": "2105.01783", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Lexin Li, Hao Helen Zhang, and Miaoyan Wang", "title": "Nonparametric Trace Regression in High Dimensions via Sign Series\n  Representation", "comments": "66 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of matrix-valued data has recently surged in a range of scientific\nand business applications. Trace regression is a widely used method to model\neffects of matrix predictors and has shown great success in matrix learning.\nHowever, nearly all existing trace regression solutions rely on two\nassumptions: (i) a known functional form of the conditional mean, and (ii) a\nglobal low-rank structure in the entire range of the regression function, both\nof which may be violated in practice. In this article, we relax these\nassumptions by developing a general framework for nonparametric trace\nregression models via structured sign series representations of high\ndimensional functions. The new model embraces both linear and nonlinear trace\neffects, and enjoys rank invariance to order-preserving transformations of the\nresponse. In the context of matrix completion, our framework leads to a\nsubstantially richer model based on what we coin as the \"sign rank\" of a\nmatrix. We show that the sign series can be statistically characterized by\nweighted classification tasks. Based on this connection, we propose a learning\nreduction approach to learn the regression model via a series of classifiers,\nand develop a parallelable computation algorithm to implement sign series\naggregations. We establish the excess risk bounds, estimation error rates, and\nsample complexities. Our proposal provides a broad nonparametric paradigm to\nmany important matrix learning problems, including matrix regression, matrix\ncompletion, multi-task learning, and compressed sensing. We demonstrate the\nadvantages of our method through simulations and two applications, one on brain\nconnectivity study and the other on high-rank image completion.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 22:20:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lee", "Chanwoo", ""], ["Li", "Lexin", ""], ["Zhang", "Hao Helen", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2105.01856", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne and Karl Wimmer", "title": "Identity testing under label mismatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing whether the observed data conforms to a purported model (probability\ndistribution) is a basic and fundamental statistical task, and one that is by\nnow well understood. However, the standard formulation, identity testing, fails\nto capture many settings of interest; in this work, we focus on one such\nnatural setting, identity testing under promise of permutation. In this\nsetting, the unknown distribution is assumed to be equal to the purported one,\nup to a relabeling (permutation) of the model: however, due to a systematic\nerror in the reporting of the data, this relabeling may not be the identity.\nThe goal is then to test identity under this assumption: equivalently, whether\nthis systematic labeling error led to a data distribution statistically far\nfrom the reference model.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 03:57:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Wimmer", "Karl", ""]]}, {"id": "2105.01874", "submitter": "Yunhua Xiang", "authors": "Yunhua Xiang, Tianyu Zhang, Xu Wang, Ali Shojaie, Noah Simon", "title": "On the Optimality of Nuclear-norm-based Matrix Completion for Problems\n  with Smooth Non-linear Structure", "comments": "47 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Originally developed for imputing missing entries in low rank, or\napproximately low rank matrices, matrix completion has proven widely effective\nin many problems where there is no reason to assume low-dimensional linear\nstructure in the underlying matrix, as would be imposed by rank constraints. In\nthis manuscript, we build some theoretical intuition for this behavior. We\nconsider matrices which are not necessarily low-rank, but lie in a\nlow-dimensional non-linear manifold. We show that nuclear-norm penalization is\nstill effective for recovering these matrices when observations are missing\ncompletely at random. In particular, we give upper bounds on the rate of\nconvergence as a function of the number of rows, columns, and observed entries\nin the matrix, as well as the smoothness and dimension of the non-linear\nembedding. We additionally give a minimax lower bound: This lower bound agrees\nwith our upper bound (up to a logarithmic factor), which shows that\nnuclear-norm penalization is (up to log terms) minimax rate optimal for these\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:34:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Xiang", "Yunhua", ""], ["Zhang", "Tianyu", ""], ["Wang", "Xu", ""], ["Shojaie", "Ali", ""], ["Simon", "Noah", ""]]}, {"id": "2105.01962", "submitter": "Marco Tarsia", "authors": "Marco Tarsia, Antonietta Mira and Daniele Cassani", "title": "On the mathematical axiomatization of approximate Bayesian computation.\n  A robust set for estimating mechanistic network models through optimal\n  transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We research relations between optimal transport theory (OTT) and approximate\nBayesian computation (ABC) possibly connected to relevant metrics defined on\nprobability measures. Those of ABC are computational methods based on Bayesian\nstatistics and applicable to a given generative model to estimate its a\nposteriori distribution in case the likelihood function is intractable. The\nidea is therefore to simulate sets of synthetic data from the model with\nrespect to assigned parameters and, rather than comparing prospects of these\ndata with the corresponding observed values as typically ABC requires, to\nemploy just a distance between a chosen distribution associated to the\nsynthetic data and another of the observed values. Our focus lies in\ntheoretical and methodological aspects, although there would exist a remarkable\npart of algorithmic implementation, and more precisely issues regarding\nmathematical foundation and asymptotic properties are carefully analysed,\ninspired by an in-depth study of what is then our main bibliographic reference,\nthat is Bernton et al. (2019), carrying out what follows: a rigorous\nformulation of the set-up for the ABC rejection algorithm, also to regain a\ntransparent and general result of convergence as the ABC threshold goes to zero\nwhereas the number n of samples from the prior stays fixed; general technical\nproposals about distances leaning on OTT; weak assumptions which lead to lower\nbounds for small values of threshold and as n goes to infinity, ultimately\nshowing a reasonable possibility of lack of concentration which is contrary to\nwhat is proposed in Bernton et al. (2019) itself.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:16:34 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tarsia", "Marco", ""], ["Mira", "Antonietta", ""], ["Cassani", "Daniele", ""]]}, {"id": "2105.02071", "submitter": "Xavier de Luna", "authors": "Niloofar Moosavi, Jenny H\\\"aggstr\\\"om and Xavier de Luna", "title": "The costs and benefits of uniformly valid causal inference with\n  high-dimensional nuisance parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Important advances have recently been achieved in developing procedures\nyielding uniformly valid inference for a low dimensional causal parameter when\nhigh-dimensional nuisance models must be estimated. In this paper, we review\nthe literature on uniformly valid causal inference and discuss the costs and\nbenefits of using uniformly valid inference procedures. Naive estimation\nstrategies based on regularisation, machine learning, or a preliminary model\nselection stage for the nuisance models have finite sample distributions which\nare badly approximated by their asymptotic distributions. To solve this serious\nproblem, estimators which converge uniformly in distribution over a class of\ndata generating mechanisms have been proposed in the literature. In order to\nobtain uniformly valid results in high-dimensional situations, sparsity\nconditions for the nuisance models need typically to be made, although a double\nrobustness property holds, whereby if one of the nuisance model is more sparse,\nthe other nuisance model is allowed to be less sparse. While uniformly valid\ninference is a highly desirable property, uniformly valid procedures pay a high\nprice in terms of inflated variability. Our discussion of this dilemma is\nillustrated by the study of a double-selection outcome regression estimator,\nwhich we show is uniformly asymptotically unbiased, but is less variable than\nuniformly valid estimators in the numerical experiments conducted.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:13:41 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Moosavi", "Niloofar", ""], ["H\u00e4ggstr\u00f6m", "Jenny", ""], ["de Luna", "Xavier", ""]]}, {"id": "2105.02073", "submitter": "Thomas Staudt", "authors": "Thomas Giacomo Nies, Thomas Staudt, Axel Munk", "title": "Transport Dependency: Optimal Transport Based Dependency Measures", "comments": "44 pages main text (14 figures), 73 pages in total (48 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding meaningful ways to determine the dependency between two random\nvariables $\\xi$ and $\\zeta$ is a timeless statistical endeavor with vast\npractical relevance. In recent years, several concepts that aim to extend\nclassical means (such as the Pearson correlation or rank-based coefficients\nlike Spearman's $\\rho$) to more general spaces have been introduced and\npopularized, a well-known example being the distance correlation. In this\narticle, we propose and study an alternative framework for measuring\nstatistical dependency, the transport dependency $\\tau \\ge 0$, which relies on\nthe notion of optimal transport and is applicable in general Polish spaces. It\ncan be estimated consistently via the corresponding empirical measure, is\nversatile and adaptable to various scenarios by proper choices of the cost\nfunction. Notably, statistical independence is characterized by $\\tau = 0$,\nwhile large values of $\\tau$ indicate highly regular relations between $\\xi$\nand $\\zeta$. Indeed, for suitable base costs, $\\tau$ is maximized if and only\nif $\\zeta$ can be expressed as 1-Lipschitz function of $\\xi$ or vice versa.\nBased on sharp upper bounds, we exploit this characterization and define three\ndistinct dependency coefficients (a-c) with values in $[0, 1]$, each of which\nemphasizes different functional relations. These transport correlations attain\nthe value $1$ if and only if $\\zeta = \\varphi(\\xi)$, where $\\varphi$ is a) a\nLipschitz function, b) a measurable function, c) a multiple of an isometry. The\nproperties of coefficient c) make it comparable to the distance correlation,\nwhile coefficient b) is a limit case of a) that was recently studied\nindependently by Wiesel (2021). Numerical results suggest that the transport\ndependency is a robust quantity that efficiently discerns structure from noise\nin simple settings, often out-performing other commonly applied coefficients of\ndependency.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:19:28 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 15:22:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Nies", "Thomas Giacomo", ""], ["Staudt", "Thomas", ""], ["Munk", "Axel", ""]]}, {"id": "2105.02083", "submitter": "Matthias L\\\"offler", "authors": "Geoffrey Chinot, Felix Kuchelmeister, Matthias L\\\"offler and Sara van\n  de Geer", "title": "AdaBoost and robust one-bit compressed sensing", "comments": "29 pages, 4 figures, code available at\n  https://github.com/Felix-127/Adaboost-and-robust-one-bit-compressed-sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies binary classification in robust one-bit compressed sensing\nwith adversarial errors. It is assumed that the model is overparameterized and\nthat the parameter of interest is effectively sparse. AdaBoost is considered,\nand, through its relation to the max-$\\ell_1$-margin-classifier, risk bounds\nare derived. In particular, this provides an explanation why interpolating\nadversarial noise can be harmless for classification problems. Simulations\nillustrate the presented theory.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:29:49 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 12:30:10 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 07:35:42 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chinot", "Geoffrey", ""], ["Kuchelmeister", "Felix", ""], ["L\u00f6ffler", "Matthias", ""], ["van de Geer", "Sara", ""]]}, {"id": "2105.02088", "submitter": "Helene Charlotte Rytgaard", "authors": "Helene C. Rytgaard, Thomas A. Gerds and Mark J. van der Laan", "title": "Continuous-time targeted minimum loss-based estimation of\n  intervention-specific mean outcomes", "comments": "27 pages (excluding supplementary material), 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization of the targeted minimum loss-based\nestimation (TMLE) framework to estimation of effects of time-varying\ninterventions in settings where both interventions, covariates, and outcome can\nhappen at subject-specific time-points on an arbitrarily fine time-scale. TMLE\nis a general template for constructing asymptotically linear substitution\nestimators for smooth low-dimensional parameters in infinite-dimensional\nmodels. Existing longitudinal TMLE methods are developed for data where\nobservations are made on a discrete time-grid.\n  We consider a continuous-time counting process model where intensity measures\ntrack the monitoring of subjects, and focus on a low-dimensional target\nparameter defined as the intervention-specific mean outcome at the end of\nfollow-up. To construct our TMLE algorithm for the given statistical estimation\nproblem we derive an expression for the efficient influence curve and represent\nthe target parameter as a functional of intensities and conditional\nexpectations. The high-dimensional nuisance parameters of our model are\nestimated and updated in an iterative manner according to separate targeting\nsteps for the involved intensities and conditional expectations.\n  The resulting estimator solves the efficient influence curve equation. We\nstate a general efficiency theorem and describe a highly adaptive lasso\nestimator for nuisance parameters that allows us to establish asymptotic\nlinearity and efficiency of our estimator under minimal conditions on the\nunderlying statistical model.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:39:14 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Rytgaard", "Helene C.", ""], ["Gerds", "Thomas A.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2105.02164", "submitter": "Lahcen Douge", "authors": "Nour-Eddine Berrahou and Lahcen Douge", "title": "A nonparametric test of independence based on L_1-error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a test of mutual independence between random vectors with\narbitrary dimensions. Our approach is based on the $L_1-$distance between the\njoint density and the product of the marginal densities. We establish the\nasymptotic normal approximation of the corresponding statistic under the null\nhypothesis without assuming any regularity conditions. From a practical point\nof view, we perform numerical studies in order to assess the efficiency of our\nprocedure and compare it to existing independence tests in the literature. For\nmany examples investigated, the proposed test provides good performance\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:21:12 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 09:24:07 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Berrahou", "Nour-Eddine", ""], ["Douge", "Lahcen", ""]]}, {"id": "2105.02180", "submitter": "Oliver Feng", "authors": "Oliver Y. Feng, Ramji Venkataramanan, Cynthia Rush and Richard J.\n  Samworth", "title": "A unifying tutorial on Approximate Message Passing", "comments": "99 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last decade or so, Approximate Message Passing (AMP) algorithms have\nbecome extremely popular in various structured high-dimensional statistical\nproblems. The fact that the origins of these techniques can be traced back to\nnotions of belief propagation in the statistical physics literature lends a\ncertain mystique to the area for many statisticians. Our goal in this work is\nto present the main ideas of AMP from a statistical perspective, to illustrate\nthe power and flexibility of the AMP framework. Along the way, we strengthen\nand unify many of the results in the existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:47:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Feng", "Oliver Y.", ""], ["Venkataramanan", "Ramji", ""], ["Rush", "Cynthia", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2105.02259", "submitter": "Mingao Yuan", "authors": "Mingao Yuan, Zuofeng Shang", "title": "Information Limits for Detecting a Subhypergraph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of recovering a subhypergraph based on an observed\nadjacency tensor corresponding to a uniform hypergraph. The uniform hypergraph\nis assumed to contain a subset of vertices called as subhypergraph. The edges\nrestricted to the subhypergraph are assumed to follow a different probability\ndistribution than other edges. We consider both weak recovery and exact\nrecovery of the subhypergraph, and establish information-theoretic limits in\neach case. Specifically, we establish sharp conditions for the possibility of\nweakly or exactly recovering the subhypergraph from an information-theoretic\npoint of view. These conditions are fundamentally different from their\ncounterparts derived in hypothesis testing literature.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:08:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2105.02471", "submitter": "Meixia Lin", "authors": "Subhroshekhar Ghosh, Meixia Lin, Dongfang Sun", "title": "Estimation and inference of signals via the stochastic geometry of\n  spectrogram level sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.SD eess.AS math.PR math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Spectrograms are fundamental tools in the detection, estimation and analysis\nof signals in the time-frequency analysis paradigm. Signal analysis via\nspectrograms have traditionally explored their peaks, i.e. their maxima,\ncomplemented by a recent interest in their zeros or minima. In particular,\nrecent investigations have demonstrated connections between Gabor spectrograms\nof Gaussian white noise and Gaussian analytic functions (abbrv. GAFs) in\ndifferent geometries. However, the zero sets (or the maxima or minima) of GAFs\nhave a complicated stochastic structure, which makes a direct theoretical\nanalysis of usual spectrogram based techniques via GAFs a difficult\nproposition. These techniques, in turn, largely rely on statistical observables\nfrom the analysis of spatial data, whose distributional properties for\nspectrogram extrema are mostly understood empirically.\n  In this work, we investigate spectrogram analysis via an examination of the\nstochastic, geometric and analytical properties of their level sets. This\nincludes a comparative analysis of relevant spectrogram structures, with vs\nwithout the presence of signals coupled with Gaussian white noise. We obtain\ntheorems demonstrating the efficacy of a spectrogram level sets based approach\nto the detection and estimation of signals, framed in a concrete inferential\nset-up. Exploiting these ideas as theoretical underpinnings, we propose a level\nsets based algorithm for signal analysis that is intrinsic to given spectrogram\ndata. We substantiate the effectiveness of the algorithm by extensive empirical\nstudies, and provide additional theoretical analysis to elucidate some of its\nkey features. Our results also have theoretical implications for spectrogram\nzero based approaches to signal analysis.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:06:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ghosh", "Subhroshekhar", ""], ["Lin", "Meixia", ""], ["Sun", "Dongfang", ""]]}, {"id": "2105.02639", "submitter": "Tom\\'a\\v{s} Gonda", "authors": "Tobias Fritz, Tom\\'a\\v{s} Gonda, Paolo Perrone", "title": "De Finetti's Theorem in Categorical Probability", "comments": "31 pages, 1 figure. v2: More accurate abstract and some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LO math.CT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel proof of de Finetti's Theorem characterizing\npermutation-invariant probability measures of infinite sequences of variables,\nso-called exchangeable measures. The proof is phrased in the language of Markov\ncategories, which provide an abstract categorical framework for probability and\ninformation flow. The diagrammatic and abstract nature of the arguments makes\nthe proof intuitive and easy to follow. We also show how the usual\nmeasure-theoretic version of de Finetti's Theorem for standard Borel spaces is\nan instance of this result.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:19:07 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:56:30 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Fritz", "Tobias", ""], ["Gonda", "Tom\u00e1\u0161", ""], ["Perrone", "Paolo", ""]]}, {"id": "2105.02797", "submitter": "Zhou Fan", "authors": "Zhou Fan and Yihong Wu", "title": "The replica-symmetric free energy for Ising spin glasses with\n  orthogonally invariant couplings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.dis-nn cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the mean-field Ising spin glass model with external field, where the\nrandom symmetric couplings matrix is orthogonally invariant in law. For\nsufficiently high temperature, we prove that the replica-symmetric prediction\nis correct for the first-order limit of the free energy. Our analysis is an\nadaption of a \"conditional quenched equals annealed\" argument used by\nBolthausen to analyze the high-temperature regime of the\nSherrington-Kirkpatrick model. We condition on a sigma-field that is generated\nby the iterates of an Approximate Message Passing algorithm for solving the TAP\nequations in this model, whose rigorous state evolution was recently\nestablished.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:41:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Fan", "Zhou", ""], ["Wu", "Yihong", ""]]}, {"id": "2105.03015", "submitter": "Minoh Jeong", "authors": "Minoh Jeong, Alex Dytso, Martina Cardone", "title": "Retrieving Data Permutations from Noisy Observations: High and Low Noise\n  Asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of recovering the permutation of an\nn-dimensional random vector X observed in Gaussian noise. First, a general\nexpression for the probability of error is derived when a linear decoder (i.e.,\nlinear estimator followed by a sorting operation) is used. The derived\nexpression holds with minimal assumptions on the distribution of X and when the\nnoise has memory. Second, for the case of isotropic noise (i.e., noise with a\ndiagonal scalar covariance matrix), the rates of convergence of the probability\nof error are characterized in the high and low noise regimes. In the low noise\nregime, for every dimension n, the probability of error is shown to behave\nproportionally to {\\sigma}, where {\\sigma} is the noise standard deviation.\nMoreover, the slope is computed exactly for several distributions and it is\nshown to behave quadratically in n. In the high noise regime, for every\ndimension n, the probability of correctness is shown to behave as 1/{\\sigma},\nand the exact expression for the rate of convergence is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 00:22:07 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Jeong", "Minoh", ""], ["Dytso", "Alex", ""], ["Cardone", "Martina", ""]]}, {"id": "2105.03067", "submitter": "Suyash Gupta", "authors": "Suyash Gupta and Dominik Rothenh\\\"ausler", "title": "The $s$-value: evaluating stability with respect to distributional\n  shifts", "comments": "43 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical measures of uncertainty such as $p$-values and confidence\nintervals quantify the uncertainty due to sampling, that is, the uncertainty\ndue to not observing the full population. However, sampling is not the only\nsource of uncertainty. In practice, distributions change between locations and\nacross time. This makes it difficult to gather knowledge that transfers across\ndata sets. We propose a measure of uncertainty or instability that quantifies\nthe distributional instability of a statistical parameter with respect to\nKullback-Leibler divergence, that is, the sensitivity of the parameter under\ngeneral distributional perturbations within a Kullback-Leibler divergence ball.\nIn addition, we propose measures to elucidate the instability of parameters\nwith respect to directional or variable-specific shifts. Measuring instability\nwith respect to directional shifts can be used to detect the type of shifts a\nparameter is sensitive to. We discuss how such knowledge can inform data\ncollection for improved estimation of statistical parameters under shifted\ndistributions. We evaluate the performance of the proposed measure on real data\nand show that it can elucidate the distributional (in-)stability of a parameter\nwith respect to certain shifts and can be used to improve the accuracy of\nestimation under shifted distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:18:12 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 00:39:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gupta", "Suyash", ""], ["Rothenh\u00e4usler", "Dominik", ""]]}, {"id": "2105.03101", "submitter": "Alexander Henzi", "authors": "Alexander Henzi", "title": "Consistent estimation of distribution functions under increasing concave\n  and convex stochastic ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random variable $Y_1$ is said to be smaller than $Y_2$ in the increasing\nconcave stochastic order if $\\mathbb{E}[\\phi(Y_1)] \\leq \\mathbb{E}[\\phi(Y_2)]$\nfor all increasing concave functions $\\phi$ for which the expected values\nexist, and smaller than $Y_2$ in the increasing convex order if\n$\\mathbb{E}[\\psi(Y_1)] \\leq \\mathbb{E}[\\psi(Y_2)]$ for all increasing convex\n$\\psi$. This article develops nonparametric estimators for the conditional\ncumulative distribution functions $F_x(y) = \\mathbb{P}(Y \\leq y \\mid X = x)$ of\na response variable $Y$ given a covariate $X$, solely under the assumption that\nthe conditional distributions are increasing in $x$ in the increasing concave\nor increasing convex order. Uniform consistency and rates of convergence are\nestablished both for the $K$-sample case $X \\in \\{1, \\dots, K\\}$ and for\ncontinuously distributed $X$.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:00:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Henzi", "Alexander", ""]]}, {"id": "2105.03102", "submitter": "Fabio Rapallo", "authors": "Elena Pesce, Fabio Rapallo, Eva Riccomagno, Henry P. Wynn", "title": "Circuit bases for randomisation", "comments": "15 pages; submitted for possible publication on 19/04/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a rich history in medicine, randomisation control trials both simple\nand complex are in increasing use in other areas such as web-based AB testing\nand planning and design decisions. A main objective is to be able to measure\nparameters, and contrasts in particular, while guarding against biases from\nhidden confounders. After careful definitions of classical entities such as\ncontrasts, an algebraic method based on circuits is introduced which gives a\nwide choice of randomisation schemes.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:02:54 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Pesce", "Elena", ""], ["Rapallo", "Fabio", ""], ["Riccomagno", "Eva", ""], ["Wynn", "Henry P.", ""]]}, {"id": "2105.03122", "submitter": "Eddie Aamari", "authors": "Eddie Aamari, Ery Arias-Castro, Cl\\'ement Berenfeld", "title": "From Graph Centrality to Data Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample of points in a Euclidean space, we can define a notion of\ndepth by forming a neighborhood graph and applying a notion of centrality. In\nthe present paper, we focus on the degree, iterates of the H-index, and the\ncoreness, which are all well-known measures of centrality. We study their\nbehaviors when applied to a sample of points drawn i.i.d. from an underlying\ndensity and with a connectivity radius properly chosen. Equivalently, we study\nthese notions of centrality in the context of random neighborhood graphs. We\nshow that, in the large-sample limit and under some standard condition on the\nconnectivity radius, the degree converges to the likelihood depth\n(unsurprisingly), while iterates of the H-index and the coreness converge to\nnew notions of depth.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:50:51 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Aamari", "Eddie", ""], ["Arias-Castro", "Ery", ""], ["Berenfeld", "Cl\u00e9ment", ""]]}, {"id": "2105.03248", "submitter": "David Heckerman", "authors": "Dan Geiger and David Heckerman", "title": "Parameter Priors for Directed Acyclic Graphical Models and the\n  Characterization of Several Probability Distributions", "comments": "This version has improved pointers to the literature. arXiv admin\n  note: substantial text overlap with arXiv:1301.6697", "journal-ref": "The Annals of Statistics, 30: 1412-1440, 2002", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop simple methods for constructing parameter priors for model choice\namong Directed Acyclic Graphical (DAG) models. In particular, we introduce\nseveral assumptions that permit the construction of parameter priors for a\nlarge number of DAG models from a small set of assessments. We then present a\nmethod for directly computing the marginal likelihood of every DAG model given\na random sample with no missing observations. We apply this methodology to\nGaussian DAG models which consist of a recursive set of linear regression\nmodels. We show that the only parameter prior for complete Gaussian DAG models\nthat satisfies our assumptions is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\n$W$ be an $n \\times n$, $n \\ge 3$, positive-definite symmetric matrix of random\nvariables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if\nand only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of\n$\\{W_{12},W_{22}\\}$ for every block partitioning $W_{11},W_{12}, W'_{12},\nW_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart\ndistributions are provided as well.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 18:01:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:44:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "2105.03308", "submitter": "Daniel Rudolf", "authors": "Viacheslav Natarovskii, Daniel Rudolf, Bj\\\"orn Sprungk", "title": "Geometric convergence of elliptical slice sampling", "comments": "13 pages, 2 figures, Accepted in the Proceedings of the 38th\n  International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Bayesian learning, given likelihood function and Gaussian prior, the\nelliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides\na tool for the construction of a Markov chain for approximate sampling of the\nunderlying posterior distribution. Besides of its wide applicability and\nsimplicity its main feature is that no tuning is necessary. Under weak\nregularity assumptions on the posterior density we show that the corresponding\nMarkov chain is geometrically ergodic and therefore yield qualitative\nconvergence guarantees. We illustrate our result for Gaussian posteriors as\nthey appear in Gaussian process regression, as well as in a setting of a\nmulti-modal distribution. Remarkably, our numerical experiments indicate a\ndimension-independent performance of elliptical slice sampling even in\nsituations where our ergodicity result does not apply.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:30 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 07:55:37 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 19:13:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Natarovskii", "Viacheslav", ""], ["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "2105.03425", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Yao Xie", "title": "Kernel MMD Two-Sample Tests for Manifold Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a study of kernel MMD two-sample test statistics in the manifold\nsetting, assuming the high-dimensional observations are close to a\nlow-dimensional manifold. We characterize the property of the test (level and\npower) in relation to the kernel bandwidth, the number of samples, and the\nintrinsic dimensionality of the manifold. Specifically, we show that when data\ndensities are supported on a $d$-dimensional sub-manifold $\\mathcal{M}$\nembedded in an $m$-dimensional space, the kernel MMD two-sample test for data\nsampled from a pair of distributions $(p, q)$ that are H\\\"older with order\n$\\beta$ is consistent and powerful when the number of samples $n$ is greater\nthan $\\delta_2(p,q)^{-2-d/\\beta}$ up to certain constant, where $\\delta_2$ is\nthe squared $\\ell_2$-divergence between two distributions on manifold.\nMoreover, to achieve testing consistency under this scaling of $n$, our theory\nsuggests that the kernel bandwidth $\\gamma$ scales with $n^{-1/(d+2\\beta)}$.\nThese results indicate that the kernel MMD two-sample test does not have a\ncurse-of-dimensionality when the data lie on the low-dimensional manifold. We\ndemonstrate the validity of our theory and the property of the MMD test for\nmanifold data using several numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:56:45 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Xie", "Yao", ""]]}, {"id": "2105.03481", "submitter": "Christophe Ley", "authors": "Andreas Anastasiou, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Bruno\n  Ebner, Robert E. Gaunt, Fatemeh Ghaderinezhad, Jackson Gorham, Arthur\n  Gretton, Christophe Ley, Qiang Liu, Lester Mackey, Chris. J. Oates, Gesine\n  Reinert, Yvik Swan", "title": "Stein's Method Meets Statistics: A Review of Some Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method is a collection of tools for analysing distributional\ncomparisons through the study of a class of linear operators called Stein\noperators. Originally studied in probability, Stein's method has also enabled\nsome important developments in statistics. This early success has led to a high\nresearch activity in this area in recent years. The goal of this survey is to\nbring together some of these developments in theoretical statistics as well as\nin computational statistics and, in doing so, to stimulate further research\ninto the successful field of Stein's method and statistics. The topics we\ndiscuss include: explicit error bounds for asymptotic approximations of\nestimators and test statistics, a measure of prior sensitivity in Bayesian\nstatistics, tools to benchmark and compare sampling methods such as approximate\nMarkov chain Monte Carlo, deterministic alternatives to sampling methods,\ncontrol variate techniques, and goodness-of-fit testing.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:55:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Ebner", "Bruno", ""], ["Gaunt", "Robert E.", ""], ["Ghaderinezhad", "Fatemeh", ""], ["Gorham", "Jackson", ""], ["Gretton", "Arthur", ""], ["Ley", "Christophe", ""], ["Liu", "Qiang", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "2105.03604", "submitter": "Rahul Singh", "authors": "Rahul Singh, Subhajit Dutta and Neeraj Misra", "title": "Some multivariate goodness of fit tests based on data depth", "comments": "19 pages, 1 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the fact that some depth functions characterize certain family of\ndistribution functions, and under some mild conditions, distribution of the\ndepth is continuous, we have constructed several new multivariate goodness of\nfit tests based on existing univariate GoF tests. Since exact computation of\ndepth is difficult, depth is computed with respect to a large random sample\ndrawn from the null distribution. It has been shown that test statistic based\non estimated depth is close to that based on true depth for a large random\nsample from the null distribution. Some two sample tests for scale difference,\nbased on data depth are also discussed. These tests are distribution-free under\nthe null hypothesis. Finite sample properties of the tests are studied through\nseveral numerical examples. A real data example is discussed to illustrate\nusefulness of the proposed tests.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:54:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Singh", "Rahul", ""], ["Dutta", "Subhajit", ""], ["Misra", "Neeraj", ""]]}, {"id": "2105.03810", "submitter": "Eric Auerbach", "authors": "Eric Auerbach and Max Tabord-Meehan", "title": "The Local Approach to Causal Inference under Network Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new unified framework for causal inference when outcomes depend\non how agents are linked in a social or economic network. Such network\ninterference describes a large literature on treatment spillovers, social\ninteractions, social learning, information diffusion, social capital formation,\nand more. Our approach works by first characterizing how an agent is linked in\nthe network using the configuration of other agents and connections nearby as\nmeasured by path distance. The impact of a policy or treatment assignment is\nthen learned by pooling outcome data across similarly configured agents. In the\npaper, we propose a new nonparametric modeling approach and consider two\napplications to causal inference. The first application is to testing policy\nirrelevance/no treatment effects. The second application is to estimating\npolicy effects/treatment response. We conclude by evaluating the finite-sample\nproperties of our estimation and inference procedures via simulation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:27:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:37:06 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Auerbach", "Eric", ""], ["Tabord-Meehan", "Max", ""]]}, {"id": "2105.04134", "submitter": "Daniel Barreiro Ures", "authors": "D. Barreiro-Ures, R. Cao and M. Francisco-Fern\\'andez", "title": "Bagging cross-validated bandwidth selection in nonparametric regression\n  estimation with applications to large-sized samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a well-known and widely used bandwidth selection method\nin nonparametric regression estimation. However, this technique has two\nremarkable drawbacks: (i) the large variability of the selected bandwidths, and\n(ii) the inability to provide results in a reasonable time for very large\nsample sizes. To overcome these problems, bagging cross-validation bandwidths\nare analyzed in this paper. This approach consists in computing the\ncross-validation bandwidths for a finite number of subsamples and then\nrescaling the averaged smoothing parameters to the original sample size. Under\na random-design regression model, asymptotic expressions up to a second-order\nfor the bias and variance of the leave-one-out cross-validation bandwidth for\nthe Nadaraya--Watson estimator are obtained. Subsequently, the asymptotic bias\nand variance and the limit distribution are derived for the bagged\ncross-validation selector. Suitable choices of the number of subsamples and the\nsubsample size lead to an $n^{-1/2}$ rate for the convergence in distribution\nof the bagging cross-validation selector, outperforming the rate $n^{-3/10}$ of\nleave-one-out cross-validation. Several simulations and an illustration on a\nreal dataset related to the COVID-19 pandemic show the behavior of our proposal\nand its better performance, in terms of statistical efficiency and computing\ntime, when compared to leave-one-out cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Barreiro-Ures", "D.", ""], ["Cao", "R.", ""], ["Francisco-Fern\u00e1ndez", "M.", ""]]}, {"id": "2105.04287", "submitter": "Nilanjana Laha", "authors": "Nilanjana Laha", "title": "Adaptive estimation in symmetric location model under log-concavity\n  constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the problem of estimating the center of symmetry $\\theta$ of an\nunknown symmetric density $f$. Although stone (1975), Eden (1970), and Sacks\n(1975) constructed adaptive estimators of $\\theta$ in this model, their\nestimators depend on external tuning parameters. In an effort to reduce the\nburden of tuning parameters, we impose an additional restriction of\nlog-concavity on $f$. We construct truncated one-step estimators which are\nadaptive under the log-concavity assumption. Our simulations suggest that the\nuntruncated version of the one step estimator, which is tuning parameter free,\nis also asymptotically efficient.\n  We also study the maximum likelihood estimator (MLE) of $\\theta$ in the\nshape-restricted model.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:58:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Laha", "Nilanjana", ""]]}, {"id": "2105.04390", "submitter": "Bennet Str\\\"oh", "authors": "Bennet Str\\\"oh", "title": "Statistical inference for continuous-time locally stationary processes\n  using stationary approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish asymptotic properties of $M$-estimators, defined in terms of a\ncontrast function and observations from a continuous-time locally stationary\nprocess. Using the stationary approximation of the sequence, $\\theta$-weak\ndependence, and hereditary properties, we give sufficient conditions on the\ncontrast function that ensure consistency and asymptotic normality of the\n$M$-estimator. As an example, we obtain consistency and asymptotic normality of\na localized least squares estimator for observations from a sequence of\ntime-varying L\\'evy-driven Ornstein-Uhlenbeck processes. Furthermore, for a\nsequence of time-varying L\\'evy-driven state space models, we show consistency\nof a localized Whittle estimator and an $M$-estimator that is based on a quasi\nmaximum likelihood contrast. Simulation studies show the applicability of the\nestimation procedures.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:09:54 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Str\u00f6h", "Bennet", ""]]}, {"id": "2105.04685", "submitter": "Kavita Ramanan", "authors": "Steven Soojin Kim and Kavita Ramanan", "title": "Large deviation principles induced by the Stiefel manifold, and random\n  multi-dimensional projections", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an $n$-dimensional random vector $X^{(n)}$ , for $k < n$, consider its\n$k$-dimensional projection $\\mathbf{a}_{n,k}X^{(n)}$, where $\\mathbf{a}_{n,k}$\nis an $n \\times k$-dimensional matrix belonging to the Stiefel manifold\n$\\mathbb{V}_{n,k}$ of orthonormal $k$-frames in $\\mathbb{R}^n$. For a class of\nsequences $\\{X^{(n)}\\}$ that includes the uniform distributions on scaled\n$\\ell_p^n$ balls, $p \\in (1,\\infty]$, and product measures with sufficiently\nlight tails, it is shown that the sequence of projected vectors\n$\\{\\mathbf{a}_{n,k}^\\intercal X^{(n)}\\}$ satisfies a large deviation principle\nwhenever the empirical measures of the rows of $\\sqrt{n} \\mathbf{a}_{n,k}$\nconverge, as $n \\rightarrow \\infty$, to a probability measure on\n$\\mathbb{R}^k$. In particular, when $\\mathbf{A}_{n,k}$ is a random matrix drawn\nfrom the Haar measure on $\\mathbb{V}_{n,k}$, this is shown to imply a large\ndeviation principle for the sequence of random projections\n$\\{\\mathbf{A}_{n,k}^\\intercal X^{(n)}\\}$ in the quenched sense (that is,\nconditioned on almost sure realizations of $\\{\\mathbf{A}_{n,k}\\}$). Moreover, a\nvariational formula is obtained for the rate function of the large deviation\nprinciple for the annealed projections $\\{\\mathbf{A}_{n,k}^\\intercal\nX^{(n)}\\}$, which is expressed in terms of a family of quenched rate functions\nand a modified entropy term. A key step in this analysis is a large deviation\nprinciple for the sequence of empirical measures of rows of $\\sqrt{n}\n\\mathbf{A}_{n,k}$, which may be of independent interest. The study of\nmulti-dimensional random projections of high-dimensional measures is of\ninterest in asymptotic functional analysis, convex geometry and statistics.\nPrior results on quenched large deviations for random projections of $\\ell_p^n$\nballs have been essentially restricted to the one-dimensional setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:54:32 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kim", "Steven Soojin", ""], ["Ramanan", "Kavita", ""]]}, {"id": "2105.04754", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Barak Sober", "title": "Non-Parametric Estimation of Manifolds from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common observation in data-driven applications is that high dimensional\ndata has a low intrinsic dimension, at least locally. In this work, we consider\nthe problem of estimating a $d$ dimensional sub-manifold of $\\mathbb{R}^D$ from\na finite set of noisy samples. Assuming that the data was sampled uniformly\nfrom a tubular neighborhood of $\\mathcal{M}\\in \\mathcal{C}^k$, a compact\nmanifold without boundary, we present an algorithm that takes a point $r$ from\nthe tubular neighborhood and outputs $\\hat p_n\\in \\mathbb{R}^D$, and\n$\\widehat{T_{\\hat p_n}\\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.\nWe prove that as the number of samples $n\\to\\infty$ the point $\\hat p_n$\nconverges to $p\\in \\mathcal{M}$ and $\\widehat{T_{\\hat p_n}\\mathcal{M}}$\nconverges to $T_p\\mathcal{M}$ (the tangent space at that point) with high\nprobability. Furthermore, we show that the estimation yields asymptotic rates\nof convergence of $n^{-\\frac{k}{2k + d}}$ for the point estimation and\n$n^{-\\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates\nare known to be optimal for the case of function estimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 02:29:33 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:53:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Sober", "Barak", ""]]}, {"id": "2105.04770", "submitter": "Qiaosheng Zhang", "authors": "Qiaosheng Zhang, Vincent Y. F. Tan", "title": "Exact Recovery in the General Hypergraph Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper investigates fundamental limits of exact recovery in the general\nd-uniform hypergraph stochastic block model (d-HSBM), wherein n nodes are\npartitioned into k disjoint communities with relative sizes (p1,..., pk). Each\nsubset of nodes with cardinality d is generated independently as an order-d\nhyperedge with a certain probability that depends on the ground-truth\ncommunities that the d nodes belong to. The goal is to exactly recover the k\nhidden communities based on the observed hypergraph. We show that there exists\na sharp threshold such that exact recovery is achievable above the threshold\nand impossible below the threshold (apart from a small regime of parameters\nthat will be specified precisely). This threshold is represented in terms of a\nquantity which we term as the generalized Chernoff-Hellinger divergence between\ncommunities. Our result for this general model recovers prior results for the\nstandard SBM and d-HSBM with two symmetric communities as special cases. En\nroute to proving our achievability results, we develop a polynomial-time\ntwo-stage algorithm that meets the threshold. The first stage adopts a certain\nhypergraph spectral clustering method to obtain a coarse estimate of\ncommunities, and the second stage refines each node individually via local\nrefinement steps to ensure exact recovery.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:39:08 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Zhang", "Qiaosheng", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2105.04852", "submitter": "Theo Lacombe", "authors": "Vincent Divol (DATASHAPE, LMO), Th\\'eo Lacombe (DATASHAPE)", "title": "Estimation and Quantization of Expected Persistence Diagrams", "comments": null, "journal-ref": "International Conference on Machine Learning, Jul 2021, Virtual\n  Conference, France", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams (PDs) are the most common descriptors used to encode the\ntopology of structured data appearing in challenging learning tasks; think e.g.\nof graphs, time series or point clouds sampled close to a manifold. Given\nrandom objects and the corresponding distribution of PDs, one may want to build\na statistical summary-such as a mean-of these random PDs, which is however not\na trivial task as the natural geometry of the space of PDs is not linear. In\nthis article, we study two such summaries, the Expected Persistence Diagram\n(EPD), and its quantization. The EPD is a measure supported on R 2 , which may\nbe approximated by its empirical counterpart. We prove that this estimator is\noptimal from a minimax standpoint on a large class of models with a parametric\nrate of convergence. The empirical EPD is simple and efficient to compute, but\npossibly has a very large support, hindering its use in practice. To overcome\nthis issue, we propose an algorithm to compute a quantization of the empirical\nEPD, a measure with small support which is shown to approximate with\nnear-optimal rates a quantization of the theoretical EPD.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 08:12:18 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Divol", "Vincent", "", "DATASHAPE, LMO"], ["Lacombe", "Th\u00e9o", "", "DATASHAPE"]]}, {"id": "2105.05106", "submitter": "Martina Cardone", "authors": "Alex Dytso and Martina Cardone", "title": "A General Derivative Identity for the Conditional Expectation with Focus\n  on the Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a pair of random vectors $(\\mathbf{X},\\mathbf{Y}) $ and the\nconditional expectation operator\n$\\mathbb{E}[\\mathbf{X}|\\mathbf{Y}=\\mathbf{y}]$. This work studies analytic\nproperties of the conditional expectation by characterizing various derivative\nidentities. The paper consists of two parts. In the first part of the paper, a\ngeneral derivative identity for the conditional expectation is derived.\nSpecifically, for the Markov chain $\\mathbf{U} \\leftrightarrow \\mathbf{X}\n\\leftrightarrow \\mathbf{Y}$, a compact expression for the Jacobian matrix of\n$\\mathbb{E}[\\mathbf{U}|\\mathbf{Y}=\\mathbf{y}]$ is derived. In the second part\nof the paper, the main identity is specialized to the exponential family.\nMoreover, via various choices of the random vector $\\mathbf{U}$, the new\nidentity is used to recover and generalize several known identities and derive\nsome new ones. As a first example, a connection between the Jacobian of $\n\\mathbb{E}[\\mathbf{X}|\\mathbf{Y}=\\mathbf{y}]$ and the conditional variance is\nestablished. As a second example, a recursive expression between higher order\nconditional expectations is found, which is shown to lead to a generalization\nof the Tweedy's identity. Finally, as a third example, it is shown that the\n$k$-th order derivative of the conditional expectation is proportional to the\n$(k+1)$-th order conditional cumulant.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 15:02:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Dytso", "Alex", ""], ["Cardone", "Martina", ""]]}, {"id": "2105.05228", "submitter": "Huy Tuan Pham", "authors": "Huy Tuan Pham, Phan-Minh Nguyen", "title": "Global Convergence of Three-layer Neural Networks in the Mean Field\n  Regime", "comments": "Appear in ICLR 2021. This is the conference version of\n  arXiv:2001.11443 (which contains treatment of the multilayer neural nets and\n  their global convergence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mean field regime, neural networks are appropriately scaled so that as\nthe width tends to infinity, the learning dynamics tends to a nonlinear and\nnontrivial dynamical limit, known as the mean field limit. This lends a way to\nstudy large-width neural networks via analyzing the mean field limit. Recent\nworks have successfully applied such analysis to two-layer networks and\nprovided global convergence guarantees. The extension to multilayer ones\nhowever has been a highly challenging puzzle, and little is known about the\noptimization efficiency in the mean field regime when there are more than two\nlayers.\n  In this work, we prove a global convergence result for unregularized\nfeedforward three-layer networks in the mean field regime. We first develop a\nrigorous framework to establish the mean field limit of three-layer networks\nunder stochastic gradient descent training. To that end, we propose the idea of\na \\textit{neuronal embedding}, which comprises of a fixed probability space\nthat encapsulates neural networks of arbitrary sizes. The identified mean field\nlimit is then used to prove a global convergence guarantee under suitable\nregularity and convergence mode assumptions, which -- unlike previous works on\ntwo-layer networks -- does not rely critically on convexity. Underlying the\nresult is a universal approximation property, natural of neural networks, which\nimportantly is shown to hold at \\textit{any} finite training time (not\nnecessarily at convergence) via an algebraic topology argument.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:45:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Pham", "Huy Tuan", ""], ["Nguyen", "Phan-Minh", ""]]}, {"id": "2105.05299", "submitter": "Wing Hung Wong", "authors": "Wing Hung Wong", "title": "An integral equation for the identification of causal effects in\n  nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When the causal relationship between X and Y is specified by a structural\nequation, the causal effect of X on Y is the expected rate of change of Y with\nrespect to changes in X, when all other variables are kept fixed. This causal\neffect is not identifiable from the distribution of (X,Y). We give conditions\nunder which this causal effect is identified as the solution of an integral\nequation based on the distributions of (X,Z) and (Y,Z), where Z is an\ninstrumental variable.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:51:03 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wong", "Wing Hung", ""]]}, {"id": "2105.05307", "submitter": "Prathapasinghe Dharmawansa", "authors": "Pasan Dissanayake, Prathapasinghe Dharmawansa, and Yang Chen", "title": "Distribution of the Scaled Condition Number of Single-spiked Complex\n  Wishart Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT eess.SP math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Let $\\mathbf{X}\\in\\mathbb{C}^{m\\times n}$ ($m\\geq n$) be a random matrix with\nindependent rows each distributed as complex multivariate Gaussian with zero\nmean and {\\it single-spiked} covariance matrix $\\mathbf{I}_n+ \\eta\n\\mathbf{u}\\mathbf{u}^*$, where $\\mathbf{I}_n$ is the $n\\times n$ identity\nmatrix, $\\mathbf{u}\\in\\mathbb{C}^{n\\times n}$ is an arbitrary vector with a\nunit Euclidean norm, $\\eta\\geq 0$ is a non-random parameter, and $(\\cdot)^*$\nrepresents conjugate-transpose. This paper investigates the distribution of the\nrandom quantity $\\kappa_{\\text{SC}}^2(\\mathbf{X})=\\sum_{k=1}^n\n\\lambda_k/\\lambda_1$, where $0<\\lambda_1<\\lambda_2<\\ldots<\\lambda_n<\\infty$ are\nthe ordered eigenvalues of $\\mathbf{X}^*\\mathbf{X}$ (i.e., single-spiked\nWishart matrix). This random quantity is intimately related to the so called\n{\\it scaled condition number} or the Demmel condition number (i.e.,\n$\\kappa_{\\text{SC}}(\\mathbf{X})$) and the minimum eigenvalue of the fixed trace\nWishart-Laguerre ensemble (i.e., $\\kappa_{\\text{SC}}^{-2}(\\mathbf{X})$). In\nparticular, we use an orthogonal polynomial approach to derive an exact\nexpression for the probability density function of\n$\\kappa_{\\text{SC}}^2(\\mathbf{X})$ which is amenable to asymptotic analysis as\nmatrix dimensions grow large. Our asymptotic results reveal that, as\n$m,n\\to\\infty$ such that $m-n$ is fixed and when $\\eta$ scales on the order of\n$1/n$, $\\kappa_{\\text{SC}}^2(\\mathbf{X})$ scales on the order of $n^3$. In this\nrespect we establish simple closed-form expressions for the limiting\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:09:11 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Dissanayake", "Pasan", ""], ["Dharmawansa", "Prathapasinghe", ""], ["Chen", "Yang", ""]]}, {"id": "2105.05341", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang and Fushing Hsieh", "title": "An Encoding Approach for Stable Change Point Detection", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Without imposing prior distributional knowledge underlying multivariate time\nseries of interest, we propose a nonparametric change-point detection approach\nto estimate the number of change points and their locations along the temporal\naxis. We develop a structural subsampling procedure such that the observations\nare encoded into multiple sequences of Bernoulli variables. A maximum\nlikelihood approach in conjunction with a newly developed searching algorithm\nis implemented to detect change points on each Bernoulli process separately.\nThen, aggregation statistics are proposed to collectively synthesize\nchange-point results from all individual univariate time series into consistent\nand stable location estimations. We also study a weighting strategy to measure\nthe degree of relevance for different subsampled groups. Simulation studies are\nconducted and shown that the proposed change-point methodology for multivariate\ntime series has favorable performance comparing with currently popular\nnonparametric methods under various settings with different degrees of\ncomplexity. Real data analyses are finally performed on categorical, ordinal,\nand continuous time series taken from fields of genetics, climate, and finance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:00:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Xiaodong", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2105.05373", "submitter": "Yue You", "authors": "Yue You, Mark van der Laan, Philip Collender, Qu Cheng, Alan Hubbard,\n  Nicholas P Jewell, Zhiyue Tom Hu, Robin Mejia and Justin Remais", "title": "Estimation of population size based on capture recapture designs and\n  evaluation of the estimation reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a modern method to estimate population size based on\ncapture-recapture designs of K samples. The observed data is formulated as a\nsample of n i.i.d. K-dimensional vectors of binary indicators, where the k-th\ncomponent of each vector indicates the subject being caught by the k-th sample,\nsuch that only subjects with nonzero capture vectors are observed. The target\nquantity is the unconditional probability of the vector being nonzero across\nboth observed and unobserved subjects. We cover models assuming a single\nconstraint (identification assumption) on the K-dimensional distribution such\nthat the target quantity is identified and the statistical model is\nunrestricted. We present solutions for linear and non-linear constraints\ncommonly assumed to identify capture-recapture models, including no K-way\ninteraction in linear and log-linear models, independence or conditional\nindependence. We demonstrate that the choice of constraint has a dramatic\nimpact on the value of the estimand, showing that it is crucial that the\nconstraint is known to hold by design. For the commonly assumed constraint of\nno K-way interaction in a log-linear model, the statistical target parameter is\nonly defined when each of the $2^K - 1$ observable capture patterns is present,\nand therefore suffers from the curse of dimensionality. We propose a targeted\nMLE based on undersmoothed lasso model to smooth across the cells while\ntargeting the fit towards the single valued target parameter of interest. For\neach identification assumption, we provide simulated inference and confidence\nintervals to assess the performance on the estimator under correct and\nincorrect identifying assumptions. We apply the proposed method, alongside\nexisting estimators, to estimate prevalence of a parasitic infection using\nmulti-source surveillance data from a region in southwestern China, under the\nfour identification assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:12:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["You", "Yue", ""], ["van der Laan", "Mark", ""], ["Collender", "Philip", ""], ["Cheng", "Qu", ""], ["Hubbard", "Alan", ""], ["Jewell", "Nicholas P", ""], ["Hu", "Zhiyue Tom", ""], ["Mejia", "Robin", ""], ["Remais", "Justin", ""]]}, {"id": "2105.05523", "submitter": "Martin Bladt", "authors": "Martin Bladt, Hansjoerg Albrecher, Jan Beirlant", "title": "Trimmed extreme value estimators for censored heavy-tailed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of the extreme value index and extreme quantiles for\nheavy-tailed data that are right-censored. We study a general procedure of\nremoving low importance observations in tail estimators. This trimming\nprocedure is applied to the state-of-the-art estimators for randomly\nright-censored tail estimators. Through an averaging procedure over the amount\nof trimming we derive new kernel type estimators. Extensive simulation suggests\nthat one of the new considered kernels leads to a highly competitive estimator\nagainst virtually any other available alternative in this framework. Moreover,\nwe propose an adaptive selection method for the amount of top data used in\nestimation based on the trimming procedure minimizing the asymptotic mean\nsquared error. We also provide an illustration of this approach to simulated as\nwell as to real-world MTPL insurance data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:56:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Bladt", "Martin", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "2105.05555", "submitter": "Honghao Lin", "authors": "Yu Cheng and Honghao Lin", "title": "Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Bayesian networks where an\n$\\epsilon$-fraction of the samples are adversarially corrupted. We focus on the\nfully-observable case where the underlying graph structure is known. In this\nwork, we present the first nearly-linear time algorithm for this problem with a\ndimension-independent error guarantee. Previous robust algorithms with\ncomparable error guarantees are slower by at least a factor of $(d/\\epsilon)$,\nwhere $d$ is the number of variables in the Bayesian network and $\\epsilon$ is\nthe fraction of corrupted samples.\n  Our algorithm and analysis are considerably simpler than those in previous\nwork. We achieve this by establishing a direct connection between robust\nlearning of Bayesian networks and robust mean estimation. As a subroutine in\nour algorithm, we develop a robust mean estimation algorithm whose runtime is\nnearly-linear in the number of nonzeros in the input samples, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:11:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cheng", "Yu", ""], ["Lin", "Honghao", ""]]}, {"id": "2105.05842", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Lester Mackey", "title": "Kernel Thinning", "comments": "Accepted for presentation as an extended abstract at the Conference\n  on Learning Theory (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:56:42 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:59:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 20:57:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Mackey", "Lester", ""]]}, {"id": "2105.05907", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "A new characterization of discrete decomposable models", "comments": "13 pages, 2 figures. The original version of paper 2012.03593 as been\n  broken into two papers. This is one of them", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposable graphical models, also known as perfect DAG models, play a\nfundamental role in standard approaches to probabilistic inference via graph\nrepresentations in modern machine learning and statistics. However, such models\nare limited by the assumption that the data-generating distribution does not\nentail strictly context-specific conditional independence relations. The family\nof staged tree models generalizes DAG models so as to accommodate\ncontext-specific knowledge. We provide a new characterization of perfect\ndiscrete DAG models in terms of their staged tree representations. This\ncharacterization identifies the family of balanced staged trees as the natural\ngeneralization of discrete decomposable models to the context-specific setting.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:53:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2105.05963", "submitter": "Souvik Ray", "authors": "Souvik Ray, Subrata Pal, Sumit Kumar Kar, Ayanendranath Basu", "title": "Characterizing Logarithmic Bregman Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum divergence procedures based on the density power divergence and the\nlogarithmic density power divergence have been extremely popular and successful\nin generating inference procedures which combine a high degree of model\nefficiency with strong outlier stability. Such procedures are always preferable\nin practical situations over procedures which achieve their robustness at a\nmajor cost of efficiency or are highly efficient but have poor robustness\nproperties. The density power divergence (DPD) family of Basu et al.(1998) and\nthe logarithmic density power divergence (LDPD) family of Jones et al.(2001)\nprovide flexible classes of divergences where the adjustment between efficiency\nand robustness is controlled by a single, real, non-negative parameter. The\nusefulness of these two families of divergences in statistical inference makes\nit meaningful to search for other related families of divergences in the same\nspirit. The DPD family is a member of the class of Bregman divergences, and the\nLDPD family is obtained by log transformations of the different segments of the\ndivergences within the DPD family. Both the DPD and LDPD families lead to the\nKullback-Leibler divergence in the limiting case as the tuning parameter\n$\\alpha \\rightarrow 0$. In this paper we study this relation in detail, and\ndemonstrate that such log transformations can only be meaningful in the context\nof the DPD (or the convex generating function of the DPD) within the general\nfold of Bregman divergences, giving us a limit to the extent to which the\nsearch for useful divergences could be successful.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:53:10 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ray", "Souvik", ""], ["Pal", "Subrata", ""], ["Kar", "Sumit Kumar", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2105.06094", "submitter": "Souvik Ray", "authors": "Souvik Ray, Subrata Pal, Sumit Kumar Kar, Ayanendranath Basu", "title": "Characterizing the Functional Density Power Divergence Class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density power divergence (DPD) and related measures have produced many\nuseful statistical procedures which provide a good balance between model\nefficiency on one hand, and outlier stability or robustness on the other. The\nlarge number of citations received by the original DPD paper (Basu et al.,\n1998) and its many demonstrated applications indicate the popularity of these\ndivergences and the related methods of inference. The estimators that are\nderived from this family of divergences are all M-estimators where the defining\n$\\psi$ function is based explicitly on the form of the model density. The\nsuccess of the minimum divergence estimators based on the density power\ndivergence makes it imperative and meaningful to look for other, similar\ndivergences in the same spirit. The logarithmic density power divergence (Jones\net al., 2001), a logarithmic transform of the density power divergence, has\nalso been very successful in producing inference procedures with a high degree\nof efficiency simultaneously with a high degree of robustness. This further\nstrengthens the motivation to look for statistical divergences that are\ntransforms of the density power divergence, or, alternatively, members of the\nfunctional density power divergence class. This note characterizes the\nfunctional density power divergence class, and thus identifies the available\ndivergence measures within this construct that may possibly be explored for\nrobust and efficient statistical inference.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 06:06:23 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ray", "Souvik", ""], ["Pal", "Subrata", ""], ["Kar", "Sumit Kumar", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2105.06347", "submitter": "Sela Fried", "authors": "Sela Fried and Geoffrey Wolfer", "title": "Identity testing of reversible Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identity testing of Markov chains based on a\nsingle trajectory of observations under the distance notion introduced by\nDaskalakis et al. [2018a] and further analyzed by Cherapanamjeri and Bartlett\n[2019]. Both works made the restrictive assumption that the Markov chains under\nconsideration are symmetric. In this work we relax the symmetry assumption to\nthe more natural assumption of reversibility, still assuming that both the\nreference and the unknown Markov chains share the same stationary distribution.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:03:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Fried", "Sela", ""], ["Wolfer", "Geoffrey", ""]]}, {"id": "2105.06367", "submitter": "Jianhua Huang", "authors": "Jianhua Z. Huang and Ya Su", "title": "Asymptotic Properties of Penalized Spline Estimators in Concave Extended\n  Linear Models: Rates of Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general theory on rates of convergence of penalized\nspline estimators for function estimation when the likelihood functional is\nconcave in candidate functions, where the likelihood is interpreted in a broad\nsense that includes conditional likelihood, quasi-likelihood, and\npseudo-likelihood. The theory allows all feasible combinations of the spline\ndegree, the penalty order, and the smoothness of the unknown functions.\nAccording to this theory, the asymptotic behaviors of the penalized spline\nestimators depends on interplay between the spline knot number and the penalty\nparameter. The general theory is applied to obtain results in a variety of\ncontexts, including regression, generalized regression such as logistic\nregression and Poisson regression, density estimation, conditional hazard\nfunction estimation for censored data, quantile regression, diffusion function\nestimation for a diffusion type process, and estimation of spectral density\nfunction of a stationary time series. For multi-dimensional function\nestimation, the theory (presented in the Supplementary Material) covers both\npenalized tensor product splines and penalized bivariate splines on\ntriangulations.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:53:54 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Huang", "Jianhua Z.", ""], ["Su", "Ya", ""]]}, {"id": "2105.06395", "submitter": "Felipe Elorrieta", "authors": "Cesar Ojeda, Wilfredo Palma, Susana Eyheramendy and Felipe Elorrieta", "title": "An irregularly spaced first-order moving average model", "comments": "33 pages, 7 figures, 9 tables, 5 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel first-order moving-average model for analyzing time series observed\nat irregularly spaced intervals is introduced. Two definitions are presented,\nwhich are equivalent under Gaussianity. The first one relies on normally\ndistributed data and the specification of second-order moments. The second\ndefinition provided is more flexible in the sense that it allows for\nconsidering other distributional assumptions. The statistical properties are\ninvestigated along with the one-step linear predictors and their mean squared\nerrors. It is established that the process is strictly stationary under\nnormality and weakly stationary in the general case. Maximum likelihood and\nbootstrap estimation procedures are discussed and the finite-sample behavior of\nthese estimates is assessed through Monte Carlo experiments. In these\nsimulations, both methods perform well in terms of estimation bias and standard\nerrors, even with relatively small sample sizes. Moreover, we show that for\nnon-Gaussian data, for t-Student and Generalized errors distributions, the\nparameters of the model can be estimated precisely by maximum likelihood. The\nproposed IMA model is compared to the continuous autoregressive moving average\n(CARMA) models, exhibiting good performance. Finally, the practical application\nand usefulness of the proposed model are illustrated with two real-life data\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:24:13 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ojeda", "Cesar", ""], ["Palma", "Wilfredo", ""], ["Eyheramendy", "Susana", ""], ["Elorrieta", "Felipe", ""]]}, {"id": "2105.06500", "submitter": "Johannes Krebs", "authors": "Johannes Krebs", "title": "On the Bahadur representation of sample quantiles for score functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the Bahadur representation of sample quantiles for stabilizing\nscore functionals in stochastic geometry and study local fluctuations of the\ncorresponding empirical distribution function. The scores are obtained from a\nPoisson process. We apply the results to trimmed and Winsorized means of the\nscore functionals and establish a law of the iterated logarithm for the sample\nquantiles of the scores.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 18:25:33 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Krebs", "Johannes", ""]]}, {"id": "2105.06638", "submitter": "Boris Ryabko", "authors": "Boris Ryabko", "title": "Calibrating random number generator tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.IT math.IT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, statistical tests for random number generators (RNGs) are widely\nused in practice, and some of them are even included in information security\nstandards. But despite the popularity of RNGs, consistent tests are known only\nfor stationary ergodic deviations of randomness (a test is consistent if it\ndetects any deviations from a given class when the sample size goes to $ \\infty\n$). However, the model of a stationary ergodic source is too narrow for some\nRNGs, in particular, for generators based on physical effects. In this article,\nwe propose computable consistent tests for some classes of deviations more\ngeneral than stationary ergodic and describe some general properties of\nstatistical tests. The proposed approach and the resulting test are based on\nthe ideas and methods of information theory.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:19:31 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ryabko", "Boris", ""]]}, {"id": "2105.06884", "submitter": "Nicolas Marie", "authors": "Nicolas Marie and Am\\'elie Rosier", "title": "Nadaraya-Watson Estimator for I.I.D. Paths of Diffusion Processes", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a nonparametric Nadaraya-Watson estimator $\\widehat b$\nof the drift function computed from independent continuous observations of a\ndiffusion process. Risk bounds on $\\widehat b$ and its discrete-time\napproximation are established. An extension of the leave-one-out cross\nvalidation bandwidth selection method for $\\widehat b$ and some numerical\nexperiments are provided.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:13:29 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Marie", "Nicolas", ""], ["Rosier", "Am\u00e9lie", ""]]}, {"id": "2105.06890", "submitter": "Mamikon Ginovyan", "authors": "Mamikon S. Ginovyan and Artur A. Sahakyan", "title": "Statistical inference for stationary linear models with tapered data", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.00343", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we survey some recent results on statistical inference\n(parametric and nonparametric statistical estimation, hypotheses testing) about\nthe spectrum of stationary models with tapered data, as well as, a question\nconcerning robustness of inferences, carried out on a linear stationary process\ncontaminated by a small trend. We also discuss some question concerning tapered\nToeplitz matrices and operators, central limit theorems for tapered Toeplitz\ntype quadratic functionals, and tapered Fej\\'er-type kernels and singular\nintegrals. These are the main tools for obtaining the corresponding results,\nand also are of interest in themselves. The processes considered will be\ndiscrete-time and continuous-time Gaussian, linear or L\\'evy-driven linear\nprocesses with memory.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:22:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ginovyan", "Mamikon S.", ""], ["Sahakyan", "Artur A.", ""]]}, {"id": "2105.07137", "submitter": "Shouri Hu", "authors": "Shouri Hu, Jingyan Huang, Hao Chen and Hock Peng Chan", "title": "Sparsity Likelihood for Sparse Signal and Change-point Detection", "comments": "Submitted to Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose here sparsity likelihood scores for sparse signal detection and\nsegmentation of multiple sequences with sparse change-points. The scores\nachieve the same optimality as the higher-criticism and Berk-Jones test\nstatistics in sparse signal detection. We extend its optimality to sparse\nchange-point estimation and detection, for both normal and Poisson models, with\nasymptotics that differ for the two models. We illustrate its application on\nsimulated datasets as well as on a single-cell copy number dataset taken from a\nmixture of normal and tumor cells of the same cancer patient.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:20:01 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hu", "Shouri", ""], ["Huang", "Jingyan", ""], ["Chen", "Hao", ""], ["Chan", "Hock Peng", ""]]}, {"id": "2105.07156", "submitter": "B.L.S. Prakasa Rao", "authors": "B.L.S. Prakasa Rao", "title": "Singularity for bifractional and trifractional Brownian motions based on\n  their Hurst indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study sufficient conditions which ensure that the probability measures\ngenerated by two bifractional Brownian motions on an interval [0,1] are\nsingular with respect to each other and sufficient conditions for the\nprobability measures generated by two trifractional Brownian motions on an\ninterval [0,1] are singular with respect to each other.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 07:06:50 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Rao", "B. L. S. Prakasa", ""]]}, {"id": "2105.07283", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Calibrating sufficiently", "comments": "26 pages, 2 figures, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When probabilistic classifiers are trained and calibrated, the so-called\ngrouping loss component of the calibration loss can easily be overlooked.\nGrouping loss refers to the gap between observable information and information\nactually exploited in the calibration exercise. We investigate the relation\nbetween grouping loss and the concept of sufficiency, identifying\ncomonotonicity as a useful criterion for sufficiency. We revisit the probing\nreduction approach of Langford & Zadrozny (2005) and find that it produces an\nestimator of probabilistic classifiers that reduces grouping loss. Finally, we\ndiscuss Brier curves as tools to support training and 'sufficient' calibration\nof probabilistic classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 19:48:28 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 18:36:00 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 13:02:08 GMT"}, {"version": "v4", "created": "Sun, 25 Jul 2021 12:41:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "2105.07406", "submitter": "Inna Gerlovina", "authors": "Inna Gerlovina, Alan E. Hubbard", "title": "General order adjusted Edgeworth expansions for generalized $t$-tests", "comments": "22 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop generalized approach to obtaining Edgeworth expansions for\n$t$-statistics of an arbitrary order using computer algebra and combinatorial\nalgorithms. To incorporate various versions of mean-based statistics, we\nintroduce Adjusted Edgeworth expansions that allow polynomials in the terms to\ndepend on a sample size in a specific way and prove their validity. Provided\nresults up to 5th order include one and two-sample ordinary $t$-statistics with\nbiased and unbiased variance estimators, Welch $t$-test, and moderated\n$t$-statistics based on empirical Bayes method, as well as general results for\nany statistic with available moments of the sampling distribution. These\nresults are included in a software package that aims to reach a broad community\nof researchers and serve to improve inference in a wide variety of analytical\nprocedures; practical considerations of using such expansions are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 10:43:36 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gerlovina", "Inna", ""], ["Hubbard", "Alan E.", ""]]}, {"id": "2105.07410", "submitter": "Gianluca Finocchio", "authors": "Gianluca Finocchio and Johannes Schmidt-Hieber", "title": "Posterior contraction for deep Gaussian process priors", "comments": "48 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study posterior contraction rates for a class of deep Gaussian process\npriors applied to the nonparametric regression problem under a general\ncomposition assumption on the regression function. It is shown that the\ncontraction rates can achieve the minimax convergence rate (up to $\\log n$\nfactors), while being adaptive to the underlying structure and smoothness of\nthe target function. The proposed framework extends the Bayesian nonparametrics\ntheory for Gaussian process priors.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 11:34:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Finocchio", "Gianluca", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "2105.07449", "submitter": "Julia Lindberg", "authors": "Julia Lindberg, Nathan Nicholson, Jose Israel Rodriguez, Zinan Wang", "title": "The maximum likelihood degree of sparse polynomial systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider statistical models arising from the common set of solutions to a\nsparse polynomial system with general coefficients. The maximum likelihood\ndegree counts the number of critical points of the likelihood function\nrestricted to the model.\n  We prove the maximum likelihood degree of a sparse polynomial system is\ndetermined by its Newton polytopes and equals the mixed volume of a related\nLagrange system of equations. As a corollary, we find that the algebraic degree\nof several optimization problems is equal to a similar mixed volume.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 14:59:35 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lindberg", "Julia", ""], ["Nicholson", "Nathan", ""], ["Rodriguez", "Jose Israel", ""], ["Wang", "Zinan", ""]]}, {"id": "2105.07536", "submitter": "Rong Ma", "authors": "T. Tony Cai and Rong Ma", "title": "Theoretical Foundations of t-SNE for Visualizing High-Dimensional\n  Clustered Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study investigates the theoretical foundations of t-distributed\nstochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction\nand data visualization method. A novel theoretical framework for the analysis\nof t-SNE based on the gradient descent approach is presented. For the early\nexaggeration stage of t-SNE, we show its asymptotic equivalence to a power\niteration based on the underlying graph Laplacian, characterize its limiting\nbehavior, and uncover its deep connection to Laplacian spectral clustering, and\nfundamental principles including early stopping as implicit regularization. The\nresults explain the intrinsic mechanism and the empirical benefits of such a\ncomputational strategy. For the embedding stage of t-SNE, we characterize the\nkinematics of the low-dimensional map throughout the iterations, and identify\nan amplification phase, featuring the intercluster repulsion and the expansive\nbehavior of the low-dimensional map. The general theory explains the fast\nconvergence rate and the exceptional empirical performance of t-SNE for\nvisualizing clustered data, brings forth the interpretations of the t-SNE\noutput, and provides theoretical guidance for selecting tuning parameters in\nvarious applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:43:20 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 01:04:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Cai", "T. Tony", ""], ["Ma", "Rong", ""]]}, {"id": "2105.07587", "submitter": "Ran Dai", "authors": "Ran Dai, Hyebin Song, Rina Foygel Barber, Garvesh Raskutti", "title": "Convergence guarantee for the sparse monotone single index model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider a high-dimensional monotone single index model (hdSIM), which is\na semiparametric extension of a high-dimensional generalize linear model\n(hdGLM), where the link function is unknown, but constrained with monotone and\nnon-decreasing shape. We develop a scalable projection-based iterative\napproach, the \"Sparse Orthogonal Descent Single-Index Model\" (SOD-SIM), which\nalternates between sparse-thresholded orthogonalized \"gradient-like\" steps and\nisotonic regression steps to recover the coefficient vector. Our main\ncontribution is that we provide finite sample estimation bounds for both the\ncoefficient vector and the link function in high-dimensional settings under\nvery mild assumptions on the design matrix $\\mathbf{X}$, the error term\n$\\epsilon$, and their dependence. The convergence rate for the link function\nmatched the low-dimensional isotonic regression minimax rate up to some\npoly-log terms ($n^{-1/3}$). The convergence rate for the coefficients is also\n$n^{-1/3}$ up to some poly-log terms. This method can be applied to many real\ndata problems, including GLMs with misspecified link, classification with\nmislabeled data, and classification with positive-unlabeled (PU) data. We study\nthe performance of this method via both numerical studies and also an\napplication on a rocker protein sequence data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:09:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dai", "Ran", ""], ["Song", "Hyebin", ""], ["Barber", "Rina Foygel", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "2105.07613", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "Nonparametric regression for locally stationary functional time series", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we develop an asymptotic theory of nonparametric regression\nfor a locally stationary functional time series. First, we introduce the notion\nof a locally stationary functional time series (LSFTS) that takes values in a\nsemi-metric space. Then, we propose a nonparametric model for LSFTS with a\nregression function that changes smoothly over time. We establish the uniform\nconvergence rates of a class of kernel estimators, the Nadaraya-Watson (NW)\nestimator of the regression function, and a central limit theorem of the NW\nestimator.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 05:13:53 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:13:20 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "2105.07641", "submitter": "Jianfeng Yao", "authors": "Weiming Li, Qinwen Wang and Jianfeng Yao", "title": "Eigenvalue distribution of a high-dimensional distance covariance matrix\n  with application", "comments": "42 pages, 6 fugures. Forthcoming in \"Statistica Sinica\" (2021+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a new random matrix model called distance covariance matrix in\nthis paper, whose normalized trace is equivalent to the distance covariance. We\nfirst derive a deterministic limit for the eigenvalue distribution of the\ndistance covariance matrix when the dimensions of the vectors and the sample\nsize tend to infinity simultaneously. This limit is valid when the vectors are\nindependent or weakly dependent through a finite-rank perturbation. It is also\nuniversal and independent of the details of the distributions of the vectors.\nFurthermore, the top eigenvalues of this distance covariance matrix are shown\nto obey an exact phase transition when the dependence of the vectors is of\nfinite rank. This finding enables the construction of a new detector for such\nweak dependence where classical methods based on large sample covariance\nmatrices or sample canonical correlations may fail in the considered\nhigh-dimensional framework.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 07:04:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Weiming", ""], ["Wang", "Qinwen", ""], ["Yao", "Jianfeng", ""]]}, {"id": "2105.07835", "submitter": "Jan Bohr", "authors": "Jan Bohr, Richard Nickl", "title": "On log-concave approximations of high-dimensional posterior measures and\n  stability properties in non-linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of efficiently generating random samples from high-dimensional\nand non-log-concave posterior measures arising from nonlinear regression\nproblems is considered. Extending investigations from arXiv:2009.05298, local\nand global stability properties of the model are identified under which such\nposterior distributions can be approximated in Wasserstein distance by suitable\nlog-concave measures. This allows the use of fast gradient based sampling\nalgorithms, for which convergence guarantees are established that scale\npolynomially in all relevant quantities (assuming `warm' initialisation). The\nscope of the general theory is illustrated in a non-linear inverse problem from\nintegral geometry for which new stability results are derived.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:49:27 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bohr", "Jan", ""], ["Nickl", "Richard", ""]]}, {"id": "2105.07856", "submitter": "Edward Simmons", "authors": "Edward Simmons", "title": "Correlations Between Learning Environments and Dropout Intention", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.28550.50245", "report-no": null, "categories": "cs.CY math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research is comparing learning environments to students dropout\nintentions. While using statistics I looked at data and the correlations\nbetween two articles to see how the two studies looked side to side. Learning\nenvironments and dropout intentions can both have vary effects on students.\nThey can both determine if a student does well, or bad in school especially\nmath.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 10:08:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Simmons", "Edward", ""]]}, {"id": "2105.08024", "submitter": "Yuting Wei", "authors": "Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, Yuting Wei", "title": "Sample-Efficient Reinforcement Learning Is Feasible for Linearly\n  Realizable MDPs with Limited Revisiting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-complexity models such as linear function representation play a pivotal\nrole in enabling sample-efficient reinforcement learning (RL). The current\npaper pertains to a scenario with value-based linear representation, which\npostulates the linear realizability of the optimal Q-function (also called the\n\"linear $Q^{\\star}$ problem\"). While linear realizability alone does not allow\nfor sample-efficient solutions in general, the presence of a large\nsub-optimality gap is a potential game changer, depending on the sampling\nmechanism in use. Informally, sample efficiency is achievable with a large\nsub-optimality gap when a generative model is available but is unfortunately\ninfeasible when we turn to standard online RL settings.\n  In this paper, we make progress towards understanding this linear $Q^{\\star}$\nproblem by investigating a new sampling protocol, which draws samples in an\nonline/exploratory fashion but allows one to backtrack and revisit previous\nstates in a controlled and infrequent manner. This protocol is more flexible\nthan the standard online RL setting, while being practically relevant and far\nmore restrictive than the generative model. We develop an algorithm tailored to\nthis setting, achieving a sample complexity that scales polynomially with the\nfeature dimension, the horizon, and the inverse sub-optimality gap, but not the\nsize of the state/action space. Our findings underscore the fundamental\ninterplay between sampling protocols and low-complexity structural\nrepresentation in RL.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:22:07 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Gen", ""], ["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Gu", "Yuantao", ""], ["Wei", "Yuting", ""]]}, {"id": "2105.08186", "submitter": "Jorge Castillo-Mateo", "authors": "Jorge Castillo-Mateo (University of Zaragoza)", "title": "Distribution-Free Changepoint Detection Tests Based on the Breaking of\n  Records", "comments": "30 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of record-breaking events is of interest in fields such as\nclimatology, hydrology, economy or sports. In connection with the record\noccurrence, we propose three distribution-free statistics for the changepoint\ndetection problem. They are CUSUM-type statistics based on the upper and/or\nlower record indicators which occur in a series. Using a version of the\nfunctional central limit theorem, we show that the CUSUM-type statistics are\nasymptotically Kolmogorov distributed. The main results under the null\nhypothesis are based on series of independent and identically distributed\nrandom variables, but a statistic to deal with series with seasonal component\nand serial correlation is also proposed. A Monte Carlo study of size, power and\nchangepoint estimate has been performed. Finally, the methods are illustrated\nby analyzing the time series of temperatures at Madrid, Spain. The $\\textsf{R}$\npackage $\\texttt{RecordTest}$ publicly available on CRAN implements the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:47:19 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Castillo-Mateo", "Jorge", "", "University of Zaragoza"]]}, {"id": "2105.08304", "submitter": "Jesus Cerquides", "authors": "Jesus Cerquides", "title": "Parametrization invariant interpretation of priors and posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we leverage on probability over Riemannian manifolds to rethink\nthe interpretation of priors and posteriors in Bayesian inference. The main\nmindshift is to move away from the idea that \"a prior distribution establishes\na probability distribution over the parameters of our model\" to the idea that\n\"a prior distribution establishes a probability distribution over probability\ndistributions\". To do that we assume that our probabilistic model is a\nRiemannian manifold with the Fisher metric. Under this mindset, any\ndistribution over probability distributions should be \"intrinsic\", that is,\ninvariant to the specific parametrization which is selected for the manifold.\nWe exemplify our ideas through a simple analysis of distributions over the\nmanifold of Bernoulli distributions. One of the major shortcomings of maximum a\nposteriori estimates is that they depend on the parametrization. Based on the\nunderstanding developed here, we can define the maximum a posteriori estimate\nwhich is independent of the parametrization.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:45:05 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 20:11:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cerquides", "Jesus", ""]]}, {"id": "2105.08338", "submitter": "Mathilde Sautreuil", "authors": "Mathilde Sautreuil, Sarah Lemler, Paul-Henry Courn\\`ede", "title": "Neural networks to predict survival from RNA-seq data in oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis consists of studying the elapsed time until an event of\ninterest, such as the death or recovery of a patient in medical studies. This\nwork explores the potential of neural networks in survival analysis from\nclinical and RNA-seq data. If the neural network approach is not recent in\nsurvival analysis, methods were classically considered for low-dimensional\ninput data. But with the emergence of high-throughput sequencing data, the\nnumber of covariates of interest has become very large, with new statistical\nissues to consider. We present and test a few recent neural network approaches\nfor survival analysis adapted to high-dimensional inputs.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 08:03:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sautreuil", "Mathilde", ""], ["Lemler", "Sarah", ""], ["Courn\u00e8de", "Paul-Henry", ""]]}, {"id": "2105.08478", "submitter": "Jan Van Waaij PhD", "authors": "B.J.K. Kleijn and J. van Waaij", "title": "A sparse stochastic block model with two unequal communities", "comments": "arXiv admin note: text overlap with arXiv:1810.09533,\n  arXiv:2005.01362", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show posterior convergence for the community structure in the planted\nbi-section model, for several interesting priors. Examples include where the\nlabel on each vertex is iid Bernoulli distributed, with some parameter\n$r\\in(0,1)$. The parameter $r$ may be fixed, or equipped with a beta\ndistribution. We do not have constraints on the class sizes, which might be as\nsmall as zero, or include all vertices, and everything in between. This enables\nus to test between a uniform (Erd\\\"os-R\\'enyi) random graph with no\ndistinguishable community or the planted bi-section model. The exact bounds for\nposterior convergence enable us to convert credible sets into confidence sets.\nSymmetric testing with posterior odds is shown to be consistent.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:41:56 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Kleijn", "B. J. K.", ""], ["van Waaij", "J.", ""]]}, {"id": "2105.08678", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian", "title": "Nonparametric Modeling of Higher-Order Interactions via Hypergraphons", "comments": "To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study statistical and algorithmic aspects of using hypergraphons, that are\nlimits of large hypergraphs, for modeling higher-order interactions. Although\nhypergraphons are extremely powerful from a modeling perspective, we consider a\nrestricted class of Simple Lipschitz Hypergraphons (SLH), that are amenable to\npractically efficient estimation. We also provide rates of convergence for our\nestimator that are optimal for the class of SLH. Simulation results are\nprovided to corroborate the theory.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:08:29 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""]]}, {"id": "2105.08766", "submitter": "Clement de Chaisemartin", "authors": "Cl\\'ement de Chaisemartin", "title": "The Minimax Estimator of the Average Treatment Effect, among Linear\n  Combinations of Conditional Average Treatment Effects Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I consider the estimation of the average treatment effect (ATE), in a\npopulation that can be divided into $G$ groups, and such that one has unbiased\nand uncorrelated estimators of the conditional average treatment effect (CATE)\nin each group. These conditions are for instance met in stratified randomized\nexperiments. I assume that the outcome is homoscedastic, and that each CATE is\nbounded in absolute value by $B$ standard deviations of the outcome, for some\nknown constant $B$. I derive, across all linear combinations of the CATEs'\nestimators, the estimator of the ATE with the lowest worst-case mean-squared\nerror. This optimal estimator assigns a weight equal to group $g$'s share in\nthe population to the most precisely estimated CATEs, and a weight proportional\nto one over the CATE's variance to the least precisely estimated CATEs. This\noptimal estimator is feasible: the weights only depend on known quantities. I\nthen allow for positive covariances known up to the outcome's variance between\nthe estimators. This condition is met in differences-in-differences designs, if\nerrors are homoscedastic and uncorrelated. Under those assumptions, I show that\nthe minimax estimator is still feasible and can easily be computed.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:32:22 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 23:49:40 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["de Chaisemartin", "Cl\u00e9ment", ""]]}, {"id": "2105.08875", "submitter": "Nicholas Sterge", "authors": "Nicholas Sterge, Bharath Sriperumbudur", "title": "Statistical Optimality and Computational Efficiency of Nystr\\\"om Kernel\n  PCA", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods provide an elegant framework for developing nonlinear learning\nalgorithms from simple linear methods. Though these methods have superior\nempirical performance in several real data applications, their usefulness is\ninhibited by the significant computational burden incurred in large sample\nsituations. Various approximation schemes have been proposed in the literature\nto alleviate these computational issues, and the approximate kernel machines\nare shown to retain the empirical performance. However, the theoretical\nproperties of these approximate kernel machines are less well understood. In\nthis work, we theoretically study the trade-off between computational\ncomplexity and statistical accuracy in Nystr\\\"om approximate kernel principal\ncomponent analysis (KPCA), wherein we show that the Nystr\\\"om approximate KPCA\nmatches the statistical performance of (non-approximate) KPCA while remaining\ncomputationally beneficial. Additionally, we show that Nystr\\\"om approximate\nKPCA outperforms the statistical behavior of another popular approximation\nscheme, the random feature approximation, when applied to KPCA.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:49:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sterge", "Nicholas", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "2105.08947", "submitter": "Yo Sheena", "authors": "Yo Sheena", "title": "MLE convergence speed to information projection of exponential family:\n  Criterion for model dimension and sample size -- complete proof version--", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a parametric model of distributions, the closest distribution in the\nmodel to the true distribution located outside the model is considered.\nMeasuring the closeness between two distributions with the Kullback-Leibler\n(K-L) divergence, the closest distribution is called the \"information\nprojection.\" The estimation risk of the maximum likelihood estimator (MLE) is\ndefined as the expectation of K-L divergence between the information projection\nand the predictive distribution with plugged-in MLE. Here, the asymptotic\nexpansion of the risk is derived up to $n^{-2}$-order, and the sufficient\ncondition on the risk for the Bayes error rate between the true distribution\nand the information projection to be lower than a specified value is\ninvestigated. Combining these results, the \"$p-n$ criterion\" is proposed, which\ndetermines whether the MLE is sufficiently close to the information projection\nfor the given model and sample. In particular, the criterion for an exponential\nfamily model is relatively simple and can be used for a complex model with no\nexplicit form of normalizing constant. This criterion can constitute a solution\nto the sample size or model acceptance problem. Use of the $p-n$ criteria is\ndemonstrated for two practical datasets. The relationship between the results\nand information criteria is also studied.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:45:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:02:13 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 05:52:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sheena", "Yo", ""]]}, {"id": "2105.09003", "submitter": "Tim Kutzker", "authors": "Tim Kutzker, Nadja Klein, Dominik Wied", "title": "Flexible Specification Testing in Semi-Parametric Quantile Regression\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several novel consistent specification tests for quantile\nregression models which generalize former tests by important characteristics.\nFirst, we allow the covariate effects to be quantile-dependent and nonlinear.\nSecond, we allow for parameterizing the conditional quantile functions by\nappropriate basis functions, rather than parametrically.We are hence able to\ntest for functional forms beyond linearity, while retaining the linear effects\nas special cases. In both cases, the induced class of conditional distribution\nfunctions is tested with a Cram\\'{e}r-von Mises type test statistic for which\nwe derive the theoretical limit distribution and propose a practical bootstrap\nmethod. To increase the power of the first test, we further suggest a modified\ntest statistic using the B-spline approach from the second test. A detailed\nMonte Carlo experiment shows that the test results in a reasonable sized\ntesting procedure with large power. Our first application to conditional income\ndisparities between East and West Germany over the period 2001-2010 indicates\nthat there are not only still significant differences between East and West but\nalso across the quantiles of the conditional income distributions, when\nconditioning on age and year. The second application to data from the\nAustralian national electricity market reveals the importance of using\ninteraction effects for modelling the highly skewed and heavy-tailed\ndistributions of energy prices conditional one day, time of day and demand.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:13:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kutzker", "Tim", ""], ["Klein", "Nadja", ""], ["Wied", "Dominik", ""]]}, {"id": "2105.09028", "submitter": "Sven Klaassen", "authors": "Sven Klaassen", "title": "A Note on High-Dimensional Confidence Regions", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in statistics introduced versions of the central limit\ntheorem for high-dimensional vectors, allowing for the construction of\nconfidence regions for high-dimensional parameters. In this note, $s$-sparsely\nconvex high-dimensional confidence regions are compared with respect to their\nvolume. Specific confidence regions which are based on $\\ell_p$-balls are found\nto have exponentially smaller volume than the corresponding hypercube. The\ntheoretical results are validated by a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:51:36 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Klaassen", "Sven", ""]]}, {"id": "2105.09032", "submitter": "Marina Bogomolov", "authors": "Marina Bogomolov", "title": "Testing partial conjunction hypotheses under dependency, with\n  applications to meta-analysis", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many statistical problems the hypotheses are naturally divided into\ngroups, and the investigators are interested to perform group-level inference,\npossibly along with inference on individual hypotheses. We consider the goal of\ndiscovering groups containing $u$ or more signals with group-level false\ndiscovery rate (FDR) control. This goal can be addressed by multiple testing of\npartial conjunction hypotheses with a parameter $u,$ which reduce to global\nnull hypotheses for $u=1.$ We consider the case where the partial conjunction\n$p$-values are combinations of within-group $p$-values, and obtain sufficient\nconditions on (1) the dependencies among the $p$-values within and across the\ngroups, (2) the combining method for obtaining partial conjunction $p$-values,\nand (3) the multiple testing procedure, for obtaining FDR control on partial\nconjunction discoveries. We consider separately the dependencies encountered in\nthe meta-analysis setting, where multiple features are tested in several\nindependent studies, and the $p$-values within each study may be dependent.\nBased on the results for this setting, we generalize the procedure of\nBenjamini, Heller, and Yekutieli (2009) for assessing replicability of signals\nacross studies, and extend their theoretical results regarding FDR control with\nrespect to replicability claims.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:56:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bogomolov", "Marina", ""]]}, {"id": "2105.09141", "submitter": "Jiguang Sun", "authors": "Jiguang Sun", "title": "Local estimators and Bayesian inverse problems with non-unique solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach is effective for inverse problems. The posterior\ndensity distribution provides useful information of the unknowns. However, for\nproblems with non-unique solutions, the classical estimators such as the\nmaximum a posterior (MAP) and conditional mean (CM) are not enough. We\nintroduce two new estimators, the local maximum a posterior (LMAP) and local\nconditional mean (LCM). Their applications are demonstrated by three inverse\nproblems: an inverse spectral problem, an inverse source problem, and an\ninverse medium problem.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:00:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sun", "Jiguang", ""]]}, {"id": "2105.09232", "submitter": "Lin Fan", "authors": "Lin Fan, Peter W. Glynn", "title": "Diffusion Approximations for Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of Thompson sampling from the perspective of weak\nconvergence. In the regime where the gaps between arm means scale as\n$1/\\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson\nsampling evolve according to discrete versions of SDEs and random ODEs. As $n\n\\to \\infty$, we show that the dynamics converge weakly to solutions of the\ncorresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)\nindependently proposed this regime and developed similar SDE and random ODE\napproximations for Thompson sampling in the multi-armed bandit setting.) Our\nweak convergence theory, which covers both multi-armed and linear bandit\nsettings, is developed from first principles using the Continuous Mapping\nTheorem and can be directly adapted to analyze other sampling-based bandit\nalgorithms, for example, algorithms using the bootstrap for exploration. We\nalso establish an invariance principle for multi-armed bandits with gaps\nscaling as $1/\\sqrt{n}$ -- for Thompson sampling and related algorithms\ninvolving posterior approximation or the bootstrap, the weak diffusion limits\nare in general the same regardless of the specifics of the reward distributions\nor the choice of prior. In particular, as suggested by the classical\nBernstein-von Mises normal approximation for posterior distributions, the weak\ndiffusion limits generally coincide with the limit for normally-distributed\nrewards and priors.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:28:01 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 03:00:42 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Fan", "Lin", ""], ["Glynn", "Peter W.", ""]]}, {"id": "2105.09254", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Numair Sani, Yizhen Xu, Ilya Shpitser", "title": "Multiply Robust Causal Mediation Analysis with Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG econ.EM stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many applications, researchers are interested in the direct and indirect\ncausal effects of an intervention on an outcome of interest. Mediation analysis\noffers a rigorous framework for the identification and estimation of such\ncausal quantities. In the case of binary treatment, efficient estimators for\nthe direct and indirect effects are derived by Tchetgen Tchetgen and Shpitser\n(2012). These estimators are based on influence functions and possess desirable\nmultiple robustness properties. However, they are not readily applicable when\ntreatments are continuous, which is the case in several settings, such as drug\ndosage in medical applications. In this work, we extend the influence\nfunction-based estimator of Tchetgen Tchetgen and Shpitser (2012) to deal with\ncontinuous treatments by utilizing a kernel smoothing approach. We first\ndemonstrate that our proposed estimator preserves the multiple robustness\nproperty of the estimator in Tchetgen Tchetgen and Shpitser (2012). Then we\nshow that under certain mild regularity conditions, our estimator is\nasymptotically normal. Our estimation scheme allows for high-dimensional\nnuisance parameters that can be estimated at slower rates than the target\nparameter. Additionally, we utilize cross-fitting, which allows for weaker\nsmoothness requirements for the nuisance functions.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:58:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Sani", "Numair", ""], ["Xu", "Yizhen", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2105.10017", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul", "title": "Segmentation of high dimensional means over multi-dimensional change\n  points and connections to regression trees", "comments": "All implementations carried out in R (code available upon request)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is motivated by the objective of providing a new analytically\ntractable and fully frequentist framework to characterize and implement\nregression trees while also allowing a multivariate (potentially high\ndimensional) response. The connection to regression trees is made by a high\ndimensional model with dynamic mean vectors over multi-dimensional change axes.\nOur theoretical analysis is carried out under a single two dimensional change\npoint setting. An optimal rate of convergence of the proposed estimator is\nobtained, which in turn allows existence of limiting distributions.\nDistributional behavior of change point estimates are split into two distinct\nregimes, the limiting distributions under each regime is then characterized, in\nturn allowing construction of asymptotically valid confidence intervals for\n$2d$-location of change. All results are obtained under a high dimensional\nscaling $s\\log^2 p=o(T_wT_h),$ where $p$ is the response dimension, $s$ is a\nsparsity parameter, and $T_w,T_h$ are sampling periods along change axes. We\ncharacterize full regression trees by defining a multiple multi-dimensional\nchange point model. Natural extensions of the single $2d$-change point\nestimation methodology are provided. Two applications, first on segmentation of\n{\\it Infra-red astronomy satellite (IRAS)} data and second to segmentation of\ndigital images are provided. Methodology and theoretical results are supported\nwith monte-carlo simulations.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:29:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kaul", "Abhishek", ""]]}, {"id": "2105.10145", "submitter": "Yuke Shi", "authors": "Yuke Shi, Wei Zhang, Aiyi Liu and Qizhai Li", "title": "Distance-based regression analysis for measuring associations", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Distance-based regression model, as a nonparametric multivariate method, has\nbeen widely used to detect the association between variations in a distance or\ndissimilarity matrix for outcomes and predictor variables of interest. Based on\nit, a pseudo-$F$ statistic which partitions the variation in distance matrices\nis often constructed to achieve the aim. To the best of our knowledge, the\nstatistical properties of the pseudo-$F$ statistic has not yet been well\nestablished in the literature. To fill this gap, we study the asymptotic null\ndistribution of the pseudo-$F$ statistic and show that it is asymptotically\nequivalent to a mixture of chi-squared random variables. Given that the\npseudo-$F$ test statistic has unsatisfactory power when the correlations of the\nresponse variables are large, we propose a square-root $F$-type test statistic\nwhich replaces the similarity matric with its square root. The asymptotic null\ndistribution of the new test statistic and power of both tests are also\ninvestigated.\n  Simulation studies are conducted to validate the asymptotic distributions of\nthe tests and demonstrate that the proposed test has more robust power than the\npseudo-$F$ test. Both test statistics are exemplified with a gene expression\ndataset for a prostate cancer pathway.\n  Keywords: Asymptotic distribution, Chi-squared-type mixture, Nonparametric\ntest, Pseudo-$F$ test, Similarity matrix.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:05:28 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Shi", "Yuke", ""], ["Zhang", "Wei", ""], ["Liu", "Aiyi", ""], ["Li", "Qizhai", ""]]}, {"id": "2105.10254", "submitter": "Sergios Agapiou", "authors": "Sergios Agapiou and Peter Math\\'e", "title": "Designing truncated priors for direct and inverse Bayesian problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach to inverse problems with functional unknowns, has\nreceived significant attention in recent years. An important component of the\ndeveloping theory is the study of the asymptotic performance of the posterior\ndistribution in the frequentist setting. The present paper contributes to the\narea of Bayesian inverse problems by formulating a posterior contraction theory\nfor linear inverse problems, with truncated Gaussian series priors, and under\ngeneral smoothness assumptions. Emphasis is on the intrinsic role of the\ntruncation point both for the direct as well as for the inverse problem, which\nare related through the modulus of continuity as this was recently highlighted\nby Knapik and Salomond (2018).\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:10:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Agapiou", "Sergios", ""], ["Math\u00e9", "Peter", ""]]}, {"id": "2105.10318", "submitter": "Ir\\`ene Waldspurger", "authors": "Ir\\`ene Waldspurger", "title": "Lecture notes on non-convex algorithms for low-rank matrix recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-rank matrix recovery problems are inverse problems which naturally arise\nin various fields like signal processing, imaging and machine learning. They\nare non-convex and NP-hard in full generality. It is therefore a delicate\nproblem to design efficient recovery algorithms and to provide rigorous\ntheoretical insights on the behavior of these algorithms. The goal of these\nnotes is to review recent progress in this direction for the class of so-called\n\"non-convex algorithms\", with a particular focus on the proof techniques.\n  Although they aim at presenting very recent research works, these notes have\nbeen written with the intent to be, as much as possible, accessible to\nnon-specialists.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:44:26 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Waldspurger", "Ir\u00e8ne", ""]]}, {"id": "2105.10360", "submitter": "Doudou Zhou", "authors": "Doudou Zhou, and Tianxi Cai, and Junwei Lu", "title": "BELT: Block-wise Missing Embedding Learning Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has attracted attention in many fields, including\nstatistics, applied mathematics, and electrical engineering. Most of the works\nfocus on the independent sampling models under which the observed entries are\nsampled independently. Motivated by applications in the integration of multiple\nElectronic Health Record (EHR) datasets, we propose the method {\\bf B}lock-wise\nmissing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT) to treat\nrow-wise/column-wise missingness. Specifically, BELT can recover block-wise\nmissing matrices efficiently when every pair of matrices has an overlap. Our\nidea is to exploit the orthogonal Procrustes problem to align the eigenspace of\nthe two sub-matrices using their overlap, then complete the missing blocks by\nthe inner product of the two low-rank components. Besides, we prove the\nstatistical rate for the eigenspace of the underlying matrix, which is\ncomparable to the rate under the independently missing assumption. Simulation\nstudies show that the method performs well under a variety of configurations.\nIn the real data analysis, the method is applied to two tasks: (i) the\nintegrating of several point-wise mutual information matrices built by English\nEHR and Chinese medical text data, and (ii) the machine translation between\nEnglish and Chinese medical concepts. Our method shows an advantage over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:55:30 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:20:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Doudou", ""], ["Cai", "Tianxi", ""], ["Lu", "Junwei", ""]]}, {"id": "2105.10417", "submitter": "Mengchu Li", "authors": "Mengchu Li and Yi Yu", "title": "Adversarially Robust Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection is becoming increasingly popular in many application\nareas. On one hand, most of the theoretically-justified methods are\ninvestigated in an ideal setting without model violations, or merely robust\nagainst identical heavy-tailed noise distribution across time and/or against\nisolate outliers; on the other hand, we are aware that there have been\nexponentially growing attacks from adversaries, who may pose systematic\ncontamination on data to purposely create spurious change points or disguise\ntrue change points. In light of the timely need for a change point detection\nmethod that is robust against adversaries, we start with, arguably, the\nsimplest univariate mean change point detection problem. The adversarial\nattacks are formulated through the Huber $\\varepsilon$-contamination framework,\nwhich in particular allows the contamination distributions to be different at\neach time point. In this paper, we demonstrate a phase transition phenomenon in\nchange point detection. This detection boundary is a function of the\ncontamination proportion $\\varepsilon$ and is the first time shown in the\nliterature. In addition, we derive the minimax-rate optimal localisation error\nrate, quantifying the cost of accuracy in terms of the contamination\nproportion. We propose a computationally feasible method, matching the minimax\nlower bound under certain conditions, saving for logarithmic factors. Extensive\nnumerical experiments are conducted with comparisons to robust change point\ndetection methods in the existing literature.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:37:04 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Mengchu", ""], ["Yu", "Yi", ""]]}, {"id": "2105.10675", "submitter": "Yi Yu", "authors": "Thomas Berrett and Yi Yu", "title": "Locally private online change point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online change point detection problems under the constraint of local\ndifferential privacy (LDP) where, in particular, the statistician does not have\naccess to the raw data. As a concrete problem, we study a multivariate\nnonparametric regression problem. At each time point $t$, the raw data are\nassumed to be of the form $(X_t, Y_t)$, where $X_t$ is a $d$-dimensional\nfeature vector and $Y_t$ is a response variable. Our primary aim is to detect\nchanges in the regression function $m_t(x)=\\mathbb{E}(Y_t |X_t=x)$ as soon as\nthe change occurs. We provide algorithms which respect the LDP constraint,\nwhich control the false alarm probability, and which detect changes with a\nminimal (minimax rate-optimal) delay. To quantify the cost of privacy, we also\npresent the optimal rate in the benchmark, non-private setting. These\nnon-private results are also new to the literature and thus are interesting\n\\emph{per se}. In addition, we study the univariate mean online change point\ndetection problem, under privacy constraints. This serves as the blueprint of\nstudying more complicated private change point detection problems.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:03:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Berrett", "Thomas", ""], ["Yu", "Yi", ""]]}, {"id": "2105.10821", "submitter": "Nikolaj Thams", "authors": "Nikolaj Thams, Sorawit Saengkyongam, Niklas Pfister and Jonas Peters", "title": "Statistical Testing under Distributional Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce statistical testing under distributional shifts.\nWe are interested in the hypothesis $P^* \\in H_0$ for a target distribution\n$P^*$, but observe data from a different distribution $Q^*$. We assume that\n$P^*$ is related to $Q^*$ through a known shift $\\tau$ and formally introduce\nhypothesis testing in this setting. We propose a general testing procedure that\nfirst resamples from the observed data to construct an auxiliary data set and\nthen applies an existing test in the target domain. We prove that if the size\nof the resample is at most $o(\\sqrt{n})$ and the resampling weights are\nwell-behaved, this procedure inherits the pointwise asymptotic level and power\nfrom the target test. If the map $\\tau$ is estimated from data, we can maintain\nthe above guarantees under mild conditions if the estimation works sufficiently\nwell. We further extend our results to uniform asymptotic level and a different\nresampling scheme. Testing under distributional shifts allows us to tackle a\ndiverse set of problems. We argue that it may prove useful in reinforcement\nlearning and covariate shift, we show how it reduces conditional to\nunconditional independence testing and we provide example applications in\ncausal inference.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 22:36:05 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 06:26:56 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Thams", "Nikolaj", ""], ["Saengkyongam", "Sorawit", ""], ["Pfister", "Niklas", ""], ["Peters", "Jonas", ""]]}, {"id": "2105.10888", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Eunice Okome Obiang, Pascal J\\'ez\\'equel, Fr\\'ed\\'eric Pro\\\"ia", "title": "A Bayesian approach for partial Gaussian graphical models with sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore various Bayesian approaches to estimate partial Gaussian graphical\nmodels. Our hierarchical structures enable to deal with single-output as well\nas multiple-output linear regressions, in small or high dimension, enforcing\neither no sparsity, sparsity, group sparsity or even sparse-group sparsity for\na bi-level selection in the direct links between predictors and responses,\nthanks to spike-and-slab priors corresponding to each setting. Adaptative and\nglobal shrinkages are also incorporated in the Bayesian modeling of the direct\nlinks. Gibbs samplers are developed and a simulation study shows the efficiency\nof our models which regularly give better results than the usual Lasso-type\nprocedures, especially in terms of support recovery. To conclude, a real\ndataset is investigated.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:47:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Obiang", "Eunice Okome", ""], ["J\u00e9z\u00e9quel", "Pascal", ""], ["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2105.10965", "submitter": "Demian Pouzo", "authors": "Marina Dias and Demian Pouzo", "title": "Inference for multi-valued heterogeneous treatment effects when the\n  number of treated units is small", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN math.ST q-fin.EC stat.AP stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method for conducting asymptotically valid inference for\ntreatment effects in a multi-valued treatment framework where the number of\nunits in the treatment arms can be small and do not grow with the sample size.\nWe accomplish this by casting the model as a semi-/non-parametric conditional\nquantile model and using known finite sample results about the law of the\nindicator function that defines the conditional quantile. Our framework allows\nfor structural functions that are non-additively separable, with flexible\nfunctional forms and heteroskedasticy in the residuals, and it also encompasses\ncommonly used designs like difference in difference. We study the finite sample\nbehavior of our test in a Monte Carlo study and we also apply our results to\nassessing the effect of weather events on GDP growth.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:06:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dias", "Marina", ""], ["Pouzo", "Demian", ""]]}, {"id": "2105.11067", "submitter": "Masayo Hirose", "authors": "Masayo Y. Hirose, Shuhei Mano", "title": "Asymptotic Moments Matching to Uniformly Minimum Variance Unbiased\n  Estimation under Ewens Sampling Formula", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ewens sampling formula is a distribution related to the random partition\nof a positive integer. In this study, we investigate the issue of non-existence\nsolutions in parameter estimation under the distribution. As a result, the\nfirst and second moments matching estimators to the uniformly minimum variance\nunbiased estimator are derived using the Ewens sampling formula in asymptotic\nsense. A Monte Carlo simulation study is performed to evaluate the efficiency\nof the resulting estimators.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 02:23:49 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hirose", "Masayo Y.", ""], ["Mano", "Shuhei", ""]]}, {"id": "2105.11580", "submitter": "Andrew Feutrill", "authors": "Andrew Feutrill, Matthew Roughan", "title": "NPD Entropy: A Non-Parametric Differential Entropy Rate Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of entropy rates for stationary discrete-valued stochastic\nprocesses is a well studied problem in information theory. However, estimating\nthe entropy rate for stationary continuous-valued stochastic processes has not\nreceived as much attention. In fact, many current techniques are not able to\naccurately estimate or characterise the complexity of the differential entropy\nrate for strongly correlated processes, such as Fractional Gaussian Noise and\nARFIMA(0,d,0). To the point that some cannot even detect the trend of the\nentropy rate, e.g. when it increases/decreases, maximum, or asymptotic trends,\nas a function of their Hurst parameter. However, a recently developed technique\nprovides accurate estimates at a high computational cost. In this paper, we\ndefine a robust technique for non-parametrically estimating the differential\nentropy rate of a continuous valued stochastic process from observed data, by\nmaking an explicit link between the differential entropy rate and the Shannon\nentropy rate of a quantised version of the original data. Estimation is\nperformed by a Shannon entropy rate estimator, and then converted to a\ndifferential entropy rate estimate. We show that this technique inherits many\nimportant statistical properties from the Shannon entropy rate estimator. The\nestimator is able to provide better estimates than the defined relative\nmeasures and much quicker estimates than known absolute measures, for strongly\ncorrelated processes. Finally, we analyse the complexity of the estimation\ntechnique and test the robustness to non-stationarity, and show that none of\nthe current techniques are robust to non-stationarity, even if they are robust\nto strong correlations.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:54:48 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Feutrill", "Andrew", ""], ["Roughan", "Matthew", ""]]}, {"id": "2105.11591", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Moulinath Banerjee and Ya'acov Ritov", "title": "On robust learning in the canonical change point problem under heavy\n  tailed errors in finite and growing dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a number of new findings about the canonical change point\nestimation problem. The first part studies the estimation of a change point on\nthe real line in a simple stump model using the robust Huber estimating\nfunction which interpolates between the $\\ell_1$ (absolute deviation) and\n$\\ell_2$ (least squares) based criteria. While the $\\ell_2$ criterion has been\nstudied extensively, its robust counterparts and in particular, the $\\ell_1$\nminimization problem have not. We derive the limit distribution of the\nestimated change point under the Huber estimating function and compare it to\nthat under the $\\ell_2$ criterion. Theoretical and empirical studies indicate\nthat it is more profitable to use the Huber estimating function (and in\nparticular, the $\\ell_1$ criterion) under heavy tailed errors as it leads to\nsmaller asymptotic confidence intervals at the usual levels compared to the\n$\\ell_2$ criterion. We also compare the $\\ell_1$ and $\\ell_2$ approaches in a\nparallel setting, where one has $m$ independent single change point problems\nand the goal is to control the maximal deviation of the estimated change points\nfrom the true values, and establish rigorously that the $\\ell_1$ estimation\ncriterion provides a superior rate of convergence to the $\\ell_2$, and that\nthis relative advantage is driven by the heaviness of the tail of the error\ndistribution. Finally, we derive minimax optimal rates for the change plane\nestimation problem in growing dimensions and demonstrate that Huber estimation\nattains the optimal rate while the $\\ell_2$ scheme produces a rate sub-optimal\nestimator for heavy tailed errors. In the process of deriving our results, we\nestablish a number of properties about the minimizers of compound Binomial and\ncompound Poisson processes which are of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:52:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2105.11720", "submitter": "Eric Gautier", "authors": "Christophe Gaillac (TSE), Eric Gautier (TSE, UT1)", "title": "Nonparametric classes for identification in random coefficients models\n  when regressors have limited variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies point identification of the distribution of the\ncoefficients in some random coefficients models with exogenous regressors when\ntheir support is a proper subset, possibly discrete but countable. We exhibit\ntrade-offs between restrictions on the distribution of the random coefficients\nand the support of the regressors. We consider linear models including those\nwith nonlinear transforms of a baseline regressor, with an infinite number of\nregressors and deconvolution, the binary choice model, and panel data models\nsuch as single-index panel data models and an extension of the Kotlarski lemma.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:43:22 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gaillac", "Christophe", "", "TSE"], ["Gautier", "Eric", "", "TSE, UT1"]]}, {"id": "2105.11873", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "On the estimation of locally stationary functional time series", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an asymptotic theory for estimating the time-varying\ncharacteristics of locally stationary functional time series. We introduce a\nkernel-based method to estimate the time-varying covariance operator and the\ntime-varying mean function of a locally stationary functional time series.\nSubsequently, we derive the convergence rate of the kernel estimator of the\ncovariance operator and associated eigenvalue and eigenfunctions. We also\nestablish a central limit theorem for the kernel-based locally weighted sample\nmean. As applications of our results, we discuss the prediction of locally\nstationary functional time series and methods for testing the equality of\ntime-varying mean functions in two functional samples.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:18:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:16:38 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 04:03:50 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "2105.11917", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jonathan A. Tawn", "title": "On the Tail Behaviour of Aggregated Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many areas of interest, modern risk assessment requires estimation of the\nextremal behaviour of sums of random variables. We derive the first order\nupper-tail behaviour of the weighted sum of bivariate random variables under\nweak assumptions on their marginal distributions and their copula. The extremal\nbehaviour of the marginal variables is characterised by the generalised Pareto\ndistribution and their extremal dependence through subclasses of the limiting\nrepresentations of Ledford and Tawn (1997) and Heffernan and Tawn (2004). We\nfind that the upper tail behaviour of the aggregate is driven by different\nfactors dependent on the signs of the marginal shape parameters; if they are\nboth negative, the extremal behaviour of the aggregate is determined by both\nmarginal shape parameters and the coefficient of asymptotic independence\n(Ledford and Tawn, 1996); if they are both positive or have different signs,\nthe upper-tail behaviour of the aggregate is given solely by the largest\nmarginal shape. We also derive the aggregate upper-tail behaviour for some well\nknown copulae which reveals further insight into the tail structure when the\ncopula falls outside the conditions for the subclasses of the limiting\ndependence representations.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:22:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:38:25 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 15:35:13 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Richards", "Jordan", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "2105.11939", "submitter": "Bal\\'azs R. Sziklai", "authors": "Bal\\'azs R. Sziklai and M\\'at\\'e Baranyi and K\\'aroly H\\'eberger", "title": "Testing Cross-Validation Variants in Ranking Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research investigates how to determine whether two rankings can come\nfrom the same distribution. We evaluate three hybrid tests: Wilcoxon's,\nDietterich's, and Alpaydin's statistical tests combined with cross-validation,\neach operating with folds ranging from 5 to 10, thus altogether 18 variants. We\nhave used the framework of a popular comparative statistical test, the Sum of\nRanking Differences, but our results are representative of all ranking\nenvironments. To compare these methods, we have followed an innovative approach\nborrowed from Economics. We designed eight scenarios for testing type I and II\nerrors. These represent typical situations (i.e., different data structures)\nthat cross-validation (CV) tests face routinely. The optimal CV method depends\non the preferences regarding the minimization of type I/II errors, size of the\ninput, and expected patterns in the data. The Wilcoxon method with eight folds\nproved to be the best under all three investigated input sizes, although there\nwere scenarios and decision aspects where other methods, namely Wilcoxon~10 and\nAlpaydin~10, performed better.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:39:53 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sziklai", "Bal\u00e1zs R.", ""], ["Baranyi", "M\u00e1t\u00e9", ""], ["H\u00e9berger", "K\u00e1roly", ""]]}, {"id": "2105.12019", "submitter": "Septimia Sarbu", "authors": "Septimia Sarbu and Abdellatif Zaidi", "title": "On learning parametric distributions from quantized samples", "comments": "Accepted for publication at the IEEE Information Theory Symposium\n  (ISIT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of learning parametric distributions from their\nquantized samples in a network. Specifically, $n$ agents or sensors observe\nindependent samples of an unknown parametric distribution; and each of them\nuses $k$ bits to describe its observed sample to a central processor whose goal\nis to estimate the unknown distribution. First, we establish a generalization\nof the well-known van Trees inequality to general $L_p$-norms, with $p > 1$, in\nterms of Generalized Fisher information. Then, we develop minimax lower bounds\non the estimation error for two losses: general $L_p$-norms and the related\nWasserstein loss from optimal transport.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:38:28 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sarbu", "Septimia", ""], ["Zaidi", "Abdellatif", ""]]}, {"id": "2105.12035", "submitter": "Neda Mohammadi Jouzdani", "authors": "Neda Mohammadi Jouzdani and Victor M. Panaretos", "title": "Functional Data Analysis with Rough Sampled Paths?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data are typically modeled as sampled paths of smooth stochastic\nprocesses in order to mitigate the fact that they are often observed discretely\nand noisily, occasionally irregularly and sparsely. The required smoothness\nallows for the use of smoothing techniques but excludes many stochastic\nprocesses, most notably diffusion processes. Such processes would otherwise be\nwell within the realm of functional data analysis, at least under complete\nobservation. In this short note we demonstrate that a simple modification of\nexisting methods allows for the functional data analysis of processes with\nnowhere differentiable sample paths, even when these are discretely and noisily\nobserved, including under irregular and sparse designs. By way of simulation it\nis shown that this is not a theoretical curiosity, but can work well in\npractice, hinting at potential closer links with the field of diffusion\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:15:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Jouzdani", "Neda Mohammadi", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2105.12061", "submitter": "Benjamin Eltzner", "authors": "Pernille Hansen, Benjamin Eltzner, Stephan F. Huckemann, Stefan Sommer", "title": "Diffusion Means in Geometric Spaces", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a location statistic for distributions on non-linear geometric\nspaces, the diffusion mean, serving both as an extension of and an alternative\nto the Fr\\'echet mean. The diffusion mean arises as the generalization of\nGaussian maximum likelihood analysis to non-linear spaces by maximizing the\nlikelihood of a Brownian motion. The diffusion mean depends on a time parameter\n$t$, which admits the interpretation of the allowed variance of the mean. The\ndiffusion $t$-mean of a distribution $X$ is the most likely origin of a\nBrownian motion at time $t$, given the end-point distribution $X$. We give a\ndetailed description of the asymptotic behavior of the diffusion estimator and\nprovide sufficient conditions for the diffusion estimator to be strongly\nconsistent. Furthermore, we present a smeary central limit theorem for\ndiffusion means and investigate properties of the diffusion mean for\ndistributions on the sphere $\\mathcal{S}^n$. Experimentally, we consider\nsimulated data and data from magnetic pole reversals, all indicating similar or\nimproved convergence rate compared to the Fr\\'echet mean. Here, we additionally\nestimate $t$ and consider its effects on smeariness and uniqueness of the\ndiffusion mean for distributions on the sphere.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:46:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hansen", "Pernille", ""], ["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan F.", ""], ["Sommer", "Stefan", ""]]}, {"id": "2105.12225", "submitter": "Mads R. Bisgaard", "authors": "Mads R. Bisgaard and Alexander Domahidi", "title": "A risk analysis framework for real-time control systems", "comments": "Comments are very welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Monte Carlo simulation framework for analysing the risk involved\nin deploying real-time control systems in safety-critical applications, as well\nas an algorithm design technique allowing one (in certain situations) to\nrobustify a control algorithm. Both approaches are very general and agnostic to\nthe initial control algorithm. We present examples showing that these\ntechniques can be used to analyse the reliability of implementations of\nnon-linear model predictive control algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 21:36:41 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bisgaard", "Mads R.", ""], ["Domahidi", "Alexander", ""]]}, {"id": "2105.12589", "submitter": "Yi-Kai Liu", "authors": "Alireza Seif, Mohammad Hafezi and Yi-Kai Liu", "title": "Compressed Sensing Measurement of Long-Range Correlated Noise", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range correlated errors can severely impact the performance of NISQ\n(noisy intermediate-scale quantum) devices, and fault-tolerant quantum\ncomputation. Characterizing these errors is important for improving the\nperformance of these devices, via calibration and error correction, and to\nensure correct interpretation of the results. We propose a compressed sensing\nmethod for detecting two-qubit correlated dephasing errors, assuming only that\nthe correlations are sparse (i.e., at most s pairs of qubits have correlated\nerrors, where s << n(n-1)/2, and n is the total number of qubits). In\nparticular, our method can detect long-range correlations between any two\nqubits in the system (i.e., the correlations are not restricted to be\ngeometrically local).\n  Our method is highly scalable: it requires as few as m = O(s log n)\nmeasurement settings, and efficient classical postprocessing based on convex\noptimization. In addition, when m = O(s log^4(n)), our method is highly robust\nto noise, and has sample complexity O(max(n,s)^2 log^4(n)), which can be\ncompared to conventional methods that have sample complexity O(n^3). Thus, our\nmethod is advantageous when the correlations are sufficiently sparse, that is,\nwhen s < O(n^(3/2) / log^2(n)). Our method also performs well in numerical\nsimulations on small system sizes, and has some resistance to\nstate-preparation-and-measurement (SPAM) errors. The key ingredient in our\nmethod is a new type of compressed sensing measurement, which works by\npreparing entangled Greenberger-Horne-Zeilinger states (GHZ states) on random\nsubsets of qubits, and measuring their decay rates with high precision.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 14:47:26 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Seif", "Alireza", ""], ["Hafezi", "Mohammad", ""], ["Liu", "Yi-Kai", ""]]}, {"id": "2105.12615", "submitter": "Jonathan Hehir", "authors": "Jonathan Hehir, Aleksandra Slavkovic, Xiaoyue Niu", "title": "Consistency of Privacy-Preserving Spectral Clustering under the\n  Stochastic Block Model", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) -- a network model featuring community\nstructure -- is often selected as the fundamental setting in which to analyze\nthe theoretical properties of community detection methods. We consider the\nproblem of privacy-preserving spectral clustering of SBMs under\n$\\varepsilon$-edge differential privacy (DP) for networks, and offer practical\ninterpretations from both the central-DP and local-DP perspectives. Using a\nrandomized response privacy mechanism called the edge-flip mechanism, we take a\nfirst step toward theoretical analysis of differentially private community\ndetection by demonstrating conditions under which this strong privacy guarantee\ncan be upheld while achieving spectral clustering convergence rates that match\nthe known rates without privacy. We prove the strongest theoretical results are\nachievable for dense networks (those with node degree linear in the number of\nnodes), while weak consistency is achievable under mild sparsity (node degree\ngreater than $n^{-1/2}$). We empirically demonstrate our results on a number of\nnetwork examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:09:46 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hehir", "Jonathan", ""], ["Slavkovic", "Aleksandra", ""], ["Niu", "Xiaoyue", ""]]}, {"id": "2105.12778", "submitter": "George Wynne", "authors": "George Wynne and Stanislav Nagy", "title": "Statistical Depth Meets Machine Learning: Kernel Mean Embeddings and\n  Depth in Functional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical depth is the act of gauging how representative a point is\ncompared to a reference probability measure. The depth allows introducing\nrankings and orderings to data living in multivariate, or function spaces.\nThough widely applied and with much experimental success, little theoretical\nprogress has been made in analysing functional depths. This article highlights\nhow the common $h$-depth and related statistical depths for functional data can\nbe viewed as a kernel mean embedding, a technique used widely in statistical\nmachine learning. This connection facilitates answers to open questions\nregarding statistical properties of functional depths, as well as it provides a\nlink between the depth and empirical characteristic function based procedures\nfor functional data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:22:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wynne", "George", ""], ["Nagy", "Stanislav", ""]]}, {"id": "2105.12793", "submitter": "Veronika Rockova", "authors": "Veronika Rockova and Judith Rousseau", "title": "Ideal Bayesian Spatial Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life applications involve estimation of curves that exhibit\ncomplicated shapes including jumps or varying-frequency oscillations. Practical\nmethods have been devised that can adapt to a locally varying complexity of an\nunknown function (e.g. variable-knot splines, sparse wavelet reconstructions,\nkernel methods or trees/forests). However, the overwhelming majority of\nexisting asymptotic minimaxity theory is predicated on homogeneous smoothness\nassumptions. Focusing on locally Holderian functions, we provide new locally\nadaptive posterior concentration rate results under the supremum loss for\nwidely used Bayesian machine learning techniques in white noise and\nnon-parametric regression. In particular, we show that popular spike-and-slab\npriors and Bayesian CART are uniformly locally adaptive. In addition, we\npropose a new class of repulsive partitioning priors which relate to variable\nknot splines and which are exact-rate adaptive. For uncertainty quantification,\nwe construct locally adaptive confidence bands whose width depends on the local\nsmoothness and which achieve uniform asymptotic coverage under local\nself-similarity. To illustrate that spatial adaptation is not at all automatic,\nwe provide lower-bound results showing that popular hierarchical Gaussian\nprocess priors fall short of spatial adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:59:42 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Rockova", "Veronika", ""], ["Rousseau", "Judith", ""]]}, {"id": "2105.12972", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Tight Lower Bounds for $\\alpha$-Divergences Under Moment Constraints and\n  Relations Between Different $\\alpha$", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\alpha$-divergences include the well-known Kullback-Leibler divergence,\nHellinger distance and $\\chi^2$-divergence. In this paper, we derive\ndifferential and integral relations between the $\\alpha$-divergences that are\ngeneralizations of the relation between the Kullback-Leibler divergence and the\n$\\chi^2$-divergence. We also show tight lower bounds for the\n$\\alpha$-divergences under given means and variances. In particular, we show a\nnecessary and sufficient condition such that the binary divergences, which are\ndivergences between probability measures on the same $2$-point set, always\nattain lower bounds. Kullback-Leibler divergence, Hellinger distance, and\n$\\chi^2$-divergence satisfy this condition.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:29:21 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 07:31:49 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 12:56:46 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "2105.12975", "submitter": "Long Yu", "authors": "Long Yu, Jiahui Xie and Wang Zhou", "title": "Central Limit Theory for Linear Spectral Statistics of Normalized\n  Separable Sample Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper focuses on the separable covariance matrix when the dimension $p$\nand the sample size $n$ grow to infinity but the ratio $p/n$ tends to zero. The\nseparable sample covariance matrix can be written as $n^{-1}A^{1/2}XBX^\\top\nA^{1/2}$, where $A$ and $B$ correspond to the cross-row and cross-column\ncorrelations, respectively. We normalize the separable sample covariance matrix\nand prove the central limit theorem for corresponding linear spectral\nstatistics, with explicit formulas for the mean and covariance function. We\napply the results to testing the correlations of a large number of variables\nwith two common examples, related to spatial-temporal model and matrix-variate\nmodel, which are beyond the scenarios considered in existing studies. The\nasymptotic sizes and powers are studied under mild conditions. The computations\nof the asymptotic mean and variance are involved under the null hypothesis\nwhere $A$ is the identity matrix, with simplified expressions which facilitate\nto practical usefulness. Extensive numerical studies show that the proposed\ntesting statistic performs sufficiently reliably under both the null and\nalternative hypothesis, while a conventional competitor fails to control\nempirical sizes.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:33:46 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:34:12 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yu", "Long", ""], ["Xie", "Jiahui", ""], ["Zhou", "Wang", ""]]}, {"id": "2105.12978", "submitter": "Antoine Barrier", "authors": "Antoine Barrier (UMPA-ENSL, LMO), Aur\\'elien Garivier (UMPA-ENSL),\n  Tom\\'a\\v{s} Koc\\'ak (UMPA-ENSL)", "title": "A Non-asymptotic Approach to Best-Arm Identification for Gaussian\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new strategy for best-arm identification with fixed confidence\nof Gaussian variables with bounded means and unit variance. This strategy\ncalled Exploration-Biased Sampling is not only asymptotically optimal: we also\nprove non-asymptotic bounds occurring with high probability. To the best of our\nknowledge, this is the first strategy with such guarantees. But the main\nadvantage over other algorithms like Track-and-Stop is an improved behavior\nregarding exploration: Exploration-Biased Sampling is slightly biased in favor\nof exploration in a subtle but natural way that makes it more stable and\ninterpretable. These improvements are allowed by a new analysis of the sample\ncomplexity optimization problem, which yields a faster numerical resolution\nscheme and several quantitative regularity results that we believe of high\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 07:42:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Barrier", "Antoine", "", "UMPA-ENSL, LMO"], ["Garivier", "Aur\u00e9lien", "", "UMPA-ENSL"], ["Koc\u00e1k", "Tom\u00e1\u0161", "", "UMPA-ENSL"]]}, {"id": "2105.13010", "submitter": "Yunfei Yang", "authors": "Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, Yunfei Yang", "title": "An error analysis of generative adversarial networks for learning\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results establish the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structures or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrated around a low-dimensional set, we show that the learning rates of\nGANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into the generator and discriminator approximation error and\nthe statistical error, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:55:19 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:28:00 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 02:19:06 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 03:03:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Li", "Zhen", ""], ["Liu", "Shiao", ""], ["Wang", "Yang", ""], ["Yang", "Yunfei", ""]]}, {"id": "2105.13199", "submitter": "Alexander Lewis", "authors": "Alexander Lewis", "title": "Stein's Method for Probability Distributions on $\\mathbb{S}^1$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a modification to the density approach to Stein's\nmethod for intervals for the unit circle $\\mathbb{S}^1$ which is motivated by\nthe differing geometry of $\\mathbb{S}^1$ to Euclidean space. We provide an\nupper bound to the Wasserstein metric for circular distributions and exhibit a\nvariety of different bounds between distributions; particularly, between the\nvon-Mises and wrapped normal distributions, and the wrapped normal and wrapped\nCauchy distributions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:48:40 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lewis", "Alexander", ""]]}, {"id": "2105.13302", "submitter": "Zhiqi Bu", "authors": "Zhiqi Bu, Jason Klusowski, Cynthia Rush, Weijie J. Su", "title": "Characterizing the SLOPE Trade-off: A Variational Perspective and the\n  Donoho-Tanner Limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted l1 regularization has been incorporated into many methods for solving\nhigh-dimensional statistical estimation problems, including the SLOPE estimator\nin linear regression. In this paper, we study how this relatively new\nregularization technique improves variable selection by characterizing the\noptimal SLOPE trade-off between the false discovery proportion (FDP) and true\npositive proportion (TPP) or, equivalently, between measures of type I error\nand power. Assuming a regime of linear sparsity and working under Gaussian\nrandom designs, we obtain an upper bound on the optimal trade-off for SLOPE,\nshowing its capability of breaking the Donoho-Tanner power limit. To put it\ninto perspective, this limit is the highest possible power that the Lasso,\nwhich is perhaps the most popular l1-based method, can achieve even with\narbitrarily strong effect sizes. Next, we derive a tight lower bound that\ndelineates the fundamental limit of sorted l1 regularization in optimally\ntrading the FDP off for the TPP. Finally, we show that on any problem instance,\nSLOPE with a certain regularization sequence outperforms the Lasso, in the\nsense of having a smaller FDP, larger TPP and smaller l2 estimation risk\nsimultaneously. Our proofs are based on a novel technique that reduces a\nvariational calculus problem to a class of infinite-dimensional convex\noptimization problems and a very recent result from approximate message passing\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:56:42 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bu", "Zhiqi", ""], ["Klusowski", "Jason", ""], ["Rush", "Cynthia", ""], ["Su", "Weijie J.", ""]]}, {"id": "2105.13342", "submitter": "Marie-Christine D\\\"uker", "authors": "Changryong Baek and Marie-Christine D\\\"uker and Vladas Pipiras", "title": "Thresholding and graphical local Whittle estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops non-asymptotic theory for estimation of the long-run\nvariance matrix and its inverse, the so-called precision matrix, for\nhigh-dimensional Gaussian time series under general assumptions on the\ndependence structure including long-range dependence. The estimation involves\nshrinkage techniques which are thresholding and penalizing versions of the\nclassical multivariate local Whittle estimator. The results ensure consistent\nestimation in a double asymptotic regime where the number of component time\nseries is allowed to grow with the sample size as long as the true model\nparameters are sparse. The key technical result is a concentration inequality\nof the local Whittle estimator for the long-run variance matrix around the true\nmodel parameters. In particular, it handles simultaneously the estimation of\nthe memory parameters which enter the underlying model. Novel algorithms for\nthe considered procedures are proposed, and a simulation study and a data\napplication are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:49:39 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Baek", "Changryong", ""], ["D\u00fcker", "Marie-Christine", ""], ["Pipiras", "Vladas", ""]]}, {"id": "2105.13346", "submitter": "Joshua Agterberg", "authors": "Joshua Agterberg, Zachary Lubberts, and Carey Priebe", "title": "Entrywise Estimation of Singular Vectors of Low-Rank Matrices with\n  Heteroskedasticity and Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.SP stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an estimator for the singular vectors of high-dimensional low-rank\nmatrices corrupted by additive subgaussian noise, where the noise matrix is\nallowed to have dependence within rows and heteroskedasticity between them. We\nprove finite-sample $\\ell_{2,\\infty}$ bounds and a Berry-Esseen theorem for the\nindividual entries of the estimator, and we apply these results to\nhigh-dimensional mixture models. Our Berry-Esseen theorem clearly shows the\ngeometric relationship between the signal matrix, the covariance structure of\nthe noise, and the distribution of the errors in the singular vector estimation\ntask. These results are illustrated in numerical simulations. Unlike previous\nresults of this type, which rely on assumptions of gaussianity or independence\nbetween the entries of the additive noise, handling the dependence between\nentries in the proofs of these results requires careful leave-one-out analysis\nand conditioning arguments. Our results depend only on the signal-to-noise\nratio, the sample size, and the spectral properties of the signal matrix.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:51:50 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 13:29:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Agterberg", "Joshua", ""], ["Lubberts", "Zachary", ""], ["Priebe", "Carey", ""]]}, {"id": "2105.13445", "submitter": "Christopher Tosh", "authors": "Christopher Tosh, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki\n  Vehtari, Daniel Hsu", "title": "The piranha problem: Large effects swimming in a small pond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some scientific fields, it is common to have certain variables of interest\nthat are of particular importance and for which there are many studies\nindicating a relationship with a different explanatory variable. In such cases,\nparticularly those where no relationships are known among explanatory\nvariables, it is worth asking under what conditions it is possible for all such\nclaimed effects to exist simultaneously. This paper addresses this question by\nreviewing some theorems from multivariate analysis that show, unless the\nexplanatory variables also have sizable effects on each other, it is impossible\nto have many such large effects. We also discuss implications for the\nreplication crisis in social science.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:56:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tosh", "Christopher", ""], ["Greengard", "Philip", ""], ["Goodrich", "Ben", ""], ["Gelman", "Andrew", ""], ["Vehtari", "Aki", ""], ["Hsu", "Daniel", ""]]}, {"id": "2105.13504", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Yi Yu, Alessandro Rinaldo", "title": "Lattice partition recovery with dyadic CART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study piece-wise constant signals corrupted by additive Gaussian noise\nover a $d$-dimensional lattice. Data of this form naturally arise in a host of\napplications, and the tasks of signal detection or testing, de-noising and\nestimation have been studied extensively in the statistical and signal\nprocessing literature. In this paper we consider instead the problem of\npartition recovery, i.e.~of estimating the partition of the lattice induced by\nthe constancy regions of the unknown signal, using the\ncomputationally-efficient dyadic classification and regression tree (DCART)\nmethodology proposed by \\citep{donoho1997cart}. We prove that, under\nappropriate regularity conditions on the shape of the partition elements, a\nDCART-based procedure consistently estimates the underlying partition at a rate\nof order $\\sigma^2 k^* \\log (N)/\\kappa^2$, where $k^*$ is the minimal number of\nrectangular sub-graphs obtained using recursive dyadic partitions supporting\nthe signal partition, $\\sigma^2$ is the noise variance, $\\kappa$ is the minimal\nmagnitude of the signal difference among contiguous elements of the partition\nand $N$ is the size of the lattice. Furthermore, under stronger assumptions,\nour method attains a sharper estimation error of order\n$\\sigma^2\\log(N)/\\kappa^2$, independent of $ k^*$, which we show to be minimax\nrate optimal. Our theoretical guarantees further extend to the partition\nestimator based on the optimal regression tree estimator (ORT) of\n\\cite{chatterjee2019adaptive} and to the one obtained through an NP-hard\nexhaustive search method. We corroborate our theoretical findings and the\neffectiveness of DCART for partition recovery in simulations.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 23:41:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2105.13724", "submitter": "Kostiantyn Ralchenko", "authors": "Yuliya Mishura, Kostiantyn Ralchenko, Olena Dehtiar", "title": "Parameter estimation in CKLS model by continuous observations", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic differential equation of the form $dr_t = (a - b\nr_t) dt + \\sigma r_t^\\beta dW_t$, where $a$, $b$ and $\\sigma$ are positive\nconstants, $\\beta\\in(\\frac12,1)$. We study the estimation of an unknown drift\nparameter $(a,b)$ by continuous observations of a sample path $\\{r_t, t \\in\n[0,T]\\}$. We prove the strong consistency and asymptotic normality of the\nmaximum likelihood estimator. We propose another strongly consistent estimator,\nwhich generalizes an estimator proposed in Dehtiar et al. (2021) for\n$\\beta=\\frac12$. The identification of the diffusion parameters $\\sigma$ and\n$\\beta$ is discussed as well.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:45:07 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Mishura", "Yuliya", ""], ["Ralchenko", "Kostiantyn", ""], ["Dehtiar", "Olena", ""]]}, {"id": "2105.13747", "submitter": "Swarnadip Ghosh", "authors": "Swarnadip Ghosh, Trevor Hastie and Art B. Owen", "title": "Scalable logistic regression with crossed random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The cost of both generalized least squares (GLS) and Gibbs sampling in a\ncrossed random effects model can easily grow faster than $N^{3/2}$ for $N$\nobservations. Ghosh et al. (2020) develop a backfitting algorithm that reduces\nthe cost to $O(N)$. Here we extend that method to a generalized linear mixed\nmodel for logistic regression. We use backfitting within an iteratively\nreweighted penalized least square algorithm. The specific approach is a version\nof penalized quasi-likelihood due to Schall (1991). A straightforward version\nof Schall's algorithm would also cost more than $N^{3/2}$ because it requires\nthe trace of the inverse of a large matrix. We approximate that quantity at\ncost $O(N)$ and prove that this substitution makes an asymptotically negligible\ndifference. Our backfitting algorithm also collapses the fixed effect with one\nrandom effect at a time in a way that is analogous to the collapsed Gibbs\nsampler of Papaspiliopoulos et al. (2020). We use a symmetric operator that\nfacilitates efficient covariance computation. We illustrate our method on a\nreal dataset from Stitch Fix. By properly accounting for crossed random effects\nwe show that a naive logistic regression could underestimate sampling variances\nby several hundred fold.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:32:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2105.13836", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop, William Kengne", "title": "Epidemic change-point detection in general causal time series", "comments": "arXiv admin note: text overlap with arXiv:2103.13336", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider an epidemic change-point detection in a large class of causal\ntime series models, including among other processes, AR($\\infty$),\nARCH($\\infty$), TARCH($\\infty$), ARMA-GARCH. A test statistic based on the\nGaussian quasi-maximum likelihood estimator of the parameter is proposed. It is\nshown that, under the null hypothesis of no change, the test statistic\nconverges to a distribution obtained from a difference of two Brownian bridge\nand diverges to infinity under the epidemic alternative. Numerical results for\nsimulation and real data example are provided.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:50:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2105.14016", "submitter": "Bingyan Wang", "authors": "Bingyan Wang, Yuling Yan, Jianqing Fan", "title": "Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs\n  with a Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of dimensionality is a widely known issue in reinforcement learning\n(RL). In the tabular setting where the state space $\\mathcal{S}$ and the action\nspace $\\mathcal{A}$ are both finite, to obtain a nearly optimal policy with\nsampling access to a generative model, the minimax optimal sample complexity\nscales linearly with $|\\mathcal{S}|\\times|\\mathcal{A}|$, which can be\nprohibitively large when $\\mathcal{S}$ or $\\mathcal{A}$ is large. This paper\nconsiders a Markov decision process (MDP) that admits a set of state-action\nfeatures, which can linearly express (or approximate) its probability\ntransition kernel. We show that a model-based approach (resp.$~$Q-learning)\nprovably learns an $\\varepsilon$-optimal policy (resp.$~$Q-function) with high\nprobability as soon as the sample size exceeds the order of\n$\\frac{K}{(1-\\gamma)^{3}\\varepsilon^{2}}$\n(resp.$~$$\\frac{K}{(1-\\gamma)^{4}\\varepsilon^{2}}$), up to some logarithmic\nfactor. Here $K$ is the feature dimension and $\\gamma\\in(0,1)$ is the discount\nfactor of the MDP. Both sample complexity bounds are provably tight, and our\nresult for the model-based approach matches the minimax lower bound. Our\nresults show that for arbitrarily large-scale MDP, both the model-based\napproach and Q-learning are sample-efficient when $K$ is relatively small, and\nhence the title of this paper.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:49:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wang", "Bingyan", ""], ["Yan", "Yuling", ""], ["Fan", "Jianqing", ""]]}, {"id": "2105.14045", "submitter": "Peter Hoff", "authors": "Peter Hoff", "title": "Bayes-optimal prediction with frequentist coverage control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article illustrates how indirect or prior information can be optimally\nused to construct a prediction region that maintains a target frequentist\ncoverage rate. If the indirect information is accurate, the volume of the\nprediction region is lower on average than that of other regions with the same\ncoverage rate. Even if the indirect information is inaccurate, the resulting\nregion still maintains the target coverage rate. Such a prediction region can\nbe constructed for models that have a complete sufficient statistic, which\nincludes many widely-used parametric and nonparametric models. Particular\nexamples include a Bayes-optimal conformal prediction procedure that maintains\na constant coverage rate across distributions in a nonparametric model, as well\nas a prediction procedure for the normal linear regression model that can\nutilize a regularizing prior distribution, yet maintain a frequentist coverage\nrate that is constant as a function of the model parameters and explanatory\nvariables. No results in this article rely on asymptotic approximations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:23:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hoff", "Peter", ""]]}, {"id": "2105.14075", "submitter": "Yonghoon Lee", "authors": "Yonghoon Lee, Rina Foygel Barber", "title": "Distribution-free inference for regression: discrete, continuous, and in\n  between", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data analysis problems where we are not able to rely on distributional\nassumptions, what types of inference guarantees can still be obtained? Many\npopular methods, such as holdout methods, cross-validation methods, and\nconformal prediction, are able to provide distribution-free guarantees for\npredictive inference, but the problem of providing inference for the underlying\nregression function (for example, inference on the conditional mean\n$\\mathbb{E}[Y|X]$) is more challenging. In the setting where the features $X$\nare continuously distributed, recent work has established that any confidence\ninterval for $\\mathbb{E}[Y|X]$ must have non-vanishing width, even as sample\nsize tends to infinity. At the other extreme, if $X$ takes only a small number\nof possible values, then inference on $\\mathbb{E}[Y|X]$ is trivial to achieve.\nIn this work, we study the problem in settings in between these two extremes.\nWe find that there are several distinct regimes in between the finite setting\nand the continuous setting, where vanishing-width confidence intervals are\nachievable if and only if the effective support size of the distribution of $X$\nis smaller than the square of the sample size.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:40:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Yonghoon", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "2105.14084", "submitter": "Clayton Sanford", "authors": "Navid Ardeshir, Clayton Sanford, Daniel Hsu", "title": "Support vector machines and linear regression coincide with very\n  high-dimensional features", "comments": "32 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The support vector machine (SVM) and minimum Euclidean norm least squares\nregression are two fundamentally different approaches to fitting linear models,\nbut they have recently been connected in models for very high-dimensional data\nthrough a phenomenon of support vector proliferation, where every training\nexample used to fit an SVM becomes a support vector. In this paper, we explore\nthe generality of this phenomenon and make the following contributions. First,\nwe prove a super-linear lower bound on the dimension (in terms of sample size)\nrequired for support vector proliferation in independent feature models,\nmatching the upper bounds from previous works. We further identify a sharp\nphase transition in Gaussian feature models, bound the width of this\ntransition, and give experimental support for its universality. Finally, we\nhypothesize that this phase transition occurs only in much higher-dimensional\nsettings in the $\\ell_1$ variant of the SVM, and we present a new geometric\ncharacterization of the problem that may elucidate this phenomenon for the\ngeneral $\\ell_p$ case.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:06:21 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ardeshir", "Navid", ""], ["Sanford", "Clayton", ""], ["Hsu", "Daniel", ""]]}, {"id": "2105.14163", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Patrik Gerber, Chen Lu, Thibaut Le Gouic, Philippe\n  Rigollet", "title": "The query complexity of sampling from strongly log-concave distributions\n  in one dimension", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the first tight lower bound of $\\Omega(\\log\\log\\kappa)$ on the\nquery complexity of sampling from the class of strongly log-concave and\nlog-smooth distributions with condition number $\\kappa$ in one dimension.\nWhereas existing guarantees for MCMC-based algorithms scale polynomially in\n$\\kappa$, we introduce a novel algorithm based on rejection sampling that\ncloses this doubly exponential gap.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:51:17 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:32:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chewi", "Sinho", ""], ["Gerber", "Patrik", ""], ["Lu", "Chen", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2105.14166", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Patrik Gerber, Chen Lu, Thibaut Le Gouic, Philippe\n  Rigollet", "title": "Rejection sampling from shape-constrained distributions in sublinear\n  time", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating exact samples from a target distribution,\nknown up to normalization, over a finite alphabet. The classical algorithm for\nthis task is rejection sampling, and although it has been used in practice for\ndecades, there is surprisingly little study of its fundamental limitations. In\nthis work, we study the query complexity of rejection sampling in a minimax\nframework for various classes of discrete distributions. Our results provide\nnew algorithms for sampling whose complexity scales sublinearly with the\nalphabet size. When applied to adversarial bandits, we show that a slight\nmodification of the Exp3 algorithm reduces the per-iteration complexity from\n$\\mathcal O(K)$ to $\\mathcal O(\\log^2 K)$, where $K$ is the number of arms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:00:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chewi", "Sinho", ""], ["Gerber", "Patrik", ""], ["Lu", "Chen", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2105.14187", "submitter": "Martina Mammarella Dr.", "authors": "Victor Mirasierra, Martina Mammarella, Fabrizio Dabbene, Teodoro Alamo", "title": "Prediction error quantification through probabilistic scaling --\n  EXTENDED VERSION", "comments": "8 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY eess.SY stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the probabilistic error quantification of a general\nclass of prediction methods. We consider a given prediction model and show how\nto obtain, through a sample-based approach, a probabilistic upper bound on the\nabsolute value of the prediction error. The proposed scheme is based on a\nprobabilistic scaling methodology in which the number of required randomized\nsamples is independent of the complexity of the prediction model. The\nmethodology is extended to address the case in which the probabilistic\nuncertain quantification is required to be valid for every member of a finite\nfamily of predictors. We illustrate the results of the paper by means of a\nnumerical example.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:16:46 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 19:32:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Mirasierra", "Victor", ""], ["Mammarella", "Martina", ""], ["Dabbene", "Fabrizio", ""], ["Alamo", "Teodoro", ""]]}, {"id": "2105.14229", "submitter": "Peng Li", "authors": "Huanmin Ge and Peng Li", "title": "The Dantzig selector: Recovery of Signal via $\\ell_1-\\alpha \\ell_2$\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we proposed the Dantzig selector based on the $\\ell_{1}-\\alpha\n\\ell_{2}$~$(0< \\alpha \\leq1)$ minimization for the signal recovery. In the\nDantzig selector, the constraint $\\|{\\bf A}^{\\top}({\\bf b}-{\\bf A}{\\bf\nx})\\|_\\infty \\leq \\eta$ for some small constant $\\eta>0$ means the columns of\n${\\bf A}$ has very weakly correlated with the error vector ${\\bf e}={\\bf A}{\\bf\nx}-{\\bf b}$. First, recovery guarantees based on the restricted isometry\nproperty (RIP) are established for signals. Next, we propose the effective\nalgorithm to solve the proposed Dantzig selector. Last, we illustrate the\nproposed model and algorithm by extensive numerical experiments for the\nrecovery of signals in the cases of Gaussian, impulsive and uniform noise. And\nthe performance of the proposed Dantzig selector is better than that of the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 06:40:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ge", "Huanmin", ""], ["Li", "Peng", ""]]}, {"id": "2105.14267", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Wei Deng", "title": "Information Directed Sampling for Sparse Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic sparse linear bandits offer a practical model for high-dimensional\nonline decision-making problems and have a rich information-regret structure.\nIn this work we explore the use of information-directed sampling (IDS), which\nnaturally balances the information-regret trade-off. We develop a class of\ninformation-theoretic Bayesian regret bounds that nearly match existing lower\nbounds on a variety of problem instances, demonstrating the adaptivity of IDS.\nTo efficiently implement sparse IDS, we propose an empirical Bayesian approach\nfor sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior.\nNumerical results demonstrate significant regret reductions by sparse IDS\nrelative to several baselines.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 10:26:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Deng", "Wei", ""]]}, {"id": "2105.14348", "submitter": "Liyan Xie", "authors": "Liyan Xie, Rui Gao, and Yao Xie", "title": "Robust Hypothesis Testing with Wasserstein Uncertainty Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a data-driven robust hypothesis test where the optimal test will\nminimize the worst-case performance regarding distributions that are close to\nthe empirical distributions with respect to the Wasserstein distance. This\nleads to a new non-parametric hypothesis testing framework based on\ndistributionally robust optimization, which is more robust when there are\nlimited samples for one or both hypotheses. Such a scenario often arises from\napplications such as health care, online change-point detection, and anomaly\ndetection. We study the computational and statistical properties of the\nproposed test by presenting a tractable convex reformulation of the original\ninfinite-dimensional variational problem exploiting Wasserstein's properties\nand characterizing the radii selection for the uncertainty sets. We also\ndemonstrate the good performance of our method on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 18:16:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xie", "Liyan", ""], ["Gao", "Rui", ""], ["Xie", "Yao", ""]]}, {"id": "2105.14368", "submitter": "Mikhail Belkin", "authors": "Mikhail Belkin", "title": "Fit without fear: remarkable mathematical phenomena of deep learning\n  through the prism of interpolation", "comments": "A version of this paper will appear in Acta Numerica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade the mathematical theory of machine learning has lagged far\nbehind the triumphs of deep neural networks on practical challenges. However,\nthe gap between theory and practice is gradually starting to close. In this\npaper I will attempt to assemble some pieces of the remarkable and still\nincomplete mathematical mosaic emerging from the efforts to understand the\nfoundations of deep learning. The two key themes will be interpolation, and its\nsibling, over-parameterization. Interpolation corresponds to fitting data, even\nnoisy data, exactly. Over-parameterization enables interpolation and provides\nflexibility to select a right interpolating model.\n  As we will see, just as a physical prism separates colors mixed within a ray\nof light, the figurative prism of interpolation helps to disentangle\ngeneralization and optimization properties within the complex picture of modern\nMachine Learning. This article is written with belief and hope that clearer\nunderstanding of these issues brings us a step closer toward a general theory\nof deep learning and machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 20:15:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Belkin", "Mikhail", ""]]}, {"id": "2105.14394", "submitter": "Sanvesh Srivastava", "authors": "Chunlei Wang and Sanvesh Srivastava", "title": "Asymptotic Normality of the Posterior Distributions in a Class of Hidden\n  Markov Models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the posterior distribution of parameters in a hidden Markov\nmodel with parametric emission distributions and discrete and known state space\nis asymptotically normal. The main novelty of our proof is that it is based on\na testing condition and the sequence of test functions is obtained using an\noptimal transportation inequality.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:11:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Chunlei", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "2105.14486", "submitter": "Jacek Wesolowski", "authors": "Jacek Weso{\\l}owski, Robert Wieczorkowski, Wojciech W\\'ojciak", "title": "Optimality of the recursive Neyman allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a formula for the optimal sample allocation in a general stratified\nscheme under upper bounds on the sample strata-sizes. Such a general scheme\nincludes SRSWOR within strata as a special case. The solution is given in terms\nof V-allocation with V being the set of take-all strata. We use V-allocation to\ngive a formal proof of optimality of the popular recursive Neyman algorithm,\nrNa. This approach is convenient also for a quick proof of optimality of the\nalgorithm of Stenger and Gabler (2005), SGa, as well as of its modification,\ncoma, we propose here. Finally, we compare running times of rNa, SGa and coma.\nReady-to-use R-implementations of these algorithms are available on CRAN\nrepository at https://cran.r-project.org/web/packages/stratallo.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:53:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Weso\u0142owski", "Jacek", ""], ["Wieczorkowski", "Robert", ""], ["W\u00f3jciak", "Wojciech", ""]]}, {"id": "2105.14558", "submitter": "Fatemeh Mohammadi", "authors": "Peter Caines, Fatemeh Mohammadi, Eduardo S\\'aenz de Cabez\\'on, and\n  Henry Wynn", "title": "Lattice Conditional Independence Models and Hibi Ideals", "comments": "comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattice Conditional Independence models are a class of models developed first\nfor the Gaussian case in which a distributive lattice classifies all the\nconditional independence statements. The main result is that these models can\nequivalently be described via a transitive acyclic graph (TDAG) in which, as is\nnormal for causal models, the conditional independence is in terms of\nconditioning on ancestors in the graph. We aim to demonstrate that a parallel\nstream of research in algebra, the theory of Hibi ideals, not only maps\ndirectly to the LCI models but gives a vehicle to generalise the theory from\nthe linear Gaussian case. Given a distributive lattice (i) each conditional\nindependence statement is associated with a Hibi relation defined on the\nlattice, (ii) the directed graph is given by chains in the lattice which\ncorrespond to chains of conditional independence, (iii) the elimination ideal\nof product terms in the chains gives the Hibi ideal and (iv) the TDAG can be\nrecovered from a special bipartite graph constructed via the Alexander dual of\nthe Hibi ideal. It is briefly demonstrated that there are natural applications\nto statistical log-linear models, time series, and Shannon information flow.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:45:53 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Caines", "Peter", ""], ["Mohammadi", "Fatemeh", ""], ["de Cabez\u00f3n", "Eduardo S\u00e1enz", ""], ["Wynn", "Henry", ""]]}, {"id": "2105.14577", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla, Sivaraman Balakrishnan, Larry Wasserman", "title": "The HulC: Confidence Regions from Convex Hulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze the HulC, an intuitive and general method for\nconstructing confidence sets using the convex hull of estimates constructed\nfrom subsets of the data. Unlike classical methods which are based on\nestimating the (limiting) distribution of an estimator, the HulC is often\nsimpler to use and effectively bypasses this step. In comparison to the\nbootstrap, the HulC requires fewer regularity conditions and succeeds in many\nexamples where the bootstrap provably fails. Unlike subsampling, the HulC does\nnot require knowledge of the rate of convergence of the estimators on which it\nis based. The validity of the HulC requires knowledge of the (asymptotic)\nmedian-bias of the estimators. We further analyze a variant of our basic\nmethod, called the Adaptive HulC, which is fully data-driven and estimates the\nmedian-bias using subsampling. We show that the Adaptive HulC retains the\naforementioned strengths of the HulC. In certain cases where the underlying\nestimators are pathologically asymmetric the HulC and Adaptive HulC can fail to\nprovide useful confidence sets. We propose a final variant, the Unimodal HulC,\nwhich can salvage the situation in cases where the distribution of the\nunderlying estimator is (asymptotically) unimodal. We discuss these methods in\nthe context of several challenging inferential problems which arise in\nparametric, semi-parametric, and non-parametric inference. Although our focus\nis on validity under weak regularity conditions, we also provide some general\nresults on the width of the HulC confidence sets, showing that in many cases\nthe HulC confidence sets have near-optimal width.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 16:21:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2105.14596", "submitter": "Yotam Leibovici", "authors": "Yotam Leibovici (1) and Yair Goldberg (1) ((1) Technion - Israel\n  Institute of Technology)", "title": "Improving Efficiency of Tests for Composite Null Hypotheses", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of mediation analysis is to study the effect of exposure on an\noutcome interceded by a mediator. Two simple hypotheses are tested: the effect\nof the exposure on the mediator, and the effect of the mediator on the outcome.\nWhen either of these hypotheses is true, a predetermined significance level can\nbe assured. When both nulls are true, the same test becomes conservative.\nAdaptively finding the correct scenario enables customizing the tests and\nconsequently enlarges their efficiency, which is most important in a multiple\ntesting framework. In this work, we link between adaptive two-stage procedures\nand shrinkage estimators. We first study the properties of shrinkage\nestimators, and characterize their behavior at different parameter points using\nlocal asymptotics. We formulate theoretical results regarding shrinkage\nestimators, compared to regular estimators. We then discuss the\nmultiple-testing framework and state results about using shrinkage estimator in\ntwo-stage procedures for controlling the FWER. Taking advantage of these\ntheoretical results, we suggest a number of estimators and test statistics for\nthe two-stage mediation procedures. We then investigate their empirical FWER\nand power, compared to regular estimators and tests, through simulations.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 18:22:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Leibovici", "Yotam", ""], ["Goldberg", "Yair", ""]]}, {"id": "2105.14673", "submitter": "Waheed Bajwa", "authors": "Batoul Taki, Mohsen Ghassemi, Anand D. Sarwate, and Waheed U. Bajwa", "title": "A Minimax Lower Bound for Low-Rank Matrix-Variate Logistic Regression", "comments": "8 pages; preprint of a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of matrix-variate logistic regression. The\nfundamental error threshold on estimating coefficient matrices in the logistic\nregression problem is found by deriving a lower bound on the minimax risk. The\nfocus of this paper is on derivation of a minimax risk lower bound for low-rank\ncoefficient matrices. The bound depends explicitly on the dimensions and\ndistribution of the covariates, the rank and energy of the coefficient matrix,\nand the number of samples. The resulting bound is proportional to the intrinsic\ndegrees of freedom in the problem, which suggests the sample complexity of the\nlow-rank matrix logistic regression problem can be lower than that for\nvectorized logistic regression. \\color{red}\\color{black} The proof techniques\nutilized in this work also set the stage for development of minimax lower\nbounds for tensor-variate logistic regression problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:06:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Taki", "Batoul", ""], ["Ghassemi", "Mohsen", ""], ["Sarwate", "Anand D.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2105.14849", "submitter": "Albert Zeyer", "authors": "Albert Zeyer and Ralf Schl\\\"uter and Hermann Ney", "title": "Why does CTC result in peaky behavior?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.NE cs.SD eess.AS math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The peaky behavior of CTC models is well known experimentally. However, an\nunderstanding about why peaky behavior occurs is missing, and whether this is a\ngood property. We provide a formal analysis of the peaky behavior and gradient\ndescent convergence properties of the CTC loss and related training criteria.\nOur analysis provides a deep understanding why peaky behavior occurs and when\nit is suboptimal. On a simple example which should be trivial to learn for any\nmodel, we prove that a feed-forward neural network trained with CTC from\nuniform initialization converges towards peaky behavior with a 100% error rate.\nOur analysis further explains why CTC only works well together with the blank\nlabel. We further demonstrate that peaky behavior does not occur on other\nrelated losses including a label prior model, and that this improves\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:03:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 21:44:23 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2105.14893", "submitter": "Fatima Antarou Ba", "authors": "Johannes Hertrich, Fatima Antarou Ba, Gabriele Steidl", "title": "Sparse ANOVA Inspired Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the analysis of variance (ANOVA) decomposition of functions which\nrelies on the assumption that the function we wish to approximate can be well\nexplained by limited variable interaction, we propose a sparse Gaussian-like\nmixture model on the high dimensional torus. We consider three approaches,\nnamely wrapped Gaussians, diagonal wrapped Gaussians and products of von Mises\ndistributions. The sparsity of the mixture model is ensured by the fact that\nits summands are products of Gaussian-like density functions acting on low\ndimensional spaces and uniform probability densities defined on the remaining\ndirections. To learn such a sparse mixture model from given samples, we propose\nan objective function consisting of the negative log-likelihood function of the\nmixture model and a regularizer that penalizes the number of its summands. For\nminimizing this functional we combine the Expectation Maximization algorithm\nwith a proximal step that takes the regularizer into account. To decide which\nsummands of the mixture model are important, we apply a Kolmogorov-Smirnov\ntest. Numerical example demonstrate the performance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:34:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hertrich", "Johannes", ""], ["Ba", "Fatima Antarou", ""], ["Steidl", "Gabriele", ""]]}, {"id": "2105.15081", "submitter": "Alexander Wein", "authors": "Cheng Mao, Alexander S. Wein", "title": "Optimal Spectral Recovery of a Planted Vector in a Subspace", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a planted vector $v$ in an $n$-dimensional random subspace of\n$\\mathbb{R}^N$ is a generic task related to many problems in machine learning\nand statistics, such as dictionary learning, subspace recovery, and principal\ncomponent analysis. In this work, we study computationally efficient estimation\nand detection of a planted vector $v$ whose $\\ell_4$ norm differs from that of\na Gaussian vector with the same $\\ell_2$ norm. For instance, in the special\ncase of an $N \\rho$-sparse vector $v$ with Rademacher nonzero entries, our\nresults include the following:\n  (1) We give an improved analysis of (a slight variant of) the spectral method\nproposed by Hopkins, Schramm, Shi, and Steurer, showing that it approximately\nrecovers $v$ with high probability in the regime $n \\rho \\ll \\sqrt{N}$. In\ncontrast, previous work required either $\\rho \\ll 1/\\sqrt{n}$ or $n \\sqrt{\\rho}\n\\lesssim \\sqrt{N}$ for polynomial-time recovery. Our result subsumes both of\nthese conditions (up to logarithmic factors) and also treats the dense case\n$\\rho = 1$ which was not previously considered.\n  (2) Akin to $\\ell_\\infty$ bounds for eigenvector perturbation, we establish\nan entrywise error bound for the spectral estimator via a leave-one-out\nanalysis, from which it follows that thresholding recovers $v$ exactly.\n  (3) We study the associated detection problem and show that in the regime $n\n\\rho \\gg \\sqrt{N}$, any spectral method from a large class (and more generally,\nany low-degree polynomial of the input) fails to detect the planted vector.\nThis establishes optimality of our upper bounds and offers evidence that no\npolynomial-time algorithm can succeed when $n \\rho \\gg \\sqrt{N}$.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:10:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mao", "Cheng", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2105.15197", "submitter": "Rahul Singh", "authors": "Victor Chernozhukov, Whitney K. Newey, Rahul Singh", "title": "A Simple and General Debiased Machine Learning Theorem with Finite\n  Sample Guarantees", "comments": "25 pages. arXiv admin note: text overlap with arXiv:2102.11076", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Debiased machine learning is a meta algorithm based on bias correction and\nsample splitting to calculate confidence intervals for functionals (i.e. scalar\nsummaries) of machine learning algorithms. For example, an analyst may desire\nthe confidence interval for a treatment effect estimated with a neural network.\nWe provide a nonasymptotic debiased machine learning theorem that encompasses\nany global or local functional of any machine learning algorithm that satisfies\na few simple, interpretable conditions. Formally, we prove consistency,\nGaussian approximation, and semiparametric efficiency by finite sample\narguments. The rate of convergence is root-n for global functionals, and it\ndegrades gracefully for local functionals. Our results culminate in a simple\nset of conditions that an analyst can use to translate modern learning theory\nrates into traditional statistical inference. The conditions reveal a new\ndouble robustness property for ill posed inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 17:57:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney K.", ""], ["Singh", "Rahul", ""]]}]