[{"id": "1707.00087", "submitter": "Jonathan Weed", "authors": "Jonathan Weed, Francis Bach", "title": "Sharp asymptotic and finite-sample rates of convergence of empirical\n  measures in Wasserstein distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein distance between two probability measures on a metric space\nis a measure of closeness with applications in statistics, probability, and\nmachine learning. In this work, we consider the fundamental question of how\nquickly the empirical measure obtained from $n$ independent samples from $\\mu$\napproaches $\\mu$ in the Wasserstein distance of any order. We prove sharp\nasymptotic and finite-sample results for this rate of convergence for general\nmeasures on general compact metric spaces. Our finite-sample results show the\nexistence of multi-scale behavior, where measures can exhibit radically\ndifferent rates of convergence as $n$ grows.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 02:44:53 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Weed", "Jonathan", ""], ["Bach", "Francis", ""]]}, {"id": "1707.00211", "submitter": "Michael Schweinberger", "authors": "Michael Schweinberger, Pavel N. Krivitsky, Carter T. Butts", "title": "A note on the role of projectivity in likelihood-based inference for\n  random graph models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is widespread confusion about the role of projectivity in\nlikelihood-based inference for random graph models. The confusion is rooted in\nclaims that projectivity, a form of marginalizability, may be necessary for\nlikelihood-based inference and consistency of maximum likelihood estimators. We\nshow that likelihood-based superpopulation inference is not affected by lack of\nprojectivity and that projectivity is not a necessary condition for consistency\nof maximum likelihood estimators.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 22:02:42 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Schweinberger", "Michael", ""], ["Krivitsky", "Pavel N.", ""], ["Butts", "Carter T.", ""]]}, {"id": "1707.00214", "submitter": "Greg Gandenberger", "authors": "Greg Gandenberger", "title": "Differences Among Noninformative Stopping Rules Are Often Relevant to\n  Bayesian Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L.J. Savage once hoped to show that \"the superficially incompatible systems\nof ideas associated on the one hand with [subjective Bayesianism] and on the\nother hand with [classical statistics]...lend each other mutual support and\nclarification.\" By 1972, however, he had largely \"lost faith in the devices\" of\nclassical statistics. One aspect of those \"devices\" that he found objectionable\nis that differences among the \"stopping rules\" that are used to decide when to\nend an experiment which are \"noninformative\" from a Bayesian perspective can\naffect decisions made using a classical approach. Two experiments that produce\nthe same data using different stopping rules seem to differ only in the\nintentions of the experimenters regarding whether or not they would have\ncarried on if the data had been different, which seem irrelevant to the\nevidential import of the data and thus to facts about what actions the data\nwarrant.\n  I argue that classical and Bayesian ideas about stopping rules do in fact\n\"lend each other\" the kind of \"mutual support and clarification\" that Savage\nhad originally hoped to find. They do so in a kind of case that is common in\nscientific practice, in which those who design an experiment have different\ninterests from those who will make decisions in light of its results. I show\nthat, in cases of this kind, Bayesian principles provide qualified support for\nthe classical statistical practice of \"penalizing\" \"biased\" stopping rules.\nHowever, they require this practice in a narrower range of circumstances than\nclassical principles do, and for different reasons. I argue that classical\narguments for this practice are compelling in precisely the class of cases in\nwhich Bayesian principles also require it, and thus that we should regard\nBayesian principles as clarifying classical statistical ideas about stopping\nrules rather than the reverse.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 22:46:23 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Gandenberger", "Greg", ""]]}, {"id": "1707.00274", "submitter": "Wicher Bergsma", "authors": "Wicher Bergsma", "title": "Regression with I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a parametric or nonparametric regression function\nin a model with normal errors is considered. For this purpose, a novel\nobjective prior for the regression function is proposed, defined as the\ndistribution maximizing entropy subject to a suitable constraint based on the\nFisher information on the regression function. The prior is named I-prior. For\nthe present model, it is Gaussian with covariance kernel proportional to the\nFisher information, and mean chosen a priori (e.g., 0). The I-prior has the\nintuitively appealing property that the more information is available about a\nlinear functional of the regression function, the larger its prior variance,\nand, broadly speaking, the less influential the prior is on the posterior.\nUnlike the Jeffreys prior, it can be used in high dimensional settings. The\nI-prior methodology can be used as a principled alternative to Tikhonov\nregularization, which suffers from well-known theoretical problems which are\nbriefly reviewed. The regression function is assumed to lie in a reproducing\nkernel Hilbert space (RKHS) over a low or high dimensional covariate space,\ngiving a high degree of generality. Analysis of some real data sets and a\nsmall-scale simulation study show competitive performance of the I-prior\nmethodology, which is implemented in the R-package iprior (Jamil, 2019).\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 10:18:08 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 16:55:31 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 20:44:00 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2018 10:55:19 GMT"}, {"version": "v5", "created": "Thu, 10 Oct 2019 17:03:06 GMT"}, {"version": "v6", "created": "Fri, 11 Oct 2019 10:03:07 GMT"}, {"version": "v7", "created": "Thu, 12 Dec 2019 14:00:16 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Bergsma", "Wicher", ""]]}, {"id": "1707.00314", "submitter": "Or Zuk", "authors": "Royi Jacobovic and Or Zuk", "title": "On The Asymptotic Efficiency of Selection Procedures for Independent\n  Gaussian Populations", "comments": "To appear in Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of discrete event simulation and optimization techniques motivates\nresearchers to adjust classic ranking and selection (R&S) procedures to the\nsettings where the number of populations is large. We use insights from extreme\nvalue theory in order to reveal the asymptotic properties of R&S procedures.\nNamely, we generalize the asymptotic result of Robbins and Siegmund regarding\nselection from independent Gaussian populations with known constant variance by\ntheir means to the case of selecting a subset of varying size out of a given\nset of populations. In addition, we revisit the problem of selecting the\npopulation with the highest mean among independent Gaussian populations with\nunknown and possibly different variances. Particularly, we derive the relative\nasymptotic efficiency of Dudewicz and Dalal's and Rinott's procedures, showing\nthat the former can be asymptotically superior by a multiplicative factor which\nis larger than one, but this factor may be reduced by proper choice of\nparameters. We also use our asymptotic results to suggest that the sample size\nin the first stage of the two procedures should be logarithmic in the number of\npopulations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 16:26:43 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:09:04 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Jacobovic", "Royi", ""], ["Zuk", "Or", ""]]}, {"id": "1707.00464", "submitter": "Phyllis Wan", "authors": "Phyllis Wan, Richard A. Davis", "title": "Threshold Selection for Multivariate Heavy-Tailed Data", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular variation is often used as the starting point for modeling\nmultivariate heavy-tailed data. A random vector is regularly varying if and\nonly if its radial part $R$ is regularly varying and is asymptotically\nindependent of the angular part $\\Theta$ as $R$ goes to infinity. The\nconditional limiting distribution of $\\Theta$ given $R$ is large characterizes\nthe tail dependence of the random vector and hence its estimation is the\nprimary goal of applications. A typical strategy is to look at the angular\ncomponents of the data for which the radial parts exceed some threshold. While\na large class of methods has been proposed to model the angular distribution\nfrom these exceedances, the choice of threshold has been scarcely discussed in\nthe literature. In this paper, we describe a procedure for choosing the\nthreshold by formally testing the independence of $R$ and $\\Theta$ using a\nmeasure of dependence called distance covariance. We generalize the limit\ntheorem for distance covariance to our unique setting and propose an algorithm\nwhich selects the threshold for $R$. This algorithm incorporates a subsampling\nscheme that is also applicable to weakly dependent data. Moreover, it avoids\nthe heavy computation in the calculation of the distance covariance, a typical\nlimitation for this measure. The performance of our method is illustrated on\nboth simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 09:52:53 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 17:20:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wan", "Phyllis", ""], ["Davis", "Richard A.", ""]]}, {"id": "1707.00486", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "A mathematical characterization of confidence as valid belief", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence is a fundamental concept in statistics, but there is a tendency to\nmisinterpret it as probability. In this paper, I argue that an intuitively and\nmathematically more appropriate interpretation of confidence is through\nbelief/plausibility functions, in particular, those that satisfy a certain\nvalidity property. Given their close connection with confidence, it is natural\nto ask how a valid belief/plausibility function can be constructed directly.\nThe inferential model (IM) framework provides such a construction, and here I\nprove a complete-class theorem stating that, for every nominal confidence\nregion, there exists a valid IM whose plausibility regions are contained by the\ngiven confidence region. This characterization has implications for statistics\nunderstanding and communication, and highlights the importance of belief\nfunctions and the IM framework.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 11:34:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1707.00544", "submitter": "A. J. van Es", "authors": "Bert van Es and Catharina Elisabeth Graafland", "title": "Nonparametric Kernel Density Estimation for Univariate Curent Status\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive estimators of the density of the event times of current status\ndata. The estimators are derived for the situations where the distribution of\nthe observation times is known and where this distribution is unknown. The\ndensity estimators are constructed from kernel estimators of the density of\ntransformed current status data, which have a distribution similar to uniform\ndeconvolution data. Expansions of the expectation and variance as well as\nasymptotic normality are derived. A reference density based bandwidth selection\nmethod is proposed. A simulated example is presented.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 13:37:43 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["van Es", "Bert", ""], ["Graafland", "Catharina Elisabeth", ""]]}, {"id": "1707.00577", "submitter": "Junhong Lin", "authors": "Junhong Lin, Lorenzo Rosasco", "title": "Generalization Properties of Doubly Stochastic Learning Algorithms", "comments": "24 pages. To appear in Journal of Complexity", "journal-ref": null, "doi": "10.1016/j.jco.2018.02.004", "report-no": null, "categories": "stat.ML cs.LG math.FA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly stochastic learning algorithms are scalable kernel methods that\nperform very well in practice. However, their generalization properties are not\nwell understood and their analysis is challenging since the corresponding\nlearning sequence may not be in the hypothesis space induced by the kernel. In\nthis paper, we provide an in-depth theoretical analysis for different variants\nof doubly stochastic learning algorithms within the setting of nonparametric\nregression in a reproducing kernel Hilbert space and considering the square\nloss. Particularly, we derive convergence results on the generalization error\nfor the studied algorithms either with or without an explicit penalty term. To\nthe best of our knowledge, the derived results for the unregularized variants\nare the first of this kind, while the results for the regularized variants\nimprove those in the literature. The novelties in our proof are a sample error\nbound that requires controlling the trace norm of a cumulative operator, and a\nrefined analysis of bounding initial error.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 14:46:05 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 21:22:12 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lin", "Junhong", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1707.00943", "submitter": "Jonathan Weed", "authors": "Amelia Perry, Jonathan Weed, Afonso S. Bandeira, Philippe Rigollet,\n  Amit Singer", "title": "The sample complexity of multi-reference alignment", "comments": "To appear in SIAM Journal on Mathematics of Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing role of data-driven approaches to scientific discovery has\nunveiled a large class of models that involve latent transformations with a\nrigid algebraic constraint. Three-dimensional molecule reconstruction in\nCryo-Electron Microscopy (cryo-EM) is a central problem in this class. Despite\ndecades of algorithmic and software development, there is still little\ntheoretical understanding of the sample complexity of this problem, that is,\nnumber of images required for 3-D reconstruction. Here we consider\nmulti-reference alignment (MRA), a simple model that captures fundamental\naspects of the statistical and algorithmic challenges arising in cryo-EM and\nrelated problems. In MRA, an unknown signal is subject to two types of\ncorruption: a latent cyclic shift and the more traditional additive white\nnoise. The goal is to recover the signal at a certain precision from\nindependent samples. While at high signal-to-noise ratio (SNR), the number of\nobservations needed to recover a generic signal is proportional to\n$1/\\mathrm{SNR}$, we prove that it rises to a surprising $1/\\mathrm{SNR}^3$ in\nthe low SNR regime. This precise phenomenon was observed empirically more than\ntwenty years ago for cryo-EM but has remained unexplained to date. Furthermore,\nour techniques can easily be extended to the heterogeneous MRA model where the\nsamples come from a mixture of signals, as is often the case in applications\nsuch as cryo-EM, where molecules may have different conformations. This\nprovides a first step towards a statistical theory for heterogeneous cryo-EM.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 12:42:46 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 14:36:09 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Perry", "Amelia", ""], ["Weed", "Jonathan", ""], ["Bandeira", "Afonso S.", ""], ["Rigollet", "Philippe", ""], ["Singer", "Amit", ""]]}, {"id": "1707.00973", "submitter": "Carla Tameling", "authors": "Carla Tameling, Max Sommerfeld, Axel Munk", "title": "Empirical optimal transport on countable metric spaces: Distributional\n  limits and statistical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive distributional limits for empirical transport distances between\nprobability measures supported on countable sets. Our approach is based on\nsensitivity analysis of optimal values of infinite dimensional mathematical\nprograms and a delta method for non-linear derivatives. A careful calibration\nof the norm on the space of probability measures is needed in order to combine\ndifferentiability and weak convergence of the underlying empirical process.\nBased on this we provide a sufficient and necessary condition for the\nunderlying distribution on the countable metric space for such a distributional\nlimit to hold. We give an explicit form of the limiting distribution for\nultra-metric spaces. Finally, we apply our findings to optimal transport based\ninference in large scale problems. An application to nanoscale microscopy is\ngiven.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 13:33:06 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 06:47:52 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 10:04:19 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Tameling", "Carla", ""], ["Sommerfeld", "Max", ""], ["Munk", "Axel", ""]]}, {"id": "1707.01019", "submitter": "Bruce Watson", "authors": "Wen-Chi Kuo, Jessica Joy Vardy and Bruce Alastair Watson", "title": "Mixingales on Riesz spaces", "comments": null, "journal-ref": "Journal of Mathematical Analysis and Applications, 402 (2013),\n  731-738", "doi": "10.1016/j.jmaa.2013.02.001", "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixingale is a stochastic process which combines properties of martingales\nand mixing sequences. McLeish introduced the term mixingale at the $4^{th}$\nConference on Stochastic Processes and Application, at York University,\nToronto, 1974, in the context of $L^2$. In this paper we generalize the concept\nof a mixingale to the measure-free Riesz space setting (this generalizes all of\nthe $L^p, 1\\le p\\le \\infty$ variants) and prove that a weak law of large\nnumbers holds for Riesz space mixingales. In the process we also generalize the\nconcept of uniform integrability to the Riesz space setting.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 14:41:21 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Kuo", "Wen-Chi", ""], ["Vardy", "Jessica Joy", ""], ["Watson", "Bruce Alastair", ""]]}, {"id": "1707.01130", "submitter": "Fatma Zehra Do\\u{g}ru", "authors": "Fatma Zehra Do\\u{g}ru, Y. Murat Bulut, Olcay Arslan", "title": "Double Reweighted Estimators for the Parameters of the Multivariate t\n  Distribution", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The t-distribution has many useful applications in robust statistical\nanalysis. The parameter estimation of the t-distribution is carried out using\nML estimation method, and the ML estimates are obtained via the EM algorithm.\nIn this study, we consider an alternative estimation method for all the\nparameters of the multivariate-t distribution using the MLq estimation method.\nWe adapt the EM algorithm to obtain the MLq estimates for all the parameters.\nWe provide a small simulation study to illustrate the performance of the MLq\nestimators over the ML estimators and observe that the MLq estimators have\nconsiderable superiority over the ML estimators.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 18:47:13 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Do\u011fru", "Fatma Zehra", ""], ["Bulut", "Y. Murat", ""], ["Arslan", "Olcay", ""]]}, {"id": "1707.01143", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Jaeyong Lee", "title": "Estimating Large Precision Matrices via Modified Cholesky Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the $k$-banded Cholesky prior for estimating a high-dimensional\nbandable precision matrix via the modified Cholesky decomposition. The bandable\nassumption is imposed on the Cholesky factor of the decomposition. We obtained\nthe P-loss convergence rate under the spectral norm and the matrix\n$\\ell_{\\infty}$ norm and the minimax lower bounds. Since the P-loss convergence\nrate (Lee and Lee (2017)) is stronger than the posterior convergence rate, the\nrates obtained are also posterior convergence rates. Furthermore, when the true\nprecision matrix is a $k_0$-banded matrix with some finite $k_0$, the obtained\nP-loss convergence rates coincide with the minimax rates. The established\nconvergence rates are slightly slower than the minimax lower bounds, but these\nare the fastest rates for bandable precision matrices among the existing\nBayesian approaches. A simulation study is conducted to compare the performance\nto the other competitive estimators in various scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 19:58:33 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lee", "Jaeyong", ""]]}, {"id": "1707.01199", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Alessandra Micheletti", "title": "A clustering algorithm for multivariate data streams with correlated\n  components", "comments": "title changed, rewritten", "journal-ref": "J Big Data (2017) 4:48", "doi": "10.1186/s40537-017-0109-0", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common clustering algorithms require multiple scans of all the data to\nachieve convergence, and this is prohibitive when large databases, with data\narriving in streams, must be processed. Some algorithms to extend the popular\nK-means method to the analysis of streaming data are present in literature\nsince 1998 (Bradley et al. in Scaling clustering algorithms to large databases.\nIn: KDD. p. 9-15, 1998; O'Callaghan et al. in Streaming-data algorithms for\nhigh-quality clustering. In: Proceedings of IEEE international conference on\ndata engineering. p. 685, 2001), based on the memorization and recursive update\nof a small number of summary statistics, but they either don't take into\naccount the specific variability of the clusters, or assume that the random\nvectors which are processed and grouped have uncorrelated components.\nUnfortunately this is not the case in many practical situations. We here\npropose a new algorithm to process data streams, with data having correlated\ncomponents and coming from clusters with different covariance matrices. Such\ncovariance matrices are estimated via an optimal double shrinkage method, which\nprovides positive definite estimates even in presence of a few data points, or\nof data having components with small variance. This is needed to invert the\nmatrices and compute the Mahalanobis distances that we use for the data\nassignment to the clusters. We also estimate the total number of clusters from\nthe data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 02:54:48 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 09:17:57 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Aletti", "Giacomo", ""], ["Micheletti", "Alessandra", ""]]}, {"id": "1707.01207", "submitter": "Dong Xia", "authors": "Dong Xia and Fan Zhou", "title": "The Sup-norm Perturbation of HOSVD and Low Rank Tensor Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The higher order singular value decomposition (HOSVD) of tensors is a\ngeneralization of matrix SVD. The perturbation analysis of HOSVD under random\nnoise is more delicate than its matrix counterpart. Recently, polynomial time\nalgorithms have been proposed where statistically optimal estimates of the\nsingular subspaces and the low rank tensors are attainable in the Euclidean\nnorm. In this article, we analyze the sup-norm perturbation bounds of HOSVD and\nintroduce estimators of the singular subspaces with sharp deviation bounds in\nthe sup-norm. We also investigate a low rank tensor denoising estimator and\ndemonstrate its fast convergence rate with respect to the entry-wise errors.\nThe sup-norm perturbation bounds reveal unconventional phase transitions for\nstatistical learning applications such as the exact clustering in high\ndimensional Gaussian mixture model and the exact support recovery in sub-tensor\nlocalizations. In addition, the bounds established for HOSVD also elaborate the\none-sided sup-norm perturbation bounds for the singular subspaces of unbalanced\n(or fat) matrices.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:58:11 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 01:39:33 GMT"}, {"version": "v3", "created": "Sat, 21 Apr 2018 22:19:15 GMT"}, {"version": "v4", "created": "Thu, 3 May 2018 15:38:51 GMT"}, {"version": "v5", "created": "Tue, 1 Jan 2019 09:53:59 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Xia", "Dong", ""], ["Zhou", "Fan", ""]]}, {"id": "1707.01227", "submitter": "Renan Gross", "authors": "Ronen Eldan and Renan Gross", "title": "Exponential random graphs behave like mixtures of stochastic block\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.SI math-ph math.CO math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of exponential random graphs in both the sparse and the\ndense regime. We show that exponential random graphs are approximate mixtures\nof graphs with independent edges whose probability matrices are critical points\nof an associated functional, thereby satisfying a certain matrix equation. In\nthe dense regime, every solution to this equation is close to a block matrix,\nconcluding that the exponential random graph behaves roughly like a mixture of\nstochastic block models. We also show existence and uniqueness of solutions to\nthis equation for several families of exponential random graphs, including the\ncase where the subgraphs are counted with positive weights and the case where\nall weights are small in absolute value. In particular, this generalizes some\nof the results in a paper by Chatterjee and Diaconis from the dense regime to\nthe sparse regime and strengthens their bounds from the cut-metric to the\none-metric.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 06:48:38 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 15:00:59 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 08:02:27 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Eldan", "Ronen", ""], ["Gross", "Renan", ""]]}, {"id": "1707.01296", "submitter": "Bertrand Iooss", "authors": "Roman Sueur, Bertrand Iooss, Thibault Delage", "title": "Sensitivity analysis using perturbed-law based indices for quantiles and\n  application to an industrial case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present perturbed law-based sensitivity indices and how to\nadapt them for quantile-oriented sensitivity analysis. We exhibit a simple way\nto compute these indices in practice using an importance sampling estimator for\nquantiles. Some useful asymptotic results about this estimator are also\nprovided. Finally, we apply this method to the study of a numerical model which\nsimulates the behaviour of a component in a hydraulic system in case of severe\ntransient solicitations. The sensitivity analysis is used to assess the impact\nof epistemic uncertainties about some physical parameters on the output of the\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:17:24 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Sueur", "Roman", ""], ["Iooss", "Bertrand", ""], ["Delage", "Thibault", ""]]}, {"id": "1707.01334", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (EDF R&D PRISME, IMT, GdR MASCOT-NUM), Cl\\'ementine\n  Prieur (AIRSEA)", "title": "Shapley effects for sensitivity analysis with correlated inputs:\n  comparisons with Sobol' indices, numerical estimation and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global sensitivity analysis of a numerical model aims to quantify, by\nmeans of sensitivity indices estimate, the contributions of each uncertain\ninput variable to the model output uncertainty. The so-called Sobol' indices,\nwhich are based on the functional variance analysis, present a difficult\ninterpretation in the presence of statistical dependence between inputs. The\nShapley effect was recently introduced to overcome this problem as they\nallocate the mutual contribution (due to correlation and interaction) of a\ngroup of inputs to each individual input within the group.In this paper, using\nseveral new analytical results, we study the effects of linear correlation\nbetween some Gaussian input variables on Shapley effects, and compare these\neffects to classical first-order and total Sobol' indices.This illustrates the\ninterest, in terms of sensitivity analysis setting and interpretation, of the\nShapley effects in the case of dependent inputs. For the practical issue of\ncomputationally demanding computer models, we show that the substitution of the\noriginal model by a metamodel (here, kriging) makes it possible to estimate\nthese indices with precision at a reasonable computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 11:49:14 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 13:28:33 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 06:27:03 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 15:35:11 GMT"}, {"version": "v5", "created": "Tue, 12 Feb 2019 09:22:32 GMT"}, {"version": "v6", "created": "Mon, 25 Mar 2019 09:24:54 GMT"}, {"version": "v7", "created": "Mon, 25 Nov 2019 10:57:25 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT, GdR MASCOT-NUM"], ["Prieur", "Cl\u00e9mentine", "", "AIRSEA"]]}, {"id": "1707.01350", "submitter": "Maxim Panov", "authors": "Maxim Panov, Konstantin Slavnov and Roman Ushakov", "title": "Consistent Estimation of Mixed Memberships with Successive Projections", "comments": null, "journal-ref": "Complex Networks & Their Applications VI. COMPLEX NETWORKS 2017.\n  Studies in Computational Intelligence, vol 689", "doi": "10.1007/978-3-319-72150-7_5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the parameter estimation problem in Mixed Membership\nStochastic Block Model (MMSB), which is a quite general instance of random\ngraph model allowing for overlapping community structure. We present the new\nalgorithm successive projection overlapping clustering (SPOC) which combines\nthe ideas of spectral clustering and geometric approach for separable\nnon-negative matrix factorization. The proposed algorithm is provably\nconsistent under MMSB with general conditions on the parameters of the model.\nSPOC is also shown to perform well experimentally in comparison to other\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:21:16 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 17:04:27 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Panov", "Maxim", ""], ["Slavnov", "Konstantin", ""], ["Ushakov", "Roman", ""]]}, {"id": "1707.01365", "submitter": "Sylvain Le Corff", "authors": "Roland Diel (JAD), Sylvain Le Corff (SAMOVAR, TSP, IP Paris), Matthieu\n  Lerasle (CREST, ENSAE, IP Paris, CNRS)", "title": "Learning the distribution of latent variables in paired comparison\n  models with round-robin scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paired comparison data considered in this paper originate from the comparison\nof a large number N of individuals in couples. The dataset is a collection of\nresults of contests between two individuals when each of them has faced n\nopponents, where n is much larger than N. Individual are represented by\nindependent and identically distributed random parameters characterizing their\nabilities.The paper studies the maximum likelihood estimator of the parameters\ndistribution. The analysis relies on the construction of a graphical model\nencoding conditional dependencies of the observations which are the outcomes of\nthe first n contests each individual is involved in. This graphical model\nallows to prove geometric loss of memory properties and deduce the asymptotic\nbehavior of the likelihood function. This paper sets the focus on graphical\nmodels obtained from round-robin scheduling of these contests.Following a\nclassical construction in learning theory, the asymptotic likelihood is used to\nmeasure performance of the maximum likelihood estimator. Risk bounds for this\nestimator are finally obtained by sub-Gaussian deviation results for Markov\nchains applied to the graphical model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:42:53 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 09:37:11 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Diel", "Roland", "", "JAD"], ["Corff", "Sylvain Le", "", "SAMOVAR, TSP, IP Paris"], ["Lerasle", "Matthieu", "", "CREST, ENSAE, IP Paris, CNRS"]]}, {"id": "1707.01522", "submitter": "Yakov Nikitin", "authors": "Ya. Yu. Nikitin", "title": "Tests based on characterizations, and their efficiencies: a survey", "comments": "Open access in Acta et Commentationes Universitatis Tartuensis de\n  Mathematica", "journal-ref": "Acta et Commentationes Universitatis Tartuensis de Mathematica,17,\n  N 1, 3-24 (2017)", "doi": "10.12697/ACUTM.2017.21.01", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of goodness-of-fit and symmetry tests based on the characterization\nproperties of distributions is presented. This approach became popular in\nrecent years. In most cases the test statistics are functionals of\n$U$-empirical processes. The limiting distributions and large deviations of new\nstatistics under the null hypothesis are described. Their local Bahadur\nefficiency for various parametric alternatives is calculated and compared with\neach other as well as with diverse previously known tests. We also describe new\ndirections of possible research in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 18:20:25 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Nikitin", "Ya. Yu.", ""]]}, {"id": "1707.01705", "submitter": "Yuping Song", "authors": "Yuping Song, Hanchao Wang", "title": "Local Nonparametric Estimation for Second-Order Jump-Diffusion Model\n  Using Gamma Asymmetric Kernels", "comments": "51 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the local linear smoothing to estimate the unknown first\nand second infinitesimal moments in second-order jump-diffusion model based on\nGamma asymmetric kernels. Under the mild conditions, we obtain the weak\nconsistency and the asymptotic normality of these estimators for both interior\nand boundary design points. Besides the standard properties of the local linear\nestimation such as simple bias representation and boundary bias correction, the\nlocal linear smoothing using Gamma asymmetric kernels possess some extra\nadvantages such as variable bandwidth, variance reduction and resistance to\nsparse design, which is validated through finite sample simulation study.\nFinally, we employ the estimators for the return of some high frequency\nfinancial data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:42:22 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Song", "Yuping", ""], ["Wang", "Hanchao", ""]]}, {"id": "1707.01706", "submitter": "Peter Math\\'e", "authors": "LiTao Ding and Peter Math\\'e", "title": "Minimax rates for statistical inverse problems under general source\n  conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the minimax reconstruction rates in linear ill-posed equations in\nHilbert space when smoothness is given in terms of general source sets. The\nunderlying fundamental result, the minimax rate on ellipsoids, is proved\nsimilarly to the seminal study by D. L. Donoho, R.~C. Liu, and B. MacGibbon,\n{\\it Minimax risk over hyperrectangles, and implications}, Ann.~ Statist. 18,\n1990. These authors highlighted the special role of the truncated series\nestimator, and for such estimators the risk can explicitly be given. We provide\nseveral examples, indicating results for statistical estimation in ill-posed\nproblems in Hilbert space.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 09:49:05 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 12:26:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ding", "LiTao", ""], ["Math\u00e9", "Peter", ""]]}, {"id": "1707.01764", "submitter": "Richard Nickl", "authors": "Richard Nickl", "title": "Bernstein - von Mises theorems for statistical inverse problems I:\n  Schr\\\"odinger equation", "comments": "46 pages, to appear in the Journal of the European Mathematical\n  Society (JEMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse problem of determining the unknown potential $f>0$ in the partial\ndifferential equation $$\\frac{\\Delta}{2} u - fu =0 \\text{ on } \\mathcal O\n~~\\text{s.t. } u = g \\text { on } \\partial \\mathcal O,$$ where $\\mathcal O$ is\na bounded $C^\\infty$-domain in $\\mathbb R^d$ and $g>0$ is a given function\nprescribing boundary values, is considered. The data consist of the solution\n$u$ corrupted by additive Gaussian noise. A nonparametric Bayesian prior for\nthe function $f$ is devised and a Bernstein - von Mises theorem is proved which\nentails that the posterior distribution given the observations is approximated\nin a suitable function space by an infinite-dimensional Gaussian measure that\nhas a `minimal' covariance structure in an information-theoretic sense. As a\nconsequence the posterior distribution performs valid and optimal frequentist\nstatistical inference on $f$ in the small noise limit.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 13:00:43 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 16:35:40 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 10:30:48 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Nickl", "Richard", ""]]}, {"id": "1707.02090", "submitter": "Olga Klopp", "authors": "Olga Klopp (CREST), Yu Lu, Alexandre B. Tsybakov (ENSAE ParisTech,\n  CREST), Harrison H. Zhou", "title": "Structured Matrix Estimation and Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of matrix estimation and matrix completion under a\ngeneral framework. This framework includes several important models as special\ncases such as the gaussian mixture model, mixed membership model, bi-clustering\nmodel and dictionary learning. We consider the optimal convergence rates in a\nminimax sense for estimation of the signal matrix under the Frobenius norm and\nunder the spectral norm. As a consequence of our general result we obtain\nminimax optimal rates of convergence for various special models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 09:16:34 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Klopp", "Olga", "", "CREST"], ["Lu", "Yu", "", "ENSAE ParisTech,\n  CREST"], ["Tsybakov", "Alexandre B.", "", "ENSAE ParisTech,\n  CREST"], ["Zhou", "Harrison H.", ""]]}, {"id": "1707.02171", "submitter": "Emilija Perkovi\\'c", "authors": "Emilija Perkovi\\'c, Markus Kalisch, Maloes H. Maathuis", "title": "Interpreting and using CPDAGs with background knowledge", "comments": "17 pages, 6 figures, UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop terminology and methods for working with maximally oriented\npartially directed acyclic graphs (maximal PDAGs). Maximal PDAGs arise from\nimposing restrictions on a Markov equivalence class of directed acyclic graphs,\nor equivalently on its graphical representation as a completed partially\ndirected acyclic graph (CPDAG), for example when adding background knowledge\nabout certain edge orientations. Although maximal PDAGs often arise in\npractice, causal methods have been mostly developed for CPDAGs. In this paper,\nwe extend such methodology to maximal PDAGs. In particular, we develop\nmethodology to read off possible ancestral relationships, we introduce a\ngraphical criterion for covariate adjustment to estimate total causal effects,\nand we adapt the IDA and joint-IDA frameworks to estimate multi-sets of\npossible causal effects. We also present a simulation study that illustrates\nthe gain in identifiability of total causal effects as the background knowledge\nincreases. All methods are implemented in the R package pcalg.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 13:45:01 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 14:19:46 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Perkovi\u0107", "Emilija", ""], ["Kalisch", "Markus", ""], ["Maathuis", "Maloes H.", ""]]}, {"id": "1707.02331", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Mohammad Arashi and S.Ejaz Ahmed", "title": "Shrinkage Estimation Strategies in Generalized Ridge Regression Models\n  Under Low/High-Dimension Regime", "comments": null, "journal-ref": "International Statistical Review, 2020", "doi": "10.1111/insr.12351", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose shrinkage methods based on {\\it generalized ridge\nregression} (GRR) estimation which is suitable for both multicollinearity and\nhigh dimensional problems with small number of samples (large $p$, small $n$).\nAlso, it is obtained theoretical properties of the proposed estimators for\nLow/High Dimensional cases. Furthermore, the performance of the listed\nestimators is demonstrated by both simulation studies and real-data analysis,\nand compare its performance with existing penalty methods. We show that the\nproposed methods compare well to competing regularization techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:47:25 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Arashi", "Mohammad", ""], ["Ahmed", "S. Ejaz", ""]]}, {"id": "1707.02333", "submitter": "Abhik Ghosh", "authors": "Ayanendranath Basu, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "Robust Wald-type tests for non-homogeneous observations based on minimum\n  density power divergence estimator", "comments": "Pre-print, Under review", "journal-ref": "Metrika (2018) 81: 493", "doi": "10.1007/s00184-018-0653-4", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robust hypothesis testing under\nnon-identically distributed data. We propose Wald-type tests for both simple\nand composite hypothesis for independent but non-homogeneous observations based\non the robust minimum density power divergence estimator of the common\nunderlying parameter. Asymptotic and theoretical robustness properties of the\nproposed tests have been discussed. Application to the problem of testing the\ngeneral linear hypothesis in a generalized linear model with fixed-design has\nbeen considered in detail with specific illustrations for its special cases\nunder normal and Poisson distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:52:44 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1707.02352", "submitter": "Zhou Fan", "authors": "Zhou Fan and Iain M. Johnstone", "title": "Tracy-Widom at each edge of real covariance and MANOVA estimators", "comments": "v4: Reorganize main text and supplementary appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample covariance matrix for real-valued data with general\npopulation covariance, as well as MANOVA-type covariance estimators in variance\ncomponents models under null hypotheses of global sphericity. In the limit as\nmatrix dimensions increase proportionally, the asymptotic spectra of such\nestimators may have multiple disjoint intervals of support, possibly\nintersecting the negative half line. We show that the distribution of the\nextremal eigenvalue at each regular edge of the support has a GOE Tracy-Widom\nlimit. Our proof extends a comparison argument of Ji Oon Lee and Kevin\nSchnelli, replacing a continuous Green function flow by a discrete Lindeberg\nswapping scheme.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 20:27:14 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:19:28 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 02:53:01 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 03:11:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Fan", "Zhou", ""], ["Johnstone", "Iain M.", ""]]}, {"id": "1707.02409", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Mario Diaz, Fady Alajaji, Tamas Linder", "title": "Estimation Efficiency Under Privacy Constraints", "comments": "To appear in IEEE Transaction on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2018.2865558", "report-no": null, "categories": "cs.IT cs.CR math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating a random variable $Y\\in \\mathcal{Y}$\nunder a privacy constraint dictated by another random variable $X\\in\n\\mathcal{X}$, where estimation efficiency and privacy are assessed in terms of\ntwo different loss functions. In the discrete case, we use the Hamming loss\nfunction and express the corresponding utility-privacy tradeoff in terms of the\nprivacy-constrained guessing probability $h(P_{XY}, \\epsilon)$, the maximum\nprobability $\\mathsf{P}_\\mathsf{c}(Y|Z)$ of correctly guessing $Y$ given an\nauxiliary random variable $Z\\in \\mathcal{Z}$, where the maximization is taken\nover all $P_{Z|Y}$ ensuring that $\\mathsf{P}_\\mathsf{c}(X|Z)\\leq \\epsilon$ for\na given privacy threshold $\\epsilon \\geq 0$. We prove that $h(P_{XY}, \\cdot)$\nis concave and piecewise linear, which allows us to derive its expression in\nclosed form for any $\\epsilon$ when $X$ and $Y$ are binary. In the non-binary\ncase, we derive $h(P_{XY}, \\epsilon)$ in the high utility regime (i.e., for\nsufficiently large values of $\\epsilon$) under the assumption that $Z$ takes\nvalues in $\\mathcal{Y}$. We also analyze the privacy-constrained guessing\nprobability for two binary vector scenarios. When $X$ and $Y$ are continuous\nrandom variables, we use the squared-error loss function and express the\ncorresponding utility-privacy tradeoff in terms of $\\mathsf{sENSR}(P_{XY},\n\\epsilon)$, which is the smallest normalized minimum mean squared-error (mmse)\nincurred in estimating $Y$ from its Gaussian perturbation $Z$, such that the\nmmse of $f(X)$ given $Z$ is within $\\epsilon$ of the variance of $f(X)$ for any\nnon-constant real-valued function $f$. We derive tight upper and lower bounds\nfor $\\mathsf{sENSR}$ when $Y$ is Gaussian. We also obtain a tight lower bound\nfor $\\mathsf{sENSR}(P_{XY}, \\epsilon)$ for general absolutely continuous random\nvariables when $\\epsilon$ is sufficiently small.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 07:46:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 16:24:00 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Diaz", "Mario", ""], ["Alajaji", "Fady", ""], ["Linder", "Tamas", ""]]}, {"id": "1707.02419", "submitter": "Markus Bibinger", "authors": "Markus Bibinger, Nikolaus Hautsch, Peter Malec and Markus Rei{\\ss}", "title": "Estimating the Spot Covariation of Asset Prices - Statistical Theory and\n  Empirical Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator for the spot covariance matrix of a\nmulti-dimensional continuous semi-martingale log asset price process which is\nsubject to noise and non-synchronous observations. The estimator is constructed\nbased on a local average of block-wise parametric spectral covariance\nestimates. The latter originate from a local method of moments (LMM) which\nrecently has been introduced. We prove consistency and a point-wise stable\ncentral limit theorem for the proposed spot covariance estimator in a very\ngeneral setup with stochastic volatility, leverage effects and general noise\ndistributions. Moreover, we extend the LMM estimator to be robust against\nautocorrelated noise and propose a method to adaptively infer the\nautocorrelations from the data. Based on simulations we provide empirical\nguidance on the effective implementation of the estimator and apply it to\nhigh-frequency data of a cross-section of Nasdaq blue chip stocks. Employing\nthe estimator to estimate spot covariances, correlations and volatilities in\nnormal but also unusual periods yields novel insights into intraday covariance\nand correlation dynamics. We show that intraday (co-)variations (i) follow\nunderlying periodicity patterns, (ii) reveal substantial intraday variability\nassociated with (co-)variation risk, and (iii) can increase strongly and nearly\ninstantaneously if new information arrives.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 09:21:03 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bibinger", "Markus", ""], ["Hautsch", "Nikolaus", ""], ["Malec", "Peter", ""], ["Rei\u00df", "Markus", ""]]}, {"id": "1707.02484", "submitter": "Jon A. Wellner", "authors": "W. J. Hall and Jon A. Wellner", "title": "Estimation of mean residual life", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang (1978) considered an empirical estimate of the mean residual life\nfunction on a fixed finite interval. She proved it to be strongly uniformly\nconsistent and (when appropriately standardized) weakly convergent to a\nGaussian process. These results are extended to the whole half line, and the\nvariance of the the limiting process is studied. Also, nonparametric\nsimultaneous confidence bands for the mean residual life function are obtained\nby transforming the limiting process to Brownian motion.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 19:46:40 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:03:33 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Hall", "W. J.", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1707.02555", "submitter": "Jonathan Hill", "authors": "Jonathan B. Hill", "title": "Asymptotic Theory for the Maximum of an Increasing Sequence of\n  Parametric Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\cite{HillMotegi2017} present a new general asymptotic theory for the maximum\nof a random array $\\{\\mathcal{X}_{n}(i)$ $:$ $1$ $\\leq $ $i$ $\\leq $\n$\\mathcal{L}\\}_{n\\geq 1}$, where each $\\mathcal{X}_{n}(i)$ is assumed to\nconverge in probability as $n$ $\\rightarrow $ $\\infty $. The array dimension\n$\\mathcal{L}$ is allowed to increase with the sample size $n$. Existing extreme\nvalue theory arguments focus on observed data $\\mathcal{X}_{n}(i)$, and require\na well defined limit law for $\\max_{1\\leq i\\leq\n\\mathcal{L}}|\\mathcal{X}_{n}(i)|$ by restricting dependence across $i$. The\nhigh dimensional central limit theory literature presumes approximability by a\nGaussian law, and also restricts attention to observed data.\n\\cite{HillMotegi2017} do not require $\\max_{1\\leq i\\leq\n\\mathcal{L}_{n}}|\\mathcal{X}_{n}(i)|$ to have a well defined limit nor be\napproximable by a Gaussian random variable, and we do not make any assumptions\nabout dependence across $i$. We apply the theory to filtered data when the\nvariable of interest $\\mathcal{X}_{n}(i,\\theta _{0})$ is not observed, but its\nsample counterpart $\\mathcal{X}_{n}(i,\\hat{\\theta}_{n})$ is observed where\n$\\hat{\\theta}_{n}$ estimates $\\theta _{0}$. The main results are illustrated by\nlooking at unit root tests for a high dimensional random variable, and a\nresiduals white noise test.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 10:24:53 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 14:00:25 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 14:29:47 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Hill", "Jonathan B.", ""]]}, {"id": "1707.02564", "submitter": "Fadil Danufane", "authors": "Fadil Habibi Danufane, Katsuyoshi Ohara, Nobuki Takayama, Constantin\n  Siriteanu", "title": "Holonomic Gradient Method-Based CDF Evaluation for the Largest\n  Eigenvalue of a Complex Noncentral Wishart Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outage probability of maximal-ratio combining (MRC) for a multiple-input\nmultiple-output (MIMO) wireless communications system under Rician fading is\ngiven by the cumulative distribution function (CDF) for the largest eigenvalue\nof a complex noncentral Wishart matrix. This CDF has previously been expressed\nas a determinant whose elements are integrals of a confluent hypergeometric\nfunction. For the determinant elements, conventional evaluation approaches,\ne.g., truncation of infinite series ensuing from the hypergeometric function or\nnumerical integration, can be unreliable and slow even for moderate antenna\nnumbers and Rician $ K $-factor values. Therefore, herein, we derive by hand\nand by computer algebra also differential equations that are then solved from\ninitial conditions computed by conventional approaches. This is the holonomic\ngradient method (HGM). Previous HGM-based evaluations of MIMO relied on\ndifferential equations that were not theoretically guaranteed to converge, and,\nthus, yielded reliable results only for few antennas or moderate $ K $. Herein,\nwe reveal that gauge transformations can yield differential equations that are\n{\\emph{stabile}}, i.e., guarantee HGM convergence. The ensuing HGM-based CDF\nevaluation is demonstrated reliable, accurate, and expeditious in computing the\nMRC outage probability even for very large antenna numbers and values of $ K $.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 11:48:29 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 09:55:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Danufane", "Fadil Habibi", ""], ["Ohara", "Katsuyoshi", ""], ["Takayama", "Nobuki", ""], ["Siriteanu", "Constantin", ""]]}, {"id": "1707.03035", "submitter": "Werner Ehm", "authors": "Werner Ehm and Fabian Kr\\\"uger", "title": "Forecast dominance testing via sign randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose randomization tests of whether forecast 1 outperforms forecast 2\nacross a class of scoring functions. This hypothesis is of applied interest:\nWhile the prediction context often prescribes a certain class of scoring\nfunctions, it is typically hard to motivate a specific choice on statistical or\nsubstantive grounds. We investigate the asymptotic behavior of the test\nstatistics under mild conditions, avoiding the need to assume particular\ndynamic properties of forecasts and realizations. The properties of the\none-sided tests depend on a corresponding version of Anderson's inequality,\nwhich we state as a conjecture of independent interest. Numerical experiments\nand a data example indicate that the tests have good size and power properties\nin practically relevant situations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 19:52:41 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 00:04:51 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 10:16:46 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Ehm", "Werner", ""], ["Kr\u00fcger", "Fabian", ""]]}, {"id": "1707.03063", "submitter": "Jie Yang", "authors": "Xianwei Bu, Dibyen Majumdar and Jie Yang", "title": "D-optimal Designs for Multinomial Logistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal designs for general multinomial logistic models, which\ncover baseline-category, cumulative, adjacent-categories, and\ncontinuation-ratio logit models, with proportional odds, non-proportional odds,\nor partial proportional odds assumption. We derive the corresponding Fisher\ninformation matrices in three different forms to facilitate their calculations,\ndetermine the conditions for their positive definiteness, and search for\noptimal designs. We conclude that, unlike the designs for binary responses, a\nfeasible design for a multinomial logistic model may contain less experimental\nsettings than parameters, which is of practical significance. We also conclude\nthat even for a minimally supported design, a uniform allocation, which is\ntypically used in practice, is not optimal in general for a multinomial\nlogistic model. We develop efficient algorithms for searching D-optimal\ndesigns. Using examples based on real experiments, we show that the efficiency\nof an experiment can be significantly improved if our designs are adopted.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 21:15:05 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 21:44:11 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 05:11:38 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Bu", "Xianwei", ""], ["Majumdar", "Dibyen", ""], ["Yang", "Jie", ""]]}, {"id": "1707.03220", "submitter": "Nicole M\\\"ucke", "authors": "Nicole M\\\"ucke", "title": "Reducing training time by efficient localized kernel regression", "comments": null, "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR:\n  Volume 89. Copyright 2019 by the author(s)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalization properties of kernel regularized least squares\nregression based on a partitioning approach. We show that optimal rates of\nconvergence are preserved if the number of local sets grows sufficiently slowly\nwith the sample size. Moreover, the partitioning approach can be efficiently\ncombined with local Nystr\\\"om subsampling, improving computational cost\ntwofold.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 11:16:08 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 12:56:12 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 11:47:53 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["M\u00fccke", "Nicole", ""]]}, {"id": "1707.03436", "submitter": "David Kaplan", "authors": "Luciano de Castro (1), Antonio F. Galvao (2), David M. Kaplan (3), Xin\n  Liu (3) ((1) University of Iowa, (2) University of Arizona, (3) University of\n  Missouri)", "title": "Smoothed GMM for quantile models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops theory for feasible estimators of finite-dimensional\nparameters identified by general conditional quantile restrictions, under much\nweaker assumptions than previously seen in the literature. This includes\ninstrumental variables nonlinear quantile regression as a special case. More\nspecifically, we consider a set of unconditional moments implied by the\nconditional quantile restrictions, providing conditions for local\nidentification. Since estimators based on the sample moments are generally\nimpossible to compute numerically in practice, we study feasible estimators\nbased on smoothed sample moments. We propose a method of moments estimator for\nexactly identified models, as well as a generalized method of moments estimator\nfor over-identified models. We establish consistency and asymptotic normality\nof both estimators under general conditions that allow for weakly dependent\ndata and nonlinear structural models. Simulations illustrate the finite-sample\nproperties of the methods. Our in-depth empirical application concerns the\nconsumption Euler equation derived from quantile utility maximization.\nAdvantages of the quantile Euler equation include robustness to fat tails,\ndecoupling of risk attitude from the elasticity of intertemporal substitution,\nand log-linearization without any approximation error. For the four countries\nwe examine, the quantile estimates of discount factor and elasticity of\nintertemporal substitution are economically reasonable for a range of quantiles\nabove the median, even when two-stage least squares estimates are not\nreasonable.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 19:01:38 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:36:44 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["de Castro", "Luciano", ""], ["Galvao", "Antonio F.", ""], ["Kaplan", "David M.", ""], ["Liu", "Xin", ""]]}, {"id": "1707.03820", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Yasin Asar, M.\\c{S}amil \\c{S}{\\i}k and\n  Ahmet Demiralp", "title": "Pretest and Stein-Type Estimations in Quantile Regression Model", "comments": "arXiv admin note: text overlap with arXiv:1707.01052", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider preliminary test and shrinkage estimation\nstrategies for quantile regression models. In classical Least Squares\nEstimation (LSE) method, the relationship between the explanatory and explained\nvariables in the coordinate plane is estimated with a mean regression line. In\norder to use LSE, there are three main assumptions on the error terms showing\nwhite noise process of the regression model, also known as Gauss-Markov\nAssumptions, must be met: (1) The error terms have zero mean, (2) The variance\nof the error terms is constant and (3) The covariance between the errors is\nzero i.e., there is no autocorrelation. However, data in many areas, including\neconometrics, survival analysis and ecology, etc. does not provide these\nassumptions. First introduced by Koenker, quantile regression has been used to\ncomplement this deficiency of classical regression analysis and to improve the\nleast square estimation. The aim of this study is to improve the performance of\nquantile regression estimators by using pre-test and shrinkage strategies. A\nMonte Carlo simulation study including a comparison with quantile $L_1$--type\nestimators such as Lasso, Ridge and Elastic Net are designed to evaluate the\nperformances of the estimators. Two real data examples are given for\nillustrative purposes. Finally, we obtain the asymptotic results of suggested\nestimators\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 12:23:45 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 12:00:39 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["Asar", "Yasin", ""], ["\u015e\u0131k", "M. \u015eamil", ""], ["Demiralp", "Ahmet", ""]]}, {"id": "1707.04010", "submitter": "Xinxin Yang", "authors": "Xinxin Yang, Xinghua Zheng and Jiaqi Chen", "title": "Testing High-dimensional Covariance Matrices under the Elliptical\n  Distribution and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop tests for high-dimensional covariance matrices under a generalized\nelliptical model. Our tests are based on a central limit theorem (CLT) for\nlinear spectral statistics of the sample covariance matrix based on\nself-normalized observations. For testing sphericity, our tests neither assume\nspecific parametric distributions nor involve the kurtosis of data. More\ngenerally, we can test against any non-negative definite matrix that can even\nbe not invertible. As an interesting application, we illustrate in empirical\nstudies that our tests can be used to test uncorrelatedness among idiosyncratic\nreturns.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 07:45:47 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 01:47:25 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 14:54:43 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yang", "Xinxin", ""], ["Zheng", "Xinghua", ""], ["Chen", "Jiaqi", ""]]}, {"id": "1707.04145", "submitter": "Marie Perrot-Dockes", "authors": "Marie Perrot-Dock\\`es, C\\'eline L\\'evy-Leduc, Laure Sansonnet and\n  Julien Chiquet", "title": "Variable selection in multivariate linear models with high-dimensional\n  covariance matrix estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable selection approach in the\nframework of multivariate linear models taking into account the dependence that\nmay exist between the responses. It consists in estimating beforehand the\ncovariance matrix of the responses and to plug this estimator in a Lasso\ncriterion, in order to obtain a sparse estimator of the coefficient matrix. The\nproperties of our approach are investigated both from a theoretical and a\nnumerical point of view. More precisely, we give general conditions that the\nestimators of the covariance matrix and its inverse have to satisfy in order to\nrecover the positions of the null and non null entries of the coefficient\nmatrix when the size of the covariance matrix is not fixed and can tend to\ninfinity. We prove that these conditions are satisfied in the particular case\nof some Toeplitz matrices. Our approach is implemented in the R package\nMultiVarSel available from the Comprehensive R Archive Network (CRAN) and is\nvery attractive since it benefits from a low computational load. We also assess\nthe performance of our methodology using synthetic data and compare it with\nalternative approaches. Our numerical experiments show that including the\nestimation of the covariance matrix in the Lasso criterion dramatically\nimproves the variable selection performance in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 14:31:42 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Perrot-Dock\u00e8s", "Marie", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Sansonnet", "Laure", ""], ["Chiquet", "Julien", ""]]}, {"id": "1707.04218", "submitter": "Yanpeng Li", "authors": "Yanpeng Li", "title": "Learning Features from Co-occurrences: A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a word by its co-occurrences with other words in context is an\neffective way to capture the meaning of the word. However, the theory behind\nremains a challenge. In this work, taking the example of a word classification\ntask, we give a theoretical analysis of the approaches that represent a word X\nby a function f(P(C|X)), where C is a context feature, P(C|X) is the\nconditional probability estimated from a text corpus, and the function f maps\nthe co-occurrence measure to a prediction score. We investigate the impact of\ncontext feature C and the function f. We also explain the reasons why using the\nco-occurrences with multiple context features may be better than just using a\nsingle one. In addition, some of the results shed light on the theory of\nfeature learning and machine learning in general.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 16:46:50 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Li", "Yanpeng", ""]]}, {"id": "1707.04300", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Elchanan Mossel, Robert Nowak, Sebastien Roch", "title": "Coalescent-based species tree estimation: a stochastic Farris transform", "comments": "Submitted. 49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a species phylogeny from genomic data faces two\nsignificant hurdles: 1) the trees describing the evolution of each individual\ngene--i.e., the gene trees--may differ from the species phylogeny and 2) the\nmolecular sequences corresponding to each gene often provide limited\ninformation about the gene trees themselves. In this paper we consider an\napproach to species tree reconstruction that addresses both these hurdles.\nSpecifically, we propose an algorithm for phylogeny reconstruction under the\nmultispecies coalescent model with a standard model of site substitution. The\nmultispecies coalescent is commonly used to model gene tree discordance due to\nincomplete lineage sorting, a well-studied population-genetic effect.\n  In previous work, an information-theoretic trade-off was derived in this\ncontext between the number of loci, $m$, needed for an accurate reconstruction\nand the length of the locus sequences, $k$. It was shown that to reconstruct an\ninternal branch of length $f$, one needs $m$ to be of the order of $1/[f^{2}\n\\sqrt{k}]$. That previous result was obtained under the molecular clock\nassumption, i.e., under the assumption that mutation rates (as well as\npopulation sizes) are constant across the species phylogeny.\n  Here we generalize this result beyond the restrictive molecular clock\nassumption, and obtain a new reconstruction algorithm that has the same data\nrequirement (up to log factors). Our main contribution is a novel reduction to\nthe molecular clock case under the multispecies coalescent. As a corollary, we\nalso obtain a new identifiability result of independent interest: for any\nspecies tree with $n \\geq 3$ species, the rooted species tree can be identified\nfrom the distribution of its unrooted weighted gene trees even in the absence\nof a molecular clock.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:22:35 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Mossel", "Elchanan", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1707.04301", "submitter": "Gery Geenens", "authors": "Gery Geenens", "title": "Mellin-Meijer-kernel density estimation on $\\mathbb{R}^+$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric kernel density estimation is a very natural procedure which\nsimply makes use of the smoothing power of the convolution operation. Yet, it\nperforms poorly when the density of a positive variable is to be estimated\n(boundary issues, spurious bumps in the tail). So various extensions of the\nbasic kernel estimator allegedly suitable for $\\mathbb{R}^+$-supported\ndensities, such as those using Gamma or other asymmetric kernels, abound in the\nliterature. Those, however, are not based on any valid smoothing operation\nanalogous to the convolution, which typically leads to inconsistencies. By\ncontrast, in this paper a kernel estimator for $\\mathbb{R}^+$-supported\ndensities is defined by making use of the Mellin convolution, the natural\nanalogue of the usual convolution on $\\mathbb{R}^+$. From there, a very\ntransparent theory flows and leads to new type of asymmetric kernels strongly\nrelated to Meijer's $G$-functions. The numerous pleasant properties of this\n`Mellin-Meijer-kernel density estimator' are demonstrated in the paper. Its\npointwise and $L_2$-consistency (with optimal rate of convergence) is\nestablished for a large class of densities, including densities unbounded at 0\nand showing power-law decay in their right tail. Its practical behaviour is\ninvestigated further through simulations and some real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:24:00 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Geenens", "Gery", ""]]}, {"id": "1707.04345", "submitter": "Caroline Uhler", "authors": "Caroline Uhler", "title": "Gaussian Graphical Models: An Algebraic and Geometric Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are used throughout the natural sciences, social\nsciences, and economics to model the statistical relationships between\nvariables of interest in the form of a graph. We here provide a pedagogic\nintroduction to Gaussian graphical models and review recent results on maximum\nlikelihood estimation for such models. Throughout, we highlight the rich\nalgebraic and geometric properties of Gaussian graphical models and explain how\nthese properties relate to convex optimization and ultimately result in\ninsights on the existence of the maximum likelihood estimator (MLE) and\nalgorithms for computing the MLE.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 22:41:39 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Uhler", "Caroline", ""]]}, {"id": "1707.04360", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai, Hans-Georg M\\\"uller, Wenwen Tao", "title": "Derivative Principal Component Analysis for Representing the Time\n  Dynamics of Longitudinal and Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric method to explicitly model and represent the\nderivatives of smooth underlying trajectories for longitudinal data. This\nrepresentation is based on a direct Karhunen--Lo\\`eve expansion of the\nunobserved derivatives and leads to the notion of derivative principal\ncomponent analysis, which complements functional principal component analysis,\none of the most popular tools of functional data analysis. The proposed\nderivative principal component scores can be obtained for irregularly spaced\nand sparsely observed longitudinal data, as typically encountered in biomedical\nstudies, as well as for functional data which are densely measured. Novel\nconsistency results and asymptotic convergence rates for the proposed estimates\nof the derivative principal component scores and other components of the model\nare derived under a unified scheme for sparse or dense observations and mild\nconditions. We compare the proposed representations for derivatives with\nalternative approaches in simulation settings and also in a wallaby growth\ncurve application. It emerges that representations using the proposed\nderivative principal component analysis recover the underlying derivatives more\naccurately compared to principal component analysis-based approaches especially\nin settings where the functional data are represented with only a very small\nnumber of components or are densely sampled. In a second wheat spectra\nclassification example, derivative principal component scores were found to be\nmore predictive for the protein content of wheat than the conventional\nfunctional principal component scores.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 00:45:45 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Dai", "Xiongtao", ""], ["M\u00fcller", "Hans-Georg", ""], ["Tao", "Wenwen", ""]]}, {"id": "1707.04371", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Sumeetpal S. Singh and Ajay Jasra", "title": "Identification of multi-object dynamical systems: consistency and Fisher\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the model parameters of a multi-object dynamical system from partial\nand perturbed observations is a challenging task. Despite recent numerical\nadvancements in learning these parameters, theoretical guarantees are extremely\nscarce. In this article, we study the identifiability of these parameters and\nthe consistency of the corresponding maximum likelihood estimate (MLE) under\nassumptions on the different components of the underlying multi-object system.\nIn order to understand the impact of the various sources of observation noise\non the ability to learn the model parameters, we study the asymptotic variance\nof the MLE through the associated Fisher information matrix. For example, we\nshow that specific aspects of the multi-target tracking (MTT) problem such as\ndetection failures and unknown data association lead to a loss of information\nwhich is quantified in special cases of interest.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 02:34:56 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Singh", "Sumeetpal S.", ""], ["Jasra", "Ajay", ""]]}, {"id": "1707.04380", "submitter": "Gourab Mukherjee", "authors": "Gourab Mukherjee and Iain M. Johnstone", "title": "On Minimax Optimality of Sparse Bayes Predictive Density Estimates", "comments": "a typos corrected: page 5, line 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study predictive density estimation under Kullback-Leibler loss in\n$\\ell_0$-sparse Gaussian sequence models. We propose proper Bayes predictive\ndensity estimates and establish asymptotic minimaxity in sparse models. A\nsurprise is the existence of a phase transition in the future-to-past variance\nratio $r$. For $r < r_0 = (\\surd 5 - 1)/4$, the natural discrete prior ceases\nto be asymptotically optimal. Instead, for subcritical $r$, a `bi-grid' prior\nwith a central region of reduced grid spacing recovers asymptotic minimaxity.\nThis phenomenon seems to have no analog in the otherwise parallel theory of\npoint estimation of a multivariate normal mean under quadratic loss. For\nspike-and-slab priors to have any prospect of minimaxity, we show that the\nsparse parameter space needs also to be magnitude constrained. Within a\nsubstantial range of magnitudes, spike-and-slab priors can attain asymptotic\nminimaxity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 04:49:54 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 22:13:49 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Mukherjee", "Gourab", ""], ["Johnstone", "Iain M.", ""]]}, {"id": "1707.04469", "submitter": "Enno Mammen", "authors": "Enno Mammen", "title": "Nonparametric estimation of locally stationary Hawkes processe", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider multivariate Hawkes processes with baseline hazard\nand kernel functions that depend on time. This defines a class of locally\nstationary processes. We discuss estimation of the time-dependent baseline\nhazard and kernel functions based on a localized criterion. Theory on\nstationary Hawkes processes is extended to develop asymptotic theory for the\nestimator in the locally stationary model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 11:39:13 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Mammen", "Enno", ""]]}, {"id": "1707.04472", "submitter": "Adrien Saumard", "authors": "Adrien Saumard and Jon A. Wellner", "title": "Efron's monotonicity property for measures on $\\mathbb{R}^2$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First we prove some kernel representations for the covariance of two\nfunctions taken on the same random variable and deduce kernel representations\nfor some functionals of a continuous one-dimensional measure. Then we apply\nthese formulas to extend Efron's monotonicity property, given in Efron [1965]\nand valid for independent log-concave measures, to the case of general measures\non $\\mathbb{R}^2$. The new formulas are also used to derive some further\nquantitative estimates in Efron's monotonicity property.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 11:55:37 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 10:52:29 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Saumard", "Adrien", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1707.04622", "submitter": "Enno Mammen", "authors": "Munir Hiabu, Enno Mammen, Maria Dolores Martinez-Miranda, and Jens\n  Perch Nielsen", "title": "Smooth backfitting of proportional hazards with multiplicative\n  components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth backfitting has proven to have a number of theoretical and practical\nadvantages in structured regression. Smooth backfitting projects the data down\nonto the structured space of interest providing a direct link between data and\nestimator. This paper introduces the ideas of smooth backfitting to survival\nanalysis in a proportional hazard model, where we assume an underlying\nconditional hazard with multiplicative components. We develop asymptotic theory\nfor the estimator and we use the smooth backfitter in a practical application,\nwhere we extend recent advances of in-sample forecasting methodology by\nallowing more information to be incorporated, while still obeying the\nstructured requirements of in-sample forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:58:31 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 04:06:36 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Hiabu", "Munir", ""], ["Mammen", "Enno", ""], ["Martinez-Miranda", "Maria Dolores", ""], ["Nielsen", "Jens Perch", ""]]}, {"id": "1707.04802", "submitter": "Rajeshwari Majumdar", "authors": "Rajeshwari Majumdar", "title": "Conditional Independence, Conditional Mean Independence, and Zero\n  Conditional Covariance", "comments": "Theorem 3 of this paper, as stated, is incorrect. For it to hold,\n  additional integrability assumptions are necessary. Further, the scope of\n  Theorem 1 is broader than indicated in this paper. These considerations\n  prompt the withdrawal of this paper. Two new papers are available as\n  arXiv:1710.06566 and arXiv:1710.06987, fixing the shortcomings of this paper\n  and adding new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the reversibility of the directional hierarchy in the\ninterdependency among the notions of conditional independence, conditional mean\nindependence, and zero conditional covariance, for two random variables X and Y\ngiven a conditioning element Z which is not constrained by any topological\nrestriction on its range, reveals that if the first moments of X, Y, and XY\nexist, then conditional independence implies conditional mean independence and\nconditional mean independence implies zero conditional covariance, but the\ndirection of the hierarchy is not reversible in general. If the conditional\nexpectation of Y given X and Z is \"affine in X,\" which happens when X is\nBernoulli, then the \"intercept\" and \"slope\" of the conditional expectation\n(that is, the nonparametric regression function) equal the \"intercept\" and\n\"slope\" of the \"least-squares linear regression function\", as a result of which\nzero conditional covariance implies conditional mean independence.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 00:09:02 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 23:45:47 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Majumdar", "Rajeshwari", ""]]}, {"id": "1707.05021", "submitter": "Xiaohong Lan", "authors": "Xiaohong Lan and Yimin Xiao", "title": "Strong Local Nondeterminism of Spherical Fractional Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Let $B = \\left\\{ B\\left( x\\right),\\, x\\in \\mathbb{S}^{2}\\right\\} $ be the\nfractional Brownian motion indexed by the unit sphere $\\mathbb{S}^{2}$ with\nindex $0<H\\leq \\frac{1}{2}$, introduced by Istas \\cite{IstasECP05}. We\nestablish optimal estimates for its angular power spectrum $\\{d_\\ell, \\ell = 0,\n1, 2, \\ldots\\}$, and then exploit its high-frequency behavior to establish the\nproperty of its strong local nondeterminism of $B$.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 07:13:49 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 05:02:01 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Lan", "Xiaohong", ""], ["Xiao", "Yimin", ""]]}, {"id": "1707.05023", "submitter": "Gerard Biau", "authors": "G\\'erard Biau (LSTA, LPMA), Beno\\^it Cadre (ENS Rennes, IRMAR)", "title": "Optimization by gradient boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting is a state-of-the-art prediction technique that\nsequentially produces a model in the form of linear combinations of simple\npredictors---typically decision trees---by solving an infinite-dimensional\nconvex optimization problem. We provide in the present paper a thorough\nanalysis of two widespread versions of gradient boosting, and introduce a\ngeneral framework for studying these algorithms from the point of view of\nfunctional optimization. We prove their convergence as the number of iterations\ntends to infinity and highlight the importance of having a strongly convex risk\nfunctional to minimize. We also present a reasonable statistical context\nensuring consistency properties of the boosting predictors as the sample size\ngrows. In our approach, the optimization procedures are run forever (that is,\nwithout resorting to an early stopping strategy), and statistical\nregularization is basically achieved via an appropriate $L^2$ penalization of\nthe loss and strong convexity arguments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 07:44:26 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA, LPMA"], ["Cadre", "Beno\u00eet", "", "ENS Rennes, IRMAR"]]}, {"id": "1707.05033", "submitter": "Adrien Hitz", "authors": "Adrien Hitz, Richard Davis and Gennady Samorodnitsky", "title": "Discrete Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our contribution is to widen the scope of extreme value analysis applied to\ndiscrete-valued data. Extreme values of a random variable $X$ are commonly\nmodeled using the generalized Pareto distribution, a method that often gives\ngood results in practice. When $X$ is discrete, we propose two other methods\nusing a discrete generalized Pareto and a generalized Zipf distribution\nrespectively. Both are theoretically motivated and we show that they perform\nwell in estimating rare events in several simulated and real data cases such as\nword frequency, tornado outbreaks and multiple births.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 08:03:43 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Hitz", "Adrien", ""], ["Davis", "Richard", ""], ["Samorodnitsky", "Gennady", ""]]}, {"id": "1707.05232", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen, Mohamed Hebiri and Joseph Salmon", "title": "On Lasso refitting strategies", "comments": "revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-know drawback of l_1-penalized estimators is the systematic shrinkage\nof the large coefficients towards zero. A simple remedy is to treat Lasso as a\nmodel-selection procedure and to perform a second refitting step on the\nselected support. In this work we formalize the notion of refitting and provide\noracle bounds for arbitrary refitting procedures of the Lasso solution. One of\nthe most widely used refitting techniques which is based on Least-Squares may\nbring a problem of interpretability, since the signs of the refitted estimator\nmight be flipped with respect to the original estimator. This problem arises\nfrom the fact that the Least-Squares refitting considers only the support of\nthe Lasso solution, avoiding any information about signs or amplitudes. To this\nend we define a sign consistent refitting as an arbitrary refitting procedure,\npreserving the signs of the first step Lasso solution and provide Oracle\ninequalities for such estimators. Finally, we consider special refitting\nstrategies: Bregman Lasso and Boosted Lasso. Bregman Lasso has a fruitful\nproperty to converge to the Sign-Least-Squares refitting (Least-Squares with\nsign constraints), which provides with greater interpretability. We\nadditionally study the Bregman Lasso refitting in the case of orthogonal\ndesign, providing with simple intuition behind the proposed method. Boosted\nLasso, in contrast, considers information about magnitudes of the first Lasso\nstep and allows to develop better oracle rates for prediction. Finally, we\nconduct an extensive numerical study to show advantages of one approach over\nothers in different synthetic and semi-real scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:28:29 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 00:33:16 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 18:24:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Hebiri", "Mohamed", ""], ["Salmon", "Joseph", ""]]}, {"id": "1707.05379", "submitter": "Lionel Truquet", "authors": "Lionel Truquet", "title": "Efficient semiparametric estimation in time-varying regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study semiparametric inference in some linear regression models with\ntime-varying coefficients, dependent regressors and dependent errors. This\nproblem, which has been considered recently by Zhang and Wu (2012) under the\nfunctional dependence measure, is interesting for parsimony reasons or for\ntesting stability of some coefficients in a linear regression model. In this\npaper, we propose a different procedure for estimating non time-varying\nparameters at the rate root n, in the spirit of the method introduced by\nRobinson (1988) for partially linear models. When the errors in the model are\nmartingale differences, this approach can lead to more effcient estimates than\nthe method considered in Zhang and Wu (2012). For a time-varying AR process\nwith exogenous covariates and conditionally Gaussian errors, we derive a notion\nof efficient information matrix from a convolution theorem adapted to\ntriangular arrays. For independent but non identically distributed Gaussian\nerrors, we construct an asymptotically efficient estimator in a semiparametric\nsense.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:35:42 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Truquet", "Lionel", ""]]}, {"id": "1707.05662", "submitter": "Vasilis Kontonis", "authors": "Dimitris Fotakis, Vasilis Kontonis, Piotr Krysta, and Paul Spirakis", "title": "Learning Powers of Poisson Binomial Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of simultaneously learning all powers of a Poisson\nBinomial Distribution (PBD). A PBD of order $n$ is the distribution of a sum of\n$n$ mutually independent Bernoulli random variables $X_i$, where\n$\\mathbb{E}[X_i] = p_i$. The $k$'th power of this distribution, for $k$ in a\nrange $[m]$, is the distribution of $P_k = \\sum_{i=1}^n X_i^{(k)}$, where each\nBernoulli random variable $X_i^{(k)}$ has $\\mathbb{E}[X_i^{(k)}] = (p_i)^k$.\nThe learning algorithm can query any power $P_k$ several times and succeeds in\nlearning all powers in the range, if with probability at least $1- \\delta$:\ngiven any $k \\in [m]$, it returns a probability distribution $Q_k$ with total\nvariation distance from $P_k$ at most $\\epsilon$. We provide almost matching\nlower and upper bounds on query complexity for this problem. We first show a\nlower bound on the query complexity on PBD powers instances with many distinct\nparameters $p_i$ which are separated, and we almost match this lower bound by\nexamining the query complexity of simultaneously learning all the powers of a\nspecial class of PBD's resembling the PBD's of our lower bound. We study the\nfundamental setting of a Binomial distribution, and provide an optimal\nalgorithm which uses $O(1/\\epsilon^2)$ samples. Diakonikolas, Kane and Stewart\n[COLT'16] showed a lower bound of $\\Omega(2^{1/\\epsilon})$ samples to learn the\n$p_i$'s within error $\\epsilon$. The question whether sampling from powers of\nPBDs can reduce this sampling complexity, has a negative answer since we show\nthat the exponential number of samples is inevitable. Having sampling access to\nthe powers of a PBD we then give a nearly optimal algorithm that learns its\n$p_i$'s. To prove our two last lower bounds we extend the classical minimax\nrisk definition from statistics to estimating functions of sequences of\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:02:43 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kontonis", "Vasilis", ""], ["Krysta", "Piotr", ""], ["Spirakis", "Paul", ""]]}, {"id": "1707.05702", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan, Sebastien Roch", "title": "Necessary and sufficient conditions for consistent root reconstruction\n  in Markov models on trees", "comments": "30 pages, 3 figures, title of reference [FR] is updated", "journal-ref": "Electronic Journal of Probability, Vol. 23 (47), 1-24, 2018", "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish necessary and sufficient conditions for consistent root\nreconstruction in continuous-time Markov models with countable state space on\nbounded-height trees. Here a root state estimator is said to be consistent if\nthe probability that it returns to the true root state converges to 1 as the\nnumber of leaves tends to infinity. We also derive quantitative bounds on the\nerror of reconstruction. Our results answer a question of Gascuel and Steel and\nhave implications for ancestral sequence reconstruction in a classical\nevolutionary model of nucleotide insertion and deletion.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:48:37 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 14:54:13 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 15:00:39 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Fan", "Wai-Tong Louis", ""], ["Roch", "Sebastien", ""]]}, {"id": "1707.05708", "submitter": "Didier Rulliere", "authors": "Fran\\c{c}ois Bachoc (IMT, GdR MASCOT-NUM), Nicolas Durrande (GdR\n  MASCOT-NUM), Didier Rulli\\`ere (Mines Saint-\\'Etienne MSE, LIMOS,\n  FAYOL-ENSMSE), Cl\\'ement Chevalier (UNINE, GdR MASCOT-NUM)", "title": "Properties and comparison of some Kriging sub-model aggregation methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kriging is a widely employed technique, in particular for computer\nexperiments, in machine learning or in geostatistics. An important challenge\nfor Kriging is the computational burden when the data set is large. This\narticle focuses on a class of methods aiming at decreasing this computational\ncost, consisting in aggregating Kriging predictors based on smaller data\nsubsets. It proves that aggregation methods that ignore the covariancebetween\nsub-models can yield an inconsistent final Kriging prediction. In contrast, a\ntheoretical study of the nested Kriging method shows additional attractive\nproperties for it: First, this predictor is consistent, second it can be\ninterpreted as an exact conditional distribution for a modified process and\nthird, the conditional covariances given the observations can be computed\nefficiently. This article also includes a theoretical and numerical analysis of\nhow the assignment of the observation points to the sub-models can affect the\nprediction ability of the aggregated model. Finally, the nested Kriging method\nis extended to measurement errors and to universal Kriging.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 11:04:18 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 07:58:11 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT, GdR MASCOT-NUM"], ["Durrande", "Nicolas", "", "GdR\n  MASCOT-NUM"], ["Rulli\u00e8re", "Didier", "", "Mines Saint-\u00c9tienne MSE, LIMOS,\n  FAYOL-ENSMSE"], ["Chevalier", "Cl\u00e9ment", "", "UNINE, GdR MASCOT-NUM"]]}, {"id": "1707.05711", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan, Sebastien Roch", "title": "Statistically consistent and computationally efficient inference of\n  ancestral DNA sequences in the TKF91 model under dense taxon sampling", "comments": "Title modified, 31 pages, 2 Figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary biology, the speciation history of living organisms is\nrepresented graphically by a phylogeny, that is, a rooted tree whose leaves\ncorrespond to current species and branchings indicate past speciation events.\nPhylogenies are commonly estimated from molecular sequences, such as DNA\nsequences, collected from the species of interest. At a high level, the idea\nbehind this inference is simple: the further apart in the Tree of Life are two\nspecies, the greater is the number of mutations to have accumulated in their\ngenomes since their most recent common ancestor. In order to obtain accurate\nestimates in phylogenetic analyses, it is standard practice to employ\nstatistical approaches based on stochastic models of sequence evolution on a\ntree. For tractability, such models necessarily make simplifying assumptions\nabout the evolutionary mechanisms involved. In particular, commonly omitted are\ninsertions and deletions of nucleotides -- also known as indels.\n  Properly accounting for indels in statistical phylogenetic analyses remains a\nmajor challenge in computational evolutionary biology. Here we consider the\nproblem of reconstructing ancestral sequences on a known phylogeny in a model\nof sequence evolution incorporating nucleotide substitutions, insertions and\ndeletions, specifically the classical TKF91 process. We focus on the case of\ndense phylogenies of bounded height, which we refer to as the taxon-rich\nsetting, where statistical consistency is achievable. We give the first\npolynomial-time ancestral reconstruction algorithm with provable guarantees\nunder constant rates of mutation. Our algorithm succeeds when the phylogeny\nsatisfies the \"big bang\" condition, a necessary and sufficient condition for\nstatistical consistency in this context.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:55:57 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 14:50:49 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 01:42:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Fan", "Wai-Tong Louis", ""], ["Roch", "Sebastien", ""]]}, {"id": "1707.05857", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Mehmet Niyazi Cankaya, Olcay Arslan", "title": "On the Robustness and Asymptotic Properties for Maximum Likelihood\n  Estimators of Parameters in Exponential Power and its Scale Mixture Form\n  Distributions", "comments": "28 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The normality assumption on data set is very restrictive approach for\nmodelling. The generalized form of normal distribution, named as an exponential\npower (EP) distribution, and its scale mixture form have been considered\nextensively to overcome the problem for modelling non-normal data set since\nlast decades. However, examining the robustness properties of maximum\nlikelihood (ML) estimators of parameters in these distributions, such as the in\nuence function, gross-error sensitivity, breakdown point and\ninformation-standardized sensitivity, has not been considered together. The\nwell-known asymptotic properties of ML estimators of location, scale and added\nskewness parameters in EP and its scale mixture form distributions are studied\nand also these ML estimators for location, scale and scale variant (skewness)\nparameters can be represented as an iterative reweighting algorithm to compute\nthe estimates of these parameters simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:11:43 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Cankaya", "Mehmet Niyazi", ""], ["Arslan", "Olcay", ""]]}, {"id": "1707.05924", "submitter": "Thomas Lumley", "authors": "Thomas Lumley", "title": "Robustness of semiparametric efficiency in nearly-true models for\n  two-phase samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the performance of efficient and AIPW estimators under two-phase\nsampling when the complete-data model is nearly correctly specified, in the\nsense that the misspecification is not reliably detectable from the data by any\npossible diagnostic or test. Asymptotic results for these nearly true models\nare obtained by representing them as sequences of misspecified models that are\nmutually contiguous with a correctly specified model. We find that for the\nleast-favourable direction of model misspecification the bias in the efficient\nestimator induced can be comparable to the extra variability in the AIPW\nestimator, so that the mean squared error of the efficient estimator is no\nlonger lower. This can happen when the most-powerful test for the model\nmisspecification still has modest power. We verify that the theoretical results\nagree with simulation in three examples: a simple informative-sampling model\nfor a Normal mean, logistic regression in the classical case-control design,\nand linear regression in a two-phase design.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:08:46 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Lumley", "Thomas", ""]]}, {"id": "1707.05987", "submitter": "James Ridgway", "authors": "James Ridgway", "title": "Probably approximate Bayesian computation: nonasymptotic convergence of\n  ABC under misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a widely used inference method in\nBayesian statistics to bypass the point-wise computation of the likelihood. In\nthis paper we develop theoretical bounds for the distance between the\nstatistics used in ABC. We show that some versions of ABC are inherently robust\nto misspecification. The bounds are given in the form of oracle inequalities\nfor a finite sample size. The dependence on the dimension of the parameter\nspace and the number of statistics is made explicit. The results are shown to\nbe amenable to oracle inequalities in parameter space. We apply our theoretical\nresults to given prior distributions and data generating processes, including a\nnon-parametric regression model. In a second part of the paper, we propose a\nsequential Monte Carlo (SMC) to sample from the pseudo-posterior, improving\nupon the state of the art samplers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 09:04:34 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 14:06:20 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ridgway", "James", ""]]}, {"id": "1707.06074", "submitter": "Clement Pellegrini", "authors": "Tristan Benoist (1), F Gamboa (1), C Pellegrini (1) ((1) IMT)", "title": "Quantum non demolition measurements: parameter estimation for mixtures\n  of multinomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Quantum Non Demolition measurements, the sequence of observations is\ndistributed as a mixture of multinomial random variables. Parameters of the\ndynamics are naturally encoded into this family of distributions. We show the\nlocal asymptotic mixed normality of the underlying statistical model and the\nconsistency of the maximum likelihood estimator. Furthermore, we prove the\nasymptotic optimality of this estimator as it saturates the usual Cram\\'er Rao\nbound.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:19:43 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Benoist", "Tristan", "", "IMT"], ["Gamboa", "F", "", "IMT"], ["Pellegrini", "C", "", "IMT"]]}, {"id": "1707.06213", "submitter": "Matthew Thorpe", "authors": "Dejan Slep\\v{c}ev and Matthew Thorpe", "title": "Analysis of $p$-Laplacian Regularization in Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a family of regression problems in a semi-supervised setting.\nThe task is to assign real-valued labels to a set of $n$ sample points,\nprovided a small training subset of $N$ labeled points. A goal of\nsemi-supervised learning is to take advantage of the (geometric) structure\nprovided by the large number of unlabeled data when assigning labels. We\nconsider random geometric graphs, with connection radius $\\epsilon(n)$, to\nrepresent the geometry of the data set. Functionals which model the task reward\nthe regularity of the estimator function and impose or reward the agreement\nwith the training data. Here we consider the discrete $p$-Laplacian\nregularization.\n  We investigate asymptotic behavior when the number of unlabeled points\nincreases, while the number of training points remains fixed. We uncover a\ndelicate interplay between the regularizing nature of the functionals\nconsidered and the nonlocality inherent to the graph constructions. We\nrigorously obtain almost optimal ranges on the scaling of $\\epsilon(n)$ for the\nasymptotic consistency to hold. We prove that the minimizers of the discrete\nfunctionals in random setting converge uniformly to the desired continuum\nlimit. Furthermore we discover that for the standard model used there is a\nrestrictive upper bound on how quickly $\\epsilon(n)$ must converge to zero as\n$n \\to \\infty$. We introduce a new model which is as simple as the original\nmodel, but overcomes this restriction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:31:14 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 18:32:44 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Slep\u010dev", "Dejan", ""], ["Thorpe", "Matthew", ""]]}, {"id": "1707.06416", "submitter": "Ananya Lahiri", "authors": "Ananya Lahiri and Rituparna Sen", "title": "Fractional Brownian markets with time-varying volatility and\n  high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion processes driven by Fractional Brownian motion (FBM) have often\nbeen considered in modeling stock price dynamics in order to capture the long\nrange dependence of stock price observed in reality. Option prices for such\nmodels had been obtained by Necula (2002) under constant drift and volatility.\nWe obtain option prices under time varying volatility model. The expression\ndepends on volatility and the Hurst parameter in a complicated manner. We\nderive a central limit theorem for the quadratic variation as an estimator for\nvolatility for both the cases, constant as well as time varying volatility.\nThat will help us to find estimators of the option prices and to find their\nasymptotic distributions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 08:48:06 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Lahiri", "Ananya", ""], ["Sen", "Rituparna", ""]]}, {"id": "1707.06467", "submitter": "Casper Albers", "authors": "Casper Albers, Frank Critchley and John Gower", "title": "Explicit minimisation of a convex quadratic under a general quadratic\n  constraint: a global, analytic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach is introduced to a very widely occurring problem, providing\na complete, explicit resolution of it: minimisation of a convex quadratic under\na general quadratic, equality or inequality, constraint. Completeness comes via\nidentification of a set of mutually exclusive and exhaustive special cases.\nExplicitness, via algebraic expressions for each solution set. Throughout,\nunderlying geometry illuminates and informs algebraic development. In\nparticular, centrally to this new approach, affine equivalence is exploited to\nre-express the same problem in simpler coordinate systems. Overall, the\nanalysis presented provides insight into the diverse forms taken both by the\nproblem itself and its solution set, showing how each may be intrinsically\nunstable. Comparisons of this global, analytic approach with the, intrinsically\ncomplementary, local, computational approach of (generalised) trust region\nmethods point to potential synergies between them. Points of contact with\nsimultaneous diagonalisation results are noted.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:14:18 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Albers", "Casper", ""], ["Critchley", "Frank", ""], ["Gower", "John", ""]]}, {"id": "1707.06476", "submitter": "Elie Wolfe", "authors": "Miguel Navascues and Elie Wolfe", "title": "The Inflation Technique Completely Solves the Causal Compatibility\n  Problem", "comments": "Updated to match forthcoming journal publication as closely as\n  possible. Some content removed for brevity. Expanded citations. Most\n  footnotes moved into the main text. Significant changes to subsection 4.1,\n  where we corrected an error in the example of second order inflation not\n  converging, and added an converse example where second order inflation\n  outperforms other techniques", "journal-ref": null, "doi": "10.1515/jci-2018-0008", "report-no": null, "categories": "quant-ph math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal compatibility question asks whether a given causal structure graph\n-- possibly involving latent variables -- constitutes a genuinely plausible\ncausal explanation for a given probability distribution over the graph's\nobserved variables. Algorithms predicated on merely necessary constraints for\ncausal compatibility typically suffer from false negatives, i.e. they admit\nincompatible distributions as apparently compatible with the given graph. In\n[arXiv:1609.00672], one of us introduced the inflation technique for\nformulating useful relaxations of the causal compatibility problem in terms of\nlinear programming. In this work, we develop a formal hierarchy of such causal\ncompatibility relaxations. We prove that inflation is asymptotically tight,\ni.e., that the hierarchy converges to a zero-error test for causal\ncompatibility. In this sense, the inflation technique fulfills a longstanding\ndesideratum in the field of causal inference. We quantify the rate of\nconvergence by showing that any distribution which passes the $n^{th}$-order\ninflation test must be $O\\left(n^{-1/2}\\right)$-close in Euclidean norm to some\ndistribution genuinely compatible with the given causal structure. Furthermore,\nwe show that for many causal structures, the (unrelaxed) causal compatibility\nproblem is faithfully formulated already by either the first or second order\ninflation test.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:38:06 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:37:07 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 19:06:47 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Navascues", "Miguel", ""], ["Wolfe", "Elie", ""]]}, {"id": "1707.06692", "submitter": "Nan Bi", "authors": "Nan Bi, Jelena Markovic, Lucy Xia, Jonathan Taylor", "title": "Inferactive data analysis", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe inferactive data analysis, so-named to denote an interactive\napproach to data analysis with an emphasis on inference after data analysis.\nOur approach is a compromise between Tukey's exploratory (roughly speaking\n\"model free\") and confirmatory data analysis (roughly speaking classical and\n\"model based\"), also allowing for Bayesian data analysis. We view this approach\nas close in spirit to current practice of applied statisticians and data\nscientists while allowing frequentist guarantees for results to be reported in\nthe scientific literature, or Bayesian results where the data scientist may\nchoose the statistical model (and hence the prior) after some initial\nexploratory analysis. While this approach to data analysis does not cover every\nscenario, and every possible algorithm data scientists may use, we see this as\na useful step in concrete providing tools (with frequentist statistical\nguarantees) for current data scientists. The basis of inference we use is\nselective inference [Lee et al., 2016, Fithian et al., 2014], in particular its\nrandomized form [Tian and Taylor, 2015a]. The randomized framework, besides\nproviding additional power and shorter confidence intervals, also provides\nexplicit forms for relevant reference distributions (up to normalization)\nthrough the {\\em selective sampler} of Tian et al. [2016]. The reference\ndistributions are constructed from a particular conditional distribution formed\nfrom what we call a DAG-DAG -- a Data Analysis Generative DAG. As sampling\nconditional distributions in DAGs is generally complex, the selective sampler\nis crucial to any practical implementation of inferactive data analysis. Our\nprincipal goal is in reviewing the recent developments in selective inference\nas well as describing the general philosophy of selective inference.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:59:31 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Bi", "Nan", ""], ["Markovic", "Jelena", ""], ["Xia", "Lucy", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1707.06768", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Integrability conditions for Compound Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compound random measures (CoRM's) are a flexible and tractable framework for\nvectors of completely random measure. In this paper, we provide conditions to\nguarantee the existence of a CoRM. Furthermore, we prove some interesting\nproperties of CoRM's when exponential scores and regularly varying L\\'evy\nintensities are considered.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 06:33:29 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 17:31:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1707.06842", "submitter": "Simon Michael Papalexiou Ph.D", "authors": "Simon Michael Papalexiou", "title": "A unified theory for exact stochastic modelling of univariate and\n  multivariate processes with continuous, mixed type, or discrete marginal\n  distributions and any correlation structure", "comments": "46 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydroclimatic processes are characterized by heterogeneous spatiotemporal\ncorrelation structures and marginal distributions that can be continuous,\nmixed-type, discrete or even binary. Simulating exactly such processes can\ngreatly improve hydrological analysis and design. Yet this challenging task is\naccomplished often by ad hoc and approximate methodologies that are devised for\nspecific variables and purposes. In this study, a single framework is proposed\nallowing the exact simulation of processes with any marginal and any\ncorrelation structure. We unify, extent, and improve of a general-purpose\nmodelling strategy based on the assumption that any process can emerge by\ntransforming a parent Gaussian process with a specific correlation structure. A\nnovel mathematical representation of the parent-Gaussian scheme provides a\nconsistent and fully general description that supersedes previous specific\nparameterizations, resulting in a simple, fast and efficient simulation\nprocedure for every spatiotemporal process. In particular, introducing a simple\nbut flexible procedure we obtain a parametric expression of the correlation\ntransformation function, allowing to assess the correlation structure of the\nparent-Gaussian process that yields the prescribed correlation of the target\nprocess after marginal back transformation. The same framework is also\napplicable for cyclostationary and multivariate modelling. The simulation of a\nvariety of hydroclimatic variables with very different correlation structures\nand marginals, such as precipitation, stream flow, wind speed, humidity,\nextreme events per year, etc., as well as a multivariate application,\nhighlights the flexibility, advantages, and complete generality of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:11:03 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Papalexiou", "Simon Michael", ""]]}, {"id": "1707.06852", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "A Statistical Perspective on Inverse and Inverse Regression Problems", "comments": "To appear in RASHI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems, where in broad sense the task is to learn from the noisy\nresponse about some unknown function, usually represented as the argument of\nsome known functional form, has received wide attention in the general\nscientific disciplines. How- ever, in mainstream statistics such inverse\nproblem paradigm does not seem to be as popular. In this article we provide a\nbrief overview of such problems from a statistical, particularly Bayesian,\nperspective.\n  We also compare and contrast the above class of problems with the perhaps\nmore statistically familiar inverse regression problems, arguing that this\nclass of problems contains the traditional class of inverse problems. In course\nof our review we point out that the statistical literature is very scarce with\nrespect to both the inverse paradigms, and substantial research work is still\nnecessary to develop the fields.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:30:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1707.07158", "submitter": "Mohammad Arashi", "authors": "Jibo Wu, Yasin Asar and M. Arashi", "title": "On the restricted almost unbiased Liu estimator in the Logistic\n  regression model", "comments": "15 pages, 1 Figure, 9 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that when the multicollinearity exists in the logistic regression\nmodel, variance of maximum likelihood estimator is unstable. As a remedy, in\nthe context of biased shrinkage ridge estimation, Chang (2015) introduced an\nalmost unbiased Liu estimator in the logistic regression model. Making use of\nhis approach, when some prior knowledge in the form of linear restrictions are\nalso available, we introduce a restricted almost unbiased Liu estimator in the\nlogistic regression model. Statistical properties of this newly defined\nestimator are derived and some comparison result are also provided in the form\nof theorems. A Monte Carlo simulation study along with a real data example are\ngiven to investigate the performance of this estimator.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 13:23:17 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wu", "Jibo", ""], ["Asar", "Yasin", ""], ["Arashi", "M.", ""]]}, {"id": "1707.07163", "submitter": "Salem Said", "authors": "Salem Said, Lionel Bombrun, Yannick Berthoumieu", "title": "Warped Riemannian metrics for location-scale models", "comments": "first version, before submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper shows that warped Riemannian metrics, a class of Riemannian\nmetrics which play a prominent role in Riemannian geometry, are also of\nfundamental importance in information geometry. Precisely, the paper features a\nnew theorem, which states that the Rao-Fisher information metric of any\nlocation-scale model, defined on a Riemannian manifold, is a warped Riemannian\nmetric, whenever this model is invariant under the action of some Lie group.\nThis theorem is a valuable tool in finding the expression of the Rao-Fisher\ninformation metric of location-scale models defined on high-dimensional\nRiemannian manifolds. Indeed, a warped Riemannian metric is fully determined by\nonly two functions of a single variable, irrespective of the dimension of the\nunderlying Riemannian manifold. Starting from this theorem, several original\ncontributions are made. The expression of the Rao-Fisher information metric of\nthe Riemannian Gaussian model is provided, for the first time in the\nliterature. A generalised definition of the Mahalanobis distance is introduced,\nwhich is applicable to any location-scale model defined on a Riemannian\nmanifold. The solution of the geodesic equation is obtained, for any Rao-Fisher\ninformation metric defined in terms of warped Riemannian metrics. Finally,\nusing a mixture of analytical and numerical computations, it is shown that the\nparameter space of the von Mises-Fisher model of $n$-dimensional directional\ndata, when equipped with its Rao-Fisher information metric, becomes a Hadamard\nmanifold, a simply-connected complete Riemannian manifold of negative sectional\ncurvature, for $n = 2,\\ldots,8$. Hopefully, in upcoming work, this will be\nproved for any value of $n$.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 13:39:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Said", "Salem", ""], ["Bombrun", "Lionel", ""], ["Berthoumieu", "Yannick", ""]]}, {"id": "1707.07269", "submitter": "Damien Garreau", "authors": "Damien Garreau, Wittawat Jitkrittum, Motonobu Kanagawa", "title": "Large sample analysis of the median heuristic", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In kernel methods, the median heuristic has been widely used as a way of\nsetting the bandwidth of RBF kernels. While its empirical performances make it\na safe choice under many circumstances, there is little theoretical\nunderstanding of why this is the case. Our aim in this paper is to advance our\nunderstanding of the median heuristic by focusing on the setting of kernel\ntwo-sample test. We collect new findings that may be of interest for both\ntheoreticians and practitioners. In theory, we provide a convergence analysis\nthat shows the asymptotic normality of the bandwidth chosen by the median\nheuristic in the setting of kernel two-sample test. Systematic empirical\ninvestigations are also conducted in simple settings, comparing the\nperformances based on the bandwidths chosen by the median heuristic and those\nby the maximization of test power.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 09:32:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 13:25:51 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 09:48:51 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Garreau", "Damien", ""], ["Jitkrittum", "Wittawat", ""], ["Kanagawa", "Motonobu", ""]]}, {"id": "1707.07275", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Likelihood test in permutations with bias. Premier League and La Liga:\n  surprises during the last 25 seasons", "comments": "Bibliography updated. Thanks to Prof Karlsson to have suggested the\n  paper [8] H. Stern. Models for distributions on permutations. JASA (1990)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the models of permutations with bias, which are\nrandom permutations of a set, biased by some preference values. We present a\nnew parametric test, together with an efficient way to calculate its p-value.\nThe final tables of the English and Spanish major soccer leagues are tested\naccording to this new procedure, to discover whether these results were aligned\nwith expectations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 10:17:40 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 10:23:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1707.07329", "submitter": "Evgeny Burnaev", "authors": "A. V. Artemov and E. V. Burnaev", "title": "Optimal estimation of a signal perturbed by a fractional Brownian noise", "comments": "8 pages, 1 figure", "journal-ref": "Theory Probab. Appl., 60(1), 126-134, 2016", "doi": "10.1137/S0040585X97T987521", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal estimation of the value of a vector\nparameter $\\thetavector=(\\theta_0,\\ldots,\\theta_n)^{\\top}$ of the drift term in\na fractional Brownian motion represented by the finite sum\n$\\sum_{i=0}^{n}\\theta_{i}\\varphi_{i}(t)$ over known functions $\\varphi_i(t)$,\n$\\alli$. For the value of parameter $\\thetavector$, we obtain a maximum\nlikelihood estimate as well as Bayesian estimates for normal and uniform a\npriori distributions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 18:34:57 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Artemov", "A. V.", ""], ["Burnaev", "E. V.", ""]]}, {"id": "1707.07637", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "Copy the dynamics using a learning machine", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to generally construct a dynamical system to simulate a black\nsystem without recovering the equations of motion of the latter? Here we show\nthat this goal can be approached by a learning machine. Trained by a set of\ninput-output responses or a segment of time series of a black system, a\nlearning machine can be served as a copy system to mimic the dynamics of\nvarious black systems. It can not only behave as the black system at the\nparameter set that the training data are made, but also recur the evolution\nhistory of the black system. As a result, the learning machine provides an\neffective way for prediction, and enables one to probe the global dynamics of a\nblack system. These findings have significance for practical systems whose\nequations of motion cannot be approached accurately. Examples of copying the\ndynamics of an artificial neural network, the Lorenz system, and a variable\nstar are given. Our idea paves a possible way towards copy a living brain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 16:35:23 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1707.08092", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Mark Rudelson", "title": "Restricted Eigenvalue from Stable Rank with Applications to Sparse\n  Linear Regression", "comments": "27 pages, Updated paper with stronger results, Corrected Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional settings, where the data dimension ($d$) far exceeds the\nnumber of observations ($n$), are common in many statistical and machine\nlearning applications. Methods based on $\\ell_1$-relaxation, such as Lasso, are\nvery popular for sparse recovery in these settings. Restricted Eigenvalue (RE)\ncondition is among the weakest, and hence the most general, condition in\nliterature imposed on the Gram matrix that guarantees nice statistical\nproperties for the Lasso estimator. It is natural to ask: what families of\nmatrices satisfy the RE condition? Following a line of work in this area, we\nconstruct a new broad ensemble of dependent random design matrices that have an\nexplicit RE bound. Our construction starts with a fixed (deterministic) matrix\n$X \\in \\mathbb{R}^{n \\times d}$ satisfying a simple stable rank condition, and\nwe show that a matrix drawn from the distribution $X \\Phi^\\top \\Phi$, where\n$\\Phi \\in \\mathbb{R}^{m \\times d}$ is a subgaussian random matrix, with high\nprobability, satisfies the RE condition. This construction allows incorporating\na fixed matrix that has an easily {\\em verifiable} condition into the design\nprocess, and allows for generation of {\\em compressed} design matrices that\nhave a lower storage requirement than a standard design matrix. We give two\napplications of this construction to sparse linear regression problems,\nincluding one to a compressed sparse regression setting where the regression\nalgorithm only has access to a compressed representation of a fixed design\nmatrix $X$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:15:18 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 03:15:26 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 15:00:12 GMT"}, {"version": "v4", "created": "Sat, 17 Feb 2018 23:44:07 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""]]}, {"id": "1707.08505", "submitter": "Almut Veraart", "authors": "Andrea Granelli and Almut E. D. Veraart", "title": "A weak law of large numbers for estimating the correlation in bivariate\n  Brownian semistationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents various weak laws of large numbers for the so-called\nrealised covariation of a bivariate stationary stochastic process which is not\na semimartingale. More precisely, we consider two cases: Bivariate moving\naverage processes with stochastic correlation and bivariate Brownian\nsemistationary processes with stochastic correlation. In both cases, we can\nshow that the (possibly scaled) realised covariation converges to the\nintegrated (possibly volatility modulated) stochastic correlation process.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 15:43:00 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Granelli", "Andrea", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1707.08507", "submitter": "Almut Veraart", "authors": "Andrea Granelli and Almut E. D. Veraart", "title": "A central limit theorem for the realised covariation of a bivariate\n  Brownian semistationary process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a weak law of large numbers and a central limit theorem\nfor the scaled realised covariation of a bivariate Brownian semistationary\nprocess. The novelty of our results lies in the fact that we derive the\nsuitable asymptotic theory both in a multivariate setting and outside the\nclassical semimartingale framework. The proofs rely heavily on recent\ndevelopments in Malliavin calculus.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 15:50:00 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Granelli", "Andrea", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1707.08644", "submitter": "Arthur Berg", "authors": "J. G. Liao and Arthur Berg", "title": "Sharpening Jensen's Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new sharpened version of the Jensen's inequality. The\nproposed new bound is simple and insightful, is broadly applicable by imposing\nminimum assumptions, and provides fairly accurate result in spite of its simple\nform. Applications to the moment generating function, power mean inequalities,\nand Rao-Blackwell estimation are presented. This presentation can be\nincorporated in any calculus-based statistical course.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 21:13:41 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 23:05:18 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Liao", "J. G.", ""], ["Berg", "Arthur", ""]]}, {"id": "1707.08788", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Kengo Kamatani, Hiroki Masuda", "title": "Bayesian inference for Stable Levy driven Stochastic Differential\n  Equations with high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider parametric Bayesian inference for stochastic\ndifferential equations (SDE) driven by a pure-jump stable Levy process, which\nis observed at high frequency. In most cases of practical interest, the\nlikelihood function is not available, so we use a quasi-likelihood and place an\nassociated prior on the unknown parameters. It is shown under regularity\nconditions that there is a Bernstein-von Mises theorem associated to the\nposterior. We then develop a Markov chain Monte Carlo (MCMC) algorithm for\nBayesian inference and assisted by our theoretical results, we show how to\nscale Metropolis-Hastings proposals when the frequency of the data grows, in\norder to prevent the acceptance ratio going to zero in the large data limit.\nOur algorithm is presented on numerical examples that help to verify our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 09:22:13 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Jasra", "Ajay", ""], ["Kamatani", "Kengo", ""], ["Masuda", "Hiroki", ""]]}, {"id": "1707.08904", "submitter": "Marianna Bolla CSc", "authors": "Marianna Bolla, Ahmed Elbanna, Jozsef Mala", "title": "Estimating parameters of a directed weighted graph model with\n  beta-distributed edge-weights", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a directed, weighted random graph model, where the edge-weights\nare independent and beta-distributed with parameters depending on their\nendpoints. We will show that the row- and column-sums of the transformed\nedge-weight matrix are sufficient statistics for the parameters, and use the\ntheory of exponential families to prove that the ML estimate of the parameters\nexists and is unique. Then an algorithm to find this estimate is introduced\ntogether with convergence proof that uses properties of the digamma function.\nSimulation results and applications are also presented.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 15:12:04 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 15:53:20 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 14:08:35 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Bolla", "Marianna", ""], ["Elbanna", "Ahmed", ""], ["Mala", "Jozsef", ""]]}, {"id": "1707.09037", "submitter": "Thomas Sch\\\"urmann", "authors": "Thomas Sch\\\"urmann and Ingo Hoffmann", "title": "On Biased Correlation Estimation", "comments": "5 pages, 6 figures, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST physics.data-an q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, underestimation of risk is something which should be avoided as\nfar as possible. Especially in financial asset management, equity risk is\ntypically characterized by the measure of portfolio variance, or indirectly by\nquantities which are derived from it. Since there is a linear dependency of the\nvariance and the empirical correlation between asset classes, one is compelled\nto control or to avoid the possibility of underestimating correlation\ncoefficients. In the present approach, we formalize common practice and\nclassify these approaches by computing their probability of underestimation. In\naddition, we introduce a new estimator which is characterized by having the\nadvantage of a constant and controllable probability of underestimation. We\nprove that the new estimator is statistically consistent.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 18:18:42 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Sch\u00fcrmann", "Thomas", ""], ["Hoffmann", "Ingo", ""]]}, {"id": "1707.09114", "submitter": "Junwei Lu", "authors": "Junwei Lu, Matey Neykov and Han Liu", "title": "Adaptive Inferential Method for Monotone Graph Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of undirected graphical model inference. In many\napplications, instead of perfectly recovering the unknown graph structure, a\nmore realistic goal is to infer some graph invariants (e.g., the maximum\ndegree, the number of connected subgraphs, the number of isolated nodes). In\nthis paper, we propose a new inferential framework for testing nested multiple\nhypotheses and constructing confidence intervals of the unknown graph\ninvariants under undirected graphical models. Compared to perfect graph\nrecovery, our methods require significantly weaker conditions. This paper makes\ntwo major contributions: (i) Methodologically, for testing nested multiple\nhypotheses, we propose a skip-down algorithm on the whole family of monotone\ngraph invariants (The invariants which are non-decreasing under addition of\nedges). We further show that the same skip-down algorithm also provides valid\nconfidence intervals for the targeted graph invariants. (ii) Theoretically, we\nprove that the length of the obtained confidence intervals are optimal and\nadaptive to the unknown signal strength. We also prove generic lower bounds for\nthe confidence interval length for various invariants. Numerical results on\nboth synthetic simulations and a brain imaging dataset are provided to\nillustrate the usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 06:08:44 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Lu", "Junwei", ""], ["Neykov", "Matey", ""], ["Liu", "Han", ""]]}, {"id": "1707.09161", "submitter": "Ramji Venkataramanan", "authors": "Pavan Srinath and Ramji Venkataramanan", "title": "Empirical Bayes Estimators for High-Dimensional Sparse Vectors", "comments": "35 pages, to appear in Information and Inference: A Journal of the\n  IMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a high-dimensional sparse vector\n$\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ from an observation in i.i.d. Gaussian\nnoise is considered. The performance is measured using squared-error loss. An\nempirical Bayes shrinkage estimator, derived using a Bernoulli-Gaussian prior,\nis analyzed and compared with the well-known soft-thresholding estimator. We\nobtain concentration inequalities for the Stein's unbiased risk estimate and\nthe loss function of both estimators. The results show that for large $n$, both\nthe risk estimate and the loss function concentrate on deterministic values\nclose to the true risk.\n  Depending on the underlying $\\boldsymbol{\\theta}$, either the proposed\nempirical Bayes (eBayes) estimator or soft-thresholding may have smaller loss.\nWe consider a hybrid estimator that attempts to pick the better of the\nsoft-thresholding estimator and the eBayes estimator by comparing their risk\nestimates. It is shown that: i) the loss of the hybrid estimator concentrates\non the minimum of the losses of the two competing estimators, and ii) the risk\nof the hybrid estimator is within order $\\frac{1}{\\sqrt{n}}$ of the minimum of\nthe two risks. Simulation results are provided to support the theoretical\nresults. Finally, we use the eBayes and hybrid estimators as denoisers in the\napproximate message passing (AMP) algorithm for compressed sensing, and show\nthat their performance is superior to the soft-thresholding denoiser in a wide\nrange of settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 09:17:12 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 07:54:54 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 14:30:05 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Srinath", "Pavan", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1707.09350", "submitter": "Michael Schaub", "authors": "Marco Avella-Medina and Francesca Parise and Michael T. Schaub and\n  Santiago Segarra", "title": "Centrality measures for graphons: Accounting for uncertainty in networks", "comments": "Authors ordered alphabetically, all authors contributed equally. 21\n  pages, 7 figures", "journal-ref": "IEEE Transactions on Network Science and Engineering, 2020, vol.\n  7, no. 1, pp. 520-537", "doi": "10.1109/TNSE.2018.2884235", "report-no": null, "categories": "cs.SI cs.SY math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As relational datasets modeled as graphs keep increasing in size and their\ndata-acquisition is permeated by uncertainty, graph-based analysis techniques\ncan become computationally and conceptually challenging. In particular, node\ncentrality measures rely on the assumption that the graph is perfectly known --\na premise not necessarily fulfilled for large, uncertain networks. Accordingly,\ncentrality measures may fail to faithfully extract the importance of nodes in\nthe presence of uncertainty. To mitigate these problems, we suggest a\nstatistical approach based on graphon theory: we introduce formal definitions\nof centrality measures for graphons and establish their connections to\nclassical graph centrality measures. A key advantage of this approach is that\ncentrality measures defined at the modeling level of graphons are inherently\nrobust to stochastic variations of specific graph realizations. Using the\ntheory of linear integral operators, we define degree, eigenvector, Katz and\nPageRank centrality functions for graphons and establish concentration\ninequalities demonstrating that graphon centrality functions arise naturally as\nlimits of their counterparts defined on sequences of graphs of increasing size.\nThe same concentration inequalities also provide high-probability bounds\nbetween the graphon centrality functions and the centrality measures on any\nsampled graph, thereby establishing a measure of uncertainty of the measured\ncentrality score. The same concentration inequalities also provide\nhigh-probability bounds between the graphon centrality functions and the\ncentrality measures on any sampled graph, thereby establishing a measure of\nuncertainty of the measured centrality score.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 17:47:00 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 14:58:27 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 18:55:14 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 23:34:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Avella-Medina", "Marco", ""], ["Parise", "Francesca", ""], ["Schaub", "Michael T.", ""], ["Segarra", "Santiago", ""]]}, {"id": "1707.09561", "submitter": "Jelena Bradic", "authors": "Jue Hou, Jelena Bradic, Ronghui Xu", "title": "Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:53:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:03:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1707.09592", "submitter": "Xiaoqiang Ren", "authors": "Xiaoqiang Ren, Jiaqi Yan and Yilin Mo", "title": "Binary Hypothesis Testing with Byzantine Sensors: Fundamental Trade-off\n  Between Security and Efficiency", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2788420", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies binary hypothesis testing based on measurements from a set\nof sensors, a subset of which can be compromised by an attacker. The\nmeasurements from a compromised sensor can be manipulated arbitrarily by the\nadversary. The asymptotic exponential rate, with which the probability of error\ngoes to zero, is adopted to indicate the detection performance of a detector.\nIn practice, we expect the attack on sensors to be sporadic, and therefore the\nsystem may operate with all the sensors being benign for extended period of\ntime. This motivates us to consider the trade-off between the detection\nperformance of a detector, i.e., the probability of error, when the attacker is\nabsent (defined as efficiency) and the worst-case detection performance when\nthe attacker is present (defined as security). We first provide the fundamental\nlimits of this trade-off, and then propose a detection strategy that achieves\nthese limits. We then consider a special case, where there is no trade-off\nbetween security and efficiency. In other words, our detection strategy can\nachieve the maximal efficiency and the maximal security simultaneously. Two\nextensions of the secure hypothesis testing problem are also studied and\nfundamental limits and achievability results are provided: 1) a subset of\nsensors, namely \"secure\" sensors, are assumed to be equipped with better\nsecurity countermeasures and hence are guaranteed to be benign, 2) detection\nperformance with unknown number of compromised sensors. Numerical examples are\ngiven to illustrate the main results.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 07:50:06 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 02:50:11 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 06:47:26 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Ren", "Xiaoqiang", ""], ["Yan", "Jiaqi", ""], ["Mo", "Yilin", ""]]}, {"id": "1707.09604", "submitter": "Jonas Brehmer", "authors": "Jonas Brehmer", "title": "Elicitability and its Application in Risk Management", "comments": "97 pages, Master's thesis, University of Mannheim, April 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elicitability is a property of $\\mathbb{R}^k$-valued functionals defined on a\nset of distribution functions. These functionals represent statistical\nproperties of a distribution, for instance its mean, variance, or median. They\nare called elicitable if there exists a scoring function such that the expected\nscore under a distribution takes its unique minimum at the functional value of\nthis distribution. If such a scoring function exists, it is called strictly\nconsistent for the functional. Motivated by the recent findings of Fissler and\nZiegel concerning higher order elicitability, this thesis reviews the most\nimportant results, examples, and applications which are found in the relevant\nliterature. Moreover, we also contribute our own examples and findings in order\nto give the reader a well-founded overview of the topic as well as of the most\nused tools and techniques. We include necessary and sufficient conditions for\nstrictly consistent scoring functions, several elicitable as well as\nnon-elicitable functionals and the use of elicitability in forecast comparison,\nregression, and estimation. Special emphasis is placed on quantitative risk\nmanagement and the result that Value at Risk and Expected Shortfall are jointly\nelicitable.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 10:21:02 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Brehmer", "Jonas", ""]]}, {"id": "1707.09631", "submitter": "Ruoqing Zhu", "authors": "Yifan Cui, Ruoqing Zhu, Mai Zhou, Michael Kosorok", "title": "Consistency of survival tree and forest models: splitting bias and\n  correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random survival forest and survival trees are popular models in statistics\nand machine learning. However, there is a lack of general understanding\nregarding consistency, splitting rules and influence of the censoring\nmechanism. In this paper, we investigate the statistical properties of existing\nmethods from several interesting perspectives. First, we show that traditional\nsplitting rules with censored outcomes rely on a biased estimation of the\nwithin-node failure distribution. To exactly quantify this bias, we develop a\nconcentration bound of the within-node estimation based on non i.i.d. samples\nand apply it to the entire forest. Second, we analyze the entanglement between\nthe failure and censoring distributions caused by univariate splits, and show\nthat without correcting the bias at an internal node, survival tree and forest\nmodels can still enjoy consistency under suitable conditions. In particular, we\ndemonstrate this property under two cases: a finite-dimensional case where the\nsplitting variables and cutting points are chosen randomly, and a\nhigh-dimensional case where the covariates are weakly correlated. Our results\ncan also degenerate into an independent covariate setting, which is commonly\nused in the random forest literature for high-dimensional sparse models.\nHowever, it may not be avoidable that the convergence rate depends on the total\nnumber of variables in the failure and censoring distributions. Third, we\npropose a new splitting rule that compares bias-corrected cumulative hazard\nfunctions at each internal node. We show that the rate of consistency of this\nnew model depends only on the number of failure variables, which improves from\nnon-bias-corrected versions. We perform simulation studies to confirm that this\ncan substantially benefit the prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:24:02 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 03:56:30 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 00:19:56 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2019 03:17:26 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Cui", "Yifan", ""], ["Zhu", "Ruoqing", ""], ["Zhou", "Mai", ""], ["Kosorok", "Michael", ""]]}, {"id": "1707.09632", "submitter": "Ruoqing Zhu", "authors": "Yifan Cui, Ruoqing Zhu, Michael Kosorok", "title": "Tree based weighted learning for estimating individualized treatment\n  rules with censored data", "comments": "Accepted by EJS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating individualized treatment rules is a central task for personalized\nmedicine. [zhao2012estimating] and [zhang2012robust] proposed outcome weighted\nlearning to estimate individualized treatment rules directly through maximizing\nthe expected outcome without modeling the response directly. In this paper, we\nextend the outcome weighted learning to right censored survival data without\nrequiring either an inverse probability of censoring weighting or a\nsemiparametric modeling of the censoring and failure times as done in\n[zhao2015doubly]. To accomplish this, we take advantage of the tree based\napproach proposed in [zhu2012recursively] to nonparametrically impute the\nsurvival time in two different ways. The first approach replaces the reward of\neach individual by the expected survival time, while in the second approach\nonly the censored observations are imputed by their conditional expected\nfailure times. We establish consistency and convergence rates for both\nestimators. In simulation studies, our estimators demonstrate improved\nperformance compared to existing methods. We also illustrate the proposed\nmethod on a phase III clinical trial of non-small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:24:14 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 05:15:02 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Cui", "Yifan", ""], ["Zhu", "Ruoqing", ""], ["Kosorok", "Michael", ""]]}, {"id": "1707.09697", "submitter": "Wanli Qiao", "authors": "Wanli Qiao", "title": "Asymptotics and Optimal Bandwidth for Nonparametric Estimation of\n  Density Level Sets", "comments": "43 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandwidth selection is crucial in the kernel estimation of density level\nsets. A risk based on the symmetric difference between the estimated and true\nlevel sets is usually used to measure their proximity. In this paper we provide\nan asymptotic $L^p$ approximation to this risk, where $p$ is characterized by\nthe weight function in the risk. In particular the excess risk corresponds to\nan $L^2$ type of risk, and is adopted to derive an optimal bandwidth for\nnonparametric level set estimation of $d$-dimensional density functions ($d\\geq\n1$). A direct plug-in bandwidth selector is developed for kernel density level\nset estimation and its efficacy is verified in numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 02:20:34 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 00:50:07 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2019 18:55:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Qiao", "Wanli", ""]]}, {"id": "1707.09825", "submitter": "Yu Guang Wang", "authors": "Vo V. Anh, Philip Broadbridge, Andriy Olenko, Yu Guang Wang", "title": "On Approximation for Fractional Stochastic Partial Differential\n  Equations on the Sphere", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s00477-018-1517-1", "report-no": null, "categories": "math.ST math.AP math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives the exact solution in terms of the Karhunen-Lo\\`{e}ve\nexpansion to a fractional stochastic partial differential equation on the unit\nsphere $\\mathbb{S}^{2}\\subset \\mathbb{R}^{3}$ with fractional Brownian motion\nas driving noise and with random initial condition given by a fractional\nstochastic Cauchy problem. A numerical approximation to the solution is given\nby truncating the Karhunen-Lo\\`{e}ve expansion. We show the convergence rates\nof the truncation errors in degree and the mean square approximation errors in\ntime. Numerical examples using an isotropic Gaussian random field as initial\ncondition and simulations of evolution of cosmic microwave background (CMB) are\ngiven to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 12:52:36 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:00:20 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Anh", "Vo V.", ""], ["Broadbridge", "Philip", ""], ["Olenko", "Andriy", ""], ["Wang", "Yu Guang", ""]]}, {"id": "1707.09971", "submitter": "Cong Ma", "authors": "Yuxin Chen, Jianqing Fan, Cong Ma, Kaizheng Wang", "title": "Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking", "comments": "Add discussions on the setting of the general condition number", "journal-ref": "Annals of Statististics, Volume 47, Number 4 (2019), 2204-2235", "doi": "10.1214/18-AOS1745", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of top-$K$ ranking from pairwise\ncomparisons. Given a collection of $n$ items and a few pairwise comparisons\nacross them, one wishes to identify the set of $K$ items that receive the\nhighest ranks. To tackle this problem, we adopt the logistic parametric model\n--- the Bradley-Terry-Luce model, where each item is assigned a latent\npreference score, and where the outcome of each pairwise comparison depends\nsolely on the relative scores of the two items involved. Recent works have made\nsignificant progress towards characterizing the performance (e.g. the mean\nsquare error for estimating the scores) of several classical methods, including\nthe spectral method and the maximum likelihood estimator (MLE). However, where\nthey stand regarding top-$K$ ranking remains unsettled.\n  We demonstrate that under a natural random sampling model, the spectral\nmethod alone, or the regularized MLE alone, is minimax optimal in terms of the\nsample complexity --- the number of paired comparisons needed to ensure exact\ntop-$K$ identification, for the fixed dynamic range regime. This is\naccomplished via optimal control of the entrywise error of the score estimates.\nWe complement our theoretical studies by numerical experiments, confirming that\nboth methods yield low entrywise errors for estimating the underlying scores.\nOur theory is established via a novel leave-one-out trick, which proves\neffective for analyzing both iterative and non-iterative procedures. Along the\nway, we derive an elementary eigenvector perturbation bound for probability\ntransition matrices, which parallels the Davis-Kahan $\\sin\\Theta$ theorem for\nsymmetric matrices. This also allows us to close the gap between the $\\ell_2$\nerror upper bound for the spectral method and the minimax lower limit.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:33:15 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 03:20:10 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:35:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Wang", "Kaizheng", ""]]}]