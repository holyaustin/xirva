[{"id": "2007.00137", "submitter": "Ioannis Papastathopoulos", "authors": "Justin A. Kasin and Ioannis Papastathopoulos", "title": "A spatial Poisson hurdle model with application to wildfires", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling wildfire occurrences is important for disaster management including\nprevention, detection and suppression of large catastrophic events. We present\na spatial Poisson hurdle model for exploring geographical variation of monthly\ncounts of wildfire occurrences and apply it to Indonesia and Australia. The\nmodel includes two a priori independent spatially structured latent effects\nthat account for residual spatial variation in the probability of wildfire\noccurrence, and the positive count rate given an occurrence. Inference is\nprovided by empirical Bayes using the Laplace approximation to the marginal\nposterior which provides fast inference for latent Gaussian models with sparse\nstructures. In both cases, our model matched several empirically known facts\nabout wildfires. We conclude that elevation, percentage tree cover, relative\nhumidity, surface temperature, and the interaction between humidity and\ntemperature to be important predictors of monthly counts of wildfire\noccurrences. Further, our findings show opposing effects for surface\ntemperature and its interaction with relative humidity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 22:31:37 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kasin", "Justin A.", ""], ["Papastathopoulos", "Ioannis", ""]]}, {"id": "2007.00292", "submitter": "Chao Ying", "authors": "Chao Ying and Zhou Yu", "title": "Fr\\'echet Sufficient Dimension Reduction for Random Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We in this paper consider Fr\\'echet sufficient dimension reduction with\nresponses being complex random objects in a metric space and high dimension\nEuclidean predictors. We propose a novel approach called weighted inverse\nregression ensemble method for linear Fr\\'echet sufficient dimension reduction.\nThe method is further generalized as a new operator defined on reproducing\nkernel Hilbert spaces for nonlinear Fr\\'echet sufficient dimension reduction.\nWe provide theoretical guarantees for the new method via asymptotic analysis.\nIntensive simulation studies verify the performance of our proposals. And we\napply our methods to analyze the handwritten digits data to demonstrate its use\nin real applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:32:16 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ying", "Chao", ""], ["Yu", "Zhou", ""]]}, {"id": "2007.00360", "submitter": "Dominic Richards", "authors": "Dominic Richards, Patrick Rebeschini and Lorenzo Rosasco", "title": "Decentralised Learning with Random Features and Distributed Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalisation performance of Distributed Gradient Descent\nwith Implicit Regularisation and Random Features in the homogenous setting\nwhere a network of agents are given data sampled independently from the same\nunknown distribution. Along with reducing the memory footprint, Random Features\nare particularly convenient in this setting as they provide a common\nparameterisation across agents that allows to overcome previous difficulties in\nimplementing Decentralised Kernel Regression. Under standard source and\ncapacity assumptions, we establish high probability bounds on the predictive\nperformance for each agent as a function of the step size, number of\niterations, inverse spectral gap of the communication matrix and number of\nRandom Features. By tuning these parameters, we obtain statistical rates that\nare minimax optimal with respect to the total number of samples in the network.\nThe algorithm provides a linear improvement over single machine Gradient\nDescent in memory cost and, when agents hold enough data with respect to the\nnetwork size and inverse spectral gap, a linear speed-up in computational\nruntime for any network topology. We present simulations that show how the\nnumber of Random Features, iterations and samples impact predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:55:09 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Richards", "Dominic", ""], ["Rebeschini", "Patrick", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "2007.00363", "submitter": "Junho Yang", "authors": "Sourav Das, Suhasini Subba Rao, Junho Yang", "title": "Spectral methods for small sample time series: A complete periodogram\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The periodogram is a widely used tool to analyze second order stationary time\nseries. An attractive feature of the periodogram is that the expectation of the\nperiodogram is approximately equal to the underlying spectral density of the\ntime series. However, this is only an approximation, and it is well known that\nthe periodogram has a finite sample bias, which can be severe in small samples.\nIn this paper, we show that the bias arises because of the finite boundary of\nobservation in one of the discrete Fourier transforms which is used in the\nconstruction of the periodogram. Moreover, we show that by using the best\nlinear predictors of the time series over the boundary of observation we can\nobtain a \"complete periodogram\" that is an unbiased estimator of the spectral\ndensity. In practice, the \"complete periodogram\" cannot be evaluated as the\nbest linear predictors are unknown. We propose a method for estimating the best\nlinear predictors and prove that the resulting \"estimated complete periodogram\"\nhas a smaller bias than the regular periodogram. The estimated complete\nperiodogram and a tapered version of it are used to estimate parameters, which\ncan be represented in terms of the integrated spectral density. We prove that\nthe resulting estimators have a smaller bias than their regular periodogram\ncounterparts. The proposed method is illustrated with simulations and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:10:41 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 19:38:33 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Das", "Sourav", ""], ["Rao", "Suhasini Subba", ""], ["Yang", "Junho", ""]]}, {"id": "2007.00566", "submitter": "Weijie J. Su", "authors": "Hua Wang, Yachong Yang, Weijie J. Su", "title": "The Price of Competition: Effect Size Heterogeneity Matters in High\n  Dimensions", "comments": "Revised Figure 2 for a better illustration of the Lasso Crescent", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional linear regression, would increasing effect sizes always\nimprove model selection, while maintaining all the other conditions unchanged\n(especially fixing the sparsity of regression coefficients)? In this paper, we\nanswer this question in the \\textit{negative} in the regime of linear sparsity\nfor the Lasso method, by introducing a new notion we term effect size\nheterogeneity. Roughly speaking, a regression coefficient vector has high\neffect size heterogeneity if its nonzero entries have significantly different\nmagnitudes. From the viewpoint of this new measure, we prove that the false and\ntrue positive rates achieve the optimal trade-off uniformly along the Lasso\npath when this measure is maximal in a certain sense, and the worst trade-off\nis achieved when it is minimal in the sense that all nonzero effect sizes are\nroughly equal. Moreover, we demonstrate that the first false selection occurs\nmuch earlier when effect size heterogeneity is minimal than when it is maximal.\nThe underlying cause of these two phenomena is, metaphorically speaking, the\n\"competition\" among variables with effect sizes of the same magnitude in\nentering the model. Taken together, our findings suggest that effect size\nheterogeneity shall serve as an important complementary measure to the sparsity\nof regression coefficients in the analysis of high-dimensional regression\nproblems. Our proofs use techniques from approximate message passing theory as\nwell as a novel technique for estimating the rank of the first false variable.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:51:57 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 14:22:16 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Hua", ""], ["Yang", "Yachong", ""], ["Su", "Weijie J.", ""]]}, {"id": "2007.00568", "submitter": "Indrabati Bhattacharya", "authors": "Indrabati Bhattacharya, Subhashis Ghosal", "title": "Bayesian nonparametric tests for multivariate locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Bayesian non-parametric tests for one-sample and\ntwo-sample multivariate location problems. We model the underlying\ndistributions using a Dirichlet process prior. For the one-sample problem, we\ncompute a Bayesian credible set of the multivariate spatial median and accept\nthe null hypothesis if the credible set contains the null value. For the\ntwo-sample problem, we form a credible set for the difference of the spatial\nmedians of the two samples and we accept the null hypothesis of equality if the\ncredible set contains zero. We derive the local asymptotic power of the tests\nunder shrinking alternatives, and also present a simulation study to compare\nthe finite-sample performance of our testing procedures with existing\nparametric and non-parametric tests.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:53:09 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2007.00574", "submitter": "Imma Valentina Curato Dr", "authors": "Dirk-Philip Brandes, Imma Valentina Curato and Robert Stelzer", "title": "Inheritance of strong mixing and weak dependence under renewal sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a continuous-time strongly mixing or weakly dependent process and\n$T$ a renewal process independent of $X$ with inter-arrival times $\\{\\tau_i\\}$.\nWe show general conditions under which the sampled process\n$(X_{T_i},\\tau_i)^{\\top}$ is strongly mixing or weakly dependent. Moreover, we\nexplicitly compute the strong mixing or weak dependence coefficients of the\nrenewal sampled process and show that exponential or power decay of the\ncoefficients of $X$ is preserved (at least asymptotically). Our results imply\nthat essentially all central limit theorems available in the literature for\nstrongly mixing or weakly dependent processes can be applied when renewal\nsampled observations of the process $X$ are at disposal.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:00:28 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Brandes", "Dirk-Philip", ""], ["Curato", "Imma Valentina", ""], ["Stelzer", "Robert", ""]]}, {"id": "2007.00723", "submitter": "Ning Ning", "authors": "Ning Ning, Edward Ionides, Ya'acov Ritov", "title": "Scalable Monte Carlo Inference and Rescaled Local Asymptotic Normality", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize the property of local asymptotic normality (LAN)\nto an enlarged neighborhood, under the name of rescaled local asymptotic\nnormality (RLAN). We obtain sufficient conditions for a regular parametric\nmodel to satisfy RLAN. We show that RLAN supports the construction of a\nstatistically efficient estimator which maximizes a cubic approximation to the\nlog-likelihood on this enlarged neighborhood. In the context of Monte Carlo\ninference, we find that this maximum cubic likelihood estimator can maintain\nits statistical efficiency in the presence of asymptotically increasing Monte\nCarlo error in likelihood evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:54:51 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 20:56:20 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ning", "Ning", ""], ["Ionides", "Edward", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2007.00830", "submitter": "Charles R Doss", "authors": "Fadoua Balabdaoui, Charles R. Doss, and C\\'ecile Durot", "title": "Unlinked monotone regression", "comments": "60 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider so-called univariate unlinked (sometimes ``decoupled,'' or\n``shuffled'') regression when the unknown regression curve is monotone. In\nstandard monotone regression, one observes a pair $(X,Y)$ where a response $Y$\nis linked to a covariate $X$ through the model $Y= m_0(X) + \\epsilon$, with\n$m_0$ the (unknown) monotone regression function and $\\epsilon$ the unobserved\nerror (assumed to be independent of $X$). In the unlinked regression setting\none gets only to observe a vector of realizations from both the response $Y$\nand from the covariate $X$ where now $Y \\stackrel{d}{=} m_0(X) + \\epsilon$.\nThere is no (observed) pairing of $X$ and $Y$. Despite this, it is actually\nstill possible to derive a consistent non-parametric estimator of $m_0$ under\nthe assumption of monotonicity of $m_0$ and knowledge of the distribution of\nthe noise $\\epsilon$. In this paper, we establish an upper bound on the rate of\nconvergence of such an estimator under minimal assumption on the distribution\nof the covariate $X$. We discuss extensions to the case in which the\ndistribution of the noise is unknown. We develop a second order algorithm for\nits computation, and we demonstrate its use on synthetic data. Finally, we\napply our method (in a fully data driven way, without knowledge of the error\ndistribution) on longitudinal data from the US Consumer Expenditure Survey.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 01:42:50 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 22:47:21 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Doss", "Charles R.", ""], ["Durot", "C\u00e9cile", ""]]}, {"id": "2007.00909", "submitter": "Irene Gannaz", "authors": "Sophie Achard (LMC - IMAG), Pierre Borgnat (Phys-ENS), Ir\\`ene Gannaz\n  (PSPM, ICJ)", "title": "Asymptotic control of FWER under Gaussian assumption: application to\n  correlation tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, hypothesis testing is based on an asymptotic\ndistribution of statistics. The aim of this paper is to clarify and extend\nmultiple correction procedures when the statistics are asymptotically Gaussian.\nWe propose a unified framework to prove their asymptotic behavior which is\nvalid in the case of highly correlated tests. We focus on correlation tests\nwhere several test statistics are proposed. All these multiple testing\nprocedures on correlations are shown to control FWER. An extensive simulation\nstudy on correlation-based graph estimation highlights finite sample behavior,\nindependence on the sparsity of graphs and dependence on the values of\ncorrelations. Empirical evaluation of power provides comparisons of the\nproposed methods. Finally validation of our procedures is proposed on real\ndataset of rats brain connectivity measured by fMRI. We confirm our theoretical\nfindings by applying our procedures on a full null hypotheses with data from\ndead rats. Data on alive rats show the performance of the proposed procedures\nto correctly identify brain connectivity graphs with controlled errors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:33:10 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Achard", "Sophie", "", "LMC - IMAG"], ["Borgnat", "Pierre", "", "Phys-ENS"], ["Gannaz", "Ir\u00e8ne", "", "PSPM, ICJ"]]}, {"id": "2007.01058", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Miles E. Lopes and Hans-Georg M\\\"uller", "title": "High-dimensional MANOVA via Bootstrapping and its Application to\n  Functional and Sparse Count Data", "comments": "80 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of high-dimensional multivariate\nANOVA via bootstrapping max statistics that involve the differences of sample\nmean vectors. The proposed method proceeds via the construction of simultaneous\nconfidence regions for the differences of population mean vectors. It is suited\nto simultaneously test the equality of several pairs of mean vectors of\npotentially more than two populations. By exploiting the variance decay\nproperty that is a natural feature in relevant applications, we are able to\nprovide dimension-free and nearly-parametric convergence rates for Gaussian\napproximation, bootstrap approximation, and the size of the test. We\ndemonstrate the proposed approach with ANOVA problems for functional data and\nsparse count data. The proposed methodology is shown to work well in\nsimulations and several real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:31:21 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 14:02:30 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 06:46:17 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 02:23:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Zhenhua", ""], ["Lopes", "Miles E.", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2007.01147", "submitter": "Paul Rolland", "authors": "Paul Rolland, Armin Eftekhari, Ali Kavis and Volkan Cevher", "title": "Double-Loop Unadjusted Langevin Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known first-order method for sampling from log-concave probability\ndistributions is the Unadjusted Langevin Algorithm (ULA). This work proposes a\nnew annealing step-size schedule for ULA, which allows to prove new convergence\nguarantees for sampling from a smooth log-concave distribution, which are not\ncovered by existing state-of-the-art convergence guarantees. To establish this\nresult, we derive a new theoretical bound that relates the Wasserstein distance\nto total variation distance between any two log-concave distributions that\ncomplements the reach of Talagrand T2 inequality. Moreover, applying this new\nstep size schedule to an existing constrained sampling algorithm, we show\nstate-of-the-art convergence rates for sampling from a constrained log-concave\ndistribution, as well as improved dimension dependence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:31:04 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Rolland", "Paul", ""], ["Eftekhari", "Armin", ""], ["Kavis", "Ali", ""], ["Cevher", "Volkan", ""]]}, {"id": "2007.01165", "submitter": "Anthony Nouy", "authors": "Bertrand Michel and Anthony Nouy", "title": "Learning with tree tensor networks: complexity estimates and model\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree tensor networks, or tree-based tensor formats, are prominent model\nclasses for the approximation of high-dimensional functions in computational\nand data science. They correspond to sum-product neural networks with a sparse\nconnectivity associated with a dimension tree and widths given by a tuple of\ntensor ranks. The approximation power of these models has been proved to be\n(near to) optimal for classical smoothness classes. However, in an empirical\nrisk minimization framework with a limited number of observations, the\ndimension tree and ranks should be selected carefully to balance estimation and\napproximation errors. We propose and analyze a complexity-based model selection\nmethod for tree tensor networks in an empirical risk minimization framework and\nwe analyze its performance over a wide range of smoothness classes. Given a\nfamily of model classes associated with different trees, ranks, tensor product\nfeature spaces and sparsity patterns for sparse tensor networks, a model is\nselected (\\`a la Barron, Birg\\'e, Massart) by minimizing a penalized empirical\nrisk, with a penalty depending on the complexity of the model class and derived\nfrom estimates of the metric entropy of tree tensor networks. This choice of\npenalty yields a risk bound for the selected predictor. In a least-squares\nsetting, after deriving fast rates of convergence of the risk, we show that our\nstrategy is (near to) minimax adaptive to a wide range of smoothness classes\nincluding Sobolev or Besov spaces (with isotropic, anisotropic or mixed\ndominating smoothness) and analytic functions. We discuss the role of sparsity\nof the tensor network for obtaining optimal performance in several regimes. In\npractice, the amplitude of the penalty is calibrated with a slope heuristics\nmethod. Numerical experiments in a least-squares regression setting illustrate\nthe performance of the strategy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:52:08 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:59:26 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 10:15:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Michel", "Bertrand", ""], ["Nouy", "Anthony", ""]]}, {"id": "2007.01460", "submitter": "Jingjun Guo", "authors": "Cuiyun Zhang, Jingjun Guo, Aiqin Ma, Bo Peng", "title": "Least Squares Estimator for Vasicek Model Driven by Sub-fractional\n  Brownian Processes from Discrete Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameter estimation problem of Vasicek Model driven by\nsub-fractional Brownian processes from discrete observations, and let\n{S_t^H,t>=0} denote a sub-fractional Brownian motion whose Hurst parameter\n1/2<H<1 . The studies are as follows: firstly, two unknown parameters in the\nmodel are estimated by the least squares method. Secondly, the strong\nconsistency and the asymptotic distribution of the estimators are studied\nrespectively. Finally, our estimators are validated by numerical simulation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:05:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhang", "Cuiyun", ""], ["Guo", "Jingjun", ""], ["Ma", "Aiqin", ""], ["Peng", "Bo", ""]]}, {"id": "2007.01478", "submitter": "Yongyi Guo", "authors": "Jianqing Fan, Yongyi Guo, Ziwei Zhu", "title": "When is best subset selection the \"best\"?", "comments": "47 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best subset selection (BSS) is fundamental in statistics and machine\nlearning. Despite the intensive studies of it, the fundamental question of when\nBSS is truly the \"best\", namely yielding the oracle estimator, remains\npartially answered. In this paper, we address this important issue by giving a\nweak sufficient condition and a strong necessary condition for BSS to exactly\nrecover the true model. We also give a weak sufficient condition for BSS to\nachieve the sure screening property. On the optimization aspect, we find that\nthe exact combinatorial minimizer for BSS is unnecessary: all the established\nstatistical properties for the best subset carry over to any sparse model whose\nresidual sum of squares is close enough to that of the best subset. In\nparticular, we show that an iterative hard thresholding (IHT) algorithm can\nfind a sparse subset with the sure screening property within logarithmic steps;\nanother round of BSS within this set can recover the true model. The simulation\nstudies and real data examples show that IHT yields lower false discovery rates\nand higher true positive rates than the competing approaches including LASSO,\nSCAD and SIS.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:32:02 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Yongyi", ""], ["Zhu", "Ziwei", ""]]}, {"id": "2007.01615", "submitter": "Debraj Das", "authors": "Debraj Das and Priyam Das", "title": "On Second order correctness of Bootstrap in Logistic Regression", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fields of clinical trials, biomedical surveys, marketing, banking,\nwith dichotomous response variable, the logistic regression is considered as an\nalternative convenient approach to linear regression. In this paper, we develop\na novel bootstrap technique based on perturbation resampling method for\napproximating the distribution of the maximum likelihood estimator (MLE) of the\nregression parameter vector. We establish second order correctness of the\nproposed bootstrap method after proper studentization and smoothing. It is\nshown that inferences drawn based on the proposed bootstrap method are more\naccurate compared to that based on asymptotic normality. The main challenge in\nestablishing second order correctness remains in the fact that the response\nvariable being binary, the resulting MLE has a lattice structure. We show the\ndirect bootstrapping approach fails even after studentization. We adopt\nsmoothing technique developed in Lahiri (1993) to ensure that the smoothed\nstudentized version of the MLE has a density. Similar smoothing strategy is\nemployed to the bootstrap version also to achieve second order correct\napproximation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:18:20 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:58:05 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Das", "Debraj", ""], ["Das", "Priyam", ""]]}, {"id": "2007.01672", "submitter": "Sotirios Sabanis", "authors": "Sotirios Sabanis, Ying Zhang", "title": "A fully data-driven approach to minimizing CVaR for portfolio of assets\n  via SGLD with discontinuous updating", "comments": "arXiv admin note: text overlap with arXiv:1910.02008", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC math.PR math.ST q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach in stochastic optimization via the use of stochastic gradient\nLangevin dynamics (SGLD) algorithms, which is a variant of stochastic gradient\ndecent (SGD) methods, allows us to efficiently approximate global minimizers of\npossibly complicated, high-dimensional landscapes. With this in mind, we extend\nhere the non-asymptotic analysis of SGLD to the case of discontinuous\nstochastic gradients. We are thus able to provide theoretical guarantees for\nthe algorithm's convergence in (standard) Wasserstein distances for both convex\nand non-convex objective functions. We also provide explicit upper estimates of\nthe expected excess risk associated with the approximation of global minimizers\nof these objective functions. All these findings allow us to devise and present\na fully data-driven approach for the optimal allocation of weights for the\nminimization of CVaR of portfolio of assets with complete theoretical\nguarantees for its performance. Numerical results illustrate our main findings.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:11:57 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Sabanis", "Sotirios", ""], ["Zhang", "Ying", ""]]}, {"id": "2007.01757", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Monotonicity preservation properties of kernel regression estimators", "comments": "A shorter version, without pictures, to appear in Statistics and\n  Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three common classes of kernel regression estimators are considered: the\nNadaraya--Watson (NW) estimator, the Priestley--Chao (PC) estimator, and the\nGasser--M\\\"uller (GM) estimator. It is shown that (i) the GM estimator has a\ncertain monotonicity preservation property for any kernel $K$, (ii) the NW\nestimator has this property if and only the kernel $K$ is log concave, and\n(iii) the PC estimator does not have this property for any kernel $K$. Other\nrelated properties of these regression estimators are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:26:00 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 13:15:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "2007.01784", "submitter": "Lixia Hu", "authors": "Lixia Hu, Tao Huang and Jinhong You", "title": "Unified statistical inference for a novel nonlinear dynamic\n  functional/longitudinal data model", "comments": "29 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of recent work studying massive functional/longitudinal data, such\nas the resulting data from the COVID-19 pandemic, we propose a novel\nfunctional/longitudinal data model which is a combination of the popular\nvarying coefficient (VC) model and additive model. We call it Semi-VCAM in\nwhich the response could be a functional/longitudinal variable, and the\nexplanatory variables could be a mixture of functional/longitudinal and scalar\nvariables. Notably some of the scalar variables could be categorical variables\nas well. The Semi-VCAM simultaneously allows for both substantial flexibility\nand the maintaining of one-dimensional rates of convergence. A local linear\nsmoothing with the aid of an initial B spline series approximation is developed\nto estimate the unknown functional effects in the model. To avoid the\nsubjective choice between the sparse and dense cases of the data, we establish\nthe asymptotic theories of the resultant Pilot Estimation Based Local Linear\nEstimators (PEBLLE) on a unified framework of sparse, dense and ultra-dense\ncases of the data. Moreover, we construct unified consistent tests to justify\nwhether a parsimony submodel is sufficient or not. These test methods also\navoid the subjective choice between the sparse, dense and ultra dense cases of\nthe data. Extensive Monte Carlo simulation studies investigating the finite\nsample performance of the proposed methodologies confirm our asymptotic\nresults. We further illustrate our methodologies via analyzing the COVID-19\ndata from China and the CD4 data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:18:22 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Hu", "Lixia", ""], ["Huang", "Tao", ""], ["You", "Jinhong", ""]]}, {"id": "2007.01958", "submitter": "Alon Kipnis", "authors": "David L. Donoho and Alon Kipnis", "title": "Higher Criticism to Compare Two Large Frequency Tables, with sensitivity\n  to Possible Rare and Weak Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt Higher Criticism (HC) to the comparison of two frequency tables\nwhich may -- or may not -- exhibit moderate differences between the tables in\nsome unknown, relatively small subset out of a large number of categories.\n  Our analysis of the power of the proposed HC test quantifies the rarity and\nsize of assumed differences and applies moderate deviations-analysis to\ndetermine the asymptotic powerfulness/powerlessness of our proposed HC\nprocedure. Our analysis considers the null hypothesis of no difference in\nunderlying generative model against a rare/weak perturbation alternative, in\nwhich the frequencies of $N^{1-\\beta}$ out of the $N$ categories are perturbed\nby $r(\\log N)/2n$ in the Hellinger distance; here $n$ is the size of each\nsample. Our proposed Higher Criticism (HC) test \\newtext{for} this setting uses\nP-values obtained from $N$ exact binomial tests. We characterize the asymptotic\nperformance of the HC-based test in terms of the sparsity parameter $\\beta$ and\nthe perturbation intensity parameter $r$. Specifically, we derive a region in\nthe $(\\beta,r)$-plane where the test asymptotically has maximal power, while\nhaving asymptotically no power outside this region. Our analysis distinguishes\nbetween cases in which the counts in both tables are low, versus cases in which\ncounts are high, corresponding to the cases of sparse and dense frequency\ntables. The phase transition curve of HC in the high-counts regime matches\nformally the curve delivered by HC in a two-sample normal means model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 22:38:28 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:53:38 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 22:52:08 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 07:41:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Donoho", "David L.", ""], ["Kipnis", "Alon", ""]]}, {"id": "2007.01961", "submitter": "Alfredo Alegr\\'ia", "authors": "Alfredo Alegr\\'ia and Francisco Cuevas-Pacheco", "title": "Karhunen-Lo\\`eve Expansions for Axially Symmetric Gaussian Processes:\n  Modeling Strategies and $L^2$ Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axially symmetric processes on spheres, for which the second-order dependency\nstructure may substantially vary with shifts in latitude, are a prominent\nalternative to model the spatial uncertainty of natural variables located over\nlarge portions of the Earth. In this paper, we focus on Karhunen-Lo\\`eve\nexpansions of axially symmetric Gaussian processes. First, we investigate a\nparametric family of Karhunen-Lo\\`eve coefficients that allows for versatile\nspatial covariance functions. The isotropy as well as the longitudinal\nindependence can be obtained as limit cases of our proposal. Second, we\nintroduce a strategy to render any longitudinally reversible process\nirreversible, which means that its covariance function could admit certain\ntypes of asymmetries along longitudes. Then, finitely truncated\nKarhunen-Lo\\`eve expansions are used to approximate axially symmetric\nprocesses. For such approximations, bounds for the $L^2$-error are provided.\nNumerical experiments are conducted to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 23:05:41 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Alegr\u00eda", "Alfredo", ""], ["Cuevas-Pacheco", "Francisco", ""]]}, {"id": "2007.02153", "submitter": "Chun-Hao Yang", "authors": "Chun-Hao Yang, Hani Doss, Baba C. Vemuri", "title": "An Empirical Bayes Approach to Shrinkage Estimation on the Manifold of\n  Symmetric Positive-Definite Matrices", "comments": "54 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The James-Stein estimator is an estimator of the multivariate normal mean and\ndominates the maximum likelihood estimator (MLE) under squared error loss. The\noriginal work inspired great interest in developing shrinkage estimators for a\nvariety of problems. Nonetheless, research on shrinkage estimation for\nmanifold-valued data is scarce. In this paper, we propose shrinkage estimators\nfor the parameters of the Log-Normal distribution defined on the manifold of $N\n\\times N$ symmetric positive-definite matrices. For this manifold, we choose\nthe Log-Euclidean metric as its Riemannian metric since it is easy to compute\nand is widely used in applications. By using the Log-Euclidean distance in the\nloss function, we derive a shrinkage estimator in an analytic form and show\nthat it is asymptotically optimal within a large class of estimators including\nthe MLE, which is the sample Fr\\'echet mean of the data. We demonstrate the\nperformance of the proposed shrinkage estimator via several simulated data\nexperiments. Furthermore, we apply the shrinkage estimator to perform\nstatistical inference in diffusion magnetic resonance imaging problems.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:05:34 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:14:33 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 00:14:49 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yang", "Chun-Hao", ""], ["Doss", "Hani", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "2007.02156", "submitter": "Cong Mu", "authors": "Cong Mu, Angelo Mele, Lingxin Hao, Joshua Cape, Avanti Athreya, Carey\n  E. Priebe", "title": "On spectral algorithms for community detection in stochastic blockmodel\n  graphs with vertex covariates", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network inference applications, it is often desirable to detect community\nstructure, namely to cluster vertices into groups, or blocks, according to some\nmeasure of similarity. Beyond mere adjacency matrices, many real networks also\ninvolve vertex covariates that carry key information about underlying block\nstructure in graphs. To assess the effects of such covariates on block\nrecovery, we present a comparative analysis of two model-based spectral\nalgorithms for clustering vertices in stochastic blockmodel graphs with vertex\ncovariates. The first algorithm uses only the adjacency matrix, and directly\nestimates the induced block assignments. The second algorithm incorporates both\nthe adjacency matrix and the vertex covariates into the estimation of block\nassignments, and moreover quantifies the explicit impact of the vertex\ncovariates on the resulting estimate of the block assignments. We employ\nChernoff information to analytically compare the algorithms' performance and\nderive the Chernoff ratio for certain models of interest. Analytic results and\nsimulations suggest that the second algorithm is often preferred: we can often\nbetter estimate the induced block assignments by first estimating the effect of\nvertex covariates. In addition, real data examples on diffusion MRI connectome\ndatasets and social network datasets also indicate that the second algorithm\nhas the advantages of revealing underlying block structure and taking observed\nvertex heterogeneity into account in real applications. Our findings emphasize\nthe importance of distinguishing between observed and unobserved factors that\ncan affect block structure in graphs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:22:22 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 00:31:53 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mu", "Cong", ""], ["Mele", "Angelo", ""], ["Hao", "Lingxin", ""], ["Cape", "Joshua", ""], ["Athreya", "Avanti", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2007.02186", "submitter": "Fang Han", "authors": "Hongjian Shi, Marc Hallin, Mathias Drton, and Fang Han", "title": "On universally consistent and fully distribution-free rank tests of\n  vector independence", "comments": "52 pages with title changed and more materials put in, including,\n  particularly, a more general local power analysis covering many smooth\n  alternatives beyond the Konijn ones", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank correlations have found many innovative applications in the last decade.\nIn particular, suitable rank correlations have been used for consistent tests\nof independence between pairs of random variables. Using ranks is especially\nappealing for continuous data as tests become distribution-free. However, the\ntraditional concept of ranks relies on ordering data and is, thus, tied to\nunivariate observations. As a result, it has long remained unclear how one may\nconstruct distribution-free yet consistent tests of independence between random\nvectors. This is the problem addressed in this paper, in which we lay out a\ngeneral framework for designing dependence measures that give tests of\nmultivariate independence that are not only consistent and distribution-free\nbut which we also prove to be statistically efficient. Our framework leverages\nthe recently introduced concept of center-outward ranks and signs, a\nmultivariate generalization of traditional ranks, and adopts a common standard\nform for dependence measures that encompasses many popular examples. In a\nunified study, we derive a general asymptotic representation of center-outward\nrank-based test statistics under independence, extending to the multivariate\nsetting the classical H\\'{a}jek asymptotic representation results. This\nrepresentation permits direct calculation of limiting null distributions and\nfacilitates a local power analysis that provides strong support for the\ncenter-outward approach by establishing, for the first time, the nontrivial\npower of center-outward rank-based tests over root-$n$ neighborhoods within the\nclass of quadratic mean differentiable alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:12:14 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 03:41:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shi", "Hongjian", ""], ["Hallin", "Marc", ""], ["Drton", "Mathias", ""], ["Han", "Fang", ""]]}, {"id": "2007.02188", "submitter": "Vaidotas Characiejus", "authors": "Cl\\'ement Cerovecki and Vaidotas Characiejus and Siegfried H\\\"ormann", "title": "The maximum of the periodogram of Hilbert space valued time series", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested to detect periodic signals in Hilbert space valued time\nseries when the length of the period is unknown. A natural test statistic is\nthe maximum Hilbert-Schmidt norm of the periodogram operator over all\nfundamental frequencies. In this paper we analyze the asymptotic distribution\nof this test statistic. We consider the case where the noise variables are\nindependent and then generalize our results to functional linear processes.\nDetails for implementing the test are provided for the class of functional\nautoregressive processes. We illustrate the usefulness of our approach by\nexamining air quality data from Graz, Austria. The accuracy of the asymptotic\ntheory in finite samples is evaluated in a simulation experiment.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:17:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cerovecki", "Cl\u00e9ment", ""], ["Characiejus", "Vaidotas", ""], ["H\u00f6rmann", "Siegfried", ""]]}, {"id": "2007.02189", "submitter": "Tahani Coolen-Maturi Dr", "authors": "Tahani Coolen-Maturi, Frank P.A. Coolen, Narayanaswamy Balakrishnan", "title": "The joint survival signature of coherent systems with shared components", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of joint bivariate signature, introduced by Navarro et al.\n(2013), is a useful tool for quantifying the reliability of two systems with\nshared components. As with the univariate system signature, introduced by\nSamaniego (2007), its applications are limited to systems with only one type of\ncomponents, which restricts its practical use. Coolen and Coolen-Maturi (2012)\nintroduced the survival signature, which generalizes Samaniego's signature and\ncan be used for systems with multiple types of components. This paper\nintroduces a joint survival signature for multiple systems with multiple types\nof components and with some components shared between systems. A particularly\nimportant feature is that the functioning of these systems can be considered at\ndifferent times, enabling computation of relevant conditional probabilities\nwith regard to a system's functioning conditional on the status of another\nsystem with which it shares components. Several opportunities for practical\napplication and related challenges for further development of the presented\nconcept are briefly discussed, setting out an important direction for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:23:18 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 18:57:21 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Coolen-Maturi", "Tahani", ""], ["Coolen", "Frank P. A.", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2007.02192", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Debdeep Pati, Bani K. Mallick", "title": "Tail-adaptive Bayesian shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens and\nthousands of predictors. Under diverse sparsity regimes, the nature of signal\ndetection is associated with a tail behaviour of a prior. A desirable tail\nbehaviour is called tail-adaptive shrinkage property where tail-heaviness of a\nprior gets adaptively larger (or smaller) as a sparsity level increases (or\ndecreases) to accommodate more (or less) signals. We propose a\nglobal-local-tail (GLT) Gaussian mixture distribution to ensure this property\nand provide accurate inference under diverse sparsity regimes. Incorporating a\npeaks-over-threshold method in extreme value theory, we develop an automated\ntail learning algorithm for the GLT prior. We compare the performance of the\nGLT prior to the Horseshoe in two gene expression datasets and numerical\nexamples. Results suggest that varying tail rule is advantageous over fixed\ntail rule under diverse sparsity domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:40:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:47:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lee", "Se Yoon", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2007.02305", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sudheesh K Kattumannil, Sreedevi E P, Sankaran P G", "title": "Semiparametric transformation model for competing risks data with cure\n  fraction", "comments": "We propose a new methodology in mixture cure rate model", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling and analysis of competing risks data with long-term survivors is an\nimportant area of research in recent years. For example, in the study of cancer\npatients treated for soft tissue sarcoma, patient may die due to different\ncauses. Considerable portion of the patients may remain cancer free after the\ntreatment. Accordingly, it is important to incorporate long-term survivors in\nthe analysis of competing risks data. Motivated by this, we propose a new\nmethod for the analysis of competing risks data with long term survivors. The\nnew method enables us to estimate the overall survival probability without\nestimating the cure fraction. We formulate the effects of covariates on\nsub-distribution (cumulative incidence) functions using linear transformation\nmodel. Estimating equations based on counting process are developed to find the\nestimators of regression coefficients. The asymptotic properties of the\nestimators are studied using martingale theory. An extensive Monte Carlo\nsimulation study is carried out to assess the finite sample performance of the\nproposed estimators. Finally, we illustrate our method using a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:08:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kattumannil", "Sudheesh K", ""], ["P", "Sreedevi E", ""], ["G", "Sankaran P", ""]]}, {"id": "2007.02310", "submitter": "Zhongyi Hu", "authors": "Zhongyi Hu, Robin Evans", "title": "Faster algorithms for Markov equivalence", "comments": "Accepted", "journal-ref": "36th Conference on Uncertainty in Artificial Intelligence (UAI),\n  2020", "doi": null, "report-no": null, "categories": "math.CO math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal ancestral graphs (MAGs) have many desirable properties; in particular\nthey can fully describe conditional independences from directed acyclic graphs\n(DAGs) in the presence of latent and selection variables. However, different\nMAGs may encode the same conditional independences, and are said to be\n\\emph{Markov equivalent}. Thus identifying necessary and sufficient conditions\nfor equivalence is essential for structure learning. Several criteria for this\nalready exist, but in this paper we give a new non-parametric characterization\nin terms of the heads and tails that arise in the parameterization for discrete\nmodels. We also provide a polynomial time algorithm ($O(ne^{2})$, where $n$ and\n$e$ are the number of vertices and edges respectively) to verify equivalence.\nMoreover, we extend our criterion to ADMGs and summary graphs and propose an\nalgorithm that converts an ADMG or summary graph to an equivalent MAG in\npolynomial time ($O(n^{2}e)$). Hence by combining both algorithms, we can also\nverify equivalence between two summary graphs or ADMGs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:24:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hu", "Zhongyi", ""], ["Evans", "Robin", ""]]}, {"id": "2007.02392", "submitter": "Alkis Kalavasis", "authors": "Dimitris Fotakis, Alkis Kalavasis, Christos Tzamos", "title": "Efficient Parameter Estimation of Truncated Boolean Product\n  Distributions", "comments": "33 pages, 33rd Conference on Learning Theory (COLT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Boolean product\ndistribution in $d$ dimensions, when the samples are truncated by a set $S\n\\subset \\{0, 1\\}^d$ accessible through a membership oracle. This is the first\ntime that the computational and statistical complexity of learning from\ntruncated samples is considered in a discrete setting.\n  We introduce a natural notion of fatness of the truncation set $S$, under\nwhich truncated samples reveal enough information about the true distribution.\nWe show that if the truncation set is sufficiently fat, samples from the true\ndistribution can be generated from truncated samples. A stunning consequence is\nthat virtually any statistical task (e.g., learning in total variation\ndistance, parameter estimation, uniformity or identity testing) that can be\nperformed efficiently for Boolean product distributions, can also be performed\nfrom truncated samples, with a small increase in sample complexity. We\ngeneralize our approach to ranking distributions over $d$ alternatives, where\nwe show how fatness implies efficient parameter estimation of Mallows models\nfrom truncated samples.\n  Exploring the limits of learning discrete models from truncated samples, we\nidentify three natural conditions that are necessary for efficient\nidentifiability: (i) the truncation set $S$ should be rich enough; (ii) $S$\nshould be accessible through membership queries; and (iii) the truncation by\n$S$ should leave enough randomness in all directions. By carefully adapting the\nStochastic Gradient Descent approach of (Daskalakis et al., FOCS 2018), we show\nthat these conditions are also sufficient for efficient learning of truncated\nBoolean product distributions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kalavasis", "Alkis", ""], ["Tzamos", "Christos", ""]]}, {"id": "2007.02514", "submitter": "Aaron Fisher", "authors": "Aaron Fisher", "title": "Treatment Effect Bias from Sample Snooping: Blinding Outcomes is Neither\n  Necessary nor Sufficient", "comments": "version notes: dramatic rewrite with new theoretical results and\n  simulations. Most of the technical results from first version are now\n  contained in the supplement, with new results taking their place in the main\n  text. The supplement is available as an ancillary file (see link on\n  right-hand side of this page)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular guidance on observational data analysis states that outcomes should\nbe blinded when determining matching criteria or propensity scores. Such a\nblinding is informally said to maintain the \"objectivity\" of the analysis, and\nto prevent analysts from artificially amplifying the treatment effect by\nexploiting chance imbalances. Contrary to this notion, we show that outcome\nblinding is not a sufficient safeguard against fishing. Blinded and unblinded\nanalysts can produce bias of the same order of magnitude in cases where the\noutcomes can be approximately predicted from baseline covariates. We illustrate\nthis vulnerability with a combination of analytical results and simulations.\nFinally, to show that outcome blinding is not necessary to prevent bias, we\noutline an alternative sample partitioning procedure for estimating the average\ntreatment effect on the controls, or the average treatment effect on the\ntreated. This procedure uses all of the the outcome data from all partitions in\nthe final analysis step, but does not require the analysis to not be fully\nprespecified.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:47:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 20:57:07 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 23:05:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Fisher", "Aaron", ""]]}, {"id": "2007.02596", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Norbert Henze and David Strieder", "title": "Testing normality in any dimension by Fourier methods in a multivariate\n  Stein equation", "comments": "37 pages, 1 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel class of affine invariant and consistent tests for\nmultivariate normality. The tests are based on a characterization of the\nstandard $d$-variate normal distribution by means of the unique solution of an\ninitial value problem connected to a partial differential equation, which is\nmotivated by a multivariate Stein equation. The test criterion is a suitably\nweighted $L^2$-statistic. We derive the limit distribution of the test\nstatistic under the null hypothesis as well as under contiguous and fixed\nalternatives to normality. A consistent estimator of the limiting variance\nunder fixed alternatives as well as an asymptotic confidence interval of the\ndistance of an underlying alternative with respect to the multivariate normal\nlaw is derived. In simulation studies, we show that the tests are strong in\ncomparison with prominent competitors, and that the empirical coverage rate of\nthe asymptotic confidence interval converges to the nominal level. We present a\nreal data example, and we outline topics for further research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:15:59 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""], ["Strieder", "David", ""]]}, {"id": "2007.02677", "submitter": "Simon Weissmann", "authors": "Neil K. Chada, Claudia Schillings, Xin T. Tong and Simon Weissmann", "title": "Consistency analysis of bilevel data-driven learning in inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental problem when solving inverse problems is how to find\nregularization parameters. This article considers solving this problem using\ndata-driven bilevel optimization, i.e. we consider the adaptive learning of the\nregularization parameter from data by means of optimization. This approach can\nbe interpreted as solving an empirical risk minimization problem, and we\nanalyze its performance in the large data sample size limit for general\nnonlinear problems. We demonstrate how to implement our framework on linear\ninverse problems, where we can further show the inverse accuracy does not\ndepend on the ambient space dimension. To reduce the associated computational\ncost, online numerical schemes are derived using the stochastic gradient\ndescent method. We prove convergence of these numerical schemes under suitable\nassumptions on the forward problem. Numerical experiments are presented\nillustrating the theoretical results and demonstrating the applicability and\nefficiency of the proposed approaches for various linear and nonlinear inverse\nproblems, including Darcy flow, the eikonal equation, and an image denoising\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:23:29 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 15:37:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chada", "Neil K.", ""], ["Schillings", "Claudia", ""], ["Tong", "Xin T.", ""], ["Weissmann", "Simon", ""]]}, {"id": "2007.02904", "submitter": "Bruno Mera", "authors": "Bruno Mera, Paulo Mateus, Alexandra M. Carvalho", "title": "On the minmax regret for statistical manifolds: the role of curvature", "comments": "16 pages; comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.DG math.IT physics.data-an quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model complexity plays an essential role in its selection, namely, by\nchoosing a model that fits the data and is also succinct. Two-part codes and\nthe minimum description length have been successful in delivering procedures to\nsingle out the best models, avoiding overfitting. In this work, we pursue this\napproach and complement it by performing further assumptions in the parameter\nspace. Concretely, we assume that the parameter space is a smooth manifold, and\nby using tools of Riemannian geometry, we derive a sharper expression than the\nstandard one given by the stochastic complexity, where the scalar curvature of\nthe Fisher information metric plays a dominant role. Furthermore, we derive the\nminmax regret for general statistical manifolds and apply our results to derive\noptimal dimensional reduction in the context of principal component analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 17:28:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Mera", "Bruno", ""], ["Mateus", "Paulo", ""], ["Carvalho", "Alexandra M.", ""]]}, {"id": "2007.02938", "submitter": "Anant Raj", "authors": "Anant Raj, Stefan Bauer, Ashkan Soleymani, Michel Besserve and\n  Bernhard Sch\\\"olkopf", "title": "Causal Feature Selection via Orthogonal Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring the direct causal parents of a response variable\namong a large set of explanatory variables is of high practical importance in\nmany disciplines. Recent work in the field of causal discovery exploits\ninvariance properties of models across different experimental conditions for\ndetecting direct causal links. However, these approaches generally do not scale\nwell with the number of explanatory variables, are difficult to extend to\nnonlinear relationships, and require data across different experiments.\nInspired by {\\em Debiased} machine learning methods, we study a\none-vs.-the-rest feature selection approach to discover the direct causal\nparent of the response. We propose an algorithm that works for purely\nobservational data, while also offering theoretical guarantees, including the\ncase of partially nonlinear relationships. Requiring only one estimation for\neach variable, we can apply our approach even to large graphs, demonstrating\nsignificant improvements compared to established approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:56:43 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Raj", "Anant", ""], ["Bauer", "Stefan", ""], ["Soleymani", "Ashkan", ""], ["Besserve", "Michel", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2007.03210", "submitter": "Emmanouil Zampetakis", "authors": "Vasilis Syrgkanis and Manolis Zampetakis", "title": "Estimation and Inference with Trees and Forests in High Dimensions", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the finite sample mean squared error (MSE) performance of\nregression trees and forests in the high dimensional regime with binary\nfeatures, under a sparsity constraint. We prove that if only $r$ of the $d$\nfeatures are relevant for the mean outcome function, then shallow trees built\ngreedily via the CART empirical MSE criterion achieve MSE rates that depend\nonly logarithmically on the ambient dimension $d$. We prove upper bounds, whose\nexact dependence on the number relevant variables $r$ depends on the\ncorrelation among the features and on the degree of relevance. For strongly\nrelevant features, we also show that fully grown honest forests achieve fast\nMSE rates and their predictions are also asymptotically normal, enabling\nasymptotically valid inference that adapts to the sparsity of the regression\nfunction.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 05:45:32 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 18:44:09 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2007.03253", "submitter": "Stefano Favaro", "authors": "Stefano Peluchetti and Stefano Favaro", "title": "Doubly infinite residual networks: a diffusion process approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When neural network's parameters are initialized as i.i.d., neural networks\nexhibit undesirable forward and backward properties as the number of layers\nincreases, e.g., vanishing dependency on the input, and perfectly correlated\noutputs for any two inputs. To overcome these drawbacks Peluchetti and Favaro\n(2020) considered fully connected residual networks (ResNets) with parameters'\ndistributions that shrink as the number of layers increases. In particular,\nthey established an interplay between infinitely deep ResNets and solutions to\nstochastic differential equations, i.e. diffusion processes, showing that\ninfinitely deep ResNets does not suffer from undesirable forward properties. In\nthis paper, we review the forward-propagation results of Peluchetti and Favaro\n(2020), extending them to the setting of convolutional ResNets. Then, we study\nanalogous backward-propagation results, which directly relate to the problem of\ntraining deep ResNets. Finally, we extend our study to the doubly infinite\nregime where both network's width and depth grow unboundedly. Within this novel\nregime the dynamics of quantities of interest converge, at initialization, to\ndeterministic limits. This allow us to provide analytical expressions for\ninference, both in the case of weakly trained and fully trained networks. These\nresults point to a limited expressive power of doubly infinite ResNets when the\nunscaled parameters are i.i.d, and residual blocks are shallow.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:45:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Peluchetti", "Stefano", ""], ["Favaro", "Stefano", ""]]}, {"id": "2007.03569", "submitter": "Oliver Johnson", "authors": "Oliver Johnson", "title": "Information-theoretic convergence of extreme values to the Gumbel\n  distribution", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how convergence to the Gumbel distribution in an extreme value\nsetting can be understood in an information-theoretic sense. We introduce a new\ntype of score function which behaves well under the maximum operation, and\nwhich implies simple expressions for entropy and relative entropy. We show\nthat, assuming certain properties of the von Mises representation, convergence\nto the Gumbel can be proved in the strong sense of relative entropy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:46:18 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Johnson", "Oliver", ""]]}, {"id": "2007.03571", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti, Molay Kumar Ruidas, Sumanta Adhya", "title": "A Natural Discrete One Parameter Polynomial Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new natural discrete version of the one parameter polynomial\nexponential family of distributions have been proposed and studied. The\ndistribution is named as Natural Discrete One Parameter Polynomial Exponential\n(NDOPPE) distribution. Structural and reliability properties have been studied.\nEstimation procedure of the parameter of the distribution have been mentioned.\nCompound NDOPPE distribution in the context of collective risk model have been\nobtained in closed form. The new compound distribution has been compared with\nthe classical compound Poisson, compound Negative binomial, compound discrete\nLindley, compound xgamma-I and compound xgamma-II distributions regarding\nsuitability of modelling extreme data with the help of some automobile claim.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:47:19 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Ruidas", "Molay Kumar", ""], ["Adhya", "Sumanta", ""]]}, {"id": "2007.03714", "submitter": "Yuqing Li", "authors": "Yuqing Li, Tao Luo, Nung Kwan Yip", "title": "Towards an Understanding of Residual Networks Using Neural Tangent\n  Hierarchy (NTH)", "comments": "72 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent yields zero training loss in polynomial time for deep neural\nnetworks despite non-convex nature of the objective function. The behavior of\nnetwork in the infinite width limit trained by gradient descent can be\ndescribed by the Neural Tangent Kernel (NTK) introduced in\n\\cite{Jacot2018Neural}. In this paper, we study dynamics of the NTK for finite\nwidth Deep Residual Network (ResNet) using the neural tangent hierarchy (NTH)\nproposed in \\cite{Huang2019Dynamics}. For a ResNet with smooth and Lipschitz\nactivation function, we reduce the requirement on the layer width $m$ with\nrespect to the number of training samples $n$ from quartic to cubic. Our\nanalysis suggests strongly that the particular skip-connection structure of\nResNet is the main reason for its triumph over fully-connected network.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:08:16 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Yuqing", ""], ["Luo", "Tao", ""], ["Yip", "Nung Kwan", ""]]}, {"id": "2007.03911", "submitter": "Binghui Liu", "authors": "Long Feng, Tiefeng Jiang, Binghui Liu, Wei Xiong", "title": "Max-sum tests for cross-sectional dependence of high-demensional panel\n  data", "comments": "106 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a testing problem for cross-sectional dependence for\nhigh-dimensional panel data, where the number of cross-sectional units is\npotentially much larger than the number of observations. The cross-sectional\ndependence is described through a linear regression model. We study three tests\nnamed the sum test, the max test and the max-sum test, where the latter two are\nnew. The sum test is initially proposed by Breusch and Pagan (1980). We design\nthe max and sum tests for sparse and non-sparse residuals in the linear\nregressions, respectively.And the max-sum test is devised to compromise both\nsituations on the residuals. Indeed, our simulation shows that the max-sum test\noutperforms the previous two tests. This makes the max-sum test very useful in\npractice where sparsity or not for a set of data is usually vague. Towards the\ntheoretical analysis of the three tests, we have settled two conjectures\nregarding the sum of squares of sample correlation coefficients asked by\nPesaran (2004 and 2008). In addition, we establish the asymptotic theory for\nmaxima of sample correlations coefficients appeared in the linear regression\nmodel for panel data, which is also the first successful attempt to our\nknowledge. To study the max-sum test, we create a novel method to show\nasymptotic independence between maxima and sums of dependent random variables.\nWe expect the method itself is useful for other problems of this nature.\nFinally, an extensive simulation study as well as a case study are carried out.\nThey demonstrate advantages of our proposed methods in terms of both empirical\npowers and robustness for residuals regardless of sparsity or not.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 06:10:02 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Feng", "Long", ""], ["Jiang", "Tiefeng", ""], ["Liu", "Binghui", ""], ["Xiong", "Wei", ""]]}, {"id": "2007.03926", "submitter": "Ulysse Marteau-Ferey", "authors": "Ulysse Marteau-Ferey (PSL, DI-ENS, SIERRA), Francis Bach (PSL, DI-ENS,\n  SIERRA), Alessandro Rudi (PSL, DI-ENS, SIERRA)", "title": "Non-parametric Models for Non-negative Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear models have shown great effectiveness and flexibility in many fields\nsuch as machine learning, signal processing and statistics. They can represent\nrich spaces of functions while preserving the convexity of the optimization\nproblems where they are used, and are simple to evaluate, differentiate and\nintegrate. However, for modeling non-negative functions, which are crucial for\nunsupervised learning, density estimation, or non-parametric Bayesian methods,\nlinear models are not applicable directly. Moreover, current state-of-the-art\nmodels like generalized linear models either lead to non-convex optimization\nproblems, or cannot be easily integrated. In this paper we provide the first\nmodel for non-negative functions which benefits from the same good properties\nof linear models. In particular, we prove that it admits a representer theorem\nand provide an efficient dual formulation for convex problems. We study its\nrepresentation power, showing that the resulting space of functions is strictly\nricher than that of generalized linear models. Finally we extend the model and\nthe theoretical results to functions with outputs in convex cones. The paper is\ncomplemented by an experimental evaluation of the model showing its\neffectiveness in terms of formulation, algorithmic derivation and practical\nresults on the problems of density estimation, regression with heteroscedastic\nerrors, and multiple quantile regression.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:17:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Marteau-Ferey", "Ulysse", "", "PSL, DI-ENS, SIERRA"], ["Bach", "Francis", "", "PSL, DI-ENS,\n  SIERRA"], ["Rudi", "Alessandro", "", "PSL, DI-ENS, SIERRA"]]}, {"id": "2007.03940", "submitter": "Pirmin Lemberger", "authors": "Pirmin Lemberger and Denis Oblin", "title": "Reconciling Causality and Statistics", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians have warned us since the early days of their discipline that\nexperimental correlation between two observations by no means implies the\nexistence of a causal relation. The question about what clues exist in\nobservational data that could informs us about the existence of such causal\nrelations is nevertheless more that legitimate. It lies actually at the root of\nany scientific endeavor. For decades however the only accepted method among\nstatisticians to elucidate causal relationships was the so called Randomized\nControlled Trial. Besides this notorious exception causality questions remained\nlargely taboo for many. One reason for this state of affairs was the lack of an\nappropriate mathematical framework to formulate such questions in an\nunambiguous way. Fortunately thinks have changed these last years with the\nadvent of the so called Causality Revolution initiated by Judea Pearl and\ncoworkers. The aim of this pedagogical paper is to present their ideas and\nmethods in a compact and self-contained fashion with concrete business examples\nas illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:52:13 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 07:27:20 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Lemberger", "Pirmin", ""], ["Oblin", "Denis", ""]]}, {"id": "2007.04204", "submitter": "Ana Paula Martins", "authors": "Marta Ferreira and Ana Paula Martins and Helena Ferreira", "title": "pMAX Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of occurrence of atypical phenomena is a cross-cutting concern in\nseveral areas, such as engineering, climatology, finance, actuarial, among\nothers. Extreme value theory is the natural tool to approach this theme. Many\nof these random phenomena carry variables defined in time and space, usually\nmodeled through random fields. Thus, the study of random fields in the context\nof extreme values becomes imperative and has been developed especially in the\nlast decade. In this work, we propose a new random field, called pMAX, designed\nfor modeling extremes. We analyze its dependence and pre-asymptotic dependence\nstructure through the corresponding bivariate tail dependence coefficients.\nEstimators for the model parameters are obtained and their finite sample\nproperties analyzed. Examples with simulations illustrate the results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:45:40 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ferreira", "Marta", ""], ["Martins", "Ana Paula", ""], ["Ferreira", "Helena", ""]]}, {"id": "2007.04443", "submitter": "Henry Lam", "authors": "Henry Lam, Haidong Li, Xuhui Zhang", "title": "Minimax Efficient Finite-Difference Stochastic Gradient Estimators Using\n  Black-Box Function Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches to stochastic gradient estimation, with only noisy\nblack-box function evaluations, use the finite-difference method or its\nvariants. While natural, it is open to our knowledge whether their statistical\naccuracy is the best possible. This paper argues so by showing that central\nfinite-difference is a nearly minimax optimal zeroth-order gradient estimator\nfor a suitable class of objective functions and mean squared risk, among both\nthe class of linear estimators and the much larger class of all (nonlinear)\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:29:45 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:05:05 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lam", "Henry", ""], ["Li", "Haidong", ""], ["Zhang", "Xuhui", ""]]}, {"id": "2007.04470", "submitter": "Diana Cai", "authors": "Diana Cai, Trevor Campbell, Tamara Broderick", "title": "Finite mixture models do not reliably learn the number of components", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML), to appear. 25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists and engineers are often interested in learning the number of\nsubpopulations (or components) present in a data set. A common suggestion is to\nuse a finite mixture model (FMM) with a prior on the number of components. Past\nwork has shown the resulting FMM component-count posterior is consistent; that\nis, the posterior concentrates on the true, generating number of components.\nBut consistency requires the assumption that the component likelihoods are\nperfectly specified, which is unrealistic in practice. In this paper, we add\nrigor to data-analysis folk wisdom by proving that under even the slightest\nmodel misspecification, the FMM component-count posterior diverges: the\nposterior probability of any particular finite number of components converges\nto 0 in the limit of infinite data. Contrary to intuition, posterior-density\nconsistency is not sufficient to establish this result. We develop novel\nsufficient conditions that are more realistic and easily checkable than those\ncommon in the asymptotics literature. We illustrate practical consequences of\nour theory on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:05:18 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 16:12:03 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:38:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cai", "Diana", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "2007.04547", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao", "title": "An Optimal Uniform Concentration Inequality for Discrete Entropies on\n  Finite Alphabets in the High-dimensional Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an exponential decay concentration inequality to bound the tail\nprobability of the difference between the log-likelihood of discrete random\nvariables on a finite alphabet and the negative entropy. The concentration\nbound we derive holds uniformly over all parameter values. The new result\nimproves the convergence rate in an earlier result of Zhao (2020), from\n$(K^2\\log K)/n=o(1)$ to $ (\\log K)^2/n=o(1)$, where $n$ is the sample size and\n$K$ is the size of the alphabet. We further prove that the rate $(\\log\nK)^2/n=o(1)$ is optimal. The results are extended to misspecified\nlog-likelihoods for grouped random variables. We give applications of the new\nresult in information theory.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:20:38 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 00:30:52 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 20:42:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhao", "Yunpeng", ""]]}, {"id": "2007.04803", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Randal Douc", "title": "A Global Stochastic Optimization Particle Filter Algorithm", "comments": "67 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm to learn on the fly the parameter value\n$\\theta_\\star:=\\mathrm{argmax}_{\\theta\\in\\Theta}\\mathbb{E}[\\log f_\\theta(Y_0)]$\nfrom a sequence $(Y_t)_{t\\geq 1}$ of independent copies of $Y_0$, with\n$\\{f_\\theta,\\,\\theta\\in\\Theta\\subseteq\\mathbb{R}^d\\}$ a parametric model. The\nmain idea of the proposed approach is to define a sequence\n$(\\tilde{\\pi}_t)_{t\\geq 1}$ of probability distributions on $\\Theta$ which (i)\nis shown to concentrate on $\\theta_\\star$ as $t\\rightarrow\\infty$ and (ii) can\nbe estimated in an online fashion by means of a standard particle filter (PF)\nalgorithm. The sequence $(\\tilde{\\pi}_t)_{t\\geq 1}$ depends on a learning rate\n$h_t\\rightarrow 0$, with the slower $h_t$ converges to zero the greater is the\nability of the PF approximation $\\tilde{\\pi}_t^N$ of $\\tilde{\\pi}_t$ to escape\nfrom a local optimum of the objective function, but the slower is the rate at\nwhich $\\tilde{\\pi}_t$ concentrates on $\\theta_\\star$. To conciliate ability to\nescape from a local optimum and fast convergence towards $\\theta_\\star$ we\nexploit the acceleration property of averaging, well-known in the stochastic\ngradient descent literature, by letting $\\bar{\\theta}_t^N:=t^{-1}\\sum_{s=1}^t\n\\int_{\\Theta}\\theta\\ \\tilde{\\pi}_s^N(\\mathrm{d} \\theta)$ be the proposed\nestimator of $\\theta_\\star$. Our numerical experiments suggest that\n$\\bar{\\theta}_t^N$ converges to $\\theta_\\star$ at the optimal $t^{-1/2}$ rate\nin challenging models and in situations where $\\tilde{\\pi}_t^N$ concentrates on\nthis parameter value at a slower rate. We illustrate the practical usefulness\nof the proposed optimization algorithm for online parameter learning and for\ncomputing the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:17:43 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:33:23 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 14:49:53 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 11:14:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gerber", "Mathieu", ""], ["Douc", "Randal", ""]]}, {"id": "2007.04849", "submitter": "Mankei Tsang", "authors": "Mankei Tsang", "title": "Physics-inspired forms of the Bayesian Cram\\'er-Rao bound", "comments": "13 pages, 3 figures. v4: added generalization for a vectoral\n  parameter of interest in the Appendix. Published", "journal-ref": "Phys. Rev. A 102, 062217 (2020)", "doi": "10.1103/PhysRevA.102.062217", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using differential geometry, I derive a form of the Bayesian Cram\\'er-Rao\nbound that remains invariant under reparametrization. With the invariant\nformulation at hand, I find the optimal and naturally invariant bound among the\nGill-Levit family of bounds. By assuming that the prior probability density is\nthe square of a wavefunction, I also express the bounds in terms of functionals\nthat are quadratic with respect to the wavefunction and its gradient. The\nproblem of finding an unfavorable prior to tighten the bound for minimax\nestimation is shown, in a special case, to be equivalent to finding the ground\nstate of a Schr\\\"odinger equation, with the Fisher information playing the role\nof the potential. To illustrate the theory, two quantum estimation problems,\nnamely, optomechanical waveform estimation and subdiffraction incoherent\noptical imaging, are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:53:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:35:28 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 09:48:29 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 16:24:44 GMT"}], "update_date": "2021-01-04", "authors_parsed": [["Tsang", "Mankei", ""]]}, {"id": "2007.05071", "submitter": "Volodymyr Shyianov", "authors": "Bamelak Tadele, Volodymyr Shyianov, Faouzi Bellili, Amine Mezghani", "title": "Age-Limited Capacity of Massive MIMO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the age-limited capacity of the Gaussian many\nchannel with total $N$ users, out of which a random subset of $K_{a}$ users are\nactive in any transmission period and a large-scale antenna array at the base\nstation (BS). Motivated by IoT applications and promises of the massive MIMO\ntechnology, we consider the setting in which both the number of users, $N$, and\nthe number of antennas at the BS, $M$, are allowed to grow large at a fixed\nratio $\\zeta = \\frac{M}{N}$. Assuming perfect channel state information (CSI)\nat the receiver, we derive the achievability bound under maximal ratio\ncombining. As the number of active users, $K_{a}$, increases, the achievable\nspectral efficiency is found to increase monotonically to a limit\n$\\log_2\\left(1+\\frac{M}{K_{a}}\\right)$. Using the age of information (AoI)\nmetric, first coined in \\cite{kaul2011minimizing}, as our measure of data\ntimeliness/freshness, we investigate the trade-offs between the AoI and\nspectral efficiency in the context massive connectivity with large-scale\nreceiving antenna arrays. Based on our large system analysis, we provide an\naccurate characterization of the asymptotic spectral efficiency as a function\nof the number of antennas/users, the attempt probability, and the AoI. It is\nfound that while the spectral efficiency can be made large, the penalty is an\nincrease in the minimum AoI obtainable. The proposed achievability bound is\nfurther compared against recent massive MIMO-based massive unsourced random\naccess (URA) schemes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:20:44 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Tadele", "Bamelak", ""], ["Shyianov", "Volodymyr", ""], ["Bellili", "Faouzi", ""], ["Mezghani", "Amine", ""]]}, {"id": "2007.05176", "submitter": "Mueenuddin Azad", "authors": "Mueen-ud-Din Azad and Muhammad Mohsin", "title": "A concise modification of Marshall-Olkin family of distributions for\n  reliability analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of Marshall-Olkin distribution in reliability theory has\nmotivated us to introduce a generalized exponentiated Marshall-Olkin (GEMO), a\nfamily of distributions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 05:50:46 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Azad", "Mueen-ud-Din", ""], ["Mohsin", "Muhammad", ""]]}, {"id": "2007.05215", "submitter": "Jan O. Bauer", "authors": "Jan O. Bauer and Bernhard Drabant", "title": "Principal Loading Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a tool for dimension reduction where the dimension of the\noriginal space is reduced: a Principal Loading Analysis (PLA). PLA is a tool to\nreduce dimensions by discarding variables. The intuition is that variables are\ndropped which distort the covariance matrix only by a little. Our method is\nintroduced and an algorithm for conducting PLA is provided. Further, we give\nbounds for the noise arising in the sample case.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:38:11 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 12:52:38 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bauer", "Jan O.", ""], ["Drabant", "Bernhard", ""]]}, {"id": "2007.05434", "submitter": "Maram Akila", "authors": "Joachim Sicking, Maram Akila, Tim Wirtz, Sebastian Houben, Asja\n  Fischer", "title": "Characteristics of Monte Carlo Dropout in Wide Neural Networks", "comments": "Accepted at the ICML 2020 workshop for Uncertainty and Robustness in\n  Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) dropout is one of the state-of-the-art approaches for\nuncertainty estimation in neural networks (NNs). It has been interpreted as\napproximately performing Bayesian inference. Based on previous work on the\napproximation of Gaussian processes by wide and deep neural networks with\nrandom weights, we study the limiting distribution of wide untrained NNs under\ndropout more rigorously and prove that they as well converge to Gaussian\nprocesses for fixed sets of weights and biases. We sketch an argument that this\nproperty might also hold for infinitely wide feed-forward networks that are\ntrained with (full-batch) gradient descent. The theory is contrasted by an\nempirical analysis in which we find correlations and non-Gaussian behaviour for\nthe pre-activations of finite width NNs. We therefore investigate how\n(strongly) correlated pre-activations can induce non-Gaussian behavior in NNs\nwith strongly correlated weights.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:14:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Sicking", "Joachim", ""], ["Akila", "Maram", ""], ["Wirtz", "Tim", ""], ["Houben", "Sebastian", ""], ["Fischer", "Asja", ""]]}, {"id": "2007.05455", "submitter": "Pierre Monmarch\\'e", "authors": "Pierre Monmarch\\'e", "title": "High-dimensional MCMC with a standard splitting scheme for the\n  underdamped Langevin diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of a Markov sampler based on the underdamped Langevin\ndiffusion is studied for high dimensional targets with convex and smooth\npotentials. We consider a classical second-order integrator which requires only\none gradient computation per iteration. Contrary to previous works on similar\nsamplers, a dimension-free contraction of Wasserstein distances and convergence\nrate for the total variance distance are proven for the discrete time chain\nitself. Non-asymptotic Wasserstein and total variation efficiency bounds and\nconcentration inequalities are obtained for both the Metropolis adjusted and\nunadjusted chains. \\nv{In particular, for the unadjusted chain,} in terms of\nthe dimension $d$ and the desired accuracy $\\varepsilon$, the Wasserstein\nefficiency bounds are of order $\\sqrt d / \\varepsilon$ in the general case,\n$\\sqrt{d/\\varepsilon}$ if the Hessian of the potential is Lipschitz, and\n$d^{1/4}/\\sqrt\\varepsilon$ in the case of a separable target, in accordance\nwith known results for other kinetic Langevin or HMC schemes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 15:51:53 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 16:47:04 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 17:16:09 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 08:52:33 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Monmarch\u00e9", "Pierre", ""]]}, {"id": "2007.05670", "submitter": "Yimin Huang", "authors": "Yimin Huang, Yujun Li, Hanrong Ye, Zhenguo Li, Zhihua Zhang", "title": "An Asymptotically Optimal Multi-Armed Bandit Algorithm and\n  Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of hyperparameters, neural architectures, or data augmentation\npolicies becomes a critical model selection problem in advanced deep learning\nwith a large hyperparameter search space. In this paper, we propose an\nefficient and robust bandit-based algorithm called Sub-Sampling (SS) in the\nscenario of hyperparameter search evaluation. It evaluates the potential of\nhyperparameters by the sub-samples of observations and is theoretically proved\nto be optimal under the criterion of cumulative regret. We further combine SS\nwith Bayesian Optimization and develop a novel hyperparameter optimization\nalgorithm called BOSS. Empirical studies validate our theoretical arguments of\nSS and demonstrate the superior performance of BOSS on a number of\napplications, including Neural Architecture Search (NAS), Data Augmentation\n(DA), Object Detection (OD), and Reinforcement Learning (RL).\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 03:15:21 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 10:28:43 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Huang", "Yimin", ""], ["Li", "Yujun", ""], ["Ye", "Hanrong", ""], ["Li", "Zhenguo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2007.05709", "submitter": "Jonas Brehmer", "authors": "Jonas Brehmer and Tilmann Gneiting", "title": "Scoring Interval Forecasts: Equal-Tailed, Shortest, and Modal Interval", "comments": "24 pages", "journal-ref": "Bernoulli, Volume 27 (3), 2021, 1993-2010", "doi": "10.3150/20-BEJ1298", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider different types of predictive intervals and ask whether they are\nelicitable, i.e. are unique minimizers of a loss or scoring function in\nexpectation. The equal-tailed interval is elicitable, with a rich class of\nsuitable loss functions, though subject to translation invariance, or positive\nhomogeneity and differentiability, the Winkler interval score becomes a unique\nchoice. The modal interval also is elicitable, with a sole consistent scoring\nfunction, up to equivalence. However, the shortest interval fails to be\nelicitable relative to practically relevant classes of distributions. These\nresults provide guidance in interval forecast evaluation and support recent\nchoices of performance measures in forecast competitions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 07:55:02 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 07:35:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Brehmer", "Jonas", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "2007.05737", "submitter": "Stefan Richter", "authors": "Nathawut Phandoidaen and Stefan Richter", "title": "Empirical process theory for locally stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a framework for empirical process theory of locally stationary\nprocesses using the functional dependence measure. Our results extend known\nresults for stationary mixing sequences by another common possibility to\nmeasure dependence and allow for additional time dependence. We develop maximal\ninequalities for expectations and provide functional limit theorems and\nBernstein-type inequalities. We show their applicability to a variety of\nsituations, for instance we prove the weak functional convergence of the\nempirical distribution function and uniform convergence rates for kernel\ndensity and regression estimation if the observations are locally stationary\nprocesses.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 10:31:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Phandoidaen", "Nathawut", ""], ["Richter", "Stefan", ""]]}, {"id": "2007.05905", "submitter": "Erik Ordentlich", "authors": "Erik Ordentlich", "title": "Denoising as well as the best of any two denoisers", "comments": "19 pages. Appeared, in part, in Proceedings of 2013 IEEE Intl. Symp.\n  on Info. Theory. This version has full proofs (e.g., of Proposition 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two arbitrary sequences of denoisers for block lengths tending to\ninfinity we ask if it is possible to construct a third sequence of denoisers\nwith an asymptotically vanishing (in block length) excess expected loss\nrelative to the best expected loss of the two given denoisers for all clean\nchannel input sequences. As in the setting of DUDE [1], which solves this\nproblem when the given denoisers are sliding block denoisers, the construction\nis allowed to depend on the two given denoisers and the channel transition\nprobabilities. We show that under certain restrictions on the two given\ndenoisers the problem can be solved using a straightforward application of a\nknown loss estimation paradigm. We then show by way of a counter-example that\nthe loss estimation approach fails in the general case. Finally, we show that\nfor the binary symmetric channel, combining the loss estimation with a\nrandomization step leads to a solution to the stated problem under no\nrestrictions on the given denoisers.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 04:19:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ordentlich", "Erik", ""]]}, {"id": "2007.05912", "submitter": "Daniel Kane", "authors": "Daniel M. Kane", "title": "Robust Learning of Mixtures of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We resolve one of the major outstanding problems in robust statistics. In\nparticular, if $X$ is an evenly weighted mixture of two arbitrary\n$d$-dimensional Gaussians, we devise a polynomial time algorithm that given\naccess to samples from $X$ an $\\eps$-fraction of which have been adversarially\ncorrupted, learns $X$ to error $\\poly(\\eps)$ in total variation distance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 05:15:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kane", "Daniel M.", ""]]}, {"id": "2007.05986", "submitter": "Taeho Lee", "authors": "Jong Mun Lee, Taeho Lee", "title": "Technical Note -- Exact simulation of the first passage time of Brownian\n  motion to a symmetric linear boundary", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We state an exact simulation scheme for the first passage time of a Brownian\nmotion to a symmetric linear boundary.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 13:31:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lee", "Jong Mun", ""], ["Lee", "Taeho", ""]]}, {"id": "2007.06083", "submitter": "Sounak Paul", "authors": "Michael A. Kouritzin (1) and Sounak Paul (2) ((1) University of\n  Alberta, (2) University of Chicago)", "title": "On almost sure limit theorems for detecting long-range dependent,\n  heavy-tailed processes", "comments": "28 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marcinkiewicz strong law of large numbers, ${n^{-\\frac1p}}\\sum_{k=1}^{n}\n(d_{k}- d)\\rightarrow 0\\ $ almost surely with $p\\in(1,2)$, are developed for\nproducts $d_k=\\prod_{r=1}^s x_k^{(r)}$, where the $x_k^{(r)} =\n\\sum_{l=-\\infty}^{\\infty}c_{k-l}^{(r)}\\xi_l^{(r)}$ are two-sided linear process\nwith coefficients $\\{c_l^{(r)}\\}_{l\\in \\mathbb{Z}}$ and i.i.d. zero-mean\ninnovations $\\{\\xi_l^{(r)}\\}_{l\\in \\mathbb{Z}}$. The decay of the coefficients\n$c_l^{(r)}$ as $|l|\\to\\infty$, can be slow enough for $\\{x_k^{(r)}\\}$ to have\nlong memory while $\\{d_k\\}$ can have heavy tails. The long-range dependence and\nheavy tails for $\\{d_k\\}$ are handled simultaneously and a decoupling property\nshows the convergence rate is dictated by the worst of long-range dependence\nand heavy tails, but not their combination. The results provide a means to\nestimate how much (if any) long-range dependence and heavy tails a sequential\ndata set possesses, which is done for real financial data. All of the stocks we\nconsidered had some degree of heavy tails. The majority also had long-range\ndependence. The Marcinkiewicz strong law of large numbers is also extended to\nthe multivariate linear process case.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:14:48 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 22:30:56 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Kouritzin", "Michael A.", ""], ["Paul", "Sounak", ""]]}, {"id": "2007.06114", "submitter": "Luca Insolia", "authors": "Luca Insolia, Ana Kenney, Francesca Chiaromonte, and Giovanni Felici", "title": "Simultaneous Feature Selection and Outlier Detection with Optimality\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods capable of tolerating outliers have been broadly\ninvestigated in the last decade. We contribute to this research considering\nhigh-dimensional regression problems contaminated by multiple mean-shift\noutliers which affect both the response and the design matrix. We develop a\ngeneral framework for this class of problems and propose the use of\nmixed-integer programming to simultaneously perform feature selection and\noutlier detection with provably optimal guarantees. We characterize the\ntheoretical properties of our approach, i.e. a necessary and sufficient\ncondition for the robustly strong oracle property, which allows the number of\nfeatures to exponentially increase with the sample size; the optimal estimation\nof the parameters; and the breakdown point of the resulting estimates.\nMoreover, we provide computationally efficient procedures to tune integer\nconstraints and to warm-start the algorithm. We show the superior performance\nof our proposal compared to existing heuristic methods through numerical\nsimulations and an application investigating the relationships between the\nhuman microbiome and childhood obesity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 22:26:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Insolia", "Luca", ""], ["Kenney", "Ana", ""], ["Chiaromonte", "Francesca", ""], ["Felici", "Giovanni", ""]]}, {"id": "2007.06169", "submitter": "Tetsuya Kaji", "authors": "Tetsuya Kaji, Elena Manresa, Guillaume Pouliot", "title": "An Adversarial Approach to Structural Estimation", "comments": "58 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simulation-based estimation method, adversarial estimation,\nfor structural models. The estimator is formulated as the solution to a minimax\nproblem between a generator (which generates synthetic observations using the\nstructural model) and a discriminator (which classifies if an observation is\nsynthetic). The discriminator maximizes the accuracy of its classification\nwhile the generator minimizes it. We show that, with a sufficiently rich\ndiscriminator, the adversarial estimator attains parametric efficiency under\ncorrect specification and the parametric rate under misspecification. We\nadvocate the use of a neural network as a discriminator that can exploit\nadaptivity properties and attain fast rates of convergence. We apply our method\nto the elderly's saving decision model and show that including gender and\nhealth profiles in the discriminator uncovers the bequest motive as an\nimportant source of saving across the wealth distribution, not only for the\nrich.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:31:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kaji", "Tetsuya", ""], ["Manresa", "Elena", ""], ["Pouliot", "Guillaume", ""]]}, {"id": "2007.06283", "submitter": "Aryeh Kontorovich", "authors": "Yair Ashlagi, Lee-Ad Gottlieb, Aryeh Kontorovich", "title": "Functions with average smoothness: structure, algorithms, and learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate a program of average smoothness analysis for efficiently learning\nreal-valued functions on metric spaces. Rather than using the Lipschitz\nconstant as the regularizer, we define a local slope at each point and gauge\nthe function complexity as the average of these values. Since the mean can be\ndramatically smaller than the maximum, this complexity measure can yield\nconsiderably sharper generalization bounds -- assuming that these admit a\nrefinement where the Lipschitz constant is replaced by our average of local\nslopes.\n  Our first major contribution is to obtain just such distribution-sensitive\nbounds. This required overcoming a number of technical challenges, perhaps the\nmost formidable of which was bounding the {\\em empirical} covering numbers,\nwhich can be much worse-behaved than the ambient ones. Our combinatorial\nresults are accompanied by efficient algorithms for smoothing the labels of the\nrandom sample, as well as guarantees that the extension from the sample to the\nwhole space will continue to be, with high probability, smooth on average.\nAlong the way we discover a surprisingly rich combinatorial and analytic\nstructure in the function class we define.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:06:58 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 09:35:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ashlagi", "Yair", ""], ["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "2007.06357", "submitter": "Phillip Murray", "authors": "Phillip Murray, Riccardo Passeggeri, Almut E.D. Veraart and Mikko S.\n  Pakkanen", "title": "Feasible Inference for Stochastic Volatility in Brownian Semistationary\n  Processes", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the finite sample behaviour of a number of estimators\nfor the integrated power volatility process of a Brownian semistationary\nprocess in the non semi-martingale setting. We establish three consistent\nfeasible estimators for the integrated volatility, two derived from parametric\nmethods and one non-parametrically. We then use a simulation study to compare\nthe convergence properties of the estimators to one another, and to a benchmark\nof an infeasible estimator. We further establish bounds for the asymptotic\nvariance of the infeasible estimator and assess whether a central limit theorem\nwhich holds for the infeasible estimator can be translated into a feasible\nlimit theorem for the non-parametric estimator.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:06:42 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 14:00:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murray", "Phillip", ""], ["Passeggeri", "Riccardo", ""], ["Veraart", "Almut E. D.", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "2007.06382", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ruodu Wang", "title": "A class of ie-merging functions", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": "05", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general class of ie-merging functions and pose the problem of\nfinding ie-merging functions outside this class.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:47:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Vovk", "Vladimir", ""], ["Wang", "Ruodu", ""]]}, {"id": "2007.06388", "submitter": "Sandra Schluttenhofer", "authors": "Sandra Schluttenhofer, Jan Johannes", "title": "Adaptive minimax testing for circular convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given observations from a circular random variable contaminated by an\nadditive measurement error, we consider the problem of minimax optimal\ngoodness-of-fit testing in a non-asymptotic framework. We propose direct and\nindirect testing procedures using a projection approach. The structure of the\noptimal tests depends on regularity and ill-posedness parameters of the model,\nwhich are unknown in practice. Therefore, adaptive testing strategies that\nperform optimally over a wide range of regularity and ill-posedness classes\nsimultaneously are investigated. Considering a multiple testing procedure, we\nobtain adaptive i.e. assumption-free procedures and analyse their performance.\nCompared with the non-adaptive tests, their radii of testing face a\ndeterioration by a log-factor. We show that for testing of uniformity this loss\nis unavoidable by providing a lower bound. The results are illustrated\nconsidering Sobolev spaces and ordinary or super smooth error densities.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:02:36 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Schluttenhofer", "Sandra", ""], ["Johannes", "Jan", ""]]}, {"id": "2007.06408", "submitter": "Nan Wu", "authors": "Hau-Tieng Wu and Nan Wu", "title": "Strong Uniform Consistency with Rates for Kernel Density Estimators with\n  General Kernels on Manifolds", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing modern machine learning algorithms, we may need to handle\nkernel density estimation (KDE) with intricate kernels that are not designed by\nthe user and might even be irregular and asymmetric. To handle this emerging\nchallenge, we provide a strong uniform consistency result with the $L^\\infty$\nconvergence rate for KDE on Riemannian manifolds with Riemann integrable\nkernels (in the ambient Euclidean space). We also provide an $L^1$ consistency\nresult for kernel density estimation on Riemannian manifolds with Lebesgue\nintegrable kernels. The isotropic kernels considered in this paper are\ndifferent from the kernels in the Vapnik-Chervonenkis class that are frequently\nconsidered in statistics society. We illustrate the difference when we apply\nthem to estimate the probability density function. Moreover, we elaborate the\ndelicate difference when the kernel is designed on the intrinsic manifold and\non the ambient Euclidian space, both might be encountered in practice. At last,\nwe prove the necessary and sufficient condition for an isotropic kernel to be\nRiemann integrable on a submanifold in the Euclidean space.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:36:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 16:45:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "2007.06697", "submitter": "Sebastian Roch", "authors": "Max Hill and Brandon Legried and Sebastien Roch", "title": "Species tree estimation under joint modeling of coalescence and\n  duplication: sample complexity of quartet methods", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider species tree estimation under a standard stochastic model of gene\ntree evolution that incorporates incomplete lineage sorting (as modeled by a\ncoalescent process) and gene duplication and loss (as modeled by a branching\nprocess). Through a probabilistic analysis of the model, we derive sample\ncomplexity bounds for widely used quartet-based inference methods that\nhighlight the effect of the duplication and loss rates in both subcritical and\nsupercritical regimes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:17:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hill", "Max", ""], ["Legried", "Brandon", ""], ["Roch", "Sebastien", ""]]}, {"id": "2007.06715", "submitter": "Debdeep Pati", "authors": "Sean Plummer, Debdeep Pati and Anirban Bhattacharya", "title": "Dynamics of coordinate ascent variational inference: A case study in 2D\n  Ising models", "comments": null, "journal-ref": null, "doi": "10.3390/e22111263", "report-no": null, "categories": "math.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational algorithms have gained prominence over the past two decades as a\nscalable computational environment for Bayesian inference. In this article, we\nexplore tools from the dynamical systems literature to study convergence of\ncoordinate ascent algorithms for mean field variational inference. Focusing on\nthe Ising model defined on two nodes, we fully characterize the dynamics of the\nsequential coordinate ascent algorithm and its parallel version. We observe\nthat in the regime where the objective function is convex, both the algorithms\nare stable and exhibit convergence to the unique fixed point. Our analyses\nreveal interesting {\\em discordances} between these two versions of the\nalgorithm in the region when the objective function is non-convex. In fact, the\nparallel version exhibits a periodic oscillatory behavior which is absent in\nthe sequential version. Drawing intuition from the Markov chain Monte Carlo\nliterature, we {\\em empirically} show that a parameter expansion of the Ising\nmodel, popularly called as the Edward--Sokal coupling, leads to an enlargement\nof the regime of convergence to the global optima.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:39:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Plummer", "Sean", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2007.06735", "submitter": "Dennis Koh Ph.D.", "authors": "Dennis Koh", "title": "The Distribution Function of the Longest Head Run", "comments": "Due to an error in Theorem 4.3 on page 4. The proof does not work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the open problem of finding a closed analytical expression for\nthe distribution function of the length of the longest pure head run in coin\ntosses of a possibly biased coin is solved by studying the closely related\nMarkov chain of current head runs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 23:32:19 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 08:49:47 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 08:00:37 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Koh", "Dennis", ""]]}, {"id": "2007.06799", "submitter": "Anjaly Parayil", "authors": "Anjaly Parayil, He Bai, Jemin George, and Prudhvi Gurram", "title": "A Decentralized Approach to Bayesian Learning", "comments": "42 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by decentralized approaches to machine learning, we propose a\ncollaborative Bayesian learning algorithm taking the form of decentralized\nLangevin dynamics in a non-convex setting. Our analysis show that the initial\nKL-divergence between the Markov Chain and the target posterior distribution is\nexponentially decreasing while the error contributions to the overall\nKL-divergence from the additive noise is decreasing in polynomial time. We\nfurther show that the polynomial-term experiences speed-up with number of\nagents and provide sufficient conditions on the time-varying step-sizes to\nguarantee convergence to the desired distribution. The performance of the\nproposed algorithm is evaluated on a wide variety of machine learning tasks.\nThe empirical results show that the performance of individual agents with\nlocally available data is on par with the centralized setting with considerable\nimprovement in the convergence rate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 03:59:17 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:33:20 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 18:08:40 GMT"}, {"version": "v4", "created": "Sat, 9 Jan 2021 17:01:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Parayil", "Anjaly", ""], ["Bai", "He", ""], ["George", "Jemin", ""], ["Gurram", "Prudhvi", ""]]}, {"id": "2007.06827", "submitter": "Yaroslav Averyanov", "authors": "Yaroslav Averyanov and Alain Celisse", "title": "Early stopping and polynomial smoothing in regression with reproducing\n  kernels", "comments": "typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of early stopping for iterative learning\nalgorithms in a reproducing kernel Hilbert space (RKHS) in the nonparametric\nregression framework. In particular, we work with the gradient descent and\n(iterative) kernel ridge regression algorithms. We present a data-driven rule\nto perform early stopping without a validation set that is based on the\nso-called minimum discrepancy principle. This method enjoys only one assumption\non the regression function: it belongs to a reproducing kernel Hilbert space\n(RKHS). The proposed rule is proved to be minimax-optimal over different types\nof kernel spaces, including finite-rank and Sobolev smoothness classes. The\nproof is derived from the fixed-point analysis of the localized Rademacher\ncomplexities, which is a standard technique for obtaining optimal rates in the\nnonparametric regression literature. In addition to that, we present simulation\nresults on artificial datasets that show the comparable performance of the\ndesigned rule with respect to other stopping rules such as the one determined\nby V-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:27:18 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 21:26:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Averyanov", "Yaroslav", ""], ["Celisse", "Alain", ""]]}, {"id": "2007.07065", "submitter": "Ulrich Mueller", "authors": "Ulrich K. Mueller", "title": "A More Robust t-Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard inference about a scalar parameter estimated via GMM amounts to\napplying a t-test to a particular set of observations. If the number of\nobservations is not very large, then moderately heavy tails can lead to poor\nbehavior of the t-test. This is a particular problem under clustering, since\nthe number of observations then corresponds to the number of clusters, and\nheterogeneity in cluster sizes induces a form of heavy tails. This paper\ncombines extreme value theory for the smallest and largest observations with a\nnormal approximation for the average of the remaining observations to construct\na more robust alternative to the t-test. The new test is found to control size\nmuch more successfully in small samples compared to existing methods.\nAnalytical results in the canonical inference for the mean problem demonstrate\nthat the new test provides a refinement over the full sample t-test under more\nthan two but less than three moments, while the bootstrapped t-test does not.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:42:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mueller", "Ulrich K.", ""]]}, {"id": "2007.07472", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla and Sabyasachi Chatterjee", "title": "Risk Bounds for Quantile Trend Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quantile trend filtering, a recently proposed method for\nnonparametric quantile regression with the goal of generalizing existing risk\nbounds known for the usual trend filtering estimators which perform mean\nregression. We study both the penalized and the constrained version (of order\n$r \\geq 1$) of quantile trend filtering. Our results show that both the\nconstrained and the penalized version (of order $r \\geq 1$) attain the minimax\nrate up to log factors, when the $(r-1)$th discrete derivative of the true\nvector of quantiles belongs to the class of bounded variation signals. We also\nshow that if the true vector of quantiles is a discrete spline with a few\npolynomial pieces then the constrained version attains a near parametric rate\nof convergence. Corresponding results for the usual trend filtering estimators\nare known to hold only when the errors are sub-Gaussian. In contrast, our risk\nbounds are shown to hold under minimal assumptions on the error variables. In\nparticular, no moment assumptions are needed and our results hold under\nheavy-tailed errors. On the other hand, we prove all our results for a Huber\ntype loss which can be smaller than the mean squared error loss employed for\nshowing risk bounds for usual trend filtering. Our proof techniques are general\nand thus can potentially be used to study other nonparametric quantile\nregression methods. To illustrate this generality we also employ our proof\ntechniques to obtain new results for multivariate quantile total variation\ndenoising and high dimensional quantile linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 04:20:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 20:31:53 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 04:46:49 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 20:57:17 GMT"}, {"version": "v5", "created": "Thu, 24 Jun 2021 23:30:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Chatterjee", "Sabyasachi", ""]]}, {"id": "2007.07507", "submitter": "Anuran Makur", "authors": "Anuran Makur", "title": "Coding Theorems for Noisy Permutation Channels", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": "10.1109/TIT.2020.3009468", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formally define and analyze the class of noisy permutation\nchannels. The noisy permutation channel model constitutes a standard discrete\nmemoryless channel (DMC) followed by an independent random permutation that\nreorders the output codeword of the DMC. While coding theoretic aspects of this\nmodel have been studied extensively, particularly in the context of reliable\ncommunication in network settings where packets undergo transpositions, and\nclosely related models of DNA based storage systems have also been analyzed\nrecently, we initiate an information theoretic study of this model by defining\nan appropriate notion of noisy permutation channel capacity. Specifically, on\nthe achievability front, we prove a lower bound on the noisy permutation\nchannel capacity of any DMC in terms of the rank of the stochastic matrix of\nthe DMC. On the converse front, we establish two upper bounds on the noisy\npermutation channel capacity of any DMC whose stochastic matrix is strictly\npositive (entry-wise). Together, these bounds yield coding theorems that\ncharacterize the noisy permutation channel capacities of every strictly\npositive and \"full rank\" DMC, and our achievability proof yields a conceptually\nsimple, computationally efficient, and capacity achieving coding scheme for\nsuch DMCs. Furthermore, we also demonstrate the relation between the output\ndegradation preorder over channels and noisy permutation channel capacity. In\nfact, the proof of one of our converse bounds exploits a degradation result\nthat constructs a symmetric channel for any DMC such that the DMC is a degraded\nversion of the symmetric channel. Finally, we illustrate some examples such as\nthe special cases of binary symmetric channels and (general) erasure channels.\nSomewhat surprisingly, our results suggest that noisy permutation channel\ncapacities are generally quite agnostic to the parameters that define the DMCs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:50:44 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Makur", "Anuran", ""]]}, {"id": "2007.07623", "submitter": "Lionel Truquet", "authors": "Paul Doukhan, Michael H. Neumann and Lionel Truquet", "title": "Stationarity and ergodic properties for some observation-driven models\n  in random environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first motivation of this paper is to study stationarity and ergodic\nproperties for a general class of time series models defined conditional on an\nexogenous covariates process. The dynamic of these models is given by an\nautoregressive latent process which forms a Markov chain in random\nenvironments. Contrarily to existing contributions in the field of Markov\nchains in random environments, the state space is not discrete and we do not\nuse small set type assumptions or uniform contraction conditions for the random\nMarkov kernels. Our assumptions are quite general and allows to deal with\nmodels that are not fully contractive, such as threshold autoregressive\nprocesses. Using a coupling approach, we study the existence of a limit, in\nWasserstein metric, for the backward iterations of the chain. We also derive\nergodic properties for the corresponding skew-product Markov chain. Our results\nare illustrated with many examples of autoregressive processes widely used in\nstatistics or in econometrics, including GARCH type processes, count\nautoregressions and categorical time series.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:27:40 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Doukhan", "Paul", ""], ["Neumann", "Michael H.", ""], ["Truquet", "Lionel", ""]]}, {"id": "2007.07847", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "A Bayesian Multiple Testing Paradigm for Model Selection in Inverse\n  Regression Problems", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel Bayesian multiple testing formulation for\nmodel and variable selection in inverse setups, judiciously embedding the idea\nof inverse reference distributions proposed by Bhattacharya (2013) in a mixture\nframework consisting of the competing models. We develop the theory and methods\nin the general context encompassing parametric and nonparametric competing\nmodels, dependent data, as well as misspecifications. Our investigation shows\nthat asymptotically the multiple testing procedure almost surely selects the\nbest possible inverse model that minimizes the minimum Kullback-Leibler\ndivergence from the true model. We also show that the error rates, namely,\nversions of the false discovery rate and the false non-discovery rate converge\nto zero almost surely as the sample size goes to infinity. Asymptotic\n{\\alpha}-control of versions of the false discovery rate and its impact on the\nconvergence of false non-discovery rate versions, are also investigated.\n  Our simulation experiments involve small sample based selection among inverse\nPoisson log regression and inverse geometric logit and probit regression, where\nthe regressions are either linear or based on Gaussian processes. Additionally,\nvariable selection is also considered. Our multiple testing results turn out to\nbe very encouraging in the sense of selecting the best models in all the\nnon-misspecified and misspecified cases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:12:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2007.07876", "submitter": "Yunbei Xu", "authors": "Yunbei Xu and Assaf Zeevi", "title": "Upper Counterfactual Confidence Bounds: a New Optimism Principle for\n  Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of optimism in the face of uncertainty is one of the most\nwidely used and successful ideas in multi-armed bandits and reinforcement\nlearning. However, existing optimistic algorithms (primarily UCB and its\nvariants) are often unable to deal with large context spaces. Essentially all\nexisting well performing algorithms for general contextual bandit problems rely\non weighted action allocation schemes; and theoretical guarantees for\noptimism-based algorithms are only known for restricted formulations. In this\npaper we study general contextual bandits under the realizability condition,\nand propose a simple generic principle to design optimistic algorithms, dubbed\n\"Upper Counterfactual Confidence Bounds\" (UCCB). We show that these algorithms\nare provably optimal and efficient in the presence of large context spaces. Key\ncomponents of UCCB include: 1) a systematic analysis of confidence bounds in\npolicy space rather than in action space; and 2) the potential function\nperspective that is used to express the power of optimism in the contextual\nsetting. We further show how the UCCB principle can be extended to infinite\naction spaces, by constructing confidence bounds via the newly introduced\nnotion of \"counterfactual action divergence.\"\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:50:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 04:03:41 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 19:05:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Xu", "Yunbei", ""], ["Zeevi", "Assaf", ""]]}, {"id": "2007.07878", "submitter": "Uthsav Chitra", "authors": "Uthsav Chitra, Kimberly Ding, Jasper C.H. Lee, Benjamin J. Raphael", "title": "Quantifying and Reducing Bias in Maximum Likelihood Estimation of\n  Structured Anomalies", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly estimation, or the problem of finding a subset of a dataset that\ndiffers from the rest of the dataset, is a classic problem in machine learning\nand data mining. In both theoretical work and in applications, the anomaly is\nassumed to have a specific structure defined by membership in an\n$\\textit{anomaly family}$. For example, in temporal data the anomaly family may\nbe time intervals, while in network data the anomaly family may be connected\nsubgraphs. The most prominent approach for anomaly estimation is to compute the\nMaximum Likelihood Estimator (MLE) of the anomaly; however, it was recently\nobserved that for normally distributed data, the MLE is a $\\textit{biased}$\nestimator for some anomaly families. In this work, we demonstrate that in the\nnormal means setting, the bias of the MLE depends on the size of the anomaly\nfamily. We prove that if the number of sets in the anomaly family that contain\nthe anomaly is sub-exponential, then the MLE is asymptotically unbiased. We\nalso provide empirical evidence that the converse is true: if the number of\nsuch sets is exponential, then the MLE is asymptotically biased. Our analysis\nunifies a number of earlier results on the bias of the MLE for specific anomaly\nfamilies. Next, we derive a new anomaly estimator using a mixture model, and we\nprove that our anomaly estimator is asymptotically unbiased regardless of the\nsize of the anomaly family. We illustrate the advantages of our estimator\nversus the MLE on disease outbreak and highway traffic data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:54:27 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 16:54:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chitra", "Uthsav", ""], ["Ding", "Kimberly", ""], ["Lee", "Jasper C. H.", ""], ["Raphael", "Benjamin J.", ""]]}, {"id": "2007.08054", "submitter": "Ilya Timofeyev", "authors": "Xi Chen and Ilya Timofeyev", "title": "Non-parametric estimation of Stochastic Differential Equations from\n  stationary time-series", "comments": "Submitted to J. Stat. Phys", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study efficiency of non-parametric estimation of diffusions (stochastic\ndifferential equations driven by Brownian motion) from long stationary\ntrajectories. First, we introduce estimators based on conditional expectation\nwhich is motivated by the definition of drift and diffusion coefficients. These\nestimators involve time- and space-discretization parameters for computing\nexpected values from discretely-sampled stationary data. Next, we analyze\nconsistency and mean squared error of these estimators depending on\ncomputational parameters. We derive relationships between the number of\nobservational points, time- and space-discretization parameters in order to\nachieve the optimal speed of convergence and minimize computational complexity.\nWe illustrate our approach with numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:57:16 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 00:18:07 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Chen", "Xi", ""], ["Timofeyev", "Ilya", ""]]}, {"id": "2007.08369", "submitter": "Ivan Kojadinovic", "authors": "Mark Holmes and Ivan Kojadinovic", "title": "Open-end nonparametric sequential change-point detection based on the\n  retrospective CUSUM statistic", "comments": "41 pages, 7 figures, 3 tables, some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of online monitoring is to issue an alarm as soon as there is\nsignificant evidence in the collected observations to suggest that the\nunderlying data generating mechanism has changed. This work is concerned with\nopen-end, nonparametric procedures that can be interpreted as statistical\ntests. The proposed monitoring schemes consist of computing the so-called\nretrospective CUSUM statistic (or minor variations thereof) after the arrival\nof each new observation. After proposing suitable threshold functions for the\nchosen detectors, the asymptotic validity of the procedures is investigated in\nthe special case of monitoring for changes in the mean, both under the null\nhypothesis of stationarity and relevant alternatives. To carry out the\nsequential tests in practice, an approach based on an asymptotic regression\nmodel is used to estimate high quantiles of relevant limiting distributions.\nMonte Carlo experiments demonstrate the good finite-sample behavior of the\nproposed monitoring schemes and suggest that they are superior to existing\ncompetitors as long as changes do not occur at the very beginning of the\nmonitoring. Extensions to statistics exhibiting an asymptotic mean-like\nbehavior are briefly discussed. Finally, the application of the derived\nsequential change-point detection tests is succinctly illustrated on\ntemperature anomaly data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:44:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 11:43:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Holmes", "Mark", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "2007.08484", "submitter": "Alejandro  Cholaquidis", "authors": "Catherine Aaron, Alejandro Cholaquidis, Ricardo Fraiman", "title": "Surface and length estimation based on Crofton's formula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the surface area of the boundary of a\nsufficiently smooth set when the available information is only a set of points\n(random or not) that becomes dense (with respect to Hausdorff distance) in the\nset or the trajectory of a reflected diffusion.\n  We obtain consistency results in this general setup, and we derive rates of\nconvergence for the iid case or when the data corresponds to the trajectory of\na reflected Brownian motion.\n  We propose an algorithm based on Crofton's formula, which estimates the\nnumber of intersections of random lines with the boundary of the set by\ncounting, in a suitable way (given by the proposed algorithm), the number of\nintersections with the boundary of two different estimators: the Devroye--Wise\nestimator and the $\\alpha$-convex hull of the data. \\rm\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:39:07 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 20:23:49 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Aaron", "Catherine", ""], ["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""]]}, {"id": "2007.08588", "submitter": "Emily C Hector", "authors": "Emily C. Hector and Peter X.-K. Song", "title": "Doubly Distributed Supervised Learning and Inference with\n  High-Dimensional Correlated Outcomes", "comments": "49 pages, 1 figure", "journal-ref": "Journal of Machine Learning Research, 21(173):1-35, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified framework for supervised learning and inference\nprocedures using the divide-and-conquer approach for high-dimensional\ncorrelated outcomes. We propose a general class of estimators that can be\nimplemented in a fully distributed and parallelized computational scheme.\nModelling, computational and theoretical challenges related to high-dimensional\ncorrelated outcomes are overcome by dividing data at both outcome and subject\nlevels, estimating the parameter of interest from blocks of data using a broad\nclass of supervised learning procedures, and combining block estimators in a\nclosed-form meta-estimator asymptotically equivalent to estimates obtained by\nHansen (1982)'s generalized method of moments (GMM) that does not require the\nentire data to be reloaded on a common server. We provide rigorous theoretical\njustifications for the use of distributed estimators with correlated outcomes\nby studying the asymptotic behaviour of the combined estimator with fixed and\ndiverging number of data divisions. Simulations illustrate the finite sample\nperformance of the proposed method, and we provide an R package for ease of\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:49:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hector", "Emily C.", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "2007.08806", "submitter": "Alexis Rosuel", "authors": "Philippe Loubaton (LIGM), Alexis Rosuel (LIGM)", "title": "Large random matrix approach for testing independence of a large number\n  of Gaussian time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behaviour of Linear Spectral Statistics (LSS) of the smoothed\nperiodogram estimator of the spectral coherency matrix of a complex Gaussian\nhigh-dimensional time series (yn) n$\\in$Z with independent components is\nstudied under the asymptotic regime where both the dimension M of y and the\nsmoothing span of the estimator grow to infinity at the same rate. It is\nestablished that the estimated spectral coherency matrix is close from the\nsample covariance matrix of an independent identically N C (0, I M) distributed\nsequence, and that its empirical eigenvalue distribution converges towards the\nMarcenko-Pastur distribution. This allows to conclude that each LSS has a\ndeterministic behaviour that can be evaluated explicitely. Using concentration\ninequalities, it is shown that the order of magnitude of the deviation of each\nLSS from its deterministic approximation is of the order of M N where N is the\nsample size. Numerical simulations suggest that these results can be used to\ntest whether a large number of time series are uncorrelated or not. MSC 2010\nsubject classifications: Primary 60B20, 62H15; secondary 62M15.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:02:54 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:20:53 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 08:30:59 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 09:46:47 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Loubaton", "Philippe", "", "LIGM"], ["Rosuel", "Alexis", "", "LIGM"]]}, {"id": "2007.08807", "submitter": "Alexis Rosuel", "authors": "A Rosuel (UPEM), P. Vallet (IMS), P Loubaton (UPEM), X. Mestre (CTTC)", "title": "On the frequency domain detection of high dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of detection, in the frequency domain,\nof a M-dimensional time series modeled as the output of a M x K MIMO filter\ndriven by a K-dimensional Gaussian white noise, and disturbed by an additive\nM-dimensional Gaussian colored noise. We consider the study of test statistics\nbased of the Spectral Coherence Matrix (SCM) obtained as renormalization of the\nsmoothed periodogram matrix of the observed time series over N samples, and\nwith smoothing span B. To that purpose, we consider the asymptotic regime in\nwhich M, B, N all converge to infinity at certain specific rates, while K\nremains fixed. We prove that the SCM may be approximated in operator norm by a\ncorrelated Wishart matrix, for which Random Matrix Theory (RMT) provides a\nprecise description of the asymptotic behaviour of the eigenvalues. These\nresults are then exploited to study the consistency of a test based on the\nlargest eigenvalue of the SCM, and provide some numerical illustrations to\nevaluate the statistical performance of such a test.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:06:11 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Rosuel", "A", "", "UPEM"], ["Vallet", "P.", "", "IMS"], ["Loubaton", "P", "", "UPEM"], ["Mestre", "X.", "", "CTTC"]]}, {"id": "2007.08820", "submitter": "Pierre Bertrand", "authors": "Pierre Bertrand (LPMA), Michel Broniatowski (LPMA), Jean-Fran\\c{c}ois\n  Marcotorchino", "title": "Independence versus Indetermination: basis of two canonical clustering\n  criteria", "comments": "arXiv admin note: text overlap with arXiv:2012.14674", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.SI math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at comparing two coupling approaches as basic layers for\nbuilding clustering criteria, suited for modularizing and clustering very large\nnetworks. We briefly use \"optimal transport theory\" as a starting point, and a\nway as well, to derive two canonical couplings: \"statistical independence\" and\n\"logical indetermination\". A symmetric list of properties is provided and\nnotably the so called \"Monge's properties\", applied to contingency matrices,\nand justifying the $\\otimes$ versus $\\oplus$ notation. A study is proposed,\nhighlighting \"logical indetermination\", because it is, by far, lesser known.\nEventually we estimate the average difference between both couplings as the key\nexplanation of their usually close results in network clustering.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:35:17 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:18:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Bertrand", "Pierre", "", "LPMA"], ["Broniatowski", "Michel", "", "LPMA"], ["Marcotorchino", "Jean-Fran\u00e7ois", ""]]}, {"id": "2007.08936", "submitter": "Marius Kroll", "authors": "Marius Kroll", "title": "Asymptotic Behaviour of the Empirical Distance Covariance for Dependent\n  Data", "comments": "23 pages; Added references and discussion of further work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two asymptotic results for the empirical distance covariance on\nseparable metric spaces without any iid assumption on the samples. In\nparticular, we show the almost sure convergence of the empirical distance\ncovariance for any measure with finite first moments, provided that the samples\nform a strictly stationary and ergodic process. We further give a result\nconcerning the asymptotic distribution of the empirical distance covariance\nunder the assumption of absolute regularity of the samples and extend these\nresults to certain types of pseudometric spaces. In the process, we derive a\ngeneral theorem concerning the asymptotic distribution of degenerate\nV-statistics of order 2 under a strong mixing condition.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:32:31 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 14:38:15 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 21:49:22 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Kroll", "Marius", ""]]}, {"id": "2007.09024", "submitter": "Arnab Auddy", "authors": "Arnab Auddy, Ming Yuan", "title": "Perturbation Bounds for Orthogonally Decomposable Tensors and Their\n  Applications in High Dimensional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop deterministic perturbation bounds for singular values and vectors\nof orthogonally decomposable tensors, in a spirit similar to classical results\nfor matrices. Our bounds exhibit intriguing differences between matrices and\nhigher-order tensors. Most notably, they indicate that for higher-order tensors\nperturbation affects each singular value/vector in isolation. In particular,\nits effect on a singular vector does not depend on the multiplicity of its\ncorresponding singular value or its distance from other singular values. Our\nresults can be readily applied and provide a unified treatment to many\ndifferent problems involving higher-order orthogonally decomposable tensors. In\nparticular, we illustrate the implications of our bounds through three\nconnected yet seemingly different high dimensional data analysis tasks: tensor\nSVD, tensor regression and estimation of latent variable models, leading to new\ninsights in each of these settings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:34:58 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Auddy", "Arnab", ""], ["Yuan", "Ming", ""]]}, {"id": "2007.09284", "submitter": "Ilsang Ohn", "authors": "Ilsang Ohn, Lizhen Lin", "title": "Optimal Bayesian estimation of Gaussian mixtures with growing number of\n  components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study posterior concentration properties of Bayesian procedures for\nestimating finite Gaussian mixtures in which the number of components is\nunknown and allowed to grow with the sample size. Under this general setup, we\nderive a series of new theoretical results. More specifically, we first show\nthat under mild conditions on the prior, the posterior distribution\nconcentrates around the true mixing distribution at a near optimal rate with\nrespect to the Wasserstein distance. Under a separation condition on the true\nmixing distribution, we further show that a better and adaptive convergence\nrate can be achieved, and the number of components can be consistently\nestimated. Furthermore, we derive optimal convergence rates for the\nhigher-order mixture models where the number of components diverges arbitrarily\nfast. In addition, we consider the fractional posterior and investigate its\nposterior contraction rates, which are also shown to be minimax optimal in\nestimating the mixing distribution under mild conditions. We also investigate\nBayesian estimation of general mixtures with strong identifiability conditions,\nand derive the optimal convergence rates when the number of components is\nfixed. Lastly, we study theoretical properties of the posterior of the popular\nDirichlet process (DP) mixture prior, and show that such a model can provide a\nreasonable estimate for the number of components while only guaranteeing a slow\nconvergence rate of the mixing distribution estimation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:54:50 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ohn", "Ilsang", ""], ["Lin", "Lizhen", ""]]}, {"id": "2007.09349", "submitter": "Chuancun Yin", "authors": "Baishuai Zuo, Chuancun Yin, Narayanaswamy Balakrishnan", "title": "Explicit expressions for joint moments of $n$-dimensional elliptical\n  distributions", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by Stein's lemma, we derive two expressions for the joint moments of\nelliptical distributions. We use two different methods to derive\n$E[X_{1}^{2}f(\\mathbf{X})]$ for any measurable function $f$ satisfying some\nregularity conditions. Then, by applying this result, we obtain new formulae\nfor expectations of product of normally distributed random variables, and also\npresent simplified expressions of $E[X_{1}^{2}f(\\mathbf{X})]$ for multivariate\nStudent-$t$, logistic and Laplace distributions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 07:08:28 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 00:12:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zuo", "Baishuai", ""], ["Yin", "Chuancun", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2007.09350", "submitter": "Chuancun Yin", "authors": "Baishuai Zuo, Chuancun Yin", "title": "Conditional tail risk expectations for location-scale mixture of\n  elliptical distributions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present general results on the univariate tail conditional expectation\n(TCE) and multivariate tail conditional expectation for location-scale mixture\nof elliptical distributions. Examples include the location-scale mixture of\nnormal distributions, location-scale mixture of Student-$t$ distributions,\nlocation-scale mixture of Logistic distributions and location-scale mixture of\nLaplace distributions. We also consider portfolio risk decomposition with TCE\nfor location-scale mixture of elliptical distributions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 07:12:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zuo", "Baishuai", ""], ["Yin", "Chuancun", ""]]}, {"id": "2007.09660", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Introduction to Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General linear models (GLM) are often constructed and used in statistical\ninference at the voxel level in brain imaging. In this paper, we explore the\nbasics of random fields and the multiple comparisons on the random fields,\nwhich are necessary to properly threshold statistical maps for the whole image\nat specific statistical significance level. The multiple comparisons are\ncrucial in determining overall statistical significance in correlated test\nstatistics over the whole brain. In practice, t- or F-statistics in adjacent\nvoxels are correlated. So there is the problem of multiple comparisons, which\nwe have simply neglected up to now. For multiple comparisons that account for\nspatially correlated test statistics, various methods were proposed: Bonferroni\ncorrection, random field theory, false discovery rates and permutation tests.\nAmong them, we will explore the random field approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 12:29:58 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2007.09691", "submitter": "Tilo Wiklund", "authors": "Tilo Wiklund", "title": "Information in additional observations of a non-parametric experiment\n  that is not estimable", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ independent and identically distributed observations and measuring\nthe value of obtaining an additional observation in terms of Le Cam's notion of\ndeficiency between experiments we show for certain types of non-parametric\nexperiments that the value of an additional observation decreases at a rate of\n$1/\\sqrt{n}$. This is distinct from the known typical decrease at a rate of\n$1/n$ for parametric experiments and non-decreasing value in the case of very\nlarge experiments. In particular the rate of $1/\\sqrt{n}$ holds for the\nexperiment given by observing samples from a density about which we know only\nthat it is bounded from below by some fixed constant. Thus there exists an\nexperiment where the value of additional observations tends to zero but for\nwhich no consistent, in total variation distance, estimator exists.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 15:28:56 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wiklund", "Tilo", ""]]}, {"id": "2007.09707", "submitter": "Kazuki Okamura", "authors": "Kazuki Okamura", "title": "Characterizations of the Cauchy distribution associated with integral\n  transforms", "comments": "9 pages, to appear in Studia Scientiarum Mathematicarum Hungarica", "journal-ref": "Studia Scientiarum Mathematicarum Hungarica, Volume 57 Issue 3\n  (2020) 385-396", "doi": "10.1556/012.2020.57.3.1469", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give two new simple characterizations of the Cauchy distribution by using\nthe M\\\"obius and Mellin transforms. They also yield characterizations of the\ncircular Cauchy distribution and the mixture Cauchy model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 16:29:29 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Okamura", "Kazuki", ""]]}, {"id": "2007.09751", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla and Alessandro Rinaldo and Larry Wasserman", "title": "Berry-Esseen Bounds for Projection Parameters and Partial Correlations\n  with Increasing Dimension", "comments": "54 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear regression model can be used even when the true regression\nfunction is not linear. The resulting estimated linear function is the best\nlinear approximation to the regression function and the vector $\\beta$ of the\ncoefficients of this linear approximation are the projection parameter. We\nprovide finite sample bounds on the Normal approximation to the law of the\nleast squares estimator of the projection parameters normalized by the\nsandwich-based standard error. Our results hold in the increasing dimension\nsetting and under minimal assumptions on the distribution of the response\nvariable. Furthermore, we construct confidence sets for $\\beta$ in the form of\nhyper-rectangles and establish rates on their coverage accuracy. We provide\nanalogous results for partial correlations among the entries of sub-Gaussian\nvectors.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:53:20 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "2007.09811", "submitter": "Liangyu Zhu", "authors": "Liangyu Zhu, Wenbin Lu, Michael R. Kosorok, Rui Song", "title": "Kernel Assisted Learning for Personalized Dose Finding", "comments": "Accepted for KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individualized dose rule recommends a dose level within a continuous safe\ndose range based on patient level information such as physical conditions,\ngenetic factors and medication histories. Traditionally, personalized dose\nfinding process requires repeating clinical visits of the patient and frequent\nadjustments of the dosage. Thus the patient is constantly exposed to the risk\nof underdosing and overdosing during the process. Statistical methods for\nfinding an optimal individualized dose rule can lower the costs and risks for\npatients. In this article, we propose a kernel assisted learning method for\nestimating the optimal individualized dose rule. The proposed methodology can\nalso be applied to all other continuous decision-making problems. Advantages of\nthe proposed method include robustness to model misspecification and capability\nof providing statistical inference for the estimated parameters. In the\nsimulation studies, we show that this method is capable of identifying the\noptimal individualized dose rule and produces favorable expected outcomes in\nthe population. Finally, we illustrate our approach using data from a warfarin\ndosing study for thrombosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:03:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Liangyu", ""], ["Lu", "Wenbin", ""], ["Kosorok", "Michael R.", ""], ["Song", "Rui", ""]]}, {"id": "2007.09895", "submitter": "Shyam Narayanan", "authors": "Shyam Narayanan", "title": "On Distribution Testing in the Conditional Sampling Model", "comments": "53 pages. Added result on monotonicity testing. Abridged version to\n  appear in Symposium on Discrete Algorithms (SODA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant work studying distribution testing under\nthe Conditional Sampling model. In this model, a query specifies a subset $S$\nof the domain, and the output received is a sample drawn from the distribution\nconditioned on being in $S$. In this paper, we improve query complexity bounds\nfor several classic distribution testing problems in this model.\n  First, we prove that tolerant uniformity testing in the conditional sampling\nmodel can be solved using $\\tilde{O}(\\varepsilon^{-2})$ queries, which is\noptimal and improves upon the $\\tilde{O}(\\varepsilon^{-20})$-query algorithm of\nCanonne et al. [CRS15]. This bound even holds under a restricted version of the\nconditional sampling model called the Pair Conditional Sampling model. Next, we\nprove that tolerant identity testing in the conditional sampling model can be\nsolved in $\\tilde{O}(\\varepsilon^{-4})$ queries, which is the first known bound\nindependent of the support size of the distribution for this problem. Next, we\nuse our algorithm for tolerant uniformity testing to get an\n$\\tilde{O}(\\varepsilon^{-4})$-query algorithm for monotonicity testing in the\nconditional sampling model, improving on the\n$\\tilde{O}(\\varepsilon^{-22})$-query algorithm of Canonne [Can15]. Finally, we\nstudy (non-tolerant) identity testing under the pair conditional sampling\nmodel, and provide a tight bound of $\\tilde{\\Theta}(\\sqrt{\\log N} \\cdot\n\\varepsilon^{-2})$ for the query complexity, where the domain of the\ndistribution has size $N$. This improves upon both the known upper and lower\nbounds in [CRS15].\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:51:35 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 04:23:50 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Narayanan", "Shyam", ""]]}, {"id": "2007.09910", "submitter": "Yi Yu", "authors": "Yi Yu and Sabyasachi Chatterjee", "title": "Localising change points in piecewise polynomials of general degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with a sequence of univariate random variables\nwith piecewise polynomial means and independent sub-Gaussian noise. The\nunderlying polynomials are allowed to be of arbitrary but fixed degrees. All\nthe other model parameters are allowed to vary depending on the sample size.\n  We propose a two-step estimation procedure based on the $\\ell_0$-penalisation\nand provide upper bounds on the localisation error. We complement these results\nby deriving a global information-theoretic lower bounds, which show that our\ntwo-step estimators are nearly minimax rate-optimal. We also show that our\nestimator enjoys near optimally adaptive performance by attaining individual\nlocalisation errors depending on the level of smoothness at individual change\npoints of the underlying signal. In addition, under a special smoothness\nconstraint, we provide a minimax lower bound on the localisation errors. This\nlower bound is independent of the polynomial orders and is sharper than the\nglobal minimax lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 07:28:11 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 12:33:20 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yu", "Yi", ""], ["Chatterjee", "Sabyasachi", ""]]}, {"id": "2007.10050", "submitter": "Seth Strimas-Mackey", "authors": "Xin Bing, Florentina Bunea, Seth Strimas-Mackey, Marten Wegkamp", "title": "Prediction in latent factor regression: Adaptive PCR and beyond", "comments": "46 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is devoted to the finite sample prediction risk analysis of a class\nof linear predictors of a response $Y\\in \\mathbb{R}$ from a high-dimensional\nrandom vector $X\\in \\mathbb{R}^p$ when $(X,Y)$ follows a latent factor\nregression model generated by a unobservable latent vector $Z$ of dimension\nless than $p$. Our primary contribution is in establishing finite sample risk\nbounds for prediction with the ubiquitous Principal Component Regression (PCR)\nmethod, under the factor regression model, with the number of principal\ncomponents adaptively selected from the data -- a form of theoretical guarantee\nthat is surprisingly lacking from the PCR literature. To accomplish this, we\nprove a master theorem that establishes a risk bound for a large class of\npredictors, including the PCR predictor as a special case. This approach has\nthe benefit of providing a unified framework for the analysis of a wide range\nof linear prediction methods, under the factor regression setting. In\nparticular, we use our main theorem to recover known risk bounds for the\nminimum-norm interpolating predictor, which has received renewed attention in\nthe past two years, and a prediction method tailored to a subclass of factor\nregression models with identifiable parameters. This model-tailored method can\nbe interpreted as prediction via clusters with latent centers.\n  To address the problem of selecting among a set of candidate predictors, we\nanalyze a simple model selection procedure based on data-splitting, providing\nan oracle inequality under the factor model to prove that the performance of\nthe selected predictor is close to the optimal candidate. We conclude with a\ndetailed simulation study to support and complement our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:42:47 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:34:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Strimas-Mackey", "Seth", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2007.10206", "submitter": "Viswambhara Makam", "authors": "Harm Derksen and Visu Makam", "title": "Maximum likelihood estimation for matrix normal models via quiver\n  representations", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RT math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the log-likelihood function and Maximum Likelihood\nEstimate (MLE) for the matrix normal model for both real and complex models. We\ndescribe the exact number of samples needed to achieve (almost surely) three\nconditions, namely a bounded log-likelihood function, existence of MLEs, and\nuniqueness of MLEs. As a consequence, we observe that almost sure boundedness\nof log-likelihood function guarantees almost sure existence of an MLE, thereby\nproving a conjecture of Drton, Kuriki and Hoff. The main tools we use are from\nthe theory of quiver representations, in particular, results of Kac, King and\nSchofield on canonical decomposition and stability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:41:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Derksen", "Harm", ""], ["Makam", "Visu", ""]]}, {"id": "2007.10393", "submitter": "Isabel Fulcher", "authors": "Katherine Evans, Isabel Fulcher, and Eric J. Tchetgen Tchetgen", "title": "A coherent likelihood parametrization for doubly robust estimation of a\n  causal effect with missing confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data and confounding are two problems researchers face in\nobservational studies for comparative effectiveness. Williamson et al. (2012)\nrecently proposed a unified approach to handle both issues concurrently using a\nmultiply-robust (MR) methodology under the assumption that confounders are\nmissing at random. Their approach considers a union of models in which any\nsubmodel has a parametric component while the remaining models are\nunrestricted. We show that while their estimating function is MR in theory, the\npossibility for multiply robust inference is complicated by the fact that\nparametric models for different components of the union model are not variation\nindependent and therefore the MR property is unlikely to hold in practice. To\naddress this, we propose an alternative transparent parametrization of the\nlikelihood function, which makes explicit the model dependencies between\nvarious nuisance functions needed to evaluate the MR efficient score. The\nproposed method is genuinely doubly-robust (DR) in that it is consistent and\nasymptotic normal if one of two sets of modeling assumptions holds. We evaluate\nthe performance and doubly robust property of the DR method via a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:28:42 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Evans", "Katherine", ""], ["Fulcher", "Isabel", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "2007.10438", "submitter": "William Fithian", "authors": "William Fithian and Lihua Lei", "title": "Conditional calibration for false discovery rate control under\n  dependence", "comments": "26 pages main text, 17 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of methods for finite-sample false discovery rate\n(FDR) control in multiple testing problems with dependent test statistics where\nthe dependence is fully or partially known. Our approach separately calibrates\na data-dependent p-value rejection threshold for each hypothesis, relaxing or\ntightening the threshold as appropriate to target exact FDR control. In\naddition to our general framework we propose a concrete algorithm, the\ndependence-adjusted Benjamini-Hochberg (dBH) procedure, which adaptively\nthresholds the q-value for each hypothesis. Under positive regression\ndependence the dBH procedure uniformly dominates the standard BH procedure, and\nin general it uniformly dominates the Benjamini-Yekutieli (BY) procedure (also\nknown as BH with log correction). Simulations and real data examples illustrate\npower gains over competing approaches to FDR control under dependence.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:04:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Fithian", "William", ""], ["Lei", "Lihua", ""]]}, {"id": "2007.10586", "submitter": "Archer Zhang", "authors": "Archer Gong Zhang, Guangyu Zhu and Jiahua Chen", "title": "Empirical Likelihood Ratio Test on quantiles under a Density Ratio Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population quantiles are important parameters in many applications.\nEnthusiasm for the development of effective statistical inference procedures\nfor quantiles and their functions has been high for the past decade. In this\narticle, we study inference methods for quantiles when multiple samples from\nlinked populations are available. The research problems we consider have a wide\nrange of applications. For example, to study the evolution of the economic\nstatus of a country, economists monitor changes in the quantiles of annual\nhousehold incomes, based on multiple survey datasets collected annually. Even\nwith multiple samples, a routine approach would estimate the quantiles of\ndifferent populations separately. Such approaches ignore the fact that these\npopulations are linked and share some intrinsic latent structure. Recently,\nmany researchers have advocated the use of the density ratio model (DRM) to\naccount for this latent structure and have developed more efficient procedures\nbased on pooled data. The nonparametric empirical likelihood (EL) is\nsubsequently employed. Interestingly, there has been no discussion in this\ncontext of the EL-based likelihood ratio test (ELRT) for population quantiles.\nWe explore the use of the ELRT for hypotheses concerning quantiles and\nconfidence regions under the DRM. We show that the ELRT statistic has a\nchi-square limiting distribution under the null hypothesis. Simulation\nexperiments show that the chi-square distributions approximate the\nfinite-sample distributions well and lead to accurate tests and confidence\nregions. The DRM helps to improve statistical efficiency. We also give a\nreal-data example to illustrate the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 03:59:54 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 06:36:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Zhang", "Archer Gong", ""], ["Zhu", "Guangyu", ""], ["Chen", "Jiahua", ""]]}, {"id": "2007.10612", "submitter": "Art Owen", "authors": "Swarnadip Ghosh and Trevor Hastie and Art B. Owen", "title": "Backfitting for large scale crossed random effects regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with crossed random effect errors can be very expensive to\ncompute. The cost of both generalized least squares and Gibbs sampling can\neasily grow as $N^{3/2}$ (or worse) for $N$ observations. Papaspiliopoulos et\nal. (2020) present a collapsed Gibbs sampler that costs $O(N)$, but under an\nextremely stringent sampling model. We propose a backfitting algorithm to\ncompute a generalized least squares estimate and prove that it costs $O(N)$. A\ncritical part of the proof is in ensuring that the number of iterations\nrequired is $O(1)$ which follows from keeping a certain matrix norm below\n$1-\\delta$ for some $\\delta>0$. Our conditions are greatly relaxed compared to\nthose for the collapsed Gibbs sampler, though still strict. Empirically, the\nbackfitting algorithm has a norm below $1-\\delta$ under conditions that are\nless strict than those in our assumptions. We illustrate the new algorithm on a\nratings data set from Stitch Fix.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:00:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:06:38 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 22:20:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2007.10725", "submitter": "Edward Wheatcroft", "authors": "Victoria Volodina, Nikki Sonenberg, Edward Wheatcroft and Henry Wynn", "title": "Majorisation as a theory for uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorisation, also called rearrangement inequalities, yields a type of\nstochastic ordering in which two or more distributions can be compared. In this\npaper we argue that majorisation is a good candidate as a theory for\nuncertainty. We present operations that can be applied to study uncertainty in\na range of settings and demonstrate our approach to assessing uncertainty with\nexamples from well known distributions and from applications of climate\nprojections and energy systems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:38:12 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:38:32 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 15:30:30 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Volodina", "Victoria", ""], ["Sonenberg", "Nikki", ""], ["Wheatcroft", "Edward", ""], ["Wynn", "Henry", ""]]}, {"id": "2007.10874", "submitter": "Imma Valentina Curato Dr", "authors": "Imma Valentina Curato, Robert Stelzer and Bennet Str\\\"oh", "title": "Central limit theorems for stationary random fields under weak\n  dependence with application to ambit and mixed moving average fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain central limit theorems for stationary random fields employing a\nnovel measure of dependence called $\\theta$-lex weak dependence. We show that\nthis dependence notion is more general than strong mixing, i.e., it applies to\na broader class of models. Moreover, we discuss hereditary properties for\n$\\theta$-lex and $\\eta$-weak dependence and illustrate the possible\napplications of the weak dependence notions to the study of the asymptotic\nproperties of stationary random fields. Our general results apply to mixed\nmoving average fields (MMAF in short) and ambit fields. We show general\nconditions such that MMAF and ambit fields, with the volatility field being an\nMMAF or a $p$-dependent random field, are weakly dependent. For all the models\nmentioned above, we give a complete characterization of their weak dependence\ncoefficients and sufficient conditions to obtain the asymptotic normality of\ntheir sample moments. Finally, we give explicit computations of the weak\ndependence coefficients of MSTOU processes and analyze under which conditions\nthe developed asymptotic theory applies to CARMA fields.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:03:04 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 10:36:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Curato", "Imma Valentina", ""], ["Stelzer", "Robert", ""], ["Str\u00f6h", "Bennet", ""]]}, {"id": "2007.10952", "submitter": "Robert Adamek", "authors": "Robert Adamek, Stephan Smeekes and Ines Wilms", "title": "Lasso Inference for High-Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop valid inference for high-dimensional time series. We\nextend the desparsified lasso to a time series setting under Near-Epoch\nDependence (NED) assumptions allowing for non-Gaussian, serially correlated and\nheteroskedastic processes, where the number of regressors can possibly grow\nfaster than the time dimension. We first derive an oracle inequality for the\n(regular) lasso, relaxing the commonly made exact sparsity assumption to a\nweaker alternative, which permits many small but non-zero parameters. The weak\nsparsity coupled with the NED assumption means this inequality can also be\napplied to the (inherently misspecified) nodewise regressions performed in the\ndesparsified lasso. This allows us to establish the uniform asymptotic\nnormality of the desparsified lasso under general conditions. Additionally, we\nshow consistency of a long-run variance estimator, thus providing a complete\nset of tools for performing inference in high-dimensional linear time series\nmodels. Finally, we perform a simulation exercise to demonstrate the small\nsample properties of the desparsified lasso in common time series settings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:12:39 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:10:09 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 11:17:19 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adamek", "Robert", ""], ["Smeekes", "Stephan", ""], ["Wilms", "Ines", ""]]}, {"id": "2007.10976", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Yuhan Liu, Ziteng Sun, and\n  Himanshu Tyagi", "title": "Interactive Inference under Information Constraints", "comments": "Adding a section on information losses; improving presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of interactivity in distributed statistical inference under\ninformation constraints, e.g., communication constraints and local differential\nprivacy. We focus on the tasks of goodness-of-fit testing and estimation of\ndiscrete distributions. From prior work, these tasks are well understood under\nnoninteractive protocols. Extending these approaches directly for interactive\nprotocols is difficult due to correlations that can build due to interactivity;\nin fact, gaps can be found in prior claims of tight bounds of distribution\nestimation using interactive protocols.\n  We propose a new approach to handle this correlation and establish a unified\nmethod to establish lower bounds for both tasks. As an application, we obtain\noptimal bounds for both estimation and testing under local differential privacy\nand communication constraints. We also provide an example of a natural testing\nproblem where interactivity helps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:51:34 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 18:26:27 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 03:09:45 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 04:25:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Liu", "Yuhan", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2007.11037", "submitter": "Mike West", "authors": "Mike West", "title": "Bayesian Decision Analysis and Constrained Forecasting", "comments": "17 pages (including title page and Supplementary Material). 4 figures\n  in main text, 3 in supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian decision analysis perspective on problems of constrained\nforecasting is presented and developed, motivated by increasing interest in\nproblems of aggregate and hierarchical forecasting coupled with short-comings\nof traditional, purely inferential approaches. Foundational and pedagogic\ndevelopments underlie new methodological approaches to such problems, explored\nand exemplified in contexts of total-constrained forecasting linked to\nmotivating applications in commercial forecasting. The new perspective is\ncomplementary and integrated with traditional Bayesian inference approaches,\nwhile offering new practical methodology when the traditional view is\nchallenged. Examples explore ranges of practically relevant loss functions in\nsimple, illustrative contexts that highlight the opportunities for methodology\nas well as practically important questions of how constrained forecasting is\nimpacted by dependencies among outcomes being predicted. The paper couples this\ncore development with arguments in support of a broader view of Bayesian\ndecision analysis than is typically adopted, involving studies of predictive\ndistributions of loss function values under putative optimal decisions.\nAdditional examples highlight the practical importance of this broader view in\nthe constrained forecasting context. Extensions to more general constrained\nforecasting problems, and connections with broader interests in forecast\nreconciliation and aggregation are noted along with other broader\nconsiderations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:38:16 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["West", "Mike", ""]]}, {"id": "2007.11041", "submitter": "Tyler Chen", "authors": "Tyler Chen", "title": "Non-asymptotic moment bounds for random variables rounded to\n  non-uniformly spaced sets", "comments": "This material is based upon work supported by the National Science\n  Foundation Graduate Research Fellowship Program under Grant No. DGE-1762114", "journal-ref": null, "doi": "10.1002/sta4.395", "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effects of rounding on the moments of random variables.\nSpecifically, given a random variable $X$ and its rounded counterpart\n$\\operatorname{rd}(X)$, we study $|\\mathbb{E}[X^k] -\n\\mathbb{E}[\\operatorname{rd}(X)^{k}]|$ for non-negative integer $k$. We\nconsider the case that the rounding function $\\operatorname{rd} :\n\\mathbb{R}\\to\\mathbb{F}$ corresponds either to (i) rounding to the nearest\npoint in some discrete set $\\mathbb{F}$ or (ii) rounding randomly to either the\nnearest larger or smaller point in this same set with probabilities\nproportional to the distances to these points. In both cases, we show, under\nreasonable assumptions on the density function of $X$, how to compute a\nconstant $C$ such that $|\\mathbb{E}[X^k] -\n\\mathbb{E}[\\operatorname{rd}(X)^{k}]| < C\\epsilon^2$, provided\n$|\\operatorname{rd}(x) - x| \\leq \\epsilon \\: E(x)$, where $E : \\mathbb{R} \\to\n\\mathbb{R}_{\\geq 0}$ is some fixed positive piecewise linear function. Refined\nbounds for the absolute moments $\\mathbb{E}[ |X^k-\\operatorname{rd}(X)^{k}|]$\nare also given.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:55:02 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 01:37:00 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 05:53:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chen", "Tyler", ""]]}, {"id": "2007.11048", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen", "title": "Maximum likelihood estimation of potential energy in interacting\n  particle systems from single-trajectory data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the parameter estimation problem for the quadratic\npotential energy in interacting particle systems from continuous-time and\nsingle-trajectory data. Even though such dynamical systems are\nhigh-dimensional, we show that the vanilla maximum likelihood estimator\n(without regularization) is able to estimate the interaction potential\nparameter with optimal rate of convergence simultaneously in mean-field limit\nand in long-time dynamics. This to some extend avoids the\ncurse-of-dimensionality for estimating large dynamical systems under symmetry\nof the particle interaction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:15:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:24:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xiaohui", ""]]}, {"id": "2007.11078", "submitter": "Hua Wang", "authors": "Hua Wang, Yachong Yang, Zhiqi Bu, Weijie J. Su", "title": "The Complete Lasso Tradeoff Diagram", "comments": "To appear in the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in the high-dimensional regression is to understand the\ntradeoff between type I and type II errors or, equivalently, false discovery\nrate (FDR) and power in variable selection. To address this important problem,\nwe offer the first complete tradeoff diagram that distinguishes all pairs of\nFDR and power that can be asymptotically realized by the Lasso with some choice\nof its penalty parameter from the remaining pairs, in a regime of linear\nsparsity under random designs. The tradeoff between the FDR and power\ncharacterized by our diagram holds no matter how strong the signals are. In\nparticular, our results improve on the earlier Lasso tradeoff diagram of\narXiv:1511.01957 by recognizing two simple but fundamental constraints on the\npairs of FDR and power. The improvement is more substantial when the regression\nproblem is above the Donoho--Tanner phase transition. Finally, we present\nextensive simulation studies to confirm the sharpness of the complete Lasso\ntradeoff diagram.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 20:19:02 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:49:57 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 02:56:19 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 03:34:10 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Hua", ""], ["Yang", "Yachong", ""], ["Bu", "Zhiqi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2007.11138", "submitter": "Ilias Zadik", "authors": "Jonathan Niles-Weed, Ilias Zadik", "title": "The All-or-Nothing Phenomenon in Sparse Tensor PCA", "comments": "Corrected a typo in the statement of Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical problem of estimating a rank-one sparse tensor\ncorrupted by additive Gaussian noise, a model also known as sparse tensor PCA.\nWe show that for Bernoulli and Bernoulli-Rademacher distributed signals and\n\\emph{for all} sparsity levels which are sublinear in the dimension of the\nsignal, the sparse tensor PCA model exhibits a phase transition called the\n\\emph{all-or-nothing phenomenon}. This is the property that for some\nsignal-to-noise ratio (SNR) $\\mathrm{SNR_c}$ and any fixed $\\epsilon>0$, if the\nSNR of the model is below $\\left(1-\\epsilon\\right)\\mathrm{SNR_c}$, then it is\nimpossible to achieve any arbitrarily small constant correlation with the\nhidden signal, while if the SNR is above $\\left(1+\\epsilon\n\\right)\\mathrm{SNR_c}$, then it is possible to achieve almost perfect\ncorrelation with the hidden signal. The all-or-nothing phenomenon was initially\nestablished in the context of sparse linear regression, and over the last year\nalso in the context of sparse 2-tensor (matrix) PCA, Bernoulli group testing,\nand generalized linear models. Our results follow from a more general result\nshowing that for any Gaussian additive model with a discrete uniform prior, the\nall-or-nothing phenomenon follows as a direct outcome of an appropriately\ndefined \"near-orthogonality\" property of the support of the prior distribution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 00:02:48 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 14:52:00 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Niles-Weed", "Jonathan", ""], ["Zadik", "Ilias", ""]]}, {"id": "2007.11401", "submitter": "Takeru Matsuda", "authors": "Shun-ichi Amari, Takeru Matsuda", "title": "Wasserstein Statistics in One-dimensional Location-Scale Model", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.05479", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein geometry and information geometry are two important structures to\nbe introduced in a manifold of probability distributions. Wasserstein geometry\nis defined by using the transportation cost between two distributions, so it\nreflects the metric of the base manifold on which the distributions are\ndefined. Information geometry is defined to be invariant under reversible\ntransformations of the base space. Both have their own merits for applications.\nIn particular, statistical inference is based upon information geometry, where\nthe Fisher metric plays a fundamental role, whereas Wasserstein geometry is\nuseful in computer vision and AI applications. In this study, we analyze\nstatistical inference based on the Wasserstein geometry in the case that the\nbase space is one-dimensional. By using the location-scale model, we further\nderive the W-estimator that explicitly minimizes the transportation cost from\nthe empirical distribution to a statistical model and study its asymptotic\nbehaviors. We show that the W-estimator is consistent and explicitly give its\nasymptotic distribution by using the functional delta method. The W-estimator\nis Fisher efficient in the Gaussian case.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:23:26 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 04:30:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Amari", "Shun-ichi", ""], ["Matsuda", "Takeru", ""]]}, {"id": "2007.11405", "submitter": "Qiang Liu", "authors": "Lidan He and Qiang Liu and Zhi Liu", "title": "Edgeworth corrections for spot volatility estimator", "comments": null, "journal-ref": "Statistics and Probability Letters 164 (2020)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Edgeworth expansion theory for spot volatility estimator under\ngeneral assumptions on the log-price process that allow for drift and leverage\neffect. The result is based on further estimation of skewness and kurtosis,\nwhen compared with existing second order asymptotic normality result. Thus our\ntheory can provide with a refinement result for the finite sample distribution\nof spot volatility. We also construct feasible confidence intervals (one-sided\nand two-sided) for spot volatility by using Edgeworth expansion. The Monte\nCarlo simulation study we conduct shows that the intervals based on Edgeworth\nexpansion perform better than the conventional intervals based on normal\napproximation, which justifies the correctness of our theoretical conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 04:27:58 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["He", "Lidan", ""], ["Liu", "Qiang", ""], ["Liu", "Zhi", ""]]}, {"id": "2007.11482", "submitter": "Elad Romanov", "authors": "Elad Romanov, Tamir Bendory, Or Ordentlich", "title": "Multi-reference alignment in high dimensions: sample complexity and\n  phase transition", "comments": null, "journal-ref": null, "doi": "10.1137/20M1354994", "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-reference alignment entails estimating a signal in $\\mathbb{R}^L$ from\nits circularly-shifted and noisy copies. This problem has been studied\nthoroughly in recent years, focusing on the finite-dimensional setting (fixed\n$L$). Motivated by single-particle cryo-electron microscopy, we analyze the\nsample complexity of the problem in the high-dimensional regime $L\\to\\infty$.\nOur analysis uncovers a phase transition phenomenon governed by the parameter\n$\\alpha = L/(\\sigma^2\\log L)$, where $\\sigma^2$ is the variance of the noise.\nWhen $\\alpha>2$, the impact of the unknown circular shifts on the sample\ncomplexity is minor. Namely, the number of measurements required to achieve a\ndesired accuracy $\\varepsilon$ approaches $\\sigma^2/\\varepsilon$ for small\n$\\varepsilon$; this is the sample complexity of estimating a signal in additive\nwhite Gaussian noise, which does not involve shifts. In sharp contrast, when\n$\\alpha\\leq 2$, the problem is significantly harder and the sample complexity\ngrows substantially quicker with $\\sigma^2$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:04:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Romanov", "Elad", ""], ["Bendory", "Tamir", ""], ["Ordentlich", "Or", ""]]}, {"id": "2007.11521", "submitter": "Alexandre M\\\"osching", "authors": "Alexandre M\\\"osching and Lutz D\\\"umbgen", "title": "Estimation of a Likelihood Ratio Ordered Family of Distributions -- with\n  a Connection to Total Positivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider bivariate observations $(X_1,Y_1), \\ldots, (X_n,Y_n) \\in\n\\mathbb{R}\\times \\mathbb{R}$ with unknown conditional distributions $Q_x$ of\n$Y$, given that $X = x$. The goal is to estimate these distributions under the\nsole assumption that $Q_x$ is isotonic in $x$ with respect to likelihood ratio\norder. If the observations are identically distributed, a related goal is to\nestimate the joint distribution $\\mathcal{L}(X,Y)$ under the sole assumption\nthat it is totally positive of order two in a certain sense. After reviewing\nand generalizing the concepts of likelihood ratio order and total positivity of\norder two, an algorithm is developed which estimates the unknown family of\ndistributions $(Q_x)_x$ via empirical likelihood. The benefit of the stronger\nregularization imposed by likelihood ratio order over the usual stochastic\norder is evaluated in terms of estimation and predictive performances on\nsimulated as well as real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:28:32 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:18:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["M\u00f6sching", "Alexandre", ""], ["D\u00fcmbgen", "Lutz", ""]]}, {"id": "2007.11581", "submitter": "Mikhail Moklyachuk", "authors": "Maksym Luz and Mikhail Moklyachuk", "title": "Minimax-robust forecasting of sequences with periodically stationary\n  long memory multiple seasonal increments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce stochastic sequences $\\zeta(k)$ with periodically stationary\ngeneralized multiple increments of fractional order which combines\ncyclostationary, multi-seasonal, integrated and fractionally integrated\npatterns. We solve the problem of optimal estimation of linear functionals\nconstructed from unobserved values of stochastic sequences $\\zeta(k)$ based on\ntheir observations at points $ k<0$. For sequences with known spectral\ndensities, we obtain formulas for calculating values of the mean square errors\nand the spectral characteristics of the optimal estimates of functionals.\nFormulas that determine the least favorable spectral densities and minimax\n(robust) spectral characteristics of the optimal linear estimates of\nfunctionals are proposed in the case where spectral densities of sequences are\nnot exactly known while some sets of admissible spectral densities are given.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:34:40 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Luz", "Maksym", ""], ["Moklyachuk", "Mikhail", ""]]}, {"id": "2007.11665", "submitter": "Konstantinos Spiliopoulos", "authors": "Solesne Bourguin, Siragan Gailus, Konstantinos Spiliopoulos", "title": "Discrete-time inference for slow-fast systems driven by fractional\n  Brownian motion", "comments": "arXiv admin note: text overlap with arXiv:1906.02131", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical inference for small-noise-perturbed multiscale dynamical\nsystems where the slow motion is driven by fractional Brownian motion. We\ndevelop statistical estimators for both the Hurst index as well as a vector of\nunknown parameters in the model based on a single time series of observations\nfrom the slow process only. We prove that these estimators are both consistent\nand asymptotically normal as the amplitude of the perturbation and the\ntime-scale separation parameter go to zero. Numerical simulations illustrate\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:46:37 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 15:34:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bourguin", "Solesne", ""], ["Gailus", "Siragan", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "2007.11771", "submitter": "Peng Liao", "authors": "Peng Liao, Zhengling Qi, Susan Murphy", "title": "Batch Policy Learning in Average Reward Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the batch (off-line) policy learning problem in the infinite\nhorizon Markov Decision Process. Motivated by mobile health applications, we\nfocus on learning a policy that maximizes the long-term average reward. We\npropose a doubly robust estimator for the average reward and show that it\nachieves semiparametric efficiency given multiple trajectories collected under\nsome behavior policy. Based on the proposed estimator, we develop an\noptimization algorithm to compute the optimal policy in a parameterized\nstochastic policy class. The performance of the estimated policy is measured by\nthe difference between the optimal average reward in the policy class and the\naverage reward of the estimated policy and we establish a finite-sample regret\nguarantee. To the best of our knowledge, this is the first regret bound for\nbatch policy learning in the infinite time horizon setting. The performance of\nthe method is illustrated by simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 03:28:14 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Liao", "Peng", ""], ["Qi", "Zhengling", ""], ["Murphy", "Susan", ""]]}, {"id": "2007.11848", "submitter": "Nicolas Meyer", "authors": "Meyer Nicolas (LPSM), Olivier Wintenberger (LPSM)", "title": "Multivariate sparse clustering for extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying directions where extreme events occur is a major challenge in\nmultivariate extreme value analysis. In this paper, we use the concept of\nsparse regular variation introduced by Meyer and Wintenberger to infer the tail\ndependence of a random vector X. This approach relies on the Euclidean\nprojection onto the simplex which better exhibits the sparsity structure of the\ntail of X than the standard methods. Our procedure based on a rigorous\nmethodology aims at capturing clusters of extremal coordinates of X. It also\nincludes the identification of a threshold above which the values taken by X\nare considered as extreme. We provide an efficient and scalable algorithm\ncalled MUSCLE and apply it on numerical experiments to highlight the relevance\nof our findings. Finally we illustrate our approach with wind speed data and\nfinancial return data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:20:23 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:19:49 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Nicolas", "Meyer", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2007.12032", "submitter": "Yair Daon", "authors": "Yair Daon", "title": "Sensor Clusterization in D-optimal Design in Infinite Dimensional\n  Bayesian Inverse Problems", "comments": "19 pages, two figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of sensor clusterization in optimal experimental\ndesign for infinite-dimensional Bayesian inverse problems. We suggest an\nanalytically tractable model for such designs and reason how it may lead to\nsensor clusterization in the case of iid measurement noise. We also show that\nin the case of spatially correlated measurement error clusterization does not\noccur. As a part of the analysis we prove a matrix determinant lemma analog in\ninfinite dimensions, as well as a lemma for calculating derivatives of $\\log\n\\det$ of operators.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:20:57 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Daon", "Yair", ""]]}, {"id": "2007.12124", "submitter": "Jana Jure\\v{c}kov\\'a", "authors": "Olcay Arslan, Yesim G\\\"uney, Jana Jureckova, Yetkin Tuac", "title": "Nonparametric Tests in Linear Model with Autoregressive Errors", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the linear regression model with possibly autoregressive errors, we\npropose a family of nonparametric tests for regression under a nuisance\nautoregression. The tests avoid the estimation of nuisance parameters, in\ncontrast to the tests proposed in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:48:37 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Arslan", "Olcay", ""], ["G\u00fcney", "Yesim", ""], ["Jureckova", "Jana", ""], ["Tuac", "Yetkin", ""]]}, {"id": "2007.12175", "submitter": "Tomas Masak", "authors": "Tomas Masak, Soham Sarkar, Victor M. Panaretos", "title": "Principal Separable Component Analysis via the Partial Inner Product", "comments": "23 pages + appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-parametric estimation of covariance lies at the heart of functional\ndata analysis, whether for curve or surface-valued data. The case of a\ntwo-dimensional domain poses both statistical and computational challenges,\nwhich are typically alleviated by assuming separability. However, separability\nis often questionable, sometimes even demonstrably inadequate. We propose a\nframework for the analysis of covariance operators of random surfaces that\ngeneralises separability, while retaining its major advantages. Our approach is\nbased on the additive decomposition of the covariance into a series of\nseparable components. The decomposition is valid for any covariance over a\ntwo-dimensional domain. Leveraging the key notion of the partial inner product,\nwe generalise the power iteration method to general Hilbert spaces and show how\nthe aforementioned decomposition can be efficiently constructed in practice.\nTruncation of the decomposition and retention of the principal separable\ncomponents automatically induces a non-parametric estimator of the covariance,\nwhose parsimony is dictated by the truncation level. The resulting estimator\ncan be calculated, stored and manipulated with little computational overhead\nrelative to separability. The framework and estimation method are genuinely\nnon-parametric, since the considered decomposition holds for any covariance.\nConsistency and rates of convergence are derived under mild regularity\nassumptions, illustrating the trade-off between bias and variance regulated by\nthe truncation level. The merits and practical performance of the proposed\nmethodology are demonstrated in a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:21:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Masak", "Tomas", ""], ["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2007.12241", "submitter": "Margaryta Myronyuk Dr", "authors": "Margaryta Myronyuk", "title": "The Heyde theorem on a group $\\mathbb{R}^n\\times D$, where $D$ is a\n  discrete Abelian group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heyde proved that a Gaussian distribution on the real line is characterized\nby the symmetry of the conditional distribution of one linear statistic given\nanother. The present article is devoted to a group analogue of the Heyde\ntheorem. We describe distributions of independent random variables $\\xi_1$,\n$\\xi_2$ with values in a group $X=\\mathbb{R}^n\\times D$, where $D$ is a\ndiscrete Abelian group, which are characterized by the symmetry of the\nconditional distribution of the linear statistic $L_2 = \\xi_1 + \\delta\\xi_2$\ngiven $L_1 = \\xi_1 + \\xi_2$, where $\\delta$ is a topological automorphism of\n$X$ such that ${Ker}(I+\\delta)=\\{0\\}$.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:26:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Myronyuk", "Margaryta", ""]]}, {"id": "2007.12313", "submitter": "Igor Silin", "authors": "Igor Silin, Jianqing Fan", "title": "Canonical thresholding for non-sparse high-dimensional linear regression", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a high-dimensional linear regression problem. Unlike many papers\non the topic, we do not require sparsity of the regression coefficients;\ninstead, our main structural assumption is a decay of eigenvalues of the\ncovariance matrix of the data. We propose a new family of estimators, called\nthe canonical thresholding estimators, which pick largest regression\ncoefficients in the canonical form. The estimators admit an explicit form and\ncan be linked to LASSO and Principal Component Regression (PCR). A theoretical\nanalysis for both fixed design and random design settings is provided. Obtained\nbounds on the mean squared error and the prediction error of a specific\nestimator from the family allow to clearly state sufficient conditions on the\ndecay of eigenvalues to ensure convergence. In addition, we promote the use of\nthe relative errors, strongly linked with the out-of-sample $R^2$. The study of\nthese relative errors leads to a new concept of joint effective dimension,\nwhich incorporates the covariance of the data and the regression coefficients\nsimultaneously, and describes the complexity of a linear regression problem.\nNumerical simulations confirm good performance of the proposed estimators\ncompared to the previously developed methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 01:12:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Silin", "Igor", ""], ["Fan", "Jianqing", ""]]}, {"id": "2007.12366", "submitter": "Peng Liu", "authors": "Yuyu Chen, Peng Liu, Ken Seng Tan and Ruodu Wang", "title": "Trade-off between validity and efficiency of merging p-values under\n  arbitrary dependence", "comments": "38 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods of combining individual p-values into one p-value are widely\nused in many areas of statistical applications. We say that a combining method\nis valid for arbitrary dependence (VAD) if it does not require any assumption\non the dependence structure of the p-values, whereas it is valid for some\ndependence (VSD) if it requires some specific, perhaps realistic but\nunjustifiable, dependence structures. The trade-off between validity and\nefficiency of these methods is studied via analyzing the choices of critical\nvalues under different dependence assumptions. We introduce the notions of\nindependence-comonotonicity balance (IC-balance) and the price for validity. In\nparticular, IC-balanced methods always produce an identical critical value for\nindependent and perfectly positively dependent p-values, a specific type of\ninsensitivity to a family of dependence assumptions. We show that, among two\nvery general classes of merging methods commonly used in practice, the Cauchy\ncombination method and the Simes method are the only IC-balanced ones.\nSimulation studies and a real data analysis are conducted to analyze the sizes\nand powers of various combining methods in the presence of weak and strong\ndependence.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:18:23 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 07:43:36 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Chen", "Yuyu", ""], ["Liu", "Peng", ""], ["Tan", "Ken Seng", ""], ["Wang", "Ruodu", ""]]}, {"id": "2007.12378", "submitter": "Agnes Lagnoux", "authors": "Jean-Claude Fort (MAP5 - UMR 8145), Thierry Klein (ENAC), Agn\\`es\n  Lagnoux (IMT)", "title": "Global sensitivity analysis and Wasserstein spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity indices are commonly used to quantity the relative inuence of any\nspecic group of input variables on the output of a computer code. In this\npaper, we focus both on computer codes the output of which is a cumulative\ndistribution function and on stochastic computer codes. We propose a way to\nperform a global sensitivity analysis for these kinds of computer codes. In the\nrst setting, we dene two indices: the rst one is based on Wasserstein\nFr{\\'e}chet means while the second one is based on the Hoeding decomposition of\nthe indicators of Wasserstein balls. Further, when dealing with the stochastic\ncomputer codes, we dene an ideal version of the stochastic computer code thats\nts into the frame of the rst setting. Finally, we deduce a procedure to realize\na second level global sensitivity analysis, namely when one is interested in\nthe sensitivity related to the input distributions rather than in the\nsensitivity related to the inputs themselves. Several numerical studies are\nproposed as illustrations in the dierent settings.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 07:01:58 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Fort", "Jean-Claude", "", "MAP5 - UMR 8145"], ["Klein", "Thierry", "", "ENAC"], ["Lagnoux", "Agn\u00e8s", "", "IMT"]]}, {"id": "2007.12441", "submitter": "Michael S{\\o}rensen", "authors": "Emil S. J{\\o}rgensen and Michael S{\\o}rensen", "title": "Prediction-based estimation for diffusion models with high-frequency\n  data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper obtains asymptotic results for parametric inference using\nprediction-based estimating functions when the data are high frequency\nobservations of a diffusion process with an infinite time horizon.\nSpecifically, the data are observations of a diffusion process at $n$\nequidistant time points $\\Delta_n i$, and the asymptotic scenario is $\\Delta_n\n\\to 0$ and $n\\Delta_n \\to \\infty$. For a useful and tractable classes of\nprediction-based estimating functions, existence of a consistent estimator is\nproved under standard weak regularity conditions on the diffusion process and\nthe estimating function. Asymptotic normality of the estimator is established\nunder the additional rate condition $n\\Delta_n^3 \\to 0$. The prediction-based\nestimating functions are approximate martingale estimating functions to a\nsmaller order than what has previously been studied, and new non-standard\nasymptotic theory is needed. A Monte Carlo method for calculating the\nasymptotic variance of the estimators is proposed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 10:35:53 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["J\u00f8rgensen", "Emil S.", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "2007.12671", "submitter": "Pierre Bayle", "authors": "Pierre Bayle, Alexandre Bayle, Lucas Janson, Lester Mackey", "title": "Cross-validation Confidence Intervals for Test Error", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020); 40 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops central limit theorems for cross-validation and consistent\nestimators of its asymptotic variance under weak stability conditions on the\nlearning algorithm. Together, these results provide practical,\nasymptotically-exact confidence intervals for $k$-fold test error and valid,\npowerful hypothesis tests of whether one learning algorithm has smaller\n$k$-fold test error than another. These results are also the first of their\nkind for the popular choice of leave-one-out cross-validation. In our real-data\nexperiments with diverse learning algorithms, the resulting intervals and tests\noutperform the most popular alternative methods from the literature.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:40:06 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 17:24:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bayle", "Pierre", ""], ["Bayle", "Alexandre", ""], ["Janson", "Lucas", ""], ["Mackey", "Lester", ""]]}, {"id": "2007.12807", "submitter": "Boyu Ren", "authors": "Boyu Ren, Prasad Patil, Francesca Dominici, Giovanni Parmigiani,\n  Lorenzo Trippa", "title": "Cross-study learning for generalist and specialist predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration and use of data from multiple studies, for the development of\nprediction models is an important task in several scientific fields. We propose\na framework for generalist and specialist predictions that leverages multiple\ndatasets, with potential differences in the relationships between predictors\nand outcomes. Our framework uses stacking, and it includes three major\ncomponents: 1) an ensemble of prediction models trained on one or more\ndatasets, 2) task-specific utility functions and 3) a no-data-reuse technique\nfor estimating stacking weights. We illustrate that under mild regularity\nconditions the framework produces stacked PFs with oracle properties. In\nparticular we show that the the stacking weights are nearly optimal. We also\ncharacterize the scenario where the proposed no-data-reuse technique increases\nprediction accuracy compared to stacking with data reuse in a special case.We\nperform a simulation study to illustrate these results. We apply our framework\nto predict mortality using a collection of datasets on long-term exposure to\nair pollutants.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:51:18 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:43:47 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 14:45:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ren", "Boyu", ""], ["Patil", "Prasad", ""], ["Dominici", "Francesca", ""], ["Parmigiani", "Giovanni", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2007.12826", "submitter": "Yiqiao Zhong", "authors": "Andrea Montanari and Yiqiao Zhong", "title": "The Interpolation Phase Transition in Neural Networks: Memorization and\n  Generalization under Lazy Training", "comments": "69 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are often operated in a strongly overparametrized\nregime: they comprise so many parameters that they can interpolate the training\nset, even if actual labels are replaced by purely random ones. Despite this,\nthey achieve good prediction error on unseen data: interpolating the training\nset does not induce overfitting. Further, overparametrization appears to be\nbeneficial in that it simplifies the optimization landscape. Here we study\nthese phenomena in the context of two-layers neural networks in the neural\ntangent (NT) regime. We consider a simple data model, with isotropic feature\nvectors in $d$ dimensions, and $N$ hidden neurons. Under the assumption $N \\le\nCd$ (for $C$ a constant), we show that the network can exactly interpolate the\ndata as soon as the number of parameters is significantly larger than the\nnumber of samples: $Nd\\gg n$. Under these assumptions, we show that the\nempirical NT kernel has minimum eigenvalue bounded away from zero, and\ncharacterize the generalization error of min-$\\ell_2$ norm interpolants, when\nthe target function is linear. In particular, we show that the network\napproximately performs ridge regression in the raw features, with a strictly\npositive `self-induced' regularization.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 01:51:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Montanari", "Andrea", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "2007.12936", "submitter": "Mikhail Zhitlukhin", "authors": "Mikhail Zhitlukhin", "title": "A sequential test for the drift of a Brownian motion with a possibility\n  to change a decision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a Bayesian sequential test of two simple hypotheses about the\nvalue of the unobservable drift coefficient of a Brownian motion, with a\npossibility to change the initial decision at subsequent moments of time for\nsome penalty. Such a testing procedure allows to correct the initial decision\nif it turns out to be wrong. The test is based on observation of the posterior\nmean process and makes the initial decision and, possibly, changes it later,\nwhen this process crosses certain thresholds. The solution of the problem is\nobtained by reducing it to joint optimal stopping and optimal switching\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:55:06 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhitlukhin", "Mikhail", ""]]}, {"id": "2007.13099", "submitter": "Aniket Biswas", "authors": "Aniket Biswas, Gaurangadeb Chattopadhyay and Aditya Chatterjee", "title": "Bias corrected estimators for proportion of true null hypotheses under\n  exponential model: Application of adaptive FDR-controlling in segmented\n  failure data", "comments": "27 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recently introduced model based bias corrected estimators for proportion\nof true null hypotheses ($\\pi_0$) under multiple hypotheses testing scenario\nhave been restructured for exponentially distributed random observations\navailable for each of the common hypotheses. Based on stochastic ordering, a\nnew motivation behind formulation of some related estimators for $\\pi_0$ is\ngiven. The reduction of bias for the model based estimators are theoretically\njustified and algorithms for computing the estimators are also presented. The\nestimators are also used to formulate a popular adaptive multiple testing\nprocedure. Extensive numerical study supports superiority of the bias corrected\nestimators. We also point out the adverse effect of using the model based bias\ncorrection method without proper assessment of the underlying distribution. A\ncase-study is done with a synthetic dataset in connection with reliability and\nwarranty studies to demonstrate the applicability of the procedure, under a\nnon-Gaussian set up. The results obtained are in line with the intuition and\nexperience of the subject expert. An intriguing discussion has been attempted\nto conclude the article that also indicates the future scope of study.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 11:01:32 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Biswas", "Aniket", ""], ["Chattopadhyay", "Gaurangadeb", ""], ["Chatterjee", "Aditya", ""]]}, {"id": "2007.13302", "submitter": "Shuangning Li", "authors": "Shuangning Li, Stefan Wager", "title": "Random Graph Asymptotics for Treatment Effect Estimation under Network\n  Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network interference model for causal inference places all experimental\nunits at the vertices of an undirected exposure graph, such that treatment\nassigned to one unit may affect the outcome of another unit if and only if\nthese two units are connected by an edge. This model has recently gained\npopularity as means of incorporating interference effects into the\nNeyman--Rubin potential outcomes framework; and several authors have considered\nestimation of various causal targets, including the direct and indirect effects\nof treatment. In this paper, we consider large-sample asymptotics for treatment\neffect estimation under network interference in a setting where the exposure\ngraph is a random draw from a graphon. When targeting the direct effect, we\nshow that -- in our setting -- popular estimators are considerably more\naccurate than existing results suggest, and provide a central limit theorem in\nterms of moments of the graphon. Meanwhile, when targeting the indirect effect,\nwe leverage our generative assumptions to propose a consistent estimator in a\nsetting where no other consistent estimators are currently available. We also\nshow how our results can be used to conduct a practical assessment of the\nsensitivity of randomized study inference to potential interference effects.\nOverall, our results highlight the promise of random graph asymptotics in\nunderstanding the practicality and limits of causal inference under network\ninterference.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 04:06:31 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 14:40:33 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 13:51:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Shuangning", ""], ["Wager", "Stefan", ""]]}, {"id": "2007.13473", "submitter": "Marcel Klatt", "authors": "Marcel Klatt, Axel Munk and Yoav Zemel", "title": "Limit Laws for Empirical Optimal Solutions in Stochastic Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general linear program in standard form whose right-hand side\nconstraint vector is subject to random perturbations. This defines a stochastic\nlinear program for which, under general conditions, we characterize the\nfluctuations of the corresponding empirical optimal solution by a central\nlimit-type theorem. Our approach relies on the combinatorial nature and the\nconcept of degeneracy inherent in linear programming, in strong contrast to\nwell-known results for smooth stochastic optimization programs. In particular,\nif the corresponding dual linear program is degenerate the asymptotic limit law\nmight not be unique and is determined from the way the empirical optimal\nsolution is chosen. Furthermore, we establish consistency and convergence rates\nof the Hausdorff distance between the empirical and the true optimality sets.\nAs a consequence, we deduce a limit law for the empirical optimal value\ncharacterized by the set of all dual optimal solutions which turns out to be a\nsimple consequence of our general proof techniques.\n  Our analysis is motivated from recent findings in statistical optimal\ntransport that will be of special focus here. In addition to the asymptotic\nlimit laws for optimal transport solutions, we obtain results linking\ndegeneracy of the dual transport problem to geometric properties of the\nunderlying ground space, and prove almost sure uniqueness statements that may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:20:17 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Klatt", "Marcel", ""], ["Munk", "Axel", ""], ["Zemel", "Yoav", ""]]}, {"id": "2007.13482", "submitter": "Dmitri Koroliouk", "authors": "D. Koroliouk and V. S. Koroliuk", "title": "Equilibrium in Wright-Fisher models of population genetics", "comments": "6 pages, a genetic Wright-Fisher model is considered as a\n  multivariate statistical experiment which has a representation as a Discrete\n  Markov Diffusion", "journal-ref": "Kibernetika i Sistemnyi Analiz, No. 2, March-April, 2019, pp.\n  96-101", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For multivariant Wright-Fisher models in population genetics, we introduce\nequilibrium states, expressed by fluctuations of probability ratio, in contrast\nto the traditionally used fluctuations, expressed by the difference between the\ncurrent value of the random process and its equilibrium value. Then the drift\ncomponent of the dynamic process of gene frequencies, primarily expressed as a\nratio of two quadratic forms, is transformed into a cubic parabola with a\ncertain normalization factor.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 10:29:52 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 08:52:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Koroliouk", "D.", ""], ["Koroliuk", "V. S.", ""]]}, {"id": "2007.13716", "submitter": "Yuting Wei", "authors": "Michael Celentano, Andrea Montanari, Yuting Wei", "title": "The Lasso with general Gaussian designs with applications to hypothesis\n  testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is a method for high-dimensional regression, which is now commonly\nused when the number of covariates $p$ is of the same order or larger than the\nnumber of observations $n$. Classical asymptotic normality theory is not\napplicable for this model due to two fundamental reasons: $(1)$ The regularized\nrisk is non-smooth; $(2)$ The distance between the estimator $\\bf\n\\widehat{\\theta}$ and the true parameters vector $\\bf \\theta^\\star$ cannot be\nneglected. As a consequence, standard perturbative arguments that are the\ntraditional basis for asymptotic normality fail.\n  On the other hand, the Lasso estimator can be precisely characterized in the\nregime in which both $n$ and $p$ are large, while $n/p$ is of order one. This\ncharacterization was first obtained in the case of standard Gaussian designs,\nand subsequently generalized to other high-dimensional estimation procedures.\nHere we extend the same characterization to Gaussian correlated designs with\nnon-singular covariance structure. This characterization is expressed in terms\nof a simpler ``fixed design'' model. We establish non-asymptotic bounds on the\ndistance between distributions of various quantities in the two models, which\nhold uniformly over signals $\\bf \\theta^\\star$ in a suitable sparsity class,\nand values of the regularization parameter.\n  As applications, we study the distribution of the debiased Lasso, and show\nthat a degrees-of-freedom correction is necessary for computing valid\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:48:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""], ["Wei", "Yuting", ""]]}, {"id": "2007.13804", "submitter": "Majid Al Sadoon", "authors": "Majid M. Al-Sadoon", "title": "The Spectral Approach to Linear Rational Expectations Models", "comments": "JEL Classification: C10, C32, C62, E32", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers linear rational expectations models in the frequency\ndomain under general conditions. The paper develops necessary and sufficient\nconditions for existence and uniqueness of particular and generic systems and\ncharacterizes the space of all solutions as an affine space in the frequency\ndomain. It is demonstrated that solutions are not generally continuous with\nrespect to the parameters of the models, invalidating mainstream frequentist\nand Bayesian methods. The ill-posedness of the problem motivates regularized\nsolutions with theoretically guaranteed uniqueness, continuity, and even\ndifferentiability properties. Regularization is illustrated in an analysis of\nthe limiting Gaussian likelihood functions of two analytically tractable\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 18:39:45 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 10:48:18 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 09:10:48 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Al-Sadoon", "Majid M.", ""]]}, {"id": "2007.13855", "submitter": "Bruno Scalzo Dees", "authors": "Bruno Scalzo, Ljubisa Stankovic, Danilo P. Mandic", "title": "A Probabilistic Spectral Analysis of Multivariate Real-Valued\n  Nonstationary Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of multivariate spectral representations for real-valued\nnonstationary random variables is introduced, which is characterised by a\ngeneral complex Gaussian distribution. In this way, the temporal signal\nproperties -- harmonicity, wide-sense stationarity and cyclostationarity -- are\ndesignated respectively by the mean, Hermitian variance and pseudo-variance of\nthe associated time-frequency representation (TFR). For rigour, the estimators\nof the TFR distribution parameters are derived within a maximum likelihood\nframework and are shown to be statistically consistent, owing to the\nstatistical identifiability of the proposed distribution parametrization. By\nvirtue of the assumed probabilistic model, a generalised likelihood ratio test\n(GLRT) for nonstationarity detection is also proposed. Intuitive examples\ndemonstrate the utility of the derived probabilistic framework for spectral\nanalysis in low-SNR environments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:45:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Scalzo", "Bruno", ""], ["Stankovic", "Ljubisa", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "2007.13858", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop and William Kengne", "title": "Poisson QMLE for change-point detection in general integer-valued time\n  series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider together the retrospective and the sequential change-point\ndetection in a general class of integer-valued time series.\n  The conditional mean of the process depends on a parameter $\\theta^*$ which\nmay change over time. We propose procedures which are based on the Poisson\nquasi-maximum likelihood estimator of the parameter, and where the updated\nestimator is computed without the historical observations in the sequential\nframework. For both the retrospective and the sequential detection, the test\nstatistics converge to some distributions obtained from the standard Brownian\nmotion under the null hypothesis of no change and diverge to infinity under the\nalternative; that is, these procedures are consistent.\n  Some results of simulations as well as real data application are provided.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:49:34 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2007.14208", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk, Bin Wang, and Ruodu Wang", "title": "Admissible ways of merging p-values under arbitrary dependence", "comments": "43 pages and 9 figures; as compared with the previous version, there\n  are numerous improvements and further simulation studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of merging several p-values into a single p-value are important in\ntheir own right and widely used in multiple hypothesis testing. This paper is\nthe first to systematically study the admissibility (in Wald's sense) of\np-merging functions and their domination structure, without any information on\nthe dependence structure of the input p-values. As a technical tool we use the\nnotion of e-values, which are alternatives to p-values recently promoted by\nseveral authors. We obtain several results on the representation of admissible\np-merging functions via e-values and on (in)admissibility of existing p-merging\nfunctions. By introducing new admissible p-merging functions, we show that some\nclassic merging methods can be strictly improved to enhance power without\ncompromising validity under arbitrary dependence.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:39:33 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 16:50:20 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 14:09:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Vovk", "Vladimir", ""], ["Wang", "Bin", ""], ["Wang", "Ruodu", ""]]}, {"id": "2007.14265", "submitter": "Nicolas Schreuder", "authors": "Evgenii Chzhen, Nicolas Schreuder", "title": "A minimax framework for quantifying risk-fairness trade-off in\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretical framework for the problem of learning a real-valued\nfunction which meets fairness requirements. This framework is built upon the\nnotion of $\\alpha$-relative (fairness) improvement of the regression function\nwhich we introduce using the theory of optimal transport. Setting $\\alpha = 0$\ncorresponds to the regression problem under the Demographic Parity constraint,\nwhile $\\alpha = 1$ corresponds to the classical regression problem without any\nconstraints. For $\\alpha \\in (0, 1)$ the proposed framework allows to\ncontinuously interpolate between these two extreme cases and to study partially\nfair predictors. Within this framework we precisely quantify the cost in risk\ninduced by the introduction of the fairness constraint. We put forward a\nstatistical minimax setup and derive a general problem-dependent lower bound on\nthe risk of any estimator satisfying $\\alpha$-relative improvement constraint.\nWe illustrate our framework on a model of linear regression with Gaussian\ndesign and systematic group-dependent bias, deriving matching (up to absolute\nconstants) upper and lower bounds on the minimax risk under the introduced\nconstraint. Finally, we perform a simulation study of the latter setup.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:21:56 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 16:04:09 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Schreuder", "Nicolas", ""]]}, {"id": "2007.14365", "submitter": "Weichi Wu", "authors": "Weichi Wu, Sofia Olhede, Patrick Wolfe", "title": "Tractably Modelling Dependence in Networks Beyond Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for modelling network data that is designed to\ndescribe aspects of non-exchangeable networks. Conditional on latent\n(unobserved) variables, the edges of the network are generated by their finite\ngrowth history (with latent orders) while the marginal probabilities of the\nadjacency matrix are modeled by a generalization of a graph limit function (or\na graphon). In particular, we study the estimation, clustering and degree\nbehavior of the network in our setting. We determine (i) the minimax estimator\nof a composite graphon with respect to squared error loss; (ii) that spectral\nclustering is able to consistently detect the latent membership when the\nblock-wise constant composite graphon is considered under additional\nconditions; and (iii) we are able to construct models with heavy-tailed\nempirical degrees under specific scenarios and parameter choices. This explores\nwhy and under which general conditions non-exchangeable network data can be\ndescribed by a stochastic block model. The new modelling framework is able to\ncapture empirically important characteristics of network data such as sparsity\ncombined with heavy tailed degree distribution, and add understanding as to\nwhat generative mechanisms will make them arise.\n  Keywords: statistical network analysis, exchangeable arrays, stochastic block\nmodel, nonlinear stochastic processes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:13:59 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wu", "Weichi", ""], ["Olhede", "Sofia", ""], ["Wolfe", "Patrick", ""]]}, {"id": "2007.14475", "submitter": "Georgios Rovatsos", "authors": "Georgios Rovatsos, George V. Moustakides, Venugopal V. Veeravalli", "title": "Quickest Detection of Moving Anomalies in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sequentially detecting a moving anomaly which affects\ndifferent parts of a sensor network with time is studied. Each network sensor\nis characterized by a non-anomalous and anomalous distribution, governing the\ngeneration of sensor data. Initially, the observations of each sensor are\ngenerated according to the corresponding non-anomalous distribution. After some\nunknown but deterministic time instant, a moving anomaly emerges, affecting\ndifferent sets of sensors as time progresses. As a result, the observations of\nthe affected sensors are generated according to the corresponding anomalous\ndistribution. Our goal is to design a stopping procedure to detect the\nemergence of the anomaly as quickly as possible, subject to constraints on the\nfrequency of false alarms. The problem is studied in a quickest change\ndetection framework where it is assumed that the evolution of the anomaly is\nunknown but deterministic. To this end, we propose a modification of Lorden's\nworst average detection delay metric to account for the trajectory of the\nanomaly that maximizes the detection delay of a candidate detection procedure.\nWe establish that a Cumulative Sum-type test solves the resulting sequential\ndetection problem exactly when the sensors are homogeneous. For the case of\nheterogeneous sensors, the proposed detection scheme can be modified to provide\na first-order asymptotically optimal algorithm. We conclude by presenting\nnumerical simulations to validate our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 20:36:58 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Rovatsos", "Georgios", ""], ["Moustakides", "George V.", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2007.14490", "submitter": "Mikayla Kelley", "authors": "Mikayla Kelley", "title": "On Accuracy and Coherence with Infinite Opinion Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a well-known equivalence between avoiding accuracy dominance and\nhaving probabilistically coherent credences (see, e.g., de Finetti 1974, Joyce\n2009, Predd et al. 2009, Schervish et al. 2009, Pettigrew 2016). However, this\nequivalence has been established only when the set of propositions on which\ncredence functions are defined is finite. In this paper, we establish\nconnections between accuracy dominance and coherence when credence functions\nare defined on an infinite set of propositions. In particular, we establish the\nnecessary results to extend the classic accuracy argument for probabilism\noriginally due to Joyce (1998) to certain classes of infinite sets of\npropositions including countably infinite partitions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:11:26 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kelley", "Mikayla", ""]]}, {"id": "2007.14539", "submitter": "Dhruv Rohatgi", "authors": "Constantinos Daskalakis, Dhruv Rohatgi, Manolis Zampetakis", "title": "Truncated Linear Regression in High Dimensions", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in standard linear regression, in truncated linear regression, we are\ngiven access to observations $(A_i, y_i)_i$ whose dependent variable equals\n$y_i= A_i^{\\rm T} \\cdot x^* + \\eta_i$, where $x^*$ is some fixed unknown vector\nof interest and $\\eta_i$ is independent noise; except we are only given an\nobservation if its dependent variable $y_i$ lies in some \"truncation set\" $S\n\\subset \\mathbb{R}$. The goal is to recover $x^*$ under some favorable\nconditions on the $A_i$'s and the noise distribution. We prove that there\nexists a computationally and statistically efficient method for recovering\n$k$-sparse $n$-dimensional vectors $x^*$ from $m$ truncated samples, which\nattains an optimal $\\ell_2$ reconstruction error of $O(\\sqrt{(k \\log n)/m})$.\nAs a corollary, our guarantees imply a computationally efficient and\ninformation-theoretically optimal algorithm for compressed sensing with\ntruncation, which may arise from measurement saturation effects. Our result\nfollows from a statistical and computational analysis of the Stochastic\nGradient Descent (SGD) algorithm for solving a natural adaptation of the LASSO\noptimization problem that accommodates truncation. This generalizes the works\nof both: (1) [Daskalakis et al. 2018], where no regularization is needed due to\nthe low-dimensionality of the data, and (2) [Wainright 2009], where the\nobjective function is simple due to the absence of truncation. In order to deal\nwith both truncation and high-dimensionality at the same time, we develop new\ntechniques that not only generalize the existing ones but we believe are of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 00:31:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Rohatgi", "Dhruv", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2007.14566", "submitter": "Quntao Zhuang", "authors": "Quntao Zhuang and Stefano Pirandola", "title": "Ultimate limits for multiple quantum channel discrimination", "comments": "6+12 pages, 6 figures", "journal-ref": "Phys. Rev. Lett. 125, 080505 (2020)", "doi": "10.1103/PhysRevLett.125.080505", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum hypothesis testing is a central task in the entire field of quantum\ninformation theory. Understanding its ultimate limits will give insight into a\nwide range of quantum protocols and applications, from sensing to\ncommunication. Although the limits of hypothesis testing between quantum states\nhave been completely clarified by the pioneering works of Helstrom in the 70s,\nthe more difficult problem of hypothesis testing with quantum channels, i.e.,\nchannel discrimination, is less understood. This is mainly due to the\ncomplications coming from the use of input entanglement and the possibility of\nemploying adaptive strategies. In this paper, we establish a lower limit for\nthe ultimate error probability affecting the discrimination of an arbitrary\nnumber of quantum channels. We also show that this lower bound is achievable\nwhen the channels have certain symmetries. As an example, we apply our results\nto the problem of channel position finding, where the goal is to identify the\nlocation of a target channel among multiple background channels. In this\ngeneral setting, we find that the use of entanglement offers a great advantage\nover strategies without entanglement, with non-trivial implications for data\nreadout, target detection and quantum spectroscopy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:08:48 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:55:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhuang", "Quntao", ""], ["Pirandola", "Stefano", ""]]}, {"id": "2007.14659", "submitter": "Manon Costa", "authors": "Bernard Bercu, Manon Costa and S\\'ebastien Gadat", "title": "Stochastic approximation algorithms for superquantiles estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to two different two-time-scale stochastic\napproximation algorithms for superquantile estimation. We shall investigate the\nasymptotic behavior of a Robbins-Monro estimator and its convexified version.\nOur main contribution is to establish the almost sure convergence, the\nquadratic strong law and the law of iterated logarithm for our estimates via a\nmartingale approach. A joint asymptotic normality is also provided. Our\ntheoretical analysis is illustrated by numerical experiments on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:09:21 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bercu", "Bernard", ""], ["Costa", "Manon", ""], ["Gadat", "S\u00e9bastien", ""]]}, {"id": "2007.14684", "submitter": "Fran\\c{c}ois Bachoc", "authors": "Fran\\c{c}ois Bachoc and Emilio Porcu and Moreno Bevilacqua and\n  Reinhard Furrer and Tarik Faouzi", "title": "Asymptotically Equivalent Prediction in Multivariate Geostatistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cokriging is the common method of spatial interpolation (best linear unbiased\nprediction) in multivariate geostatistics. While best linear prediction has\nbeen well understood in univariate spatial statistics, the literature for the\nmultivariate case has been elusive so far. The new challenges provided by\nmodern spatial datasets, being typically multivariate, call for a deeper study\nof cokriging. In particular, we deal with the problem of misspecified cokriging\nprediction within the framework of fixed domain asymptotics. Specifically, we\nprovide conditions for equivalence of measures associated with multivariate\nGaussian random fields, with index set in a compact set of a d-dimensional\nEuclidean space. Such conditions have been elusive for over about 50 years of\nspatial statistics.\n  We then focus on the multivariate Mat\\'ern and Generalized Wendland classes\nof matrix valued covariance functions, that have been very popular for having\nparameters that are crucial to spatial interpolation, and that control the mean\nsquare differentiability of the associated Gaussian process. We provide\nsufficient conditions, for equivalence of Gaussian measures, relying on the\ncovariance parameters of these two classes. This enables to identify the\nparameters that are crucial to asymptotically equivalent interpolation in\nmultivariate geostatistics. Our findings are then illustrated through\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:01:43 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Porcu", "Emilio", ""], ["Bevilacqua", "Moreno", ""], ["Furrer", "Reinhard", ""], ["Faouzi", "Tarik", ""]]}, {"id": "2007.14717", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov and Maximilien Dreveton", "title": "Almost exact recovery in noisy semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates noisy graph-based semi-supervised learning or\ncommunity detection. We consider the Stochastic Block Model (SBM), where, in\naddition to the graph observation, an oracle gives a non-perfect information\nabout some nodes' cluster assignment. We derive the Maximum A Priori (MAP)\nestimator, and show that a continuous relaxation of the MAP performs almost\nexact recovery under non-restrictive conditions on the average degree and\namount of oracle noise. In particular, this method avoids some pitfalls of\nseveral graph-based semi-supervised learning methods such as the flatness of\nthe classification functions, appearing in the problems with a very large\namount of unlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 09:56:05 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Dreveton", "Maximilien", ""]]}, {"id": "2007.14842", "submitter": "Pavlina Jordanova", "authors": "Pavlina K. Jordanova, Milan Stehl\\'i k", "title": "Distribution sensitive estimators of the index of regular variation\n  based on ratios of order statistics", "comments": null, "journal-ref": "20th International Conference - AMiTaNS '20, June 24-29, 2020,\n  Albena, Bulgaria", "doi": "10.1063/5.0033940", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ratios of central order statistics seem to be very useful for estimating the\ntail of the distributions and therefore, quantiles outside the range of the\ndata. In 1995 Isabel Fraga Alves investigated the rate of convergence of three\nsemi-parametric estimators of the parameter of the tail index in case when the\ncumulative distribution function of the observed random variable belongs to the\nmax-domain of attraction of a fixed Generalized Extreme Value Distribution.\nThey are based on ratios of specific linear transformations of two extreme\norder statistics. In 2019 we considered Pareto case and found two very simple\nand unbiased estimators of the index of regular variation. Then, using the\ncentral order statistics we showed that these estimators have many good\nproperties. Then, we observed that although the assumptions are different, one\nof them is equivalent to one of Alves's estimators. Using central order\nstatistics we proved unbiasedness, asymptotic consistency, asymptotic normality\nand asymptotic efficiency. Here we use again central order statistics and a\nparametric approach and obtain distribution sensitive estimators of the index\nof regular variation in some particular cases. Then, we find conditions which\nguarantee that these estimators are unbiased, consistent and asymptotically\nnormal. The results are depicted via simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:00:38 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Jordanova", "Pavlina K.", ""], ["k", "Milan Stehl\u00ed", ""]]}, {"id": "2007.14845", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Jeffrey W. Miller", "title": "Robust and Reproducible Model Selection Using Bagged Posteriors", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.07104", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection is premised on the assumption that the data are\ngenerated from one of the postulated models. However, in many applications, all\nof these models are incorrect, which is known as model misspecification. When\nthe models are misspecified, two or more models can provide a nearly equally\ngood fit to the data, in which case Bayesian model selection can be highly\nunstable, potentially leading to self-contradictory findings. To remedy this\ninstability, we explore instead using bagging on the posterior distribution\n(\"BayesBag\") when performing model selection -- that is, averaging the\nposterior model probabilities over many bootstrapped datasets. We provide\ntheoretical results characterizing the asymptotic behavior of the standard\nposterior and the BayesBag posterior under misspecification, in the model\nselection setting. We empirically assess the BayesBag approach on synthetic and\nreal-world data in (i) feature selection for linear regression and (ii)\nphylogenetic tree reconstruction. Our theory and experiments show that, when\nall models are misspecified, BayesBag provides (a) greater reproducibility and\n(b) greater accuracy in selecting the correct model, compared to the standard\nBayesian posterior; on the other hand, under correct specification, BayesBag is\nslightly more conservative than the standard posterior, in the sense that\nBayesBag posterior probabilities tend to be slightly farther from the extremes\nof zero and one. Overall, our results demonstrate that BayesBag provides an\neasy-to-use and widely applicable approach that improves upon standard Bayesian\nmodel selection by making it more stable and reproducible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:23:17 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 19:02:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Miller", "Jeffrey W.", ""]]}, {"id": "2007.14971", "submitter": "Maryna Prus", "authors": "Maryna Prus", "title": "Equivalence theorems for compound design problems with application in\n  mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider design criteria which depend on several\ndesigns simultaneously. We formulate equivalence theorems based on moment\nmatrices (if criteria depend on designs via moment matrices) or with respect to\nthe designs themselves (for finite design regions). We apply the obtained\noptimality conditions to the multiple-group random coefficient regression\nmodels and illustrate the results by simple examples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:31:41 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Prus", "Maryna", ""]]}, {"id": "2007.14980", "submitter": "Larissa Matos", "authors": "Christian E. Galarza, Larissa A. Matos, Victor H. Lachos", "title": "Moments of the doubly truncated selection elliptical distributions with\n  emphasis on the unified multivariate skew-$t$ distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compute doubly truncated moments for the selection\nelliptical (SE) class of distributions, which includes some multivariate\nasymmetric versions of well-known elliptical distributions, such as, the\nnormal, Student's t, slash, among others. We address the moments for doubly\ntruncated members of this family, establishing neat formulation for high order\nmoments as well as for its first two moments. We establish sufficient and\nnecessary conditions for the existence of these truncated moments. Further, we\npropose optimized methods able to deal with extreme setting of the parameters,\npartitions with almost zero volume or no truncation which are validated with a\nbrief numerical study. Finally, we present some results useful in interval\ncensoring models. All results has been particularized to the unified skew-t\n(SUT) distribution, a complex multivariate asymmetric heavy-tailed distribution\nwhich includes the extended skew-t (EST), extended skew-normal (ESN), skew-t\n(ST) and skew-normal (SN) distributions as particular and limiting cases.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:46:58 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Galarza", "Christian E.", ""], ["Matos", "Larissa A.", ""], ["Lachos", "Victor H.", ""]]}, {"id": "2007.15174", "submitter": "Sui Tang", "authors": "Fei Lu, Mauro Maggioni, Sui Tang", "title": "Learning interaction kernels in stochastic systems of interacting\n  particles from multiple trajectories", "comments": "38 pages; 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic systems of interacting particles or agents, with\ndynamics determined by an interaction kernel which only depends on pairwise\ndistances. We study the problem of inferring this interaction kernel from\nobservations of the positions of the particles, in either continuous or\ndiscrete time, along multiple independent trajectories. We introduce a\nnonparametric inference approach to this inverse problem, based on a\nregularized maximum likelihood estimator constrained to suitable hypothesis\nspaces adaptive to data. We show that a coercivity condition enables us to\ncontrol the condition number of this problem and prove the consistency of our\nestimator, and that in fact it converges at a near-optimal learning rate, equal\nto the min-max rate of $1$-dimensional non-parametric regression. In\nparticular, this rate is independent of the dimension of the state space, which\nis typically very high. We also analyze the discretization errors in the case\nof discrete-time observations, showing that it is of order $1/2$ in terms of\nthe time gaps between observations. This term, when large, dominates the\nsampling error and the approximation error, preventing convergence of the\nestimator. Finally, we exhibit an efficient parallel algorithm to construct the\nestimator from data, and we demonstrate the effectiveness of our algorithm with\nnumerical tests on prototype systems including stochastic opinion dynamics and\na Lennard-Jones model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:28:06 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Lu", "Fei", ""], ["Maggioni", "Mauro", ""], ["Tang", "Sui", ""]]}, {"id": "2007.15252", "submitter": "Jake Soloff", "authors": "Jake A. Soloff, Adityanand Guntuboyina, Michael I. Jordan", "title": "Covariance estimation with nonnegative partial correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional covariance estimation under the\nconstraint that the partial correlations are nonnegative. The sign constraints\ndramatically simplify estimation: the Gaussian maximum likelihood estimator is\nwell defined with only two observations regardless of the number of variables.\nWe analyze its performance in the setting where the dimension may be much\nlarger than the sample size. We establish that the estimator is both\nhigh-dimensionally consistent and minimax optimal in the symmetrized Stein\nloss. We also prove a negative result which shows that the sign-constraints can\nintroduce substantial bias for estimating the top eigenvalue of the covariance\nmatrix.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:24:44 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Soloff", "Jake A.", ""], ["Guntuboyina", "Adityanand", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2007.15301", "submitter": "Mathias M{\\o}rck Ljungdahl", "authors": "Mathias M{\\o}rck Ljungdahl and Mark Podolskij", "title": "Multi-dimensional parameter estimation of heavy-tailed moving averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a parametric estimation method for certain\nmulti-parameter heavy-tailed L\\'evy-driven moving averages. The theory relies\non recent multivariate central limit theorems obtained in [3] via Malliavin\ncalculus on Poisson spaces. Our minimal contrast approach is related to the\npapers [14, 15], which propose to use the marginal empirical characteristic\nfunction to estimate the one-dimensional parameter of the kernel function and\nthe stability index of the driving L\\'evy motion. We extend their work to allow\nfor a multi-parametric framework that in particular includes the important\nexamples of the linear fractional stable motion, the stable Ornstein-Uhlenbeck\nprocess, certain CARMA(2, 1) models and Ornstein-Uhlenbeck processes with a\nperiodic component among other models. We present both the consistency and the\nassociated central limit theorem of the minimal contrast estimator.\nFurthermore, we demonstrate numerical analysis to uncover the finite sample\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:26:06 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 11:39:54 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 12:18:05 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 04:45:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ljungdahl", "Mathias M\u00f8rck", ""], ["Podolskij", "Mark", ""]]}, {"id": "2007.15346", "submitter": "Asaf Weinstein", "authors": "Asaf Weinstein, Weijie J. Su, Ma{\\l}gorzata Bogdan, Rina F. Barber,\n  Emmanuel J. Cand\\`es", "title": "A Power Analysis for Knockoffs with the Lasso Coefficient-Difference\n  Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a linear model with possibly many predictors, we consider variable\nselection procedures given by $$ \\{1\\leq j\\leq p: |\\widehat{\\beta}_j(\\lambda)|\n> t\\}, $$ where $\\widehat{\\beta}(\\lambda)$ is the Lasso estimate of the\nregression coefficients, and where $\\lambda$ and $t$ may be data dependent.\nOrdinary Lasso selection is captured by using $t=0$, thus allowing to control\nonly $\\lambda$, whereas thresholded-Lasso selection allows to control both\n$\\lambda$ and $t$. The potential advantages of the latter over the former in\nterms of power---figuratively, opening up the possibility to look further down\nthe Lasso path---have been quantified recently leveraging advances in\napproximate message-passing (AMP) theory, but the implications are actionable\nonly when assuming substantial knowledge of the underlying signal.\n  In this work we study theoretically the power of a knockoffs-calibrated\ncounterpart of thresholded-Lasso that enables us to control FDR in the\nrealistic situation where no prior information about the signal is available.\nAlthough the basic AMP framework remains the same, our analysis requires a\nsignificant technical extension of existing theory in order to handle the\npairing between original variables and their knockoffs. Relying on this\nextension we obtain exact asymptotic predictions for the true positive\nproportion achievable at a prescribed type I error level. In particular, we\nshow that the knockoffs version of thresholded-Lasso can perform much better\nthan ordinary Lasso selection if $\\lambda$ is chosen by cross-validation on the\naugmented matrix.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:50:43 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Weinstein", "Asaf", ""], ["Su", "Weijie J.", ""], ["Bogdan", "Ma\u0142gorzata", ""], ["Barber", "Rina F.", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2007.15421", "submitter": "Abhirup Datta", "authors": "Arkajyoti Saha, Sumanta Basu, Abhirup Datta", "title": "Random Forests for dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n  We propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n  We establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n  We empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:36:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:10:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Saha", "Arkajyoti", ""], ["Basu", "Sumanta", ""], ["Datta", "Abhirup", ""]]}, {"id": "2007.15496", "submitter": "Daniel Hlubinka", "authors": "Marc Hallin, Daniel Hlubinka, and \\v{S}\\'arka Hudecov\\'a", "title": "Fully distribution-free center-outward rank tests for multiple-output\n  regression and MANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending rank-based inference to a multivariate setting such as\nmultiple-output regression or MANOVA with unspecified d-dimensional error\ndensity has remained an open problem for more than half a century. None of the\nmany solutions proposed so far is enjoying the combination of\ndistribution-freeness and efficiency that makes rank-based inference a\nsuccessful tool in the univariate setting. A concept of center-outward\nmultivariate ranks and signs based on measure transportation ideas has been\nintroduced recently. Center-outward ranks and signs are not only\ndistribution-free but achieve in dimension d > 1 the (essential) maximal\nancillarity property of traditional univariate ranks, hence carry all the\n\"distribution-free information\" available in the sample. We derive here the\nH\\'ajek representation and asymptotic normality results required in the\nconstruction of center-outward rank tests for multiple-output regression and\nMANOVA. When based on appropriate spherical scores, these fully\ndistribution-free tests achieve parametric efficiency in the corresponding\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:41:51 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hallin", "Marc", ""], ["Hlubinka", "Daniel", ""], ["Hudecov\u00e1", "\u0160\u00e1rka", ""]]}, {"id": "2007.15518", "submitter": "Van Ha Hoang", "authors": "Gaelle Chagny, Antoine Channarond, Van Ha Hoang and Angelina Roche", "title": "Adaptive nonparametric estimation of a component density in a two-class\n  mixture model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-class mixture model, where the density of one of the components is\nknown, is considered. We address the issue of the nonparametric adaptive\nestimation of the unknown probability density of the second component. We\npropose a randomly weighted kernel estimator with a fully data-driven bandwidth\nselection method, in the spirit of the Goldenshluger and Lepski method. An\noracle-type inequality for the pointwise quadratic risk is derived as well as\nconvergence rates over Holder smoothness classes. The theoretical results are\nillustrated by numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:16:27 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:01:53 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chagny", "Gaelle", ""], ["Channarond", "Antoine", ""], ["Hoang", "Van Ha", ""], ["Roche", "Angelina", ""]]}, {"id": "2007.15618", "submitter": "Ankit Pensia", "authors": "Ilias Diakonikolas, Daniel M. Kane, Ankit Pensia", "title": "Outlier Robust Mean Estimation with Subgaussian Rates via Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of outlier robust high-dimensional mean estimation under\na finite covariance assumption, and more broadly under finite low-degree moment\nassumptions. We consider a standard stability condition from the recent robust\nstatistics literature and prove that, except with exponentially small failure\nprobability, there exists a large fraction of the inliers satisfying this\ncondition. As a corollary, it follows that a number of recently developed\nalgorithms for robust mean estimation, including iterative filtering and\nnon-convex gradient descent, give optimal error estimators with\n(near-)subgaussian rates. Previous analyses of these algorithms gave\nsignificantly suboptimal rates. As a corollary of our approach, we obtain the\nfirst computationally efficient algorithm with subgaussian rate for\noutlier-robust mean estimation in the strong contamination model under a finite\ncovariance assumption.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:33:03 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 15:58:25 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pensia", "Ankit", ""]]}, {"id": "2007.15766", "submitter": "Wicher Bergsma", "authors": "Wicher Bergsma and Haziq Jamil", "title": "Regression modelling with I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the I-prior methodology as a unifying framework for estimating a\nvariety of regression models, including varying coefficient, multilevel,\nlongitudinal models, and models with functional covariates and responses. It\ncan also be used for multi-class classification, with low or high dimensional\ncovariates.\n  The I-prior is generally defined as a maximum entropy prior. For a regression\nfunction, the I-prior is Gaussian with covariance kernel proportional to the\nFisher information on the regression function, which is estimated by its\nposterior distribution under the I-prior. The I-prior has the intuitively\nappealing property that the more information is available on a linear\nfunctional of the regression function, the larger the prior variance, and the\nsmaller the influence of the prior mean on the posterior distribution.\n  Advantages compared to competing methods, such as Gaussian process regression\nor Tikhonov regularization, are ease of estimation and model comparison. In\nparticular, we develop an EM algorithm with a simple E and M step for\nestimating hyperparameters, facilitating estimation for complex models. We also\npropose a novel parsimonious model formulation, requiring a single scale\nparameter for each (possibly multidimensional) covariate and no further\nparameters for interaction effects. This simplifies estimation because fewer\nhyperparameters need to be estimated, and also simplifies model comparison of\nmodels with the same covariates but different interaction effects; in this\ncase, the model with the highest estimated likelihood can be selected.\n  Using a number of widely analyzed real data sets we show that predictive\nperformance of our methodology is competitive. An R-package implementing the\nmethodology is available (Jamil, 2019).\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 22:52:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:39:01 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 11:54:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bergsma", "Wicher", ""], ["Jamil", "Haziq", ""]]}, {"id": "2007.15839", "submitter": "Fred Zhang", "authors": "Samuel B. Hopkins, Jerry Li, Fred Zhang", "title": "Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret\n  Minimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mean of a distribution in high\ndimensions when either the samples are adversarially corrupted or the\ndistribution is heavy-tailed. Recent developments in robust statistics have\nestablished efficient and (near) optimal procedures for both settings. However,\nthe algorithms developed on each side tend to be sophisticated and do not\ndirectly transfer to the other, with many of them having ad-hoc or complicated\nanalyses.\n  In this paper, we provide a meta-problem and a duality theorem that lead to a\nnew unified view on robust and heavy-tailed mean estimation in high dimensions.\nWe show that the meta-problem can be solved either by a variant of the Filter\nalgorithm from the recent literature on robust estimation or by the quantum\nentropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By\nleveraging our duality theorem, these results translate into simple and\nefficient algorithms for both robust and heavy-tailed settings. Furthermore,\nthe QUE-based procedure has run-time that matches the fastest known algorithms\non both fronts.\n  Our analysis of Filter is through the classic regret bound of the\nmultiplicative weights update method. This connection allows us to avoid the\ntechnical complications in previous works and improve upon the run-time\nanalysis of a gradient-descent-based algorithm for robust mean estimation by\nCheng, Diakonikolas, Ge and Soltanolkotabi (ICML '20).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:18:32 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 00:27:26 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Li", "Jerry", ""], ["Zhang", "Fred", ""]]}, {"id": "2007.15877", "submitter": "Hang Deng", "authors": "Hang Deng", "title": "Slightly Conservative Bootstrap for Maxima of Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bootstrap for the maxima of the sums of independent random\nvariables, a problem of high relevance to many applications in modern\nstatistics. Since the consistency of bootstrap was justified by Gaussian\napproximation in Chernozhukov et al. (2013), quite a few attempts have been\nmade to sharpen the error bound for bootstrap and reduce the sample size\nrequirement for bootstrap consistency. In this paper, we show that the sample\nsize requirement can be dramatically improved when we make the inference\nslightly conservative, that is, to inflate the bootstrap quantile\n$t_{\\alpha}^*$ by a small fraction, e.g. by $1\\%$ to $1.01\\,t^*_\\alpha$. This\nsimple procedure yields error bounds for the coverage probability of\nconservative bootstrap at as fast a rate as $\\sqrt{(\\log p)/n}$ under suitable\nconditions, so that not only the sample size requirement can be reduced to\n$\\log p \\ll n$ but also the overall convergence rate is nearly parametric.\nFurthermore, we improve the error bound for the coverage probability of the\nstandard non-conservative bootstrap to $[(\\log (np))^3 (\\log p)^2/n]^{1/4}$\nunder general assumptions on data. These results are established for the\nempirical bootstrap and the multiplier bootstrap with third moment match. An\nimproved coherent Lindeberg interpolation method, originally proposed in Deng\nand Zhang (2017), is developed to derive sharper comparison bounds, especially\nfor the maxima.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:16:42 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Deng", "Hang", ""]]}, {"id": "2007.15892", "submitter": "Fran\\c{c}ois Monard", "authors": "Fran\\c{c}ois Monard, Richard Nickl, Gabriel P. Paternain", "title": "Statistical guarantees for Bayesian uncertainty quantification in\n  non-linear inverse problems with Gaussian process priors", "comments": "42 pages, 2 figures; to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference and uncertainty quantification in a general class of\nnon-linear inverse regression models is considered. Analytic conditions on the\nregression model $\\{\\mathscr G(\\theta): \\theta \\in \\Theta\\}$ and on Gaussian\nprocess priors for $\\theta$ are provided such that semi-parametrically\nefficient inference is possible for a large class of linear functionals of\n$\\theta$. A general semi-parametric Bernstein-von Mises theorem is proved that\nshows that the (non-Gaussian) posterior distributions are approximated by\ncertain Gaussian measures centred at the posterior mean. As a consequence\nposterior-based credible sets are valid and optimal from a frequentist point of\nview. The theory is illustrated with two applications with PDEs that arise in\nnon-linear tomography problems: an elliptic inverse problem for a Schr\\\"odinger\nequation, and inversion of non-Abelian X-ray transforms. New analytical\ntechniques are deployed to show that the relevant Fisher information operators\nare invertible between suitable function spaces\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 08:04:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 20:30:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Monard", "Fran\u00e7ois", ""], ["Nickl", "Richard", ""], ["Paternain", "Gabriel P.", ""]]}, {"id": "2007.15929", "submitter": "Maria Jaenada", "authors": "Elena Castilla, Abhik Ghosh, Mar\\'ia Jaenada and Leandro Pardo", "title": "On regularization methods based on R\\'enyi's pseudodistances for sparse\n  high-dimensional linear regression models", "comments": "The main paper has 36 pages, and a supplementary material is added\n  with proofs, additional results and R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several regularization methods have been considered over the last decade for\nsparse high-dimensional linear regression models, but the most common ones use\nthe least square (quadratic) or likelihood loss and hence are not robust\nagainst data contamination. Some authors have overcome the problem of\nnon-robustness by considering suitable loss function based on divergence\nmeasures (e.g., density power divergence, gamma-divergence, etc.) instead of\nthe quadratic loss. In this paper we shall consider a loss function based on\nthe R\\'enyi's pseudodistance jointly with non-concave penalties in order to\nsimultaneously perform variable selection and get robust estimators of the\nparameters in a high-dimensional linear regression model of non-polynomial\ndimensionality. The desired oracle properties of our proposed method are\nderived theoretically and its usefulness is illustustrated numerically through\nsimulations and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:48:45 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Castilla", "Elena", ""], ["Ghosh", "Abhik", ""], ["Jaenada", "Mar\u00eda", ""], ["Pardo", "Leandro", ""]]}, {"id": "2007.15930", "submitter": "Ryan Martin", "authors": "Yue Yang and Ryan Martin", "title": "Variational approximations of empirical Bayes posteriors in\n  high-dimensional linear models", "comments": "30 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensions, the prior tails can have a significant effect on both\nposterior computation and asymptotic concentration rates. To achieve optimal\nrates while keeping the posterior computations relatively simple, an empirical\nBayes approach has recently been proposed, featuring thin-tailed conjugate\npriors with data-driven centers. While conjugate priors ease some of the\ncomputational burden, Markov chain Monte Carlo methods are still needed, which\ncan be expensive when dimension is high. In this paper, we develop a\nvariational approximation to the empirical Bayes posterior that is fast to\ncompute and retains the optimal concentration rate properties of the original.\nIn simulations, our method is shown to have superior performance compared to\nexisting variational approximations in the literature across a wide range of\nhigh-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:50:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Yue", ""], ["Martin", "Ryan", ""]]}, {"id": "2007.15998", "submitter": "Louis Sharrock", "authors": "Louis Sharrock, Nikolas Kantas", "title": "Two-Timescale Stochastic Gradient Descent in Continuous Time with\n  Applications to Joint Online Parameter Estimation and Optimal Sensor\n  Placement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish the almost sure convergence of two-timescale\nstochastic gradient descent algorithms in continuous time under general noise\nand stability conditions, extending well known results in discrete time. We\nanalyse algorithms with additive noise and those with non-additive noise. In\nthe non-additive case, our analysis is carried out under the assumption that\nthe noise is a continuous-time Markov process, controlled by the algorithm\nstates. The algorithms we consider can be applied to a broad class of bilevel\noptimisation problems. We study one such problem in detail, namely, the problem\nof joint online parameter estimation and optimal sensor placement for a\npartially observed diffusion process. We demonstrate how this can be formulated\nas a bilevel optimisation problem, and propose a solution in the form of a\ncontinuous-time, two-timescale, stochastic gradient descent algorithm.\nFurthermore, under suitable conditions on the latent signal, the filter, and\nthe filter derivatives, we establish almost sure convergence of the online\nparameter estimates and optimal sensor placements to the stationary points of\nthe asymptotic log-likelihood and asymptotic filter covariance, respectively.\nWe also provide numerical examples, illustrating the application of the\nproposed methodology to a partially observed Bene\\v{s} equation, and a\npartially observed stochastic advection-diffusion equation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 12:12:28 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:05:10 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Sharrock", "Louis", ""], ["Kantas", "Nikolas", ""]]}]