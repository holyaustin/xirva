[{"id": "2107.00109", "submitter": "Qiang Sun", "authors": "Qiang Sun, Rui Mao, Wen-Xin Zhou", "title": "Adaptive Capped Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes the capped least squares regression with an adaptive\nresistance parameter, hence the name, adaptive capped least squares regression.\nThe key observation is, by taking the resistant parameter to be data dependent,\nthe proposed estimator achieves full asymptotic efficiency without losing the\nresistance property: it achieves the maximum breakdown point asymptotically.\nComputationally, we formulate the proposed regression problem as a quadratic\nmixed integer programming problem, which becomes computationally expensive when\nthe sample size gets large. The data-dependent resistant parameter, however,\nmakes the loss function more convex-like for larger-scale problems. This makes\na fast randomly initialized gradient descent algorithm possible for global\noptimization. Numerical examples indicate the superiority of the proposed\nestimator compared with classical methods. Three data applications to cancer\ncell lines, stationary background recovery in video surveillance, and blind\nimage inpainting showcase its broad applicability.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 21:28:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sun", "Qiang", ""], ["Mao", "Rui", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "2107.00118", "submitter": "Qiang Sun", "authors": "Qiang Sun", "title": "Do we need to estimate the variance in robust mean estimation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies robust mean estimators for distributions with only finite\nvariances. We propose a new loss function that is a function of the mean\nparameter and a robustification parameter. By simultaneously optimizing the\nempirical loss with respect to both parameters, we show that the resulting\nestimator for the robustification parameter can automatically adapt to the data\nand the unknown variance. Thus the resulting mean estimator can achieve\nnear-optimal finite-sample performance. Compared with prior work, our method is\ncomputationally efficient and user-friendly. It does not need cross-validation\nto tune the robustification parameter.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:00:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sun", "Qiang", ""]]}, {"id": "2107.00134", "submitter": "Tobias Boege", "authors": "Tobias Boege, Thomas Kahle, Andreas Kretschmer, Frank R\\\"ottger", "title": "The geometry of Gaussian double Markovian distributions", "comments": "29 pages. Comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian double Markovian models consist of covariance matrices constrained\nby a pair of graphs specifying zeros simultaneously in the covariance matrix\nand its inverse. We study the semi-algebraic geometry of these models, in\nparticular their dimension, smoothness and connectedness. Results on their\nvanishing ideals and conditional independence ideals are also included, and we\nput them into the general framework of conditional independence models. We end\nwith several open questions and conjectures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:42:28 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Boege", "Tobias", ""], ["Kahle", "Thomas", ""], ["Kretschmer", "Andreas", ""], ["R\u00f6ttger", "Frank", ""]]}, {"id": "2107.00146", "submitter": "Monitirtha Dey", "authors": "Monitirtha Dey", "title": "Behaviour of Familywise Error Rate in Normal Distributions", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behaviour of the familywise error rate (FWER) for\nBonferroni-type procedure in multiple testing problem. Das and Bhandari in a\nrecent article have shown that, in the equicorrelated normal setup, FWER\nasymptotically (i.e when number of hypotheses is very large) is a convex\nfunction of correlation $\\rho$ and hence an upper bound on the FWER of\nBonferroni-$\\alpha$ procedure is given by $\\alpha(1 - \\rho)$. We derive upper\nbounds on FWER for Bonferroni method under the equicorrelated and general\nnormal setups in asymptotic and non-asymptotic case. We show similar results\nfor generalized familywise error rates.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 23:07:38 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dey", "Monitirtha", ""]]}, {"id": "2107.00179", "submitter": "Hongji Wei", "authors": "T. Tony Cai and Hongji Wei", "title": "Distributed Nonparametric Function Estimation: Optimal Rate of\n  Convergence and Cost of Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed minimax estimation and distributed adaptive estimation under\ncommunication constraints for Gaussian sequence model and white noise model are\nstudied. The minimax rate of convergence for distributed estimation over a\ngiven Besov class, which serves as a benchmark for the cost of adaptation, is\nestablished. We then quantify the exact communication cost for adaptation and\nconstruct an optimally adaptive procedure for distributed estimation over a\nrange of Besov classes. The results demonstrate significant differences between\nnonparametric function estimation in the distributed setting and the\nconventional centralized setting. For global estimation, adaptation in general\ncannot be achieved for free in the distributed setting. The new technical tools\nto obtain the exact characterization for the cost of adaptation can be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:16:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "2107.00211", "submitter": "Jingbo Liu", "authors": "Jingbo Liu", "title": "A Few Interactions Improve Distributed Nonparametric Estimation,\n  Optimally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of nonparametric estimation of an unknown\n$\\beta$-H\\\"older smooth density $p_{XY}$ at a given point, where $X$ and $Y$\nare both $d$ dimensional. An infinite sequence of i.i.d.\\ samples $(X_i,Y_i)$\nare generated according to this distribution, and Alice and Bob observe $(X_i)$\nand $(Y_i)$, respectively. They are allowed to exchange $k$ bits either in\noneway or interactively in order for Bob to estimate the unknown density. For\n$\\beta\\in(0,2]$, we show that the minimax mean square risk is order\n$\\left(\\frac{k}{\\log k} \\right)^{-\\frac{2\\beta}{d+2\\beta}}$ for one-way\nprotocols and $k^{-\\frac{2\\beta}{d+2\\beta}}$ for interactive protocols. The\nlogarithmic improvement is nonexistent in the parametric counterparts, and\ntherefore can be regarded as a consequence of nonparametric nature of the\nproblem. Moreover, a few rounds of interactions achieve the interactive minimax\nrate: we show that the number of rounds can grow as slowly as the\nsuper-logarithm (i.e., inverse tetration) of $k$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 04:37:22 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Jingbo", ""]]}, {"id": "2107.00215", "submitter": "Gernot Holler", "authors": "Gernot Holler", "title": "How many samples are needed to reliably approximate the best linear\n  estimator for a linear inverse problem?", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear minimum mean squared error (LMMSE) estimator is the best linear\nestimator for a Bayesian linear inverse problem with respect to the mean\nsquared error. It arises as the solution operator to a Tikhonov-type\nregularized inverse problem with a particular quadratic discrepancy term and a\nparticular quadratic regularization operator. To be able to evaluate the LMMSE\nestimator, one must know the forward operator and the first two statistical\nmoments of both the prior and the noise. If such knowledge is not available,\none may approximate the LMMSE estimator based on given samples. In this work,\nit is investigated, in a finite-dimensional setting, how many samples are\nneeded to reliably approximate the LMMSE estimator, in the sense that, with\nhigh probability, the mean squared error of the approximation is smaller than a\ngiven multiple of the mean squared error of the LMMSE estimator.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 04:55:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Holler", "Gernot", ""]]}, {"id": "2107.00394", "submitter": "Nora L\\\"uthen", "authors": "Nora L\\\"uthen, Olivier Roustant, Fabrice Gamboa, Bertrand Iooss,\n  Stefano Marelli, and Bruno Sudret", "title": "Global sensitivity analysis using derivative-based sparse Poincar\\'e\n  chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-004", "categories": "stat.CO cs.NA math.CA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance-based global sensitivity analysis, in particular Sobol' analysis, is\nwidely used for determining the importance of input variables to a\ncomputational model. Sobol' indices can be computed cheaply based on spectral\nmethods like polynomial chaos expansions (PCE). Another choice are the recently\ndeveloped Poincar\\'e chaos expansions (PoinCE), whose orthonormal\ntensor-product basis is generated from the eigenfunctions of one-dimensional\nPoincar\\'e differential operators. In this paper, we show that the Poincar\\'e\nbasis is the unique orthonormal basis with the property that partial\nderivatives of the basis form again an orthogonal basis with respect to the\nsame measure as the original basis. This special property makes PoinCE ideally\nsuited for incorporating derivative information into the surrogate modelling\nprocess. Assuming that partial derivative evaluations of the computational\nmodel are available, we compute spectral expansions in terms of Poincar\\'e\nbasis functions or basis partial derivatives, respectively, by sparse\nregression. We show on two numerical examples that the derivative-based\nexpansions provide accurate estimates for Sobol' indices, even outperforming\nPCE in terms of bias and variance. In addition, we derive an analytical\nexpression based on the PoinCE coefficients for a second popular sensitivity\nindex, the derivative-based sensitivity measure (DGSM), and explore its\nperformance as upper bound to the corresponding total Sobol' indices.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:13:11 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 16:03:26 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["L\u00fcthen", "Nora", ""], ["Roustant", "Olivier", ""], ["Gamboa", "Fabrice", ""], ["Iooss", "Bertrand", ""], ["Marelli", "Stefano", ""], ["Sudret", "Bruno", ""]]}, {"id": "2107.00535", "submitter": "Robert Gaunt", "authors": "Robert E. Gaunt", "title": "Bounds for the chi-square approximation of the power divergence family\n  of statistics", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that each statistic in the family of power divergence of\nstatistics, across $n$ trials and $r$ classifications with index parameter\n$\\lambda\\in\\mathbb{R}$ (the Pearson, likelihood ratio and Freeman-Tukey\nstatistics correspond to $\\lambda=1,0,-1/2$, respectively) is asymptotically\nchi-square distributed as the sample size tends to infinity. In this paper, we\nobtain explicit bounds on this distributional approximation, measured using\nsmooth test functions, that hold for a given finite sample $n$, and all index\nparameters ($\\lambda>-1$) for which such finite sample bounds are meaningful.\nWe obtain bounds that are of the optimal order $n^{-1}$. The dependence of our\nbounds on the index parameter $\\lambda$ and the cell classification\nprobabilities is also optimal, and the dependence on the number of cells is\nalso respectable. Our bounds generalise, complement and improve on recent\nresults from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:18:41 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Gaunt", "Robert E.", ""]]}, {"id": "2107.00539", "submitter": "Vytaut\\.e Pilipauskait\\.e", "authors": "Denis Belomestny, Vytaut\\.e Pilipauskait\\.e, Mark Podolskij", "title": "Semiparametric estimation of McKean-Vlasov SDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of semiparametric estimation for a class\nof McKean-Vlasov stochastic differential equations. Our aim is to estimate the\ndrift coefficient of a MV-SDE based on observations of the corresponding\nparticle system. We propose a semiparametric estimation procedure and derive\nthe rates of convergence for the resulting estimator. We further prove that the\nobtained rates are essentially optimal in the minimax sense.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:29:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Belomestny", "Denis", ""], ["Pilipauskait\u0117", "Vytaut\u0117", ""], ["Podolskij", "Mark", ""]]}, {"id": "2107.00563", "submitter": "Sofiane Arradi-Alaoui", "authors": "Sofiane Arradi-Alaoui", "title": "Optimal use of auxiliary information : information geometry and\n  empirical process", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We incorporate into the empirical measure the auxiliary information given by\na finite collection of expectation in an optimal information geometry way. This\nallows to unify several methods exploiting a side information and to uniquely\ndefine an informed empirical measure. These methods are shown to share the same\nasymptotic properties. Then we study the informed empirical process subject to\na true information. We establish the Glivenko-Cantelli and Donsker theorems for\nthe informed empirical measure under minimal assumptions and we quantify the\nasymptotic uniform variance reduction. Moreover, we prove that the informed\nempirical process is more concentrated than the classical empirical process for\nall large $n$. Finally, as an illustration of the variance reduction, we apply\nsome of these results to the informed empirical quantiles.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:04:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Arradi-Alaoui", "Sofiane", ""]]}, {"id": "2107.00587", "submitter": "Alexandre Lecestre", "authors": "Alexandre Lecestre", "title": "Robust Estimation in Finite Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We observe a $n$-sample, the distribution of which is assumed to belong, or\nat least to be close enough, to a given mixture model. We propose an estimator\nof this distribution that belongs to our model and possesses some robustness\nproperties with respect to a possible misspecification of it. We establish a\nnon-asymptotic deviation bound for the Hellinger distance between the target\ndistribution and its estimator when the model consists of a mixture of\ndensities that belong to VC-subgraph classes. Under suitable assumptions and\nwhen the mixture model is well-specified, we derive risk bounds for the\nparameters of the mixture. Finally, we design a statistical procedure that\nallows us to select from the data the number of components as well as suitable\nmodels for each of the densities that are involved in the mixture. These models\nare chosen among a collection of candidate ones and we show that our selection\nrule combined with our estimation strategy result in an estimator which\nsatisfies an oracle-type inequality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 16:21:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lecestre", "Alexandre", ""]]}, {"id": "2107.00633", "submitter": "Mohamed Chaouch", "authors": "Kilani Ghoudi, Na\\^amane La\\\"ib, Mohamed Chaouch", "title": "Joint parametric specification checking of conditional mean and\n  volatility in time series models with martingale difference innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using cumulative residual processes, we propose joint goodness-of-fit tests\nfor conditional means and variances functions in the context of nonlinear time\nseries with martingale difference innovations. The main challenge comes from\nthe fact the cumulative residual process no longer admits, under the null\nhypothesis, a distribution-free limit. To obtain a practical solution one\neither transforms the process in order to achieve a distribution-free limit or\napproximates the non-distribution free limit using a numerical or a re-sampling\ntechnique. Here the three solutions will be considered.It is shown that the\nproposed tests have nontrivial power against a class of root-n local\nalternatives, and are suitable when the conditioning information set is\ninfinite-dimensional, which allows including models like autoregressive\nconditional heteroscedastic stochastic models with dependent innovations. The\napproach presented assumes only certain conditions on the first- and\nsecond-order conditional moments, without imposing any autoregression model.\nThe test procedures introduced are compared with each other and with other\ncompetitors in terms of their power using a simulation study and a real data\napplication. These simulations have shown that the statistical powers of tests\nbased on re-sampling or numerical approximation of the original statistics are\nin general slightly better than those based on a martingale transformation of\nthe original process.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:48:13 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ghoudi", "Kilani", ""], ["La\u00efb", "Na\u00e2mane", ""], ["Chaouch", "Mohamed", ""]]}, {"id": "2107.00681", "submitter": "Oliver Hines", "authors": "Oliver Hines, Oliver Dukes, Karla Diaz-Ordaz, Stijn Vansteelandt", "title": "Demystifying statistical learning based on efficient influence functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of treatment effects and more general estimands is typically\nachieved via parametric modelling, which is unsatisfactory since model\nmisspecification is likely. Data-adaptive model building (e.g.\nstatistical/machine learning) is commonly employed to reduce the risk of\nmisspecification. Naive use of such methods, however, delivers estimators whose\nbias may shrink too slowly with sample size for inferential methods to perform\nwell, including those based on the bootstrap. Bias arises because standard\ndata-adaptive methods are tuned towards minimal prediction error as opposed to\ne.g. minimal MSE in the estimator. This may cause excess variability that is\ndifficult to acknowledge, due to the complexity of such strategies.\n  Building on results from non-parametric statistics, targeted learning and\ndebiased machine learning overcome these problems by constructing estimators\nusing the estimand's efficient influence function under the non-parametric\nmodel. These increasingly popular methodologies typically assume that the\nefficient influence function is given, or that the reader is familiar with its\nderivation.\n  In this paper, we focus on derivation of the efficient influence function and\nexplain how it may be used to construct statistical/machine-learning-based\nestimators. We discuss the requisite conditions for these estimators to perform\nwell and use diverse examples to convey the broad applicability of the theory.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:11:37 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Hines", "Oliver", ""], ["Dukes", "Oliver", ""], ["Diaz-Ordaz", "Karla", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2107.01120", "submitter": "Zacharie Naulet", "authors": "Zacharie Naulet and Judith Rousseau and Fran\\c{c}ois Caron", "title": "Asymptotic Analysis of Statistical Estimators related to MultiGraphex\n  Processes under Misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the asymptotic properties of Bayesian or frequentist\nestimators of a vector of parameters related to structural properties of\nsequences of graphs. The estimators studied originate from a particular class\nof graphex model introduced by Caron and Fox. The analysis is however performed\nhere under very weak assumptions on the underlying data generating process,\nwhich may be different from the model of Caron and Fox or from a graphex model.\nIn particular, we consider generic sparse graph models, with unbounded degree,\nwhose degree distribution satisfies some assumptions. We show that one can\nrelate the limit of the estimator of one of the parameters to the sparsity\nconstant of the true graph generating process. When taking a Bayesian approach,\nwe also show that the posterior distribution is asymptotically normal. We\ndiscuss situations where classical random graphs models such as configuration\nmodels, sparse graphon models, edge exchangeable models or graphon processes\nsatisfy our assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:05:53 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Naulet", "Zacharie", ""], ["Rousseau", "Judith", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "2107.01266", "submitter": "Kan Chen", "authors": "Kan Chen, Zhiqi Bu, Shiyun Xu", "title": "Asymptotic Statistical Analysis of Sparse Group LASSO via Approximate\n  Message Passing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sparse Group LASSO (SGL) is a regularized model for high-dimensional linear\nregression problems with grouped covariates. SGL applies $l_1$ and $l_2$\npenalties on the individual predictors and group predictors, respectively, to\nguarantee sparse effects both on the inter-group and within-group levels. In\nthis paper, we apply the approximate message passing (AMP) algorithm to\nefficiently solve the SGL problem under Gaussian random designs. We further use\nthe recently developed state evolution analysis of AMP to derive an\nasymptotically exact characterization of SGL solution. This allows us to\nconduct multiple fine-grained statistical analyses of SGL, through which we\ninvestigate the effects of the group information and $\\gamma$ (proportion of\n$\\ell_1$ penalty). With the lens of various performance measures, we show that\nSGL with small $\\gamma$ benefits significantly from the group information and\ncan outperform other SGL (including LASSO) or regularized models which does not\nexploit the group information, in terms of the recovery rate of signal, false\ndiscovery rate and mean squared error.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 20:48:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chen", "Kan", ""], ["Bu", "Zhiqi", ""], ["Xu", "Shiyun", ""]]}, {"id": "2107.01305", "submitter": "Zhou Fan", "authors": "Zhou Fan and Roy R. Lederman and Yi Sun and Tianhao Wang and Sheng Xu", "title": "Maximum likelihood for high-noise group orbit estimation and\n  single-particle cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to single-particle cryo-electron microscopy\n(cryo-EM), we study several problems of function estimation in a low SNR\nregime, where samples are observed under random rotations of the function\ndomain. In a general framework of group orbit estimation with linear\nprojection, we describe a stratification of the Fisher information eigenvalues\naccording to a sequence of transcendence degrees in the invariant algebra, and\nrelate critical points of the log-likelihood landscape to a sequence of\nmethod-of-moments optimization problems. This extends previous results for a\ndiscrete rotation group without projection.\n  We then compute these transcendence degrees and the forms of these moment\noptimization problems for several examples of function estimation under $SO(2)$\nand $SO(3)$ rotations, including a simplified model of cryo-EM as introduced by\nBandeira, Blum-Smith, Kileel, Perry, Weed, and Wein. For several of these\nexamples, we affirmatively resolve numerical conjectures that\n$3^\\text{rd}$-order moments are sufficient to locally identify a generic signal\nup to its rotational orbit.\n  For low-dimensional approximations of the electric potential maps of two\nsmall protein molecules, we empirically verify that the noise-scalings of the\nFisher information eigenvalues conform with these theoretical predictions over\na range of SNR, in a model of $SO(3)$ rotations without projection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 23:45:34 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Fan", "Zhou", ""], ["Lederman", "Roy R.", ""], ["Sun", "Yi", ""], ["Wang", "Tianhao", ""], ["Xu", "Sheng", ""]]}, {"id": "2107.01306", "submitter": "Yunyi Shen", "authors": "Yunyi Shen and Claudia Solis-Lemus", "title": "The Effect of the Prior and the Experimental Design on the Inference of\n  the Precision Matrix in Gaussian Chain Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Here, we investigate whether (and how) experimental design could aid in the\nestimation of the precision matrix in a Gaussian chain graph model, especially\nthe interplay between the design, the effect of the experiment and prior\nknowledge about the effect. We approximate the marginal posterior precision of\nthe precision matrix via Laplace approximation under different priors: a flat\nprior, the conjugate prior Normal-Wishart, the unconfounded prior Normal-Matrix\nGeneralized Inverse Gaussian (MGIG) and a general independent prior. We show\nthat the approximated posterior precision is not a function of the design\nmatrix for the cases of the Normal-Wishart and flat prior, but it is for the\ncases of the Normal-MGIG and the general independent prior. However, for the\nNormal-MGIG and the general independent prior, we find a sharp upper bound on\nthe approximated posterior precision that does not involve the design matrix\nwhich translates into a bound on the information that could be extracted from a\ngiven experiment. We confirm the theoretical findings via a simulation study\ncomparing the Stein's loss difference between random versus no experiment\n(design matrix equal to zero). Our findings provide practical advice for domain\nscientists conducting experiments to decode the relationships between a\nmultidimensional response and a set of predictors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 23:57:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Shen", "Yunyi", ""], ["Solis-Lemus", "Claudia", ""]]}, {"id": "2107.01352", "submitter": "Andrzej Jarosz", "authors": "Zdzislaw Burda and Andrzej Jarosz", "title": "Cleaning large-dimensional covariance matrices for correlated samples", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.ST q-fin.PM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A non-linear shrinkage estimator of large-dimensional covariance matrices is\nderived in a setting of auto-correlated samples, thus generalizing the recent\nformula by Ledoit-P\\'{e}ch\\'{e}. The calculation is facilitated by random\nmatrix theory. The result is turned into an efficient algorithm, and an\nassociated Python library, shrinkage, with help of Ledoit-Wolf kernel\nestimation technique. An example of exponentially-decaying auto-correlations is\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 06:19:46 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Burda", "Zdzislaw", ""], ["Jarosz", "Andrzej", ""]]}, {"id": "2107.01509", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu,\n  Thodoris Lykouris, Miroslav Dud\\'ik, Robert E. Schapire", "title": "Bayesian decision-making under misspecified priors with applications to\n  meta-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling and other Bayesian sequential decision-making algorithms\nare among the most popular approaches to tackle explore/exploit trade-offs in\n(contextual) bandits. The choice of prior in these algorithms offers\nflexibility to encode domain knowledge but can also lead to poor performance\nwhen misspecified. In this paper, we demonstrate that performance degrades\ngracefully with misspecification. We prove that the expected reward accrued by\nThompson sampling (TS) with a misspecified prior differs by at most\n$\\tilde{\\mathcal{O}}(H^2 \\epsilon)$ from TS with a well specified prior, where\n$\\epsilon$ is the total-variation distance between priors and $H$ is the\nlearning horizon. Our bound does not require the prior to have any parametric\nform. For priors with bounded support, our bound is independent of the\ncardinality or structure of the action space, and we show that it is tight up\nto universal constants in the worst case.\n  Building on our sensitivity analysis, we establish generic PAC guarantees for\nalgorithms in the recently studied Bayesian meta-learning setting and derive\ncorollaries for various families of priors. Our results generalize along two\naxes: (1) they apply to a broader family of Bayesian decision-making\nalgorithms, including a Monte-Carlo implementation of the knowledge gradient\nalgorithm (KG), and (2) they apply to Bayesian POMDPs, the most general\nBayesian decision-making setting, encompassing contextual bandits as a special\ncase. Through numerical simulations, we illustrate how prior misspecification\nand the deployment of one-step look-ahead (as in KG) can impact the convergence\nof meta-learning in multi-armed and contextual bandits with structured and\ncorrelated priors.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 23:17:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Simchowitz", "Max", ""], ["Tosh", "Christopher", ""], ["Krishnamurthy", "Akshay", ""], ["Hsu", "Daniel", ""], ["Lykouris", "Thodoris", ""], ["Dud\u00edk", "Miroslav", ""], ["Schapire", "Robert E.", ""]]}, {"id": "2107.01562", "submitter": "Boris Hanin", "authors": "Boris Hanin", "title": "Random Neural Networks in the Infinite Width Limit as Gaussian Processes", "comments": "26p", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article gives a new proof that fully connected neural networks with\nrandom weights and biases converge to Gaussian processes in the regime where\nthe input dimension, output dimension, and depth are kept fixed, while the\nhidden layer widths tend to infinity. Unlike prior work, convergence is shown\nassuming only moment conditions for the distribution of weights and for quite\ngeneral non-linearities.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 07:00:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hanin", "Boris", ""]]}, {"id": "2107.01618", "submitter": "Roberto Rivera", "authors": "Roberto Rivera, Axel Cortes-Cubero, Roberto Reyes-Carranza, Wolfgang\n  Rolke", "title": "Accounting for Uncertainty When Estimating Counts Through an Average\n  Rounded to the Nearest Integer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In practice, the use of rounding is ubiquitous. Although researchers have\nlooked at the implications of rounding continuous random variables, rounding\nmay be applied to functions of discrete random variables as well. For example,\nto infer on suicide difference between two time periods, authorities may\nprovide a rounded average of deaths for each period. Suicide rates tend to be\nrelatively low around the world and such rounding may seriously affect\ninference on the change of suicide rate. In this paper, we study the scenario\nwhen a rounded to nearest integer average is used to estimate a non-negative\ndiscrete random variable. Specifically, our interest is in drawing inference on\na parameter from the pmf of Y, when we get U=n[Y/n]as a proxy for Y. The\nprobability generating function of U, E(U), and Var(U) capture the effect of\nthe coarsening of the support of Y. Also, moments and estimators of\ndistribution parameters are explored for some special cases. Under certain\nconditions, there is little impact from rounding. However, we also find\nscenarios where rounding can significantly affect statistical inference as\ndemonstrated in two applications. The simple methods we propose are able to\npartially counter rounding error effects.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 13:14:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rivera", "Roberto", ""], ["Cortes-Cubero", "Axel", ""], ["Reyes-Carranza", "Roberto", ""], ["Rolke", "Wolfgang", ""]]}, {"id": "2107.01718", "submitter": "Nabarun Deb", "authors": "Nabarun Deb, Promit Ghosal, and Bodhisattva Sen", "title": "Rates of Estimation of Optimal Transport Maps using Plug-in Estimators\n  via Barycentric Projections", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport maps between two probability distributions $\\mu$ and $\\nu$\non $\\mathbb{R}^d$ have found extensive applications in both machine learning\nand statistics. In practice, these maps need to be estimated from data sampled\naccording to $\\mu$ and $\\nu$. Plug-in estimators are perhaps most popular in\nestimating transport maps in the field of computational optimal transport. In\nthis paper, we provide a comprehensive analysis of the rates of convergences\nfor general plug-in estimators defined via barycentric projections. Our main\ncontribution is a new stability estimate for barycentric projections which\nproceeds under minimal smoothness assumptions and can be used to analyze\ngeneral plug-in estimators. We illustrate the usefulness of this stability\nestimate by first providing rates of convergence for the natural\ndiscrete-discrete and semi-discrete estimators of optimal transport maps. We\nthen use the same stability estimate to show that, under additional smoothness\nassumptions of Besov type or Sobolev type, wavelet based or kernel smoothed\nplug-in estimators respectively speed up the rates of convergence and\nsignificantly mitigate the curse of dimensionality suffered by the natural\ndiscrete-discrete/semi-discrete estimators. As a by-product of our analysis, we\nalso obtain faster rates of convergence for plug-in estimators of\n$W_2(\\mu,\\nu)$, the Wasserstein distance between $\\mu$ and $\\nu$, under the\naforementioned smoothness assumptions, thereby complementing recent results in\nChizat et al. (2020). Finally, we illustrate the applicability of our results\nin obtaining rates of convergence for Wasserstein barycenters between two\nprobability distributions and obtaining asymptotic detection thresholds for\nsome recent optimal-transport based tests of independence.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 19:50:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Deb", "Nabarun", ""], ["Ghosal", "Promit", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2107.01777", "submitter": "Shashank Singh", "authors": "Shashank Singh, Justin Khim", "title": "Statistical Theory for Imbalanced Binary Classification", "comments": "Parts of this paper have been revised from arXiv:2004.04715v2\n  [math.ST]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the vast body of statistical theory developed for binary\nclassification, few meaningful results exist for imbalanced classification, in\nwhich data are dominated by samples from one of the two classes. Existing\ntheory faces at least two main challenges. First, meaningful results must\nconsider more complex performance measures than classification accuracy. To\naddress this, we characterize a novel generalization of the Bayes-optimal\nclassifier to any performance metric computed from the confusion matrix, and we\nuse this to show how relative performance guarantees can be obtained in terms\nof the error of estimating the class probability function under uniform\n($\\mathcal{L}_\\infty$) loss. Second, as we show, optimal classification\nperformance depends on certain properties of class imbalance that have not\npreviously been formalized. Specifically, we propose a novel sub-type of class\nimbalance, which we call Uniform Class Imbalance. We analyze how Uniform Class\nImbalance influences optimal classifier performance and show that it\nnecessitates different classifier behavior than other types of class imbalance.\nWe further illustrate these two contributions in the case of $k$-nearest\nneighbor classification, for which we develop novel guarantees. Together, these\nresults provide some of the first meaningful finite-sample statistical theory\nfor imbalanced binary classification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 03:55:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Singh", "Shashank", ""], ["Khim", "Justin", ""]]}, {"id": "2107.01944", "submitter": "Adam P. Kubiak", "authors": "Adam P. Kubiak, Pawel Kawalec and Adam Kiersztyn", "title": "Neyman-Pearson Hypothesis Testing, Epistemic Reliability and Pragmatic\n  Value-Laden Asymmetric Error Risks", "comments": "Axiomathes (2021)", "journal-ref": null, "doi": "10.1007/s10516-021-09541-y", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neyman and Pearson's theory of testing hypotheses does not warrant minimal\nepistemic reliability: the feature of driving to true conclusions more often\nthan to false ones. The theory does not protect from the possible negative\neffects of the pragmatic value-laden unequal setting of error probabilities on\nthe theory's epistemic reliability. Most importantly, in the case of a negative\nimpact, no methodological adjustment is available to neutralize it, so in such\ncases, the discussed pragmatic-value-ladenness of the theory inevitably\ncompromises the goal of attaining truth.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 11:21:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Kubiak", "Adam P.", ""], ["Kawalec", "Pawel", ""], ["Kiersztyn", "Adam", ""]]}, {"id": "2107.02092", "submitter": "Stylianos E. Trevlakis Mr.", "authors": "Stylianos E. Trevlakis, Nestor D. Chatzidiamantis, and George K.\n  Karagiannidis", "title": "On the symmetric and skew-symmetric K-distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a family of four-parameter distributions that contain the\nK-distribution as special case. The family is derived as a mixture distribution\nthat uses the three-parameter reflected Gamma distribution as parental and the\ntwo-parameter Gamma distribution as prior. Properties of the proposed family\nare investigated as well; these include probability density function,\ncumulative distribution function, moments, and cumulants. The family is termed\nsymmetric K-distribution (SKD) based on its resemblance to the K-distribution\nas well as its symmetric nature. The standard form of the SKD, which often\nproves to be an adequate model, is also discussed. Moreover, an order\nstatistics analysis is provided as well as the distributions of the product and\nratio of two independent and identical SKD random variables are derived.\nFinally, a generalisation of the proposed family, which enables non-zero\nskewness values, is investigated, while both the SKD and the skew-SKD are\nproven capable of describing the complex dynamics of machine learning, Bayesian\nanalysis and other fields through simplified expressions with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:31:24 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 13:32:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Trevlakis", "Stylianos E.", ""], ["Chatzidiamantis", "Nestor D.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2107.02120", "submitter": "Sergio Brenner Miguel", "authors": "Sergio Brenner Miguel", "title": "Anisotropic spectral cut-off estimation under multiplicative measurement\n  errors", "comments": "22 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-parametric estimation of an unknown density f with support\non R+^d based on an i.i.d. sample with multiplicative measurement errors. The\nproposed fully-data driven procedure is based on the estimation of the Mellin\ntransform of the density f and a regularisation of the inverse of the Mellin\ntransform by a spectral cut-off. The upcoming bias-variance trade-off is dealt\nwith by a data-driven anisotropic choice of the cut-off parameter. In order to\ndiscuss the bias term, we consider the Mellin-Sobolev spaces which characterize\nthe regularity of the unknown density f through the decay of its Mellin\ntransform. Additionally, we show minimax-optimality over Mellin-Sobolev spaces\nof the spectral cut-off density estimator.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:24:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Miguel", "Sergio Brenner", ""]]}, {"id": "2107.02146", "submitter": "Ali Mahzarnia", "authors": "Ali Mahzarnia and Jun Song", "title": "Multivariate functional group sparse regression: functional predictor\n  selection", "comments": "The R package that is developed for this paper is available at\n  GitHub. See https://github.com/Ali-Mahzarnia/MFSGrp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose methods for functional predictor selection and the\nestimation of smooth functional coefficients simultaneously in a\nscalar-on-function regression problem under high-dimensional multivariate\nfunctional data setting. In particular, we develop two methods for functional\ngroup-sparse regression under a generic Hilbert space of infinite dimension. We\nshow the convergence of algorithms and the consistency of the estimation and\nthe selection (oracle property) under infinite-dimensional Hilbert spaces.\nSimulation studies show the effectiveness of the methods in both the selection\nand the estimation of functional coefficients. The applications to the\nfunctional magnetic resonance imaging (fMRI) reveal the regions of the human\nbrain related to ADHD and IQ.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:11:28 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 15:03:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Mahzarnia", "Ali", ""], ["Song", "Jun", ""]]}, {"id": "2107.02237", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Akshay Krishnamurthy", "title": "Efficient First-Order Contextual Bandits: Prediction, Allocation, and\n  Triangular Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurring theme in statistical learning, online learning, and beyond is\nthat faster convergence rates are possible for problems with low noise, often\nquantified by the performance of the best hypothesis; such results are known as\nfirst-order or small-loss guarantees. While first-order guarantees are\nrelatively well understood in statistical and online learning, adapting to low\nnoise in contextual bandits (and more broadly, decision making) presents major\nalgorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy,\nLangford, Luo, and Schapire asked whether first-order guarantees are even\npossible for contextual bandits and -- if so -- whether they can be attained by\nefficient algorithms. We give a resolution to this question by providing an\noptimal and efficient reduction from contextual bandits to online regression\nwith the logarithmic (or, cross-entropy) loss. Our algorithm is simple and\npractical, readily accommodates rich function classes, and requires no\ndistributional assumptions beyond realizability. In a large-scale empirical\nevaluation, we find that our approach typically outperforms comparable\nnon-first-order methods.\n  On the technical side, we show that the logarithmic loss and an\ninformation-theoretic quantity called the triangular discrimination play a\nfundamental role in obtaining first-order guarantees, and we combine this\nobservation with new refinements to the regression oracle reduction framework\nof Foster and Rakhlin. The use of triangular discrimination yields novel\nresults even for the classical statistical learning model, and we anticipate\nthat it will find broader use.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 19:20:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Foster", "Dylan J.", ""], ["Krishnamurthy", "Akshay", ""]]}, {"id": "2107.02266", "submitter": "Koulik Khamaru", "authors": "Koulik Khamaru, Yash Deshpande, Lester Mackey, Martin J. Wainwright", "title": "Near-optimal inference in adaptive linear regression", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When data is collected in an adaptive manner, even simple methods like\nordinary least squares can exhibit non-normal asymptotic behavior. As an\nundesirable consequence, hypothesis tests and confidence intervals based on\nasymptotic normality can lead to erroneous results. We propose an online\ndebiasing estimator to correct these distributional anomalies in least squares\nestimation. Our proposed method takes advantage of the covariance structure\npresent in the dataset and provides sharper estimates in directions for which\nmore information has accrued. We establish an asymptotic normality property for\nour proposed online debiasing estimator under mild conditions on the data\ncollection process, and provide asymptotically exact confidence intervals. We\nadditionally prove a minimax lower bound for the adaptive linear regression\nproblem, thereby providing a baseline by which to compare estimators. There are\nvarious conditions under which our proposed estimator achieves the minimax\nlower bound up to logarithmic factors. We demonstrate the usefulness of our\ntheory via applications to multi-armed bandit, autoregressive time series\nestimation, and active learning with exploration.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:05:11 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 02:57:42 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Khamaru", "Koulik", ""], ["Deshpande", "Yash", ""], ["Mackey", "Lester", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2107.02363", "submitter": "Andrew Davison", "authors": "Andrew Davison and Morgane Austern", "title": "Asymptotics of Network Embeddings Learned via Subsampling", "comments": "98 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are ubiquitous in modern machine learning, with tasks of\ninterest including node classification, node clustering and link prediction. A\nfrequent approach begins by learning an Euclidean embedding of the network, to\nwhich algorithms developed for vector-valued data are applied. For large\nnetworks, embeddings are learned using stochastic gradient methods where the\nsub-sampling scheme can be freely chosen. Despite the strong empirical\nperformance of such methods, they are not well understood theoretically. Our\nwork encapsulates representation methods using a subsampling approach, such as\nnode2vec, into a single unifying framework. We prove, under the assumption that\nthe graph is exchangeable, that the distribution of the learned embedding\nvectors asymptotically decouples. Moreover, we characterize the asymptotic\ndistribution and provided rates of convergence, in terms of the latent\nparameters, which includes the choice of loss function and the embedding\ndimension. This provides a theoretical foundation to understand what the\nembedding vectors represent and how well these methods perform on downstream\ntasks. Notably, we observe that typically used loss functions may lead to\nshortcomings, such as a lack of Fisher consistency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:54:53 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Davison", "Andrew", ""], ["Austern", "Morgane", ""]]}, {"id": "2107.02439", "submitter": "Amandine Dubois", "authors": "Amandine Dubois, Thomas Berrett, Cristina Butucea", "title": "Goodness-of-fit testing for H\\\"older continuous densities under local\n  differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of goodness-of-fit testing for H\\\"older continuous\ndensities under local differential privacy constraints. We study minimax\nseparation rates when only non-interactive privacy mechanisms are allowed to be\nused and when both non-interactive and sequentially interactive can be used for\nprivatisation. We propose privacy mechanisms and associated testing procedures\nwhose analysis enables us to obtain upper bounds on the minimax rates. These\nresults are complemented with lower bounds. By comparing these bounds, we show\nthat the proposed privacy mechanisms and tests are optimal up to at most a\nlogarithmic factor for several choices of $f_0$ including densities from\nuniform, normal, Beta, Cauchy, Pareto, exponential distributions. In\nparticular, we observe that the results are deteriorated in the private setting\ncompared to the non-private one. Moreover, we show that sequentially\ninteractive mechanisms improve upon the results obtained when considering only\nnon-interactive privacy mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:35:45 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Dubois", "Amandine", ""], ["Berrett", "Thomas", ""], ["Butucea", "Cristina", ""]]}, {"id": "2107.02522", "submitter": "Yudi Pawitan", "authors": "Yudi Pawitan", "title": "A nonBayesian view of Hempel's paradox of the ravens", "comments": "10 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Hempel's paradox of the ravens, seeing a red pencil is considered as\nsupporting evidence that all ravens are black. Also known as the Paradox of\nConfirmation, the paradox and its many resolutions indicate that we cannot\nunderestimate the logical and statistical elements needed in the assessment of\nevidence in support of a hypothesis. Most of the previous analyses of the\nparadox are within the Bayesian framework. These analyses and Hempel himself\ngenerally accept the paradoxical conclusion; it feels paradoxical supposedly\nbecause the amount of evidence is extremely small. Here I describe a\nnonBayesian analysis of various statistical models with an accompanying\nlikelihood-based reasoning. The analysis shows that the paradox feels\nparadoxical because there are natural models where observing a red pencil has\nno relevance to the color of ravens. In general the value of the evidence\ndepends crucially on the sampling scheme and on the assumption about the\nunderlying parameters of the relevant model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:28:50 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 09:45:42 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pawitan", "Yudi", ""]]}, {"id": "2107.02602", "submitter": "Yuan Liao", "authors": "Victor Chernozhukov, Christian Hansen, Yuan Liao, Yinchu Zhu", "title": "Inference for Low-Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies inference in linear models whose parameter of interest is\na high-dimensional matrix. We focus on the case where the high-dimensional\nmatrix parameter is well-approximated by a ``spiked low-rank matrix'' whose\nrank grows slowly compared to its dimensions and whose nonzero singular values\ndiverge to infinity. We show that this framework covers a broad class of models\nof latent-variables which can accommodate matrix completion problems, factor\nmodels, varying coefficient models, principal components analysis with missing\ndata, and heterogeneous treatment effects. For inference, we propose a new\n``rotation-debiasing\" method for product parameters initially estimated using\nnuclear norm penalization. We present general high-level results under which\nour procedure provides asymptotically normal estimators. We then present\nlow-level conditions under which we verify the high-level conditions in a\ntreatment effects example.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:24:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""], ["Liao", "Yuan", ""], ["Zhu", "Yinchu", ""]]}, {"id": "2107.02711", "submitter": "Tengyu Xu", "authors": "Tengyu Xu, Zhuoran Yang, Zhaoran Wang, Yingbin Liang", "title": "A Unified Off-Policy Evaluation Approach for General Value Function", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General Value Function (GVF) is a powerful tool to represent both the {\\em\npredictive} and {\\em retrospective} knowledge in reinforcement learning (RL).\nIn practice, often multiple interrelated GVFs need to be evaluated jointly with\npre-collected off-policy samples. In the literature, the gradient temporal\ndifference (GTD) learning method has been adopted to evaluate GVFs in the\noff-policy setting, but such an approach may suffer from a large estimation\nerror even if the function approximation class is sufficiently expressive.\nMoreover, none of the previous work have formally established the convergence\nguarantee to the ground truth GVFs under the function approximation settings.\nIn this paper, we address both issues through the lens of a class of GVFs with\ncausal filtering, which cover a wide range of RL applications such as reward\nvariance, value gradient, cost in anomaly detection, stationary distribution\ngradient, etc. We propose a new algorithm called GenTD for off-policy GVFs\nevaluation and show that GenTD learns multiple interrelated multi-dimensional\nGVFs as efficiently as a single canonical scalar value function. We further\nshow that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to\nthe ground truth GVFs as long as the function approximation power is\nsufficiently large. To our best knowledge, GenTD is the first off-policy GVF\nevaluation algorithm that has global optimality guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:20:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Xu", "Tengyu", ""], ["Yang", "Zhuoran", ""], ["Wang", "Zhaoran", ""], ["Liang", "Yingbin", ""]]}, {"id": "2107.02730", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Wenyan Gong, Qiang Sun", "title": "A provable two-stage algorithm for penalized hazards regression", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  From an optimizer's perspective, achieving the global optimum for a general\nnonconvex problem is often provably NP-hard using the classical worst-case\nanalysis. In the case of Cox's proportional hazards model, by taking its\nstatistical model structures into account, we identify local strong convexity\nnear the global optimum, motivated by which we propose to use two convex\nprograms to optimize the folded-concave penalized Cox's proportional hazards\nregression. Theoretically, we investigate the statistical and computational\ntradeoffs of the proposed algorithm and establish the strong oracle property of\nthe resulting estimators. Numerical studies and real data analysis lend further\nsupport to our algorithm and theory.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:59:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Fan", "Jianqing", ""], ["Gong", "Wenyan", ""], ["Sun", "Qiang", ""]]}, {"id": "2107.02780", "submitter": "Rahul Singh", "authors": "Anish Agarwal and Rahul Singh", "title": "Causal Inference with Corrupted Data: Measurement Error, Missing Values,\n  Discretization, and Differential Privacy", "comments": "99 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even the most carefully curated economic data sets have variables that are\nnoisy, missing, discretized, or privatized. The standard workflow for empirical\nresearch involves data cleaning followed by data analysis that typically\nignores the bias and variance consequences of data cleaning. We formulate a\nsemiparametric model for causal inference with corrupted data to encompass both\ndata cleaning and data analysis. We propose a new end-to-end procedure for data\ncleaning, estimation, and inference with data cleaning-adjusted confidence\nintervals. We prove root-n consistency, Gaussian approximation, and\nsemiparametric efficiency for our estimator of the causal parameter by finite\nsample arguments. Our key assumption is that the true covariates are\napproximately low rank. In our analysis, we provide nonasymptotic theoretical\ncontributions to matrix completion, statistical learning, and semiparametric\nstatistics. We verify the coverage of the data cleaning-adjusted confidence\nintervals in simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:42:49 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Agarwal", "Anish", ""], ["Singh", "Rahul", ""]]}, {"id": "2107.02849", "submitter": "Xinran Li", "authors": "Xinran Li and Dylan S. Small", "title": "Randomization-based Test for Censored Outcomes: A New Look at the\n  Logrank Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample tests have been one of the most classical topics in statistics\nwith wide application even in cutting edge applications. There are at least two\nmodes of inference used to justify the two-sample tests. One is usual\nsuperpopulation inference assuming the units are independent and identically\ndistributed (i.i.d.) samples from some superpopulation; the other is finite\npopulation inference that relies on the random assignments of units into\ndifferent groups. When randomization is actually implemented, the latter has\nthe advantage of avoiding distributional assumptions on the outcomes. In this\npaper, we will focus on finite population inference for censored outcomes,\nwhich has been less explored in the literature. Moreover, we allow the\ncensoring time to depend on treatment assignment, under which exact permutation\ninference is unachievable. We find that, surprisingly, the usual logrank test\ncan also be justified by randomization. Specifically, under a Bernoulli\nrandomized experiment with non-informative i.i.d. censoring within each\ntreatment arm, the logrank test is asymptotically valid for testing Fisher's\nnull hypothesis of no treatment effect on any unit. Moreover, the asymptotic\nvalidity of the logrank test does not require any distributional assumption on\nthe potential event times. We further extend the theory to the stratified\nlogrank test, which is useful for randomized blocked designs and when censoring\nmechanisms vary across strata. In sum, the developed theory for the logrank\ntest from finite population inference supplements its classical theory from\nusual superpopulation inference, and helps provide a broader justification for\nthe logrank test.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 19:29:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Xinran", ""], ["Small", "Dylan S.", ""]]}, {"id": "2107.02891", "submitter": "Alexis Rosuel", "authors": "Alexis Rosuel (LIGM), Philippe Loubaton (LIGM), Pascal Vallet (IMS)", "title": "On the asymptotic distribution of the maximum sample spectral coherence\n  of Gaussian time series in the high dimensional regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic distribution of the maximum of a frequency\nsmoothed estimate of the spectral coherence of a M-variate complex Gaussian\ntime series with mutually independent components when the dimension M and the\nnumber of samples N both converge to infinity. If B denotes the smoothing span\nof the underlying smoothed periodogram estimator, a type I extreme value\nlimiting distribution is obtained under the rate assumptions M N $\\rightarrow$\n0 and M B $\\rightarrow$ c $\\in$ (0, +$\\infty$). This result is then exploited\nto build a statistic with controlled asymptotic level for testing independence\nbetween the M components of the observed time series. Numerical simulations\nsupport our results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:07:49 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rosuel", "Alexis", "", "LIGM"], ["Loubaton", "Philippe", "", "LIGM"], ["Vallet", "Pascal", "", "IMS"]]}, {"id": "2107.02903", "submitter": "Harsh Tripathi", "authors": "Harsh Tripathi, Mahendra Saha", "title": "An application of time truncated single acceptance sampling inspection\n  plan based on transmuted Rayleigh distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce single acceptance sampling inspection plan\n(SASIP) for transmuted Rayleigh (TR) distribution when the lifetime experiment\nis truncated at a prefixed time. Establish the proposed plan for different\nchoices of confidence level, acceptance number and ratio of true mean lifetime\nto specified mean lifetime. Minimum sample size necessary to ensure a certain\nspecified lifetime is obtained. Operating characteristic(OC) values and\nproducer's risk of proposed plan are presented. Two real life example has been\npresented to show the applicability of proposed SASIP.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 18:24:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Tripathi", "Harsh", ""], ["Saha", "Mahendra", ""]]}, {"id": "2107.02947", "submitter": "Mark Rubin", "authors": "Mark Rubin", "title": "When to adjust alpha during multiple testing: A consideration of\n  disjunction, conjunction, and individual testing", "comments": "Synthese (2021)", "journal-ref": null, "doi": "10.1007/s11229-021-03276-4", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scientists often adjust their significance threshold (alpha level) during\nnull hypothesis significance testing in order to take into account multiple\ntesting and multiple comparisons. This alpha adjustment has become particularly\nrelevant in the context of the replication crisis in science. The present\narticle considers the conditions in which this alpha adjustment is appropriate\nand the conditions in which it is inappropriate. A distinction is drawn between\nthree types of multiple testing: disjunction testing, conjunction testing, and\nindividual testing. It is argued that alpha adjustment is only appropriate in\nthe case of disjunction testing, in which at least one test result must be\nsignificant in order to reject the associated joint null hypothesis. Alpha\nadjustment is inappropriate in the case of conjunction testing, in which all\nrelevant results must be significant in order to reject the joint null\nhypothesis. Alpha adjustment is also inappropriate in the case of individual\ntesting, in which each individual result must be significant in order to reject\neach associated individual null hypothesis. The conditions under which each of\nthese three types of multiple testing is warranted are examined. It is\nconcluded that researchers should not automatically (mindlessly) assume that\nalpha adjustment is necessary during multiple testing. Illustrations are\nprovided in relation to joint studywise hypotheses and joint multiway ANOVAwise\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 23:46:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rubin", "Mark", ""]]}, {"id": "2107.02999", "submitter": "Zeyu Wu", "authors": "Zeyu Wu, Cheng Wang, Weidong Liu", "title": "High dimensional precision matrix estimation under weak sparsity", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we estimate the high dimensional precision matrix under the\nweak sparsity condition where many entries are nearly zero. We study a\nLasso-type method for high dimensional precision matrix estimation and derive\ngeneral error bounds under the weak sparsity condition. The common\nirrepresentable condition is relaxed and the results are applicable to the weak\nsparse matrix. As applications, we study the precision matrix estimation for\nthe heavy-tailed data, the non-paranormal data, and the matrix data with the\nLasso-type method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 03:41:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wu", "Zeyu", ""], ["Wang", "Cheng", ""], ["Liu", "Weidong", ""]]}, {"id": "2107.03041", "submitter": "Annika Betken", "authors": "Annika Betken and Herold Dehling", "title": "Distance correlation for long-range dependent time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the concept of distance correlation for testing independence of\nlong-range dependent time series. For this, we establish a non-central limit\ntheorem for stochastic processes with values in an $L_2$-Hilbert space. This\nlimit theorem is of a general theoretical interest that goes beyond the context\nof this article. For the purpose of this article, it provides the basis for\nderiving the asymptotic distribution of the distance covariance of subordinated\nGaussian processes. Depending on the dependence in the data, the\nstandardization and the limit of distance correlation vary. In any case, the\nlimit is not feasible, such that test decisions are based on a subsampling\nprocedure. We prove the validity of the subsampling procedure and assess the\nfinite sample performance of a hypothesis test based on the distance\ncovariance. In particular, we compare its finite sample performance to that of\na test based on Pearson's sample correlation coefficient. For this purpose, we\nadditionally establish convergence results for this dependence measure.\nDifferent dependencies between the vectors are considered. It turns out that\nonly linear correlation is better detected by Pearson's sample correlation\ncoefficient, while all other dependencies are better detected by distance\ncorrelation. An analysis with regard to cross-dependencies between the mean\nmonthly discharges of three different rivers provides an application of the\ntheoretical results established in this article.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 06:57:04 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Betken", "Annika", ""], ["Dehling", "Herold", ""]]}, {"id": "2107.03162", "submitter": "Muneya Matsui", "authors": "Muneya Matsui, Thomas Mikosch, Rasool Roozegar and Laleh Tafakori", "title": "Distance covariance for random fields", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an independence test based on distance correlation for random fields\n$(X,Y)$. We consider the situations when $(X,Y)$ is observed on a lattice with\nequidistant grid sizes and when $(X,Y)$ is observed at random locations. We\nprovide asymptotic theory for the sample distance correlation in both\nsituations and show bootstrap consistency. The latter fact allows one to build\na test for independence of $X$ and $Y$ based on the considered discretizations\nof these fields. We illustrate the performance of the bootstrap test in a\nsimulation study involving fractional Brownian and infinite variance stable\nfields. The independence test is applied to Japanese meteorological data, which\nare observed over the entire area of Japan.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:41:16 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:47:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Matsui", "Muneya", ""], ["Mikosch", "Thomas", ""], ["Roozegar", "Rasool", ""], ["Tafakori", "Laleh", ""]]}, {"id": "2107.03183", "submitter": "Kaspar Thommen", "authors": "Kaspar Thommen", "title": "A Closed-Form Approximation to the Conjugate Prior of the Dirichlet and\n  Beta Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the conjugate prior of the Dirichlet and beta distributions and\nexplore it with numerical examples to gain an intuitive understanding of the\ndistribution itself, its hyperparameters, and conditions concerning its\nconvergence. Due to the prior's intractability, we proceed to define and\nanalyze a closed-form approximation. Finally, we provide an algorithm\nimplementing this approximation that enables fully tractable Bayesian conjugate\ntreatment of Dirichlet and beta likelihoods without the need for Monte Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:32:29 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Thommen", "Kaspar", ""]]}, {"id": "2107.03245", "submitter": "Hajo Holzmann", "authors": "Philipp Hermann and Hajo Holzmann", "title": "Bounded support in linear random coefficient models: Identification and\n  variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider linear random coefficient regression models, where the regressors\nare allowed to have a finite support. First, we investigate identifiability,\nand show that the means and the variances and covariances of the random\ncoefficients are identified from the first two conditional moments of the\nresponse given the covariates if the support of the covariates, excluding the\nintercept, contains a Cartesian product with at least three points in each\ncoordinate. Next we show the variable selection consistency of the adaptive\nLASSO for the variances and covariances of the random coefficients in finite\nand moderately high dimensions. This implies that the estimated covariance\nmatrix will actually be positive semidefinite and hence a valid covariance\nmatrix, in contrast to the estimate arising from a simple least squares fit. We\nillustrate the proposed method in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:21:55 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hermann", "Philipp", ""], ["Holzmann", "Hajo", ""]]}, {"id": "2107.03494", "submitter": "Iain Carmichael", "authors": "Iain Carmichael", "title": "The folded concave Laplacian spectral penalty learns block diagonal\n  sparsity patterns with the strong oracle property", "comments": "First draft! 60 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured sparsity is an important part of the modern statistical toolkit.\nWe say set of model parameters has block diagonal sparsity up to permutations\nif its elements can be viewed as the edges of a graph that has multiple\nconnected components. For example, a block diagonal correlation matrix with K\nblocks of variables corresponds to a graph with K connected components whose\nnodes are the variables and whose edges are the correlations. This type of\nsparsity captures clusters of model parameters. To learn block diagonal\nsparsity patterns we develop the folded concave Laplacian spectral penalty and\nprovide a majorization-minimization algorithm for the resulting non-convex\nproblem. We show this algorithm has the appealing computational and statistical\nguarantee of converging to the oracle estimator after two steps with high\nprobability, even in high-dimensional settings. The theory is then demonstrated\nin several classical problems including covariance estimation, linear\nregression, and logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:45:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Carmichael", "Iain", ""]]}, {"id": "2107.03684", "submitter": "Suzanne Sigalla", "authors": "Olga Klopp (CREST), Maxim Panov (Skoltech), Suzanne Sigalla (CREST),\n  Alexandre Tsybakov (CREST)", "title": "Assigning Topics to Documents by Successive Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models provide a useful tool to organize and understand the structure\nof large corpora of text documents, in particular, to discover hidden thematic\nstructure. Clustering documents from big unstructured corpora into topics is an\nimportant task in various areas, such as image analysis, e-commerce, social\nnetworks, population genetics. A common approach to topic modeling is to\nassociate each topic with a probability distribution on the dictionary of words\nand to consider each document as a mixture of topics. Since the number of\ntopics is typically substantially smaller than the size of the corpus and of\nthe dictionary, the methods of topic modeling can lead to a dramatic dimension\nreduction. In this paper, we study the problem of estimating topics\ndistribution for each document in the given corpus, that is, we focus on the\nclustering aspect of the problem. We introduce an algorithm that we call\nSuccessive Projection Overlapping Clustering (SPOC) inspired by the Successive\nProjection Algorithm for separable matrix factorization. This algorithm is\nsimple to implement and computationally fast. We establish theoretical\nguarantees on the performance of the SPOC algorithm, in particular, near\nmatching minimax upper and lower bounds on its estimation risk. We also propose\na new method that estimates the number of topics. We complement our theoretical\nresults with a numerical study on synthetic and semi-synthetic data to analyze\nthe performance of this new algorithm in practice. One of the conclusions is\nthat the error of the algorithm grows at most logarithmically with the size of\nthe dictionary, in contrast to what one observes for Latent Dirichlet\nAllocation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 08:58:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Klopp", "Olga", "", "CREST"], ["Panov", "Maxim", "", "Skoltech"], ["Sigalla", "Suzanne", "", "CREST"], ["Tsybakov", "Alexandre", "", "CREST"]]}, {"id": "2107.03772", "submitter": "Sylvia Wenmackers", "authors": "Leander Vignero and Sylvia Wenmackers", "title": "Degrees of riskiness, falsifiability, and truthlikeness. A neo-Popperian\n  account applicable to probabilistic theories", "comments": "41 pages; 3 figures; accepted for publication in Synthese", "journal-ref": null, "doi": "10.1007/s11229-021-03310-5", "report-no": null, "categories": "physics.hist-ph cs.AI cs.IT math.IT math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we take a fresh look at three Popperian concepts: riskiness,\nfalsifiability, and truthlikeness (or verisimilitude) of scientific hypotheses\nor theories. First, we make explicit the dimensions that underlie the notion of\nriskiness. Secondly, we examine if and how degrees of falsifiability can be\ndefined, and how they are related to various dimensions of the concept of\nriskiness as well as the experimental context. Thirdly, we consider the\nrelation of riskiness to (expected degrees of) truthlikeness. Throughout, we\npay special attention to probabilistic theories and we offer a tentative,\nquantitative account of verisimilitude for probabilistic theories.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:36:50 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Vignero", "Leander", ""], ["Wenmackers", "Sylvia", ""]]}, {"id": "2107.03826", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec, Yiwei Shen, Cun-Hui Zhang", "title": "Asymptotic normality of robust $M$-estimators with convex penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops asymptotic normality results for individual coordinates\nof robust M-estimators with convex penalty in high-dimensions, where the\ndimension $p$ is at most of the same order as the sample size $n$, i.e,\n$p/n\\le\\gamma$ for some fixed constant $\\gamma>0$. The asymptotic normality\nrequires a bias correction and holds for most coordinates of the M-estimator\nfor a large class of loss functions including the Huber loss and its smoothed\nversions regularized with a strongly convex penalty.\n  The asymptotic variance that characterizes the width of the resulting\nconfidence intervals is estimated with data-driven quantities. This estimate of\nthe variance adapts automatically to low ($p/n\\to0)$ or high ($p/n \\le \\gamma$)\ndimensions and does not involve the proximal operators seen in previous works\non asymptotic normality of M-estimators. For the Huber loss, the estimated\nvariance has a simple expression involving an effective degrees-of-freedom as\nwell as an effective sample size. The case of the Huber loss with Elastic-Net\npenalty is studied in details and a simulation study confirms the theoretical\nfindings. The asymptotic normality results follow from Stein formulae for\nhigh-dimensional random vectors on the sphere developed in the paper which are\nof independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:12:46 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bellec", "Pierre C", ""], ["Shen", "Yiwei", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2107.03848", "submitter": "Brijesh Kumar Jha", "authors": "Brijesh Kumar Jha and Ajaya Kumar Mahapatra and Suchandan Kayal", "title": "Inadmissibility Results for the Selected Hazard Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let us consider $k ~(\\ge 2)$ independent populations $\\Pi_1, \\ldots,\\Pi_k$,\nwhere $\\Pi_i$ follows exponential distribution with hazard rate ${\\sigma_i},$\n($i = 1,\\ldots,k$). Suppose $Y_{i1},\\ldots, Y_{in}$ be a random sample of size\n$n$ drawn from the $i$th population $\\Pi_i$, where $i = 1,\\ldots,k.$ For $i =\n1,\\ldots,k$, consider $Y_i=\\sum_{j=1}^nY_{ij}$. The natural selection rule is\nto select a population associated with the largest sample mean. That is,\n$\\Pi_i$, ($i = 1,\\ldots,k$) is selected if $Y_i=\\max(Y_1,\\ldots,Y_k)$. Based on\nthis selection rule, a population is chosen. Then, we consider the estimation\nof the hazard rate of the selected population with respect to the entropy loss\nfunction. Some natural estimators are proposed. The minimaxity of a natural\nestimator is established. Improved estimators improving upon the natural\nestimators are derived. Finally, numerical study is carried out in order to\ncompare the proposed estimators in terms of the risk values.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:59:48 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Jha", "Brijesh Kumar", ""], ["Mahapatra", "Ajaya Kumar", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2107.03940", "submitter": "Yann Issartel", "authors": "Cristina Butucea and Yann Issartel", "title": "Locally differentially private estimation of nonlinear functionals of\n  discrete distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating non-linear functionals of discrete\ndistributions in the context of local differential privacy. The initial data\n$x_1,\\ldots,x_n \\in [K]$ are supposed i.i.d. and distributed according to an\nunknown discrete distribution $p = (p_1,\\ldots,p_K)$. Only $\\alpha$-locally\ndifferentially private (LDP) samples $z_1,...,z_n$ are publicly available,\nwhere the term 'local' means that each $z_i$ is produced using one individual\nattribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.\nthey are allowed to use already published confidential data) or\nnon-interactive. We describe the behavior of the quadratic risk for estimating\nthe power sum functional $F_{\\gamma} = \\sum_{k=1}^K p_k^{\\gamma}$, $\\gamma >0$\nas a function of $K, \\, n$ and $\\alpha$. In the non-interactive case, we study\ntwo plug-in type estimators of $F_{\\gamma}$, for all $\\gamma >0$, that are\nsimilar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.\nHowever, due to the privacy constraint the rates we attain are slower and\nsimilar to those obtained in the Gaussian model by Collier et al. (2020). In\nthe interactive case, we introduce for all $\\gamma >1$ a two-step procedure\nwhich attains the faster parametric rate $(n \\alpha^2)^{-1/2}$ when $\\gamma\n\\geq 2$. We give lower bounds results over all $\\alpha$-LDP mechanisms and all\nestimators using the private samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:11:10 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Butucea", "Cristina", ""], ["Issartel", "Yann", ""]]}, {"id": "2107.03955", "submitter": "Felix Biggs", "authors": "Felix Biggs, Benjamin Guedj", "title": "On Margins and Derandomisation in PAC-Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop a framework for derandomising PAC-Bayesian generalisation bounds\nachieving a margin on training data, relating this process to the\nconcentration-of-measure phenomenon. We apply these tools to linear prediction,\nsingle-hidden-layer neural networks with an unusual erf activation function,\nand deep ReLU networks, obtaining new bounds. The approach is also extended to\nthe idea of \"partial-derandomisation\" where only some layers are derandomised\nand the others are stochastic. This allows empirical evaluation of\nsingle-hidden-layer networks on more complex datasets, and helps bridge the gap\nbetween generalisation bounds for non-stochastic deep networks and those for\nrandomised deep networks as generally examined in PAC-Bayes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:30:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Biggs", "Felix", ""], ["Guedj", "Benjamin", ""]]}, {"id": "2107.04136", "submitter": "Rebecca E. Morrison", "authors": "Rebecca E Morrison, Ricardo Baptista, Estelle L Basor", "title": "Diagonal Nonlinear Transformations Preserve Structure in Covariance and\n  Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a multivariate normal distribution, the sparsity of the covariance and\nprecision matrices encodes complete information about independence and\nconditional independence properties. For general distributions, the covariance\nand precision matrices reveal correlations and so-called partial correlations\nbetween variables, but these do not, in general, have any correspondence with\nrespect to independence properties. In this paper, we prove that, for a certain\nclass of non-Gaussian distributions, these correspondences still hold, exactly\nfor the covariance and approximately for the precision. The distributions --\nsometimes referred to as \"nonparanormal\" -- are given by diagonal\ntransformations of multivariate normal random variables. We provide several\nanalytic and numerical examples illustrating these results.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:31:48 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Morrison", "Rebecca E", ""], ["Baptista", "Ricardo", ""], ["Basor", "Estelle L", ""]]}, {"id": "2107.04230", "submitter": "Aaid Algahtani", "authors": "Aaid Algahtani and Vic Patrangenaru", "title": "Two Sample Test for Extrinsic Antimeans on Planar Kendall Shape Spaces\n  with an Application to Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper one develops nonparametric inference procedures for comparing\ntwo extrinsic antimeans on compact manifolds. Based on recent Central limit\ntheorems for extrinsic sample antimeans w.r.t. an arbitrary embedding of a\ncompact manifold in a Euclidean space, one derives an asymptotic chi square\ntest for the equality of two extrinsic antimeans. Applications are given to\ndistributions on complex projective space $CP^{k-2}$ w.r.t. the\nVeronese-Whitney embedding, that is a submanifold representation for the\nKendall planar shape space. Two medical imaging analysis applications are also\ngiven.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:09:03 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 04:51:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Algahtani", "Aaid", ""], ["Patrangenaru", "Vic", ""]]}, {"id": "2107.04516", "submitter": "Lisa Nicklasson", "authors": "Christiane G\\\"orgen, Aida Maraj, Lisa Nicklasson", "title": "Staged tree models with toric structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A staged tree model is a discrete statistical model encoding relationships\nbetween events. These models are realised by directed trees with coloured\nvertices. In algebro-geometric terms, the model consists of points inside a\ntoric variety. For certain trees, called balanced, the model is in fact the\nintersection of the toric variety and the probability simplex. This gives the\nmodel a straightforward description, and has computational advantages. In this\npaper we show that the class of staged tree models with a toric structure\nextends far outside of the balanced case, if we allow a change of coordinates.\nIt is an open problem whether all staged tree models have toric structure.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:03:33 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 15:17:17 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["G\u00f6rgen", "Christiane", ""], ["Maraj", "Aida", ""], ["Nicklasson", "Lisa", ""]]}, {"id": "2107.04542", "submitter": "Tim Weninger PhD", "authors": "Justus Hibshman and Tim Weninger", "title": "Higher Order Imprecise Probabilities and Statistical Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We generalize standard credal set models for imprecise probabilities to\ninclude higher order credal sets -- confidences about confidences. In doing so,\nwe specify how an agent's higher order confidences (credal sets) update upon\nobserving an event. Our model begins to address standard issues with imprecise\nprobability models, like Dilation and Belief Inertia. We conjecture that when\nhigher order credal sets contain all possible probability functions, then in\nthe limiting case the highest order confidences converge to form a uniform\ndistribution over the first order credal set, where we define uniformity in\nterms of the statistical distance metric (total variation distance). Finite\nsimulation supports the conjecture. We further suggest that this convergence\npresents the total-variation-uniform distribution as a natural, privileged\nprior for statistical hypothesis testing.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:58:43 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 15:50:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hibshman", "Justus", ""], ["Weninger", "Tim", ""]]}, {"id": "2107.04592", "submitter": "Qizhu Liang", "authors": "Qizhu Liang, Jie Xiong and Xingqiu Zhao", "title": "Statistical Estimation and Nonlinear Filtering in Environmental\n  Pollution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper studies a nonlinear filtering problem over an infinite time\ninterval. The signal to be estimated is driven by a stochastic partial\ndifferential equation involves unknown parameters. Based on discrete\nobservation, strongly consistent estimators of the parameters are derived at\nfirst. With the optimal filter given by Bayes formula, the uniqueness of\ninvariant measure for the signal-filter pair has been verified. The paper then\nestablishes approximation to the optimal filter, showing that the pathwise\naverage distance, per unit time, of the computed approximating filter from the\noptimal filter converges to zero in probability. Simulation results are\npresented at last.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 01:44:18 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liang", "Qizhu", ""], ["Xiong", "Jie", ""], ["Zhao", "Xingqiu", ""]]}, {"id": "2107.04620", "submitter": "Sihang Jiang", "authors": "Sihang Jiang", "title": "Relative Performance of Fisher Information in Interval Estimation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum likelihood estimates and corresponding confidence regions of the\nestimates are commonly used in statistical inference. In practice, people often\nconstruct approximate confidence regions with the Fisher information at given\nsample data based on the asymptotic normal distribution of the MLE (maximum\nlikelihood estimate). Two common Fisher information matrices (FIMs, for\nmultivariate parameters) are the observed FIM (the Hessian matrix of negative\nlog-likelihood function) and the expected FIM (the expectation of the observed\nFIM). In this article, we prove that under certain conditions and with an MSE\n(mean-squared error) criterion, approximate confidence interval of each element\nof the MLE with the expected FIM is at least as accurate as that with the\nobserved FIM.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 18:24:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jiang", "Sihang", ""]]}, {"id": "2107.04632", "submitter": "Mart\\'i Pedemonte", "authors": "Mart\\'i Pedemonte, Jordi Vitri\\`a and \\'Alvaro Parafita (Universitat\n  de Barcelona)", "title": "Algorithmic Causal Effect Identification with causaleffect", "comments": "40 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our evolution as a species made a huge step forward when we understood the\nrelationships between causes and effects. These associations may be trivial for\nsome events, but they are not in complex scenarios. To rigorously prove that\nsome occurrences are caused by others, causal theory and causal inference were\nformalized, introducing the $do$-operator and its associated rules. The main\ngoal of this report is to review and implement in Python some algorithms to\ncompute conditional and non-conditional causal queries from observational data.\nTo this end, we first present some basic background knowledge on probability\nand graph theory, before introducing important results on causal theory, used\nin the construction of the algorithms. We then thoroughly study the\nidentification algorithms presented by Shpitser and Pearl in 2006, explaining\nour implementation in Python alongside. The main identification algorithm can\nbe seen as a repeated application of the rules of $do$-calculus, and it\neventually either returns an expression for the causal query from experimental\nprobabilities or fails to identify the causal effect, in which case the effect\nis non-identifiable. We introduce our newly developed Python library and give\nsome usage examples.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:00:33 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pedemonte", "Mart\u00ed", "", "Universitat\n  de Barcelona"], ["Vitri\u00e0", "Jordi", "", "Universitat\n  de Barcelona"], ["Parafita", "\u00c1lvaro", "", "Universitat\n  de Barcelona"]]}, {"id": "2107.04668", "submitter": "Ruda Zhang", "authors": "Ruda Zhang and Simon Mak and David Dunson", "title": "Gaussian Process Subspace Regression for Model Reduction", "comments": "20 pages, 4 figures; with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace-valued functions arise in a wide range of problems, including\nparametric reduced order modeling (PROM). In PROM, each parameter point can be\nassociated with a subspace, which is used for Petrov-Galerkin projections of\nlarge system matrices. Previous efforts to approximate such functions use\ninterpolations on manifolds, which can be inaccurate and slow. To tackle this,\nwe propose a novel Bayesian nonparametric model for subspace prediction: the\nGaussian Process Subspace regression (GPS) model. This method is extrinsic and\nintrinsic at the same time: with multivariate Gaussian distributions on the\nEuclidean space, it induces a joint probability model on the Grassmann\nmanifold, the set of fixed-dimensional subspaces. The GPS adopts a simple yet\ngeneral correlation structure, and a principled approach for model selection.\nIts predictive distribution admits an analytical form, which allows for\nefficient subspace prediction over the parameter space. For PROM, the GPS\nprovides a probabilistic prediction at a new parameter point that retains the\naccuracy of local reduced models, at a computational complexity that does not\ndepend on system dimension, and thus is suitable for online computation. We\ngive four numerical examples to compare our method to subspace interpolation,\nas well as two methods that interpolate local reduced models. Overall, GPS is\nthe most data efficient, more computationally efficient than subspace\ninterpolation, and gives smooth predictions with uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:41:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Ruda", ""], ["Mak", "Simon", ""], ["Dunson", "David", ""]]}, {"id": "2107.04845", "submitter": "Marko Obradovi\\'c", "authors": "Wiktor Ejsmont, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "A test for normality and independence based on characteristic function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we prove a generalization of the Ejsmont characterization of\nthe multivariate normal distribution. Based on it, we propose a new test for\nindependence and normality. The test uses an integral of the squared modulus of\nthe difference between the product of empirical characteristic functions and\nsome constant. Special attention is given to the case of testing univariate\nnormality in which we derive the test statistic explicitly in terms of Bessel\nfunction, and the case of testing bivariate normality and independence. The\ntests show quality performance in comparison to some popular powerful\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 14:13:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ejsmont", "Wiktor", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "2107.04873", "submitter": "Salil Koner", "authors": "Salil Koner, Jonathan P Williams", "title": "The EAS approach to variable selection for multivariate response data in\n  high-dimensional settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the epsilon admissible subsets (EAS) model selection\napproach, from its original construction in the high-dimensional linear\nregression setting, to an EAS framework for performing group variable selection\nin the high-dimensional multivariate regression setting. Assuming a\nmatrix-Normal linear model we show that the EAS strategy is asymptotically\nconsistent if there exists a sparse, true data generating set of predictors.\nNonetheless, our EAS strategy is designed to estimate a posterior-like,\ngeneralized fiducial distribution over a parsimonious class of models in the\nsetting of correlated predictors and/or in the absence of a sparsity\nassumption. The effectiveness of our approach, to this end, is demonstrated\nempirically in simulation studies, and is compared to other state-of-the-art\nmodel/variable selection procedures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 16:47:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Koner", "Salil", ""], ["Williams", "Jonathan P", ""]]}, {"id": "2107.04907", "submitter": "Jian Huang", "authors": "Guohao Shen, Yuling Jiao, Yuanyuan Lin, Joel L. Horowitz, Jian Huang", "title": "Deep Quantile Regression: Mitigating the Curse of Dimensionality Through\n  Composition", "comments": "Guohao Shen and Yuling Jiao contributed equally to this work.\n  Co-corresponding authors: Yuanyuan Lin (email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (email: jian-huang@uiowa.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of nonparametric quantile regression under\nthe assumption that the target conditional quantile function is a composition\nof a sequence of low-dimensional functions. We study the nonparametric quantile\nregression estimator using deep neural networks to approximate the target\nconditional quantile function. For convenience, we shall refer to such an\nestimator as a deep quantile regression (DQR) estimator. We establish\nnon-asymptotic error bounds for the excess risk and the mean integrated squared\nerrors of the DQR estimator. Our results show that the DQR estimator has an\noracle property in the sense that it achieves the nonparametric minimax optimal\nrate determined by the intrinsic dimension of the underlying compositional\nstructure of the conditional quantile function, not the ambient dimension of\nthe predictor. Therefore, DQR is able to mitigate the curse of dimensionality\nunder the assumption that the conditional quantile function has a compositional\nstructure. To establish these results, we analyze the approximation error of a\ncomposite function by neural networks and show that the error rate only depends\non the dimensions of the component functions, instead of the ambient dimension\nof the function. We apply our general results to several important statistical\nmodels often used in mitigating the curse of dimensionality, including the\nsingle index, the additive, the projection pursuit, the univariate composite,\nand the generalized hierarchical interaction models. We explicitly describe the\nprefactors in the error bounds in terms of the dimensionality of the data and\nshow that the prefactors depends on the dimensionality linearly or\nquadratically in these models.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 20:41:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shen", "Guohao", ""], ["Jiao", "Yuling", ""], ["Lin", "Yuanyuan", ""], ["Horowitz", "Joel L.", ""], ["Huang", "Jian", ""]]}, {"id": "2107.04994", "submitter": "Junho Yang", "authors": "Suhasini Subba Rao, Junho Yang", "title": "A prediction perspective on the Wiener-Hopf equations for discrete time\n  series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wiener-Hopf equations are a Toeplitz system of linear equations that have\nseveral applications in time series. These include the update and prediction\nstep of the stationary Kalman filter equations and the prediction of bivariate\ntime series. The Wiener-Hopf technique is the classical tool for solving the\nequations, and is based on a comparison of coefficients in a Fourier series\nexpansion. The purpose of this note is to revisit the (discrete) Wiener-Hopf\nequations and obtain an alternative expression for the solution that is more in\nthe spirit of time series analysis. Specifically, we propose a solution to the\nWiener-Hopf equations that combines linear prediction with deconvolution.\n  The solution of the Wiener-Hopf equations requires one to obtain the spectral\nfactorization of the underlying spectral density function. For general spectral\ndensity functions this is infeasible. Therefore, it is usually assumed that the\nspectral density is rational, which allows one to obtain a computationally\ntractable solution. This leads to an approximation error when the underlying\nspectral density is not a rational function. We use the proposed solution\ntogether with Baxter's inequality to derive an error bound for the rational\nspectral density approximation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 08:49:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Rao", "Suhasini Subba", ""], ["Yang", "Junho", ""]]}, {"id": "2107.05008", "submitter": "Thomas Cailleteau Mr", "authors": "Thomas Cailleteau", "title": "Jaynes & Shannon's Constrained Ignorance and Surprise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this simple article, with possible applications in theoretical and applied\nphysics, we suggest an original way to derive the expression of Shannon's\nentropy from a purely variational approach,using constraints. Based on the work\nof Edwin T. Jaynes, our results are not fundamentally new but the context in\nwhich they are derived might, however, lead to a remarkably consistent\nformalism,where the maximum entropy principle appears naturally. After having\ngiven a general definition of \"ignorance\" in this framework, we derive the\nsomehow general expected expression for the entropy using two approaches. In\nthe first, one is biased and has a vague idea of the shape of the entropy\nfunction. In the second, we consider the general case, where nothing is a\npriori known. The merits of both ways of thinking are compared.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:20:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cailleteau", "Thomas", ""]]}, {"id": "2107.05143", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec, Yiwei Shen", "title": "Derivatives and residual distribution of regularized M-estimators with\n  application to adaptive tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies M-estimators with gradient-Lipschitz loss function\nregularized with convex penalty in linear models with Gaussian design matrix\nand arbitrary noise distribution. A practical example is the robust M-estimator\nconstructed with the Huber loss and the Elastic-Net penalty and the noise\ndistribution has heavy-tails. Our main contributions are three-fold. (i) We\nprovide general formulae for the derivatives of regularized M-estimators\n$\\hat\\beta(y,X)$ where differentiation is taken with respect to both $y$ and\n$X$; this reveals a simple differentiability structure shared by all convex\nregularized M-estimators. (ii) Using these derivatives, we characterize the\ndistribution of the residual $r_i = y_i-x_i^\\top\\hat\\beta$ in the intermediate\nhigh-dimensional regime where dimension and sample size are of the same order.\n(iii) Motivated by the distribution of the residuals, we propose a novel\nadaptive criterion to select tuning parameters of regularized M-estimators. The\ncriterion approximates the out-of-sample error up to an additive constant\nindependent of the estimator, so that minimizing the criterion provides a proxy\nfor minimizing the out-of-sample error. The proposed adaptive criterion does\nnot require the knowledge of the noise distribution or of the covariance of the\ndesign. Simulated data confirms the theoretical findings, regarding both the\ndistribution of the residuals and the success of the criterion as a proxy of\nthe out-of-sample error. Finally our results reveal new relationships between\nthe derivatives of $\\hat\\beta(y,X)$ and the effective degrees of freedom of the\nM-estimator, which are of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 23:20:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bellec", "Pierre C", ""], ["Shen", "Yiwei", ""]]}, {"id": "2107.05183", "submitter": "Paramahansa Pramanik", "authors": "Paramahansa Pramanik", "title": "Consensus as a Nash Equilibrium of a stochastic differential game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper a consensus has been constructed in a social network which is\nmodeled by a stochastic differential game played by agents of that network.\nEach agent independently minimizes a cost function which represents their\nmotives. A conditionally expected integral cost function has been considered\nunder an agent's opinion filtration. The dynamic cost functional is minimized\nsubject to a stochastic differential opinion dynamics. As opinion dynamics\nrepresents an agent's differences of opinion from the others as well as from\ntheir previous opinions, random influences and stubbornness make it more\nvolatile. An agent uses their rate of change of opinion at certain time point\nas a control input. This turns out to be a non-cooperative stochastic\ndifferential game which have a feedback Nash equilibrium. A Feynman-type path\nintegral approach has been used to determine an optimal feedback opinion and\ncontrol. This is a new approach in this literature. Later in this paper an\nexplicit solution of a feedback Nash equilibrium opinion is determined.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 03:54:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pramanik", "Paramahansa", ""]]}, {"id": "2107.05267", "submitter": "Sergio Brenner Miguel", "authors": "Sergio Brenner Miguel and Nathawut Phandoidaen", "title": "Multiplicative deconvolution in survival analysis under dependency", "comments": "29 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-parametric estimation of an unknown survival function S with\nsupport on R+ based on a sample with multiplicative measurement errors. The\nproposed fully-data driven procedure is based on the estimation of the Mellin\ntransform of the survival function S and a regularisation of the inverse of the\nMellin transform by a spectral cut-off. The upcoming bias-variance trade-off is\ndealt with by a data-driven choice of the cut-off parameter. In order to\ndiscuss the bias term, we consider the Mellin-Sobolev spaces which characterize\nthe regularity of the unknown survival function S through the decay of its\nMellin transform. For the analysis of the variance term, we consider the i.i.d.\ncase and incorporate dependent observations in form of Bernoulli shift\nprocesses and beta mixing sequences. Additionally, we show minimax-optimality\nover Mellin-Sobolev spaces of the spectral cut-off estimator.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:02:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Miguel", "Sergio Brenner", ""], ["Phandoidaen", "Nathawut", ""]]}, {"id": "2107.05291", "submitter": "Jeremie Bigot", "authors": "Bernard Bercu and J\\'er\\'emie Bigot and S\\'ebastien Gadat and Emilia\n  Siviero", "title": "A stochastic Gauss-Newton algorithm for regularized semi-discrete\n  optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new second order stochastic algorithm to estimate the\nentropically regularized optimal transport cost between two probability\nmeasures. The source measure can be arbitrary chosen, either absolutely\ncontinuous or discrete, while the target measure is assumed to be discrete. To\nsolve the semi-dual formulation of such a regularized and semi-discrete optimal\ntransportation problem, we propose to consider a stochastic Gauss-Newton\nalgorithm that uses a sequence of data sampled from the source measure. This\nalgorithm is shown to be adaptive to the geometry of the underlying convex\noptimization problem with no important hyperparameter to be accurately tuned.\nWe establish the almost sure convergence and the asymptotic normality of\nvarious estimators of interest that are constructed from this stochastic\nGauss-Newton algorithm. We also analyze their non-asymptotic rates of\nconvergence for the expected quadratic risk in the absence of strong convexity\nof the underlying objective function. The results of numerical experiments from\nsimulated data are also reported to illustrate the finite sample properties of\nthis Gauss-Newton algorithm for stochastic regularized optimal transport, and\nto show its advantages over the use of the stochastic gradient descent,\nstochastic Newton and ADAM algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 10:03:07 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bercu", "Bernard", ""], ["Bigot", "J\u00e9r\u00e9mie", ""], ["Gadat", "S\u00e9bastien", ""], ["Siviero", "Emilia", ""]]}, {"id": "2107.05538", "submitter": "Abdellatif Zaidi", "authors": "Abdellatif Zaidi", "title": "Rate-Exponent Region for a Class of Distributed Hypothesis Testing\n  Against Conditional Independence Problems", "comments": "Submitted for possible publication in the IEEE Transactions of\n  Information Theory. arXiv admin note: substantial text overlap with\n  arXiv:1904.03028, arXiv:1811.03933", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of $K$-encoder hypothesis testing against conditional\nindependence problems. Under the criterion that stipulates minimization of the\nType II error exponent subject to a (constant) upper bound $\\epsilon$ on the\nType I error rate, we characterize the set of encoding rates and exponent for\nboth discrete memoryless and memoryless vector Gaussian settings. For the DM\nsetting, we provide a converse proof and show that it is achieved using the\nQuantize-Bin-Test scheme of Rahman and Wagner. For the memoryless vector\nGaussian setting, we develop a tight outer bound by means of a technique that\nrelies on the de Bruijn identity and the properties of Fisher information. In\nparticular, the result shows that for memoryless vector Gaussian sources the\nrate-exponent region is exhausted using the Quantize-Bin-Test scheme with\nGaussian test channels; and there is no loss in performance caused by\nrestricting the sensors' encoders not to employ time sharing. Furthermore, we\nalso study a variant of the problem in which the source, not necessarily\nGaussian, has finite differential entropy and the sensors' observations noises\nunder the null hypothesis are Gaussian. For this model, our main result is an\nupper bound on the exponent-rate function. The bound is shown to mirror a\ncorresponding explicit lower bound, except that the lower bound involves the\nsource power (variance) whereas the upper bound has the source entropy power.\nPart of the utility of the established bound is for investigating asymptotic\nexponent/rates and losses incurred by distributed detection as function of the\nnumber of sensors.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:06:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zaidi", "Abdellatif", ""]]}, {"id": "2107.05567", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky and Jonathan Niles-Weed", "title": "Strong recovery of geometric planted matchings", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of efficiently recovering the matching between an\nunlabelled collection of $n$ points in $\\mathbb{R}^d$ and a small random\nperturbation of those points. We consider a model where the initial points are\ni.i.d. standard Gaussian vectors, perturbed by adding i.i.d. Gaussian vectors\nwith variance $\\sigma^2$. In this setting, the maximum likelihood estimator\n(MLE) can be found in polynomial time as the solution of a linear assignment\nproblem. We establish thresholds on $\\sigma^2$ for the MLE to perfectly recover\nthe planted matching (making no errors) and to strongly recover the planted\nmatching (making $o(n)$ errors) both for $d$ constant and $d = d(n)$ growing\narbitrarily. Between these two thresholds, we show that the MLE makes\n$n^{\\delta + o(1)}$ errors for an explicit $\\delta \\in (0, 1)$. These results\nextend to the geometric setting a recent line of work on recovering matchings\nplanted in random graphs with independently-weighted edges. Our proof\ntechniques rely on careful analysis of the combinatorial structure of partial\nmatchings in large, weakly dependent random graphs using the first and second\nmoment methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:44:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "2107.05682", "submitter": "Angelica Louren\\c{c}o Oliveira", "authors": "Angelica Louren\\c{c}o Oliveira and Marcos Eduardo Valle", "title": "Least-Squares Linear Dilation-Erosion Regressor Trained using Stochastic\n  Descent Gradient or the Difference of Convex Methods", "comments": "15 pages", "journal-ref": "BRACIS 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hybrid morphological neural network for regression\ntasks called linear dilation-erosion regression ($\\ell$-DER). In few words, an\n$\\ell$-DER model is given by a convex combination of the composition of linear\nand elementary morphological operators. As a result, they yield continuous\npiecewise linear functions and, thus, are universal approximators. Apart from\nintroducing the $\\ell$-DER models, we present three approaches for training\nthese models: one based on stochastic descent gradient and two based on the\ndifference of convex programming problems. Finally, we evaluate the performance\nof the $\\ell$-DER model using 14 regression tasks. Although the approach based\non SDG revealed faster than the other two, the $\\ell$-DER trained using a\ndisciplined convex-concave programming problem outperformed the others in terms\nof the least mean absolute error score.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:41:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Oliveira", "Angelica Louren\u00e7o", ""], ["Valle", "Marcos Eduardo", ""]]}, {"id": "2107.05766", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Seth Strimas-Mackey and Marten\n  Wegkamp", "title": "Likelihood estimation of sparse topic distributions in topic models and\n  its applications to Wasserstein document distance calculations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of high-dimensional, discrete, possibly\nsparse, mixture models in topic models. The data consists of observed\nmultinomial counts of $p$ words across $n$ independent documents. In topic\nmodels, the $p\\times n$ expected word frequency matrix is assumed to be\nfactorized as a $p\\times K$ word-topic matrix $A$ and a $K\\times n$\ntopic-document matrix $T$. Since columns of both matrices represent conditional\nprobabilities belonging to probability simplices, columns of $A$ are viewed as\n$p$-dimensional mixture components that are common to all documents while\ncolumns of $T$ are viewed as the $K$-dimensional mixture weights that are\ndocument specific and are allowed to be sparse. The main interest is to provide\nsharp, finite sample, $\\ell_1$-norm convergence rates for estimators of the\nmixture weights $T$ when $A$ is either known or unknown. For known $A$, we\nsuggest MLE estimation of $T$. Our non-standard analysis of the MLE not only\nestablishes its $\\ell_1$ convergence rate, but reveals a remarkable property:\nthe MLE, with no extra regularization, can be exactly sparse and contain the\ntrue zero pattern of $T$. We further show that the MLE is both minimax optimal\nand adaptive to the unknown sparsity in a large class of sparse topic\ndistributions. When $A$ is unknown, we estimate $T$ by optimizing the\nlikelihood function corresponding to a plug in, generic, estimator $\\hat{A}$ of\n$A$. For any estimator $\\hat{A}$ that satisfies carefully detailed conditions\nfor proximity to $A$, the resulting estimator of $T$ is shown to retain the\nproperties established for the MLE. The ambient dimensions $K$ and $p$ are\nallowed to grow with the sample sizes. Our application is to the estimation of\n1-Wasserstein distances between document generating distributions. We propose,\nestimate and analyze new 1-Wasserstein distances between two probabilistic\ndocument representations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:22:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Strimas-Mackey", "Seth", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2107.05824", "submitter": "March Boedihardjo", "authors": "March Boedihardjo, Thomas Strohmer, Roman Vershynin", "title": "Covariance's Loss is Privacy's Gain: Computationally Efficient, Private\n  and Accurate Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The protection of private information is of vital importance in data-driven\nresearch, business, and government. The conflict between privacy and utility\nhas triggered intensive research in the computer science and statistics\ncommunities, who have developed a variety of methods for privacy-preserving\ndata release. Among the main concepts that have emerged are $k$-anonymity\n(often implemented via microaggregation) and differential privacy. Today,\nanother solution is gaining traction, synthetic data. However, the road to\nprivacy is paved with NP-hard problems. In this paper we focus on the NP-hard\nchallenge to develop a synthetic data generation method that is computationally\nefficient, comes with provable privacy guarantees, and rigorously quantifies\ndata utility. We solve a relaxed version of this problem by studying a\nfundamental, but a first glance completely unrelated, problem in probability\nconcerning the concept of covariance loss. Namely, we find a nearly optimal and\nconstructive answer to the question how much information is lost when we take\nconditional expectation. Surprisingly, this excursion into theoretical\nprobability produces mathematical techniques that allow us to derive\nconstructive, approximately optimal solutions to difficult applied problems\nconcerning microaggregation, privacy, and synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 03:09:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Boedihardjo", "March", ""], ["Strohmer", "Thomas", ""], ["Vershynin", "Roman", ""]]}, {"id": "2107.05852", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud, Barak Sober", "title": "Convergence rates of vector-valued local polynomial regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric estimation of functions as well as their derivatives by means\nof local-polynomial regression is a subject that was studied in the literature\nsince the late 1970's. Given a set of noisy samples of a $\\mathcal{C}^k$ smooth\nfunction, we perform a local polynomial fit, and by taking its $m$-th\nderivative we obtain an estimate for the $m$-th function derivative. The known\noptimal rates of convergence for this problem for a $k$-times smooth function\n$f:\\mathbb{R}^d \\to \\mathbb{R}$ are $n^{-\\frac{k-m}{2k + d}}$. However in\nmodern applications it is often the case that we have to estimate a function\noperating to $\\mathbb{R}^D$, for $D \\gg d$ extremely large. In this work, we\nprove that these same rates of convergence are also achievable by\nlocal-polynomial regression in case of a high dimensional target, given some\nassumptions on the noise distribution. This result is an extension to Stone's\nseminal work from 1980 to the regime of high-dimensional target domain. In\naddition, we unveil a connection between the failure probability $\\varepsilon$\nand the number of samples required to achieve the optimal rates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 05:11:54 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:05:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Sober", "Barak", ""]]}, {"id": "2107.05915", "submitter": "Chaonan Jiang", "authors": "Chaonan Jiang, Davide La Vecchia and Riccardo Rastelli", "title": "Graphical Laplace-approximated maximum likelihood estimation:\n  approximated likelihood inference for network data analysis", "comments": "This paper still needs large modifications in the theory and\n  applications. I will submit it when it is ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive Laplace-approximated maximum likelihood estimators (GLAMLEs) of\nparameters in our Graph Generalized Linear Latent Variable Models. Then, we\nstudy the statistical properties of GLAMLEs when the number of nodes $n_V$ and\nthe observed times of a graph denoted by $K$ diverge to infinity. Finally, we\ndisplay the estimation results in a Monte Carlo simulation considering\ndifferent numbers of latent variables. Besides, we make a comparison between\nLaplace and variational approximations for inference of our model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 08:34:16 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:24:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Jiang", "Chaonan", ""], ["La Vecchia", "Davide", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "2107.06040", "submitter": "Mingya Long", "authors": "Mingya Long, Zhengbang Li, Wei Zhang, and Qizhai Li*", "title": "Cauchy Combination Test for Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Aggregating multiple effects is often encountered in large-scale data\nanalysis where the fraction of significant effects is generally small. Many\nexisting methods cannot handle it effectively because of lack of computational\naccuracy for small p-values. The Cauchy combination test (abbreviated as CCT) (\nJ Am Statist Assoc, 2020, 115(529):393-402) is a powerful and computational\neffective test to aggregate individual $p$-values under arbitrary correlation\nstructures. This work revisits CCT and shows three key contributions including\nthat (i) the tail probability of CCT can be well approximated by a standard\nCauchy distribution under much more relaxed conditions placed on individual\np-values instead of the original test statistics; (ii) the relaxation\nconditions are shown to be satisfied for many popular copulas formulating\nbivariate distributions; (iii) the power of CCT is no less than that of the\nminimum-type test as the number of tests goes to infinity with some regular\nconditions. These results further broaden the theories and applications of CCT.\nThe simulation results verify the theoretic results and the performance of CCT\nis further evaluated with data from a prostate cancer study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:50:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Long", "Mingya", ""], ["Li", "Zhengbang", ""], ["Zhang", "Wei", ""], ["Li*", "Qizhai", ""]]}, {"id": "2107.06091", "submitter": "Maxime ElMasri", "authors": "Maxime ElMasri, J\\'er\\^ome Morio, Florian Simatos", "title": "Optimal projection to improve parametric importance sampling in high\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a dimension-reduction strategy in order to improve\nthe performance of importance sampling in high dimension. The idea is to\nestimate variance terms in a small number of suitably chosen directions. We\nfirst prove that the optimal directions, i.e., the ones that minimize the\nKullback--Leibler divergence with the optimal auxiliary density, are the\neigenvectors associated to extreme (small or large) eigenvalues of the optimal\ncovariance matrix. We then perform extensive numerical experiments that show\nthat as dimension increases, these directions give estimations which are very\nclose to optimal. Moreover, we show that the estimation remains accurate even\nwhen a simple empirical estimator of the covariance matrix is used to estimate\nthese directions. These theoretical and numerical results open the way for\ndifferent generalizations, in particular the incorporation of such ideas in\nadaptive importance sampling schemes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:50:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["ElMasri", "Maxime", ""], ["Morio", "J\u00e9r\u00f4me", ""], ["Simatos", "Florian", ""]]}, {"id": "2107.06238", "submitter": "Ting Ye", "authors": "Ting Ye and Zhonghua Liu and Baoluo Sun and Eric Tchetgen Tchetgen", "title": "GENIUS-MAWII: For Robust Mendelian Randomization with Many Weak Invalid\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) has become a popular approach to study causal\neffects by using genetic variants as instrumental variables. We propose a new\nMR method, GENIUS-MAWII, which simultaneously addresses the two salient\nphenomena that adversely affect MR analyses: many weak instruments and\nwidespread horizontal pleiotropy. Similar to MR GENIUS\n\\citep{Tchetgen2019_GENIUS}, we achieve identification of the treatment effect\nby leveraging heteroscedasticity of the exposure. We then derive the class of\ninfluence functions of the treatment effect, based on which, we construct a\ncontinuous updating estimator and establish its consistency and asymptotic\nnormality under a many weak invalid instruments asymptotic regime by developing\nnovel semiparametric theory. We also provide a measure of weak identification\nand graphical diagnostic tool. We demonstrate in simulations that GENIUS-MAWII\nhas clear advantages in the presence of directional or correlated horizontal\npleiotropy compared to other methods. We apply our method to study the effect\nof body mass index on systolic blood pressure using UK Biobank.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:50:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ye", "Ting", ""], ["Liu", "Zhonghua", ""], ["Sun", "Baoluo", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2107.06338", "submitter": "Julia Gaudio", "authors": "Souvik Dhara, Julia Gaudio, Elchanan Mossel, and Colin Sandon", "title": "Spectral Recovery of Binary Censored Block Models", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community detection is the problem of identifying community structure in\ngraphs. Often the graph is modeled as a sample from the Stochastic Block Model,\nin which each vertex belongs to a community. The probability that two vertices\nare connected by an edge depends on the communities of those vertices. In this\npaper, we consider a model of censored community detection with two\ncommunities, where most of the data is missing as the status of only a small\nfraction of the potential edges is revealed. In this model, vertices in the\nsame community are connected with probability $p$ while vertices in opposite\ncommunities are connected with probability $q$. The connectivity status of a\ngiven pair of vertices $\\{u,v\\}$ is revealed with probability $\\alpha$,\nindependently across all pairs, where $\\alpha = \\frac{t \\log(n)}{n}$. We\nestablish the information-theoretic threshold $t_c(p,q)$, such that no\nalgorithm succeeds in recovering the communities exactly when $t < t_c(p,q)$.\nWe show that when $t > t_c(p,q)$, a simple spectral algorithm based on a\nweighted, signed adjacency matrix succeeds in recovering the communities\nexactly.\n  While spectral algorithms are shown to have near-optimal performance in the\nsymmetric case, we show that they may fail in the asymmetric case where the\nconnection probabilities inside the two communities are allowed to be\ndifferent. In particular, we show the existence of a parameter regime where a\nsimple two-phase algorithm succeeds but any algorithm based on thresholding a\nlinear combination of the top two eigenvectors of the weighted, signed\nadjacency matrix fails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:04:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Dhara", "Souvik", ""], ["Gaudio", "Julia", ""], ["Mossel", "Elchanan", ""], ["Sandon", "Colin", ""]]}, {"id": "2107.06345", "submitter": "Simon Barthelm\\'e", "authors": "Nicolas Tremblay, Simon Barthelm\\'e, Konstantin Usevich,\n  Pierre-Olivier Amblard", "title": "Extended L-ensembles: a new representation for Determinantal Point\n  Processes", "comments": "Most of this material appeared in a previous arxiv submission\n  (arXiv:2007.04117), two sections are new, and some things have been rephrased", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are a class of repulsive point\nprocesses, popular for their relative simplicity. They are traditionally\ndefined via their marginal distributions, but a subset of DPPs called\n\"L-ensembles\" have tractable likelihoods and are thus particularly easy to work\nwith. Indeed, in many applications, DPPs are more naturally defined based on\nthe L-ensemble formulation rather than through the marginal kernel. The fact\nthat not all DPPs are L-ensembles is unfortunate, but there is a unifying\ndescription. We introduce here extended L-ensembles, and show that all DPPs are\nextended L-ensembles (and vice-versa). Extended L-ensembles have very simple\nlikelihood functions, contain L-ensembles and projection DPPs as special cases.\nFrom a theoretical standpoint, they fix some pathologies in the usual formalism\nof DPPs, for instance the fact that projection DPPs are not L-ensembles. From a\npractical standpoint, they extend the set of kernel functions that may be used\nto define DPPs: we show that conditional positive definite kernels are good\ncandidates for defining DPPs, including DPPs that need no spatial scale\nparameter. Finally, extended L-ensembles are based on so-called ``saddle-point\nmatrices'', and we prove an extension of the Cauchy-Binet theorem for such\nmatrices that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:32:06 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Barthelm\u00e9", "Simon", ""], ["Usevich", "Konstantin", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "2107.06388", "submitter": "Xiao Li", "authors": "Xiao Li, William Fithian", "title": "Whiteout: when do fixed-X knockoffs fail?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core strength of knockoff methods is their virtually limitless\ncustomizability, allowing an analyst to exploit machine learning algorithms and\ndomain knowledge without threatening the method's robust finite-sample false\ndiscovery rate control guarantee. While several previous works have\ninvestigated regimes where specific implementations of knockoffs are provably\npowerful, general negative results are more difficult to obtain for such a\nflexible method. In this work we recast the fixed-$X$ knockoff filter for the\nGaussian linear model as a conditional post-selection inference method. It adds\nuser-generated Gaussian noise to the ordinary least squares estimator\n$\\hat\\beta$ to obtain a \"whitened\" estimator $\\widetilde\\beta$ with\nuncorrelated entries, and performs inference using\n$\\text{sgn}(\\widetilde\\beta_j)$ as the test statistic for $H_j:\\; \\beta_j = 0$.\nWe prove equivalence between our whitening formulation and the more standard\nformulation involving negative control predictor variables, showing how the\nfixed-$X$ knockoffs framework can be used for multiple testing on any problem\nwith (asymptotically) multivariate Gaussian parameter estimates. Relying on\nthis perspective, we obtain the first negative results that universally\nupper-bound the power of all fixed-$X$ knockoff methods, without regard to\nchoices made by the analyst. Our results show roughly that, if the leading\neigenvalues of $\\text{Var}(\\hat\\beta)$ are large with dense leading\neigenvectors, then there is no way to whiten $\\hat\\beta$ without irreparably\nerasing nearly all of the signal, rendering $\\text{sgn}(\\widetilde\\beta_j)$ too\nuninformative for accurate inference. We give conditions under which the true\npositive rate (TPR) for any fixed-$X$ knockoff method must converge to zero\neven while the TPR of Bonferroni-corrected multiple testing tends to one, and\nwe explore several examples illustrating this phenomenon.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:57:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Li", "Xiao", ""], ["Fithian", "William", ""]]}, {"id": "2107.06621", "submitter": "Michele Coghi", "authors": "Michele Coghi and Torstein Nilssen and Nikolas N\\\"usken", "title": "Rough McKean-Vlasov dynamics for robust ensemble Kalman filtering", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the challenge of incorporating data into misspecified and\nmultiscale dynamical models, we study a McKean-Vlasov equation that contains\nthe data stream as a common driving rough path. This setting allows us to prove\nwell-posedness as well as continuity with respect to the driver in an\nappropriate rough-path topology. The latter property is key in our subsequent\ndevelopment of a robust data assimilation methodology: We establish propagation\nof chaos for the associated interacting particle system, which in turn is\nsuggestive of a numerical scheme that can be viewed as an extension of the\nensemble Kalman filter to a rough-path framework. Finally, we discuss a\ndata-driven method based on subsampling to construct suitable rough path lifts\nand demonstrate the robustness of our scheme in a number of numerical\nexperiments related to parameter estimation problems in multiscale contexts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 11:45:03 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Coghi", "Michele", ""], ["Nilssen", "Torstein", ""], ["N\u00fcsken", "Nikolas", ""]]}, {"id": "2107.06689", "submitter": "Carlo Orsi", "authors": "Carlo Orsi", "title": "New Developments on the Non-Central Chi-Squared and Beta Distributions", "comments": "accepted for publication at Austrian Journal of Statistics on 06 May\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New formulas for the moments about zero of the Non-central Chi-Squared and\nthe Non-central Beta distributions are achieved by means of novel approaches.\nThe mixture representation of the former model and a new expansion of the\nascending factorial of a binomial are the main ingredients of the first\napproach, whereas the second one hinges on an interesting relationship of\nconditional independence and a simple conditional density of the latter model.\nThen, a simulation study is carried out in order to pursue a twofold purpose:\nproviding numerical validations of the derived moment formulas on one side and\ndiscussing the advantages of the new formulas over the existing ones on the\nother.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:25:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Orsi", "Carlo", ""]]}, {"id": "2107.06767", "submitter": "Miklos Z. Racz", "authors": "Miklos Z. Racz, Anirudh Sridhar", "title": "Correlated Stochastic Block Models: Exact Graph Matching with\n  Applications to Recovering Communities", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG cs.SI math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning latent community structure from multiple\ncorrelated networks. First, we study the problem of learning the latent vertex\ncorrespondence between two edge-correlated stochastic block models, focusing on\nthe regime where the average degree is logarithmic in the number of vertices.\nWe derive the precise information-theoretic threshold for exact recovery: above\nthe threshold there exists an estimator that outputs the true correspondence\nwith probability close to 1, while below it no estimator can recover the true\ncorrespondence with probability bounded away from 0. As an application of our\nresults, we show how one can exactly recover the latent communities using\nmultiple correlated graphs in parameter regimes where it is\ninformation-theoretically impossible to do so using just a single graph.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:27:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Racz", "Miklos Z.", ""], ["Sridhar", "Anirudh", ""]]}, {"id": "2107.06815", "submitter": "Thien Le", "authors": "Thien-Minh Le and Ping-Shou Zhong", "title": "High-dimensional Precision Matrix Estimation with a Known Graphical\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A precision matrix is the inverse of a covariance matrix. In this paper, we\nstudy the problem of estimating the precision matrix with a known graphical\nstructure under high-dimensional settings. We propose a simple estimator of the\nprecision matrix based on the connection between the known graphical structure\nand the precision matrix. We obtain the rates of convergence of the proposed\nestimators and derive the asymptotic normality of the proposed estimator in the\nhigh-dimensional setting when the data dimension grows with the sample size.\nNumerical simulations are conducted to demonstrate the performance of the\nproposed method. We also show that the proposed method outperforms some\nexisting methods that do not utilize the graphical structure information.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:44:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Le", "Thien-Minh", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "2107.06936", "submitter": "Jean Barbier Dr.", "authors": "Jean Barbier, Wei-Kuo Chen, Dmitry Panchenko, and Manuel S\\'aenz", "title": "Performance of Bayesian linear regression in a model with mismatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.dis-nn cs.IT cs.LG math-ph math.IT math.MP math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For a model of high-dimensional linear regression with random design, we\nanalyze the performance of an estimator given by the mean of a log-concave\nBayesian posterior distribution with gaussian prior. The model is mismatched in\nthe following sense: like the model assumed by the statistician, the\nlabels-generating process is linear in the input data, but both the classifier\nground-truth prior and gaussian noise variance are unknown to her. This\ninference model can be rephrased as a version of the Gardner model in spin\nglasses and, using the cavity method, we provide fixed point equations for\nvarious overlap order parameters, yielding in particular an expression for the\nmean-square reconstruction error on the classifier (under an assumption of\nuniqueness of solutions). As a direct corollary we obtain an expression for the\nfree energy. Similar models have already been studied by Shcherbina and Tirozzi\nand by Talagrand, but our arguments are more straightforward and some\nassumptions are relaxed. An interesting consequence of our analysis is that in\nthe random design setting of ridge regression, the performance of the posterior\nmean is independent of the noise variance (or \"temperature\") assumed by the\nstatistician, and matches the one of the usual (zero temperature) ridge\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:50:13 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Barbier", "Jean", ""], ["Chen", "Wei-Kuo", ""], ["Panchenko", "Dmitry", ""], ["S\u00e1enz", "Manuel", ""]]}, {"id": "2107.06939", "submitter": "Shihao Wu", "authors": "Ziwei Zhu, Shihao Wu", "title": "On the early solution path of best subset selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early solution path, which tracks the first few variables that enter the\nmodel of a selection procedure, is of profound importance to scientific\ndiscovery. In practice, it is often statistically intangible to identify all\nthe important features with no false discovery, let alone the intimidating\nexpense of experiments to test their significance. Such realistic limitation\ncalls for statistical guarantee for the early discovery of a model selector to\nnavigate scientific adventure on the sea of big data. In this paper, we focus\non the early solution path of best subset selection (BSS), where the sparsity\nconstraint is set to be lower than the true sparsity. Under a sparse\nhigh-dimensional linear model, we establish the sufficient and (near) necessary\ncondition for BSS to achieve sure early selection, or equivalently, zero false\ndiscovery throughout its entire early path. Essentially, this condition boils\ndown to a lower bound of the minimum projected signal margin that characterizes\nthe fundamental gap in signal capturing between sure selection models and those\nwith spurious discovery. Defined through projection operators, this margin is\nindependent of the restricted eigenvalues of the design, suggesting the\nrobustness of BSS against collinearity. On the numerical aspect, we choose\nCoSaMP (Compressive Sampling Matching Pursuit) to approximate the BSS\nsolutions, and we show that the resulting early path exhibits much lower false\ndiscovery rate (FDR) than LASSO, MCP and SCAD, especially in presence of highly\ncorrelated design. Finally, we apply CoSaMP to perform preliminary feature\nscreening for the knockoff filter to enhance its power.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:06:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Zhu", "Ziwei", ""], ["Wu", "Shihao", ""]]}, {"id": "2107.07140", "submitter": "Rami Tabri", "authors": "Rami V. Tabri", "title": "The Information Projection in Moment Inequality Models: Existence, Dual\n  Representation, and Approximation", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new existence, dual representation and approximation\nresults for the information projection in the infinite-dimensional setting for\nmoment inequality models. These results are established under a general\nspecification of the moment inequality model, nesting both conditional and\nunconditional models, and allowing for an infinite number of such inequalities.\nAn important innovation of the paper is the exhibition of the dual variable as\na weak vector-valued integral to formulate an approximation scheme of the\n$I$-projection's equivalent Fenchel dual problem. In particular, it is shown\nunder suitable assumptions that the dual problem's optimum value can be\napproximated by the values of finite-dimensional programs, and that, in\naddition, every accumulation point of a sequence of optimal solutions for the\napproximating programs is an optimal solution for the dual problem. This paper\nillustrates the verification of assumptions and the construction of the\napproximation scheme's parameters for the cases of unconditional and\nconditional first-order stochastic dominance constraints.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 05:57:12 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 07:05:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tabri", "Rami V.", ""]]}, {"id": "2107.07213", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Nicolas Tremblay, Konstantin Usevich,\n  Pierre-Olivier Amblard", "title": "Determinantal Point Processes in the Flat Limit", "comments": "Most of this material first appeared in arXiv:2007.04117, which has\n  been split into two. The presentation has been simplified and some material\n  is new", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are repulsive point processes where the\ninteraction between points depends on the determinant of a positive-semi\ndefinite matrix.\n  In this paper, we study the limiting process of L-ensembles based on kernel\nmatrices, when the kernel function becomes flat (so that every point interacts\nwith every other point, in a sense). We show that these limiting processes are\nbest described in the formalism of extended L-ensembles and partial projection\nDPPs, and the exact limit depends mostly on the smoothness of the kernel\nfunction. In some cases, the limiting process is even universal, meaning that\nit does not depend on specifics of the kernel function, but only on its degree\nof smoothness.\n  Since flat-limit DPPs are still repulsive processes, this implies that\npractically useful families of DPPs exist that do not require a spatial\nlength-scale parameter.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:40:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Tremblay", "Nicolas", ""], ["Usevich", "Konstantin", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "2107.07257", "submitter": "Oliver Feng", "authors": "Oliver Y. Feng, Yining Chen, Qiyang Han, Raymond J. Carroll and\n  Richard J. Samworth", "title": "Nonparametric, tuning-free estimation of S-shaped functions", "comments": "79 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the nonparametric estimation of an S-shaped regression function.\nThe least squares estimator provides a very natural, tuning-free approach, but\nresults in a non-convex optimisation problem, since the inflection point is\nunknown. We show that the estimator may nevertheless be regarded as a\nprojection onto a finite union of convex cones, which allows us to propose a\nmixed primal-dual bases algorithm for its efficient, sequential computation.\nAfter developing a projection framework that demonstrates the consistency and\nrobustness to misspecification of the estimator, our main theoretical results\nprovide sharp oracle inequalities that yield worst-case and adaptive risk\nbounds for the estimation of the regression function, as well as a rate of\nconvergence for the estimation of the inflection point. These results reveal\nnot only that the estimator achieves the minimax optimal rate of convergence\nfor both the estimation of the regression function and its inflection point (up\nto a logarithmic factor in the latter case), but also that it is able to\nachieve an almost-parametric rate when the true regression function is\npiecewise affine with not too many affine pieces. Simulations and a real data\napplication to air pollution modelling also confirm the desirable finite-sample\nproperties of the estimator, and our algorithm is implemented in the R package\nSshaped.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:28:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Feng", "Oliver Y.", ""], ["Chen", "Yining", ""], ["Han", "Qiyang", ""], ["Carroll", "Raymond J.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2107.07317", "submitter": "Jin Zhu", "authors": "Xueqin Wang, Jin Zhu, Wenliang Pan, Junhao Zhu, Heping Zhang", "title": "Nonparametric Statistical Inference via Metric Distribution Function in\n  Metric Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution function is essential in statistical inference, and connected\nwith samples to form a directed closed loop by the correspondence theorem in\nmeasure theory and the Glivenko-Cantelli and Donsker properties. This\nconnection creates a paradigm for statistical inference. However, existing\ndistribution functions are defined in Euclidean spaces and no longer convenient\nto use in rapidly evolving data objects of complex nature. It is imperative to\ndevelop the concept of distribution function in a more general space to meet\nemerging needs. Note that the linearity allows us to use hypercubes to define\nthe distribution function in a Euclidean space, but without the linearity in a\nmetric space, we must work with the metric to investigate the probability\nmeasure. We introduce a class of metric distribution functions through the\nmetric between random objects and a fixed location in metric spaces. We\novercome this challenging step by proving the correspondence theorem and the\nGlivenko-Cantelli theorem for metric distribution functions in metric spaces\nthat lie the foundation for conducting rational statistical inference for\nmetric space-valued data. Then, we develop homogeneity test and mutual\nindependence test for non-Euclidean random objects, and present comprehensive\nempirical evidence to support the performance of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:37:09 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Xueqin", ""], ["Zhu", "Jin", ""], ["Pan", "Wenliang", ""], ["Zhu", "Junhao", ""], ["Zhang", "Heping", ""]]}, {"id": "2107.07350", "submitter": "Kartik Waghmare", "authors": "Kartik G. Waghmare and Victor M. Panaretos", "title": "The Completion of Covariance Kernels", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of positive-semidefinite continuation: extending a\npartially specified covariance kernel from a subdomain $\\Omega$ of a domain\n$I\\times I$ to a covariance kernel on the entire domain $I\\times I$. For a\nbroad class of domains $\\Omega$ called serrated domains, we are able to present\na complete theory. Namely, we demonstrate that a canonical completion always\nexists and can be explicitly constructed. We characterise all possible\ncompletions as suitable perturbations of the canonical completion, and\ndetermine necessary and sufficient conditions for a unique completion to exist.\nWe interpret the canonical completion via the graphical model structure it\ninduces on the associated Gaussian process. Furthermore, we show how the\nestimation of the canonical completion reduces to the solution of a system of\nlinear statistical inverse problems in the space of Hilbert-Schmidt operators,\nand derive rates of convergence under standard source conditions. We conclude\nby providing extensions of our theory to more general forms of domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:06:40 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:15:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Waghmare", "Kartik G.", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2107.07420", "submitter": "Fang-Yi Yu", "authors": "Yiling Chen and Fang-Yi Yu", "title": "Optimal Scoring Rule Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces an optimization problem for proper scoring rule design.\nConsider a principal who wants to collect an agent's prediction about an\nunknown state. The agent can either report his prior prediction or access a\ncostly signal and report the posterior prediction. Given a collection of\npossible distributions containing the agent's posterior prediction\ndistribution, the principal's objective is to design a bounded scoring rule to\nmaximize the agent's worst-case payoff increment between reporting his\nposterior prediction and reporting his prior prediction.\n  We study two settings of such optimization for proper scoring rules: static\nand asymptotic settings. In the static setting, where the agent can access one\nsignal, we propose an efficient algorithm to compute an optimal scoring rule\nwhen the collection of distributions is finite. The agent can adaptively and\nindefinitely refine his prediction in the asymptotic setting. We first consider\na sequence of collections of posterior distributions with vanishing covariance,\nwhich emulates general estimators with large samples, and show the optimality\nof the quadratic scoring rule. Then, when the agent's posterior distribution is\na Beta-Bernoulli process, we find that the log scoring rule is optimal. We also\nprove the optimality of the log scoring rule over a smaller set of functions\nfor categorical distributions with Dirichlet priors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:05:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chen", "Yiling", ""], ["Yu", "Fang-Yi", ""]]}, {"id": "2107.07511", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios N. Angelopoulos, Stephen Bates", "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free\n  Uncertainty Quantification", "comments": "Blog and tutorial video\n  http://angelopoulos.ai/blog/posts/gentle-intro/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:59:50 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Angelopoulos", "Anastasios N.", ""], ["Bates", "Stephen", ""]]}, {"id": "2107.07575", "submitter": "Caleb Miles", "authors": "Caleb H. Miles and Antoine Chambaz", "title": "Optimal tests of the composite null hypothesis arising in mediation\n  analysis", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The indirect effect of an exposure on an outcome through an intermediate\nvariable can be identified by a product of regression coefficients under\ncertain causal and regression modeling assumptions. Thus, the null hypothesis\nof no indirect effect is a composite null hypothesis, as the null holds if\neither regression coefficient is zero. A consequence is that existing\nhypothesis tests are either severely underpowered near the origin (i.e., when\nboth coefficients are small with respect to standard errors) or do not preserve\ntype 1 error uniformly over the null hypothesis space. We propose hypothesis\ntests that (i) preserve level alpha type 1 error, (ii) meaningfully improve\npower when both true underlying effects are small relative to sample size, and\n(iii) preserve power when at least one is not. One approach gives a closed-form\ntest that is minimax optimal with respect to local power over the alternative\nparameter space. Another uses sparse linear programming to produce an\napproximately optimal test for a Bayes risk criterion. We provide an R package\nthat implements the minimax optimal test.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:20:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Miles", "Caleb H.", ""], ["Chambaz", "Antoine", ""]]}, {"id": "2107.07623", "submitter": "Luca Ganassali", "authors": "Luca Ganassali, Laurent Massouli\\'e, Marc Lelarge", "title": "Correlation detection in trees for partial graph alignment", "comments": "22 pages, 1 figure. Preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider alignment of sparse graphs, which consists in finding a mapping\nbetween the nodes of two graphs which preserves most of the edges. Our approach\nis to compare local structures in the two graphs, matching two nodes if their\nneighborhoods are 'close enough': for correlated Erd\\H{o}s-R\\'enyi random\ngraphs, this problem can be locally rephrased in terms of testing whether a\npair of branching trees is drawn from either a product distribution, or a\ncorrelated distribution. We design an optimal test for this problem which gives\nrise to a message-passing algorithm for graph alignment, which provably returns\nin polynomial time a positive fraction of correctly matched vertices, and a\nvanishing fraction of mismatches. With an average degree $\\lambda = O(1)$ in\nthe graphs, and a correlation parameter $s \\in [0,1]$, this result holds with\n$\\lambda s$ large enough, and $1-s$ small enough, completing the recent\nstate-of-the-art diagram. Tighter conditions for determining whether partial\ngraph alignment (or correlation detection in trees) is feasible in polynomial\ntime are given in terms of Kullback-Leibler divergences.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 22:02:27 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ganassali", "Luca", ""], ["Massouli\u00e9", "Laurent", ""], ["Lelarge", "Marc", ""]]}, {"id": "2107.07682", "submitter": "Yukun Jiang", "authors": "Yukun Jiang", "title": "The Application of Active Query K-Means in Text Classification", "comments": "6 pages, 3 algorithms, 4 tables, 8 figures For source code and\n  questions, please email Yukun Jiang at jy2363@nyu.edu Reply would follow\n  shortly", "journal-ref": "In proceedings of 3rd International Conference on Natural Language\n  Processing 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning is a state-of-art machine learning approach to deal with an\nabundance of unlabeled data. In the field of Natural Language Processing,\ntypically it is costly and time-consuming to have all the data annotated. This\ninefficiency inspires out our application of active learning in text\nclassification. Traditional unsupervised k-means clustering is first modified\ninto a semi-supervised version in this research. Then, a novel attempt is\napplied to further extend the algorithm into active learning scenario with\nPenalized Min-Max-selection, so as to make limited queries that yield more\nstable initial centroids. This method utilizes both the interactive query\nresults from users and the underlying distance representation. After tested on\na Chinese news dataset, it shows a consistent increase in accuracy while\nlowering the cost in training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:06:35 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Jiang", "Yukun", ""]]}, {"id": "2107.07750", "submitter": "Thomas Hamm", "authors": "Thomas Hamm, Ingo Steinwart", "title": "Intrinsic Dimension Adaptive Partitioning for Kernel Methods", "comments": "36 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove minimax optimal learning rates for kernel ridge regression,\nresp.~support vector machines based on a data dependent partition of the input\nspace, where the dependence of the dimension of the input space is replaced by\nthe fractal dimension of the support of the data generating distribution. We\nfurther show that these optimal rates can be achieved by a training validation\nprocedure without any prior knowledge on this intrinsic dimension of the data.\nFinally, we conduct extensive experiments which demonstrate that our considered\nlearning methods are actually able to generalize from a dataset that is\nnon-trivially embedded in a much higher dimensional space just as well as from\nthe original dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 07:51:09 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Hamm", "Thomas", ""], ["Steinwart", "Ingo", ""]]}, {"id": "2107.07828", "submitter": "Gabriel Romon", "authors": "Pierre C Bellec, Gabriel Romon", "title": "Chi-square and normal inference in high-dimensional multi-task\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes chi-square and normal inference methodologies for the\nunknown coefficient matrix $B^*$ of size $p\\times T$ in a Multi-Task (MT)\nlinear model with $p$ covariates, $T$ tasks and $n$ observations under a\nrow-sparse assumption on $B^*$. The row-sparsity $s$, dimension $p$ and number\nof tasks $T$ are allowed to grow with $n$. In the high-dimensional regime\n$p\\ggg n$, in order to leverage row-sparsity, the MT Lasso is considered.\n  We build upon the MT Lasso with a de-biasing scheme to correct for the bias\ninduced by the penalty. This scheme requires the introduction of a new\ndata-driven object, coined the interaction matrix, that captures effective\ncorrelations between noise vector and residuals on different tasks. This matrix\nis psd, of size $T\\times T$ and can be computed efficiently.\n  The interaction matrix lets us derive asymptotic normal and $\\chi^2_T$\nresults under Gaussian design and $\\frac{sT+s\\log(p/s)}{n}\\to0$ which\ncorresponds to consistency in Frobenius norm. These asymptotic distribution\nresults yield valid confidence intervals for single entries of $B^*$ and valid\nconfidence ellipsoids for single rows of $B^*$, for both known and unknown\ndesign covariance $\\Sigma$. While previous proposals in grouped-variables\nregression require row-sparsity $s\\lesssim\\sqrt n$ up to constants depending on\n$T$ and logarithmic factors in $n,p$, the de-biasing scheme using the\ninteraction matrix provides confidence intervals and $\\chi^2_T$ confidence\nellipsoids under the conditions ${\\min(T^2,\\log^8p)}/{n}\\to 0$ and $$\n\\frac{sT+s\\log(p/s)+\\|\\Sigma^{-1}e_j\\|_0\\log p}{n}\\to0, \\quad\n\\frac{\\min(s,\\|\\Sigma^{-1}e_j\\|_0)}{\\sqrt n} \\sqrt{[T+\\log(p/s)]\\log p}\\to 0,\n$$ allowing row-sparsity $s\\ggg\\sqrt n$ when $\\|\\Sigma^{-1}e_j\\|_0 \\sqrt T\\lll\n\\sqrt{n}$ up to logarithmic factors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:19:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bellec", "Pierre C", ""], ["Romon", "Gabriel", ""]]}, {"id": "2107.07836", "submitter": "Anatoli Juditsky B.", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "Aggregating estimates by convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the approach to estimate aggregation and adaptive estimation based\nupon (nearly optimal) testing of convex hypotheses. We show that in the\nsituation where the observations stem from {\\em simple observation schemes} and\nwhere set of unknown signals is a finite union of convex and compact sets, the\nproposed approach leads to aggregation and adaptation routines with nearly\noptimal performance. As an illustration, we consider application of the\nproposed estimates to the problem of recovery of unknown signal known to belong\nto a union of ellitopes in Gaussian observation scheme. The proposed approach\ncan be implemented efficiently when the number of sets in the union is \"not\nvery large.\" We conclude the paper with a small simulation study illustrating\npractical performance of the proposed procedures in the problem of signal\nestimation in the single-index model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:49:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "2107.07963", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza and Ngai Hang Chan", "title": "Nearly Unstable Integer-Valued ARCH Process and Unit Root Testing", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Nearly Unstable INteger-valued AutoRegressive\nConditional Heteroskedasticity (NU-INARCH) process for dealing with count time\nseries data. It is proved that a proper normalization of the NU-INARCH process\nendowed with a Skorohod topology weakly converges to a Cox-Ingersoll-Ross\ndiffusion. The asymptotic distribution of the conditional least squares\nestimator of the correlation parameter is established as a functional of\ncertain stochastic integrals. Numerical experiments based on Monte Carlo\nsimulations are provided to verify the behavior of the asymptotic distribution\nunder finite samples. These simulations reveal that the nearly unstable\napproach provides satisfactory and better results than those based on the\nstationarity assumption even when the true process is not that close to\nnon-stationarity. A unit root test is proposed and its Type-I error and power\nare examined via Monte Carlo simulations. As an illustration, the proposed\nmethodology is applied to the daily number of deaths due to COVID-19 in the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:24:21 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Chan", "Ngai Hang", ""]]}, {"id": "2107.08023", "submitter": "Achintya Roy", "authors": "Achintya Roy and Nitin Gupta", "title": "A study on reliability of a $k$-out-of-$n$ system equipped with a cold\n  standby component based on copula", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $k$-out-of-$n$ system consisting of $n$ exchangeable components equipped\nwith a single cold standby component is considered. All the components\nincluding standby are assumed to be dependent and this dependence is modeled by\na copula function.\n  An exact expression for the reliability function of the considered system has\nbeen obtained. We also compute three different mean residual life functions of\nthe system.\n  Finally, some numerical examples are provided to illustrate the theoretical\nresults. Our studies subsume some of the initial studies in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:31:42 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Roy", "Achintya", ""], ["Gupta", "Nitin", ""]]}, {"id": "2107.08089", "submitter": "Dena Asta", "authors": "Dena Asta", "title": "Non-Parametric Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an estimator for manifold distances based on graph Laplacian\nestimates of the Laplace-Beltrami operator. We show that the estimator is\nconsistent for suitable choices of graph Laplacians in the literature, based on\nan equidistributed sample of points drawn from a smooth density bounded away\nfrom zero on an unknown compact Riemannian submanifold of Euclidean space. The\nestimator resembles, and in fact its convergence properties are derived from, a\nspecial case of the Kontorovic dual reformulation of Wasserstein distance known\nas Connes' Distance Formula.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 19:32:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Asta", "Dena", ""]]}, {"id": "2107.08209", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Minimising quantifier variance under prior probability shift", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the binary prevalence quantification problem under prior probability\nshift, we determine the asymptotic variance of the maximum likelihood\nestimator. We find that it is a function of the Brier score for the regression\nof the class label against the features under the test data set distribution.\nThis observation suggests that optimising the accuracy of a base classifier on\nthe training data set helps to reduce the variance of the related quantifier on\nthe test data set. Therefore, we also point out training criteria for the base\nclassifier that imply optimisation of both of the Brier scores on the training\nand the test data sets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 09:28:06 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 19:31:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "2107.08380", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Pierpaolo De Blasi and Mar\\'ia F. Gil-Leyva", "title": "Gibbs sampling for mixtures in order of appearance: the ordered\n  allocation sampler", "comments": "36 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling methods for mixture models are based on data augmentation\nschemes that account for the unobserved partition in the data. Conditional\nsamplers rely on allocation variables that identify each observation with a\nmixture component. They are known to suffer from slow mixing in infinite\nmixtures, where some form of truncation, either deterministic or random, is\nrequired. In mixtures with random number of components, the exploration of\nparameter spaces of different dimensions can also be challenging. We tackle\nthese issues by expressing the mixture components in the random order of\nappearance in an exchangeable sequence directed by the mixing distribution. We\nderive a sampler that is straightforward to implement for mixing distributions\nwith tractable size-biased ordered weights. In infinite mixtures, no form of\ntruncation is necessary. As for finite mixtures with random dimension, a simple\nupdating of the number of components is obtained by a blocking argument, thus,\neasing challenges found in trans-dimensional moves via Metropolis-Hasting\nsteps. Additionally, the latent clustering structure of the model is encrypted\nby means of an ordered partition with blocks labelled in the least element\norder, which mitigates the label-switching problem. We illustrate through a\nsimulation study the good mixing performance of the new sampler.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 07:16:43 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["De Blasi", "Pierpaolo", ""], ["Gil-Leyva", "Mar\u00eda F.", ""]]}, {"id": "2107.08529", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "Regression model selection via log-likelihood ratio and constrained\n  minimum criterion", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the log-likelihood is widely used in model selection, the\nlog-likelihood ratio has had few applications in this area. We develop a\nlog-likelihood ratio based method for selecting regression models by focusing\non the set of models deemed plausible by the likelihood ratio test. We show\nthat when the sample size is large and the significance level of the test is\nsmall, there is a high probability that the smallest model in the set is the\ntrue model; thus, we select this smallest model. The significance level serves\nas a parameter for this method. We consider three levels of this parameter in a\nsimulation study and compare this method with the Akaike Information Criterion\nand Bayesian Information Criterion to demonstrate its excellent accuracy and\nadaptability to different sample sizes. We also apply this method to select a\nlogistic regression model for a South African heart disease dataset.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 19:59:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "2107.08723", "submitter": "Martin Wahl", "authors": "Martin Wahl", "title": "Van Trees inequality, group equivariance, and estimation of principal\n  subspaces", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish non-asymptotic lower bounds for the estimation of principal\nsubspaces. As applications, we obtain new results for the excess risk of\nprincipal component analysis and the matrix denoising problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:51:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wahl", "Martin", ""]]}, {"id": "2107.08724", "submitter": "Tengyao Wang", "authors": "Hanqing Cai and Tengyao Wang", "title": "Estimation of high-dimensional change-points under a group sparsity\n  structure", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change-points are a routine feature of 'big data' observed in the form of\nhigh-dimensional data streams. In many such data streams, the component series\npossess group structures and it is natural to assume that changes only occur in\na small number of all groups. We propose a new change point procedure, called\n'groupInspect', that exploits the group sparsity structure to estimate a\nprojection direction so as to aggregate information across the component series\nto successfully estimate the change-point in the mean structure of the series.\nWe prove that the estimated projection direction is minimax optimal, up to\nlogarithmic factors, when all group sizes are of comparable order. Moreover,\nour theory provide strong guarantees on the rate of convergence of the\nchange-point location estimator. Numerical studies demonstrates the competitive\nperformance of groupInspect in a wide range of settings and a real data example\nconfirms the practical usefulness of our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:51:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cai", "Hanqing", ""], ["Wang", "Tengyao", ""]]}, {"id": "2107.08900", "submitter": "Alessia Caponera", "authors": "Alessia Caponera and Claudio Durastanti", "title": "Parametric estimation for functional autoregressive processes on the\n  sphere", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to define a nonlinear least squares estimator for\nthe spectral parameters of a spherical autoregressive process of order 1 in a\nparametric setting. Furthermore, we investigate on its asymptotic properties,\nsuch as weak consistency and asymptotic normality.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 14:15:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Caponera", "Alessia", ""], ["Durastanti", "Claudio", ""]]}, {"id": "2107.09023", "submitter": "Jorge Yslas Altamirano", "authors": "Martin Bladt and Jorge Yslas", "title": "Heavy-tailed phase-type distributions: A unified approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed phase-type random variables have mathematically tractable\ndistributions and are conceptually attractive to model physical phenomena due\nto their interpretation in terms of a hidden Markov structure. Three recent\nextensions of regular phase-type distributions give rise to models which allow\nfor heavy tails: discrete- or continuous-scaling; fractional-time semi-Markov\nextensions; and inhomogeneous time-change of the underlying Markov process. In\nthis paper, we present a unifying theory for heavy-tailed phase-type\ndistributions for which all three approaches are particular cases. Our main\nobjective is to provide useful models for heavy-tailed phase-type\ndistributions, but any other tail behavior is also captured by our\nspecification. We provide relevant new examples and also show how existing\napproaches are naturally embedded. Subsequently, two multivariate extensions\nare presented, inspired by the univariate construction which can be considered\nas a matrix version of a frailty model. We provide fully explicit EM-algorithms\nfor all models and illustrate them using synthetic and real-life data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:09:42 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bladt", "Martin", ""], ["Yslas", "Jorge", ""]]}, {"id": "2107.09150", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul and George Michailidis", "title": "Inference for Change Points in High Dimensional Mean Shift Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider the problem of constructing confidence intervals for the\nlocations of change points in a high-dimensional mean shift model. To that end,\nwe develop a locally refitted least squares estimator and obtain component-wise\nand simultaneous rates of estimation of the underlying change points. The\nsimultaneous rate is the sharpest available in the literature by at least a\nfactor of $\\log p,$ while the component-wise one is optimal. These results\nenable existence of limiting distributions. Component-wise distributions are\ncharacterized under both vanishing and non-vanishing jump size regimes, while\njoint distributions for any finite subset of change point estimates are\ncharacterized under the latter regime, which also yields asymptotic\nindependence of these estimates. The combined results are used to construct\nasymptotically valid component-wise and simultaneous confidence intervals for\nthe change point parameters. The results are established under a high\ndimensional scaling, allowing for diminishing jump sizes, in the presence of\ndiverging number of change points and under subexponential errors. They are\nillustrated on synthetic data and on sensor measurements from smartphones for\nactivity recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 20:56:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kaul", "Abhishek", ""], ["Michailidis", "George", ""]]}, {"id": "2107.09296", "submitter": "Eitan Greenshtein", "authors": "Eitan Greenshtein, Ya'acov Ritov", "title": "Generalized maximum likelihood estimation of the mean of parameters of\n  mixtures, with applications to sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $f(y|\\theta), \\; \\theta \\in \\Omega$ be a parametric family,\n$\\eta(\\theta)$ a given function, and $G$ an unknown mixing distribution. It is\ndesired to estimate $E_G (\\eta(\\theta))\\equiv \\eta_G$ based on independent\nobservations $Y_1,...,Y_n$, where $Y_i \\sim f(y|\\theta_i)$, and $\\theta_i \\sim\nG$ are iid.\n  We explore the Generalized Maximum Likelihood Estimators (GMLE) for this\nproblem. Some basic properties and representations of those estimators are\nshown. In particular we suggest a new perspective, of the weak convergence\nresult by Kiefer and Wolfowitz (1956), with implications to a corresponding\nsetup in which $\\theta_1,...,\\theta_n$ are {\\it fixed} parameters. We also\nrelate the above problem, of estimating $\\eta_G$, to non-parametric empirical\nBayes estimation under a squared loss.\n  Applications of GMLE to sampling problems are presented. The performance of\nthe GMLE is demonstrated both in simulations and through a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:25:04 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Greenshtein", "Eitan", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2107.09316", "submitter": "Vijay Kumar", "authors": "M. Arshad, M. Khetan, V. Kumar, A.K. Pathak", "title": "Record-Based Transmuted Generalized Linear Exponential Distribution with\n  Increasing, Decreasing and Bathtub Shaped Failure Rates", "comments": "29 pages, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear exponential distribution is a generalization of the exponential\nand Rayleigh distributions. This distribution is one of the best models to fit\ndata with increasing failure rate (IFR). But it does not provide a reasonable\nfit for modeling data with decreasing failure rate (DFR) and bathtub shaped\nfailure rate (BTFR). To overcome this drawback, we propose a new record-based\ntransmuted generalized linear exponential (RTGLE) distribution by using the\ntechnique of Balakrishnan and He (2021). The family of RTGLE distributions is\nmore flexible to fit the data sets with IFR, DFR, and BTFR, and also\ngeneralizes several well-known models as well as some new record-based\ntransmuted models. This paper aims to study the statistical properties of RTGLE\ndistribution, like, the shape of the probability density function and hazard\nfunction, quantile function and its applications, moments and its generating\nfunction, order and record statistics, Renyi entropy. The maximum likelihood\nestimators, least squares and weighted least squares estimators,\nAnderson-Darling estimators, Cramer-von Mises estimators of the unknown\nparameters are constructed and their biases and mean squared errors are\nreported via Monte Carlo simulation study. Finally, the real data set based on\nfailure time illustrates the goodness of fit and applicability of the proposed\ndistribution; hence, suitable recommendations are forwarded.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:07:26 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Arshad", "M.", ""], ["Khetan", "M.", ""], ["Kumar", "V.", ""], ["Pathak", "A. K.", ""]]}, {"id": "2107.09384", "submitter": "Dirk Ostwald", "authors": "Dirk Ostwald and Franziska Us\\'ee", "title": "An induction proof of the backpropagation algorithm in matrix notation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Backpropagation (BP) is a core component of the contemporary deep learning\nincarnation of neural networks. Briefly, BP is an algorithm that exploits the\ncomputational architecture of neural networks to efficiently evaluate the\ngradient of a cost function during neural network parameter optimization. The\nvalidity of BP rests on the application of a multivariate chain rule to the\ncomputational architecture of neural networks and their associated objective\nfunctions. Introductions to deep learning theory commonly present the\ncomputational architecture of neural networks in matrix form, but eschew a\nparallel formulation and justification of BP in the framework of matrix\ndifferential calculus. This entails several drawbacks for the theory and\ndidactics of deep learning. In this work, we overcome these limitations by\nproviding a full induction proof of the BP algorithm in matrix notation.\nSpecifically, we situate the BP algorithm in the framework of matrix\ndifferential calculus, encompass affine-linear potential functions, prove the\nvalidity of the BP algorithm in inductive form, and exemplify the\nimplementation of the matrix form BP algorithm in computer code.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 10:02:17 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ostwald", "Dirk", ""], ["Us\u00e9e", "Franziska", ""]]}, {"id": "2107.09409", "submitter": "Evgeny Prokopenko", "authors": "Marie Kratz and Evgeny Prokopenko", "title": "Multi-Normex Distributions for the Sum of Random Vectors. Rates of\n  Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We build a sharp approximation of the whole distribution of the sum of iid\nheavy-tailed random vectors, combining mean and extreme behaviors. It extends\nthe so-called 'normex' approach from a univariate to a multivariate framework.\nWe propose two possible multi-normex distributions, named $d$-Normex and\nMRV-Normex. Both rely on the Gaussian distribution for describing the mean\nbehavior, via the CLT, while the difference between the two versions comes from\nusing the exact distribution or the EV theorem for the maximum. The main\ntheorems provide the rate of convergence for each version of the multi-normex\ndistributions towards the distribution of the sum, assuming second order\nregular variation property for the norm of the parent random vector when\nconsidering the MRV-normex case. Numerical illustrations and comparisons are\nproposed with various dependence structures on the parent random vector, using\nQQ-plots based on geometrical quantiles.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:15:46 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kratz", "Marie", ""], ["Prokopenko", "Evgeny", ""]]}, {"id": "2107.09418", "submitter": "Caizhu Huang", "authors": "Caizhu Huang and Claudia Di Caterina and Nicola Sartori", "title": "Directional testing for high-dimensional multivariate normal\n  distributions", "comments": "71 pages, 39 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thanks to its favorable properties, the multivariate normal distribution is\nstill largely employed for modeling phenomena in various scientific fields.\nHowever, when the number of components $p$ is of the same asymptotic order as\nthe sample size $n$, standard inferential techniques are generally inadequate\nto conduct hypothesis testing on the mean vector and/or the covariance matrix.\nWithin several prominent frameworks, we propose then to draw reliable\nconclusions via a directional test. We show that under the null hypothesis the\ndirectional $p$-value is exactly uniformly distributed even when $p$ is of the\nsame order of $n$, provided that conditions for the existence of the maximum\nlikelihood estimate for the normal model are satisfied. Extensive simulation\nresults confirm the theoretical findings across different values of $p/n$, and\nshow that the proposed approach outperforms not only the usual finite-$p$\napproaches but also alternative methods tailored for high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:29:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Huang", "Caizhu", ""], ["Di Caterina", "Claudia", ""], ["Sartori", "Nicola", ""]]}, {"id": "2107.09488", "submitter": "Richard Nickl", "authors": "Richard Nickl and Gabriel Paternain", "title": "On some information-theoretic aspects of non-linear statistical inverse\n  problems", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results by van der Vaart (1991) from semi-parametric statistics about the\nexistence of a non-zero Fisher information are reviewed in an\ninfinite-dimensional non-linear Gaussian regression setting.\nInformation-theoretically optimal inference on aspects of the unknown parameter\nis possible if and only if the adjoint of the linearisation of the regression\nmap satisfies a certain range condition. It is shown that this range condition\nmay fail in a commonly studied elliptic inverse problem with a divergence form\nequation, and that a large class of smooth linear functionals of the\nconductivity parameter cannot be estimated efficiently in this case. In\nparticular, Gaussian `Bernstein von Mises'-type approximations for Bayesian\nposterior distributions do not hold in this setting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:39:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Nickl", "Richard", ""], ["Paternain", "Gabriel", ""]]}, {"id": "2107.09532", "submitter": "Sophie Langer Dr.", "authors": "Michael Kohler, Sophie Langer and Ulrich Reif", "title": "Estimation of a regression function on a manifold by fully connected\n  deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of a regression function from independent and identically\ndistributed data is considered. The $L_2$ error with integration with respect\nto the distribution of the predictor variable is used as the error criterion.\nThe rate of convergence of least squares estimates based on fully connected\nspaces of deep neural networks with ReLU activation function is analyzed for\nsmooth regression functions. It is shown that in case that the distribution of\nthe predictor variable is concentrated on a manifold, these estimates achieve a\nrate of convergence which depends on the dimension of the manifold and not on\nthe number of components of the predictor variable.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:43:59 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kohler", "Michael", ""], ["Langer", "Sophie", ""], ["Reif", "Ulrich", ""]]}, {"id": "2107.09542", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Open Problem: Is There an Online Learning Algorithm That Learns Whenever\n  Online Learning Is Possible?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This open problem asks whether there exists an online learning algorithm for\nbinary classification that guarantees, for all target concepts, to make a\nsublinear number of mistakes, under only the assumption that the (possibly\nrandom) sequence of points X allows that such a learning algorithm can exist\nfor that sequence. As a secondary problem, it also asks whether a specific\nconcise condition completely determines whether a given (possibly random)\nsequence of points X admits the existence of online learning algorithms\nguaranteeing a sublinear number of mistakes for all target concepts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:57:37 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "2107.09550", "submitter": "Sophie Langer Dr.", "authors": "Alina Braun, Michael Kohler, Sophie Langer and Harro Walk", "title": "The Smoking Gun: Statistical Theory Improves Neural Network Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the $L_2$ error of neural network regression\nestimates with one hidden layer. Under the assumption that the Fourier\ntransform of the regression function decays suitably fast, we show that an\nestimate, where all initial weights are chosen according to proper uniform\ndistributions and where the weights are learned by gradient descent, achieves a\nrate of convergence of $1/\\sqrt{n}$ (up to a logarithmic factor). Our\nstatistical analysis implies that the key aspect behind this result is the\nproper choice of the initial inner weights and the adjustment of the outer\nweights via gradient descent. This indicates that we can also simply use linear\nleast squares to choose the outer weights. We prove a corresponding theoretical\nresult and compare our new linear least squares neural network estimate with\nstandard neural network estimates via simulated data. Our simulations show that\nour theoretical considerations lead to an estimate with an improved\nperformance. Hence the development of statistical theory can indeed improve\nneural network estimates.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:10:03 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Braun", "Alina", ""], ["Kohler", "Michael", ""], ["Langer", "Sophie", ""], ["Walk", "Harro", ""]]}, {"id": "2107.09660", "submitter": "Arnab Auddy", "authors": "Arnab Auddy and Ming Yuan", "title": "On Estimating Rank-One Spiked Tensors in the Presence of Heavy Tailed\n  Errors", "comments": "46 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.NA math.IT math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the estimation of a rank-one spiked tensor in the\npresence of heavy tailed noise. Our results highlight some of the fundamental\nsimilarities and differences in the tradeoff between statistical and\ncomputational efficiencies under heavy tailed and Gaussian noise. In\nparticular, we show that, for $p$ th order tensors, the tradeoff manifests in\nan identical fashion as the Gaussian case when the noise has finite $4(p-1)$ th\nmoment. The difference in signal strength requirements, with or without\ncomputational constraints, for us to estimate the singular vectors at the\noptimal rate, interestingly, narrows for noise with heavier tails and vanishes\nwhen the noise only has finite fourth moment. Moreover, if the noise has less\nthan fourth moment, tensor SVD, perhaps the most natural approach, is\nsuboptimal even though it is computationally intractable. Our analysis exploits\na close connection between estimating the rank-one spikes and the spectral norm\nof a random tensor with iid entries. In particular, we show that the order of\nthe spectral norm of a random tensor can be precisely characterized by the\nmoment of its entries, generalizing classical results for random matrices. In\naddition to the theoretical guarantees, we propose estimation procedures for\nthe heavy tailed regime, which are easy to implement and efficient to run.\nNumerical experiments are presented to demonstrate their practical merits.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:45:55 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Auddy", "Arnab", ""], ["Yuan", "Ming", ""]]}, {"id": "2107.09773", "submitter": "Vardis Kandiros", "authors": "Yuval Dagan, Constantinos Daskalakis, Nishanth Dikkala, Surbhi Goel,\n  Anthimos Vardis Kandiros", "title": "Statistical Estimation from Dependent Data", "comments": "41 pages, ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a general statistical estimation problem wherein binary labels\nacross different observations are not independent conditioned on their feature\nvectors, but dependent, capturing settings where e.g. these observations are\ncollected on a spatial domain, a temporal domain, or a social network, which\ninduce dependencies. We model these dependencies in the language of Markov\nRandom Fields and, importantly, allow these dependencies to be substantial, i.e\ndo not assume that the Markov Random Field capturing these dependencies is in\nhigh temperature. As our main contribution we provide algorithms and\nstatistically efficient estimation rates for this model, giving several\ninstantiations of our bounds in logistic regression, sparse logistic\nregression, and neural network settings with dependent data. Our estimation\nguarantees follow from novel results for estimating the parameters (i.e.\nexternal fields and interaction strengths) of Ising models from a {\\em single}\nsample. {We evaluate our estimation approach on real networked data, showing\nthat it outperforms standard regression approaches that ignore dependencies,\nacross three text classification datasets: Cora, Citeseer and Pubmed.}\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 21:18:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Dagan", "Yuval", ""], ["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Goel", "Surbhi", ""], ["Kandiros", "Anthimos Vardis", ""]]}, {"id": "2107.09921", "submitter": "Chacko V M", "authors": "Anakha K K, V M Chacko", "title": "On ageing properties of lifetime distributions", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reasonable segment of reliability theory is perpetrated to the study of\nfailure rates, their properties, connections and applications. The present\nstudy focused on failure rate distributions and their shape properties. Failure\nrates of various generalizations of Lindley distribution are discussed and the\ndistinguishable roles of DUS transformation is also discussed\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:53:14 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["K", "Anakha K", ""], ["Chacko", "V M", ""]]}, {"id": "2107.09947", "submitter": "Jerome Dockes", "authors": "J\\'ero\\^ome Dock\\`es, Ga\\\"el Varoquaux (PARIETAL), Jean-Baptiste\n  Poline", "title": "Preventing dataset shift from breaking machine-learning biomarkers", "comments": "GigaScience, BioMed Central, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning brings the hope of finding new biomarkers extracted from\ncohorts with rich biomedical measurements. A good biomarker is one that gives\nreliable detection of the corresponding condition. However, biomarkers are\noften extracted from a cohort that differs from the target population. Such a\nmismatch, known as a dataset shift, can undermine the application of the\nbiomarker to new individuals. Dataset shifts are frequent in biomedical\nresearch, e.g. because of recruitment biases. When a dataset shift occurs,\nstandard machine-learning techniques do not suffice to extract and validate\nbiomarkers. This article provides an overview of when and how dataset shifts\nbreaks machine-learning extracted biomarkers, as well as detection and\ncorrection strategies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 08:54:23 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Dock\u00e8s", "J\u00e9ro\u00f4me", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Poline", "Jean-Baptiste", ""]]}, {"id": "2107.10036", "submitter": "Holger Dette", "authors": "Nina D\\\"ornemann, Holger Dette", "title": "Linear spectral statistics of sequential sample covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent $p$-dimensional vectors with independent complex or real valued\nentries such that $\\mathbb{E} [\\mathbf{x}_i] = \\mathbf{0}$, ${\\rm Var }\n(\\mathbf{x}_i) = \\mathbf{I}_p$, $i=1, \\ldots,n$, let $\\mathbf{T }_n$ be a $p\n\\times p$ Hermitian nonnegative definite matrix and $f $ be a given function.\nWe prove that an approriately standardized version of the stochastic process $\n\\big ( {\\operatorname{tr}} ( f(\\mathbf{B}_{n,t}) ) \\big )_{t \\in [t_0, 1]} $\ncorresponding to a linear spectral statistic of the sequential empirical\ncovariance estimator $$ \\big ( \\mathbf{B}_{n,t} )_{t\\in [ t_0 , 1]} = \\Big (\n\\frac{1}{n} \\sum_{i=1}^{\\lfloor n t \\rfloor} \\mathbf{T }^{1/2}_n \\mathbf{x}_i\n\\mathbf{x}_i ^\\star \\mathbf{T }^{1/2}_n \\Big)_{t\\in [ t_0 , 1]} $$ converges\nweakly to a non-standard Gaussian process for $n,p\\to\\infty$. As an application\nwe use these results to develop a novel approach for monitoring the sphericity\nassumption in a high-dimensional framework, even if the dimension of the\nunderlying data is larger than the sample size.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 12:07:15 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 11:13:07 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["D\u00f6rnemann", "Nina", ""], ["Dette", "Holger", ""]]}, {"id": "2107.10078", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Aditya Vikram Singh, and\n  Himanshu Tyagi", "title": "Optimal Rates for Nonparametric Density Estimation under Communication\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider density estimation for Besov spaces when each sample is quantized\nto only a limited number of bits. We provide a noninteractive adaptive\nestimator that exploits the sparsity of wavelet bases, along with a\nsimulate-and-infer technique from parametric estimation under communication\nconstraints. We show that our estimator is nearly rate-optimal by deriving\nminimax lower bounds that hold even when interactive protocols are allowed.\nInterestingly, while our wavelet-based estimator is almost rate-optimal for\nSobolev spaces as well, it is unclear whether the standard Fourier basis, which\narise naturally for those spaces, can be used to achieve the same performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 13:43:44 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Singh", "Aditya Vikram", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2107.10127", "submitter": "Yang Li", "authors": "Yang Li and Jinqiao Duan", "title": "Extracting Governing Laws from Sample Path Data of Non-Gaussian\n  Stochastic Dynamical Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.03769", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in data science are leading to new progresses in the analysis and\nunderstanding of complex dynamics for systems with experimental and\nobservational data. With numerous physical phenomena exhibiting bursting,\nflights, hopping, and intermittent features, stochastic differential equations\nwith non-Gaussian L\\'evy noise are suitable to model these systems. Thus it is\ndesirable and essential to infer such equations from available data to\nreasonably predict dynamical behaviors. In this work, we consider a data-driven\nmethod to extract stochastic dynamical systems with non-Gaussian asymmetric\n(rather than the symmetric) L\\'evy process, as well as Gaussian Brownian\nmotion. We establish a theoretical framework and design a numerical algorithm\nto compute the asymmetric L\\'evy jump measure, drift and diffusion (i.e.,\nnonlocal Kramers-Moyal formulas), hence obtaining the stochastic governing law,\nfrom noisy data. Numerical experiments on several prototypical examples confirm\nthe efficacy and accuracy of this method. This method will become an effective\ntool in discovering the governing laws from available data sets and in\nunderstanding the mechanisms underlying complex random phenomena.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 14:50:36 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Li", "Yang", ""], ["Duan", "Jinqiao", ""]]}, {"id": "2107.10148", "submitter": "Jingyu Ji", "authors": "Jingyu Ji, Deyuan Li and Zhengjun Zhang", "title": "Decoupling Systemic Risk into Endopathic and Exopathic Competing Risks\n  Through Autoregressive Conditional Accelerated Fr\\'echet Model", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying systemic risk patterns in geopolitical, economic, financial,\nenvironmental, transportation, epidemiological systems, and their impacts is\nthe key to risk management. This paper introduces two new endopathic and\nexopathic competing risks. The paper integrates the new extreme value theory\nfor maxima of maxima and the autoregressive conditional Fr\\'echet model for\nsystemic risk into a new autoregressive conditional accelerated Fr\\'echet\n(AcAF) model, which enables decoupling systemic risk into endopathic and\nexopathic competing risks. The paper establishes the probabilistic properties\nof stationarity and ergodicity of the AcAF model. Statistical inference is\ndeveloped through conditional maximum likelihood estimation. The consistency\nand asymptotic normality of the estimators are derived. Simulation demonstrates\nthe efficiency of the proposed estimators and the AcAF model's flexibility in\nmodeling heterogeneous data. Empirical studies on the stock returns in S\\&P 500\nand the cryptocurrency trading show the superior performance of the proposed\nmodel in terms of the identified risk patterns, endopathic and exopathic\ncompeting risks, being informative with greater interpretability, enhancing the\nunderstanding of the systemic risks of a market and their causes, and making\nbetter risk management possible.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:25:23 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ji", "Jingyu", ""], ["Li", "Deyuan", ""], ["Zhang", "Zhengjun", ""]]}, {"id": "2107.10223", "submitter": "Bernardo Nipoti", "authors": "Antonio Canale, Antonio Lijoi, Bernardo Nipoti, Igor Pr\\\"unster", "title": "Inner spike and slab Bayesian nonparametric models", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Bayesian nonparametric models whose expectation is a convex linear\ncombination of a point mass at some point of the support and a diffuse\nprobability distribution allow to incorporate strong prior information, while\nstill being extremely flexible. Recent contributions in the statistical\nliterature have successfully implemented such a modelling strategy in a variety\nof applications, including density estimation, nonparametric regression and\nmodel-based clustering. We provide a thorough study of a large class of\nnonparametric models we call inner spike and slab hNRMI models, which are\nobtained by considering homogeneous normalized random measures with independent\nincrements (hNRMI) with base measure given by a convex linear combination of a\npoint mass and a diffuse probability distribution. In this paper we investigate\nthe distributional properties of these models and our results include: i) the\nexchangeable partition probability function they induce, ii) the distribution\nof the number of distinct values in an exchangeable sample, iii) the posterior\npredictive distribution, and iv) the distribution of the number of elements\nthat coincide with the only point of the support with positive probability. Our\nfindings are the main building block for an actual implementation of Bayesian\ninner spike and slab hNRMI models by means of a generalized P\\'olya urn scheme.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:22:58 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Canale", "Antonio", ""], ["Lijoi", "Antonio", ""], ["Nipoti", "Bernardo", ""], ["Pr\u00fcnster", "Igor", ""]]}, {"id": "2107.10343", "submitter": "Jian Huang", "authors": "Guohao Shen, Yuling Jiao, Yuanyuan Lin and Jian Huang", "title": "Robust Nonparametric Regression with Deep Neural Networks", "comments": "Guohao Shen and Yuling Jiao contributed equally to this work.\n  Corresponding authors: Yuanyuan Lin (Email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (Email: jian-huang@uiowa.edu). arXiv admin note: substantial text\n  overlap with arXiv:2104.06708", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the properties of robust nonparametric estimation\nusing deep neural networks for regression models with heavy tailed error\ndistributions. We establish the non-asymptotic error bounds for a class of\nrobust nonparametric regression estimators using deep neural networks with ReLU\nactivation under suitable smoothness conditions on the regression function and\nmild conditions on the error term. In particular, we only assume that the error\ndistribution has a finite p-th moment with p greater than one. We also show\nthat the deep robust regression estimators are able to circumvent the curse of\ndimensionality when the distribution of the predictor is supported on an\napproximate lower-dimensional set. An important feature of our error bound is\nthat, for ReLU neural networks with network width and network size (number of\nparameters) no more than the order of the square of the dimensionality d of the\npredictor, our excess risk bounds depend sub-linearly on d. Our assumption\nrelaxes the exact manifold support assumption, which could be restrictive and\nunrealistic in practice. We also relax several crucial assumptions on the data\ndistribution, the target regression function and the neural networks required\nin the recent literature. Our simulation studies demonstrate that the robust\nmethods can significantly outperform the least squares method when the errors\nhave heavy-tailed distributions and illustrate that the choice of loss function\nis important in the context of deep nonparametric regression.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 20:18:05 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Shen", "Guohao", ""], ["Jiao", "Yuling", ""], ["Lin", "Yuanyuan", ""], ["Huang", "Jian", ""]]}, {"id": "2107.10450", "submitter": "Sutanu Gayen", "authors": "Arnab Bhattacharyya, Davin Choo, Rishikesh Gajjala, Sutanu Gayen,\n  Yuhao Wang", "title": "Learning Sparse Fixed-Structure Gaussian Bayesian Networks", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation\nmodels) are widely used to model causal interactions among continuous\nvariables. In this work, we study the problem of learning a fixed-structure\nGaussian Bayesian network up to a bounded error in total variation distance. We\nanalyze the commonly used node-wise least squares regression (LeastSquares) and\nprove that it has a near-optimal sample complexity. We also study a couple of\nnew algorithms for the problem:\n  - BatchAvgLeastSquares takes the average of several batches of least squares\nsolutions at each node, so that one can interpolate between the batch size and\nthe number of batches. We show that BatchAvgLeastSquares also has near-optimal\nsample complexity.\n  - CauchyEst takes the median of solutions to several batches of linear\nsystems at each node. We show that the algorithm specialized to polytrees,\nCauchyEstTree, has near-optimal sample complexity.\n  Experimentally, we show that for uncontaminated, realizable data, the\nLeastSquares algorithm performs best, but in the presence of contamination or\nDAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares\nrespectively perform better.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:17:46 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Choo", "Davin", ""], ["Gajjala", "Rishikesh", ""], ["Gayen", "Sutanu", ""], ["Wang", "Yuhao", ""]]}, {"id": "2107.10578", "submitter": "Worachet Bukaew", "authors": "Worachet Bukaew and Sikarin Yoo-Kong", "title": "One-parameter generalised Fisher information", "comments": "10 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the generalised Fisher information or the one-parameter extended\nclass of the Fisher information. This new form of the Fisher information is\nobtained from the intriguing connection between the standard Fisher information\nand the variational principle together with the non-uniqueness property of the\nLagrangian. Furthermore, one could treat this one-parameter Fisher information\nas a generating function for obtaining what is called Fisher information\nhierarchy. The generalised Cramer-Rao inequality is also derived. The\ninteresting point is about the fact that the whole Fisher information\nhierarchy, except for the standard Fisher information, does not follow the\nadditive rule. This could suggest that there is an indirect connection between\nthe Tsallis entropy and the one-parameter Fisher information. Furthermore, the\nwhole Fisher information hierarchy is also obtained from the two-parameter\nKullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 11:17:52 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bukaew", "Worachet", ""], ["Yoo-Kong", "Sikarin", ""]]}, {"id": "2107.10654", "submitter": "Matthew Fahrbach", "authors": "Matthew Fahrbach, Mehrdad Ghadiri, Thomas Fu", "title": "Fast Low-Rank Tensor Decomposition by Ridge Leverage Score Sampling", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor decomposition generalizes low-rank matrix approximation and\nis a powerful technique for discovering low-dimensional structure in\nhigh-dimensional data. In this paper, we study Tucker decompositions and use\ntools from randomized numerical linear algebra called ridge leverage scores to\naccelerate the core tensor update step in the widely-used alternating least\nsquares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,\nis a highly-structured ridge regression problem where the design matrix is a\nKronecker product of the factor matrices. We show how to use approximate ridge\nleverage scores to construct a sketched instance for any ridge regression\nproblem such that the solution vector for the sketched problem is a\n$(1+\\varepsilon)$-approximation to the original instance. Moreover, we show\nthat classical leverage scores suffice as an approximation, which then allows\nus to exploit the Kronecker structure and update the core tensor in time that\ndepends predominantly on the rank and the sketching parameters (i.e., sublinear\nin the size of the input tensor). We also give upper bounds for ridge leverage\nscores as rows are removed from the design matrix (e.g., if the tensor has\nmissing entries), and we demonstrate the effectiveness of our approximate ridge\nregressioni algorithm for large, low-rank Tucker decompositions on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 13:32:47 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fahrbach", "Matthew", ""], ["Ghadiri", "Mehrdad", ""], ["Fu", "Thomas", ""]]}, {"id": "2107.10735", "submitter": "Qiuping Wang", "authors": "MengXu and Qiuping Wang", "title": "A network Poisson model for weighted directed networks with covariates", "comments": "24 pages, 5 tables, 2 figures. arXiv admin note: substantial text\n  overlap with arXiv:2106.03285; text overlap with arXiv:1609.04558 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edges in networks are not only binary, either present or absent, but also\ntake weighted values in many scenarios (e.g., the number of emails between two\nusers). The covariate-$p_0$ model has been proposed to model binary directed\nnetworks with the degree heterogeneity and covariates. However, it may cause\ninformation loss when it is applied in weighted networks. In this paper, we\npropose to use the Poisson distribution to model weighted directed networks,\nwhich admits the sparsity of networks, the degree heterogeneity and the\nhomophily caused by covariates of nodes. We call it the \\emph{network Poisson\nmodel}. The model contains a density parameter $\\mu$, a $2n$-dimensional node\nparameter ${\\theta}$ and a fixed dimensional regression coefficient ${\\gamma}$\nof covariates. Since the number of parameters increases with $n$, asymptotic\ntheory is nonstandard. When the number $n$ of nodes goes to infinity, we\nestablish the $\\ell_\\infty$-errors for the maximum likelihood estimators\n(MLEs), $\\hat{\\theta}$ and $\\hat{{\\gamma}}$, which are $O_p( (\\log n/n)^{1/2}\n)$ for $\\hat{\\theta}$ and $O_p( \\log n/n)$ for $\\hat{{\\gamma}}$, up to an\nadditional factor. We also obtain the asymptotic normality of the MLE.\nNumerical studies and a data analysis demonstrate our theoretical findings. )\nfor b{\\theta} and Op(log n/n) for b{\\gamma}, up to an additional factor. We\nalso obtain the asymptotic normality of the MLE. Numerical studies and a data\nanalysis demonstrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:42:06 GMT"}], "update_date": "2021-07-24", "authors_parsed": [["MengXu", "", ""], ["Wang", "Qiuping", ""]]}, {"id": "2107.10766", "submitter": "Damian Kozbur", "authors": "Damian Kozbur", "title": "Dimension-Free Anticoncentration Bounds for Gaussian Order Statistics\n  with Discussion of Applications to Multiple Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The following anticoncentration property is proved. The probability that the\n$k$-order statistic of an arbitrarily correlated jointly Gaussian random vector\n$X$ with unit variance components lies within an interval of length\n$\\varepsilon$ is bounded above by $2{\\varepsilon}k ({ 1+\\mathrm{E}[\\|X\\|_\\infty\n]}) $. This bound has implications for generalized error rate control in\nstatistical high-dimensional multiple hypothesis testing problems, which are\ndiscussed subsequently.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:49:15 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kozbur", "Damian", ""]]}, {"id": "2107.10885", "submitter": "Yanbo Tang", "authors": "Yanbo Tang and Nancy Reid", "title": "Laplace and Saddlepoint Approximations in High Dimensions", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine the behaviour of the Laplace and saddlepoint approximations in the\nhigh-dimensional setting, where the dimension of the model is allowed to\nincrease with the number of observations. Approximations to the joint density,\nthe marginal posterior density and the conditional density are considered. Our\nresults show that under the mildest assumptions on the model, the error of the\njoint density approximation is $O(p^4/n)$ if $p = o(n^{1/4})$ for the Laplace\napproximation and saddlepoint approximation, with improvements being possible\nunder additional assumptions. Stronger results are obtained for the\napproximation to the marginal posterior density.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:04:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tang", "Yanbo", ""], ["Reid", "Nancy", ""]]}, {"id": "2107.10955", "submitter": "Xiaodong Li", "authors": "Xingmei Lou, Yu Hu, Xiaodong Li", "title": "Linear Polytree Structural Equation Models: Structural Learning and\n  Inverse Correlation Estimation", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the problem of learning the directed acyclic graph (DAG)\nwhen data are generated from a linear structural equation model (SEM) and the\ncausal structure can be characterized by a polytree. Specially, under both\nGaussian and sub-Gaussian models, we study the sample size conditions for the\nwell-known Chow-Liu algorithm to exactly recover the equivalence class of the\npolytree, which is uniquely represented by a CPDAG. We also study the error\nrate for the estimation of the inverse correlation matrix under such models.\nOur theoretical findings are illustrated by comprehensive numerical\nsimulations, and experiments on benchmark data also demonstrate the robustness\nof the method when the ground truth graphical structure can only be\napproximated by a polytree.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 23:22:20 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lou", "Xingmei", ""], ["Hu", "Yu", ""], ["Li", "Xiaodong", ""]]}, {"id": "2107.10974", "submitter": "Zhiyong Zhou", "authors": "Zhiyong Zhou", "title": "A note on sharp oracle bounds for Slope and Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the sharp oracle bounds for Slope and Lasso and\ngeneralize the results in Bellec et al. (2018) to allow the case that the\nparameter vector is not exactly sparse and obtain the optimal bounds for\n$\\ell_q$ estimation errors with $1\\leq q\\leq \\infty$ by using some extended\nRestricted Eigenvalue type conditions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 00:51:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Zhou", "Zhiyong", ""]]}, {"id": "2107.11176", "submitter": "Ji-Eun Byun Dr", "authors": "Ji-Eun Byun, Johannes O. Royset", "title": "Data-driven optimization of reliability using buffered failure\n  probability", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Design and operation of complex engineering systems rely on reliability\noptimization. Such optimization requires us to account for uncertainties\nexpressed in terms of compli-cated, high-dimensional probability distributions,\nfor which only samples or data might be available. However, using data or\nsamples often degrades the computational efficiency, particularly as the\nconventional failure probability is estimated using the indicator function\nwhose gradient is not defined at zero. To address this issue, by leveraging the\nbuffered failure probability, the paper develops the buffered optimization and\nreliability method (BORM) for efficient, data-driven optimization of\nreliability. The proposed formulations, algo-rithms, and strategies greatly\nimprove the computational efficiency of the optimization and thereby address\nthe needs of high-dimensional and nonlinear problems. In addition, an\nanalytical formula is developed to estimate the reliability sensitivity, a\nsubject fraught with difficulty when using the conventional failure\nprobability. The buffered failure probability is thoroughly investigated in the\ncontext of many different distributions, leading to a novel measure of\ntail-heaviness called the buffered tail index. The efficiency and accuracy of\nthe proposed optimization methodology are demonstrated by three numerical\nexamples, which underline the unique advantages of the buffered failure\nprobability for data-driven reliability analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:56:00 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Byun", "Ji-Eun", ""], ["Royset", "Johannes O.", ""]]}, {"id": "2107.11203", "submitter": "Remy Messadene", "authors": "Thomas Cass, Remy Messadene", "title": "Signature asymptotics, empirical processes, and optimal transport", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rough path theory provides one with the notion of signature, a graded family\nof tensors which characterise, up to a negligible equivalence class, and\nordered stream of vector-valued data. In the last few years, use of the\nsignature has gained traction in time-series analysis, machine learning , deep\nlearning and more recently in kernel methods. In this article, we lay down the\ntheoretical foundations for a connection between signature asymptotics, the\ntheory of empirical processes, and Wasserstein distances, opening up the\nlandscape and toolkit of the second and third in the study of the first. Our\nmain contribution is to show that the Hambly-Lyons limit can be reinterpreted\nas a statement about the asymptotic behaviour of Wasserstein distances between\ntwo independent empirical measures of samples from the same underlying\ndistribution. In the setting studied here, these measures are derived from\nsamples from a probability distribution which is determined by geometrical\nproperties of the underlying path. The general question of rates of convergence\nfor these objects has been studied in depth in the recent monograph of Bobkov\nand Ledoux. By using these results, we generalise the original result of Hambly\nand Lyons from $C^3$ curves to a broad class of $C^2$ ones. We conclude by\nproviding an explicit way to compute the limit in terms of a second-order\ndifferential equation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 13:01:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cass", "Thomas", ""], ["Messadene", "Remy", ""]]}, {"id": "2107.11270", "submitter": "Efstathios Paparoditis", "authors": "Jens-Peter Kreiss, Efstathios Paparoditis", "title": "Bootstrapping Whittle Estimators", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting parametric models by optimizing frequency domain objective functions\nis an attractive approach of parameter estimation in time series analysis.\nWhittle estimators are a prominent example in this context. Under weak\nconditions and the (realistic) assumption that the true spectral density of the\nunderlying process does not necessarily belong to the parametric class of\nspectral densities fitted, the distribution of Whittle estimators typically\ndepends on difficult to estimate characteristics of the underlying process.\nThis makes the implementation of asymptotic results for the construction of\nconfidence intervals or for assessing the variability of estimators, difficult\nin practice. This paper proposes a frequency domain bootstrap method to\nestimate the distribution of Whittle estimators which is asymptotically valid\nunder assumptions that not only allow for (possible) model misspecification but\nalso for weak dependence conditions which are satisfied by a wide range of\nstationary stochastic processes. Adaptions of the bootstrap procedure developed\nto incorporate different modifications of Whittle estimators proposed in the\nliterature, like for instance, tapered, de-biased or boundary extended Whittle\nestimators, are also considered. Simulations demonstrate the capabilities of\nthe bootstrap method proposed and its good finite sample performance. A\nreal-life data analysis also is presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:39:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kreiss", "Jens-Peter", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2107.11329", "submitter": "Ana Luc\\'ia Garc\\'ia-Pulido", "authors": "Ana Lucia Garcia-Pulido, Kathryn Hess, Jane Tan, Katharine Turner, Bei\n  Wang and Naya Yerolemou", "title": "Graph Pseudometrics from a Topological Point of View", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT math.MG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore pseudometrics for directed graphs in order to better understand\ntheir topological properties. The directed flag complex associated to a\ndirected graph provides a useful bridge between network science and topology.\nIndeed, it has often been observed that phenomena exhibited by real-world\nnetworks reflect the topology of their flag complexes, as measured, for\nexample, by Betti numbers or simplex counts. As it is often computationally\nexpensive (or even unfeasible) to determine such topological features exactly,\nit would be extremely valuable to have pseudometrics on the set of directed\ngraphs that can both detect the topological differences and be computed\nefficiently.\n  To facilitate work in this direction, we introduce methods to measure how\nwell a graph pseudometric captures the topology of a directed graph. We then\nuse these methods to evaluate some well-established pseudometrics, using test\ndata drawn from several families of random graphs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:18:28 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Garcia-Pulido", "Ana Lucia", ""], ["Hess", "Kathryn", ""], ["Tan", "Jane", ""], ["Turner", "Katharine", ""], ["Wang", "Bei", ""], ["Yerolemou", "Naya", ""]]}, {"id": "2107.11403", "submitter": "Gecia Bravo-Hermsdorff", "authors": "Gecia Bravo-Hermsdorff, Lee M. Gunderson, Pierre-Andr\\'e Maugis, Carey\n  E. Priebe", "title": "A principled (and practical) test for network comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How might one test the hypothesis that graphs were sampled from the same\ndistribution? Here, we compare two statistical tests that address this\nquestion. The first uses the observed subgraph densities themselves as\nestimates of those of the underlying distribution. The second test uses a new\napproach that converts these subgraph densities into estimates of the graph\ncumulants of the distribution. We demonstrate -- via theory, simulation, and\napplication to real data -- the superior statistical power of using graph\ncumulants.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:10:59 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 03:02:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Bravo-Hermsdorff", "Gecia", ""], ["Gunderson", "Lee M.", ""], ["Maugis", "Pierre-Andr\u00e9", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2107.11538", "submitter": "Xiaochao Xia", "authors": "Xiaochao Xia", "title": "A Robust Partial Correlation-based Screening Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a computationally fast and working efficient tool, sure independence\nscreening has received much attention in solving ultrahigh dimensional\nproblems. This paper contributes two robust sure screening approaches that\nsimultaneously take into account heteroscedasticity, outliers, heavy-tailed\ndistribution, continuous or discrete response, and confounding effect, from the\nperspective of model-free. First, we define a robust correlation measure only\nusing two random indicators, and introduce a screener using that correlation.\nSecond, we propose a robust partial correlation-based screening approach when\nan exposure variable is available. To remove the confounding effect of the\nexposure on both response and each covariate, we use a nonparametric regression\nwith some specified loss function. More specifically, a robust\ncorrelation-based screening method (RC-SIS) and a robust partial\ncorrelation-based screening framework (RPC-SIS) including two concrete\nscreeners: RPC-SIS(L2) and RPC-SIS(L1), are formed. Third, we establish sure\nscreening properties of RC-SIS for which the response variable can be either\ncontinuous or discrete, as well as those of RPC-SIS(L2) and RPC-SIS(L1) under\nsome regularity conditions. Our approaches are essentially nonparametric, and\nperform robustly for both the response and the covariates. Finally, extensive\nsimulation studies and two applications are carried out to demonstrate the\nsuperiority of our proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:40:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xia", "Xiaochao", ""]]}, {"id": "2107.11546", "submitter": "Nilanjana Laha", "authors": "Nilanjana Laha, Zoe Moodie, Ying Huang, Alex Luedtke", "title": "Improved inference for vaccine-induced immune responses via\n  shape-constrained methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the performance of shape-constrained methods for evaluating immune\nresponse profiles from early-phase vaccine trials. The motivating problem for\nthis work involves quantifying and comparing the IgG binding immune responses\nto the first and second variable loops (V1V2 region) arising in HVTN 097 and\nHVTN 100 HIV vaccine trials. We consider unimodal and log-concave\nshape-constrained methods to compare the immune profiles of the two vaccines,\nwhich is reasonable because the data support that the underlying densities of\nthe immune responses could have these shapes. To this end, we develop novel\nshape-constrained tests of stochastic dominance and shape-constrained plug-in\nestimators of the Hellinger distance between two densities. Our techniques are\neither tuning parameter free, or rely on only one tuning parameter, but their\nperformance is either better (the tests of stochastic dominance) or comparable\nwith the nonparametric methods (the estimators of Hellinger distance). The\nminimal dependence on tuning parameters is especially desirable in clinical\ncontexts where analyses must be prespecified and reproducible. Our methods are\nsupported by theoretical results and simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 06:47:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Laha", "Nilanjana", ""], ["Moodie", "Zoe", ""], ["Huang", "Ying", ""], ["Luedtke", "Alex", ""]]}, {"id": "2107.11565", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "On the Le Cam distance between multivariate hypergeometric and\n  multivariate normal experiments", "comments": "6 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we develop a local approximation for the log-ratio of the\nmultivariate hypergeometric probability mass function over the corresponding\nmultinomial probability mass function. In conjunction with the bounds from\nCarter (2002) and Ouimet (2021) on the total variation between the law of a\nmultinomial vector jittered by a uniform on $(-1/2,1/2)^d$ and the law of the\ncorresponding multivariate normal distribution, the local expansion for the\nlog-ratio is then used to obtain a total variation bound between the law of a\nmultivariate hypergeometric random vector jittered by a uniform on\n$(-1/2,1/2)^d$ and the law of the corresponding multivariate normal\ndistribution. As a corollary, we find an upper bound on the Le Cam distance\nbetween multivariate hypergeometric and multivariate normal experiments.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 09:27:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2107.11858", "submitter": "Kevin O'Connor", "authors": "Kevin O'Connor, Kevin McGoff, Andrew B Nobel", "title": "Estimation of Stationary Optimal Transport Plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal transport problems in which finite-valued quantities of\ninterest evolve dynamically over time in a stationary fashion. Mathematically,\nthis is a special case of the general optimal transport problem in which the\ndistributions under study represent stationary processes and the cost depends\non a finite number of time points. In this setting, we argue that one should\nrestrict attention to stationary couplings, also known as joinings, which have\nclose connections with long run average cost. We introduce estimators of both\noptimal joinings and the optimal joining cost, and we establish their\nconsistency under mild conditions. Under stronger mixing assumptions we\nestablish finite-sample error rates for the same estimators that extend the\nbest known results in the iid case. Finally, we extend the consistency and rate\nanalysis to an entropy-penalized version of the optimal joining problem.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 17:46:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["O'Connor", "Kevin", ""], ["McGoff", "Kevin", ""], ["Nobel", "Andrew B", ""]]}, {"id": "2107.11925", "submitter": "Ting-Kam Leonard Wong", "authors": "Ting-Kam Leonard Wong, Jun Zhang", "title": "Tsallis and R\\'{e}nyi deformations linked via a new $\\lambda$-duality", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsallis and R\\'{e}nyi entropies, which are monotone transformations of such\nother, generalize the classical Shannon entropy and the exponential family of\nprobability distributions to non-extensive statistical physics, information\ntheory, and statistics. The $q$-exponential family, as a deformed exponential\nfamily with subtractive normalization, nevertheless reflects the classical\nLegendre duality of convex functions as well as the associated concept of\nBregman divergence. In this paper we show that a generalized $\\lambda$-duality,\nwhere $\\lambda = 1 - q$ is the constant information-geometric curvature,\ninduces a deformed exponential family with divisive normalization and links to\nR\\'{e}nyi entropy and optimal transport. Our $\\lambda$-duality unifies the two\ndeformation models, which differ by a mere reparameterization, and provides an\nelegant and deep framework to study the underlying mathematical structure.\nUsing this duality, under which the R\\'{e}nyi entropy and divergence appear\nnaturally, the $\\lambda$-exponential family satisfies properties that parallel\nand generalize those of the exponential family. In particular, we give a new\nproof of the Tsallis entropy maximizing property of the $q$-exponential family.\nWe also introduce a $\\lambda$-mixture family which may be regared as the dual\nof the $\\lambda$-exponential family. Finally, we discuss a duality between the\n$\\lambda$-exponential family and the logarithmic divergence, and study its\nstatistical consequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 02:11:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wong", "Ting-Kam Leonard", ""], ["Zhang", "Jun", ""]]}, {"id": "2107.11998", "submitter": "Mohd Arshad", "authors": "Ashok Kumar Pathak, Mohd. Arshad, Qazi J. Azhad, Mukti Khetan and\n  Arvind Pandey", "title": "A Novel Bivariate Generalized Weibull Distribution with Properties and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Univariate Weibull distribution is a well-known lifetime distribution and has\nbeen widely used in reliability and survival analysis. In this paper, we\nintroduce a new family of bivariate generalized Weibull (BGW) distributions,\nwhose univariate marginals are exponentiated Weibull distribution. Different\nstatistical quantiles like marginals, conditional distribution, conditional\nexpectation, product moments, correlation and a measure component reliability\nare derived. Various measures of dependence and statistical properties along\nwith ageing properties are examined. Further, the copula associated with BGW\ndistribution and its various important properties are also considered. The\nmethods of maximum likelihood and Bayesian estimation are employed to estimate\nunknown parameters of the model. A Monte Carlo simulation and real data study\nare carried out to demonstrate the performance of the estimators and results\nhave proven the effectiveness of the distribution in real-life situations\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:17:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pathak", "Ashok Kumar", ""], ["Arshad", "Mohd.", ""], ["Azhad", "Qazi J.", ""], ["Khetan", "Mukti", ""], ["Pandey", "Arvind", ""]]}, {"id": "2107.12011", "submitter": "Yannick Baraud", "authors": "Yannick Baraud", "title": "From robust tests to Bayes-like posterior distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Bayes paradigm and for a given loss function, we propose the\nconstruction of a new type of posterior distributions for estimating the law of\nan $n$-sample. The loss functions we have in mind are based on the total\nvariation distance, the Hellinger distance as well as some\n$\\mathbb{L}_{j}$-distances. We prove that, with a probability close to one,\nthis new posterior distribution concentrates its mass in a neighbourhood of the\nlaw of the data, for the chosen loss function, provided that this law belongs\nto the support of the prior or, at least, lies close enough to it. We therefore\nestablish that the new posterior distribution enjoys some robustness properties\nwith respect to a possible misspecification of the prior, or more precisely,\nits support. For the total variation and squared Hellinger losses, we also show\nthat the posterior distribution keeps its concentration properties when the\ndata are only independent, hence not necessarily i.i.d., provided that most of\ntheir marginals are close enough to some probability distribution around which\nthe prior puts enough mass. The posterior distribution is therefore also stable\nwith respect to the equidistribution assumption. We illustrate these results by\nseveral applications. We consider the problems of estimating a location\nparameter or both the location and the scale of a density in a nonparametric\nframework. Finally, we also tackle the problem of estimating a density, with\nthe squared Hellinger loss, in a high-dimensional parametric model under some\nsparcity conditions. The results established in this paper are non-asymptotic\nand provide, as much as possible, explicit constants.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:02:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Baraud", "Yannick", ""]]}, {"id": "2107.12058", "submitter": "Antoine Godichon-Baggioni", "authors": "Antoine Godichon-Baggioni (LPSM)", "title": "Convergence in quadratic mean of averaged stochastic gradient algorithms\n  without strong convexity nor bounded gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online averaged stochastic gradient algorithms are more and more studied\nsince (i) they can deal quickly with large sample taking values in high\ndimensional spaces, (ii) they enable to treat data sequentially, (iii) they are\nknown to be asymptotically efficient. In this paper, we focus on giving\nexplicit bounds of the quadratic mean error of the estimates, and this, with\nvery weak assumptions, i.e without supposing that the function we would like to\nminimize is strongly convex or admits a bounded gradient.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:27:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Godichon-Baggioni", "Antoine", "", "LPSM"]]}, {"id": "2107.12364", "submitter": "Tudor Manole", "authors": "Tudor Manole, Sivaraman Balakrishnan, Jonathan Niles-Weed, Larry\n  Wasserman", "title": "Plugin Estimation of Smooth Optimal Transport Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a number of natural estimators for the optimal transport map\nbetween two distributions and show that they are minimax optimal. We adopt the\nplugin approach: our estimators are simply optimal couplings between measures\nderived from our observations, appropriately extended so that they define\nfunctions on $\\mathbb{R}^d$. When the underlying map is assumed to be\nLipschitz, we show that computing the optimal coupling between the empirical\nmeasures, and extending it using linear smoothers, already gives a minimax\noptimal estimator. When the underlying map enjoys higher regularity, we show\nthat the optimal coupling between appropriate nonparametric density estimates\nyields faster rates. Our work also provides new bounds on the risk of\ncorresponding plugin estimators for the quadratic Wasserstein distance, and we\nshow how this problem relates to that of estimating optimal transport maps\nusing stability arguments for smooth and strongly convex Brenier potentials. As\nan application of our results, we derive a central limit theorem for a density\nplugin estimator of the squared Wasserstein distance, which is centered at its\npopulation counterpart when the underlying distributions have sufficiently\nsmooth densities. In contrast to known central limit theorems for empirical\nestimators, this result easily lends itself to statistical inference for\nWasserstein distances.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:58:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Manole", "Tudor", ""], ["Balakrishnan", "Sivaraman", ""], ["Niles-Weed", "Jonathan", ""], ["Wasserman", "Larry", ""]]}, {"id": "2107.12365", "submitter": "Yuling Yan", "authors": "Yuling Yan, Yuxin Chen, Jianqing Fan", "title": "Inference for Heteroskedastic PCA with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to construct confidence regions for principal\ncomponent analysis (PCA) in high dimension, a problem that has been vastly\nunder-explored. While computing measures of uncertainty for nonlinear/nonconvex\nestimators is in general difficult in high dimension, the challenge is further\ncompounded by the prevalent presence of missing data and heteroskedastic noise.\nWe propose a suite of solutions to perform valid inference on the principal\nsubspace based on two estimators: a vanilla SVD-based approach, and a more\nrefined iterative scheme called $\\textsf{HeteroPCA}$ (Zhang et al., 2018). We\ndevelop non-asymptotic distributional guarantees for both estimators, and\ndemonstrate how these can be invoked to compute both confidence regions for the\nprincipal subspace and entrywise confidence intervals for the spiked covariance\nmatrix. Particularly worth highlighting is the inference procedure built on top\nof $\\textsf{HeteroPCA}$, which is not only valid but also statistically\nefficient for broader scenarios (e.g., it covers a wider range of missing rates\nand signal-to-noise ratios). Our solutions are fully data-driven and adaptive\nto heteroskedastic random noise, without requiring prior knowledge about the\nnoise levels and noise distributions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:59:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yan", "Yuling", ""], ["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""]]}, {"id": "2107.12420", "submitter": "Zhaonan Qu", "authors": "Zhaonan Qu, Ruoxuan Xiong, Jizhou Liu, Guido Imbens", "title": "Efficient Treatment Effect Estimation in Observational Studies under\n  Heterogeneous Partial Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many observational studies in social science and medical applications,\nsubjects or individuals are connected, and one unit's treatment and attributes\nmay affect another unit's treatment and outcome, violating the stable unit\ntreatment value assumption (SUTVA) and resulting in interference. To enable\nfeasible inference, many previous works assume the ``exchangeability'' of\ninterfering units, under which the effect of interference is captured by the\nnumber or ratio of treated neighbors. However, in many applications with\ndistinctive units, interference is heterogeneous. In this paper, we focus on\nthe partial interference setting, and restrict units to be exchangeable\nconditional on observable characteristics. Under this framework, we propose\ngeneralized augmented inverse propensity weighted (AIPW) estimators for general\ncausal estimands that include direct treatment effects and spillover effects.\nWe show that they are consistent, asymptotically normal, semiparametric\nefficient, and robust to heterogeneous interference as well as model\nmisspecifications. We also apply our method to the Add Health dataset and find\nthat smoking behavior exhibits interference on academic outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:27:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Qu", "Zhaonan", ""], ["Xiong", "Ruoxuan", ""], ["Liu", "Jizhou", ""], ["Imbens", "Guido", ""]]}, {"id": "2107.12494", "submitter": "Zheng Fang", "authors": "Zheng Fang", "title": "A Unifying Framework for Testing Shape Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes the following econometric contributions. First, we develop a\nunifying framework for testing shape restrictions based on the Wald principle.\nSecond, we examine the applicability and usefulness of some prominent shape\nenforcing operators in implementing our test, including rearrangement and the\ngreatest convex minorization (or the least concave majorization). In\nparticular, the influential rearrangement operator is inapplicable due to a\nlack of convexity, while the greatest convex minorization is shown to enjoy the\nanalytic properties required to employ our framework. The importance of\nconvexity in establishing size control has been noted elsewhere in the\nliterature. Third, we show that, despite that the projection operator may not\nbe well-defined/behaved in general non-Hilbert parameter spaces (e.g., ones\ndefined by uniform norms), one may nonetheless devise a powerful distance-based\ntest by applying our framework. The finite sample performance of our test is\nevaluated through Monte Carlo simulations, and its empirical relevance is\nshowcased by investigating the relationship between weekly working hours and\nthe annual wage growth in the high-end labor market.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:57:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Fang", "Zheng", ""]]}, {"id": "2107.12525", "submitter": "Daniel Kang", "authors": "Daniel Kang, John Guibas, Peter Bailis, Tatsunori Hashimoto, Yi Sun,\n  Matei Zaharia", "title": "Proof: Accelerating Approximate Aggregation Queries with Expensive\n  Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DB cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset $\\mathcal{D}$, we are interested in computing the mean of a\nsubset of $\\mathcal{D}$ which matches a predicate. ABae leverages stratified\nsampling and proxy models to efficiently compute this statistic given a\nsampling budget $N$. In this document, we theoretically analyze ABae and show\nthat the MSE of the estimate decays at rate $O(N_1^{-1} + N_2^{-1} +\nN_1^{1/2}N_2^{-3/2})$, where $N=K \\cdot N_1+N_2$ for some integer constant $K$\nand $K \\cdot N_1$ and $N_2$ represent the number of samples used in Stage 1 and\nStage 2 of ABae respectively. Hence, if a constant fraction of the total sample\nbudget $N$ is allocated to each stage, we will achieve a mean squared error of\n$O(N^{-1})$ which matches the rate of mean squared error of the optimal\nstratified sampling algorithm given a priori knowledge of the predicate\npositive rate and standard deviation per stratum.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 00:18:21 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 18:29:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kang", "Daniel", ""], ["Guibas", "John", ""], ["Bailis", "Peter", ""], ["Hashimoto", "Tatsunori", ""], ["Sun", "Yi", ""], ["Zaharia", "Matei", ""]]}, {"id": "2107.12554", "submitter": "Aldo Taranto", "authors": "Aldo Taranto, Ron Addie, Shahjahan Khan", "title": "Bi-Directional Grid Constrained Stochastic Processes' Link to Multi-Skew\n  Brownian Motion", "comments": "Manuscript accepted for publication in the Journal of Applied\n  Probability & Statistics and will appear in issue 1 of volume 17 to be\n  published in April 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-Directional Grid Constrained (BGC) stochastic processes (BGCSPs) constrain\nthe random movement toward the origin steadily more and more, the further they\ndeviate from the origin, rather than all at once imposing reflective barriers,\nas does the well-established theory of It^o diffusions with such reflective\nbarriers. We identify that BGCSPs are a variant rather than a special case of\nthe multi-skew Brownian motion (M-SBM). This is because they have their own\ncomplexities, such as the barriers being hidden (not known in advance) and not\nnecessarily constant over time. We provide an M-SBM theoretical framework and\nalso a simulation framework to elaborate deeper properties of BGCSPs. The\nsimulation framework is then applied by generating numerous simulations of the\nconstrained paths and the results are analysed. BGCSPs have applications in\nfinance and indeed many other fields requiring graduated constraining, from\nboth above and below the initial position.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:12:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Taranto", "Aldo", ""], ["Addie", "Ron", ""], ["Khan", "Shahjahan", ""]]}, {"id": "2107.12649", "submitter": "Martin Kroll", "authors": "L\\'aszl\\'o Gy\\\"orfi and Martin Kroll", "title": "$L_1$ density estimation from privatised data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical problem of nonparametric density estimation, but\nimpose local differential privacy constraints. Under such constraints, the\noriginal data $X_1,\\ldots,X_n$, taking values in $\\mathbb{R}^d $, cannot be\ndirectly observed, and all estimators are functions of the randomised output of\na suitable privacy mechanism. The statistician is free to choose the form of\nthe privacy mechanism, and in this work we propose to add Laplace distributed\nnoise to a discretisation of the location of a vector $X_i$. Based on these\nrandomised data, we design a novel estimator of the density function, which can\nbe viewed as a privatised version of the well-studied histogram density\nestimator. Our theoretical results include universal pointwise consistency and\nstrong universal $L_1$-consistency. In addition, a convergence rate over\nclasses of Lipschitz functions is derived, which is complemented by a matching\nminimax lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:00:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Kroll", "Martin", ""]]}, {"id": "2107.12723", "submitter": "Dominic Richards", "authors": "Dominic Richards, Ilja Kuzborskij", "title": "Stability & Generalisation of Gradient Descent for Shallow Neural\n  Networks without the Neural Tangent Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit on-average algorithmic stability of Gradient Descent (GD) for\ntraining overparameterised shallow neural networks and prove new generalisation\nand excess risk bounds without the Neural Tangent Kernel (NTK) or\nPolyak-{\\L}ojasiewicz (PL) assumptions. In particular, we show oracle type\nbounds which reveal that the generalisation and excess risk of GD is controlled\nby an interpolating network with the shortest GD path from initialisation (in a\nsense, an interpolating network with the smallest relative norm). While this\nwas known for kernelised interpolants, our proof applies directly to networks\ntrained by GD without intermediate kernelisation. At the same time, by relaxing\noracle inequalities developed here we recover existing NTK-based risk bounds in\na straightforward way, which demonstrates that our analysis is tighter.\nFinally, unlike most of the NTK-based analyses we focus on regression with\nlabel noise and show that GD with early stopping is consistent.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:53:15 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Richards", "Dominic", ""], ["Kuzborskij", "Ilja", ""]]}, {"id": "2107.13205", "submitter": "Lan Gao", "authors": "Lan Gao and Qi-Man Shao and Jiasheng Shi", "title": "Refined Cram\\'er Type Moderate Deviation Theorems for General\n  Self-normalized Sums with Applications to Dependent Random Variables and\n  Winsorized Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let {(X_i,Y_i)}_{i=1}^n be a sequence of independent bivariate random\nvectors. In this paper, we establish a refined Cram\\'er type moderate deviation\ntheorem for the general self-normalized sum \\sum_{i=1}^n X_i/(\\sum_{i=1}^n\nY_i^2)^{1/2}, which unifies and extends the classical Cram\\'er (1938) theorem\nand the self-normalized Cram\\'er type moderate deviation theorems by Jing, Shao\nand Wang (2003) as well as the further refined version by Wang (2011). The\nadvantage of our result is evidenced through successful applications to weakly\ndependent random variables and self-normalized winsorized mean. Specifically,\nby applying our new framework on general self-normalized sum, we significantly\nimprove Cram\\'er type moderate deviation theorems for one-dependent random\nvariables, geometrically \\beta-mixing random variables and causal processes\nunder geometrical moment contraction. As an additional application, we also\nderive the Cram\\'er type moderate deviation theorems for self-normalized\nwinsorized mean.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 07:35:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gao", "Lan", ""], ["Shao", "Qi-Man", ""], ["Shi", "Jiasheng", ""]]}, {"id": "2107.13289", "submitter": "El Mehdi Achour", "authors": "El Mehdi Achour (IMT), Fran\\c{c}ois Malgouyres, S\\'ebastien\n  Gerchinovitz", "title": "Global minimizers, strict and non-strict saddle points, and implicit\n  regularization for deep linear neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-convex settings, it is established that the behavior of gradient-based\nalgorithms is different in the vicinity of local structures of the objective\nfunction such as strict and non-strict saddle points, local and global minima\nand maxima. It is therefore crucial to describe the landscape of non-convex\nproblems. That is, to describe as well as possible the set of points of each of\nthe above categories. In this work, we study the landscape of the empirical\nrisk associated with deep linear neural networks and the square loss. It is\nknown that, under weak assumptions, this objective function has no spurious\nlocal minima and no local maxima. We go a step further and characterize, among\nall critical points, which are global minimizers, strict saddle points, and\nnon-strict saddle points. We enumerate all the associated critical values. The\ncharacterization is simple, involves conditions on the ranks of partial matrix\nproducts, and sheds some light on global convergence or implicit regularization\nthat have been proved or observed when optimizing a linear neural network. In\npassing, we also provide an explicit parameterization of the set of all global\nminimizers and exhibit large sets of strict and non-strict saddle points.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:33:18 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Achour", "El Mehdi", "", "IMT"], ["Malgouyres", "Fran\u00e7ois", ""], ["Gerchinovitz", "S\u00e9bastien", ""]]}, {"id": "2107.13291", "submitter": "Antoine Chambaz", "authors": "Geoffrey Ecoto (MAP5 - UMR 8145), Aur\\'elien Bibaut, Antoine Chambaz\n  (MAP5 - UMR 8145)", "title": "One-step ahead sequential Super Learning from short times series of many\n  slightly dependent data, and anticipating the cost of natural disasters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe a short time series where each time-t-specific\ndata-structure consists of many slightly dependent data indexed by a and that\nwe want to estimate a feature of the law of the experiment that depends neither\non t nor on a. We develop and study an algorithm to learn sequentially which\nbase algorithm in a user-supplied collection best carries out the estimation\ntask in terms of excess risk and oracular inequalities. The analysis, which\nuses dependency graph to model the amount of conditional independence within\neach t-specific data-structure and a concentration inequality by Janson [2004],\nleverages a large ratio of the number of distinct a's to the degree of the\ndependency graph in the face of a small number of t-specific data-structures.\nThe so-called one-step ahead Super Learner is applied to the motivating example\nwhere the challenge is to anticipate the cost of natural disasters in France.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 11:35:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ecoto", "Geoffrey", "", "MAP5 - UMR 8145"], ["Bibaut", "Aur\u00e9lien", "", "MAP5 - UMR 8145"], ["Chambaz", "Antoine", "", "MAP5 - UMR 8145"]]}, {"id": "2107.13369", "submitter": "Arnaud Guyader", "authors": "Lucie Bernard (IDP), Albert Cohen (LJLL (UMR\\_7598)), Arnaud Guyader\n  (LPSM (UMR\\_8001), CERMICS), Florent Malrieu (IDP)", "title": "Recursive Estimation of a Failure Probability for a Lipschitz Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let g : $\\Omega$ = [0, 1] d $\\rightarrow$ R denote a Lipschitz function that\ncan be evaluated at each point, but at the price of a heavy computational time.\nLet X stand for a random variable with values in $\\Omega$ such that one is able\nto simulate, at least approximately, according to the restriction of the law of\nX to any subset of $\\Omega$. For example, thanks to Markov chain Monte Carlo\ntechniques, this is always possible when X admits a density that is known up to\na normalizing constant. In this context, given a deterministic threshold T such\nthat the failure probability p := P(g(X) > T) may be very low, our goal is to\nestimate the latter with a minimal number of calls to g. In this aim, building\non Cohen et al. [9], we propose a recursive and optimal algorithm that selects\non the fly areas of interest and estimate their respective probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:56:49 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bernard", "Lucie", "", "IDP"], ["Cohen", "Albert", "", "LJLL"], ["Guyader", "Arnaud", "", "LPSM"], ["Malrieu", "Florent", "", "IDP"]]}, {"id": "2107.13422", "submitter": "Jakob Zech", "authors": "Jakob Zech and Youssef Marzouk", "title": "Sparse approximation of triangular transports. Part II: the infinite\n  dimensional case", "comments": "The original manuscript arXiv:2006.06994v1 has been split into two\n  parts; the present paper is the second part", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two probability measures $\\rho$ and $\\pi$ on $[-1,1]^{\\mathbb{N}}$ we\ninvestigate the approximation of the triangular Knothe-Rosenblatt transport\n$T:[-1,1]^{\\mathbb{N}}\\to [-1,1]^{\\mathbb{N}}$ that pushes forward $\\rho$ to\n$\\pi$. Under suitable assumptions, we show that $T$ can be approximated by\nrational functions without suffering from the curse of dimension. Our results\nare applicable to posterior measures arising in certain inference problems\nwhere the unknown belongs to an (infinite dimensional) Banach space. In\nparticular, we show that it is possible to efficiently approximately sample\nfrom certain high-dimensional measures by transforming a lower-dimensional\nlatent variable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:10:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zech", "Jakob", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2107.13494", "submitter": "Ziv Goldfeld", "authors": "Ritwik Sadhu and Ziv Goldfeld and Kengo Kato", "title": "Limit Distribution Theory for the Smooth 1-Wasserstein Distance with\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The smooth 1-Wasserstein distance (SWD) $W_1^\\sigma$ was recently proposed as\na means to mitigate the curse of dimensionality in empirical approximation\nwhile preserving the Wasserstein structure. Indeed, SWD exhibits parametric\nconvergence rates and inherits the metric and topological structure of the\nclassic Wasserstein distance. Motivated by the above, this work conducts a\nthorough statistical study of the SWD, including a high-dimensional limit\ndistribution result for empirical $W_1^\\sigma$, bootstrap consistency,\nconcentration inequalities, and Berry-Esseen type bounds. The derived\nnondegenerate limit stands in sharp contrast with the classic empirical $W_1$,\nfor which a similar result is known only in the one-dimensional case. We also\nexplore asymptotics and characterize the limit distribution when the smoothing\nparameter $\\sigma$ is scaled with $n$, converging to $0$ at a sufficiently slow\nrate. The dimensionality of the sampled distribution enters empirical SWD\nconvergence bounds only through the prefactor (i.e., the constant). We provide\na sharp characterization of this prefactor's dependence on the smoothing\nparameter and the intrinsic dimension. This result is then used to derive new\nempirical convergence rates for classic $W_1$ in terms of the intrinsic\ndimension. As applications of the limit distribution theory, we study\ntwo-sample testing and minimum distance estimation (MDE) under $W_1^\\sigma$. We\nestablish asymptotic validity of SWD testing, while for MDE, we prove\nmeasurability, almost sure convergence, and limit distributions for optimal\nestimators and their corresponding $W_1^\\sigma$ error. Our results suggest that\nthe SWD is well suited for high-dimensional statistical learning and inference.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:02:24 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:39:02 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sadhu", "Ritwik", ""], ["Goldfeld", "Ziv", ""], ["Kato", "Kengo", ""]]}, {"id": "2107.13656", "submitter": "Gholamali Aminian", "authors": "Gholamali Aminian, Yuheng Bu, Laura Toni, Miguel R. D. Rodrigues and\n  Gregory Wornell", "title": "Characterizing the Generalization Error of Gibbs Algorithm with\n  Symmetrized KL information", "comments": "The first and second author have contributed equally to the paper.\n  This paper is accepted in the ICML-21 Workshop on Information-Theoretic\n  Methods for Rigorous, Responsible, and Reliable Machine Learning:\n  https://sites.google.com/view/itr3/schedule", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bounding the generalization error of a supervised learning algorithm is one\nof the most important problems in learning theory, and various approaches have\nbeen developed. However, existing bounds are often loose and lack of\nguarantees. As a result, they may fail to characterize the exact generalization\nability of a learning algorithm. Our main contribution is an exact\ncharacterization of the expected generalization error of the well-known Gibbs\nalgorithm in terms of symmetrized KL information between the input training\nsamples and the output hypothesis. Such a result can be applied to tighten\nexisting expected generalization error bound. Our analysis provides more\ninsight on the fundamental role the symmetrized KL information plays in\ncontrolling the generalization error of the Gibbs algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 22:20:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Aminian", "Gholamali", ""], ["Bu", "Yuheng", ""], ["Toni", "Laura", ""], ["Rodrigues", "Miguel R. D.", ""], ["Wornell", "Gregory", ""]]}, {"id": "2107.13756", "submitter": "Yuting Ye", "authors": "Yuting Ye, Peter J. Bickel", "title": "Binomial Mixture Model With U-shape Constraint", "comments": "45 pages, 26 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the binomial mixture model under the regime that\nthe binomial size $m$ can be relatively large compared to the sample size $n$.\nThis project is motivated by the GeneFishing method (Liu et al., 2019), whose\noutput is a combination of the parameter of interest and the subsampling noise.\nTo tackle the noise in the output, we utilize the observation that the density\nof the output has a U shape and model the output with the binomial mixture\nmodel under a U shape constraint. We first analyze the estimation of the\nunderlying distribution F in the binomial mixture model under various\nconditions for F. Equipped with these theoretical understandings, we propose a\nsimple method Ucut to identify the cutoffs of the U shape and recover the\nunderlying distribution based on the Grenander estimator (Grenander, 1956). It\nhas been shown that when $m = {\\Omega}(n^{\\frac{2}{3}})$, he identified cutoffs\nconverge at the rate $O(n^{-\\frac{1}{3}})$. The $L_1$ distance between the\nrecovered distribution and the true one decreases at the same rate. To\ndemonstrate the performance, we apply our method to varieties of simulation\nstudies, a GTEX dataset used in (Liu et al., 2019) and a single cell dataset\nfrom Tabula Muris.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 05:53:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ye", "Yuting", ""], ["Bickel", "Peter J.", ""]]}, {"id": "2107.14004", "submitter": "Masatoshi Goda", "authors": "Masatoshi Goda", "title": "Sparse estimation for generalized exponential marked Hawkes process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have established a sparse estimation method for the generalized\nexponential marked Hawkes process by the penalized method to the ordinary\nmethod (P-O) estimator. Furthermore, we evaluated the probability of correct\nvariable selection. In order to achieve this, we established a framework for a\nlikelihood analysis and the P-O estimation when there might be nuisance\nparameters and the true value of the parameter could be realized at the\nboundary of the parameter space. Numerical simulations are given for several\nimportant examples.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:21:28 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Goda", "Masatoshi", ""]]}, {"id": "2107.14021", "submitter": "Abdenour Hamdaoui", "authors": "Abdelkader Benkhaled, Mekki Terbeche and Abdenour Hamdaoui", "title": "Polynomials shrinkage estimators of a multivariate normal mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, the estimation of the multivariate normal mean by different\nclasses of shrinkage estimators is investigated. The risk associated with the\nbalanced loss function is used to compare two estimators. We start by\nconsidering estimators that generalize the James-Stein estimator and show that\nthese estimators dominate the maximum likelihood estimator (MLE), therefore are\nminimax, when the shrinkage function satisfies some conditions. Then, we treat\nestimators of polynomial form and prove the increase of the degree of the\npolynomial allows us to build a better estimator from the one previously\nconstructed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:34:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Benkhaled", "Abdelkader", ""], ["Terbeche", "Mekki", ""], ["Hamdaoui", "Abdenour", ""]]}, {"id": "2107.14172", "submitter": "Michael Celentano", "authors": "Michael Celentano, Andrea Montanari", "title": "CAD: Debiasing the Lasso with inaccurate covariate model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating a low-dimensional parameter in\nhigh-dimensional linear regression. Constructing an approximately unbiased\nestimate of the parameter of interest is a crucial step towards performing\nstatistical inference. Several authors suggest to orthogonalize both the\nvariable of interest and the outcome with respect to the nuisance variables,\nand then regress the residual outcome with respect to the residual variable.\nThis is possible if the covariance structure of the regressors is perfectly\nknown, or is sufficiently structured that it can be estimated accurately from\ndata (e.g., the precision matrix is sufficiently sparse).\n  Here we consider a regime in which the covariate model can only be estimated\ninaccurately, and hence existing debiasing approaches are not guaranteed to\nwork. When errors in estimating the covariate model are correlated with errors\nin estimating the linear model parameter, an incomplete elimination of the bias\noccurs. We propose the Correlation Adjusted Debiased Lasso (CAD), which nearly\neliminates this bias in some cases, including cases in which the estimation\nerrors are neither negligible nor orthogonal.\n  We consider a setting in which some unlabeled samples might be available to\nthe statistician alongside labeled ones (semi-supervised learning), and our\nguarantees hold under the assumption of jointly Gaussian covariates. The new\ndebiased estimator is guaranteed to cancel the bias in two cases: (1) when the\ntotal number of samples (labeled and unlabeled) is larger than the number of\nparameters, or (2) when the covariance of the nuisance (but not the effect of\nthe nuisance on the variable of interest) is known. Neither of these cases is\ntreated by state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:49:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""]]}, {"id": "2107.14184", "submitter": "Andrew Warren", "authors": "Andrew Warren", "title": "Wasserstein Conditional Independence Testing", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a test for the conditional independence of random variables $X$\nand $Y$ given a random variable $Z$, specifically by sampling from the joint\ndistribution $(X,Y,Z)$, binning the support of the distribution of $Z$, and\nconducting multiple $p$-Wasserstein two-sample tests. Under a $p$-Wasserstein\nLipschitz assumption on the conditional distributions $\\mathcal{L}_{X|Z}$,\n$\\mathcal{L}_{Y|Z}$, and $\\mathcal{L}_{(X,Y)|Z}$, we show that it is possible\nto control the Type I and Type II error of this test, and give examples of\nexplicit finite-sample error bounds in the case where the distribution of $Z$\nhas compact support.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:11:13 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Warren", "Andrew", ""]]}]