[{"id": "1404.0122", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and G\\'abor J. Sz\\'ekely and Uri Ascher", "title": "Assessing stochastic algorithms for large scale nonlinear least squares\n  problems using extremal probabilities of linear combinations of gamma random\n  variables", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification. 3 (2015) 61-90", "doi": "10.1137/14096311X", "report-no": null, "categories": "math.NA cs.NA math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers stochastic algorithms for efficiently solving a class\nof large scale non-linear least squares (NLS) problems which frequently arise\nin applications. We propose eight variants of a practical randomized algorithm\nwhere the uncertainties in the major stochastic steps are quantified. Such\nstochastic steps involve approximating the NLS objective function using\nMonte-Carlo methods, and this is equivalent to the estimation of the trace of\ncorresponding symmetric positive semi-definite (SPSD) matrices. For the latter,\nwe prove tight necessary and sufficient conditions on the sample size (which\ntranslates to cost) to satisfy the prescribed probabilistic accuracy. We show\nthat these conditions are practically computable and yield small sample sizes.\nThey are then incorporated in our stochastic algorithm to quantify the\nuncertainty in each randomized step. The bounds we use are applications of more\ngeneral results regarding extremal tail probabilities of linear combinations of\ngamma distributed random variables. We derive and prove new results concerning\nthe maximal and minimal tail probabilities of such linear combinations, which\ncan be considered independently of the rest of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 04:25:42 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 02:32:28 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Sz\u00e9kely", "G\u00e1bor J.", ""], ["Ascher", "Uri", ""]]}, {"id": "1404.0198", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Johannes Rauh, Nihat Ay", "title": "On the Fisher Metric of Conditional Probability Polytopes", "comments": "26 pages, 2 figures", "journal-ref": "Entropy. 2014; 16(6):3207-3233", "doi": "10.3390/e16063207", "report-no": null, "categories": "math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three different approaches to define natural Riemannian metrics\non polytopes of stochastic matrices. First, we define a natural class of\nstochastic maps between these polytopes and give a metric characterization of\nChentsov type in terms of invariance with respect to these maps. Second, we\nconsider the Fisher metric defined on arbitrary polytopes through their\nembeddings as exponential families in the probability simplex. We show that\nthese metrics can also be characterized by an invariance principle with respect\nto morphisms of exponential families. Third, we consider the Fisher metric\nresulting from embedding the polytope of stochastic matrices in a simplex of\njoint distributions by specifying a marginal distribution. All three approaches\nresult in slight variations of products of Fisher metrics. This is consistent\nwith the nature of polytopes of stochastic matrices, which are Cartesian\nproducts of probability simplices. The first approach yields a scaled product\nof Fisher metrics; the second, a product of Fisher metrics; and the third, a\nproduct of Fisher metrics scaled by the marginal distribution.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 11:07:05 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 15:02:56 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Montufar", "Guido", ""], ["Rauh", "Johannes", ""], ["Ay", "Nihat", ""]]}, {"id": "1404.0202", "submitter": "St\\'ephanie van der Pas", "authors": "S. L. van der Pas, B. J. K. Kleijn, and A. W. van der Vaart", "title": "The Horseshoe Estimator: Posterior Concentration around Nearly Black\n  Vectors", "comments": "This version differs from the final published version in pagination\n  and typographical detail; Available at\n  http://projecteuclid.org/euclid.ejs/1418134265", "journal-ref": "Electron. J. Statist. Volume 8, Number 2 (2014), 2585-2618", "doi": "10.1214/14-EJS962", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the horseshoe estimator due to Carvalho, Polson and Scott (2010)\nfor the multivariate normal mean model in the situation that the mean vector is\nsparse in the nearly black sense. We assume the frequentist framework where the\ndata is generated according to a fixed mean vector. We show that if the number\nof nonzero parameters of the mean vector is known, the horseshoe estimator\nattains the minimax $\\ell_2$ risk, possibly up to a multiplicative constant. We\nprovide conditions under which the horseshoe estimator combined with an\nempirical Bayes estimate of the number of nonzero means still yields the\nminimax risk. We furthermore prove an upper bound on the rate of contraction of\nthe posterior distribution around the horseshoe estimator, and a lower bound on\nthe posterior variance. These bounds indicate that the posterior distribution\nof the horseshoe prior may be more informative than that of other one-component\npriors, including the Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 11:34:35 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 08:42:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["van der Pas", "S. L.", ""], ["Kleijn", "B. J. K.", ""], ["van der Vaart", "A. W.", ""]]}, {"id": "1404.0216", "submitter": "Samuel Maistre", "authors": "Samuel Maistre, Pascal Lavergne and Valentin Patilea", "title": "Powerful nonparametric checks for quantile regression", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of lack-of-fit testing for a parametric quantile\nregression. We propose a simple test that involves one-dimensional kernel\nsmoothing, so that the rate at which it detects local alternatives is\nindependent of the number of covariates. The test has asymptotically gaussian\ncritical values, and wild bootstrap can be applied to obtain more accurate ones\nin small samples. Our procedure appears to be competitive with existing ones in\nsimulations. We illustrate the usefulness of our test on birthweight data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 12:31:31 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 08:41:03 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Maistre", "Samuel", ""], ["Lavergne", "Pascal", ""], ["Patilea", "Valentin", ""]]}, {"id": "1404.0229", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti, Irene Matuonto, Mirella Pontello", "title": "A randomized most powerful test to detect a cheater's action. Applicaton\n  to identification of listeriosis in Lombardy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new randomized non-parametric test based on a sample\nof independent but not identically distributed variables; this test detects if\na cheater replaces one of the distributions of the sample with a\nconvex-dominating one. The presented test is the uniformely most powerful, in\nthe sense that it is the most powerful for any change of the cheater. We show\nthat this test may be applied when we have variables with distribution\nsatisfying the monotone likelihood ratio property and we need to check whether\na parameter of a variable has been changed. The application we present concerns\nthe detection of epidemics of listeriosis in Lombardy from 2005 to 2011.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 13:18:52 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 14:01:15 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Aletti", "Giacomo", ""], ["Matuonto", "Irene", ""], ["Pontello", "Mirella", ""]]}, {"id": "1404.0396", "submitter": "James Johndrow", "authors": "James E. Johndrow, Anirban Battacharya, and David B. Dunson", "title": "Tensor decompositions and sparse log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contingency table analysis routinely relies on log linear models, with latent\nstructure analysis providing a common alternative. Latent structure models lead\nto a low rank tensor factorization of the probability mass function for\nmultivariate categorical data, while log linear models achieve dimensionality\nreduction through sparsity. Little is known about the relationship between\nthese notions of dimensionality reduction in the two paradigms. We derive\nseveral results relating the support of a log-linear model to the nonnegative\nrank of the associated probability tensor. Motivated by these findings, we\npropose a new collapsed Tucker class of tensor decompositions, which bridge\nexisting PARAFAC and Tucker decompositions, providing a more flexible framework\nfor parsimoniously characterizing multivariate categorical data. Taking a\nBayesian approach to inference, we illustrate advantages of the new\ndecompositions in simulations and an application to functional disability data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 21:05:22 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Johndrow", "James E.", ""], ["Battacharya", "Anirban", ""], ["Dunson", "David B.", ""]]}, {"id": "1404.0646", "submitter": "Bertrand Michel", "authors": "J\\'er\\^ome Dedecker (MAP5), Aur\\'elie Fischer (LPMA), Bertrand Michel\n  (LSTA)", "title": "Improved rates for Wasserstein deconvolution with ordinary smooth error\n  in dimension one", "comments": null, "journal-ref": null, "doi": null, "report-no": "MAP5 2014-11", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of a probability measure on the real\nline from data observed with an additive noise. We are interested in rates of\nconvergence for the Wasserstein metric of order $p\\geq 1$. The distribution of\nthe errors is assumed to be known and to belong to a class of supersmooth or\nordinary smooth distributions. We obtain in the univariate situation an\nimproved upper bound in the ordinary smooth case and less restrictive\nconditions for the existing bound in the supersmooth one. In the ordinary\nsmooth case, a lower bound is also provided, and numerical experiments\nillustrating the rates of convergence are presented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 18:12:10 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 07:40:05 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Dedecker", "J\u00e9r\u00f4me", "", "MAP5"], ["Fischer", "Aur\u00e9lie", "", "LPMA"], ["Michel", "Bertrand", "", "LSTA"]]}, {"id": "1404.0788", "submitter": "Antti Knowles", "authors": "Alex Bloemendal, Antti Knowles, Horng-Tzer Yau, Jun Yin", "title": "On the principal components of sample covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of $M \\times M$ sample covariance matrices $\\mathcal Q$\nwhich subsumes and generalizes several previous models. The associated\npopulation covariance matrix $\\Sigma = \\mathbb E \\cal Q$ is assumed to differ\nfrom the identity by a matrix of bounded rank. All quantities except the rank\nof $\\Sigma - I_M$ may depend on $M$ in an arbitrary fashion. We investigate the\nprincipal components, i.e.\\ the top eigenvalues and eigenvectors, of $\\mathcal\nQ$. We derive precise large deviation estimates on the generalized components\n$\\langle \\mathbf w, \\boldsymbol \\xi_i \\rangle$ of the outlier and non-outlier\neigenvectors $\\boldsymbol \\xi_i$. Our results also hold near the so-called BBP\ntransition, where outliers are created or annihilated, and for degenerate or\nnear-degenerate outliers. We believe the obtained rates of convergence to be\noptimal. In addition, we derive the asymptotic distribution of the generalized\ncomponents of the non-outlier eigenvectors. A novel observation arising from\nour results is that, unlike the eigenvalues, the eigenvectors of the principal\ncomponents contain information about the \\emph{subcritical} spikes of $\\Sigma$.\n  The proofs use several results on the eigenvalues and eigenvectors of the\nuncorrelated matrix $\\mathcal Q$, satisfying $\\mathbb E \\mathcal Q = I_M$, as\ninput: the isotropic local Marchenko-Pastur law established in [9], level\nrepulsion, and quantum unique ergodicity of the eigenvectors. The latter is a\nspecial case of a new universality result for the joint eigenvalue-eigenvector\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 07:41:08 GMT"}, {"version": "v2", "created": "Fri, 23 May 2014 16:02:27 GMT"}, {"version": "v3", "created": "Fri, 16 Jan 2015 17:46:10 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Bloemendal", "Alex", ""], ["Knowles", "Antti", ""], ["Yau", "Horng-Tzer", ""], ["Yin", "Jun", ""]]}, {"id": "1404.1214", "submitter": "Ulrich Bauer", "authors": "Ulrich Bauer, Axel Munk, Hannes Sieling, Max Wardetzky", "title": "Persistence Barcodes versus Kolmogorov Signatures: Detecting Modes of\n  One-Dimensional Signals", "comments": null, "journal-ref": null, "doi": "10.1007/s10208-015-9281-9", "report-no": null, "categories": "math.ST cs.CG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the number of modes (i.e., local\nmaxima) - a well known question in statistical inference - and we show how to\ndo so without presmoothing the data. To this end, we modify the ideas of\npersistence barcodes by first relating persistence values in dimension one to\ndistances (with respect to the supremum norm) to the sets of functions with a\ngiven number of modes, and subsequently working with norms different from the\nsupremum norm. As a particular case we investigate the Kolmogorov norm. We\nargue that this modification has certain statistical advantages. We offer\nconfidence bands for the attendant Kolmogorov signatures, thereby allowing for\nthe selection of relevant signatures with a statistically controllable error.\nAs a result of independent interest, we show that taut strings minimize the\nnumber of critical points for a very general class of functions. We illustrate\nour results by several numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 11:03:15 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 11:25:14 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Bauer", "Ulrich", ""], ["Munk", "Axel", ""], ["Sieling", "Hannes", ""], ["Wardetzky", "Max", ""]]}, {"id": "1404.1256", "submitter": "Tamas Biro S", "authors": "T.S. Bir\\'o, G.G. Barnaf\\\"oldi, P. V\\'an, K. \\\"Urm\\\"ossy", "title": "Statistical Power-Law Spectra due to Reservoir Fluctuations", "comments": "Latex 4 pages, 2 figs, PRL style", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LHC ALICE data are interpreted in terms of statistical power-law tailed pT\nspectra. As explanation we derive such statistical distributions for particular\nparticle number fluctuation patterns in a finite heat bath exactly, and for\ngeneral thermodynamical systems in the subleading canonical expansion\napproximately. Our general result, $q = 1 - 1/C + \\Delta T^2 / T^2$,\ndemonstrates how the heat capacity and the temperature fluctuation effects\ncompete, and cancel only in the standard Gaussian approximation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 14:04:28 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Bir\u00f3", "T. S.", ""], ["Barnaf\u00f6ldi", "G. G.", ""], ["V\u00e1n", "P.", ""], ["\u00dcrm\u00f6ssy", "K.", ""]]}, {"id": "1404.1310", "submitter": "Benedikt M. P\\\"otscher", "authors": "David Preinerstorfer and Benedikt M. P\\\"otscher", "title": "On the Power of Invariant Tests for Hypotheses on a Covariance Matrix", "comments": null, "journal-ref": "Econom. Theory 33 (2017) 1-68", "doi": "10.1017/S026646661500033X", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of the power function of autocorrelation tests such as the\nDurbin-Watson test in time series regressions or the Cliff-Ord test in spatial\nregression models has been intensively studied in the literature. When the\ncorrelation becomes strong, Kr\\\"amer (1985) (for the Durbin-Watson test) and\nKr\\\"amer (2005) (for the Cliff-Ord test) have shown that the power can be very\nlow, in fact can converge to zero, under certain circumstances. Motivated by\nthese results, Martellosio (2010) set out to build a general theory that would\nexplain these findings. Unfortunately, Martellosio (2010) does not achieve this\ngoal, as a substantial portion of his results and proofs suffer from serious\nflaws. The present paper now builds a theory as envisioned in Martellosio\n(2010) in a fairly general framework, covering general invariant tests of a\nhypothesis on the disturbance covariance matrix in a linear regression model.\nThe general results are then specialized to testing for spatial correlation and\nto autocorrelation testing in time series regression models. We also\ncharacterize the situation where the null and the alternative hypothesis are\nindistinguishable by invariant tests.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 16:47:26 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 14:33:14 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Preinerstorfer", "David", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1404.1347", "submitter": "Jonathan D. Gammell", "authors": "Jonathan D. Gammell and Timothy D. Barfoot", "title": "The Probability Density Function of a Transformation-based\n  Hyperellipsoid Sampling Technique", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": "TR-2014-JDG004", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sun and Farooq [2] showed that random samples can be efficiently drawn from\nan arbitrary n-dimensional hyperellipsoid by transforming samples drawn\nrandomly from the unit n-ball. They stated that it was a straightforward to\nshow that, given a uniform distribution over the n-ball, the transformation\nresults in a uniform distribution over the hyperellipsoid, but did not present\na full proof. This technical note presents such a proof.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:21:51 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 15:29:41 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Gammell", "Jonathan D.", ""], ["Barfoot", "Timothy D.", ""]]}, {"id": "1404.1356", "submitter": "Olivier Wintenberger", "authors": "Olivier Wintenberger (LSTA)", "title": "Optimal learning with Bernstein Online Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new recursive aggregation procedure called Bernstein Online\nAggregation (BOA). The exponential weights include an accuracy term and a\nsecond order term that is a proxy of the quadratic variation as in Hazan and\nKale (2010). This second term stabilizes the procedure that is optimal in\ndifferent senses. We first obtain optimal regret bounds in the deterministic\ncontext. Then, an adaptive version is the first exponential weights algorithm\nthat exhibits a second order bound with excess losses that appears first in\nGaillard et al. (2014). The second order bounds in the deterministic context\nare extended to a general stochastic context using the cumulative predictive\nrisk. Such conversion provides the main result of the paper, an inequality of a\nnovel type comparing the procedure with any deterministic aggregation procedure\nfor an integrated criteria. Then we obtain an observable estimate of the excess\nof risk of the BOA procedure. To assert the optimality, we consider finally the\niid case for strongly convex and Lipschitz continuous losses and we prove that\nthe optimal rate of aggregation of Tsybakov (2003) is achieved. The batch\nversion of the BOA procedure is then the first adaptive explicit algorithm that\nsatisfies an optimal oracle inequality with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:33:55 GMT"}, {"version": "v2", "created": "Sun, 20 Apr 2014 19:26:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 17:58:04 GMT"}, {"version": "v4", "created": "Tue, 30 Aug 2016 06:44:14 GMT"}, {"version": "v5", "created": "Tue, 13 Sep 2016 14:23:48 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Wintenberger", "Olivier", "", "LSTA"]]}, {"id": "1404.1406", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen", "title": "A Note on Moment Inequality for Quadratic Forms", "comments": "12 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moment inequality for quadratic forms of random vectors is of particular\ninterest in covariance matrix testing and estimation problems. In this paper,\nwe prove a Rosenthal-type inequality, which exhibits new features and certain\nimprovement beyond the unstructured Rosenthal inequality of quadratic forms\nwhen dimension of the vectors increases without bound. Applications to test the\nblock diagonal structures and detect the sparsity in the high-dimensional\ncovariance matrix are presented.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 22:36:30 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 15:25:36 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Chen", "Xiaohui", ""]]}, {"id": "1404.1495", "submitter": "Alexander Kushpel", "authors": "Alexander Kushpel", "title": "Pricing of basket options II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximation of density functions which is\nimportant in the theory of pricing of basket options. Our method is well\nadopted to the multidimensional case. Observe that implementations of\npolynomial and spline approximation in this situation are connected with\ndifficulties of fundamental nature. A simple approximation formula for European\ncall options is presented. It is shown that this approximation formula has\nexponential rate of convergence.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 18:05:34 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Kushpel", "Alexander", ""]]}, {"id": "1404.1530", "submitter": "Christos Boutsidis", "authors": "Dimitris Papailiopoulos, Anastasios Kyrillidis, Christos Boutsidis", "title": "Provable Deterministic Leverage Score Sampling", "comments": "20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.NA math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain theoretically a curious empirical phenomenon: \"Approximating a\nmatrix by deterministically selecting a subset of its columns with the\ncorresponding largest leverage scores results in a good low-rank matrix\nsurrogate\". To obtain provable guarantees, previous work requires randomized\nsampling of the columns with probabilities proportional to their leverage\nscores.\n  In this work, we provide a novel theoretical analysis of deterministic\nleverage score sampling. We show that such deterministic sampling can be\nprovably as accurate as its randomized counterparts, if the leverage scores\nfollow a moderately steep power-law decay. We support this power-law assumption\nby providing empirical evidence that such decay laws are abundant in real-world\ndata sets. We then demonstrate empirically the performance of deterministic\nleverage score sampling, which many times matches or outperforms the\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 00:08:54 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 10:19:07 GMT"}, {"version": "v3", "created": "Tue, 3 Jun 2014 01:23:16 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Papailiopoulos", "Dimitris", ""], ["Kyrillidis", "Anastasios", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1404.1709", "submitter": "Rajesh  Singh", "authors": "Prayas Sharma and Rajesh Singh", "title": "Method of estimation in the presence of non-response and measurement\n  errors simultaneously", "comments": "14 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper discusses the problem of estimating the finite population\nmean of study variable in simple random sampling in the presence of non\nresponse and response error together. The estimators in this article use\nauxiliary information to improve efficiency and we suppose that non response\nand measurement error are present in both the study and auxiliary variables. A\nclass of estimators has been proposed and its properties are studied in the\nsimultaneous presence of non-response and response errors. It has been shown\nthat proposed class of estimators is more efficient than the usual unbiased\nestimator, ratio and product estimators under non-response and response error\ntogether. In addition, a numerical study is carried out to compare the\nperformance of the proposed class of estimators over others.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 09:41:36 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Sharma", "Prayas", ""], ["Singh", "Rajesh", ""]]}, {"id": "1404.1781", "submitter": "Matteo Ruggiero", "authors": "Pierpaolo De Blasi, Matteo Ruggiero and Dario Spano'", "title": "Inhomogeneous Wright-Fisher construction of two-parameter\n  Poisson-Dirichlet diffusions", "comments": "This paper has been withdrawn by the authors. A substantial revision\n  of this paper has been published at arXiv:1601.06064", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced two-parameter Poisson-Dirichlet diffusion extends the\ninfinitely-many-neutral-alleles model, related to Kingman's distribution and to\nFleming-Viot processes. The role of the additional parameter has been shown to\nregulate the clustering structure of the population, but is yet to be fully\nunderstood in the way it governs the reproductive process. Here we shed some\nlight on these dynamics by providing a finite-population construction, with\nfinitely-many species, of the two-parameter infinite-dimensional diffusion. The\ncostruction is obtained in terms of Wright-Fisher chains that feature a\nclassical symmetric mutation mechanism and a frequency-dependent immigration,\nwhose inhomogeneity is investigated in detail. The local immigration dynamics\nare built upon an underlying array of Bernoulli trials and can be described by\nmeans of a dartboard experiment and a rank-dependent type distribution. These\ninvolve a delicate balance between reinforcement and redistributive effects,\namong the current species abundances, for the convergence to hold.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 13:29:37 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 16:56:17 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["De Blasi", "Pierpaolo", ""], ["Ruggiero", "Matteo", ""], ["Spano'", "Dario", ""]]}, {"id": "1404.1839", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Kinga Sikolya and Milan Stehl\\'ik", "title": "Optimal designs for the methane flux in troposphere", "comments": "25 pages, 4 figures", "journal-ref": "Chemometrics and Intelligent Laboratory Systems 146 (2015),\n  407-417", "doi": "10.1016/j.chemolab.2015.06.002", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of methane emission and methane absorption plays a central\nrole both in the atmosphere and on the surface of the Earth. Several important\necological processes, e.g., ebullition of methane and its natural\nmicroergodicity request better designs for observations in order to decrease\nvariability in parameter estimation. Thus, a crucial fact, before the\nmeasurements are taken, is to give an optimal design of the sites where\nobservations should be collected in order to stabilize the variability of\nestimators. In this paper we introduce a realistic parametric model of\ncovariance and provide theoretical and numerical results on optimal designs.\nFor parameter estimation D-optimality, while for prediction integrated mean\nsquare error and entropy criteria are used. We illustrate applicability of\nobtained benchmark designs for increasing/measuring the efficiency of the\nengineering designs for estimation of methane rate in various temperature\nranges and under different correlation parameters. We show that in most\nsituations these benchmark designs have higher efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 16:29:24 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 16:12:33 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Sikolya", "Kinga", ""], ["Stehl\u00edk", "Milan", ""]]}, {"id": "1404.2006", "submitter": "Jaehyung Choi", "authors": "Jaehyung Choi, Andrew P. Mullhaupt", "title": "K\\\"ahlerian information geometry for signal processing", "comments": "24 pages, published version", "journal-ref": "Entropy 17(4), 1581-1605 (2015)", "doi": "10.3390/e17041581", "report-no": null, "categories": "math.DG cs.IT cs.SY math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the correspondence between the information geometry of a signal\nfilter and a K\\\"ahler manifold. The information geometry of a minimum-phase\nlinear system with a finite complex cepstrum norm is a K\\\"ahler manifold. The\nsquare of the complex cepstrum norm of the signal filter corresponds to the\nK\\\"ahler potential. The Hermitian structure of the K\\\"ahler manifold is\nexplicitly emergent if and only if the impulse response function of the highest\ndegree in $z$ is constant in model parameters. The K\\\"ahlerian information\ngeometry takes advantage of more efficient calculation steps for the metric\ntensor and the Ricci tensor. Moreover, $\\alpha$-generalization on the geometric\ntensors is linear in $\\alpha$. It is also robust to find Bayesian predictive\npriors, such as superharmonic priors, because Laplace-Beltrami operators on\nK\\\"ahler manifolds are in much simpler forms than those of the non-K\\\"ahler\nmanifolds. Several time series models are studied in the K\\\"ahlerian\ninformation geometry.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 04:16:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 20:56:03 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 17:25:47 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Choi", "Jaehyung", ""], ["Mullhaupt", "Andrew P.", ""]]}, {"id": "1404.2298", "submitter": "Richard Samworth", "authors": "Arlene K. H. Kim and Richard J. Samworth", "title": "Global rates of convergence in log-concave density estimation", "comments": "58 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of a log-concave density on $\\mathbb{R}^d$ represents a\ncentral problem in the area of nonparametric inference under shape constraints.\nIn this paper, we study the performance of log-concave density estimators with\nrespect to global loss functions, and adopt a minimax approach. We first show\nthat no statistical procedure based on a sample of size $n$ can estimate a\nlog-concave density with respect to the squared Hellinger loss function with\nsupremum risk smaller than order $n^{-4/5}$, when $d=1$, and order\n$n^{-2/(d+1)}$ when $d \\geq 2$. In particular, this reveals a sense in which,\nwhen $d \\geq 3$, log-concave density estimation is fundamentally more\nchallenging than the estimation of a density with two bounded derivatives (a\nproblem to which it has been compared). Second, we show that for $d \\leq 3$,\nthe Hellinger $\\epsilon$-bracketing entropy of a class of log-concave densities\nwith small mean and covariance matrix close to the identity grows like\n$\\max\\{\\epsilon^{-d/2},\\epsilon^{-(d-1)}\\}$ (up to a logarithmic factor when\n$d=2$). This enables us to prove that when $d \\leq 3$ the log-concave maximum\nlikelihood estimator achieves the minimax optimal rate (up to logarithmic\nfactors when $d = 2,3$) with respect to squared Hellinger loss.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 20:29:22 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 14:37:40 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Kim", "Arlene K. H.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1404.2355", "submitter": "Debashis Paul", "authors": "Jiming Jiang and Cong Li and Debashis Paul and Can Yang and Hongyu\n  Zhao", "title": "High-dimensional genome-wide association study and misspecified mixed\n  model analysis", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study behavior of the restricted maximum likelihood (REML) estimator under\na misspecified linear mixed model (LMM) that has received much attention in\nrecent gnome-wide association studies. The asymptotic analysis establishes\nconsistency of the REML estimator of the variance of the errors in the LMM, and\nconvergence in probability of the REML estimator of the variance of the random\neffects in the LMM to a certain limit, which is equal to the true variance of\nthe random effects multiplied by the limiting proportion of the nonzero random\neffects present in the LMM. The aymptotic results also establish convergence\nrate (in probability) of the REML estimators as well as a result regarding\nconvergence of the asymptotic conditional variance of the REML estimator. The\nasymptotic results are fully supported by the results of empirical studies,\nwhich include extensive simulation studies that compare the performance of the\nREML estimator (under the misspecified LMM) with other existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 02:25:03 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Jiang", "Jiming", ""], ["Li", "Cong", ""], ["Paul", "Debashis", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1404.2405", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (- M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques, IMT), Paul Lema\\^itre (EDF R\\&D, INRIA Bordeaux -\n  Sud-Ouest)", "title": "A review on global sensitivity analysis methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter makes a review, in a complete methodological framework, of\nvarious global sensitivity analysis methods of model output. Numerous\nstatistical and probabilistic tools (regression, smoothing, tests, statistical\nlearning, Monte Carlo, \\ldots) aim at determining the model input variables\nwhich mostly contribute to an interest quantity depending on model output. This\nquantity can be for instance the variance of an output variable. Three kinds of\nmethods are distinguished: the screening (coarse sorting of the most\ninfluential inputs among a large number), the measures of importance\n(quantitative sensitivity indices) and the deep exploration of the model\nbehaviour (measuring the effects of inputs on their all variation range). A\nprogressive application methodology is illustrated on a scholar application. A\nsynthesis is given to place every method according to several axes, mainly the\ncost in number of model evaluations, the model complexity and the nature of\nbrought information.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 09:33:54 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Iooss", "Bertrand", "", "- M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques, IMT"], ["Lema\u00eetre", "Paul", "", "EDF R\\&D, INRIA Bordeaux -\n  Sud-Ouest"]]}, {"id": "1404.2608", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh, Prayas Sharma and Florentin Smarandache", "title": "Exponential ratio-product type estimators under second order\n  approximation in stratified random sampling", "comments": "11 pages, 1 table; The efficient use of supplementary information in\n  finite population sampling,Education Publishing, USA, (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singh et al (20009) introduced a family of exponential ratio and product type\nestimators in stratified random sampling. Under stratified random sampling\nwithout replacement scheme, the expressions of bias and mean square error (MSE)\nof Singh et al (2009) and some other estimators, up to the first- and\nsecond-order approximations are derived. Also, the theoretical findings are\nsupported by a numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 07:52:18 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Singh", "Rajesh", ""], ["Sharma", "Prayas", ""], ["Smarandache", "Florentin", ""]]}, {"id": "1404.2664", "submitter": "Shiro Ishikawa", "authors": "Shiro Ishikawa, Kohshi Kikuchi", "title": "Kalman filter in quantum language", "comments": "arXiv admin note: text overlap with arXiv:1207.0407", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we proposed measurement theory ( or. quantum language) as a\nlinguistic turn of quantum mechanics (with the Copenhagen interpretation). This\ntheory has a great power of scientific descriptions. In fact, we have continued\nasserting that even statistics can be described in terms of measurement theory.\nThus, we believe that quantum language is future statistics (i.e., statistics\nwill develop into quantum language). However, now we think that our arguments\nwere too abstract and philosophical, that is, we should have presented concrete\nexamples much more. Thus, in this paper, we show that the calculation of Kalman\nfilter is more understandable in terms of quantum language than in terms of\nusual statistics. For this, we devote ourselves to statistical measurement\ntheory, in which the Bertrand paradox is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 01:32:46 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Ishikawa", "Shiro", ""], ["Kikuchi", "Kohshi", ""]]}, {"id": "1404.2787", "submitter": "Andr\\'as L\\'aszl\\'o", "authors": "Andras Laszlo", "title": "Convergence and error propagation results on a linear iterative\n  unfolding method", "comments": "27 pages, 1 figure", "journal-ref": "SIAM/ASA Journal of Uncertainty Quantification 4 (2016) 1345", "doi": "10.1137/15M1035744", "report-no": null, "categories": "stat.AP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unfolding problems often arise in the context of statistical data analysis.\nSuch problematics occur when the probability distribution of a physical\nquantity is to be measured, but it is randomized (smeared) by some well\nunderstood process, such as a non-ideal detector response or a well described\nphysical phenomenon. In such case it is said that the original probability\ndistribution of interest is folded by a known response function. The\nreconstruction of the original probability distribution from the measured one\nis called unfolding. That technically involves evaluation of the non-bounded\ninverse of an integral operator over the space of L^1 functions, which is known\nto be an ill-posed problem. For the pertinent regularized operator inversion,\nwe propose a linear iterative formula and provide proof of convergence in a\nprobability theory context. Furthermore, we provide formulae for error\nestimates at finite iteration stopping order which are of utmost importance in\npractical applications: the approximation error, the propagated statistical\nerror, and the propagated systematic error can be quantified. The arguments are\nbased on the Riesz-Thorin theorem mapping the original L^1 problem to L^2\nspace, and subsequent application of ordinary L^2 spectral theory of operators.\nA library implementation in C of the algorithm along with corresponding error\npropagation is also provided. A numerical example also illustrates the method\nin operation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 12:58:39 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 20:18:12 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Laszlo", "Andras", ""]]}, {"id": "1404.2825", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Asymptotics of Fingerprinting and Group Testing: Capacity-Achieving\n  Log-Likelihood Decoders", "comments": "14 pages, 2 figures", "journal-ref": "EURASIP Journal on Information Security, 2016:3", "doi": "10.1186/s13635-015-0026-8", "report-no": null, "categories": "cs.IT cs.CR math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the large-coalition asymptotics of fingerprinting and group testing,\nand derive explicit decoders that provably achieve capacity for many of the\nconsidered models. We do this both for simple decoders (fast but suboptimal)\nand for joint decoders (slow but optimal), and both for informed and uninformed\nsettings.\n  For fingerprinting, we show that if the pirate strategy is known, the\nNeyman-Pearson-based log-likelihood decoders provably achieve capacity,\nregardless of the strategy. The decoder built against the interleaving attack\nis further shown to be a universal decoder, able to deal with arbitrary attacks\nand achieving the uninformed capacity. This universal decoder is shown to be\nclosely related to the Lagrange-optimized decoder of Oosterwijk et al. and the\nempirical mutual information decoder of Moulin. Joint decoders are also\nproposed, and we conjecture that these also achieve the corresponding joint\ncapacities.\n  For group testing, the simple decoder for the classical model is shown to be\nmore efficient than the one of Chan et al. and it provably achieves the simple\ngroup testing capacity. For generalizations of this model such as noisy group\ntesting, the resulting simple decoders also achieve the corresponding simple\ncapacities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 19:03:59 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1404.2910", "submitter": "Karthik Bharath", "authors": "Karthik Bharath, Prabhanjan Kambadur, Dipak. K. Dey, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "Statistical Tests for Large Tree-structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general statistical framework for the analysis and inference of\nlarge tree-structured data, with a focus on developing asymptotic\ngoodness-of-fit tests. We first propose a consistent statistical model for\nbinary trees, from which we develop a class of invariant tests. Using the model\nfor binary trees, we then construct tests for general trees by using the\ndistributional properties of the Continuum Random Tree, which arises as the\ninvariant limit for a broad class of models for tree-structured data based on\nconditioned Galton--Watson processes. The test statistics for the\ngoodness-of-fit tests are simple to compute and are asymptotically distributed\nas $\\chi^2$ and $F$ random variables. We illustrate our methods on an important\napplication of detecting tumour heterogeneity in brain cancer. We use a novel\napproach with tree-based representations of magnetic resonance images and\nemploy the developed tests to ascertain tumor heterogeneity between two groups\nof patients.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 19:19:21 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 16:43:11 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 21:43:13 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Bharath", "Karthik", ""], ["Kambadur", "Prabhanjan", ""], ["Dey", "Dipak. K.", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1404.2957", "submitter": "Richard Samworth", "authors": "Yining Chen and Richard J. Samworth", "title": "Generalised additive and index models with shape constraints", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalised additive models, with shape restrictions (e.g.\nmonotonicity, convexity, concavity) imposed on each component of the additive\nprediction function. We show that this framework facilitates a nonparametric\nestimator of each additive component, obtained by maximising the likelihood.\nThe procedure is free of tuning parameters and under mild conditions is proved\nto be uniformly consistent on compact intervals. More generally, our\nmethodology can be applied to generalised additive index models. Here again,\nthe procedure can be justified on theoretical grounds and, like the original\nalgorithm, possesses highly competitive finite-sample performance. Practical\nutility is illustrated through the use of these methods in the analysis of two\nreal datasets. Our algorithms are publicly available in the \\texttt{R} package\n\\textbf{scar}, short for \\textbf{s}hape-\\textbf{c}onstrained \\textbf{a}dditive\n\\textbf{r}egression.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 21:45:13 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Chen", "Yining", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1404.3046", "submitter": "Mate Manfay", "authors": "M\\'at\\'e M\\'anfay and L\\'aszl\\'o Gerencs\\'er and Zsanett Orlovits", "title": "ECF identification of GARCH systems driven by L\\'evy processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L\\'evy processes are widely used in financial mathematics, telecommunication,\neconomics, queueing theory and natural sciences for modelling. We propose an\nessentially asymptotically efficient estimation method for the system\nparameters of general autoregressive conditional heteroscedasticity (GARCH)\nprocesses. As an alternative to the maximum likelihood (ML) method we develop\nand analyze a novel identification method by adapting the so-called empirical\ncharacteristic function method (ECF) originally devised for estimating\nparameters of c.f.-s from i.i.d. samples. Precise characterization of the\nerrors of these estimators will be given, and their asymptotic covariance\nmatrices will be obtained.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 09:29:14 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["M\u00e1nfay", "M\u00e1t\u00e9", ""], ["Gerencs\u00e9r", "L\u00e1szl\u00f3", ""], ["Orlovits", "Zsanett", ""]]}, {"id": "1404.3051", "submitter": "Mate Manfay", "authors": "L\\'aszl\\'o Gerencs\\'er and M\\'at\\'e M\\'anfay", "title": "Recursive ECF identification of linear systems driven by L\\'evy\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature the empirical characteristic function method is presented\nas an off-line identification method. While the results of the off-line methods\nare attractive, the proposed algorithms are ill-conditioned in many cases so\nthat they requires special attention. As an alternative to the off-line method\nin this paper we propose and analyze on-line empirical characteristic function\nmethods. Such recursive methods enables us to carry out real-time statistical\nanalysis as new data points are processed instantly. In constructing these\nalgorithms we follow the general framework proposed by Djereveckii and Fradkov.\nOn-line methods are also used to complement a computationally expensive\noff-line identification method. Namely, it would be uneconomical to re-estimate\n$\\theta^*$ using the off-line method when a new data point is received.\nInstead, we can argue that only a refinement of the estimate $\\hat{\\theta}_N$\nshould be computed using the newly received data point. This scenario not only\nshows a motivation behind the study of recursive algorithms but also suggests\nthat it is reasonable to suppose that an initial guess of the parameter is\nclose to $\\theta^*.$\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 09:40:21 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Gerencs\u00e9r", "L\u00e1szl\u00f3", ""], ["M\u00e1nfay", "M\u00e1t\u00e9", ""]]}, {"id": "1404.3094", "submitter": "C\\'ecile Durot", "authors": "Fadoua Balabdaoui, C\\'ecile Durot and Fran\\c{c}ois Koladjo", "title": "On asymptotics of the discrete convex LSE of a pmf", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive the weak limiting distribution of the least\nsquares estimator (LSE) of a convex probability mass function (pmf) with a\nfinite support. We show that it can be defined via a certain convex projection\nof a Gaussian vector. Furthermore, samples of any given size from this limit\ndistribution can be generated using an efficient Dykstra-like algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 12:53:24 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Durot", "C\u00e9cile", ""], ["Koladjo", "Fran\u00e7ois", ""]]}, {"id": "1404.3116", "submitter": "Guillaume Lecu\\'e", "authors": "Guillaume Lecu\\'e and Shahar Mendelson", "title": "Necessary moment conditions for exact reconstruction via basis pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X=(x_1,...,x_n)$ be a random vector that satisfies a weak small ball\nproperty and whose coordinates $x_i$ satisfy that $\\|x_i\\|_{L_p} \\lesssim\n\\sqrt{p} \\|x_i\\|_{L_2}$ for $p \\sim \\log n$. In \\cite{LM_compressed}, it was\nshown that $N$ independent copies of $X$ can be used as measurement vectors in\nCompressed Sensing (using the basis pursuit algorithm) to reconstruct any\n$d$-sparse vector with the optimal number of measurements $N\\gtrsim d\n\\log\\big(e n/d\\big)$. In this note we show that the result is almost optimal.\nWe construct a random vector $X$ with iid, mean-zero, variance one coordinates\nthat satisfies the same weak small ball property and whose coordinates satisfy\nthat $\\|x_i\\|_{L_p} \\lesssim \\sqrt{p} \\|x_i\\|_{L_2}$ for $p \\sim (\\log n)/(\\log\nN)$, but the basis pursuit algorithm fails to recover even $1$-sparse vectors.\n  The construction shows that `spiky' measurement vectors may lead to a poor\nperformance by the basis pursuit algorithm, but on the other hand may still\nperform in an optimal way if one chooses a different reconstruction algorithm\n(like $\\ell_0$-minimization). This exhibits the fact that the convex relaxation\nof $\\ell_0$-minimization comes at a significant cost when using `spiky'\nmeasurement vectors.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 14:19:38 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Lecu\u00e9", "Guillaume", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1404.3152", "submitter": "George K Atia", "authors": "George Atia", "title": "Change Detection with Compressive Measurements", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2014.2352116", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quickest change point detection is concerned with the detection of\nstatistical change(s) in sequences while minimizing the detection delay subject\nto false alarm constraints. In this paper, the problem of change point\ndetection is studied when the decision maker only has access to compressive\nmeasurements. First, an expression for the average detection delay of\nShiryaev's procedure with compressive measurements is derived in the asymptotic\nregime where the probability of false alarm goes to zero. Second, the\ndependence of the delay on the compression ratio and the signal to noise ratio\nis explicitly quantified. The ratio of delays with and without compression is\nstudied under various sensing matrix constructions, including Gaussian\nensembles and random projections. For a target ratio of the delays after and\nbefore compression, a sufficient condition on the number of measurements\nrequired to meet this objective with prespecified probability is derived.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 16:34:30 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Atia", "George", ""]]}, {"id": "1404.3188", "submitter": "Jeremie Kellner", "authors": "J\\'er\\'emie Kellner (INRIA Lille - Nord Europe), Alain Celisse (INRIA\n  Lille - Nord Europe)", "title": "New normality test in high dimension with kernel methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new goodness-of-fit test for normality in high-dimension (and Reproducing\nKernel Hilbert Space) is proposed. It shares common ideas with the Maximum Mean\nDiscrepancy (MMD) it outperforms both in terms of computation time and\napplicability to a wider range of data. Theoretical results are derived for the\nType-I and Type-II errors. They guarantee the control of Type-I error at\nprescribed level and an exponentially fast decrease of the Type-II error.\nSynthetic and real data also illustrate the practical improvement allowed by\nour test compared with other leading approaches in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:59:23 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Kellner", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe"], ["Celisse", "Alain", "", "INRIA\n  Lille - Nord Europe"]]}, {"id": "1404.3203", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Dustin G. Mixon and Benjamin Recht", "title": "Compressive classification and the rare eclipse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper addresses the fundamental question of when convex sets remain\ndisjoint after random projection. We provide an analysis using ideas from\nhigh-dimensional convex geometry. For ellipsoids, we provide a bound in terms\nof the distance between these ellipsoids and simple functions of their\npolynomial coefficients. As an application, this theorem provides bounds for\ncompressive classification of convex sets. Rather than assuming that the data\nto be classified is sparse, our results show that the data can be acquired via\nvery few measurements yet will remain linearly separable. We demonstrate the\nfeasibility of this approach in the context of hyperspectral imaging.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 19:49:05 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Mixon", "Dustin G.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1404.3239", "submitter": "Senniang Chen", "authors": "Senniang Chen and Cindy L Yu", "title": "Generalized Method of Moments Estimator Based On Semiparametric Quantile\n  Regression Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider an imputation method to handle missing response\nvalues based on semiparametric quantile regression estimation. In the proposed\nmethod, the missing response values are generated using the estimated\nconditional quantile regression function at given values of covariates. We\nadopt the generalized method of moments for estimation of parameters defined\nthrough a general estimation equation. We demonstrate that the proposed\nestimator, which combines both semiparametric quantile regression imputation\nand generalized method of moments, has competitive edge against some of the\nmost widely used parametric and non-parametric imputation estimators. The\nconsistency and the asymptotic normality of our estimator are established and\nvariance estimation is provided. Results from a limited simulation study and an\nempirical study are presented to show the adequacy of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 22:32:20 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Chen", "Senniang", ""], ["Yu", "Cindy L", ""]]}, {"id": "1404.3304", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva", "title": "Extremes of Aggregated Dirichlet Risks", "comments": "published version", "journal-ref": "Journal of Multivariate Analysis, 133, 334-345, 2015", "doi": "10.1016/j.jmva.2014.09.018", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of Dirichlet random vectors is central in numerous probabilistic\nand statistical applications. The main result of this paper derives the exact\ntail asymptotics of the aggregated risk of powers of Dirichlet random vectors\nwhen the radial component has df in the Gumbel or the Weibull max-domain of\nattraction. We present further results for the joint asymptotic independence\nand the max-sum equivalence.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 17:15:11 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 15:00:31 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Hashorva", "Enkelejd", ""]]}, {"id": "1404.3397", "submitter": "Anna Bonnet", "authors": "Anna Bonnet and Elisabeth Gassiat and C\\'eline L\\'evy-Leduc", "title": "Heritability estimation in high dimensional linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in genetic fields, we propose to estimate the\nheritability in high dimensional sparse linear mixed models. The heritability\ndetermines how the variance is shared between the different random components\nof a linear mixed model. The main novelty of our approach is to consider that\nthe random effects can be sparse, that is may contain null components, but we\ndo not know neither their proportion nor their positions. The estimator that we\nconsider is strongly inspired by the one proposed by Pirinen et al. (2013), and\nis based on a maximum likelihood approach. We also study the theoretical\nproperties of our estimator, namely we establish that our estimator of the\nheritability is $\\sqrt{n}$-consistent when both the number of observations $n$\nand the number of random effects $N$ tend to infinity under mild assumptions.\nWe also prove that our estimator of the heritability satisfies a central limit\ntheorem which gives as a byproduct a confidence interval for the heritability.\nSome Monte-Carlo experiments are also conducted in order to show the finite\nsample performances of our estimator.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 16:14:10 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 13:23:16 GMT"}, {"version": "v3", "created": "Wed, 6 May 2015 06:13:16 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Bonnet", "Anna", ""], ["Gassiat", "Elisabeth", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""]]}, {"id": "1404.3418", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats, Robert D. Nowak, Richard G. Baraniuk", "title": "Active Learning for Undirected Graphical Model Selection", "comments": "AISTATS 2014", "journal-ref": "Proceedings of the 17th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. JMLR: W&CP\n  volume 33", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies graphical model selection, i.e., the problem of estimating\na graph of statistical relationships among a collection of random variables.\nConventional graphical model selection algorithms are passive, i.e., they\nrequire all the measurements to have been collected before processing begins.\nWe propose an active learning algorithm that uses junction tree representations\nto adapt future measurements based on the information gathered from prior\nmeasurements. We prove that, under certain conditions, our active learning\nalgorithm requires fewer scalar measurements than any passive algorithm to\nreliably estimate a graph. A range of numerical results validate our theory and\ndemonstrates the benefits of active learning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 19:09:43 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Vats", "Divyanshu", ""], ["Nowak", "Robert D.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1404.3441", "submitter": "Annalisa Cerquetti", "authors": "Annalisa Cerquetti", "title": "Bayesian nonparametric estimation of Tsallis diversity indices under\n  Gnedin-Pitman priors", "comments": "17 pages, new improved version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsallis entropy is a generalized diversity index first derived in Patil and\nTaillie (1982) and then rediscovered in community ecology by Keylock (2005).\nBayesian nonparametric estimation of Shannon entropy and Simpson's diversity\nunder uniform and symmetric Dirichlet priors has been already advocated as an\nalternative to maximum likelihood estimation based on frequency counts, which\nis negatively biased in the undersampled regime. Here we present a fully\ngeneral Bayesian nonparametric estimation of the whole class of Tsallis\ndiversity indices under Gnedin-Pitman priors, a large family of random discrete\ndistributions recently deeply investigated in posterior predictive species\nrichness and discovery probability estimation. We provide both prior and\nposterior analysis. The results, illustrated through examples and an\napplication to a real dataset, show the procedure is easily implementable,\nflexible and overcomes limitations of previous frequentist and Bayesian\nsolutions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 00:11:53 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 18:04:24 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Cerquetti", "Annalisa", ""]]}, {"id": "1404.3466", "submitter": "Giovanni Strona", "authors": "Giovanni Strona, Domenico Nappo, Francesco Boccacci, Simone Fattorini,\n  Jesus San-Miguel-Ayanz", "title": "The Babe Ruth Algorithm: a fast, unbiased procedure to randomize\n  presence-absence data matrices with fixed row and column totals", "comments": "Manuscript under review", "journal-ref": "Nature Communications, 5:4114", "doi": "10.1038/ncomms5114", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known problem in numerical ecology is how to recombine\npresence-absence matrices without altering row and column totals. A few\nsolutions have been proposed, but all of them present some issues in terms of\nstatistical robustness (i.e. their capability to generate different matrix\nconfigurations with the same probability) and their performance (i.e. the\ncomputational effort they require to generate a null matrix). Here we introduce\nthe 'Babe Ruth Algorithm', a new procedure that differs from existing ones in\nthat it focuses rather on matrix information content than on matrix structure.\nWe demonstrate that the algorithm can sample uniformly the set of all possible\nmatrix configurations requiring a computational effort orders of magnitude\nlower than that required by available methods, making it possible to easily\nrandomize matrices larger than 10^8 cells.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 06:25:00 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Strona", "Giovanni", ""], ["Nappo", "Domenico", ""], ["Boccacci", "Francesco", ""], ["Fattorini", "Simone", ""], ["San-Miguel-Ayanz", "Jesus", ""]]}, {"id": "1404.3749", "submitter": "Elena Yudovina", "authors": "Yaniv Plan and Roman Vershynin and Elena Yudovina", "title": "High-dimensional estimation with geometric constraints", "comments": "This version incorporates minor revisions suggested by referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider measuring an n-dimensional vector x through the inner product with\nseveral measurement vectors, a_1, a_2, ..., a_m. It is common in both signal\nprocessing and statistics to assume the linear response model y_i = <a_i, x> +\ne_i, where e_i is a noise term. However, in practice the precise relationship\nbetween the signal x and the observations y_i may not follow the linear model,\nand in some cases it may not even be known. To address this challenge, in this\npaper we propose a general model where it is only assumed that each observation\ny_i may depend on a_i only through <a_i, x>. We do not assume that the\ndependence is known. This is a form of the semiparametric single index model,\nand it includes the linear model as well as many forms of the generalized\nlinear model as special cases. We further assume that the signal x has some\nstructure, and we formulate this as a general assumption that x belongs to some\nknown (but arbitrary) feasible set K. We carefully detail the benefit of using\nthe signal structure to improve estimation. The theory is based on the mean\nwidth of K, a geometric parameter which can be used to understand its effective\ndimension in estimation problems. We determine a simple, efficient two-step\nprocedure for estimating the signal based on this model -- a linear estimation\nfollowed by metric projection onto K. We give general conditions under which\nthe estimator is minimax optimal up to a constant. This leads to the intriguing\nconclusion that in the high noise regime, an unknown non-linearity in the\nobservations does not significantly reduce one's ability to determine the\nsignal, even when the non-linearity may be non-invertible. Our results may be\nspecialized to understand the effect of non-linearities in compressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 20:23:17 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 02:56:18 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Plan", "Yaniv", ""], ["Vershynin", "Roman", ""], ["Yudovina", "Elena", ""]]}, {"id": "1404.3763", "submitter": "Andres Santos", "authors": "Zheng Fang and Andres Santos", "title": "Inference on Directionally Differentiable Functions", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an asymptotic framework for conducting inference on\nparameters of the form $\\phi(\\theta_0)$, where $\\phi$ is a known directionally\ndifferentiable function and $\\theta_0$ is estimated by $\\hat \\theta_n$. In\nthese settings, the asymptotic distribution of the plug-in estimator $\\phi(\\hat\n\\theta_n)$ can be readily derived employing existing extensions to the Delta\nmethod. We show, however, that the \"standard\" bootstrap is only consistent\nunder overly stringent conditions -- in particular we establish that\ndifferentiability of $\\phi$ is a necessary and sufficient condition for\nbootstrap consistency whenever the limiting distribution of $\\hat \\theta_n$ is\nGaussian. An alternative resampling scheme is proposed which remains consistent\nwhen the bootstrap fails, and is shown to provide local size control under\nrestrictions on the directional derivative of $\\phi$. We illustrate the utility\nof our results by developing a test of whether a Hilbert space valued parameter\nbelongs to a convex set -- a setting that includes moment inequality problems\nand certain tests of shape restrictions as special cases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 21:51:59 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 17:10:34 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Fang", "Zheng", ""], ["Santos", "Andres", ""]]}, {"id": "1404.3827", "submitter": "Shankar C. Venkataramani", "authors": "Juan M. Restrepo, Shankar C. Venkataramani, Darin Comeau and Hermann\n  Flaschka", "title": "Defining a Trend for a Time Series Which Makes Use of the Intrinsic\n  Time-Scale Decomposition", "comments": "27 Pages, 16 Figures, 1 Appendix", "journal-ref": "New Journal of Physics 16 (8), 085004 (2014)", "doi": "10.1088/1367-2630/16/8/085004", "report-no": null, "categories": "physics.data-an math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose criteria that define a trend for time series with inherent\nmulti-scale features. We call this trend the {\\it tendency} of a time series.\nThe tendency is defined empirically by a set of criteria and captures the\nlarge-scale temporal variability of the original signal as well as the most\nfrequent events in its histogram. Among other properties, the tendency has a\nvariance no larger than that of the original signal; the histogram of the\ndifference between the original signal and the tendency is as symmetric as\npossible; and with reduced complexity, the tendency captures essential features\nof the signal.\n  To find the tendency we first use the Intrinsic Time-Scale Decomposition\n(ITD) of the signal, introduced in 2007 by Frei and Osorio, to produce a set of\ncandidate tendencies. We then apply the criteria to each of the candidates to\nsingle out the one that best agrees with them.\n  While the criteria for the tendency are independent of the signal\ndecomposition scheme, it is found that the ITD is a simple and stable\nmethodology, well suited for multi-scale signals. The ITD is a relatively new\ndecomposition and little is known about its outcomes. In this study we take the\nfirst steps towards a probabilistic model of the ITD analysis of random time\nseries. This analysis yields details concerning the universality and scaling\nproperties of the components of the decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 06:52:30 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Restrepo", "Juan M.", ""], ["Venkataramani", "Shankar C.", ""], ["Comeau", "Darin", ""], ["Flaschka", "Hermann", ""]]}, {"id": "1404.4002", "submitter": "Piero Barone", "authors": "Piero Barone", "title": "Bivariate one-sample optimal location test for spherical stable\n  densities by Pade' methods", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex signal detection in additive noise can be performed by a one-sample\nbivariate location test. Spherical symmetry is assumed for the noise density as\nwell as closedness with respect to linear transformation. Therefore the noise\nis assumed to have spherical distribution with $\\alpha-$stable radial density.\nIn order to cope with this difficult setting the original sample is transformed\nby Pade' methods giving rise to a new sample with universality properties. The\nstability assumption is then reduced to the Gaussian one and it is proved that\na known van der Waerden type test, with optimal properties, based on the new\nsample can be used. Furthermore a new test in the same class of optimal tests\nis proposed which is more powerful that the van der Waerden type one.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 18:11:03 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Barone", "Piero", ""]]}, {"id": "1404.4032", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently established RPCA method provides us a convenient way to restore\nlow-rank matrices from grossly corrupted observations. While elegant in theory\nand powerful in reality, RPCA may be not an ultimate solution to the low-rank\nmatrix recovery problem. Indeed, its performance may not be perfect even when\ndata are strictly low-rank. This is because conventional RPCA ignores the\nclustering structures of the data which are ubiquitous in modern applications.\nAs the number of cluster grows, the coherence of data keeps increasing, and\naccordingly, the recovery performance of RPCA degrades. We show that the\nchallenges raised by coherent data (i.e., the data with high coherence) could\nbe alleviated by Low-Rank Representation (LRR), provided that the dictionary in\nLRR is configured appropriately. More precisely, we mathematically prove that\nif the dictionary itself is low-rank then LRR is immune to the coherence\nparameter which increases with the underlying cluster number. This provides an\nelementary principle for dealing with coherent data. Subsequently, we devise a\npractical algorithm to obtain proper dictionaries in unsupervised environments.\nOur extensive experiments on randomly generated matrices verify our claims.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 19:35:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 17:57:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4093", "submitter": "Jan Mandel", "authors": "Evan Kwiatkowski and Jan Mandel", "title": "Convergence of the Square Root Ensemble Kalman Filter in the Large\n  Ensemble Limit", "comments": "14 pages, final draft", "journal-ref": "SIAM/ASA J. Uncertainty Quantification 3 (1), 1-17, 2015", "doi": "10.1137/140965363", "report-no": "UCD CCM Report 321", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble filters implement sequential Bayesian estimation by representing the\nprobability distribution by an ensemble mean and covariance. Unbiased square\nroot ensemble filters use deterministic algorithms to produce an analysis\n(posterior) ensemble with prescribed mean and covariance, consistent with the\nKalman update. This includes several filters used in practice, such as the\nEnsemble Transform Kalman Filter (ETKF), the Ensemble Adjustment Kalman Filter\n(EAKF), and a filter by Whitaker and Hamill. We show that at every time index,\nas the number of ensemble members increases to infinity, the mean and\ncovariance of an unbiased ensemble square root filter converge to those of the\nKalman filter, in the case a linear model and an initial distribution of which\nall moments exist. The convergence is in $L^{p}$ and the convergence rate does\nnot depend on the model dimension. The result holds in the infinitely\ndimensional Hilbert space as well.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:03:34 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 03:01:06 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Kwiatkowski", "Evan", ""], ["Mandel", "Jan", ""]]}, {"id": "1404.4210", "submitter": "Anna Leister", "authors": "Grigory Alexandrovich, Hajo Holzmann and Anna Leister", "title": "Nonparametric identification and maximum likelihood estimation for\n  hidden Markov model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric identification and maximum likelihood estimation for\nfinite-state hidden Markov models are investigated. We obtain identification of\nthe parameters as well as the order of the Markov chain if the transition\nprobability matrices have full-rank and are ergodic, and if the state-dependent\ndistributions are all distinct, but not necessarily linearly independent. Based\non this identification result, we develop nonparametric maximum likelihood\nestimation theory. First, we show that the asymptotic contrast, the\nKullback--Leibler divergence of the hidden Markov model, identifies the true\nparameter vector nonparametrically as well. Second, for classes of\nstate-dependent densities which are arbitrary mixtures of a parametric family,\nwe show consistency of the nonparametric maximum likelihood estimator. Here,\nidentification of the mixing distributions need not be assumed. Numerical\nproperties of the estimates as well as of nonparametric goodness of fit tests\nare investigated in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 11:28:03 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 08:55:26 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Alexandrovich", "Grigory", ""], ["Holzmann", "Hajo", ""], ["Leister", "Anna", ""]]}, {"id": "1404.4235", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent and Christopher S. Henry", "title": "A Broadened Approach for Improved Estimation in Survey Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sufficient statistic for the population parameter vector\nby allowing for the sampling design to first be selected at random amongst a\nset of candidate sampling designs. In contrast to the traditional approach in\nsurvey sampling, we achieve this by defining the observed data to include a\nmention of the sampling design used for the data collection aspect of the\nstudy. We show that the reduced data consisting of the unit labels together\nwith their corresponding responses of interest is a sufficient statistic under\nthis setup. A Rao-Blackwellization inference procedure is outlined and it is\nshown how averaging over hypothetical observed data outcomes results in\nimproved estimators; the improved strategy includes considering all possible\nsampling designs in the candidate set that could have given rise to the reduced\ndata. Expressions for the variance of the Rao-Blackwell estimators are also\nderived. The results from two simulation studies are presented to demonstrate\nthe practicality of our approach. A discussion on how our approach can be\nuseful when the analyst has limited information on the data collection\nprocedure is also provided.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 13:29:54 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 12:52:28 GMT"}, {"version": "v3", "created": "Sun, 9 Nov 2014 18:05:24 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Vincent", "Kyle", ""], ["Henry", "Christopher S.", ""]]}, {"id": "1404.4408", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang and Alexander Rakhlin", "title": "Geometric Inference for General High-Dimensional Linear Inverse Problems", "comments": "39 pages, 6 figures", "journal-ref": "The Annals of Statistics 44 (2016) 1536-1563", "doi": "10.1214/15-AOS1426", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified geometric framework for the statistical\nanalysis of a general ill-posed linear inverse model which includes as special\ncases noisy compressed sensing, sign vector recovery, trace regression,\northogonal matrix estimation, and noisy matrix completion. We propose\ncomputationally feasible convex programs for statistical inference including\nestimation, confidence intervals and hypothesis testing. A theoretical\nframework is developed to characterize the local estimation rate of convergence\nand to provide statistical inference guarantees. Our results are built based on\nthe local conic geometry and duality. The difficulty of statistical inference\nis captured by the geometric characterization of the local tangent cone through\nthe Gaussian width and Sudakov minoration estimate.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 01:10:47 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 17:27:23 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 22:51:29 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1404.4414", "submitter": "Gery Geenens", "authors": "Gery Geenens, Arthur Charpentier and Davy Paindaveine", "title": "Probit transformation for nonparametric kernel estimation of the copula\n  density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula modelling has become ubiquitous in modern statistics. Here, the\nproblem of nonparametrically estimating a copula density is addressed. Arguably\nthe most popular nonparametric density estimator, the kernel estimator is not\nsuitable for the unit-square-supported copula densities, mainly because it is\nheavily affected by boundary bias issues. In addition, most common copulas\nadmit unbounded densities, and kernel methods are not consistent in that case.\nIn this paper, a kernel-type copula density estimator is proposed. It is based\non the idea of transforming the uniform marginals of the copula density into\nnormal distributions via the probit function, estimating the density in the\ntransformed domain, which can be accomplished without boundary problems, and\nobtaining an estimate of the copula density through back-transformation.\nAlthough natural, a raw application of this procedure was, however, seen not to\nperform very well in the earlier literature. Here, it is shown that, if\ncombined with local likelihood density estimation methods, the idea yields very\ngood and easy to implement estimators, fixing boundary issues in a natural way\nand able to cope with unbounded copula densities. The asymptotic properties of\nthe suggested estimators are derived, and a practical way of selecting the\ncrucially important smoothing parameters is devised. Finally, extensive\nsimulation studies and a real data analysis evidence their excellent\nperformance compared to their main competitors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 02:21:00 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Geenens", "Gery", ""], ["Charpentier", "Arthur", ""], ["Paindaveine", "Davy", ""]]}, {"id": "1404.4441", "submitter": "Amadou Sarr", "authors": "Amadou Sarr", "title": "On a Kotz-Wishart distribution: Multivariate Varma transform", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduced a generalized Wishart distribution, namely, the Kotz-Wishart\ndistribution. Several existing results based on the normality assumption have\nbeen extended. Inspired by the particular form of the pdf of the Kotz-Wishart\nmatrix, we extended the Laplace transform operator. This generalized Laplace\ntransform is applied to some functions of matrix argument.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 07:54:19 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Sarr", "Amadou", ""]]}, {"id": "1404.4452", "submitter": "M{\\aa}ns Thulin", "authors": "Maik G\\\"orgens and M{\\aa}ns Thulin", "title": "Bias-correction of the maximum likelihood estimator for the\n  $\\alpha$-Brownian bridge", "comments": null, "journal-ref": "Statistics and Probability Letters, 93, 78-86 (2014)", "doi": "10.1016/j.spl.2014.06.020", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\alpha$-Brownian bridge, or scaled Brownian bridge, is a generalization\nof the Brownian bridge with a scaling parameter that determines how strong the\nforce that pulls the process back to 0 is. The bias of the maximum likelihood\nestimator of the parameter $\\alpha$ is derived and a bias-correction that\nimproves the estimator substantially is proposed. The properties of the\nbias-corrected estimator and four Bayesian estimators based on non-informative\npriors are evaluated in a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 08:38:27 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["G\u00f6rgens", "Maik", ""], ["Thulin", "M\u00e5ns", ""]]}, {"id": "1404.4605", "submitter": "Stefan Birr", "authors": "Stefan Birr, Stanislav Volgushev, Tobias Kley, Holger Dette, Marc\n  Hallin", "title": "Quantile Spectral Analysis for Locally Stationary Time Series", "comments": "AMS 1980 subject classification : 62M15, 62G35. Key words and phrases\n  : Copulas, Nonstationarity, Ranks, Periodogram, Laplace spectrum", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical spectral methods are subject to two fundamental limitations: they\nonly can account for covariance-related serial dependencies, and they require\nsecond-order stationarity. Much attention has been devoted lately to\nquantile-based spectral methods that go beyond covariance-based serial\ndependence features. At the same time, covariance-based methods relaxing\nstationarity into much weaker {\\it local stationarity} conditions have been\ndeveloped for a variety of time-series models. Here, we are combining those two\napproaches by proposing quantile-based spectral methods for locally stationary\nprocesses. We therefore introduce a time-varying version of the copula spectra\nthat have been recently proposed in the literature, along with a suitable local\nlag-window estimator. We propose a new definition of local {\\it strict}\nstationarity that allows us to handle completely general non-linear processes\nwithout any moment assumptions, thus accommodating our quantile-based concepts\nand methods. We establish a central limit theorem for the new estimators, and\nillustrate the power of the proposed methodology by means of a simulation\nstudy. Moreover, in two empirical studies (namely of the Standard \\& Poor's 500\nseries and a temperature dataset recorded in Hohenpeissenberg) we demonstrate\nthat the new approach detects important variations in serial dependence\nstructures both across time and across quantiles. Such variations remain\ncompletely undetected, and are actually undetectable, via classical\ncovariance-based spectral methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 18:44:50 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 11:29:30 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 08:58:26 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Birr", "Stefan", ""], ["Volgushev", "Stanislav", ""], ["Kley", "Tobias", ""], ["Dette", "Holger", ""], ["Hallin", "Marc", ""]]}, {"id": "1404.4646", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Advancing Matrix Completion by Modeling Extra Structures beyond\n  Low-Rankness", "comments": "arXiv admin note: text overlap with arXiv:1404.4032", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known method for completing low-rank matrices based on convex\noptimization has been established by Cand{\\`e}s and Recht. Although\ntheoretically complete, the method may not entirely solve the low-rank matrix\ncompletion problem. This is because the method captures only the low-rankness\nproperty which gives merely a rough constraint that the data points locate on\nsome low-dimensional subspace, but generally ignores the extra structures which\nspecify in more detail how the data points locate on the subspace. Whenever the\ngeometric distribution of the data points is not uniform, the coherence\nparameters of data might be large and, accordingly, the method might fail even\nif the latent matrix we want to recover is fairly low-rank. To better handle\nnon-uniform data, in this paper we propose a method termed Low-Rank Factor\nDecomposition (LRFD), which imposes an additional restriction that the data\npoints must be represented as linear combinations of the bases in a dictionary\nconstructed or learnt in advance. We show that LRFD can well handle non-uniform\ndata, provided that the dictionary is configured properly: We mathematically\nprove that if the dictionary itself is low-rank then LRFD is immune to the\ncoherence parameters which might be large on non-uniform data. This provides an\nelementary principle for learning the dictionary in LRFD and, naturally, leads\nto a practical algorithm for advancing matrix completion. Extensive experiments\non randomly generated matrices and motion datasets show encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:50:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 18:04:35 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4730", "submitter": "Dimitris Cheliotis", "authors": "Dimitris Cheliotis", "title": "Triangular random matrices and biorthogonal ensembles", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the singular values of certain triangular random matrices. When\ntheir elements are i.i.d. standard complex Gaussian random variables, the\nsquares of the singular values form a biorthogonal ensemble, and with an\nappropriate change in the distribution of the diagonal elements, they give the\nbiorthogonal Laguerre ensemble. For triangular Wigner matrices, we give\nalternative proofs for the convergence of the empirical distribution of the\nappropriately scaled squares of the singular eigenvalues to a distribution with\nsupport $[0, e]$, as well as for the almost sure convergence of the rescaled\nlargest singular eigenvalue to $\\sqrt{e}$ under the additional assumption of\nmean zero and finite fourth moment for the law of the matrix elements.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 09:37:45 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Cheliotis", "Dimitris", ""]]}, {"id": "1404.4884", "submitter": "David Eubanks", "authors": "David A. Eubanks", "title": "Causal Interfaces", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction of two binary variables, assumed to be empirical\nobservations, has three degrees of freedom when expressed as a matrix of\nfrequencies. Usually, the size of causal influence of one variable on the other\nis calculated as a single value, as increase in recovery rate for a medical\ntreatment, for example. We examine what is lost in this simplification, and\npropose using two interface constants to represent positive and negative\nimplications separately. Given certain assumptions about non-causal outcomes,\nthe set of resulting epistemologies is a continuum. We derive a variety of\nparticular measures and contrast them with the one-dimensional index.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 20:51:54 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Eubanks", "David A.", ""]]}, {"id": "1404.4917", "submitter": "Daniel Andr\\'es D\\'iaz Pach\\'on", "authors": "Daniel A. Diaz-Pachon, J. Sunil Rao, Jean-Eudes Dazard", "title": "On the explanatory power of principal components", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that if we have an orthogonal base ($u_1,\\ldots,u_p$) in a\n$p$-dimensional vector space, and select $p+1$ vectors $v_1,\\ldots, v_p$ and\n$w$ such that the vectors traverse the origin, then the probability of $w$\nbeing to closer to all the vectors in the base than to $v_1,\\ldots, v_p$ is at\nleast 1/2 and converges as $p$ increases to infinity to a normal distribution\non the interval [-1,1]; i.e., $\\Phi(1)-\\Phi(-1)\\approx0.6826$. This result has\nrelevant consequences for Principal Components Analysis in the context of\nregression and other learning settings, if we take the orthogonal base as the\ndirection of the principal components.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 04:00:13 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Diaz-Pachon", "Daniel A.", ""], ["Rao", "J. Sunil", ""], ["Dazard", "Jean-Eudes", ""]]}, {"id": "1404.5126", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Ayanendranath Basu, Leandro Pardo", "title": "On the Robustness of a Divergence based Test of Simple Statistical\n  Hypotheses", "comments": "Pre-print Version, 25 pages, 5 figures", "journal-ref": "Journal of Statistical Planning and Inference, 2015, 161, 91-108", "doi": "10.1016/j.jspi.2015.01.003", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular hypothesis testing procedure, the likelihood ratio test, is\nknown to be highly non-robust in many real situations. Basu et al. (2013a)\nprovided an alternative robust procedure of hypothesis testing based on the\ndensity power divergence; however, although the robustness properties of the\nlatter test were intuitively argued for by the authors together with extensive\nempirical substantiation of the same, no theoretical robustness properties were\npresented in this work. In the present paper we will consider a more general\nclass of tests which form a superfamily of the procedures described by Basu et\nal. (2013a). This superfamily derives from the class of $S$-divergences\nrecently proposed by Basu et al. (2013a). In this context we theoretically\nprove several robustness results of the new class of tests and illustrate them\nin the normal model. All the theoretical robustness properties of the Basu et\nal. (2013a) proposal follows as special cases of our results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 06:58:59 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 10:38:46 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1404.5406", "submitter": "Shrisha Rao", "authors": "Avinash Saxena, Shrisha Rao", "title": "Degradation Analysis of Probabilistic Parallel Choice Systems", "comments": null, "journal-ref": "International Journal of Reliability, Quality and Safety\n  Engineering, vol. 21(3), June 2014", "doi": "10.1142/S0218539314500120", "report-no": null, "categories": "cs.PF math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degradation analysis is used to analyze the useful lifetimes of systems,\ntheir failure rates, and various other system parameters like mean time to\nfailure (MTTF), mean time between failures (MTBF), and the system failure rate\n(SFR). In many systems, certain possible parallel paths of execution that have\ngreater chances of success are preferred over others. Thus we introduce here\nthe concept of probabilistic parallel choice. We use binary and $n$-ary\nprobabilistic choice operators in describing the selections of parallel paths.\nThese binary and $n$-ary probabilistic choice operators are considered so as to\nrepresent the complete system (described as a series-parallel system) in terms\nof the probabilities of selection of parallel paths and their relevant\nparameters. Our approach allows us to derive new and generalized formulae for\nsystem parameters like MTTF, MTBF, and SFR. We use a generalized exponential\ndistribution, allowing distinct installation times for individual components,\nand use this model to derive expressions for such system parameters.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 07:40:51 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Saxena", "Avinash", ""], ["Rao", "Shrisha", ""]]}, {"id": "1404.5465", "submitter": "Nirian Mart\\'in", "authors": "Nirian Martin and Isabel Molina", "title": "Best prediction under a nested error model with log transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression models involving economic variables such as income, log\ntransformation is typically taken to achieve approximate normality and\nstabilize the variance. However, often the interest is predicting individual\nvalues or means of the variable in the original scale. Back transformation of\npredicted values introduces a non-negligible bias. Moreover, assessing the\nuncertainty of the actual predictor is not straightforward. In this paper, a\nnested error model for the log transformation of the target variable is\nconsidered. Nested error models are widely used for estimation of means in\nsubpopulations with small sample sizes (small areas), by linking all the areas\nthrough common parameters. These common parameters are estimated using the\noverall set of sample data, which leads to much more efficient small area\nestimators. Analytical expressions for the best predictors of individual values\nof the original variable and of small area means are obtained under the nested\nerror model with log transformation of the target variable. Empirical best\npredictors are defined by estimating the unknown model parameters in the best\npredictors. Exact mean squared errors of the best predictors and second order\napproximations to the mean squared errors of the empirical best predictors are\nderived. Mean squared error estimators that are second order correct are also\nobtained. An example with Mexican data on living conditions illustrates the\nprocedures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 12:07:07 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 06:02:20 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Martin", "Nirian", ""], ["Molina", "Isabel", ""]]}, {"id": "1404.5557", "submitter": "Samuel Vaiter", "authors": "Samuel Vaiter (CEREMADE), Charles-Alban Deledalle (IMB), Jalal M.\n  Fadili, Gabriel Peyr\\'e (CEREMADE), Charles Dossal (IMB)", "title": "The Degrees of Freedom of Partly Smooth Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with regularized regression problems where\nthe prior regularizer is a proper lower semicontinuous and convex function\nwhich is also partly smooth relative to a Riemannian submanifold. This\nencompasses as special cases several known penalties such as the Lasso\n($\\ell^1$-norm), the group Lasso ($\\ell^1-\\ell^2$-norm), the\n$\\ell^\\infty$-norm, and the nuclear norm. This also includes so-called\nanalysis-type priors, i.e. composition of the previously mentioned penalties\nwith linear operators, typical examples being the total variation or fused\nLasso penalties.We study the sensitivity of any regularized minimizer to\nperturbations of the observations and provide its precise local\nparameterization.Our main sensitivity analysis result shows that the predictor\nmoves locally stably along the same active submanifold as the observations\nundergo small perturbations. This local stability is a consequence of the\nsmoothness of the regularizer when restricted to the active submanifold, which\nin turn plays a pivotal role to get a closed form expression for the variations\nof the predictor w.r.t. observations. We also show that, for a variety of\nregularizers, including polyhedral ones or the group Lasso and its analysis\ncounterpart, this divergence formula holds Lebesgue almost everywhere.When the\nperturbation is random (with an appropriate continuous distribution), this\nallows us to derive an unbiased estimator of the degrees of freedom and of the\nrisk of the estimator prediction.Our results hold true without requiring the\ndesign matrix to be full column rank.They generalize those already known in the\nliterature such as the Lasso problem, the general Lasso problem (analysis\n$\\ell^1$-penalty), or the group Lasso where existing results for the latter\nassume that the design is full column rank.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 17:04:45 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 13:54:36 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 14:26:15 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2016 18:27:24 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Vaiter", "Samuel", "", "CEREMADE"], ["Deledalle", "Charles-Alban", "", "IMB"], ["Fadili", "Jalal M.", "", "CEREMADE"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"], ["Dossal", "Charles", "", "IMB"]]}, {"id": "1404.5609", "submitter": "Rina Foygel Barber", "authors": "Rina Foygel Barber, Emmanuel J. Cand\\`es", "title": "Controlling the false discovery rate via knockoffs", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1337 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 2055-2085", "doi": "10.1214/15-AOS1337", "report-no": "IMS-AOS-AOS1337", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of science, we observe a response variable together with a\nlarge number of potential explanatory variables, and would like to be able to\ndiscover which variables are truly associated with the response. At the same\ntime, we need to know that the false discovery rate (FDR) - the expected\nfraction of false discoveries among all discoveries - is not too high, in order\nto assure the scientist that most of the discoveries are indeed true and\nreplicable. This paper introduces the knockoff filter, a new variable selection\nprocedure controlling the FDR in the statistical linear model whenever there\nare at least as many observations as variables. This method achieves exact FDR\ncontrol in finite sample settings no matter the design or covariates, the\nnumber of variables in the model, or the amplitudes of the unknown regression\ncoefficients, and does not require any knowledge of the noise level. As the\nname suggests, the method operates by manufacturing knockoff variables that are\ncheap - their construction does not require any new data - and are designed to\nmimic the correlation structure found within the existing variables, in a way\nthat allows for accurate FDR control, beyond what is possible with\npermutation-based methods. The method of knockoffs is very general and\nflexible, and can work with a broad class of test statistics. We test the\nmethod in combination with statistics from the Lasso for sparse regression, and\nobtain empirical results showing that the resulting method has far more power\nthan existing selection rules when the proportion of null variables is high.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 19:56:40 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 14:42:12 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 07:54:32 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1404.5886", "submitter": "Jon A. Wellner", "authors": "Adrien Saumard and Jon A. Wellner", "title": "Log-concavity and strong log-concavity: a review", "comments": "67 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review and formulate results concerning log-concavity and\nstrong-log-concavity in both discrete and continuous settings. We show how\npreservation of log-concavity and strongly log-concavity on $\\mathbb{R}$ under\nconvolution follows from a fundamental monotonicity result of Efron (1969). We\nprovide a new proof of Efron's theorem using the recent asymmetric\nBrascamp-Lieb inequality due to Otto and Menz (2013). Along the way we review\nconnections between log-concavity and other areas of mathematics and\nstatistics, including concentration of measure, log-Sobolev inequalities,\nconvex geometry, MCMC algorithms, Laplace approximations, and machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 16:49:34 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Saumard", "Adrien", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1404.6000", "submitter": "T. Tony Cai", "authors": "T. Tony Cai, Xiaodong Li", "title": "Robust and computationally feasible community detection in the presence\n  of arbitrary outlier nodes", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1290 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1027-1059", "doi": "10.1214/14-AOS1290", "report-no": "IMS-AOS-AOS1290", "categories": "math.ST cs.IT math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection, which aims to cluster $N$ nodes in a given graph into\n$r$ distinct groups based on the observed undirected edges, is an important\nproblem in network data analysis. In this paper, the popular stochastic block\nmodel (SBM) is extended to the generalized stochastic block model (GSBM) that\nallows for adversarial outlier nodes, which are connected with the other nodes\nin the graph in an arbitrary way. Under this model, we introduce a procedure\nusing convex optimization followed by $k$-means algorithm with $k=r$. Both\ntheoretical and numerical properties of the method are analyzed. A theoretical\nguarantee is given for the procedure to accurately detect the communities with\nsmall misclassification rate under the setting where the number of clusters can\ngrow with $N$. This theoretical result admits to the best-known result in the\nliterature of computationally feasible community detection in SBM without\noutliers. Numerical results show that our method is both computationally fast\nand robust to different kinds of outliers, while some popular computationally\nfast community detection algorithms, such as spectral clustering applied to\nadjacency matrices or graph Laplacians, may fail to retrieve the major clusters\ndue to a small portion of outliers. We apply a slight modification of our\nmethod to a political blogs data set, showing that our method is competent in\npractice and comparable to existing computationally feasible methods in the\nliterature. To the best of the authors' knowledge, our result is the first in\nthe literature in terms of clustering communities with fast growing numbers\nunder the GSBM where a portion of arbitrary outlier nodes exist.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 23:36:19 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 14:50:49 GMT"}, {"version": "v3", "created": "Fri, 21 Nov 2014 16:11:01 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 06:00:00 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Cai", "T. Tony", ""], ["Li", "Xiaodong", ""]]}, {"id": "1404.6224", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel (CREST)", "title": "Convex set detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of one dimensional segment detection and estimation,\nin a regression setup. At each point of a fixed or random design, one observes\nwhether that point belongs to the unknown segment or not, up to some additional\nnoise. We try to understand what the minimal size of the segment is so it can\nbe accurately seen by some statistical procedure, and how this minimal size\ndepends on some a priori knowledge about the location of the unknown segment.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:44:42 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Brunel", "Victor-Emmanuel", "", "CREST"]]}, {"id": "1404.6298", "submitter": "Luke Bornn", "authors": "Luke Bornn, Natesh Pillai, Aaron Smith, Dawn Woodard", "title": "The Use of a Single Pseudo-Sample in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational efficiency of approximate Bayesian computation\n(ABC), which approximates a likelihood function by drawing pseudo-samples from\nthe associated model. For the rejection sampling version of ABC, it is known\nthat multiple pseudo-samples cannot substantially increase (and can\nsubstantially decrease) the efficiency of the algorithm as compared to\nemploying a high-variance estimate based on a single pseudo-sample. We show\nthat this conclusion also holds for a Markov chain Monte Carlo version of ABC,\nimplying that it is unnecessary to tune the number of pseudo-samples used in\nABC-MCMC. This conclusion is in contrast to particle MCMC methods, for which\nincreasing the number of particles can provide large gains in computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 01:13:02 GMT"}, {"version": "v2", "created": "Fri, 18 Jul 2014 03:30:20 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 17:51:48 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 21:08:42 GMT"}, {"version": "v5", "created": "Tue, 16 Feb 2016 22:07:01 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Bornn", "Luke", ""], ["Pillai", "Natesh", ""], ["Smith", "Aaron", ""], ["Woodard", "Dawn", ""]]}, {"id": "1404.6392", "submitter": "Johannes Rauh", "authors": "Johannes Rauh and Seth Sullivant", "title": "Lifting Markov Bases and Higher Codimension Toric Fiber Products", "comments": "40 pages, 6 figures. Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to lift Markov bases and Gr\\\"obner bases along linear maps of\nlattices. We give a lifting algorithm that allows to compute such bases\niteratively provided a certain associated semigroup is normal. Our main\napplication is the toric fiber product of toric ideals, where lifting gives\nMarkov bases of the factor ideals that satisfy the compatible projection\nproperty. We illustrate the technique by computing Markov bases of various\ninfinite families of hierarchical models. The methodology also implies new\nfiniteness results for iterated toric fiber products.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:57:34 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 15:53:56 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 22:56:00 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Rauh", "Johannes", ""], ["Sullivant", "Seth", ""]]}, {"id": "1404.6572", "submitter": "Rafael Stern", "authors": "Fernando Vieira Bonassi, Rafael Bassi Stern, Sergio Wechsler, Claudia\n  Monteiro Peixoto", "title": "Exchangeability and the Law of Maturity", "comments": "12 pages, 3 figures", "journal-ref": "Theory and Decision, 2014. The final publication is available at\n  springerlink.com", "doi": "10.1007/s11238-014-9441-4", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The law of maturity is the belief that less-observed events are becoming\nmature and, therefore, more likely to occur in the future. Previous studies\nhave shown that the assumption of infinite exchangeability contradicts the law\nof maturity. In particular, it has been shown that infinite exchangeability\ncontradicts probabilistic descriptions of the law of maturity such as the\ngambler's belief and the belief in maturity. We show that the weaker assumption\nof finite exchangeability is compatible with both the gambler's belief and\nbelief in maturity. We provide sufficient conditions under which these beliefs\nhold under finite exchangeability. These conditions are illustrated with\ncommonly used parametric models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 22:24:18 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Bonassi", "Fernando Vieira", ""], ["Stern", "Rafael Bassi", ""], ["Wechsler", "Sergio", ""], ["Peixoto", "Claudia Monteiro", ""]]}, {"id": "1404.6640", "submitter": "Martin Slawski", "authors": "Martin Slawski, Matthias Hein", "title": "Estimation of positive definite M-matrices and structure learning for\n  attractive Gaussian Markov Random fields", "comments": "long version of a manuscript accepted for publication in Linear\n  Algebra and its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a random vector with finite second moments. If its precision matrix\nis an M-matrix, then all partial correlations are non-negative. If that random\nvector is additionally Gaussian, the corresponding Markov random field (GMRF)\nis called attractive. We study estimation of M-matrices taking the role of\ninverse second moment or precision matrices using sign-constrained\nlog-determinant divergence minimization. We also treat the high-dimensional\ncase with the number of variables exceeding the sample size. The additional\nsign-constraints turn out to greatly simplify the estimation problem: we\nprovide evidence that explicit regularization is no longer required. To solve\nthe resulting convex optimization problem, we propose an algorithm based on\nblock coordinate descent, in which each sub-problem can be recast as\nnon-negative least squares problem. Illustrations on both simulated and real\nworld data are provided.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 13:22:37 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Slawski", "Martin", ""], ["Hein", "Matthias", ""]]}, {"id": "1404.6769", "submitter": "Christophe Giraud", "authors": "Christophe Giraud, Fran\\c{c}ois Roueff, Andres Sanchez-Perez", "title": "Aggregation of predictors for nonstationary sub-linear processes and\n  online adaptive forecasting of time varying autoregressive processes", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1345 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2412-2450", "doi": "10.1214/15-AOS1345", "report-no": "IMS-AOS-AOS1345", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of aggregating a finite number of\npredictors for nonstationary sub-linear processes. We provide oracle\ninequalities relying essentially on three ingredients: (1) a uniform bound of\nthe $\\ell^1$ norm of the time varying sub-linear coefficients, (2) a Lipschitz\nassumption on the predictors and (3) moment conditions on the noise appearing\nin the linear representation. Two kinds of aggregations are considered giving\nrise to different moment conditions on the noise and more or less sharp oracle\ninequalities. We apply this approach for deriving an adaptive predictor for\nlocally stationary time varying autoregressive (TVAR) processes. It is obtained\nby aggregating a finite number of well chosen predictors, each of them enjoying\nan optimal minimax convergence rate under specific smoothness conditions on the\nTVAR coefficients. We show that the obtained aggregated predictor achieves a\nminimax rate while adapting to the unknown smoothness. To prove this result, a\nlower bound is established for the minimax rate of the prediction risk for the\nTVAR process. Numerical experiments complete this study. An important feature\nof this approach is that the aggregated predictor can be computed recursively\nand is thus applicable in an online prediction context.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 15:26:52 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 18:47:09 GMT"}, {"version": "v3", "created": "Thu, 6 Nov 2014 12:43:57 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2015 17:26:27 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2015 10:11:03 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Giraud", "Christophe", ""], ["Roueff", "Fran\u00e7ois", ""], ["Sanchez-Perez", "Andres", ""]]}, {"id": "1404.6841", "submitter": "Brian St. Thomas", "authors": "Brian St. Thomas, Lizhen Lin, Lek-Heng Lim, Sayan Mukherjee", "title": "Learning Subspaces of Different Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian model for inferring mixtures of subspaces of\ndifferent dimensions. The key challenge in such a mixture model is\nspecification of prior distributions over subspaces of different dimensions. We\naddress this challenge by embedding subspaces or Grassmann manifolds into a\nsphere of relatively low dimension and specifying priors on the sphere. We\nprovide an efficient sampling algorithm for the posterior distribution of the\nmodel parameters. We illustrate that a simple extension of our mixture of\nsubspaces model can be applied to topic modeling. We also prove posterior\nconsistency for the mixture of subspaces model. The utility of our approach is\ndemonstrated with applications to real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 23:45:05 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 20:13:01 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 01:31:39 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Thomas", "Brian St.", ""], ["Lin", "Lizhen", ""], ["Lim", "Lek-Heng", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1404.6875", "submitter": "Adrian Barker", "authors": "Adrian W. Barker", "title": "On the log quantile difference of the temporal aggregation of a stable\n  moving average process", "comments": "13 pages, 3 figures. Most of this paper is included in chapter 2 of\n  my PhD thesis, which is yet to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formula is derived for the log quantile difference of the temporal\naggregation of some types of stable moving average processes, MA(q). The shape\nof the log quantile difference as a function of the aggregation level is\nexamined and shown to be dependent on the parameters of the moving average\nprocess but not the quantile levels. The classes of invertible, stable MA(1)\nand MA(2) processes are examined in more detail.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 06:29:02 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Barker", "Adrian W.", ""]]}, {"id": "1404.6989", "submitter": "Elizabeth Gross", "authors": "Elizabeth Gross and Seth Sullivant", "title": "The Maximum Likelihood Threshold of a Graph", "comments": "Added Section 6 and Section 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood threshold of a graph is the smallest number of data\npoints that guarantees that maximum likelihood estimates exist almost surely in\nthe Gaussian graphical model associated to the graph. We show that this graph\nparameter is connected to the theory of combinatorial rigidity. In particular,\nif the edge set of a graph $G$ is an independent set in the $n-1$-dimensional\ngeneric rigidity matroid, then the maximum likelihood threshold of $G$ is less\nthan or equal to $n$. This connection allows us to prove many results about the\nmaximum likelihood threshold.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 14:07:58 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 19:36:13 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2015 02:07:22 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Gross", "Elizabeth", ""], ["Sullivant", "Seth", ""]]}, {"id": "1404.7055", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, and Sebastien Roch", "title": "Data Requirement for Phylogenetic Inference from Multiple Loci: A New\n  Distance Method", "comments": "19 pages, 2 figures. Preliminary version to appear in IEEE ISIT 2014.\n  Added acknowledgements and made the proof of the \"equality\" part of Theorem 3\n  explicit in Appendix C", "journal-ref": null, "doi": "10.1109/TCBB.2014.2361685", "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the evolutionary history of a set of\nspecies (phylogeny or species tree) from several genes. It is known that the\nevolutionary history of individual genes (gene trees) might be topologically\ndistinct from each other and from the underlying species tree, possibly\nconfounding phylogenetic analysis. A further complication in practice is that\none has to estimate gene trees from molecular sequences of finite length. We\nprovide the first full data-requirement analysis of a species tree\nreconstruction method that takes into account estimation errors at the gene\nlevel. Under that criterion, we also devise a novel reconstruction algorithm\nthat provably improves over all previous methods in a regime of interest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 16:54:20 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 09:53:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1404.7080", "submitter": "Mariela  Sued", "authors": "Graciela Boente, Daniela Rodriguez, Mariela Sued", "title": "Testing equality between several populations covariance operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations, when dealing with several populations, equality of the\ncovariance operators is assumed. An important issue is to study if this\nassumption holds before making other inferences. In this paper, we develop a\ntest for comparing covariance operators of several functional data samples. The\nproposed test is based on the Hilbert--Schmidt norm of the difference between\nestimated covariance operators. In particular, when dealing with two\npopulations, the tests statistic is just the squared norm of the difference\nbetween the two covariance operators estimators. The asymptotic behaviour of\nthe test statistic under the null and under local alternatives is obtained.\nSince the statistic null asymptotic distribution does not allow to obtain\neasily its quantiles, a bootstrap procedure to compute the critical values is\nconsidered. The performance of the test statistics for small sample sizes is\nillustrated through a Monte Carlo study.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 18:19:36 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 19:58:27 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Boente", "Graciela", ""], ["Rodriguez", "Daniela", ""], ["Sued", "Mariela", ""]]}, {"id": "1404.7105", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Andrea J. Goldsmith", "title": "Information Recovery from Pairwise Measurements", "comments": "This version is no longer updated -- please find the latest version\n  at (arXiv:1504.01369)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of information processing tasks in practice involve recovering $n$\nobjects from single-shot graph-based measurements, particularly those taken\nover the edges of some measurement graph $\\mathcal{G}$. This paper concerns the\nsituation where each object takes value over a group of $M$ different values,\nand where one is interested to recover all these values based on observations\nof certain pairwise relations over $\\mathcal{G}$. The imperfection of\nmeasurements presents two major challenges for information recovery: 1)\n$\\textit{inaccuracy}$: a (dominant) portion $1-p$ of measurements are\ncorrupted; 2) $\\textit{incompleteness}$: a significant fraction of pairs are\nunobservable, i.e. $\\mathcal{G}$ can be highly sparse.\n  Under a natural random outlier model, we characterize the $\\textit{minimax\nrecovery rate}$, that is, the critical threshold of non-corruption rate $p$\nbelow which exact information recovery is infeasible. This accommodates a very\ngeneral class of pairwise relations. For various homogeneous random graph\nmodels (e.g. Erdos Renyi random graphs, random geometric graphs, small world\ngraphs), the minimax recovery rate depends almost exclusively on the edge\nsparsity of the measurement graph $\\mathcal{G}$ irrespective of other graphical\nmetrics. This fundamental limit decays with the group size $M$ at a square root\nrate before entering a connectivity-limited regime. Under the Erdos Renyi\nrandom graph, a tractable combinatorial algorithm is proposed to approach the\nlimit for large $M$ ($M=n^{\\Omega(1)}$), while order-optimal recovery is\nenabled by semidefinite programs in the small $M$ regime.\n  The extended (and most updated) version of this work can be found at\n(http://arxiv.org/abs/1504.01369).\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 19:28:18 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 21:23:37 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 22:30:17 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Chen", "Yuxin", ""], ["Goldsmith", "Andrea J.", ""]]}, {"id": "1404.7397", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "A fully data-driven method for estimating the shape of a point cloud", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support $S$. Under the\nmild assumption that $S$ is $r$-convex, the smallest $r$-convex set which\ncontains the sample points is the natural estimator. The main problem for using\nthis estimator in practice is that $r$ is an unknown geometric characteristic\nof the set $S$. A stochastic algorithm is proposed for selecting it from the\ndata under the hypothesis that the sample is uniformly generated. The new\ndata-driven reconstruction of $S$ is able to achieve the same convergence rates\nas the convex hull for estimating convex sets, but under a much more flexible\nsmoothness shape condition. The practical performance of the estimator is\nillustrated through a real data example and a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 15:34:04 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 21:30:31 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1404.7552", "submitter": "Geoffrey Schiebinger", "authors": "Geoffrey Schiebinger, Martin J. Wainwright, Bin Yu", "title": "The geometry of kernelized spectral clustering", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1283 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 2, 819-846", "doi": "10.1214/14-AOS1283", "report-no": "IMS-AOS-AOS1283", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data sets is a standard problem in many areas of science and\nengineering. The method of spectral clustering is based on embedding the data\nset using a kernel function, and using the top eigenvectors of the normalized\nLaplacian to recover the connected components. We study the performance of\nspectral clustering in recovering the latent labels of i.i.d. samples from a\nfinite mixture of nonparametric distributions. The difficulty of this label\nrecovery problem depends on the overlap between mixture components and how\neasily a mixture component is divided into two nonoverlapping components. When\nthe overlap is small compared to the indivisibility of the mixture components,\nthe principal eigenspace of the population-level normalized Laplacian operator\nis approximately spanned by the square-root kernelized component densities. In\nthe finite sample setting, and under the same assumption, embedded samples from\ndifferent components are approximately orthogonal with high probability when\nthe sample size is large. As a corollary we control the fraction of samples\nmislabeled by spectral clustering under finite mixtures with nonparametric\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 23:23:57 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 00:03:06 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 05:16:34 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Schiebinger", "Geoffrey", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}]