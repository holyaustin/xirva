[{"id": "2011.00216", "submitter": "Thomas Berrett", "authors": "Thomas Berrett, L\\'aszl\\'o Gy\\\"orfi, Harro Walk", "title": "Strongly universally consistent nonparametric regression and\n  classification with privatised data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the classical problem of nonparametric regression,\nbut impose local differential privacy constraints. Under such constraints, the\nraw data $(X_1,Y_1),\\ldots,(X_n,Y_n)$, taking values in $\\mathbb{R}^d \\times\n\\mathbb{R}$, cannot be directly observed, and all estimators are functions of\nthe randomised output from a suitable privacy mechanism. The statistician is\nfree to choose the form of the privacy mechanism, and here we add Laplace\ndistributed noise to a discretisation of the location of a feature vector $X_i$\nand to the value of its response variable $Y_i$. Based on this randomised data,\nwe design a novel estimator of the regression function, which can be viewed as\na privatised version of the well-studied partitioning regression estimator. The\nmain result is that the estimator is strongly universally consistent. Our\nmethods and analysis also give rise to a strongly universally consistent binary\nclassification rule for locally differentially private data.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 09:00:43 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Berrett", "Thomas", ""], ["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Walk", "Harro", ""]]}, {"id": "2011.00308", "submitter": "Lukas Trottner", "authors": "Niklas Dexheimer, Claudia Strauch and Lukas Trottner", "title": "Mixing it up: A general framework for Markovian statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up to now, the nonparametric analysis of multidimensional continuous-time\nMarkov processes has focussed strongly on specific model choices, mostly\nrelated to symmetry of the semigroup. While this approach allows to study the\nperformance of estimators for the characteristics of the process in the minimax\nsense, it restricts the applicability of results to a rather constrained set of\nstochastic processes and in particular hardly allows incorporating jump\nstructures. As a consequence, for many models of applied and theoretical\ninterest, no statement can be made about the robustness of typical statistical\nprocedures beyond the beautiful, but limited framework available in the\nliterature. To close this gap, we identify $\\beta$-mixing of the process and\nheat kernel bounds on the transition density as a suitable combination to\nobtain $\\sup$-norm and $L^2$ kernel invariant density estimation rates matching\nthe case of reversible multidimenisonal diffusion processes and outperforming\ndensity estimation based on discrete i.i.d. or weakly dependent data. Moreover,\nwe demonstrate how up to $\\log$-terms, optimal $\\sup$-norm adaptive invariant\ndensity estimation can be achieved within our general framework based on tight\nuniform moment bounds and deviation inequalities for empirical processes\nassociated to additive functionals of Markov processes. The underlying\nassumptions are verifiable with classical tools from stability theory of\ncontinuous time Markov processes and PDE techniques, which opens the door to\nevaluate statistical performance for a vast amount of Markov models. We\nhighlight this point by showing how multidimensional jump SDEs with L\\'evy\ndriven jump part under different coefficient assumptions can be seamlessly\nintegrated into our framework, thus establishing novel adaptive $\\sup$-norm\nestimation rates for this class of processes.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 16:47:19 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 16:28:20 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 07:26:42 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dexheimer", "Niklas", ""], ["Strauch", "Claudia", ""], ["Trottner", "Lukas", ""]]}, {"id": "2011.00617", "submitter": "Brittany Carr", "authors": "Henry Adams, Brittany Carr, Elin Farnell", "title": "Support Vector Machines and Radon's Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO math.GN math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A support vector machine (SVM) is an algorithm which finds a hyperplane that\noptimally separates labeled data points in $\\mathbb{R}^n$ into positive and\nnegative classes. The data points on the margin of this separating hyperplane\nare called support vectors. We study the possible configurations of support\nvectors for points in general position. In particular, we connect the possible\nconfigurations to Radon's theorem, which provides guarantees for when a set of\npoints can be divided into two classes (positive and negative) whose convex\nhulls intersect. If the positive and negative support vectors in a generic SVM\nconfiguration are projected to the separating hyperplane, then these projected\npoints will form a Radon configuration. Further, with a particular type of\ngeneral position, we show there are at most $n+1$ support vectors. This can be\nused to test the level of machine precision needed in a support vector machine\nimplementation. We also show the projections of the convex hulls of the support\nvectors intersect in a single Radon point, and under a small enough\nperturbation, the points labeled as support vectors remain labeled as support\nvectors. We furthermore consider computations studying the expected number of\nsupport vectors for randomly generated data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 19:57:46 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Adams", "Henry", ""], ["Carr", "Brittany", ""], ["Farnell", "Elin", ""]]}, {"id": "2011.00629", "submitter": "Lek-Heng Lim", "authors": "Yuhang Cai and Lek-Heng Lim", "title": "Distances between probability distributions of different dimensions", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing probability distributions is an indispensable and ubiquitous task\nin machine learning and statistics. The most common way to compare a pair of\nBorel probability measures is to compute a metric between them, and by far the\nmost widely used notions of metric are the Wasserstein metric and the total\nvariation metric. The next most common way is to compute a divergence between\nthem, and in this case almost every known divergences such as those of\nKullback--Leibler, Jensen--Shannon, R\\'enyi, and many more, are special cases\nof the $f$-divergence. Nevertheless these metrics and divergences may only be\ncomputed, in fact, are only defined, when the pair of probability measures are\non spaces of the same dimension. How would one quantify, say, a KL-divergence\nbetween the uniform distribution on the interval $[-1,1]$ and a Gaussian\ndistribution on $\\mathbb{R}^3$? We will show that, in a completely natural\nmanner, various common notions of metrics and divergences give rise to a\ndistance between Borel probability measures defined on spaces of different\ndimensions, e.g., one on $\\mathbb{R}^m$ and another on $\\mathbb{R}^n$ where $m,\nn$ are distinct, so as to give a meaningful answer to the previous question.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 21:20:36 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 06:01:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cai", "Yuhang", ""], ["Lim", "Lek-Heng", ""]]}, {"id": "2011.00640", "submitter": "Reiko Aoki", "authors": "Reiko Aoki, Dorival Le\\~ao, Juan Pablo Mamani Bustamante and Filidor\n  Vilca Labra", "title": "Data Analysis for Proficiency Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proficiency Testing (PT) determines the performance of individual\nlaboratories for specific tests or measurements and it is used to monitor the\nreliability of laboratories measurements. PT plays a highly valuable role as it\nprovides an objective evidence of the competence of the participant\nlaboratories. In this paper, we propose a multivariate model to assess\nequivalence among laboratories measurements in proficiency testing. Our method\nallow to include type B source of variation and to deal with multivariate data,\nwhere the item under test is measured at different levels. Although intuitive,\nthe proposed model is nonergodic, which means that the asymptotic Fisher\ninformation matrix is random. As a consequence, a detailed asymptotic analysis\nwas carried out to establish the strategy for comparing the results of the\nparticipating laboratories. To illustrate, we apply our method to analyze the\ndata from the Brazilian Engine test group, PT program, where the power of an\nengine was measured by 8 laboratories at several levels of rotation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 22:57:56 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Aoki", "Reiko", ""], ["Le\u00e3o", "Dorival", ""], ["Bustamante", "Juan Pablo Mamani", ""], ["Labra", "Filidor Vilca", ""]]}, {"id": "2011.00647", "submitter": "Jiangzhou Wang", "authors": "Jiangzhou Wang, Jingfei Zhang, Binghui Liu, Ji Zhu, and Jianhua Guo", "title": "Fast Network Community Detection with Profile-Pseudo Likelihood Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is one of the most studied network models for\ncommunity detection. It is well-known that most algorithms proposed for fitting\nthe stochastic block model likelihood function cannot scale to large-scale\nnetworks. One prominent work that overcomes this computational challenge is\nAmini et al.(2013), which proposed a fast pseudo-likelihood approach for\nfitting stochastic block models to large sparse networks. However, this\napproach does not have convergence guarantee, and is not well suited for small-\nor medium- scale networks. In this article, we propose a novel likelihood based\napproach that decouples row and column labels in the likelihood function, which\nenables a fast alternating maximization; the new method is computationally\nefficient, performs well for both small and large scale networks, and has\nprovable convergence guarantee. We show that our method provides strongly\nconsistent estimates of the communities in a stochastic block model. As\ndemonstrated in simulation studies, the proposed method outperforms the\npseudo-likelihood approach in terms of both estimation accuracy and computation\nefficiency, especially for large sparse networks. We further consider\nextensions of our proposed method to handle networks with degree heterogeneity\nand bipartite properties.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:40:26 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 10:46:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Jiangzhou", ""], ["Zhang", "Jingfei", ""], ["Liu", "Binghui", ""], ["Zhu", "Ji", ""], ["Guo", "Jianhua", ""]]}, {"id": "2011.00787", "submitter": "Santosh Kumar", "authors": "Peter J. Forrester and Santosh Kumar", "title": "Differential recurrences for the distribution of the trace of the\n  $\\beta$-Jacobi ensemble", "comments": "28 pages, 1 figure, Mathematica codes & Codes description included as\n  ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.mes-hall math.MP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples of the $\\beta$-Jacobi ensemble specify the joint distribution of the\ntransmission eigenvalues in scattering problems. In this context, there has\nbeen interest in the distribution of the trace, as the trace corresponds to the\nconductance. Earlier, in the case $\\beta = 1$, the trace statistic was isolated\nin studies of covariance matrices in multivariate statistics, where it is\nreferred to as Pillai's $V$ statistic. In this context, Davis showed that for\n$\\beta = 1$ the trace statistic, and its Fourier-Laplace transform, can be\ncharacterised by $(N+1) \\times (N+1)$ matrix differential equations. For the\nFourier-Laplace transform, this leads to a vector recurrence for the moments.\nHowever, for the distribution itself the characterisation provided was\nincomplete, as the connection problem of determining the linear combination of\nFrobenius type solutions that correspond to the statistic was not solved. We\nsolve this connection problem for Jacobi parameter $b$ and Dyson index $\\beta$\nnon-negative integers. For the other Jacobi parameter $a$ also a non-negative\ninteger, the power series portion of each Frobenius solution terminates to a\npolynomial, and the matrix differential equation gives a recurrence for their\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:35:49 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 13:44:49 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Forrester", "Peter J.", ""], ["Kumar", "Santosh", ""]]}, {"id": "2011.01218", "submitter": "Jinghang Lin", "authors": "Jinghang Lin, Xiaoxi Shen, Qing Lu", "title": "Asymptotic Theory of Expectile Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are becoming an increasingly important tool in applications.\nHowever, neural networks are not widely used in statistical genetics. In this\npaper, we propose a new neural networks method called expectile neural\nnetworks. When the size of parameter is too large, the standard maximum\nlikelihood procedures may not work. We use sieve method to constrain parameter\nspace. And we prove its consistency and normality under nonparametric\nregression framework.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 01:14:04 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Lin", "Jinghang", ""], ["Shen", "Xiaoxi", ""], ["Lu", "Qing", ""]]}, {"id": "2011.01343", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "p-value peeking and estimating extrema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pervasive issue in statistical hypothesis testing is that the reported\n$p$-values are biased downward by data \"peeking\" -- the practice of reporting\nonly progressively extreme values of the test statistic as more data samples\nare collected. We develop principled mechanisms to estimate such running\nextrema of test statistics, which directly address the effect of peeking in\nsome general scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:00:57 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "2011.01364", "submitter": "Feicheng Wang", "authors": "Feicheng Wang and Lucas Janson", "title": "Exact Asymptotics for Linear Quadratic Adaptive Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in reinforcement learning has led to remarkable performance\nin a range of applications, but its deployment in high-stakes settings remains\nquite rare. One reason is a limited understanding of the behavior of\nreinforcement algorithms, both in terms of their regret and their ability to\nlearn the underlying system dynamics---existing work is focused almost\nexclusively on characterizing rates, with little attention paid to the\nconstants multiplying those rates that can be critically important in practice.\nTo start to address this challenge, we study perhaps the simplest non-bandit\nreinforcement learning problem: linear quadratic adaptive control (LQAC). By\ncarefully combining recent finite-sample performance bounds for the LQAC\nproblem with a particular (less-recent) martingale central limit theorem, we\nare able to derive asymptotically-exact expressions for the regret, estimation\nerror, and prediction error of a rate-optimal stepwise-updating LQAC algorithm.\nIn simulations on both stable and unstable systems, we find that our asymptotic\ntheory also describes the algorithm's finite-sample behavior remarkably well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:43:30 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Feicheng", ""], ["Janson", "Lucas", ""]]}, {"id": "2011.01479", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Hau-Tieng Wu", "title": "Convergence of Graph Laplacian with kNN Self-tuned Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelized Gram matrix $W$ constructed from data points $\\{x_i\\}_{i=1}^N$ as\n$W_{ij}= k_0( \\frac{ \\| x_i - x_j \\|^2} {\\sigma^2} )$ is widely used in\ngraph-based geometric data analysis and unsupervised learning. An important\nquestion is how to choose the kernel bandwidth $\\sigma$, and a common practice\ncalled self-tuned kernel adaptively sets a $\\sigma_i$ at each point $x_i$ by\nthe $k$-nearest neighbor (kNN) distance. When $x_i$'s are sampled from a\n$d$-dimensional manifold embedded in a possibly high-dimensional space, unlike\nwith fixed-bandwidth kernels, theoretical results of graph Laplacian\nconvergence with self-tuned kernels have been incomplete. This paper proves the\nconvergence of graph Laplacian operator $L_N$ to manifold (weighted-)Laplacian\nfor a new family of kNN self-tuned kernels $W^{(\\alpha)}_{ij} = k_0( \\frac{ \\|\nx_i - x_j \\|^2}{ \\epsilon \\hat{\\rho}(x_i)\n\\hat{\\rho}(x_j)})/\\hat{\\rho}(x_i)^\\alpha \\hat{\\rho}(x_j)^\\alpha$, where\n$\\hat{\\rho}$ is the estimated bandwidth function {by kNN}, and the limiting\noperator is also parametrized by $\\alpha$. When $\\alpha = 1$, the limiting\noperator is the weighted manifold Laplacian $\\Delta_p$. Specifically, we prove\nthe point-wise convergence of $L_N f $ and convergence of the graph Dirichlet\nform with rates. Our analysis is based on first establishing a $C^0$\nconsistency for $\\hat{\\rho}$ which bounds the relative estimation error\n$|\\hat{\\rho} - \\bar{\\rho}|/\\bar{\\rho}$ uniformly with high probability, where\n$\\bar{\\rho} = p^{-1/d}$, and $p$ is the data density function. Our theoretical\nresults reveal the advantage of self-tuned kernel over fixed-bandwidth kernel\nvia smaller variance error in low-density regions. In the algorithm, no prior\nknowledge of $d$ or data density is needed. The theoretical results are\nsupported by numerical experiments on simulated data and hand-written digit\nimage data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:55:33 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 02:17:46 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2011.01591", "submitter": "Hajo Holzmann", "authors": "Philipp Hermann and Hajo Holzmann", "title": "Support estimation in high-dimensional heteroscedastic mean regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current strand of research in high-dimensional statistics deals with\nrobustifying the available methodology with respect to deviations from the\npervasive light-tail assumptions. In this paper we consider a linear mean\nregression model with random design and potentially heteroscedastic,\nheavy-tailed errors, and investigate support estimation in this framework. We\nuse a strictly convex, smooth variant of the Huber loss function with tuning\nparameter depending on the parameters of the problem, as well as the adaptive\nLASSO penalty for computational efficiency. For the resulting estimator we show\nsign-consistency and optimal rates of convergence in the $\\ell_\\infty$ norm as\nin the homoscedastic, light-tailed setting. In our analysis, we have to deal\nwith the issue that the support of the target parameter in the linear mean\nregression model and its robustified version may differ substantially even for\nsmall values of the tuning parameter of the Huber loss function. Simulations\nillustrate the favorable numerical performance of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 09:46:31 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Hermann", "Philipp", ""], ["Holzmann", "Hajo", ""]]}, {"id": "2011.01617", "submitter": "Michel Broniatowski", "authors": "Michel Broniatowski (LPSM)", "title": "Minimum divergence estimators, Maximum Likelihood and the generalized\n  bootstrap", "comments": null, "journal-ref": null, "doi": "10.3390/e23020185", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an attempt to set a justification for making use of some\ndicrepancy indexes, starting from the classical Maximum Likelihood definition,\nand adapting the corresponding basic principle of inference to situations where\nminimization of those indexes between a model and some extension of the\nempirical measure of the data appears as its natural extension. This leads to\nthe so called generalized bootstrap setting for which minimum divergence\ninference seems to replace Maximum Likelihood one. 1 Motivation and context\nDivergences between probability measures are widely used in Statistics and Data\nScience in order to perform inference under models of various kinds,\nparamet-ric or semi parametric, or even in non parametric settings. The\ncorresponding methods extend the likelihood paradigm and insert inference in\nsome minimum \"distance\" framing, which provides a convenient description for\nthe properties of the resulting estimators and tests, under the model or under\nmisspecifica-tion. Furthermore they pave the way to a large number of\ncompetitive methods , which allows for trade-off between efficiency and\nrobustness, among others. Many families of such divergences have been proposed,\nsome of them stemming from classical statistics (such as the Chi-square), while\nothers have their origin in other fields such as Information theory. Some\nmeasures of discrepancy involve regularity of the corresponding probability\nmeasures while others seem to be restricted to measures on finite or countable\nspaces, at least when using them as inferential tools, henceforth in situations\nwhen the elements of a model have to be confronted with a dataset. The choice\nof a specific discrepancy measure in specific context is somehow arbitrary in\nmany cases, although the resulting conclusion of the inference might differ\naccordingly, above all under misspecification; however the need for such\napproaches is clear when aiming at robustness.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:57:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Broniatowski", "Michel", "", "LPSM"]]}, {"id": "2011.01657", "submitter": "Yannick Baraud", "authors": "Yannick Baraud and Juntong Chen", "title": "Robust estimation of a regression function in exponential families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe $n$ pairs $X_{1}=(W_{1},Y_{1}),\\ldots,X_{n}=(W_{n},Y_{n})$ of\nindependent random variables and assume, although this might not be true, that\nfor each $i\\in\\{1,\\ldots,n\\}$, the conditional distribution of $Y_{i}$ given\n$W_{i}$ belongs to a given exponential family with real parameter\n$\\theta_{i}^{\\star}=\\boldsymbol{\\theta}^{\\star}(W_{i})$ the value of which is a\nfunction $\\boldsymbol{\\theta}^{\\star}$ of the covariate $W_{i}$. Given a model\n$\\boldsymbol{\\overline\\Theta}$ for $\\boldsymbol{\\theta}^{\\star}$, we propose an\nestimator $\\boldsymbol{\\widehat \\theta}$ with values in\n$\\boldsymbol{\\overline\\Theta}$ the construction of which is independent of the\ndistribution of the $W_{i}$ and that possesses the properties of being robust\nto contamination, outliers and model misspecification. We establish\nnon-asymptotic exponential inequalities for the upper deviations of a\nHellinger-type distance between the true distribution of the data and the\nestimated one based on $\\boldsymbol{\\widehat \\theta}$. Under a suitable\nparametrization of the exponential family, we deduce a uniform risk bound for\n$\\boldsymbol{\\widehat \\theta}$ over the class of H\\\"olderian functions and we\nprove the optimality of this bound up to a logarithmic factor. Finally, we\nprovide an algorithm for calculating $\\boldsymbol{\\widehat \\theta}$ when\n$\\boldsymbol{\\theta}^{\\star}$ is assumed to belong to functional classes of low\nor medium dimensions (in a suitable sense) and, on a simulation study, we\ncompare the performance of $\\boldsymbol{\\widehat \\theta}$ to that of the MLE\nand median-based estimators. The proof of our main result relies on an upper\nbound, with explicit numerical constants, on the expectation of the supremum of\nan empirical process over a VC-subgraph class. This bound can be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:14:42 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Baraud", "Yannick", ""], ["Chen", "Juntong", ""]]}, {"id": "2011.01711", "submitter": "Christoph Muehlmann", "authors": "Christoph Muehlmann, Fran\\c{c}ois Bachoc, Klaus Nordhausen, Mengxi Yi", "title": "Test of the Latent Dimension of a Spatial Blind Source Separation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume a spatial blind source separation model in which the observed\nmultivariate spatial data is a linear mixture of latent spatially uncorrelated\nGaussian random fields containing a number of pure white noise components. We\npropose a test on the number of white noise components and obtain the\nasymptotic distribution of its statistic for a general domain. We also\ndemonstrate how computations can be facilitated in the case of gridded\nobservation locations. Based on this test, we obtain a consistent estimator of\nthe true dimension. Simulation studies and an environmental application\ndemonstrate that our test is at least comparable to and often outperforms\nbootstrap-based techniques, which are also introduced in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:56:14 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Muehlmann", "Christoph", ""], ["Bachoc", "Fran\u00e7ois", ""], ["Nordhausen", "Klaus", ""], ["Yi", "Mengxi", ""]]}, {"id": "2011.01737", "submitter": "Deborah Sulem", "authors": "Mihai Cucuringu, Apoorv Vikram Singh, D\\'eborah Sulem, Hemant Tyagi", "title": "Regularized spectral methods for clustering signed networks", "comments": "55 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of $k$-way clustering in signed graphs. Considerable\nattention in recent years has been devoted to analyzing and modeling signed\ngraphs, where the affinity measure between nodes takes either positive or\nnegative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral\nmethod, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem),\nwhich casts the clustering task as a generalized eigenvalue problem optimizing\na suitably defined objective function. This approach is motivated by social\nbalance theory, where the clustering task aims to decompose a given network\ninto disjoint groups, such that individuals within the same group are connected\nby as many positive edges as possible, while individuals from different groups\nare mainly connected by negative edges. Through extensive numerical\nsimulations, SPONGE was shown to achieve state-of-the-art empirical\nperformance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the\npopular Signed Laplacian method under the setting of a Signed Stochastic Block\nModel (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is\nmoderately dense.\n  In this work, we build on the results in [CDGT 2019] on two fronts for the\nnormalized versions of SPONGE and the Signed Laplacian. Firstly, for both\nalgorithms, we extend the theoretical analysis in [CDGT 2019] to the general\nsetting of $k \\geq 2$ unequal-sized clusters in the moderately dense regime.\nSecondly, we introduce regularized versions of both methods to handle sparse\ngraphs -- a regime where standard spectral methods underperform -- and provide\ntheoretical guarantees under the same SSBM model. To the best of our knowledge,\nregularized spectral methods have so far not been considered in the setting of\nclustering signed graphs. We complement our theoretical results with an\nextensive set of numerical experiments on synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:40:34 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Singh", "Apoorv Vikram", ""], ["Sulem", "D\u00e9borah", ""], ["Tyagi", "Hemant", ""]]}, {"id": "2011.01848", "submitter": "Ananda Theertha Suresh", "authors": "Ananda Theertha Suresh", "title": "Robust hypothesis testing and distribution estimation in Hellinger\n  distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple robust hypothesis test that has the same sample\ncomplexity as that of the optimal Neyman-Pearson test up to constants, but\nrobust to distribution perturbations under Hellinger distance. We discuss the\napplicability of such a robust test for estimating distributions in Hellinger\ndistance. We empirically demonstrate the power of the test on canonical\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:09:32 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Suresh", "Ananda Theertha", ""]]}, {"id": "2011.01857", "submitter": "Yi Yu", "authors": "Yi Yu", "title": "A review on minimax rates in change point detection and localisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent developments in fundamental limits and optimal\nalgorithms for change point analysis. We focus on minimax optimal rates in\nchange point detection and localisation, in both parametric and nonparametric\nmodels. We start with the univariate mean change point analysis problem and\nreview the state-of-the-art results in the literature. We then move on to more\ncomplex data types and investigate general principles behind the optimal\nprocedures that lead to minimax rate-optimal results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:19:45 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yu", "Yi", ""]]}, {"id": "2011.01980", "submitter": "Rachel Traylor", "authors": "C. Gast and R. Traylor", "title": "On Ordered Fuzzy Numbers Generated By Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new trapezoidal ordered fuzzy number representation of\nwindowed time series based on the idea of a Japanese candlestick to those\npreviously proposed in the literature. We define and illustrate several\ndescriptive statistics based on the information contained in the ordered fuzzy\nnumbers. We utilize financial trading data from three automotive companies as a\ncase study. Further expansion can be applied to any other forms of time series\ndata to offer further insight into many other data driven situations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:54:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Gast", "C.", ""], ["Traylor", "R.", ""]]}, {"id": "2011.01983", "submitter": "Jonathan Hill", "authors": "Jonathan B. Hill", "title": "Testing (Infinitely) Many Zero Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a max-test statistic for testing (possibly infinitely) many zero\nparameter restrictions in a general parametric regression framework. The test\nstatistic is based on estimating the key parameters one at a time in many\nmodels of possibly vastly smaller dimension than the original model, and\nchoosing the largest in absolute value from these individually estimated\nparameters. Under mild conditions the parsimonious models identify whether the\noriginal parameter of interest is or is not zero. The potentially much lower\ndimension of the many models used ensures greater estimator accuracy, and using\nonly the largest in a sequence of weighted estimators reduces test statistic\ncomplexity and therefore estimation error, ensuring sharper size and greater\npower in practice. Weights allow for standardization in order to control for\nestimator dispersion. We use a simulation method or parametric bootstrap for\np-value computation without directly requiring the max-statistic's limit\ndistribution. This is critically useful when asymptotically infinitely many\nparameters are estimated. Existing extreme theory does not exist for the\nmaximum of a sequence of a strongly dependent or nonstationary sequence that\narise from estimating many regression models. A simulation experiment shows the\nmax-test dominates a conventional bootstrapped test.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:58:30 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hill", "Jonathan B.", ""]]}, {"id": "2011.02258", "submitter": "Huiming Zhang", "authors": "Huiming Zhang, Song Xi Chen", "title": "Concentration Inequalities for Statistical Inference", "comments": "Invited review article on constants-specified concentration\n  inequalities published in Communications in Mathematical Research", "journal-ref": "Communications in Mathematical Research. 37(1), 1-85 (2021)", "doi": "10.4208/cmr.2020-0041", "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a review of concentration inequalities which are widely\nemployed in non-asymptotical analyses of mathematical statistics in a wide\nrange of settings, from distribution-free to distribution-dependent, from\nsub-Gaussian to sub-exponential, sub-Gamma, and sub-Weibull random variables,\nand from the mean to the maximum concentration. This review provides results in\nthese settings with some fresh new results. Given the increasing popularity of\nhigh-dimensional data and inference, results in the context of high-dimensional\nlinear and Poisson regressions are also provided. We aim to illustrate the\nconcentration inequalities with known constants and to improve existing bounds\nwith sharper constants.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 12:54:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 04:09:05 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 05:30:11 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Huiming", ""], ["Chen", "Song Xi", ""]]}, {"id": "2011.02315", "submitter": "Saeed Hayati", "authors": "Saeed Hayati, Kenji Fukumizu and Afshin Parvardeh", "title": "Kernel Mean Embedding of Probability Measures and its Applications to\n  Functional Data Analysis", "comments": "37 Pages, 2 figures, Submitted to Electronic Journal of Statistic", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study intends to introduce kernel mean embedding of probability measures\nover infinite-dimensional separable Hilbert spaces induced by functional\nresponse statistical models. The embedded function represents the concentration\nof probability measures in small open neighborhoods, which identifies a\npseudo-likelihood and fosters a rich framework for statistical inference.\nUtilizing Maximum Mean Discrepancy, we devise new tests in functional response\nmodels. The performance of new derived tests is evaluated against competitors\nin three major problems in functional data analysis including\nfunction-on-scalar regression, functional one-way ANOVA, and equality of\ncovariance operators.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:32:59 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Hayati", "Saeed", ""], ["Fukumizu", "Kenji", ""], ["Parvardeh", "Afshin", ""]]}, {"id": "2011.02522", "submitter": "Anuran Makur", "authors": "Ali Jadbabaie and Anuran Makur and Devavrat Shah", "title": "Gradient-Based Empirical Risk Minimization using Local Polynomial\n  Regression", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of empirical risk minimization (ERM)\nof smooth, strongly convex loss functions using iterative gradient-based\nmethods. A major goal of this literature has been to compare different\nalgorithms, such as gradient descent (GD) or stochastic gradient descent (SGD),\nby analyzing their rates of convergence to $\\epsilon$-approximate solutions.\nFor example, the oracle complexity of GD is $O(n\\log(\\epsilon^{-1}))$, where\n$n$ is the number of training samples. When $n$ is large, this can be expensive\nin practice, and SGD is preferred due to its oracle complexity of\n$O(\\epsilon^{-1})$. Such standard analyses only utilize the smoothness of the\nloss function in the parameter being optimized. In contrast, we demonstrate\nthat when the loss function is smooth in the data, we can learn the oracle at\nevery iteration and beat the oracle complexities of both GD and SGD in\nimportant regimes. Specifically, at every iteration, our proposed algorithm\nperforms local polynomial regression to learn the gradient of the loss\nfunction, and then estimates the true gradient of the ERM objective function.\nWe establish that the oracle complexity of our algorithm scales like\n$\\tilde{O}((p \\epsilon^{-1})^{d/(2\\eta)})$ (neglecting sub-dominant factors),\nwhere $d$ and $p$ are the data and parameter space dimensions, respectively,\nand the gradient of the loss function belongs to a $\\eta$-H\\\"{o}lder class with\nrespect to the data. Our proof extends the analysis of local polynomial\nregression in non-parametric statistics to provide interpolation guarantees in\nmultivariate settings, and also exploits tools from the inexact GD literature.\nUnlike GD and SGD, the complexity of our method depends on $d$ and $p$.\nHowever, when $d$ is small and the loss function exhibits modest smoothness in\nthe data, our algorithm beats GD and SGD in oracle complexity for a very broad\nrange of $p$ and $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 20:10:31 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Jadbabaie", "Ali", ""], ["Makur", "Anuran", ""], ["Shah", "Devavrat", ""]]}, {"id": "2011.02560", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Independent Gaussian Distributions Minimize the Kullback-Leibler (KL)\n  Divergence from Independent Gaussian Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SY eess.SP eess.SY math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note is on a property of the Kullback-Leibler (KL) divergence\nwhich indicates that independent Gaussian distributions minimize the KL\ndivergence from given independent Gaussian distributions. The primary purpose\nof this note is for the referencing of papers that need to make use of this\nproperty entirely or partially.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:05:45 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 15:54:17 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2011.03026", "submitter": "Sayan Das", "authors": "Bhaswar B. Bhattacharya, Sayan Das, Sumit Mukherjee", "title": "Motif Estimation via Subgraph Sampling: The Fourth Moment Phenomenon", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sampling is an indispensable tool for understanding features of large\ncomplex networks where it is practically impossible to search over the entire\ngraph. In this paper, we develop a framework for statistical inference for\ncounting network motifs, such as edges, triangles, and wedges, in the widely\nused subgraph sampling model, where each vertex is sampled independently, and\nthe subgraph induced by the sampled vertices is observed. We derive necessary\nand sufficient conditions for the consistency and the asymptotic normality of\nthe natural Horvitz-Thompson (HT) estimator, which can be used for constructing\nconfidence intervals and hypothesis testing for the motif counts based on the\nsampled graph. In particular, we show that the asymptotic normality of the HT\nestimator exhibits an interesting fourth-moment phenomenon, which asserts that\nthe HT estimator (appropriately centered and rescaled) converges in\ndistribution to the standard normal whenever its fourth-moment converges to 3\n(the fourth-moment of the standard normal distribution). As a consequence, we\nderive the exact thresholds for consistency and asymptotic normality of the HT\nestimator in various natural graph ensembles, such as sparse graphs with\nbounded degree, Erdos-Renyi random graphs, random regular graphs, and dense\ngraphons.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:34:30 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""], ["Das", "Sayan", ""], ["Mukherjee", "Sumit", ""]]}, {"id": "2011.03030", "submitter": "Nathan Kallus", "authors": "Yichun Hu, Nathan Kallus, Xiaojie Mao", "title": "Fast Rates for Contextual Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating side observations in decision making can reduce uncertainty and\nboost performance, but it also requires we tackle a potentially complex\npredictive relationship. While one may use off-the-shelf machine learning\nmethods to separately learn a predictive model and plug it in, a variety of\nrecent methods instead integrate estimation and optimization by fitting the\nmodel to directly optimize downstream decision performance. Surprisingly, in\nthe case of contextual linear optimization, we show that the naive plug-in\napproach actually achieves regret convergence rates that are significantly\nfaster than methods that directly optimize downstream decision performance. We\nshow this by leveraging the fact that specific problem instances do not have\narbitrarily bad near-dual-degeneracy. While there are other pros and cons to\nconsider as we discuss and illustrate numerically, our results highlight a\nnuanced landscape for the enterprise to integrate estimation and optimization.\nOur results are overall positive for practice: predictive models are easy and\nfast to train using existing tools, simple to interpret, and, as we show, lead\nto decisions that perform very well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 18:43:59 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:53:47 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hu", "Yichun", ""], ["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""]]}, {"id": "2011.03073", "submitter": "Grigory Franguridi", "authors": "Grigory Franguridi, Bulat Gafarov, Kaspar Wuthrich", "title": "Conditional quantile estimators: A small sample theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the small sample properties of conditional quantile estimators such\nas classical and IV quantile regression.\n  First, we propose a higher-order analytical framework for comparing competing\nestimators in small samples and assessing the accuracy of common inference\nprocedures. Our framework is based on a novel approximation of the\ndiscontinuous sample moments by a H\\\"older-continuous process with a negligible\nerror. For any consistent estimator, this approximation leads to asymptotic\nlinear expansions with nearly optimal rates.\n  Second, we study the higher-order bias of exact quantile estimators up to\n$O\\left(\\frac{1}{n}\\right)$. Using a novel non-smooth calculus technique, we\nuncover previously unknown non-negligible bias components that cannot be\nconsistently estimated and depend on the employed estimation algorithm. To\ncircumvent this problem, we propose a \"symmetric\" bias correction, which admits\na feasible implementation. Our simulations confirm the empirical importance of\nbias correction.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 19:43:13 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 19:50:00 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 07:28:32 GMT"}, {"version": "v4", "created": "Sat, 10 Apr 2021 01:54:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Franguridi", "Grigory", ""], ["Gafarov", "Bulat", ""], ["Wuthrich", "Kaspar", ""]]}, {"id": "2011.03074", "submitter": "Stefan Richter", "authors": "Moritz Haas, Stefan Richter", "title": "Statistical analysis of Wasserstein GANs with applications to time\n  series forecasting", "comments": "47 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide statistical theory for conditional and unconditional Wasserstein\ngenerative adversarial networks (WGANs) in the framework of dependent\nobservations. We prove upper bounds for the excess Bayes risk of the WGAN\nestimators with respect to a modified Wasserstein-type distance. Furthermore,\nwe formalize and derive statements on the weak convergence of the estimators\nand use them to develop confidence intervals for new observations. The theory\nis applied to the special case of high-dimensional time series forecasting. We\nanalyze the behavior of the estimators in simulations based on synthetic data\nand investigate a real data example with temperature data. The dependency of\nthe data is quantified with absolutely regular beta-mixing coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 19:45:59 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Haas", "Moritz", ""], ["Richter", "Stefan", ""]]}, {"id": "2011.03176", "submitter": "Krishnakumar Balasubramanian", "authors": "Ye He, Krishnakumar Balasubramanian, Murat A. Erdogdu", "title": "On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint\n  Sampling Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized midpoint method, proposed by [SL19], has emerged as an optimal\ndiscretization procedure for simulating the continuous time Langevin\ndiffusions. Focusing on the case of strong-convex and smooth potentials, in\nthis paper, we analyze several probabilistic properties of the randomized\nmidpoint discretization method for both overdamped and underdamped Langevin\ndiffusions. We first characterize the stationary distribution of the discrete\nchain obtained with constant step-size discretization and show that it is\nbiased away from the target distribution. Notably, the step-size needs to go to\nzero to obtain asymptotic unbiasedness. Next, we establish the asymptotic\nnormality for numerical integration using the randomized midpoint method and\nhighlight the relative advantages and disadvantages over other discretizations.\nOur results collectively provide several insights into the behavior of the\nrandomized midpoint discretization method, including obtaining confidence\nintervals for numerical integrations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 03:39:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["He", "Ye", ""], ["Balasubramanian", "Krishnakumar", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "2011.03219", "submitter": "Martin Bladt", "authors": "Martin Bladt and Jorge Yslas", "title": "Inhomogeneous Markov Survival Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new regression models in survival analysis based on homogeneous\nand inhomogeneous phase-type distributions. The intensity function in this\nsetting plays the role of the hazard function. For unidimensional intensity\nmatrices, we recover the proportional hazard and accelerated failure time\nmodels, among others. However, when considering higher dimensions, the proposed\nmethods are only asymptotically equivalent to their classical counterparts and\nenjoy greater flexibility in the body of the distribution. For their\nestimation, the latent path representation of semi-Markov models is exploited.\nConsequently, an adapted EM algorithm is provided and the likelihood is shown\nto increase at each iteration. We provide several examples of practical\nsignificance and outline relevant extensions. The practical feasibility of the\nmodels is illustrated on simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 07:56:39 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 18:28:51 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bladt", "Martin", ""], ["Yslas", "Jorge", ""]]}, {"id": "2011.03418", "submitter": "Boris Landa", "authors": "Boris Landa, Rihao Qu, Joseph Chang, and Yuval Kluger", "title": "Local Two-Sample Testing over Graphs and Point-Clouds by Random-Walk\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample testing is a fundamental tool for scientific discovery. Yet, aside\nfrom concluding that two samples do not come from the same probability\ndistribution, it is often of interest to characterize how the two distributions\ndiffer. Specifically, given samples from two densities $f_1$ and $f_0$, we\nconsider the problem of localizing occurrences of the inequality $f_1 > f_0$ in\nthe combined sample. We present a general hypothesis testing framework for this\ntask, and investigate a special case of this framework where localization is\nachieved by a random walk over the sample. We derive a tractable testing\nprocedure for this case employing a type of scan statistic, and provide\nnon-asymptotic lower bounds on the power and accuracy of our test to detect\nwhether $f_1>f_0$ in a local sense. We also characterize the test's consistency\naccording to a certain problem-hardness parameter, and show that our test\nachieves the minimax rate for this parameter. We conduct numerical experiments\nto validate our method, and demonstrate our approach on two real-world\napplications: detecting and localizing arsenic well contamination across the\nUnited States, and analyzing two-sample single-cell RNA sequencing data from\nmelanoma patients.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:11:38 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Landa", "Boris", ""], ["Qu", "Rihao", ""], ["Chang", "Joseph", ""], ["Kluger", "Yuval", ""]]}, {"id": "2011.03567", "submitter": "Michael Lindon", "authors": "Michael Lindon, Alan Malek", "title": "Sequential Testing of Multinomial Hypotheses with Applications to\n  Detecting Implementation Errors and Missing Data in Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simply randomized designs are one of the most common controlled experiments\nused to study causal effects. Failure of the assignment mechanism, to provide\nproper randomization of units across treatments, or the data collection\nmechanism, when data is missing not at random, can render subsequent analysis\ninvalid if not properly identified. In this paper we demonstrate that such\npractical implementation errors can often be identified, fortunately, through\nconsideration of the total unit counts resulting in each treatment group. Based\non this observation, we introduce a sequential hypothesis test constructed from\nBayesian multinomial-Dirichlet families for detecting practical implementation\nerrors in simply randomized experiments. By establishing a Martingale property\nof the posterior odds under the null hypothesis, frequentist Type-I error is\ncontrolled under both optional stopping and continuation via maximal\ninequalities, preventing practitioners from potentially inflating false\npositive probabilities through continuous monitoring. In contrast to other\nstatistical tests that are performed once all data collection is completed, the\nproposed test is sequential - frequently rejecting the null during the process\nof data collection itself, saving further units from entering an\nimproperly-executed experiment. We illustrate the utility of this test in the\ncontext of online controlled experiments (OCEs), where the assignment is\nautomated through code and data collected through complex processing pipelines,\noften in the presence of unintended bugs and logical errors. Confidence\nsequences possessing desired sequential frequentist coverage probabilities are\nprovided and their connection to the Bayesian support interval is examined. The\ndifferences between pure Bayesian and sequential frequentist testing procedures\nare finally discussed through a conditional frequentist testing perspective.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:17:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lindon", "Michael", ""], ["Malek", "Alan", ""]]}, {"id": "2011.03622", "submitter": "Allen Liu", "authors": "Allen Liu, Ankur Moitra", "title": "Settling the Robust Learnability of Mixtures of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work represents a natural coalescence of two important lines of work:\nlearning mixtures of Gaussians and algorithmic robust statistics. In particular\nwe give the first provably robust algorithm for learning mixtures of any\nconstant number of Gaussians. We require only mild assumptions on the mixing\nweights (bounded fractionality) and that the total variation distance between\ncomponents is bounded away from zero. At the heart of our algorithm is a new\nmethod for proving dimension-independent polynomial identifiability through\napplying a carefully chosen sequence of differential operations to certain\ngenerating functions that not only encode the parameters we would like to learn\nbut also the system of polynomial equations we would like to solve. We show how\nthe symbolic identities we derive can be directly used to analyze a natural\nsum-of-squares relaxation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:36:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:49:01 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 20:46:03 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2011.03655", "submitter": "Daniel Roy", "authors": "Haosui Duanmu, Daniel M. Roy, Aaron Smith", "title": "Existence of matching priors on compact spaces", "comments": "53 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matching prior at level $1-\\alpha$ is a prior such that an associated\n$1-\\alpha$ credible set is also a $1-\\alpha$ confidence set. We study the\nexistence of matching priors for general families of credible regions. Our main\nresult gives topological conditions under which matching priors for specific\nfamilies of credible sets exist. Informally: on compact parameter spaces, if\nthe so-called rejection-probability map is jointly continuous under the\nWasserstein metric on priors, a matching prior exists. In light of this general\nresult, we observe that typical families of credible regions, such as credible\nballs, highest-posterior density regions, quantiles, etc., fail to meet this\ntopological condition. We show how to design approximate posterior credible\nballs and highest-posterior-density regions that meet these topological\nconditions, yielding matching priors. The proof of our main theorem uses tools\nfrom nonstandard analysis and establishes new results about the nonstandard\nextension of the Wasserstein metric which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 01:49:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Duanmu", "Haosui", ""], ["Roy", "Daniel M.", ""], ["Smith", "Aaron", ""]]}, {"id": "2011.03668", "submitter": "Guenther Walther", "authors": "Guenther Walther, Alnur Ali, Xinyue Shen and Stephen Boyd", "title": "Confidence bands for a log-concave density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for inference about a log-concave distribution:\nInstead of using the method of maximum likelihood, we propose to incorporate\nthe log-concavity constraint in an appropriate nonparametric confidence set for\nthe cdf $F$. This approach has the advantage that it automatically provides a\nmeasure of statistical uncertainty and it thus overcomes a marked limitation of\nthe maximum likelihood estimate. In particular, we show how to construct\nconfidence bands for the density that have a finite sample guaranteed\nconfidence level. The nonparametric confidence set for $F$ which we introduce\nhere has attractive computational and statistical properties: It allows to\nbring modern tools from optimization to bear on this problem via difference of\nconvex programming, and it results in optimal statistical inference. We show\nthat the width of the resulting confidence bands converges at nearly the\nparametric $n^{-\\frac{1}{2}}$ rate when the log density is $k$-affine.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:15:27 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Walther", "Guenther", ""], ["Ali", "Alnur", ""], ["Shen", "Xinyue", ""], ["Boyd", "Stephen", ""]]}, {"id": "2011.03678", "submitter": "Aditya Gangrade", "authors": "Aditya Gangrade, Bobak Nazer, Venkatesh Saligrama", "title": "Limits on Testing Structural Changes in Ising Models", "comments": "Slightly expanded version of a paper to appear at NeurIPS 2020.\n  Supersedes arXiv:1710.10366", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel information-theoretic limits on detecting sparse changes in\nIsing models, a problem that arises in many applications where network changes\ncan occur due to some external stimuli. We show that the sample complexity for\ndetecting sparse changes, in a minimax sense, is no better than learning the\nentire model even in settings with local sparsity. This is a surprising fact in\nlight of prior work rooted in sparse recovery methods, which suggest that\nsample complexity in this context scales only with the number of network\nchanges. To shed light on when change detection is easier than structured\nlearning, we consider testing of edge deletion in forest-structured graphs, and\nhigh-temperature ferromagnets as case studies. We show for these that testing\nof small changes is similarly hard, but testing of \\emph{large} changes is\nwell-separated from structure learning. These results imply that testing of\ngraphical models may not be amenable to concepts such as restricted strong\nconvexity leveraged for sparsity pattern recovery, and algorithm development\ninstead should be directed towards detection of large changes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:33:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Gangrade", "Aditya", ""], ["Nazer", "Bobak", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "2011.03693", "submitter": "Dmitriy Kunisky", "authors": "Dmitriy Kunisky", "title": "Hypothesis testing with low-degree polynomials in the Morris class of\n  exponential families", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of low-degree polynomial algorithms is a powerful, newly-popular\nmethod for predicting computational thresholds in hypothesis testing problems.\nOne limitation of current techniques for this analysis is their restriction to\nBernoulli and Gaussian distributions. We expand this range of possibilities by\nperforming the low-degree analysis of hypothesis testing for the Morris class\nof exponential families, giving a unified treatment of Gaussian, Poisson,\ngamma, binomial, negative binomial, and generalized hyperbolic secant\ndistributions. We then give several algorithmic applications.\n  1. In models where a random signal is observed through an exponential family,\nthe success or failure of low-degree polynomials is governed by the $z$-score\noverlap, the inner product of $z$-score vectors with respect to the null\ndistribution of two independent copies of the signal.\n  2. In the same models, testing with low-degree polynomials exhibits channel\nmonotonicity: the above distributions admit a total ordering by computational\ncost of hypothesis testing, according to a scalar parameter describing how the\nvariance depends on the mean in an exponential family.\n  3. In a spiked matrix model with a particular non-Gaussian noise\ndistribution, the low-degree prediction is incorrect unless polynomials with\narbitrarily large degree in individual matrix entries are permitted. This shows\nthat polynomials summing over self-avoiding walks and variants thereof, as\nproposed recently by Ding, Hopkins, and Steurer (2020) for spiked matrix models\nwith heavy-tailed noise, are suboptimal for this model. Thus low-degree\npolynomials appear to offer a tradeoff between robustness and strong\nperformance fine-tuned to specific models, and may struggle with problems\nrequiring an algorithm to first examine the input and then use some\nintermediate computation to choose from one of several inference subroutines.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 04:48:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kunisky", "Dmitriy", ""]]}, {"id": "2011.03762", "submitter": "Laetitia Della Maestra", "authors": "Laetitia Della Maestra, Marc Hoffmann", "title": "Nonparametric estimation for interacting particle systems :\n  McKean-Vlasov models", "comments": "50 pages. Various minor changes have been made for this second\n  version v2, in order to correct minor errors and/or improve clarity,\n  following comments from anonymous peer reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a system of $N$ interacting particles, governed by transport and\ndiffusion, that converges in a mean-field limit to the solution of a\nMcKean-Vlasov equation. From the observation of a trajectory of the system over\na fixed time horizon, we investigate nonparametric estimation of the solution\nof the associated nonlinear Fokker-Planck equation, together with the drift\nterm that controls the interactions, in a large population limit $N \\rightarrow\n\\infty$. We build data-driven kernel estimators and establish oracle\ninequalities, following Lepski's principle. Our results are based on a new\nBernstein concentration inequality in McKean-Vlasov models for the empirical\nmeasure around its mean, possibly of independent interest. We obtain adaptive\nestimators over anisotropic H\\\"older smoothness classes built upon the solution\nmap of the Fokker-Planck equation, and prove their optimality in a minimax\nsense. In the specific case of the Vlasov model, we derive an estimator of the\ninteraction potential and establish its consistency.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 12:20:00 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 21:13:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Della Maestra", "Laetitia", ""], ["Hoffmann", "Marc", ""]]}, {"id": "2011.03789", "submitter": "Vladimir Koltchinskii", "authors": "Vladimir Koltchinskii", "title": "Estimation of smooth functionals in high-dimensional models: bootstrap\n  chains and Gaussian approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X^{(n)}$ be an observation sampled from a distribution\n$P_{\\theta}^{(n)}$ with an unknown parameter $\\theta,$ $\\theta$ being a vector\nin a Banach space $E$ (most often, a high-dimensional space of dimension $d$).\nWe study the problem of estimation of $f(\\theta)$ for a functional $f:E\\mapsto\n{\\mathbb R}$ of some smoothness $s>0$ based on an observation $X^{(n)}\\sim\nP_{\\theta}^{(n)}.$ Assuming that there exists an estimator $\\hat \\theta_n=\\hat\n\\theta_n(X^{(n)})$ of parameter $\\theta$ such that $\\sqrt{n}(\\hat\n\\theta_n-\\theta)$ is sufficiently close in distribution to a mean zero Gaussian\nrandom vector in $E,$ we construct a functional $g:E\\mapsto {\\mathbb R}$ such\nthat $g(\\hat \\theta_n)$ is an asymptotically normal estimator of $f(\\theta)$\nwith $\\sqrt{n}$ rate provided that $s>\\frac{1}{1-\\alpha}$ and $d\\leq\nn^{\\alpha}$ for some $\\alpha\\in (0,1).$ We also derive general upper bounds on\nOrlicz norm error rates for estimator $g(\\hat \\theta)$ depending on smoothness\n$s,$ dimension $d,$ sample size $n$ and the accuracy of normal approximation of\n$\\sqrt{n}(\\hat \\theta_n-\\theta).$ In particular, this approach yields\nasymptotically efficient estimators in some high-dimensional exponential\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 15:13:56 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:04:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Koltchinskii", "Vladimir", ""]]}, {"id": "2011.03791", "submitter": "Lirong Xia", "authors": "Lirong Xia", "title": "How Likely Are Large Elections Tied?", "comments": "The extended abstract of this paper was accepted to ACM EC-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT econ.TH math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the likelihood for an election to be tied is a classical topic\nin many disciplines including social choice, game theory, political science,\nand public choice. Despite a large body of literature and the common belief\nthat ties are rare, little is known about how rare ties are in large elections\nexcept for a few simple positional scoring rules under the i.i.d. uniform\ndistribution over the votes, known as the Impartial Culture (IC) in social\nchoice. In particular, little progress was made after Marchant explicitly posed\nthe likelihood of k-way ties under IC as an open question in 2001.\n  We give an asymptotic answer to the open question for a wide range of\ncommonly studied voting rules under a model that is much more general and\nrealistic than i.i.d. models (especially IC) -- the smoothed social choice\nframework by Xia that was inspired by the celebrated smoothed complexity\nanalysis by Spielman and Teng. We prove dichotomy theorems on the smoothed\nlikelihood of ties under positional scoring rules, edge-order-based rules, and\nsome multi-round score-based elimination rules, which include commonly studied\nvoting rules such as plurality, Borda, veto, maximin, Copeland, ranked pairs,\nSchulze, STV, and Coombs as special cases. We also complement the theoretical\nresults by experiments on synthetic data and real-world rank data on Preflib.\nOur main technical tool is an improved dichotomous characterization on the\nsmoothed likelihood for a Poisson multinomial variable to be in a polyhedron,\nwhich is proved by exploring the interplay between the V-representation and the\nmatrix representation of polyhedra and might be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 15:16:04 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 15:58:34 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 16:29:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xia", "Lirong", ""]]}, {"id": "2011.03849", "submitter": "Viswambhara Makam", "authors": "Harm Derksen, Visu Makam and Michael Walter", "title": "Maximum likelihood estimation for tensor normal models via castling\n  transforms", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG math.RT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study sample size thresholds for maximum likelihood\nestimation for tensor normal models. Given the model parameters and the number\nof samples, we determine whether, almost surely, (1) the likelihood function is\nbounded from above, (2) maximum likelihood estimates (MLEs) exist, and (3) MLEs\nexist uniquely. We obtain a complete answer for both real and complex models.\nOne consequence of our results is that almost sure boundedness of the\nlog-likelihood function guarantees almost sure existence of an MLE. Our\ntechniques are based on invariant theory and castling transforms.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 21:19:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Derksen", "Harm", ""], ["Makam", "Visu", ""], ["Walter", "Michael", ""]]}, {"id": "2011.03900", "submitter": "Yichen Wang", "authors": "T. Tony Cai, Yichen Wang, Linjun Zhang", "title": "The Cost of Privacy in Generalized Linear Models: Algorithms and Minimax\n  Lower Bounds", "comments": "56 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose differentially private algorithms for parameter estimation in both\nlow-dimensional and high-dimensional sparse generalized linear models (GLMs) by\nconstructing private versions of projected gradient descent. We show that the\nproposed algorithms are nearly rate-optimal by characterizing their statistical\nperformance and establishing privacy-constrained minimax lower bounds for GLMs.\nThe lower bounds are obtained via a novel technique, which is based on Stein's\nLemma and generalizes the tracing attack technique for privacy-constrained\nlower bounds. This lower bound argument can be of independent interest as it is\napplicable to general parametric models. Simulated and real data experiments\nare conducted to demonstrate the numerical performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 04:27:21 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 00:30:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cai", "T. Tony", ""], ["Wang", "Yichen", ""], ["Zhang", "Linjun", ""]]}, {"id": "2011.03917", "submitter": "Cem Kalkanli", "authors": "Cem Kalkanli, Ayfer Ozgur", "title": "Asymptotic Convergence of Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling has been shown to be an effective policy across a variety\nof online learning tasks. Many works have analyzed the finite time performance\nof Thompson sampling, and proved that it achieves a sub-linear regret under a\nbroad range of probabilistic settings. However its asymptotic behavior remains\nmostly underexplored. In this paper, we prove an asymptotic convergence result\nfor Thompson sampling under the assumption of a sub-linear Bayesian regret, and\nshow that the actions of a Thompson sampling agent provide a strongly\nconsistent estimator of the optimal action. Our results rely on the martingale\nstructure inherent in Thompson sampling.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 07:36:49 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kalkanli", "Cem", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2011.04018", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Csaba Szepesv\\'ari, Mengdi Wang", "title": "Online Sparse Reinforcement Learning", "comments": "Accepted at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the hardness of online reinforcement learning in fixed\nhorizon, sparse linear Markov decision process (MDP), with a special focus on\nthe high-dimensional regime where the ambient dimension is larger than the\nnumber of episodes. Our contribution is two-fold. First, we provide a lower\nbound showing that linear regret is generally unavoidable in this case, even if\nthere exists a policy that collects well-conditioned data. The lower bound\nconstruction uses an MDP with a fixed number of states while the number of\nactions scales with the ambient dimension. Note that when the horizon is fixed\nto one, the case of linear stochastic bandits, the linear regret can be\navoided. Second, we show that if the learner has oracle access to a policy that\ncollects well-conditioned data then a variant of Lasso fitted Q-iteration\nenjoys a nearly dimension-free regret of $\\tilde{O}( s^{2/3} N^{2/3})$ where\n$N$ is the number of episodes and $s$ is the sparsity level. This shows that in\nthe large-action setting, the difficulty of learning can be attributed to the\ndifficulty of finding a good exploratory policy.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:47:42 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 21:18:24 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 15:37:53 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 15:55:09 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Wang", "Mengdi", ""]]}, {"id": "2011.04019", "submitter": "Botao Hao", "authors": "Botao Hao, Yaqi Duan, Tor Lattimore, Csaba Szepesv\\'ari, Mengdi Wang", "title": "Sparse Feature Selection Makes Batch Reinforcement Learning More Sample\n  Efficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a statistical analysis of high-dimensional batch\nReinforcement Learning (RL) using sparse linear function approximation. When\nthere is a large number of candidate features, our result sheds light on the\nfact that sparsity-aware methods can make batch RL more sample efficient. We\nfirst consider the off-policy policy evaluation problem. To evaluate a new\ntarget policy, we analyze a Lasso fitted Q-evaluation method and establish a\nfinite-sample error bound that has no polynomial dependence on the ambient\ndimension. To reduce the Lasso bias, we further propose a post model-selection\nestimator that applies fitted Q-evaluation to the features selected via group\nLasso. Under an additional signal strength assumption, we derive a sharper\ninstance-dependent error bound that depends on a divergence function measuring\nthe distribution mismatch between the data distribution and occupancy measure\nof the target policy. Further, we study the Lasso fitted Q-iteration for batch\npolicy optimization and establish a finite-sample error bound depending on the\nratio between the number of relevant features and restricted minimal eigenvalue\nof the data's covariance. In the end, we complement the results with minimax\nlower bounds for batch-data policy evaluation/optimization that nearly match\nour upper bounds. The results suggest that having well-conditioned data is\ncrucial for sparse batch policy learning.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:48:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hao", "Botao", ""], ["Duan", "Yaqi", ""], ["Lattimore", "Tor", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Wang", "Mengdi", ""]]}, {"id": "2011.04020", "submitter": "Botao Hao", "authors": "Botao Hao, Tor Lattimore, Mengdi Wang", "title": "High-Dimensional Sparse Linear Bandits", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic linear bandits with high-dimensional sparse features are a\npractical model for a variety of domains, including personalized medicine and\nonline advertising. We derive a novel $\\Omega(n^{2/3})$ dimension-free minimax\nregret lower bound for sparse linear bandits in the data-poor regime where the\nhorizon is smaller than the ambient dimension and where the feature vectors\nadmit a well-conditioned exploration distribution. This is complemented by a\nnearly matching upper bound for an explore-then-commit algorithm showing that\nthat $\\Theta(n^{2/3})$ is the optimal rate in the data-poor regime. The results\ncomplement existing bounds for the data-rich regime and provide another example\nwhere carefully balancing the trade-off between information and regret is\nnecessary. Finally, we prove a dimension-free $O(\\sqrt{n})$ regret upper bound\nunder an additional assumption on the magnitude of the signal for relevant\nfeatures.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 16:48:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hao", "Botao", ""], ["Lattimore", "Tor", ""], ["Wang", "Mengdi", ""]]}, {"id": "2011.04026", "submitter": "Alexander Terenin", "authors": "James T. Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter\n  Mostowsky, and Marc Peter Deisenroth", "title": "Pathwise Conditioning of Gaussian Processes", "comments": null, "journal-ref": "Journal of Machine Learning Research (2021)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Gaussian processes are used to answer increasingly complex questions,\nanalytic solutions become scarcer and scarcer. Monte Carlo methods act as a\nconvenient bridge for connecting intractable mathematical expressions with\nactionable estimates via sampling. Conventional approaches for simulating\nGaussian process posteriors view samples as draws from marginal distributions\nof process values at finite sets of input locations. This distribution-centric\ncharacterization leads to generative strategies that scale cubically in the\nsize of the desired random vector. These methods are prohibitively expensive in\ncases where we would, ideally, like to draw high-dimensional vectors or even\ncontinuous sample paths. In this work, we investigate a different line of\nreasoning: rather than focusing on distributions, we articulate Gaussian\nconditionals at the level of random variables. We show how this pathwise\ninterpretation of conditioning gives rise to a general family of approximations\nthat lend themselves to efficiently sampling Gaussian process posteriors.\nStarting from first principles, we derive these methods and analyze the\napproximation errors they introduce. We, then, ground these results by\nexploring the practical implications of pathwise conditioning in various\napplied settings, such as global optimization and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 17:09:37 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:38:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wilson", "James T.", ""], ["Borovitskiy", "Viacheslav", ""], ["Terenin", "Alexander", ""], ["Mostowsky", "Peter", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2011.04058", "submitter": "Jiahua Chen", "authors": "Mingxing He and Jiahua Chen", "title": "Consistency of the MLE under a two-parameter gamma mixture model with a\n  structural shape parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite Gamma mixture model is often used to describe randomness in income\ndata, insurance data, and data from other applications. The popular likelihood\napproach, however, does not work for this model because the likelihood function\nis unbounded, and the maximum likelihood estimator is therefore not well\ndefined. There has been much research into ways to ensure the consistent\nestimation of the mixing distribution, including placing an upper bound on the\nshape parameter or adding a penalty to the log-likelihood function. In this\npaper, we show that if the shape parameter in the finite Gamma mixture model is\nstructural, then the maximum likelihood estimator of the mixing distribution is\nwell defined and strongly consistent. We also present simulation results\ndemonstrating the consistency of the estimator. We illustrate the application\nof the model with a structural scale parameter to household income data. The\nfitted mixture distribution leads to several possible subpopulation structures\nin terms of the level of disposable income.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 19:11:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["He", "Mingxing", ""], ["Chen", "Jiahua", ""]]}, {"id": "2011.04067", "submitter": "Ge Zhao", "authors": "Ge Zhao, Yanyuan Ma, Huazhen Lin, Yi Li", "title": "Semiparametric regression of mean residual life with censoring and\n  covariate dimension reduction", "comments": "73 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of semiparametric regression models of mean residual\nlife for censored outcome data. The models, which enable us to estimate the\nexpected remaining survival time and generalize commonly used mean residual\nlife models, also conduct covariate dimension reduction. Using the geometric\napproaches in semiparametrics literature and the martingale properties with\nsurvival data, we propose a flexible inference procedure that relaxes the\nparametric assumptions on the dependence of mean residual life on covariates\nand how long a patient has lived. We show that the estimators for the covariate\neffects are root-$n$ consistent, asymptotically normal, and semiparametrically\nefficient. With the unspecified mean residual life function, we provide a\nnonparametric estimator for predicting the residual life of a given subject,\nand establish the root-$n$ consistency and asymptotic normality for this\nestimator. Numerical experiments are conducted to illustrate the feasibility of\nthe proposed estimators. We apply the method to analyze a national kidney\ntransplantation dataset to further demonstrate the utility of the work.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:11:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhao", "Ge", ""], ["Ma", "Yanyuan", ""], ["Lin", "Huazhen", ""], ["Li", "Yi", ""]]}, {"id": "2011.04147", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Kexuan Li, Zuofeng Shang", "title": "A Computationally Efficient Classification Algorithm in Posterior Drift\n  Model: Phase Transition and Minimax Adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive data analysis, training and testing data often come from very\ndifferent sources, and their probability distributions are not necessarily\nidentical. A feature example is nonparametric classification in posterior drift\nmodel where the conditional distributions of the label given the covariates are\npossibly different. In this paper, we derive minimax rate of the excess risk\nfor nonparametric classification in posterior drift model in the setting that\nboth training and testing data have smooth distributions, extending a recent\nwork by Cai and Wei (2019) who only impose smoothness condition on the\ndistribution of testing data. The minimax rate demonstrates a phase transition\ncharacterized by the mutual relationship between the smoothness orders of the\ntraining and testing data distributions. We also propose a computationally\nefficient and data-driven nearest neighbor classifier which achieves the\nminimax excess risk (up to a logarithm factor). Simulation studies and a\nreal-world application are conducted to demonstrate our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:12:24 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Liu", "Ruiqi", ""], ["Li", "Kexuan", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2011.04185", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Peng Liao", "title": "Robust Batch Policy Learning in Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sequential decision making problem in Markov decision process\n(MDP) where each policy is evaluated by a set containing average rewards over\ndifferent horizon lengths and with different initial distributions. Given a\npre-collected dataset of multiple trajectories generated by some behavior\npolicy, our goal is to learn a robust policy in a pre-specified policy class\nthat can maximize the smallest value of this set. Leveraging the\nsemi-parametric efficiency theory from statistics, we develop a policy learning\nmethod for estimating the defined robust optimal policy that can efficiently\nbreak the curse of horizon under mild technical conditions. A rate-optimal\nregret bound up to a logarithmic factor is established in terms of the number\nof trajectories and the number of decision points.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:41:21 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 06:04:47 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 03:58:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Qi", "Zhengling", ""], ["Liao", "Peng", ""]]}, {"id": "2011.04259", "submitter": "Eddie Aamari", "authors": "Eddie Aamari and Alexander Knop", "title": "Statistical Query Complexity of Manifold Estimation", "comments": "81 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the statistical query (SQ) complexity of estimating\n$d$-dimensional submanifolds in $\\mathbb{R}^n$. We propose a purely geometric\nalgorithm called Manifold Propagation, that reduces the problem to three\nnatural geometric routines: projection, tangent space estimation, and point\ndetection. We then provide constructions of these geometric routines in the SQ\nframework. Given an adversarial $\\mathrm{STAT}(\\tau)$ oracle and a target\nHausdorff distance precision $\\varepsilon = \\Omega(\\tau^{2 / (d + 1)})$, the\nresulting SQ manifold reconstruction algorithm has query complexity $O(n\n\\operatorname{polylog}(n) \\varepsilon^{-d / 2})$, which is proved to be nearly\noptimal. In the process, we establish low-rank matrix completion results for\nSQ's and lower bounds for randomized SQ estimators in general metric spaces.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 09:14:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Aamari", "Eddie", ""], ["Knop", "Alexander", ""]]}, {"id": "2011.04369", "submitter": "Steffen Betsch", "authors": "Steffen Betsch, Bruno Ebner, Franz Nestmann", "title": "Characterizations of non-normalized discrete probability distributions\n  and their application in statistics", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the distributional characterizations that lie at the heart of Stein's\nmethod we derive explicit formulae for the mass functions of discrete\nprobability laws that identify those distributions. These identities are\napplied to develop tools for the solution of statistical problems. Our\ncharacterizations, and hence the applications built on them, do not require any\nknowledge about normalization constants of the probability laws. We discuss\nseveral examples where this lack of feasibility of the normalization constant\nis a built-in feature. To demonstrate that our statistical methods are sound,\nwe provide comparative simulation studies for the testing of fit to the Poisson\ndistribution and for parameter estimation of the negative binomial family when\nboth parameters are unknown. We also consider the problem of parameter\nestimation for discrete exponential-polynomial models which generally are\nnon-normalized.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:08:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""], ["Nestmann", "Franz", ""]]}, {"id": "2011.04470", "submitter": "Soumendu Sundar Mukherjee", "authors": "Abhinav Chakraborty and Soumendu Sundar Mukherjee and Arijit\n  Chakrabarti", "title": "High dimensional PCA: a new model selection criterion", "comments": "37 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample from a multivariate population, estimating the number\nof large eigenvalues of the population covariance matrix is an important\nproblem in Statistics with wide applications in many areas. In the context of\nPrincipal Component Analysis (PCA), the linear combinations of the original\nvariables having the largest amounts of variation are determined by this\nnumber. In this paper, we study the high dimensional asymptotic regime where\nthe number of variables grows at the same rate as the number of observations,\nand use the spiked covariance model proposed in Johnstone (2001), under which\nthe problem reduces to model selection. Our focus is on the Akaike Information\nCriterion (AIC) which is known to be strongly consistent from the work of Bai\net al. (2018). However, Bai et al. (2018) requires a certain \"gap condition\"\nensuring the dominant eigenvalues to be above a threshold strictly larger than\nthe BBP threshold (Baik et al. (2005), both quantities depending on the\nlimiting ratio of the number of variables and observations. It is well-known\nthat, below the BBP threshold, a spiked covariance structure becomes\nindistinguishable from one with no spikes. Thus the strong consistency of AIC\nrequires some extra signal strength.\n  In this paper, we investigate whether consistency continues to hold even if\nthe \"gap\" is made smaller. We show that strong consistency under arbitrarily\nsmall gap is achievable if we alter the penalty term of AIC suitably depending\non the target gap. Furthermore, another intuitive alteration of the penalty can\nindeed make the gap exactly zero, although we can only achieve weak consistency\nin this case. We compare the two newly-proposed estimators with other existing\nestimators in the literature via extensive simulation studies, and show, by\nsuitably calibrating our proposals, that a significant improvement in terms of\nmean-squared error is achievable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:42:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chakraborty", "Abhinav", ""], ["Mukherjee", "Soumendu Sundar", ""], ["Chakrabarti", "Arijit", ""]]}, {"id": "2011.04483", "submitter": "Steve Hanneke", "authors": "Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, Amir\n  Yehudayoff", "title": "A Theory of Universal Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How quickly can a given class of concepts be learned from examples? It is\ncommon to measure the performance of a supervised machine learning algorithm by\nplotting its \"learning curve\", that is, the decay of the error rate as a\nfunction of the number of training examples. However, the classical theoretical\nframework for understanding learnability, the PAC model of Vapnik-Chervonenkis\nand Valiant, does not explain the behavior of learning curves: the\ndistribution-free PAC model of learning can only bound the upper envelope of\nthe learning curves over all possible data distributions. This does not match\nthe practice of machine learning, where the data source is typically fixed in\nany given scenario, while the learner may choose the number of training\nexamples on the basis of factors such as computational resources and desired\naccuracy.\n  In this paper, we study an alternative learning model that better captures\nsuch practical aspects of machine learning, but still gives rise to a complete\ntheory of the learnable in the spirit of the PAC model. More precisely, we\nconsider the problem of universal learning, which aims to understand the\nperformance of learning algorithms on every data distribution, but without\nrequiring uniformity over the distribution. The main result of this paper is a\nremarkable trichotomy: there are only three possible rates of universal\nlearning. More precisely, we show that the learning curves of any given concept\nclass decay either at an exponential, linear, or arbitrarily slow rates.\nMoreover, each of these cases is completely characterized by appropriate\ncombinatorial parameters, and we exhibit optimal learning algorithms that\nachieve the best possible rate in each case.\n  For concreteness, we consider in this paper only the realizable case, though\nanalogous results are expected to extend to more general learning scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:10:32 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bousquet", "Olivier", ""], ["Hanneke", "Steve", ""], ["Moran", "Shay", ""], ["van Handel", "Ramon", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "2011.04493", "submitter": "Justin Krometis", "authors": "Nathan E. Glatt-Holtz, Justin A. Krometis, Cecilia F. Mondaini", "title": "On the accept-reject mechanism for Metropolis-Hastings algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a powerful and versatile framework for determining\nacceptance ratios in Metropolis-Hastings type Markov kernels widely used in\nstatistical sampling problems. Our approach allows us to derive new classes of\nkernels which unify random walk or diffusion-type sampling methods with more\ncomplicated \"extended phase space\" algorithms based around ideas from\nHamiltonian dynamics. Our starting point is an abstract result developed in the\ngenerality of measurable state spaces that addresses proposal kernels that\npossess a certain involution structure. Note that, while this underlying\nproposal structure suggests a scope which includes Hamiltonian-type kernels, we\ndemonstrate that our abstract result is, in an appropriate sense, equivalent to\nan earlier general state space setting developed in [Tierney, Annals of Applied\nProbability, 1998] where the connection to Hamiltonian methods was more\nobscure. Altogether, the theoretical unity and reach of our main result\nprovides a basis for deriving novel sampling algorithms while laying bare\nimportant relationships between existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:24:50 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 19:54:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Glatt-Holtz", "Nathan E.", ""], ["Krometis", "Justin A.", ""], ["Mondaini", "Cecilia F.", ""]]}, {"id": "2011.04622", "submitter": "Zhuoran Yang", "authors": "Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, Michael I. Jordan", "title": "On Function Approximation in Reinforcement Learning: Optimism in the\n  Face of Large State Spaces", "comments": "76 pages. The short version of this work appears in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical theory of reinforcement learning (RL) has focused on tabular\nand linear representations of value functions. Further progress hinges on\ncombining RL with modern function approximators such as kernel functions and\ndeep neural networks, and indeed there have been many empirical successes that\nhave exploited such combinations in large-scale applications. There are\nprofound challenges, however, in developing a theory to support this\nenterprise, most notably the need to take into consideration the\nexploration-exploitation tradeoff at the core of RL in conjunction with the\ncomputational and statistical tradeoffs that arise in modern\nfunction-approximation-based learning systems. We approach these challenges by\nstudying an optimistic modification of the least-squares value iteration\nalgorithm, in the context of the action-value function\n  represented by a kernel function or an overparameterized neural network. We\nestablish both polynomial runtime complexity and polynomial sample complexity\nfor this algorithm, without additional assumptions on the data-generating\nmodel. In particular, we prove that the algorithm incurs an\n$\\tilde{\\mathcal{O}}(\\delta_{\\mathcal{F}} H^2 \\sqrt{T})$ regret, where\n$\\delta_{\\mathcal{F}}$ characterizes the intrinsic complexity of the function\nclass $\\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total\nnumber of episodes. Our regret bounds are independent of the number of states,\na result which exhibits clearly the benefit of function approximation in RL.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:32:22 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 17:24:48 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yang", "Zhuoran", ""], ["Jin", "Chi", ""], ["Wang", "Zhaoran", ""], ["Wang", "Mengdi", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.04733", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher and Tobias Jennessen", "title": "Statistical analysis for stationary time series at extreme levels: new\n  estimators for the limiting cluster size distribution", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A measure of primal importance for capturing the serial dependence of a\nstationary time series at extreme levels is provided by the limiting cluster\nsize distribution. New estimators based on a blocks declustering scheme are\nproposed and analyzed both theoretically and by means of a large-scale\nsimulation study. A sliding blocks version of the estimators is shown to\noutperform a disjoint blocks version. In contrast to some competitors from the\nliterature, the estimators only depend on one unknown parameter to be chosen by\nthe statistician.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:19:20 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Jennessen", "Tobias", ""]]}, {"id": "2011.04766", "submitter": "Werner Brannath", "authors": "Werner Brannath, Charlie Hillner, Kornelius Rohmeyer", "title": "A liberal type I error rate for studies in precision medicine", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multiple type I error criterion for clinical trials with\nmultiple populations. Such trials are of interest in precision medicine where\nthe goal is to develop treatments that are targeted to specific sub-populations\ndefined by genetic and/or clinical biomarkers. The new criterion is based on\nthe observation that not all type I errors are relevant to all patients in the\noverall population. If disjoint sub-populations are considered, no multiplicity\nadjustment appears necessary, since a claim in one sub-population does not\naffect patients in the other ones. For intersecting sub-populations we suggest\nto control the average multiple type error rate, i.e. the probably that a\nrandomly selected patient will be exposed to an inefficient treatment. We call\nthis the population-wise error rate, exemplify it by a number of examples and\nillustrate how to control it with an adjustment of critical boundaries or\nadjusted p-values. We furthermore define corresponding simultaneous confidence\nintervals. We finally illustrate the power gain achieved by passing from\nfamily-wise to population-wise error rate control with two simple examples and\na recently suggest multiple testing approach for umbrella trials.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:10:14 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:52:40 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Brannath", "Werner", ""], ["Hillner", "Charlie", ""], ["Rohmeyer", "Kornelius", ""]]}, {"id": "2011.04834", "submitter": "Daniel Andr\\'es D\\'iaz-Pach\\'on", "authors": "Daniel Andr\\'es D\\'iaz-Pach\\'on and Juan Pablo S\\'aenz and J. Sunil\n  Rao", "title": "Hypothesis testing with active information", "comments": "Typo changed in one of the names in the Metadata, and a reference to\n  an equation from the paper in the Supplement", "journal-ref": "Statistics and Probability Letters 161, June 2020, 108742", "doi": "10.1016/j.spl.2020.108742", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop hypothesis testing for active information -the averaged quantity\nin the Kullback-Liebler divergence. To our knowledge, this is the first paper\nto derive exact probabilities of type-I errors for hypothesis testing in the\narea.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:43:12 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:32:18 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["D\u00edaz-Pach\u00f3n", "Daniel Andr\u00e9s", ""], ["S\u00e1enz", "Juan Pablo", ""], ["Rao", "J. Sunil", ""]]}, {"id": "2011.04874", "submitter": "Andriy Olenko", "authors": "Illia Donhauzer, Andriy Olenko, Andrei Volodin", "title": "Strong Law of Large Numbers for Functionals of Random Fields With\n  Unboundedly Increasing Covariances", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proves the Strong Law of Large Numbers for integral functionals of\nrandom fields with unboundedly increasing covariances. The case of functional\ndata and increasing domain asymptotics is studied. Conditions to guarantee that\nthe Strong Law of Large Numbers holds true are provided. The considered\nscenarios include wide classes of non-stationary random fields. The discussion\nabout application to weak and long-range dependent random fields and numerical\nexamples are given.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 03:45:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Donhauzer", "Illia", ""], ["Olenko", "Andriy", ""], ["Volodin", "Andrei", ""]]}, {"id": "2011.04907", "submitter": "Paxton Turner", "authors": "Paxton Turner, Jingbo Liu, Philippe Rigollet", "title": "A Statistical Perspective on Coreset Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coresets have emerged as a powerful tool to summarize data by selecting a\nsmall subset of the original observations while retaining most of its\ninformation. This approach has led to significant computational speedups but\nthe performance of statistical procedures run on coresets is largely\nunexplored. In this work, we develop a statistical framework to study coresets\nand focus on the canonical task of nonparameteric density estimation. Our\ncontributions are twofold. First, we establish the minimax rate of estimation\nachievable by coreset-based estimators. Second, we show that the practical\ncoreset kernel density estimators are near-minimax optimal over a large class\nof H\\\"{o}lder-smooth densities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 05:18:43 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 21:05:08 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Turner", "Paxton", ""], ["Liu", "Jingbo", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2011.04922", "submitter": "Paxton Turner", "authors": "Paxton Turner, Jingbo Liu, and Philippe Rigollet", "title": "Efficient Interpolation of Density Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of space and time efficient evaluation of a\nnonparametric estimator that approximates an unknown density. In the regime\nwhere consistent estimation is possible, we use a piecewise multivariate\npolynomial interpolation scheme to give a computationally efficient\nconstruction that converts the original estimator to a new estimator that can\nbe queried efficiently and has low space requirements, all without adversely\ndeteriorating the original approximation quality. Our result gives a new\nstatistical perspective on the problem of fast evaluation of kernel density\nestimators in the presence of underlying smoothness. As a corollary, we give a\nsuccinct derivation of a classical result of Kolmogorov---Tikhomirov on the\nmetric entropy of H\\\"{o}lder classes of smooth functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:05:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Turner", "Paxton", ""], ["Liu", "Jingbo", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2011.04953", "submitter": "Satoshi Kuriki", "authors": "Satoshi Kuriki, Takahiko Matsubara", "title": "Perturbation of the expected Minkowski functional for weakly\n  non-Gaussian isotropic fields on a bounded domain", "comments": "38 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": "KEK-TH-2273; KEK-Cosmo-0266", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minkowski functionals (MF) including the Euler characteristic (EC)\nstatistics are standard tools for morphological analysis in cosmology.\nMotivated by cosmic research, we consider the Minkowski functional of the\nexcursion set for a weakly non-Gaussian isotropic smooth random field on an\narbitrary dimensional compact domain. The weak non-Gaussianity is represented\nby the $N$-point correlation function of the order $O(\\nu^{N-2})$, where\n$\\nu\\ll 1$ is a non-Gaussianity parameter. We obtain the perturbation\nexpansions of the expected Euler characteristic and the Minkowski functional of\nthe excursion set up to $O(\\nu^2)$ including the skewness and kurtosis. The\nresulting formula reveals the local power property of the Minkowski functional\nas a statistic for testing Gaussianity. Moreover, up to an arbitrary order in\n$\\nu$, the perturbation formula for the expected Minkowski functional is shown\nto be a linear combination of the Euler characteristic density function\nmultiplied by the Lipschitz-Killing curvature of the index set, which has the\nsame form as the Gaussian kinematic formula (GKF). The application of the\nobtained perturbation formula in cosmic research is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 07:40:01 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kuriki", "Satoshi", ""], ["Matsubara", "Takahiko", ""]]}, {"id": "2011.05068", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Aaditya Ramdas", "title": "Dimension-agnostic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical asymptotic theory for statistical inference usually involves\ncalibrating a statistic by fixing the dimension $d$ while letting the sample\nsize $n$ increase to infinity. Recently, much effort has been dedicated towards\nunderstanding how these methods behave in high-dimensional settings, where\n$d_n$ and $n$ both increase to infinity together at some prescribed relative\nrate. This often leads to different inference procedures, depending on the\nassumptions about the dimensionality, leaving the practitioner in a bind: given\na dataset with 100 samples in 20 dimensions, should they calibrate by assuming\n$n \\gg d$, or $d_n/n \\approx 0.2$? This paper considers the goal of\ndimension-agnostic inference -- developing methods whose validity does not\ndepend on any assumption on $d_n$. We introduce a new, generic approach that\nuses variational representations of existing test statistics along with sample\nsplitting and self-normalization to produce a new test statistic with a\nGaussian limiting distribution. The resulting statistic can be viewed as a\ncareful modification of degenerate U-statistics, dropping diagonal blocks and\nretaining off-diagonals. We exemplify our technique for a handful of classical\nproblems including one-sample mean and covariance testing. Our tests are shown\nto have minimax rate-optimal power against appropriate local alternatives, and\nwithout explicitly targeting the high-dimensional setting their power is\noptimal up to a $\\sqrt 2$ factor. A hidden advantage is that our proofs are\nsimple and transparent. We end by describing several fruitful open directions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:21:34 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 23:50:28 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kim", "Ilmun", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2011.05195", "submitter": "Xinhe Wang", "authors": "Xinhe Wang, Tingyu Wang, Hanzhong Liu", "title": "Rerandomization in stratified randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratification and rerandomization are two well-known methods used in\nrandomized experiments for balancing the baseline covariates. Renowned scholars\nin experimental design have recommended combining these two methods; however,\nlimited studies have addressed the statistical properties of this combination.\nThis paper proposes two rerandomization methods to be used in stratified\nrandomized experiments, based on the overall and stratum-specific Mahalanobis\ndistances. The first method is applicable for nearly arbitrary numbers of\nstrata, strata sizes, and stratum-specific proportions of the treated units.\nThe second method, which is generally more efficient than the first method, is\nsuitable for situations in which the number of strata is fixed with their sizes\ntending to infinity. Under the randomization inference framework, we obtain\nthese methods' asymptotic distributions and the formulas of variance reduction\nwhen compared to stratified randomization. Our analysis does not require any\nmodeling assumption regarding the potential outcomes. Moreover, we provide\nasymptotically conservative variance estimators and confidence intervals for\nthe average treatment effect. The advantages of the proposed methods are\nexhibited through an extensive simulation study and a real-data example.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:50:11 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Xinhe", ""], ["Wang", "Tingyu", ""], ["Liu", "Hanzhong", ""]]}, {"id": "2011.05245", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang and Yi Li", "title": "Gaussian Graphical Regression Models with High Dimensional Responses and\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though Gaussian graphical models have been widely used in many scientific\nfields, limited progress has been made to link graph structures to external\ncovariates because of substantial challenges in theory and computation. We\npropose a Gaussian graphical regression model, which regresses both the mean\nand the precision matrix of a Gaussian graphical model on covariates. In the\ncontext of co-expression quantitative trait locus (QTL) studies, our framework\nfacilitates estimation of both population- and subject-level gene regulatory\nnetworks, and detection of how subject-level networks vary with genetic\nvariants and clinical conditions. Our framework accommodates high dimensional\nresponses and covariates, and encourages covariate effects on both the mean and\nthe precision matrix to be sparse. In particular for the precision matrix, we\nstipulate simultaneous sparsity, i.e., group sparsity and element-wise\nsparsity, on effective covariates and their effects on network edges,\nrespectively. We establish variable selection consistency first under the case\nwith known mean parameters and then a more challenging case with unknown means\ndepending on external covariates, and show in both cases that the convergence\nrate of the estimated precision parameters is faster than that obtained by\nlasso or group lasso, a desirable property for the sparse group lasso\nestimation. The utility and efficacy of our proposed method is demonstrated\nthrough simulation studies and an application to a co-expression QTL study with\nbrain cancer patients.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:12:44 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Jingfei", ""], ["Li", "Yi", ""]]}, {"id": "2011.05258", "submitter": "Ilias Zadik", "authors": "Fotis Iliopoulos and Ilias Zadik", "title": "Group testing and local search: is there a computational-statistical\n  gap?", "comments": "Accepted for publication in COLT 2021. Various minor mistakes are\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the fundamental limits of approximate recovery in the\ncontext of group testing. One of the most well-known, theoretically optimal,\nand easy to implement testing procedures is the non-adaptive Bernoulli group\ntesting problem, where all tests are conducted in parallel, and each item is\nchosen to be part of any certain test independently with some fixed\nprobability. In this setting, there is an observed gap between the number of\ntests above which recovery is information theoretically (IT) possible, and the\nnumber of tests required by the currently best known efficient algorithms to\nsucceed. Often times such gaps are explained by a phase transition in the\nlandscape of the solution space of the problem (an Overlap Gap Property phase\ntransition).\n  In this paper we seek to understand whether such a phenomenon takes place for\nBernoulli group testing as well. Our main contributions are the following: (1)\nWe provide first moment evidence that, perhaps surprisingly, such a phase\ntransition does not take place throughout the regime for which recovery is IT\npossible. This fact suggests that the model is in fact amenable to local search\nalgorithms ; (2) we prove the complete absence of \"bad\" local minima for a part\nof the \"hard\" regime, a fact which implies an improvement over known\ntheoretical results on the performance of efficient algorithms for approximate\nrecovery without false-negatives, and finally (3) we present extensive\nsimulations that strongly suggest that a very simple local algorithm known as\nGlauber Dynamics does indeed succeed, and can be used to efficiently implement\nthe well-known (theoretically optimal) Smallest Satisfying Set (SSS) estimator.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:25:48 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 03:15:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Iliopoulos", "Fotis", ""], ["Zadik", "Ilias", ""]]}, {"id": "2011.05433", "submitter": "Irving G\u00f3mez", "authors": "Irving G\\'omez-M\\'endez and Emilien Joly", "title": "On the Consistency of a Random Forest Algorithm in the Presence of\n  Missing Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of constructing a non-parametric predictor\nwhen the latent variables are given with incomplete information. The convenient\npredictor for this task is the random forest algorithm in conjunction to the\nso-called CART criterion. The proposed technique enables a partial imputation\nof the missing values in the data set in a way that suits both a consistent\nestimator of the regression function as well as a partial recovery of the\nmissing values. A proof of the consistency of the random forest estimator is\ngiven in the case where each latent variable is missing completely at random\n(MCAR).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:35:21 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 00:20:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["G\u00f3mez-M\u00e9ndez", "Irving", ""], ["Joly", "Emilien", ""]]}, {"id": "2011.05441", "submitter": "Alejandro  Cholaquidis", "authors": "Jos\\'e R. Berrendero, Alejandro Cholaquidis, Antonio Cuevas", "title": "On a general definition of the functional linear model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general formulation of the linear model with functional (random)\nexplanatory variable $X = X(t), t \\in T$ , and scalar response Y is proposed.\nIt includes the standard functional linear model, based on the inner product in\nthe space $L^2[0,1]$, as a particular case. It also includes all models in\nwhich Y is assumed to be (up to an additive noise) a linear combination of a\nfinite or countable collections of marginal variables X(t_j), with $t_j\\in T$\nor a linear combination of a finite number of linear projections of X. This\ngeneral formulation can be interpreted in terms of the RKHS space generated by\nthe covariance function of the process X(t). Some consistency results are\nproved. A few experimental results are given in order to show the practical\ninterest of considering, in a unified framework, linear models based on a\nfinite number of marginals $X(t_j)$ of the process $X(t)$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:45:23 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 01:34:03 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 17:17:25 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 21:45:12 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cholaquidis", "Alejandro", ""], ["Cuevas", "Antonio", ""]]}, {"id": "2011.05460", "submitter": "Nikolai Krivulin", "authors": "Nikolai Krivulin", "title": "Using parameter elimination to solve discrete linear Chebyshev\n  approximation problems", "comments": "23 pages", "journal-ref": "Mathematics, 2020, 8(12), 2210", "doi": "10.3390/math8122210", "report-no": null, "categories": "math.OC cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider discrete linear Chebyshev approximation problems in which the\nunknown parameters of linear function are fitted by minimizing the maximum\nabsolute deviation of errors. Such problems find application in the solution of\noverdetermined systems of linear equations that appear in many practical\ncontexts. The least maximum absolute deviation estimator is used in regression\nanalysis in statistics when the distribution of errors has bounded support. To\nderive a direct solution of the problem, we propose an algebraic approach based\non a parameter elimination technique. As a key component of the approach, an\nelimination lemma is proved to handle the problem by reducing it to a problem\nwith one parameter eliminated, together with a box constraint imposed on this\nparameter. We demonstrate the application of the lemma to the direct solution\nof linear regression problems with one and two parameters. We develop a\nprocedure to solve multidimensional approximation (multiple linear regression)\nproblems in a finite number of steps. The procedure follows a method that\ncomprises two phases: backward elimination and forward substitution of\nparameters. We describe the main components of the procedure and estimate its\ncomputational complexity. We implement symbolic computations in MATLAB to\nobtain exact solutions for two numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 23:40:12 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 00:00:27 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 11:05:40 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 16:40:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Krivulin", "Nikolai", ""]]}, {"id": "2011.05988", "submitter": "HaiYing Wang", "authors": "HaiYing Wang and Jae Kwang Kim", "title": "Maximum sampled conditional likelihood for informative subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsampling is a computationally effective approach to extract information\nfrom massive data sets when computing resources are limited. After a subsample\nis taken from the full data, most available methods use an inverse probability\nweighted objective function to estimate the model parameters. This type of\nweighted estimator does not fully utilize information in the selected\nsubsample. In this paper, we propose to use the maximum sampled conditional\nlikelihood estimator (MSCLE) based on the sampled data. We established the\nasymptotic normality of the MSCLE and prove that its asymptotic variance\ncovariance matrix is the smallest among a class of asymptotically unbiased\nestimators, including the inverse probability weighted estimator. We further\ndiscuss the asymptotic results with the L-optimal subsampling probabilities and\nillustrate the estimation procedure with generalized linear models. Numerical\nexperiments are provided to evaluate the practical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:01:17 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 01:38:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "HaiYing", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2011.06062", "submitter": "Hang Liu", "authors": "Marc Hallin, Davide La Vecchia, Hang Liu", "title": "Rank-Based Testing for Semiparametric VAR Models: a measure\n  transportation approach", "comments": "51 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a class of tests for semiparametric vector autoregressive (VAR)\nmodels with unspecified innovation densities, based on the recent\nmeasure-transportation-based concepts of multivariate {\\it center-outward\nranks} and {\\it signs}. We show that these concepts, combined with Le Cam's\nasymptotic theory of statistical experiments, yield novel testing procedures,\nwhich (a)~are valid under a broad class of innovation densities (possibly\nnon-elliptical, skewed, and/or with infinite moments), (b)~are optimal (locally\nasymptotically maximin or most stringent) at selected ones, and (c) are robust\nagainst additive outliers. In order to do so, we establish a H\\' ajek\nasymptotic representation result, of independent interest, for a general class\nof center-outward rank-based serial statistics. As an illustration, we consider\nthe problems of testing the absence of serial correlation in multiple-output\nand possibly non-linear regression (an extension of the classical Durbin-Watson\nproblem) and the sequential identification of the order $p$ of a vector\nautoregressive (VAR($p$)) model. A Monte Carlo comparative study of our tests\nand their routinely-applied Gaussian competitors demonstrates the benefits (in\nterms of size, power, and robustness) of our methodology; these benefits are\nparticularly significant in the presence of asymmetric and leptokurtic\ninnovation densities. A real data application concludes the paper.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:44:05 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Hallin", "Marc", ""], ["La Vecchia", "Davide", ""], ["Liu", "Hang", ""]]}, {"id": "2011.06186", "submitter": "Yunbei Xu", "authors": "Yunbei Xu, Assaf Zeevi", "title": "Towards Optimal Problem Dependent Generalization Error Bounds in\n  Statistical Learning Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study problem-dependent rates, i.e., generalization errors that scale\nnear-optimally with the variance, the effective loss, or the gradient norms\nevaluated at the \"best hypothesis.\" We introduce a principled framework dubbed\n\"uniform localized convergence,\" and characterize sharp problem-dependent rates\nfor central statistical learning problems. From a methodological viewpoint, our\nframework resolves several fundamental limitations of existing uniform\nconvergence and localization analysis approaches. It also provides improvements\nand some level of unification in the study of localized complexities, one-sided\nuniform inequalities, and sample-based iterative algorithms. In the so-called\n\"slow rate\" regime, we provides the first (moment-penalized) estimator that\nachieves the optimal variance-dependent rate for general \"rich\" classes; we\nalso establish improved loss-dependent rate for standard empirical risk\nminimization. In the \"fast rate\" regime, we establish finite-sample\nproblem-dependent bounds that are comparable to precise asymptotics. In\naddition, we show that iterative algorithms like gradient descent and\nfirst-order Expectation-Maximization can achieve optimal generalization error\nin several representative problems across the areas of non-convex learning,\nstochastic optimization, and learning with missing data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 04:07:29 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 21:01:35 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 23:01:58 GMT"}, {"version": "v4", "created": "Thu, 24 Dec 2020 03:10:54 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Xu", "Yunbei", ""], ["Zeevi", "Assaf", ""]]}, {"id": "2011.06202", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Christos Tzamos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Ilias\n  Zadik", "title": "Optimal Private Median Estimation under Minimal Distributional\n  Assumptions", "comments": "49 pages, NeurIPS 2020, Spotlight talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.DS math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the fundamental task of estimating the median of an underlying\ndistribution from a finite number of samples, under pure differential privacy\nconstraints. We focus on distributions satisfying the minimal assumption that\nthey have a positive density at a small neighborhood around the median. In\nparticular, the distribution is allowed to output unbounded values and is not\nrequired to have finite moments. We compute the exact, up-to-constant terms,\nstatistical rate of estimation for the median by providing nearly-tight upper\nand lower bounds. Furthermore, we design a polynomial-time differentially\nprivate algorithm which provably achieves the optimal performance. At a\ntechnical level, our results leverage a Lipschitz Extension Lemma which allows\nus to design and analyze differentially private algorithms solely on\nappropriately defined \"typical\" instances of the samples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 04:54:30 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tzamos", "Christos", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Zadik", "Ilias", ""]]}, {"id": "2011.06208", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh and Flavio Calmon", "title": "Bottleneck Problems: Information and Estimation-Theoretic View", "comments": null, "journal-ref": null, "doi": "10.3390/e22111325", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information bottleneck (IB) and privacy funnel (PF) are two closely related\noptimization problems which have found applications in machine learning, design\nof privacy algorithms, capacity problems (e.g., Mrs. Gerber's Lemma), strong\ndata processing inequalities, among others. In this work, we first investigate\nthe functional properties of IB and PF through a unified theoretical framework.\nWe then connect them to three information-theoretic coding problems, namely\nhypothesis testing against independence, noisy source coding and dependence\ndilution. Leveraging these connections, we prove a new cardinality bound for\nthe auxiliary variable in IB, making its computation more tractable for\ndiscrete random variables.\n  In the second part, we introduce a general family of optimization problems,\ntermed as \\textit{bottleneck problems}, by replacing mutual information in IB\nand PF with other notions of mutual information, namely $f$-information and\nArimoto's mutual information. We then argue that, unlike IB and PF, these\nproblems lead to easily interpretable guarantee in a variety of inference tasks\nwith statistical constraints on accuracy and privacy. Although the underlying\noptimization problems are non-convex, we develop a technique to evaluate\nbottleneck problems in closed form by equivalently expressing them in terms of\nlower convex or upper concave envelope of certain functions. By applying this\ntechnique to binary case, we derive closed form expressions for several\nbottleneck problems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 05:16:44 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Calmon", "Flavio", ""]]}, {"id": "2011.06229", "submitter": "Andriy Olenko", "authors": "Antoine Ayache, Myriam Fradon, Ravindi Nanayakkara, Andriy Olenko", "title": "Asymptotic normality of simultaneous estimators of cyclic long-memory\n  processes", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral singularities at non-zero frequencies play an important role in\ninvestigating cyclic or seasonal time series. The publication [2] introduced\nthe generalized filtered method-of-moments approach to simultaneously estimate\nsingularity location and long-memory parameters. This paper continues studies\nof these simultaneous estimators. A wide class of Gegenbauer-type\nsemi-parametric models is considered. Asymptotic normality of several\nstatistics of the cyclic and long-memory parameters is proved. New adjusted\nestimates are proposed and investigated. The theoretical findings are\nillustrated by numerical results. The methodology includes wavelet\ntransformations as a particular case.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:54:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ayache", "Antoine", ""], ["Fradon", "Myriam", ""], ["Nanayakkara", "Ravindi", ""], ["Olenko", "Andriy", ""]]}, {"id": "2011.06316", "submitter": "Adelchi Azzalini", "authors": "Reinaldo B. Arellano-Valle and Adelchi Azzalini", "title": "Some properties of the unified skew-normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For the family of multivariate probability distributions variously denoted as\nunified skew-normal, closed skew-normal and other names, a number of properties\nare already known, but many others are not, even some basic ones. The present\ncontribution aims at filling some of the missing gaps. Specifically, the\nmoments up to the fourth order are obtained, and from here the expressions of\nthe Mardia's measures of multivariate skewness and kurtosis. Other results\nconcern the property of log-concavity of the distribution, and closure with\nrespect to conditioning on intervals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 11:11:45 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Arellano-Valle", "Reinaldo B.", ""], ["Azzalini", "Adelchi", ""]]}, {"id": "2011.06436", "submitter": "Liliana Forzani", "authors": "R. Dennis Cook, Liliana Forzani", "title": "Fundamentals of path analysis in the social sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a recent series of diametrically opposed articles on the\nrelative value of statistical methods for the analysis of path diagrams in the\nsocial sciences, we discuss from a primarily theoretical perspective selected\nfundamental aspects of path modeling and analysis based on a common re\nreflexive setting. Since there is a paucity of technical support evident in the\ndebate, our aim is to connect it to mainline statistics literature and to\naddress selected foundational issues that may help move the discourse. We do\nnot intend to advocate for or against a particular method or analysis\nphilosophy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:10:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Cook", "R. Dennis", ""], ["Forzani", "Liliana", ""]]}, {"id": "2011.06765", "submitter": "Yisha Yao", "authors": "Yisha Yao and Cun-Hui Zhang", "title": "Adaptive Estimation In High-Dimensional Additive Models With\n  Multi-Resolution Group Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In additive models with many nonparametric components, a number of\nregularized estimators have been proposed and proven to attain various error\nbounds under different combinations of sparsity and fixed smoothness\nconditions. Some of these error bounds match minimax rates in the corresponding\nsettings. Some of the rate minimax methods are non-convex and computationally\ncostly. From these perspectives, the existing solutions to the high-dimensional\nadditive nonparametric regression problem are fragmented. In this paper, we\npropose a multi-resolution group Lasso (MR-GL) method in a unified approach to\nsimultaneously achieve or improve existing error bounds and provide new ones\nwithout the knowledge of the level of sparsity or the degree of smoothness of\nthe unknown functions. Such adaptive convergence rates are established when a\nprediction factor can be treated as a constant. Furthermore, we prove that the\nprediction factor, which can be bounded in terms of a restricted eigenvalue or\na compatibility coefficient, can be indeed treated as a constant for random\ndesigns under a nearly optimal sample size condition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 05:21:08 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yao", "Yisha", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2011.06794", "submitter": "Gilles Blanchard", "authors": "Hannah Marienwald (TUB), Jean-Baptiste Fermanian (ENS Rennes), Gilles\n  Blanchard (DATASHAPE, LMO, CNRS)", "title": "High-Dimensional Multi-Task Averaging and Application to Kernel Mean\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved estimator for the multi-task averaging problem, whose\ngoal is the joint estimation of the means of multiple distributions using\nseparate, independent data sets. The naive approach is to take the empirical\nmean of each data set individually, whereas the proposed method exploits\nsimilarities between tasks, without any related information being known in\nadvance. First, for each data set, similar or neighboring means are determined\nfrom the data by multiple testing. Then each naive estimator is shrunk towards\nthe local average of its neighbors. We prove theoretically that this approach\nprovides a reduction in mean squared error. This improvement can be significant\nwhen the dimension of the input space is large, demonstrating a \"blessing of\ndimensionality\" phenomenon. An application of this approach is the estimation\nof multiple kernel mean embeddings, which plays an important role in many\nmodern applications. The theoretical results are verified on artificial and\nreal world data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 07:31:30 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Marienwald", "Hannah", "", "TUB"], ["Fermanian", "Jean-Baptiste", "", "ENS Rennes"], ["Blanchard", "Gilles", "", "DATASHAPE, LMO, CNRS"]]}, {"id": "2011.06848", "submitter": "Oleg Szehr", "authors": "Oleg Szehr, Dario Azzimonti, Laura Azzimonti", "title": "An exact kernel framework for spatio-temporal dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kernel-based framework for spatio-temporal data analysis is introduced that\napplies in situations when the underlying system dynamics are governed by a\ndynamic equation. The key ingredient is a representer theorem that involves\ntime-dependent kernels. Such kernels occur commonly in the expansion of\nsolutions of partial differential equations. The representer theorem is applied\nto find among all solutions of a dynamic equation the one that minimizes the\nerror with given spatio-temporal samples. This is motivated by the fact that\nvery often a differential equation is given a priori (e.g.~by the laws of\nphysics) and a practitioner seeks the best solution that is compatible with her\nnoisy measurements. Our guiding example is the Fokker-Planck equation, which\ndescribes the evolution of density in stochastic diffusion processes. A\nregression and density estimation framework is introduced for spatio-temporal\nmodeling under Fokker-Planck dynamics with initial and boundary conditions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 10:32:34 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Szehr", "Oleg", ""], ["Azzimonti", "Dario", ""], ["Azzimonti", "Laura", ""]]}, {"id": "2011.06887", "submitter": "Sakshi Arya", "authors": "Anuj Abhishek and Sakshi Arya", "title": "Adaptive estimation of a function from its Exponential Radon Transform\n  in presence of noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a locally adaptive strategy for estimating a\nfunction from its Exponential Radon Transform (ERT) data, without prior\nknowledge of the smoothness of functions that are to be estimated. We build a\nnon-parametric kernel type estimator and show that for a class of functions\ncomprising a wide Sobolev regularity scale, our proposed strategy follows the\nminimax optimal rate up to a $\\log{n}$ factor. We also show that there does not\nexist an optimal adaptive estimator on the Sobolev scale when the pointwise\nrisk is used and in fact the rate achieved by the proposed estimator is the\nadaptive rate of convergence.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:54:09 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Abhishek", "Anuj", ""], ["Arya", "Sakshi", ""]]}, {"id": "2011.06931", "submitter": "Peter Gr\\\"unwald", "authors": "J. ter Schure and M.F. Perez-Ortiz and A. Ly and P. Grunwald", "title": "The Safe Logrank Test: Error Control under Continuous Monitoring with\n  Unlimited Horizon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the safe logrank test, a version of the logrank test that\nprovides type-I error guarantees under optional stopping and optional\ncontinuation. The test is sequential without the need to specify a maximum\nsample size or stopping rule and allows for cumulative meta-analysis with\nType-I error control. The method can be extended to define anytime-valid\nconfidence intervals. All these properties are a virtue of the recently\ndeveloped martingale tests based on E-variables, of which the safe logrank test\nis an instance. We demonstrate the validity of the underlying nonnegative\nmartingale in a semiparametric setting of proportional hazards and show how to\nextend it to ties, Cox' regression and confidence sequences. Using a Gaussian\napproximation on the logrank statistic, we show that the safe logrank test\n(which itself is always exact) has a similar rejection region to\nO'Brien-Fleming alpha-spending but with the potential to achieve 100% power by\noptional continuation. Although our approach to study design requires a larger\nsample size, the expected sample size is competitive by optional stopping.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:23:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 21:42:44 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 12:28:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["ter Schure", "J.", ""], ["Perez-Ortiz", "M. F.", ""], ["Ly", "A.", ""], ["Grunwald", "P.", ""]]}, {"id": "2011.06955", "submitter": "Orimar Sauri", "authors": "Orimar Sauri and Toke C. Zinn", "title": "Estimating the Copula of a class of Time-Changed Brownian Motions: A\n  non-parametric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Within a high-frequency framework, we propose a non-parametric approach to\nestimate a family of copulas associated to a time-changed Brownian motion. We\nshow that our estimator is consistent and asymptotically mixed-Gaussian.\nFurthermore, we test its finite-sample accuracy via Monte Carlo.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:01:32 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Sauri", "Orimar", ""], ["Zinn", "Toke C.", ""]]}, {"id": "2011.06957", "submitter": "Pierre Gaillard", "authors": "Anant Raj, Pierre Gaillard (SIERRA, Thoth), Christophe Saad (CMU)", "title": "Non-stationary Online Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online forecasting under a changing environment has been a problem of\nincreasing importance in many real-world applications. In this paper, we\nconsider the meta-algorithm presented in \\citet{zhang2017dynamic} combined with\ndifferent subroutines. We show that an expected cumulative error of order\n$\\tilde{O}(n^{1/3} C_n^{2/3})$ can be obtained for non-stationary online linear\nregression where the total variation of parameter sequence is bounded by $C_n$.\nOur paper extends the result of online forecasting of one-dimensional\ntime-series as proposed in \\cite{baby2019online} to general $d$-dimensional\nnon-stationary linear regression. We improve the rate $O(\\sqrt{n C_n})$\nobtained by Zhang et al. 2017 and Besbes et al. 2015. We further extend our\nanalysis to non-stationary online kernel regression. Similar to the\nnon-stationary online regression case, we use the meta-procedure of Zhang et\nal. 2017 combined with Kernel-AWV (Jezequel et al. 2020) to achieve an expected\ncumulative controlled by the effective dimension of the RKHS and the total\nvariation of the sequence. To the best of our knowledge, this work is the first\nextension of non-stationary online regression to non-stationary kernel\nregression. Lastly, we evaluate our method empirically with several existing\nbenchmarks and also compare it with the theoretical bound obtained in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 15:08:49 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Raj", "Anant", "", "SIERRA, Thoth"], ["Gaillard", "Pierre", "", "SIERRA, Thoth"], ["Saad", "Christophe", "", "CMU"]]}, {"id": "2011.07275", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "Inference Functions for Semiparametric Models", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper discusses inference techniques for semiparametric models based on\nsuitable versions of inference functions. The text contains two parts. In the\nfirst part, we review the optimality theory for non-parametric models based on\nthe notions of path differentiability and statistical functional\ndifferentiability. Those notions are adapted to the context of semiparametric\nmodels by applying the inference theory of statistical functionals to the\nfunctional that associates the value of the interest parameter to the\ncorresponding probability measure. The second part of the paper discusses the\ntheory of inference functions for semiparametric models. We define a class of\nregular inference functions, and provide two equivalent characterisations of\nthose inference functions: One adapted from the classic theory of inference\nfunctions for parametric models, and one motivated by differential geometric\nconsiderations concerning the statistical model. Those characterisations yield\nan optimality theory for estimation under semiparametric models. We present a\nnecessary and sufficient condition for the coincidence of the bound for the\nconcentration of estimators based on inference functions and the semiparametric\nCram\\`er-Rao bound. Projecting the score function for the parameter of interest\non specially designed spaces of functions, we obtain optimal inference\nfunctions. Considering estimation when a sufficient statistic is present, we\nprovide an alternative justification for the conditioning principle in a\ncontext of semiparametric models. The article closes with a characterisation of\nwhen the semiparametric Cram\\`er-Rao bound is attained by estimators derived\nfrom regular inference functions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 11:43:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "2011.07439", "submitter": "Jincheng Bai", "authors": "Jincheng Bai, Qifan Song, Guang Cheng", "title": "Efficient Variational Inference for Sparse Deep Learning with\n  Theoretical Guarantee", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse deep learning aims to address the challenge of huge storage\nconsumption by deep neural networks, and to recover the sparse structure of\ntarget functions. Although tremendous empirical successes have been achieved,\nmost sparse deep learning algorithms are lacking of theoretical support. On the\nother hand, another line of works have proposed theoretical frameworks that are\ncomputationally infeasible. In this paper, we train sparse deep neural networks\nwith a fully Bayesian treatment under spike-and-slab priors, and develop a set\nof computationally efficient variational inferences via continuous relaxation\nof Bernoulli distribution. The variational posterior contraction rate is\nprovided, which justifies the consistency of the proposed variational Bayes\nmethod. Notably, our empirical results demonstrate that this variational\nprocedure provides uncertainty quantification in terms of Bayesian predictive\ndistribution and is also capable to accomplish consistent variable selection by\ntraining a sparse multi-layer neural network.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 03:27:54 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bai", "Jincheng", ""], ["Song", "Qifan", ""], ["Cheng", "Guang", ""]]}, {"id": "2011.07503", "submitter": "Alan Huang", "authors": "Alan Huang", "title": "On arbitrarily underdispersed Conway-Maxwell-Poisson distributions", "comments": "9 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Conway--Maxwell--Poisson distribution can be arbitrarily\nunderdispersed when parametrized via its mean. More precisely, if the mean\n$\\mu$ is an integer then the limiting distribution is a unit probability mass\nat $\\mu$. If the mean $\\mu$ is not an integer then the limiting distribution is\na shifted Bernoulli on the two values $\\floor{\\mu}$ and $\\ceil{\\mu}$ with\nprobabilities equal to the fractional parts of $\\mu$. In either case, the\nlimiting distribution is the most underdispersed discrete distribution possible\nfor any given mean. This is currently the only known generalization of the\nPoisson distribution exhibiting this property. Four practical implications are\ndiscussed, each adding to the claim that the (mean-parametrized)\nConway--Maxwell--Poisson distribution should be considered the default model\nfor underdispersed counts. We suggest that all future generalizations of the\nPoisson distribution be tested against this property.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 11:39:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Huang", "Alan", ""]]}, {"id": "2011.07559", "submitter": "Mehrdad Naderi Dr", "authors": "Mehrdad Naderi, Elham Mirfarah, Matthew Bernhardt and Ding-Geng Chen", "title": "Semiparametric inference for the scale-mixture of normal partial linear\n  regression model with censored data", "comments": "17 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of censored data modeling, the classical linear regression\nmodel that assumes normally distributed random errors has received increasing\nattention in recent years, mainly for mathematical and computational\nconvenience. However, practical studies have often criticized this linear\nregression model due to its sensitivity to departure from the normality and\nfrom the partial nonlinearity. This paper proposes to solve these potential\nissues simultaneously in the context of the partial linear regression model by\nassuming that the random errors follow a scale-mixture of normal (SMN) family\nof distributions. The proposed method allows us to model data with great\nflexibility, accommodating heavy tails, and outliers. By implementing the\nB-spline function and using the convenient hierarchical representation of the\nSMN distributions, a computationally analytical EM-type algorithm is developed\nto perform maximum likelihood inference of the model parameters. Various\nsimulation studies are conducted to investigate the finite sample properties as\nwell as the robustness of the model in dealing with the heavy-tails distributed\ndatasets. Real-word data examples are finally analyzed for illustrating the\nusefulness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:38:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Naderi", "Mehrdad", ""], ["Mirfarah", "Elham", ""], ["Bernhardt", "Matthew", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2011.07568", "submitter": "Zijian Guo", "authors": "Zijian Guo", "title": "Inference for High-dimensional Maximin Effects in Heterogeneous\n  Regression Models Using a Sampling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is an important feature of modern data sets and a central task\nis to extract information from large-scale and heterogeneous data. In this\npaper, we consider multiple high-dimensional linear models and adopt the\ndefinition of maximin effect (Meinshausen, B{\\\"u}hlmann, AoS, 43(4),\n1801--1830) to summarize the information contained in this heterogeneous model.\nWe define the maximin effect for a targeted population whose covariate\ndistribution is possibly different from that of the observed data. We further\nintroduce a ridge-type maximin effect to simultaneously account for reward\noptimality and statistical stability. To identify the high-dimensional maximin\neffect, we estimate the regression covariance matrix by a debiased estimator\nand use it to construct the aggregation weights for the maximin effect. A main\nchallenge for statistical inference is that the estimated weights might have a\nmixture distribution and the resulted maximin effect estimator is not\nnecessarily asymptotic normal. To address this, we devise a novel sampling\napproach to construct the confidence interval for any linear contrast of\nhigh-dimensional maximin effects. The coverage and precision properties of the\nproposed confidence interval are studied. The proposed method is demonstrated\nover simulations and a genetic data set on yeast colony growth under different\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:15:10 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 10:50:51 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Guo", "Zijian", ""]]}, {"id": "2011.07669", "submitter": "He Lyu", "authors": "He Lyu and Rongrong Wang", "title": "An exact sin$\\Theta$ formula for matrix perturbation analysis and its\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a useful set of formulae for the $\\sin\\Theta$\ndistance between the original and the perturbed singular subspaces. These\nformulae explicitly show that how the perturbation of the original matrix\npropagates into singular vectors and singular subspaces, thus providing a\ndirect way of analyzing them. Following this, we derive a collection of new\nresults on SVD perturbation related problems, including a tighter bound on the\n$\\ell_{2,\\infty}$ norm of the singular vector perturbation errors under\nGaussian noise, a new stability analysis of the Principal Component Analysis\nand an error bound on the singular value thresholding operator. For the latter\ntwo, we consider the most general rectangular matrices with full matrix rank.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 00:53:42 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 03:32:02 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 17:02:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lyu", "He", ""], ["Wang", "Rongrong", ""]]}, {"id": "2011.07729", "submitter": "Christos Thrampoulidis", "authors": "Christos Thrampoulidis, Samet Oymak, Mahdi Soltanolkotabi", "title": "Theoretical Insights Into Multiclass Classification: A High-dimensional\n  Asymptotic View", "comments": "To Appear at NeurIPS 2020. 62 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Contemporary machine learning applications often involve classification tasks\nwith many classes. Despite their extensive use, a precise understanding of the\nstatistical properties and behavior of classification algorithms is still\nmissing, especially in modern regimes where the number of classes is rather\nlarge. In this paper, we take a step in this direction by providing the first\nasymptotically precise analysis of linear multiclass classification. Our\ntheoretical analysis allows us to precisely characterize how the test error\nvaries over different training algorithms, data distributions, problem\ndimensions as well as number of classes, inter/intra class correlations and\nclass priors. Specifically, our analysis reveals that the classification\naccuracy is highly distribution-dependent with different algorithms achieving\noptimal performance for different data distributions and/or training/features\nsizes. Unlike linear regression/binary classification, the test error in\nmulticlass classification relies on intricate functions of the trained model\n(e.g., correlation between some of the trained weights) whose asymptotic\nbehavior is difficult to characterize. This challenge is already present in\nsimple classifiers, such as those minimizing a square loss. Our novel\ntheoretical techniques allow us to overcome some of these challenges. The\ninsights gained may pave the way for a precise understanding of other\nclassification algorithms beyond those studied in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:17:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Thrampoulidis", "Christos", ""], ["Oymak", "Samet", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2011.07818", "submitter": "Emmanuel Pilliat", "authors": "Emmanuel Pilliat (IMAG), Alexandra Carpentier (OVGU), Nicolas Verzelen\n  (INRAE)", "title": "Optimal multiple change-point detection for high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript makes two contributions to the field of change-point\ndetection. In a general change-point setting, we provide a generic algorithm\nfor aggregating local homogeneity tests into an estimator of change-points in a\ntime series. Interestingly, we establish that the error rates of the collection\nof test directly translate into detection properties of the change-point\nestimator. This generic scheme is then applied to the problem of possibly\nsparse multivariate mean change-point detection setting. When the noise is\nGaussian, we derive minimax optimal rates that are adaptive to the unknown\nsparsity and to the distance between change-points. For sub-Gaussian noise, we\nintroduce a variant that is optimal in almost all sparsity regimes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:45:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Pilliat", "Emmanuel", "", "IMAG"], ["Carpentier", "Alexandra", "", "OVGU"], ["Verzelen", "Nicolas", "", "INRAE"]]}, {"id": "2011.08290", "submitter": "Gesine Reinert", "authors": "A. D. Barbour and Gesine Reinert", "title": "Estimating the correlation in network disturbance models", "comments": "20 pages, 1 Figure; updated version with more details", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Network Disturbance Model of Doreian (1989) expresses the dependency\nbetween observations taken at the vertices of a network by modelling the\ncorrelation between neighbouring vertices, using a single correlation parameter\n$\\rho$. It has been observed that estimation of $\\rho$ in dense graphs, using\nthe method of Maximum Likelihood, leads to results that can be both biased and\nvery unstable. In this paper, we sketch why this is the case, showing that the\nvariability cannot be avoided, no matter how large the network. We also propose\na more intuitive estimator of $\\rho$, which shows little bias. The related\nNetwork Effects Model is briefly discussed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:29:14 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 08:03:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Barbour", "A. D.", ""], ["Reinert", "Gesine", ""]]}, {"id": "2011.08321", "submitter": "Moritz Schauer", "authors": "Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian volatility estimation for gamma-driven stochastic\n  differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a nonparametric Bayesian approach to estimation of the volatility\nfunction of a stochastic differential equation driven by a gamma process. The\nvolatility function is modelled a priori as piecewise constant, and we specify\na gamma prior on its values. This leads to a straightforward procedure for\nposterior inference via an MCMC procedure. We give theoretical performance\nguarantees (contraction rates for the posterior) for the Bayesian estimate in\nterms of the regularity of the unknown volatility function. We illustrate the\nmethod on synthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:47:42 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Belomestny", "Denis", ""], ["Gugushvili", "Shota", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "2011.08384", "submitter": "Jasper C.H. Lee", "authors": "Jasper C.H. Lee, Paul Valiant", "title": "Optimal Sub-Gaussian Mean Estimation in $\\mathbb{R}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of estimating the mean of a real-valued distribution,\npresenting a novel estimator with sub-Gaussian convergence: intuitively, \"our\nestimator, on any distribution, is as accurate as the sample mean is for the\nGaussian distribution of matching variance.\" Crucially, in contrast to prior\nworks, our estimator does not require prior knowledge of the variance, and\nworks across the entire gamut of distributions with bounded variance, including\nthose without any higher moments. Parameterized by the sample size $n$, the\nfailure probability $\\delta$, and the variance $\\sigma^2$, our estimator is\naccurate to within $\\sigma\\cdot(1+o(1))\\sqrt{\\frac{2\\log\\frac{1}{\\delta}}{n}}$,\ntight up to the $1+o(1)$ factor. Our estimator construction and analysis gives\na framework generalizable to other problems, tightly analyzing a sum of\ndependent random variables by viewing the sum implicitly as a 2-parameter\n$\\psi$-estimator, and constructing bounds using mathematical programming and\nduality techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:47:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Valiant", "Paul", ""]]}, {"id": "2011.08411", "submitter": "Yifan Cui", "authors": "Yifan Cui, Hongming Pu, Xu Shi, Wang Miao, Eric Tchetgen Tchetgen", "title": "Semiparametric proximal causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skepticism about the assumption of no unmeasured confounding, also known as\nexchangeability, is often warranted in making causal inferences from\nobservational data; because exchangeability hinges on an investigator's ability\nto accurately measure covariates that capture all potential sources of\nconfounding. In practice, the most one can hope for is that covariate\nmeasurements are at best proxies of the true underlying confounding mechanism\noperating in a given observational study. In this paper, we consider the\nframework of proximal causal inference introduced by Tchetgen Tchetgen et al.\n(2020), which while explicitly acknowledging covariate measurements as\nimperfect proxies of confounding mechanisms, offers an opportunity to learn\nabout causal effects in settings where exchangeability on the basis of measured\ncovariates fails. We make a number of contributions to proximal inference\nincluding (i) an alternative set of conditions for nonparametric proximal\nidentification of the average treatment effect; (ii) general semiparametric\ntheory for proximal estimation of the average treatment effect including\nefficiency bounds for key semiparametric models of interest; (iii) a\ncharacterization of proximal doubly robust and locally efficient estimators of\nthe average treatment effect. Moreover, we provide analogous identification and\nefficiency results for the average treatment effect on the treated. Our\napproach is illustrated via simulation studies and a data application on\nevaluating the effectiveness of right heart catheterization in the intensive\ncare unit of critically ill patients.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:07:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Cui", "Yifan", ""], ["Pu", "Hongming", ""], ["Shi", "Xu", ""], ["Miao", "Wang", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2011.08566", "submitter": "Jean Bernard Lasserre", "authors": "Jean-Bernard Lasserre (LAAS-MAC)", "title": "The Moment-SOS hierarchy and the Christoffel-Darboux kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the global minimization of a polynomial on a compact set B. We\nshow that each step of the Moment-SOS hierarchy has a nice and simple\ninterpretation that complements the usual one. Namely, it computes coefficients\nof a polynomial in an orthonormal basis of L 2 (B, $\\mu$) where $\\mu$ is an\narbitrary reference measure whose support is exactly B. The resulting\npolynomial is a certain density (with respect to $\\mu$) of some signed measure\non B. When some relaxation is exact (which generically takes place) the\ncoefficients of the optimal polynomial density are values of orthonormal\npolynomials at the global minimizer and the optimal (signed) density is simply\nrelated to the Christoffel-Darboux (CD) kernel and the Christoffel function\nassociated with $\\mu$. In contrast to the hierarchy of upper bounds which\ncomputes positive densities, the global optimum can be achieved exactly as\nintegration against a polynomial (signed) density because the CD-kernel is a\nreproducing kernel, and so can mimic a Dirac measure (as long as finitely many\nmoments are concerned).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 11:25:38 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lasserre", "Jean-Bernard", "", "LAAS-MAC"]]}, {"id": "2011.08661", "submitter": "Yuhao Wang", "authors": "Yuhao Wang and Rajen D. Shah", "title": "Debiased Inverse Propensity Score Weighting for Estimation of Average\n  Treatment Effects with High-Dimensional Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of average treatment effects given observational data\nwith high-dimensional pretreatment variables. Existing methods for this problem\ntypically assume some form of sparsity for the regression functions. In this\nwork, we introduce a debiased inverse propensity score weighting (DIPW) scheme\nfor average treatment effect estimation that delivers $\\sqrt{n}$-consistent\nestimates of the average treatment effect when the propensity score follows a\nsparse logistic regression model; the regression functions are permitted to be\narbitrarily complex. Our theoretical results quantify the price to pay for\npermitting the regression functions to be unestimable, which shows up as an\ninflation of the variance of the estimator compared to the semiparametric\nefficient variance by at most O(1) under mild conditions. Given the lack of\nassumptions on the regression functions, averages of transformed responses\nunder each treatment may also be estimated at the $\\sqrt{n}$ rate, and so for\nexample, the variances of the potential outcomes may be estimated. We show how\nconfidence intervals centred on our estimates may be constructed, and also\ndiscuss an extension of the method to estimating projections of the\nheterogeneous treatment effect function.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:42:17 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Yuhao", ""], ["Shah", "Rajen D.", ""]]}, {"id": "2011.08791", "submitter": "Tim Seynnaeve", "authors": "Laurent Manivel, Mateusz Micha{\\l}ek, Leonid Monin, Tim Seynnaeve,\n  Martin Vodi\\v{c}ka", "title": "Complete quadrics: Schubert calculus for Gaussian models and\n  semidefinite programming", "comments": "35 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.CO math.RT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish connections between: the maximum likelihood degree (ML-degree)\nfor linear concentration models, the algebraic degree of semidefinite\nprogramming (SDP), and Schubert calculus for complete quadrics. We prove a\nconjecture by Sturmfels and Uhler on the polynomiality of the ML-degree. We\nalso prove a conjecture by Nie, Ranestad and Sturmfels providing an explicit\nformula for the degree of SDP. The interactions between the three fields shed\nnew light on the asymptotic behaviour of enumerative invariants for the variety\nof complete quadrics. We also extend these results to spaces of general\nmatrices and of skew-symmetric matrices.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:36:52 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 11:16:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Manivel", "Laurent", ""], ["Micha\u0142ek", "Mateusz", ""], ["Monin", "Leonid", ""], ["Seynnaeve", "Tim", ""], ["Vodi\u010dka", "Martin", ""]]}, {"id": "2011.08989", "submitter": "Felix Rydell", "authors": "Stefan Dye, Kathl\\'en Kohn, Felix Rydell and Rainer Sinn", "title": "Maximum Likelihood Estimation for Nets of Conics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximum likelihood estimation for $3$-dimensional\nlinear spaces of $3\\times 3$ symmetric matrices from the point of view of\nalgebraic statistics where we view these nets of conics as linear concentration\nor linear covariance models of Gaussian distributions on $\\mathbb{R}^3$. In\nparticular, we study the reciprocal surfaces of nets of conics which are\nrational surfaces in $\\mathbb{P}^5$. We show that the reciprocal surfaces are\nprojections from the Veronese surface and determine their intersection with the\npolar nets. This geometry explains the maximum likelihood degrees of these\nlinear models. We compute the reciprocal maximum likelihood degrees. This work\nis based on Wall's classification of nets of conics from 1977.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 22:37:27 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 07:10:05 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 09:40:12 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Dye", "Stefan", ""], ["Kohn", "Kathl\u00e9n", ""], ["Rydell", "Felix", ""], ["Sinn", "Rainer", ""]]}, {"id": "2011.09053", "submitter": "Takaaki Koike", "authors": "Takaaki Koike and Marius Hofert", "title": "Matrix compatibility and correlation mixture representation of\n  generalized Gini's gamma", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations of measures of concordance in terms of Pearson's correlation\ncoefficient are studied. We first characterize the transforms such that the\ncorrelation coefficient between the transformed random variables is a measure\nof concordance. This characterization improves upon that of Hofert and Koike\n(2019) and covers non-continuous transforms which lead to, for example,\nBlomqvist's beta. We then generalize Gini's gamma, and show that the\ngeneralized Gini's gamma can be represented by a mixture of measures of\nconcordance written as the Pearson's correlation coefficients between\ntransformed random variables. As an application of the correlation mixture\nrepresentation, we derive lower and upper bounds of the compatible set of\ngeneralized Gini's gamma, that is, the collection of all possible square\nmatrices whose entries are pairwise bivariate generalized Gini's gammas.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:52:42 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 12:11:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Koike", "Takaaki", ""], ["Hofert", "Marius", ""]]}, {"id": "2011.09148", "submitter": "Ke Wang", "authors": "Ke Wang, Christos Thrampoulidis", "title": "Binary Classification of Gaussian Mixtures: Abundance of Support\n  Vectors, Benign Overfitting and Regularization", "comments": "New results: Extensions to correlated features and\n  ridge-regularization; Additional numerical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep neural networks generalize well despite being exceedingly\noverparameterized and being trained without explicit regularization. This\ncurious phenomenon, often termed benign overfitting, has inspired extensive\nresearch activity in establishing its statistical principles: Under what\nconditions is the phenomenon observed? How do these depend on the data and on\nthe training algorithm? When does regularization benefit generalization? While\nthese questions remain wide open for deep neural nets, recent works have\nattempted gaining insights by studying simpler, often linear, models. Our paper\ncontributes to this growing line of work by examining binary linear\nclassification under the popular generative Gaussian mixture model. Motivated\nby recent results on the implicit bias of gradient descent, we study both\nmax-margin SVM classifiers (corresponding to logistic loss) and min-norm\ninterpolating classifiers (corresponding to least-squares loss). First, we\nleverage an idea introduced in [V. Muthukumar et al., arXiv:2005.08054, (2020)]\nto relate the SVM solution to the least-squares (LS) interpolating solution.\nSecond, we derive novel non-asymptotic bounds on the classification error of\nthe LS solution. Combining the two, we present novel sufficient conditions on\nthe overparameterization ratio and on the signal-to-noise ratio (SNR) for\nbenign overfitting to occur. Contrary to previously studied discriminative data\nmodels, our results emphasize the crucial role of the SNR. Moreover, we\ninvestigate the role of regularization and identify precise conditions under\nwhich the interpolating estimator performs better than the regularized\nestimates. We corroborate our theoretical findings with numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 07:59:55 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 04:35:42 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 01:07:30 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wang", "Ke", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2011.09211", "submitter": "Brijesh Singh", "authors": "Brijesh P. Singh, Utpal Dhar Das and Sandeep Singh", "title": "A Compounded Probability Model for Decreasing Hazard and its Inferential\n  Properties", "comments": "16 pages, 12 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are some real life issues that are exists in nature which has early\nfailure. This type of problems can be modelled either by a complex distribution\nhaving more than one parameter or by finite mixture of some distribution. In\nthis article a single parameter continuous distribution is introduced to model\nsuch type of problems. The base line distribution is exponential and it is\ncompounded by lindley distribution. Some important properties of the proposed\ndistribution such as distribution function, survival function, hazard function\nand cumulative hazard function are derived. The maximum likelihood estimate of\nthe parameter is obtained which is not in closed form, thus iteration procedure\nis used to obtain the estimate of parameter. The moments of the proposed\ndistribution does not exist thus median and mode is obtained. The distribution\nis positively skewed and the hazard rate of this distribution is decreasing.\nSome real data sets are used to see the performance of proposed distribution\nwith comparison of some other competent distributions of decreasing hazard\nusing Likelihood, AIC, AICc, BIC and KS statistics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:59:33 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Singh", "Brijesh P.", ""], ["Das", "Utpal Dhar", ""], ["Singh", "Sandeep", ""]]}, {"id": "2011.09462", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Michael I. Jordan", "title": "Post-Selection Inference via Algorithmic Stability", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern approaches to data analysis make extensive use of data-driven model\nselection. The resulting dependencies between the selected model and data used\nfor inference invalidate statistical guarantees derived from classical\ntheories. The framework of post-selection inference (PoSI) has formalized this\nproblem and proposed corrections which ensure valid inferences. Yet, obtaining\ngeneral principles that enable computationally-efficient, powerful PoSI\nmethodology with formal guarantees remains a challenge. With this goal in mind,\nwe revisit the PoSI problem through the lens of algorithmic stability. Under an\nappropriate formulation of stability---one that captures closure under\npost-processing and compositionality properties---we show that stability\nparameters of a selection method alone suffice to provide non-trivial\ncorrections to classical z-test and t-test intervals. Then, for several popular\nmodel selection methods, including the LASSO, we show how stability can be\nachieved through simple, computationally efficient randomization schemes. Our\nalgorithms offer provable unconditional simultaneous coverage and are\ncomputationally efficient; in particular, they do not rely on MCMC sampling.\nImportantly, our proposal explicitly relates the magnitude of randomization to\nthe resulting confidence interval width, allowing the analyst to tune interval\nwidth to the loss in utility due to randomizing selection.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:40:25 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zrnic", "Tijana", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.09592", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya, Zihuan Liu, Tapabrata Maiti", "title": "Variational Bayes Neural Network: Posterior Consistency, Classification\n  Accuracy and Computational Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Bayesian neural network models (BNN) have re-surged in recent years due to\nthe advancement of scalable computations and its utility in solving complex\nprediction problems in a wide variety of applications. Despite the popularity\nand usefulness of BNN, the conventional Markov Chain Monte Carlo based\nimplementation suffers from high computational cost, limiting the use of this\npowerful technique in large scale studies. The variational Bayes inference has\nbecome a viable alternative to circumvent some of the computational issues.\nAlthough the approach is popular in machine learning, its application in\nstatistics is somewhat limited. This paper develops a variational Bayesian\nneural network estimation methodology and related statistical theory. The\nnumerical algorithms and their implementational are discussed in detail. The\ntheory for posterior consistency, a desirable property in nonparametric\nBayesian statistics, is also developed. This theory provides an assessment of\nprediction accuracy and guidelines for characterizing the prior distributions\nand variational family. The loss of using a variational posterior over the true\nposterior has also been quantified. The development is motivated by an\nimportant biomedical engineering application, namely building predictive tools\nfor the transition from mild cognitive impairment to Alzheimer's disease. The\npredictors are multi-modal and may involve complex interactive relations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 00:11:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Liu", "Zihuan", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2011.09706", "submitter": "Antoine Godichon-Baggioni", "authors": "Claire Boyer (LPSM (UMR\\_8001)), Antoine Godichon-Baggioni (LPSM\n  (UMR\\_8001))", "title": "On the asymptotic rate of convergence of Stochastic Newton algorithms\n  and their Weighted Averaged versions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of machine learning methods can be regarded as the minimization\nof an unavailable risk function. To optimize the latter, given samples provided\nin a streaming fashion, we define a general stochastic Newton algorithm and its\nweighted average version. In several use cases, both implementations will be\nshown not to require the inversion of a Hessian estimate at each iteration, but\na direct update of the estimate of the inverse Hessian instead will be favored.\nThis generalizes a trick introduced in [2] for the specific case of logistic\nregression, by directly updating the estimate of the inverse Hessian. Under\nmild assumptions such as local strong convexity at the optimum, we establish\nalmost sure convergences and rates of convergence of the algorithms, as well as\ncentral limit theorems for the constructed parameter estimates. The unified\nframework considered in this paper covers the case of linear, logistic or\nsoftmax regressions to name a few. Numerical experiments on simulated data give\nthe empirical evidence of the pertinence of the proposed methods, which\noutperform popular competitors particularly in case of bad initializa-tions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 08:17:14 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 15:13:28 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Boyer", "Claire", "", "LPSM"], ["Godichon-Baggioni", "Antoine", "", "LPSM"]]}, {"id": "2011.09734", "submitter": "Hanzhong Liu", "authors": "Hanzhong Liu, Fuyi Tu, Wei Ma", "title": "A general theory of regression adjustment for covariate-adaptive\n  randomization: OLS, Lasso, and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating and inferring treatment effects in\nrandomized experiments. In practice, stratified randomization, or more\ngenerally, covariate-adaptive randomization, is routinely used in the design\nstage to balance the treatment allocations with respect to a few variables that\nare most relevant to the outcomes. Then, regression is performed in the\nanalysis stage to adjust the remaining imbalances to yield more efficient\ntreatment effect estimators. Building upon and unifying the recent results\nobtained for ordinary least squares adjusted estimators under\ncovariate-adaptive randomization, this paper presents a general theory of\nregression adjustment that allows for arbitrary model misspecification and the\npresence of a large number of baseline covariates. We exemplify the theory on\ntwo Lasso-adjusted treatment effect estimators, both of which are optimal in\ntheir respective classes. In addition, nonparametric consistent variance\nestimators are proposed to facilitate valid inferences, which work irrespective\nof the specific randomization methods used. The robustness and improved\nefficiency of the proposed estimators are demonstrated through a simulation\nstudy and a clinical trial example. This study sheds light on improving\ntreatment effect estimation efficiency by implementing machine learning methods\nin covariate-adaptive randomized experiments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:19:56 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Hanzhong", ""], ["Tu", "Fuyi", ""], ["Ma", "Wei", ""]]}, {"id": "2011.09745", "submitter": "Osama Idais", "authors": "Osama Idais and Rainer Schwabe", "title": "In- and Equivariance for Optimal Designs in Generalized Linear Models:\n  The Gamma Model", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an overview over the usefulness of the concept of equivariance and\ninvariance in the design of experiments for generalized linear models. In\ncontrast to linear models here pairs of transformations have to be considered\nwhich act simultaneously on the experimental settings and on the location\nparameters in the linear component. Given the transformation of the\nexperimental settings the parameter transformations are not unique and may be\nnonlinear to make further use of the model structure. The general concepts and\nresults are illustrated by models with gamma distributed response. Locally\noptimal and maximin efficient design are obtained for the common D- and\nIMSE-criterion.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:48:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Idais", "Osama", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2011.09829", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang and Qihua Wang and Wang Miao and Xiaohua Zhou", "title": "Sharp bounds for variance of treatment effect estimators in the finite\n  population in the presence of covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the completely randomized experiment, the variances of treatment effect\nestimators in the finite population are usually not identifiable and hence not\nestimable. Although some estimable bounds of the variances have been\nestablished in the literature, few of them are derived in the presence of\ncovariates. In this paper, the difference-in-means estimator and the Wald\nestimator are considered in the completely randomized experiment with perfect\ncompliance and noncompliance, respectively. Sharp bounds for the variances of\nthese two estimators are established when covariates are available.\nFurthermore, consistent estimators for such bounds are obtained, which can be\nused to shorten the confidence intervals and improve power of tests.\nSimulations were conducted to evaluate the proposed methods. The proposed\nmethods are also illustrated with two real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:08:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wang", "Ruoyu", ""], ["Wang", "Qihua", ""], ["Miao", "Wang", ""], ["Zhou", "Xiaohua", ""]]}, {"id": "2011.09841", "submitter": "Chen Lu", "authors": "Chen Lu, Subhabrata Sen", "title": "Contextual Stochastic Block Model: Sharp Thresholds and Contiguity", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study community detection in the contextual stochastic block model\narXiv:1807.09596 [cs.SI], arXiv:1607.02675 [stat.ME]. In arXiv:1807.09596\n[cs.SI], the second author studied this problem in the setting of sparse graphs\nwith high-dimensional node-covariates. Using the non-rigorous cavity method\nfrom statistical physics, they conjectured the sharp limits for community\ndetection in this setting. Further, the information theoretic threshold was\nverified, assuming that the average degree of the observed graph is large. It\nis expected that the conjecture holds as soon as the average degree exceeds\none, so that the graph has a giant component. We establish this conjecture, and\ncharacterize the sharp threshold for detection and weak recovery.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:14:14 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lu", "Chen", ""], ["Sen", "Subhabrata", ""]]}, {"id": "2011.10138", "submitter": "Junxi Zhang", "authors": "Yaozhong Hu and Junxi Zhang", "title": "Functional central limit theorems for stick-breaking priors", "comments": "55 pages. Proofs are given in Section Supplemental Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the empirical strong law of large numbers, empirical\nGlivenko-Cantelli theorem, central limit theorem, functional central limit\ntheorem for various nonparametric Bayesian priors which include the Dirichlet\nprocess with general stick-breaking weights, the Poisson-Dirichlet process, the\nnormalized inverse Gaussian process, the normalized generalized gamma process,\nand the generalized Dirichlet process. For the Dirichlet process with general\nstick-breaking weights, we introduce two general conditions such that the\ncentral limit theorem and functional central limit theorem hold. Except in the\ncase of the generalized Dirichlet process, since the finite dimensional\ndistributions of these processes are either hard to obtain or are complicated\nto use even they are available, we use the method of moments to obtain the\nconvergence results. For the generalized Dirichlet process we use its finite\ndimensional marginal distributions to obtain the asymptotics although the\ncomputations are highly technical.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 22:49:14 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hu", "Yaozhong", ""], ["Zhang", "Junxi", ""]]}, {"id": "2011.10423", "submitter": "Jad Beyhum", "authors": "Jad Beyhum (KU Leuven), Jean-Pierre FLorens (Toulouse School of\n  Economics), Ingrid Van Keilegom (KU Leuven)", "title": "Nonparametric instrumental regression with right censored duration\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the effect of a discrete treatment Z on a duration T. The\ntreatment is not randomly assigned. The confounding issue is treated using a\ndiscrete instrumental variable explaining the treatment and independent of the\nerror term of the model. Our framework is nonparametric and allows for random\nright censoring. This specification generates a nonlinear inverse problem and\nthe average treatment effect is derived from its solution. We provide local and\nglobal identification properties that rely on a nonlinear system of equations.\nWe propose an estimation procedure to solve this system and derive rates of\nconvergence and conditions under which the estimator is asymptotically normal.\nWhen censoring makes identification fail, we develop partial identification\nresults. Our estimators exhibit good finite sample properties in simulations.\nWe also apply our methodology to the Illinois Reemployment Bonus Experiment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:40:42 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Beyhum", "Jad", "", "KU Leuven"], ["FLorens", "Jean-Pierre", "", "Toulouse School of\n  Economics"], ["Van Keilegom", "Ingrid", "", "KU Leuven"]]}, {"id": "2011.10576", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente and Nadia Kudraszow", "title": "Robust smoothed canonical correlation analysis for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides robust estimators for the first canonical correlation and\ndirections of random elements on Hilbert separable spaces by using robust\nassociation and scale measures combined with basis expansion and/or\npenalizations as a regularization tool. Under regularity conditions, the\nresulting estimators are consistent.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:36:53 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Boente", "Graciela", ""], ["Kudraszow", "Nadia", ""]]}, {"id": "2011.10725", "submitter": "Hau-tieng Wu", "authors": "Xiucai Ding and Hau-Tieng Wu", "title": "Phase transition of graph Laplacian of high dimensional noisy random\n  point cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically explore the spectral distribution of kernel-based graph\nLaplacian constructed from high dimensional and noisy random point cloud in the\nnonnull setup. An interesting phase transition phenomenon is reported, which is\ncharacterized by the signal-to-noise ratio (SNR). We quantify how the signal\nand noise interact over different SNR regimes; for example, how signal\ninformation pops out the Marchenko-Pastur bulk. Motivated by the analysis, an\nadaptive bandwidth selection algorithm is provided and proved, which coincides\nwith the common practice in real data. Simulated data is provided to support\nthe theoretical findings. Our results paves the way towards a foundation for\nstatistical inference of various kernel-based unsupervised learning algorithms,\nlike eigenmap, diffusion map and their variations, for real data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 05:19:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ding", "Xiucai", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2011.10793", "submitter": "Yuejuan Xi", "authors": "Yaozhong Hu and Yuejuan Xi", "title": "Parameter estimation for threshold Ornstein-Uhlenbeck processes from\n  discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming that a threshold Ornstein-Uhlenbeck process is observed at discrete\ntime instants, we propose generalized moment estimators to estimate the\nparameters. Our theoretical basis is the celebrated ergodic theorem. To use\nthis theorem we need to find the explicit form of the invariant measure. With\nthe sampling time step arbitrarily fixed, we prove the strong consistency and\nasymptotic normality of our estimators as the sample size tends to infinity.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 14:04:37 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hu", "Yaozhong", ""], ["Xi", "Yuejuan", ""]]}, {"id": "2011.10810", "submitter": "Anatoly Khina", "authors": "Anatoly Khina, Arie Yeredor, Ram Zamir", "title": "Monotonicity of the Trace-Inverse of Covariance Submatrices and\n  Two-Sided Prediction", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.SY eess.SY math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common to assess the \"memory strength\" of a stationary process looking\nat how fast the normalized log-determinant of its covariance submatrices (i.e.,\nentropy rate) decreases. In this work, we propose an alternative\ncharacterization in terms of the normalized trace-inverse of the covariance\nsubmatrices. We show that this sequence is monotonically non-decreasing and is\nconstant if and only if the process is white. Furthermore, while the entropy\nrate is associated with one-sided prediction errors (present from past), the\nnew measure is associated with two-sided prediction errors (present from past\nand future). This measure can be used as an alternative to Burg's\nmaximum-entropy principle for spectral estimation. We also propose a\ncounterpart for non-stationary processes, by looking at the average\ntrace-inverse of subsets.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 15:24:53 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Khina", "Anatoly", ""], ["Yeredor", "Arie", ""], ["Zamir", "Ram", ""]]}, {"id": "2011.10880", "submitter": "Amine Amimour", "authors": "Amine Amimour, Karima Belaide, Ouagnina Hili", "title": "Minimum Hellinger distance estimates for a periodically time-varying\n  long memory parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a purely fractionally deferenced process driven by a periodically\ntime-varying long memory parameter. We will build an estimate for the vector\nparameters using the minimum Hellinger distance estimation. The results are\ninvestigated through simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 21:57:36 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Amimour", "Amine", ""], ["Belaide", "Karima", ""], ["Hili", "Ouagnina", ""]]}, {"id": "2011.10965", "submitter": "Brendan Beare", "authors": "Brendan K. Beare", "title": "Least favorability of the uniform distribution for tests of the\n  concavity of a distribution function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A test of the concavity of a distribution function with support contained in\nthe unit interval may be based on a statistic constructed from the $L^p$-norm\nof the difference between an empirical distribution function and its least\nconcave majorant. It is shown here that the uniform distribution is least\nfavorable for such a test, in the sense that the limiting distribution of the\nstatistic obtained under uniformity stochastically dominates the limiting\ndistribution obtained under any other concave distribution function.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 08:26:11 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 10:19:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Beare", "Brendan K.", ""]]}, {"id": "2011.11084", "submitter": "Federico Martellosio", "authors": "Federico Martellosio", "title": "Non-Identifiability in Network Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study identification in autoregressions defined on a general network. Most\nidentification conditions that are available for these models either rely on\nrepeated observations, are only sufficient, or require strong distributional\nassumptions. We derive conditions that apply even if only one observation of a\nnetwork is available, are necessary and sufficient for identification, and\nrequire weak distributional assumptions. We find that the models are\ngenerically identified even without repeated observations, and analyze the\ncombinations of the interaction matrix and the regressor matrix for which\nidentification fails. This is done both in the original model and after certain\ntransformations in the sample space, the latter case being important for some\nfixed effects specifications.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:38:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Martellosio", "Federico", ""]]}, {"id": "2011.11140", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Fabrice Gamboa, Leonardo\n  Moreno", "title": "Weighted lens depth: Some applications to supervised classification", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with Tukey's pioneering work in the 1970's, the notion of depth in\nstatistics has been widely extended especially in the last decade. These\nextensions include high dimensional data, functional data, and manifold-valued\ndata. In particular, in the learning paradigm, the depth-depth method has\nbecome a useful technique. In this paper we extend the notion of lens depth to\nthe case of data in metric spaces, and prove its main properties, with\nparticular emphasis on the case of Riemannian manifolds, where we extend the\nconcept of lens depth in such a way that it takes into account non-convex\nstructures on the data distribution. Next we illustrate our results with some\nsimulation results and also in some interesting real datasets, including\npattern recognition in phylogenetic trees using the depth--depth approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 23:42:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Gamboa", "Fabrice", ""], ["Moreno", "Leonardo", ""]]}, {"id": "2011.11146", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Leonardo Moreno", "title": "Level sets of depth measures in abstract spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lens depth of a point has been recently extended to general metric\nspaces, which is not the case for most depths. It is defined as the probability\nof being included in the intersection of two random balls centred at two random\npoints X and Y, with the same radius d(X, Y). We study the consistency in\nHausdorff and measure distance, of the level sets of the empirical lens depth,\nbased on an iid sample on a general metric space. We also prove that the\nboundary of the empirical level sets are consistent estimators of their\npopulation counterparts, and analyze two real-life examples\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 00:17:18 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 00:52:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Moreno", "Leonardo", ""]]}, {"id": "2011.11219", "submitter": "Naomichi Nakajima", "authors": "Naomichi Nakajima, Toru Ohmoto", "title": "The dually flat structure for singular models", "comments": "29pages, 5figures", "journal-ref": "Information Geometry (2021) online available", "doi": "10.1007/s41884-021-00044-8", "report-no": null, "categories": "math.DG math-ph math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dually flat structure introduced by Amari-Nagaoka is highlighted in\ninformation geometry and related fields. In practical applications, however,\nthe underlying pseudo-Riemannian metric may often be degenerate, and such an\nexcellent geometric structure is rarely defined on the entire space. To fix\nthis trouble, in the present paper, we propose a novel generalization of the\ndually flat structure for a certain class of singular models from the viewpoint\nof Lagrange and Legendre singularity theory - we introduce a quasi-Hessian\nmanifold endowed with a possibly degenerate metric and a particular symmetric\ncubic tensor, which exceeds the concept of statistical manifolds and is adapted\nto the theory of (weak) contrast functions. In particular, we establish\nAmari-Nagaoka's extended Pythagorean theorem and projection theorem in this\ngeneral setup, and consequently, most of applications of these theorems are\nsuitably justified even for such singular cases. This work is motivated by\nvarious interests with different backgrounds from Frobenius structure in\nmathematical physics to Deep Learning in data science.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 05:36:48 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 08:24:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Nakajima", "Naomichi", ""], ["Ohmoto", "Toru", ""]]}, {"id": "2011.11248", "submitter": "Morgane Austern", "authors": "Morgane Austern, Vasilis Syrgkanis", "title": "Asymptotics of the Empirical Bootstrap Method Beyond Asymptotic\n  Normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most commonly used methods for forming confidence intervals for\nstatistical inference is the empirical bootstrap, which is especially expedient\nwhen the limiting distribution of the estimator is unknown. However, despite\nits ubiquitous role, its theoretical properties are still not well understood\nfor non-asymptotically normal estimators. In this paper, under stability\nconditions, we establish the limiting distribution of the empirical bootstrap\nestimator, derive tight conditions for it to be asymptotically consistent, and\nquantify the speed of convergence. Moreover, we propose three alternative ways\nto use the bootstrap method to build confidence intervals with coverage\nguarantees. Finally, we illustrate the generality and tightness of our results\nby a series of examples, including uniform confidence bands, two-sample kernel\ntests, minmax stochastic programs and the empirical risk of stacked estimators.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 07:14:30 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Austern", "Morgane", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2011.11371", "submitter": "Ying Zhu", "authors": "Ying Zhu, Mozhgan Mirzaei", "title": "Classes of ODE solutions: smoothness, covering numbers, implications for\n  noisy function fitting, and the curse of smoothness phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many numerical methods for recovering ODE solutions from data rely on\napproximating the solutions using basis functions or kernel functions under a\nleast square criterion. The accuracy of this approach hinges on the smoothness\nof the solutions. This paper provides a theoretical foundation for these\nmethods by establishing novel results on the smoothness and covering numbers of\nODE solution classes (as a measure of their \"size\"). Our results provide\nanswers to \"how do the degree of smoothness and the \"size\" of a class of ODEs\naffect the \"size\" of the associated class of solutions?\" We show that: (1) for\n$y^{'}=f\\left(y\\right)$ and $y^{'}=f\\left(x,\\,y\\right)$, if the absolute values\nof all $k$th ($k\\leq\\beta+1$) order derivatives of $f$ are bounded by $1$, then\nthe solution can end up with the $(k+1)$th derivative whose magnitude grows\nfactorially fast in $k$ -- \"a curse of smoothness\"; (2) our upper bounds for\nthe covering numbers of the $(\\beta+2)-$degree smooth solution classes are\ngreater than those of the \"standard\" $(\\beta+2)-$degree smooth class of\nunivariate functions; (3) the mean squared error of least squares fitting for\nnoisy recovery has a convergence rate no larger than\n$\\left(\\frac{1}{n}\\right)^{\\frac{2\\left(\\beta+2\\right)}{2\\left(\\beta+2\\right)+1}}$\nif\n$n=\\Omega\\left(\\left(\\beta\\sqrt{\\log\\left(\\beta\\vee1\\right)}\\right)^{4\\beta+10}\\right)$,\nand under this condition, the rate\n$\\left(\\frac{1}{n}\\right)^{\\frac{2\\left(\\beta+2\\right)}{2\\left(\\beta+2\\right)+1}}$\nis minimax optimal in the case of $y^{'}=f\\left(x,\\,y\\right)$; (4) more\ngenerally, for the higher order Picard type ODEs,\n$y^{\\left(m\\right)}=f\\left(x,\\,y,\\,y^{'},\\,...,y^{\\left(m-1\\right)}\\right)$,\nthe covering number of the solution class is bounded from above by the product\nof the covering number of the class $\\mathcal{F}$ that $f$ ranges over and the\ncovering number of the set where initial values lie.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 12:54:54 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 10:49:36 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 23:48:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhu", "Ying", ""], ["Mirzaei", "Mozhgan", ""]]}, {"id": "2011.11435", "submitter": "Quentin Duchemin", "authors": "Quentin Duchemin (LAMA), Yohann de Castro (ICJ), Claire Lacour (LAMA)", "title": "Concentration inequality for U-statistics of order two for uniformly\n  ergodic Markov chains", "comments": "In this second revised version, we decided to focus on our main\n  results and the applications will be the purpose of another paper. We improve\n  our results compared to the previous version by providing a Bernstein-type\n  concentration inequality. This allows to benefit from small variance terms to\n  get faster convergence rates", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a new concentration inequality for U-statistics of order two for\nuniformly ergodic Markov chains. Working with bounded and $\\pi$-canonical\nkernels, we show that we can recover the convergence rate of Arcones and\nGin{\\'e} who proved a concentration result for U-statistics of independent\nrandom variables and canonical kernels. Our result allows for a dependence of\nthe kernels $h_{i,j}$ with the indexes in the sums, which prevents the use of\nstandard blocking tools. Our proof relies on an inductive analysis where we use\nmartingale techniques, uniform ergodicity, Nummelin splitting and Bernstein's\ntype inequality. Assuming further that the Markov chain starts from its\ninvariant distribution, we prove a Bernstein-type concentration inequality that\nprovides sharper convergence rate for small variance terms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:14:34 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 08:08:29 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 13:43:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Duchemin", "Quentin", "", "LAMA"], ["de Castro", "Yohann", "", "ICJ"], ["Lacour", "Claire", "", "LAMA"]]}, {"id": "2011.11516", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Algorithmic random duality theory -- large scale CLuP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on our \\bl{\\textbf{Random Duality Theory (RDT)}}, in a sequence of our\nrecent papers \\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19}, we\nintroduced a powerful algorithmic mechanism (called \\bl{\\textbf{CLuP}}) that\ncan be utilized to solve \\textbf{\\emph{exactly}} NP hard optimization problems\nin polynomial time. Here we move things further and utilize another of\nremarkable RDT features that we established in a long line of work in\n\\cite{StojnicCSetam09,StojnicCSetamBlock09,StojnicISIT2010binary,StojnicDiscPercp13,StojnicUpper10,StojnicGenLasso10,StojnicGenSocp10,StojnicPrDepSocp10,StojnicRegRndDlt10,Stojnicbinary16fin,Stojnicbinary16asym}.\nNamely, besides being stunningly precise in characterizing the performance of\nvarious random structures and optimization problems, RDT simultaneously also\nprovided an almost unparallel way for creating computationally efficient\noptimization algorithms that achieve such performance. One of the keys to our\nsuccess was our ability to transform the initial \\textbf{\\emph{constrained}}\noptimization into an \\textbf{\\emph{unconstrained}} one and in doing so greatly\nsimplify things both conceptually and computationally. That ultimately enabled\nus to solve a large set of classical optimization problems on a very large\nscale level. Here, we demonstrate how such a thinking can be applied to CLuP as\nwell and eventually utilized to solve pretty much any problem that the basic\nCLuP from \\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19} can solve.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:26:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "2011.11527", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Rephased CLuP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19} we introduced\nCLuP, a \\bl{\\textbf{Random Duality Theory (RDT)}} based algorithmic mechanism\nthat can be used for solving hard optimization problems. Due to their\nintroductory nature, \\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19}\ndiscuss the most fundamental CLuP concepts. On the other hand, in our companion\npaper \\cite{Stojniccluplargesc20} we started the story of going into a bit\ndeeper details that relate to many of other remarkable CLuP properties with\nsome of them reaching well beyond the basic fundamentals. Namely,\n\\cite{Stojniccluplargesc20} discusses how a somewhat silent RDT feature (its\nalgorithmic power) can be utilized to ensure that CLuP can be run on very large\nproblem instances as well. In particular, applying CLuP to the famous MIMO ML\ndetection problem we showed in \\cite{Stojniccluplargesc20} that its a large\nscale variant, $\\text{CLuP}^{r_0}$, can handle with ease problems with\n\\textbf{\\emph{several thousands}} of unknowns with theoretically minimal\ncomplexity per iteration (only a single matrix-vector multiplication suffices).\nIn this paper we revisit MIMO ML detection and discuss another remarkable\nphenomenon that emerges within the CLuP structure, namely the so-called\n\\bl{\\textbf{\\emph{rephasing}}}. As MIMO ML enters the so-called low $\\alpha$\nregime (fat system matrix with ratio of the number of rows and columns,\n$\\alpha$, going well below $1$) it becomes increasingly difficult even for the\nbasic standard CLuP to handle it. However, the discovery of the rephasing\nensures that CLuP remains on track and preserves its ability to achieve the ML\nperformance. To demonstrate the power of the rephasing we also conducted quite\na few numerical experiments, compared the results we obtained through them to\nthe theoretical predictions, and observed an excellent agreement.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:36:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "2011.11550", "submitter": "Mihailo Stojnic", "authors": "Mihailo Stojnic", "title": "Sparse linear regression -- CLuP achieves the ideal \\emph{exact} ML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit one of the classical statistical problems, the\nso-called sparse maximum-likelihood (ML) linear regression. As a way of\nattacking this type of regression, we present a novel CLuP mechanism that to a\ndegree relies on the \\bl{\\textbf{Random Duality Theory (RDT)}} based\nalgorithmic machinery that we recently introduced in\n\\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19,Stojniccluplargesc20,Stojniccluprephased20}.\nAfter the initial success that the CLuP exhibited in achieving the exact ML\nperformance while maintaining excellent computational complexity related\nproperties in MIMO ML detection in\n\\cite{Stojnicclupint19,Stojnicclupcmpl19,Stojnicclupplt19}, one would naturally\nexpect that a similar type of success can be achieved in other ML\nconsiderations. The results that we present here confirm that such an\nexpectation is indeed reasonable. In particular, within the sparse regression\ncontext, the introduced CLuP mechanism indeed turns out to be able to\n\\bl{\\textbf{\\emph{achieve the ideal ML performance}}}. Moreover, it can\nsubstantially outperform some of the most prominent earlier state of the art\nalgorithmic concepts, among them even the variants of the famous LASSO and SOCP\nfrom \\cite{StojnicPrDepSocp10,StojnicGenLasso10,StojnicGenSocp10}. Also, our\nrecent results presented in \\cite{Stojniccluplargesc20,Stojniccluprephased20}\nshowed that the CLuP has excellent \\bl{\\textbf{\\emph{large-scale}}} and the\nso-called \\bl{\\textbf{\\emph{rephasing}}} abilities. Since such large-scale\nalgorithmic features are possibly even more desirable within the sparse\nregression context we here also demonstrate that the basic CLuP ideas can be\nreformulated to enable solving with a relative ease the regression problems\nwith \\bl{\\textbf{\\emph{several thousands}}} of unknowns.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:03:13 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Stojnic", "Mihailo", ""]]}, {"id": "2011.11713", "submitter": "Boyu Zhang", "authors": "Fuchang Gao and Boyu Zhang", "title": "A Use of Even Activation Functions in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite broad interest in applying deep learning techniques to scientific\ndiscovery, learning interpretable formulas that accurately describe scientific\ndata is very challenging because of the vast landscape of possible functions\nand the \"black box\" nature of deep neural networks. The key to success is to\neffectively integrate existing knowledge or hypotheses about the underlying\nstructure of the data into the architecture of deep learning models to guide\nmachine learning. Currently, such integration is commonly done through\ncustomization of the loss functions. Here we propose an alternative approach to\nintegrate existing knowledge or hypotheses of data structure by constructing\ncustom activation functions that reflect this structure. Specifically, we study\na common case when the multivariate target function $f$ to be learned from the\ndata is partially exchangeable, \\emph{i.e.} $f(u,v,w)=f(v,u,w)$ for $u,v\\in\n\\mathbb{R}^d$. For instance, these conditions are satisfied for the\nclassification of images that is invariant under left-right flipping. Through\ntheoretical proof and experimental verification, we show that using an even\nactivation function in one of the fully connected layers improves neural\nnetwork performance. In our experimental 9-dimensional regression problems,\nreplacing one of the non-symmetric activation functions with the designated\n\"Seagull\" activation function $\\log(1+x^2)$ results in substantial improvement\nin network performance. Surprisingly, even activation functions are seldom used\nin neural networks. Our results suggest that customized activation functions\nhave great potential in neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:33:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gao", "Fuchang", ""], ["Zhang", "Boyu", ""]]}, {"id": "2011.11994", "submitter": "Chiara Amorino", "authors": "Chiara Amorino", "title": "Minimax rate of estimation for the stationary distribution of\n  jump-processes over anisotropic Holder classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of the non-parametric estimation for the density of the\nstationary distribution of the multivariate stochastic differential equation\nwith jumps (Xt) , when the dimension d is bigger than 3. From the continuous\nobservation of the sampling path on [0, T ] we show that, under anisotropic\nHolder smoothness constraints, kernel based estimators can achieve fast\nconvergence rates. In particular , they are as fast as the ones found by\nDalalyan and Reiss [9] for the estimation of the invariant density in the case\nwithout jumps under isotropic Holder smoothness constraints. Moreover, they are\nfaster than the ones found by Strauch [29] for the invariant density estimation\nof continuous stochastic differential equations, under anisotropic Holder\nsmoothness constraints. Furthermore, we obtain a minimax lower bound on the\nL2-risk for pointwise estimation, with the same rate up to a log(T) term. It\nimplies that, on a class of diffusions whose invariant density belongs to the\nanisotropic Holder class we are considering, it is impossible to find an\nestimator with a rate of estimation faster than the one we propose.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 09:45:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Amorino", "Chiara", ""]]}, {"id": "2011.12087", "submitter": "Hayk Asatryan", "authors": "Hayk Asatryan, Hanno Gottschalk, Marieke Lippert and Matthias Rottmann", "title": "A Convenient Infinite Dimensional Framework for Generative Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, generative adversarial networks (GANs) have demonstrated\nimpressive experimental results while there are only a few works that foster\nstatistical learning theory for GANs. In this work, we propose an infinite\ndimensional theoretical framework for generative adversarial learning. Assuming\nthe class of uniformly bounded $k$-times $\\alpha$-H\\\"older differentiable and\nuniformly positive densities, we show that the Rosenblatt transformation\ninduces an optimal generator, which is realizable in the hypothesis space of\n$\\alpha$-H\\\"older differentiable generators. With a consistent definition of\nthe hypothesis space of discriminators, we further show that in our framework\nthe Jensen-Shannon divergence between the distribution induced by the generator\nfrom the adversarial learning procedure and the data generating distribution\nconverges to zero. Under sufficiently strict regularity assumptions on the\ndensity of the data generating process, we also provide rates of convergence\nbased on concentration and chaining.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:45:17 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 17:17:46 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Asatryan", "Hayk", ""], ["Gottschalk", "Hanno", ""], ["Lippert", "Marieke", ""], ["Rottmann", "Matthias", ""]]}, {"id": "2011.12156", "submitter": "Tareq Alodat", "authors": "Tareq Alodat, M. T. Alodat, Dareen Omari", "title": "Nonparametric Asymptotic Distributions of Pianka's and MacArthur-Levins\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article studies the asymptotic behaviors of nonparametric estimators of\ntwo overlapping measures, namely Pianka's and MacArthur-Levins measures. The\nplug-in principle and the method of kernel density estimation are used to\nestimate such measures. The limiting theory of the functional of stochastic\nprocesses is used to study limiting behaviors of these estimators. It is shown\nthat both limiting distributions are normal under suitable assumptions. The\nresults are obtained in more general conditions on density functions and their\nkernel estimators. These conditions are suitable to deal with various\napplications. A small simulation study is also conducted to support the\ntheoretical findings. Finally, a real data set has been analyzed for\nillustrative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:13:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Alodat", "Tareq", ""], ["Alodat", "M. T.", ""], ["Omari", "Dareen", ""]]}, {"id": "2011.12215", "submitter": "Feng Ruan", "authors": "Keli Liu and Feng Ruan", "title": "A Self-Penalizing Objective Function for Scalable Interaction Detection", "comments": "34 pages; the Appendix can be found on the authors' personal websites\n  (the url is in the pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of nonparametric variable selection with a focus on\ndiscovering interactions between variables. With $p$ variables there are\n$O(p^s)$ possible order-$s$ interactions making exhaustive search infeasible.\nIt is nonetheless possible to identify the variables involved in interactions\nwith only linear computation cost, $O(p)$. The trick is to maximize a class of\nparametrized nonparametric dependence measures which we call metric learning\nobjectives; the landscape of these nonconvex objective functions is sensitive\nto interactions but the objectives themselves do not explicitly model\ninteractions. Three properties make metric learning objectives highly\nattractive:\n  (a) The stationary points of the objective are automatically sparse (i.e.\nperforms selection) -- no explicit $\\ell_1$ penalization is needed.\n  (b) All stationary points of the objective exclude noise variables with high\nprobability.\n  (c) Guaranteed recovery of all signal variables without needing to reach the\nobjective's global maxima or special stationary points.\n  The second and third properties mean that all our theoretical results apply\nin the practical case where one uses gradient ascent to maximize the metric\nlearning objective. While not all metric learning objectives enjoy good\nstatistical power, we design an objective based on $\\ell_1$ kernels that does\nexhibit favorable power: it recovers (i) main effects with $n \\sim \\log p$\nsamples, (ii) hierarchical interactions with $n \\sim \\log p$ samples and (iii)\norder-$s$ pure interactions with $n \\sim p^{2(s-1)}\\log p$ samples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 17:07:49 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 00:14:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Keli", ""], ["Ruan", "Feng", ""]]}, {"id": "2011.12220", "submitter": "Lin Zheng", "authors": "Lin Zheng", "title": "Some Theory for Texture Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of texture segmentation in images, and provide some\ntheoretical guarantees for the prototypical approach which consists in\nextracting local features in the neighborhood of a pixel and then applying a\nclustering algorithm for grouping the pixel according to these features. On the\none hand, for stationary textures, which we model with Gaussian Markov random\nfields, we construct the feature for each pixel by calculating the sample\ncovariance matrix of its neighborhood patch and cluster the pixels by an\napplication of k-means to group the covariance matrices. We show that this\ngeneric method is consistent. On the other hand, for non-stationary fields, we\ninclude the location of the pixel as an additional feature and apply\nsingle-linkage clustering. We again show that this generic and emblematic\nmethod is consistent. We complement our theory with some numerical experiments\nperformed on both generated and natural textures.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 04:21:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zheng", "Lin", ""]]}, {"id": "2011.12268", "submitter": "Giorgos Afendras", "authors": "Georgios Afendras, Marianthi Markatou and Albert Vexler", "title": "An AUK-based index for measuring and testing the joint dependence of a\n  random vector", "comments": "33 pages (plus 8 pages supplementary material), 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an index of dependence that allows one to measure the joint or\nmutual dependence of a $d$-dimensional random vector with $d>2$. The index is\nbased on a $d$-dimensional Kendall process. We further propose a standardized\nversion of our index of dependence that is easy to interpret, and provide an\nalgorithm for its computation. We discuss tests of total independence based on\nconsistent estimates of the area under the Kendall curve. We evaluate the\nperformance of our procedures via simulation, and apply our methods to a real\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:25:19 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 15:44:36 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Afendras", "Georgios", ""], ["Markatou", "Marianthi", ""], ["Vexler", "Albert", ""]]}, {"id": "2011.12387", "submitter": "Sarah Lemler", "authors": "Chiara Amorino (Uni.lu), Charlotte Dion (LPSM (UMR\\_8001)), Arnaud\n  Gloter (LaMME), Sarah Lemler (MICS)", "title": "On the nonparametric inference of coefficients of self-exciting\n  jump-diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a one-dimensional diffusion process with jumps\ndriven by a Hawkes process. We are interested in the estimations of the\nvolatility function and of the jump function from discrete high-frequency\nobservations in a long time horizon which remained an open question until now.\nFirst, we propose to estimate the volatility coefficient. For that, we\nintroduce a truncation function in our estimation procedure that allows us to\ntake into account the jumps of the process and estimate the volatility function\non a linear subspace of L2(A) where A is a compact interval of R. We obtain a\nbound for the empirical risk of the volatility estimator, ensuring its\nconsistency, and then we study an adaptive estimator w.r.t. the regularity.\nThen, we define an estimator of a sum between the volatility and the jump\ncoefficient modified with the conditional expectation of the intensity of the\njumps. We also establish a bound for the empirical risk for the non-adaptive\nestimators of this sum, the convergence rate up to the regularity of the true\nfunction, and an oracle inequality for the final adaptive estimator.Finally, we\ngive a methodology to recover the jump function in some applications. We\nconduct a simulation study to measure our estimators' accuracy in practice and\ndiscuss the possibility of recovering the jump function from our estimation\nprocedure.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:12:45 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 09:44:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Amorino", "Chiara", "", "Uni.lu"], ["Dion", "Charlotte", "", "LPSM"], ["Gloter", "Arnaud", "", "LaMME"], ["Lemler", "Sarah", "", "MICS"]]}, {"id": "2011.12416", "submitter": "Nathaniel Josephs", "authors": "Li Chen, Nathaniel Josephs, Lizhen Lin, Jie Zhou, and Eric D. Kolaczyk", "title": "A spectral-based framework for hypothesis testing in populations of\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new spectral-based approach to hypothesis testing\nfor populations of networks. The primary goal is to develop a test to determine\nwhether two given samples of networks come from the same random model or\ndistribution. Our test statistic is based on the trace of the third order for a\ncentered and scaled adjacency matrix, which we prove converges to the standard\nnormal distribution as the number of nodes tends to infinity. The asymptotic\npower guarantee of the test is also provided. The proper interplay between the\nnumber of networks and the number of nodes for each network is explored in\ncharacterizing the theoretical properties of the proposed testing statistics.\nOur tests are applicable to both binary and weighted networks, operate under a\nvery general framework where the networks are allowed to be large and sparse,\nand can be extended to multiple-sample testing. We provide an extensive\nsimulation study to demonstrate the superior performance of our test over\nexisting methods and apply our test to three real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:58:27 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chen", "Li", ""], ["Josephs", "Nathaniel", ""], ["Lin", "Lizhen", ""], ["Zhou", "Jie", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2011.12433", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Nilesh Tripuraneni, Peter L. Bartlett,\n  Michael I. Jordan", "title": "Optimal Mean Estimation without a Variance", "comments": "Fixed typographical errors in Theorem 1.2, Lemmas 4.3 and C.8", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of heavy-tailed mean estimation in settings where the\nvariance of the data-generating distribution does not exist. Concretely, given\na sample $\\mathbf{X} = \\{X_i\\}_{i = 1}^n$ from a distribution $\\mathcal{D}$\nover $\\mathbb{R}^d$ with mean $\\mu$ which satisfies the following\n\\emph{weak-moment} assumption for some ${\\alpha \\in [0, 1]}$: \\begin{equation*}\n\\forall \\|v\\| = 1: \\mathbb{E}_{X \\thicksim \\mathcal{D}}[\\lvert \\langle X - \\mu,\nv\\rangle \\rvert^{1 + \\alpha}] \\leq 1, \\end{equation*} and given a target\nfailure probability, $\\delta$, our goal is to design an estimator which attains\nthe smallest possible confidence interval as a function of $n,d,\\delta$. For\nthe specific case of $\\alpha = 1$, foundational work of Lugosi and Mendelson\nexhibits an estimator achieving subgaussian confidence intervals, and\nsubsequent work has led to computationally efficient versions of this\nestimator. Here, we study the case of general $\\alpha$, and establish the\nfollowing information-theoretic lower bound on the optimal attainable\nconfidence interval: \\begin{equation*} \\Omega \\left(\\sqrt{\\frac{d}{n}} +\n\\left(\\frac{d}{n}\\right)^{\\frac{\\alpha}{(1 + \\alpha)}} + \\left(\\frac{\\log 1 /\n\\delta}{n}\\right)^{\\frac{\\alpha}{(1 + \\alpha)}}\\right). \\end{equation*}\nMoreover, we devise a computationally-efficient estimator which achieves this\nlower bound.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:39:21 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 20:31:46 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Tripuraneni", "Nilesh", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.12478", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Phong Alain Chau", "title": "Minimax Estimation of Distances on a Surface and Minimax Manifold\n  Learning in the Isometric-to-Convex Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start by considering the problem of estimating intrinsic distances on a\nsmooth surface. We show that sharper estimates can be obtained via a\nreconstruction of the surface, and discuss the use of the tangential Delaunay\ncomplex for that purpose. We further show that the resulting approximation rate\nis in fact optimal in an information-theoretic (minimax) sense. We then turn to\nmanifold learning and argue that a variant of Isomap where the distances are\ninstead computed on a reconstructed surface is minimax optimal for the problem\nof isometric manifold embedding.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:57:51 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Chau", "Phong Alain", ""]]}, {"id": "2011.12627", "submitter": "Kwangmin Lee", "authors": "Kwangmin Lee, Kyoungjae Lee, and Jaeyong Lee", "title": "Post-Processed Posteriors for Banded Covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference of banded covariance matrices and propose a\npost-processed posterior. The post-processing of the posterior consists of two\nsteps. In the first step, posterior samples are obtained from the conjugate\ninverse-Wishart posterior which does not satisfy any structural restrictions.\nIn the second step, the posterior samples are transformed to satisfy the\nstructural restriction through a post-processing function. The conceptually\nstraightforward procedure of the post-processed posterior makes its computation\nefficient and can render interval estimators of functionals of covariance\nmatrices. We show that it has nearly optimal minimax rates for banded\ncovariances among all possible pairs of priors and post-processing functions.\nFurthermore, we prove that the expected coverage probability of the\n$(1-\\alpha)100\\%$ highest posterior density region of the post-processed\nposterior is asymptotically $1-\\alpha$ with respect to a conventional posterior\ndistribution. It implies that the highest posterior density region of the\npost-processed posterior is, on average, a credible set of a conventional\nposterior. The advantages of the post-processed posterior are demonstrated by a\nsimulation study and a real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:32:12 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lee", "Kwangmin", ""], ["Lee", "Kyoungjae", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2011.12697", "submitter": "Katerina Papagiannouli", "authors": "Katerina Papagiannouli", "title": "A Lepski\\u{i}-type stopping rule for the covariance estimation of\n  multi-dimensional L\\'evy processes", "comments": "35 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We suppose that a L\\'evy process is observed at discrete time points.\nStarting from an asymptotically minimax family of estimators for the continuous\npart of the L\\'evy Khinchine characteristics, i.e., the covariance, we derive a\ndata-driven parameter choice for the frequency of estimating the covariance. We\ninvestigate a Lepski\\u{i}-type stopping rule for the adaptive procedure.\nConsequently, we use a balancing principle for the best possible data-driven\nparameter. The adaptive estimator achieves almost the optimal rate. Numerical\nexperiments with the proposed selection rule are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:12:01 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Papagiannouli", "Katerina", ""]]}, {"id": "2011.12781", "submitter": "Won-Ki Seo", "authors": "Won-Ki Seo", "title": "Functional Principal Component Analysis of Cointegrated Functional Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component analysis (FPCA) has played an important role\nin the development of functional time series analysis. This paper investigates\nhow FPCA can be used to analyze cointegrated functional time series and\nproposes a modification of FPCA as a novel statistical tool. Our modified FPCA\nnot only provides an asymptotically more efficient estimator of the\ncointegrating vectors, but also leads to novel FPCA-based tests for examining\nsome essential properties of cointegrated functional time series. As an\nempirical illustration, our methodology is applied to two empirical examples:\nage-specific employment rates and earning densities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:43:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 15:38:29 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 15:24:21 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Seo", "Won-Ki", ""]]}, {"id": "2011.12858", "submitter": "Emil Aas Stoltenberg", "authors": "Emil Aas Stoltenberg", "title": "The standard cure model with a linear hazard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a mixture cure model with a linear hazard rate\nregression model for the event times. Cure models are statistical models for\nevent times that take into account that a fraction of the population might\nnever experience the event of interest, this fraction is said to be\n{`}cured{'}. The population survival function in a mixture cure model takes the\nform $S(t) = 1 - \\pi + \\pi\\exp(-\\int_0^t\\alpha(s)\\,d s)$, where $\\pi$ is the\nprobability of being susceptible to the event under study, and $\\alpha(s)$ is\nthe hazard rate of the susceptible fraction. We let both $\\pi$ and $\\alpha(s)$\ndepend on possibly different covariate vectors $X$ and $Z$. The probability\n$\\pi$ is taken to be the logistic function $\\pi(X^{\\prime}\\gamma) =\n1/\\{1+\\exp(-X^{\\prime}\\gamma)\\}$, while we model $\\alpha(s)$ by Aalen's linear\nhazard rate regression model. This model postulates that a susceptible\nindividual has hazard rate function $\\alpha(t;Z) = \\beta_0(t) + \\beta_1(t)Z_1 +\n\\cdots + Z_{q-1}\\beta_{q-1}(t)$ in terms of her covariate values\n$Z_1,\\ldots,Z_{q-1}$. The large-sample properties of our estimators are studied\nby way of parametric models that tend to a semiparametric model as a parameter\n$K \\to \\infty$. For each model in the sequence of parametric models, we assume\nthat the data generating mechanism is parametric, thus simplifying the\nderivation of the estimators, as well as the proofs of consistency and limiting\nnormality. Finally, we use contiguity techniques to switch back to assuming\nthat the data stem from the semiparametric model. This technique for deriving\nand studying estimators in non- and semiparametric settings has previously been\nstudied and employed in the high-frequency data literature, but seems to be\nnovel in survival analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:28:31 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Stoltenberg", "Emil Aas", ""]]}, {"id": "2011.12873", "submitter": "Adam McCloskey", "authors": "Adam McCloskey", "title": "Hybrid Confidence Intervals for Informative Uniform Asymptotic Inference\n  After Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a new type of confidence interval for correct asymptotic inference\nafter using data to select a model of interest without assuming any model is\ncorrectly specified. This hybrid confidence interval is constructed by\ncombining techniques from the selective inference and post-selection inference\nliteratures to yield a short confidence interval across a wide range of data\nrealizations. I show that hybrid confidence intervals have correct asymptotic\ncoverage, uniformly over a large class of probability distributions. I\nillustrate the use of these confidence intervals in the problem of inference\nafter using the LASSO objective function to select a regression model of\ninterest and provide evidence of their desirable length properties in finite\nsamples via a set of Monte Carlo exercises that is calibrated to real-world\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:51:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["McCloskey", "Adam", ""]]}, {"id": "2011.13030", "submitter": "Almut Veraart", "authors": "Fred Espen Benth and Dennis Schroers and Almut E. D. Veraart", "title": "A weak law of large numbers for realised covariation in a Hilbert space\n  setting", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article generalises the concept of realised covariation to\nHilbert-space-valued stochastic processes. More precisely, based on\nhigh-frequency functional data, we construct an estimator of the trace-class\noperator-valued integrated volatility process arising in general mild solutions\nof Hilbert space-valued stochastic evolution equations in the sense of Da Prato\nand Zabczyk (2014). We prove a weak law of large numbers for this estimator,\nwhere the convergence is uniform on compacts in probability with respect to the\nHilbert-Schmidt norm. In addition, we show that the conditions on the\nvolatility process are valid for most common stochastic volatility models in\nHilbert spaces.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 21:24:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Benth", "Fred Espen", ""], ["Schroers", "Dennis", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2011.13157", "submitter": "Sayar Karmakar", "authors": "Sayar Karmakar, Stefan Richter, Wei Biao Wu", "title": "Simultaneous inference for time-varying models", "comments": "To appear at Journal of Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A general class of time-varying regression models is considered in this\npaper. We estimate the regression coefficients by using local linear\nM-estimation. For these estimators, weak Bahadur representations are obtained\nand are used to construct simultaneous confidence bands. For practical\nimplementation, we propose a bootstrap based method to circumvent the slow\nlogarithmic convergence of the theoretical simultaneous bands. Our results\nsubstantially generalize and unify the treatments for several time-varying\nregression and auto-regression models. The performance for ARCH and GARCH\nmodels is studied in simulations and a few real-life applications of our study\nare presented through analysis of some popular financial datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 07:00:09 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 04:44:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Karmakar", "Sayar", ""], ["Richter", "Stefan", ""], ["Wu", "Wei Biao", ""]]}, {"id": "2011.13418", "submitter": "HongVan Le", "authors": "H\\^ong V\\^an L\\^e and Alexey A. Tuzhilin", "title": "Nonparametric estimations and the diffeological Fisher metric", "comments": "final version, 20 p.. version 1: 17 p. comments are welcome!", "journal-ref": "In: Barbaresco F., Nielsen F. (eds) Geometric Structures of\n  Statistical Physics, Information Geometry, and Learning, p. 120-138, SPIGL\n  2020. Springer Proceedings in Mathematics & Statistics, vol 361. Springer,\n  Cham", "doi": "10.1007/978-3-030-77957-3_7", "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, first, we survey the concept of diffeological Fisher metric\nand its naturality, using functorial language of probability morphisms, and\nslightly extending L\\^e's theory in \\cite{Le2020} to include weakly\n$C^k$-diffeological statistical models. Then we introduce the resulting notions\nof the diffeological Fisher distance, the diffeological Hausdorff--Jeffrey\nmeasure and explain their role in classical and Bayesian nonparametric\nestimation problems in statistics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:19:40 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 16:41:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["L\u00ea", "H\u00f4ng V\u00e2n", ""], ["Tuzhilin", "Alexey A.", ""]]}, {"id": "2011.13442", "submitter": "Antonio Russo", "authors": "Antonio E. Russo, William M. Kirby, Kenneth M. Rudinger, Andrew D.\n  Baczewski, Shelby Kimmel", "title": "Consistency testing for robust phase estimation", "comments": "22 pages, 8 figures, including 4 appendices", "journal-ref": "Phys. Rev. A 103, 042609 (2021)", "doi": "10.1103/PhysRevA.103.042609", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension to the robust phase estimation protocol, which can\nidentify incorrect results that would otherwise lie outside the expected\nstatistical range. Robust phase estimation is increasingly a method of choice\nfor applications such as estimating the effective process parameters of noisy\nhardware, but its robustness is dependent on the noise satisfying certain\nthreshold assumptions. We provide consistency checks that can indicate when\nthose thresholds have been violated, which can be difficult or impossible to\ntest directly. We test these consistency checks for several common noise\nmodels, and identify two possible checks with high accuracy in locating the\npoint in a robust phase estimation run at which further estimates should not be\ntrusted. One of these checks may be chosen based on resource availability, or\nthey can be used together in order to provide additional verification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 19:00:00 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Russo", "Antonio E.", ""], ["Kirby", "William M.", ""], ["Rudinger", "Kenneth M.", ""], ["Baczewski", "Andrew D.", ""], ["Kimmel", "Shelby", ""]]}, {"id": "2011.13496", "submitter": "Rong Huang", "authors": "Rong Huang", "title": "Detecting Sparse Heterogeneous Mixtures in a Two-Sample Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting sparse heterogeneous mixtures in a\ntwo-sample setting from a nonparametric perspective, where the effect manifests\nitself as a positive shift. We suggest a two-sample higher criticism test, and\nshow that it is first-order comparable to the likelihood ratio test for the\ngeneralized Guassian mixture models in all sparsity regimes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:20:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Huang", "Rong", ""]]}, {"id": "2011.13602", "submitter": "Sophie Langer Dr.", "authors": "Michael Kohler and Sophie Langer", "title": "Statistical theory for image classification using deep convolutional\n  neural networks with cross-entropy loss", "comments": "arXiv admin note: text overlap with arXiv:2003.01526", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks learned by minimizing the cross-entropy loss\nare nowadays the standard for image classification. Till now, the statistical\ntheory behind those networks is lacking. We analyze the rate of convergence of\nthe misclassification risk of the estimates towards the optimal\nmisclassification risk. Under suitable assumptions on the smoothness and\nstructure of the aposteriori probability it is shown that these estimates\nachieve a rate of convergence which is independent of the dimension of the\nimage. The study shed light on the good performance of CNNs learned by\ncross-entropy loss and partly explains their success in practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 08:33:51 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Kohler", "Michael", ""], ["Langer", "Sophie", ""]]}, {"id": "2011.13613", "submitter": "Jianjun Xu", "authors": "Jianjun Xu and Wenquan Cui", "title": "A new RKHS-based global testing for functional linear model", "comments": "13pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies global testing of the slope function in functional\nlinear regression model in the framework of reproducing kernel Hilbert space.\nWe propose a new testing statistic based on smoothness regularization\nestimators. The asymptotic distribution of the testing statistic is established\nunder null hypothesis. It is shown that the null asymptotic distribution is\ndetermined jointly by the reproducing kernel and the covariance function. Our\ntheoretical analysis shows that the proposed testing is consistent over a class\nof smooth local alternatives. Despite the generality of the method of\nregularization, we show the procedure is easily implementable. Numerical\nexamples are provided to demonstrate the empirical advantages over the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:03:59 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xu", "Jianjun", ""], ["Cui", "Wenquan", ""]]}, {"id": "2011.13624", "submitter": "Tengyao Wang", "authors": "Fengnan Gao and Tengyao Wang", "title": "Two-sample testing of high-dimensional linear regression coefficients\n  via complementary sketching", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new method for two-sample testing of high-dimensional linear\nregression coefficients without assuming that those coefficients are\nindividually estimable. The procedure works by first projecting the matrices of\ncovariates and response vectors along directions that are complementary in sign\nin a subset of the coordinates, a process which we call 'complementary\nsketching'. The resulting projected covariates and responses are aggregated to\nform two test statistics, which are shown to have essentially optimal\nasymptotic power under a Gaussian design when the difference between the two\nregression coefficients is sparse and dense respectively. Simulations confirm\nthat our methods perform well in a broad class of settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:31:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Gao", "Fengnan", ""], ["Wang", "Tengyao", ""]]}, {"id": "2011.13680", "submitter": "Miguel Tierz", "authors": "Leonardo Santilli and Miguel Tierz", "title": "Riemannian Gaussian distributions, random matrix ensembles and diffusion\n  kernels", "comments": "26 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Riemannian Gaussian distributions on symmetric spaces,\nintroduced in recent years, are of standard random matrix type. We exploit this\nto compute analytically marginals of the probability density functions. This\ncan be done fully, using Stieltjes-Wigert orthogonal polynomials, for the case\nof the space of Hermitian matrices, where the distributions have already\nappeared in the physics literature. For the case when the symmetric space is\nthe space of $m \\times m$ symmetric positive definite matrices, we show how to\nefficiently compute by evaluating Pfaffians at specific values of $m$.\nEquivalently, we can obtain the same result by constructing specific skew\northogonal polynomials with regards to the log-normal weight function (skew\nStieltjes-Wigert polynomials). Other symmetric spaces are studied and the same\ntype of result is obtained for the quaternionic case. Moreover, we show how the\nprobability density functions are a particular case of diffusion reproducing\nkernels of the Karlin-McGregor type, describing non-intersecting Brownian\nmotions, which are also diffusion processes in the Weyl chamber of Lie groups.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 11:41:29 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Santilli", "Leonardo", ""], ["Tierz", "Miguel", ""]]}, {"id": "2011.13967", "submitter": "Zejian Liu", "authors": "Zejian Liu and Meng Li", "title": "Equivalence of Convergence Rates of Posterior Distributions and Bayes\n  Estimators for Functions and Nonparametric Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the posterior contraction rates of a Bayesian method with Gaussian\nprocess priors in nonparametric regression and its plug-in property for\ndifferential operators. For a general class of kernels, we establish\nconvergence rates of the posterior measure of the regression function and its\nderivatives, which are both minimax optimal up to a logarithmic factor for\nfunctions in certain classes. Our calculation shows that the rate-optimal\nestimation of the regression function and its derivatives share the same choice\nof hyperparameter, indicating that the Bayes procedure remarkably adapts to the\norder of derivatives and enjoys a generalized plug-in property that extends\nreal-valued functionals to function-valued functionals. This leads to a\npractically simple method for estimating the regression function and its\nderivatives, whose finite sample performance is assessed using simulations.\n  Our proof shows that, under certain conditions, to any convergence rate of\nBayes estimators there corresponds the same convergence rate of the posterior\ndistributions (i.e., posterior contraction rate), and vice versa. This\nequivalence holds for a general class of Gaussian processes and covers the\nregression function and its derivative functionals, under both the $L_2$ and\n$L_{\\infty}$ norms. In addition to connecting these two fundamental large\nsample properties in Bayesian and non-Bayesian regimes, such equivalence\nenables a new routine to establish posterior contraction rates by calculating\nconvergence rates of nonparametric point estimators.\n  At the core of our argument is an operator-theoretic framework for kernel\nridge regression and equivalent kernel techniques. We derive a range of sharp\nnon-asymptotic bounds that are pivotal in establishing convergence rates of\nnonparametric point estimators and the equivalence theory, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:11:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Zejian", ""], ["Li", "Meng", ""]]}, {"id": "2011.14017", "submitter": "Laura Dumitrescu", "authors": "Laura Dumitrescu and Ioana Schiopu-Kratina", "title": "Asymptotic results with estimating equations for time-evolving clustered\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence, strong consistency and asymptotic normality of\nestimators obtained from estimating functions, that are p-dimensional\nmartingale transforms. The problem is motivated by the analysis of evolutionary\nclustered data, with distributions belonging to the exponential family, and\nwhich may also vary in terms of other component series. Within a\nquasi-likelihood approach, we construct estimating equations, which accommodate\ndifferent forms of dependency among the components of the response vector and\nestablish multivariate extensions of results on linear and generalized linear\nmodels, with stochastic covariates. Furthermore, we characterize estimating\nfunctions which are asymptotically optimal, in that they lead to confidence\nregions for the regression parameters which are of minimum size,\nasymptotically. Results from a simulation study and an application to a real\ndataset are included.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 22:19:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dumitrescu", "Laura", ""], ["Schiopu-Kratina", "Ioana", ""]]}, {"id": "2011.14126", "submitter": "Zakaria Mhammedi", "authors": "Zakaria Mhammedi and Hisham Husain", "title": "Risk-Monotonicity in Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of data is a difficult task in many applications of machine\nlearning, and it is only natural that one hopes and expects the populating risk\nto decrease (better performance) monotonically with increasing data points. It\nturns out, somewhat surprisingly, that this is not the case even for the most\nstandard algorithms such as empirical risk minimization. Non-monotonic\nbehaviour of the risk and instability in training have manifested and appeared\nin the popular deep learning paradigm under the description of double descent.\nThese problems highlight bewilderment in our understanding of learning\nalgorithms and generalization. It is, therefore, crucial to pursue this concern\nand provide a characterization of such behaviour. In this paper, we derive the\nfirst consistent and risk-monotonic algorithms for a general statistical\nlearning setting under weak assumptions, consequently resolving an open problem\n(Viering et al. 2019) on how to avoid non-monotonic behaviour of risk curves.\nOur work makes a significant contribution to the topic of risk-monotonicity,\nwhich may be key in resolving empirical phenomena such as double descent.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 12:52:12 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 07:21:06 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 04:26:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mhammedi", "Zakaria", ""], ["Husain", "Hisham", ""]]}, {"id": "2011.14182", "submitter": "Tim Seynnaeve", "authors": "Christopher Eur, Tara Fife, Jos\\'e Alejandro Samper, Tim Seynnaeve", "title": "Reciprocal maximum likelihood degrees of diagonal linear concentration\n  models", "comments": "13 pages, comments welcome To appear in: Le Matematiche, vol. 76 (2)\n  (special issue on Linear Spaces of Symmetric Matrices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG math.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the reciprocal maximal likelihood degree (rmld) of a diagonal\nlinear concentration model $\\mathcal L \\subseteq \\mathbb{C}^n$ of dimension $r$\nis equal to $(-2)^r\\chi_M( \\textstyle\\frac{1}{2})$, where $\\chi_M$ is the\ncharacteristic polynomial of the matroid $M$ associated to $\\mathcal L$. In\nparticular, this establishes the polynomiality of the rmld for general diagonal\nlinear concentration models, positively answering a question of Sturmfels,\nTimme, and Zwiernik.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:13:50 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:40:27 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Eur", "Christopher", ""], ["Fife", "Tara", ""], ["Samper", "Jos\u00e9 Alejandro", ""], ["Seynnaeve", "Tim", ""]]}, {"id": "2011.14185", "submitter": "Yang Ning", "authors": "Siyi Deng, Yang Ning, Jiwei Zhao, Heping Zhang", "title": "Optimal Semi-supervised Estimation and Inference for High-dimensional\n  Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There are many scenarios such as the electronic health records where the\noutcome is much more difficult to collect than the covariates. In this paper,\nwe consider the linear regression problem with such a data structure under the\nhigh dimensionality. Our goal is to investigate when and how the unlabeled data\ncan be exploited to improve the estimation and inference of the regression\nparameters in linear models, especially in light of the fact that such linear\nmodels may be misspecified in data analysis. In particular, we address the\nfollowing two important questions. (1) Can we use the labeled data as well as\nthe unlabeled data to construct a semi-supervised estimator such that its\nconvergence rate is faster than the supervised estimators? (2) Can we construct\nconfidence intervals or hypothesis tests that are guaranteed to be more\nefficient or powerful than the supervised estimators? To address the first\nquestion, we establish the minimax lower bound for parameter estimation in the\nsemi-supervised setting. We show that the upper bound from the supervised\nestimators that only use the labeled data cannot attain this lower bound. We\nclose this gap by proposing a new semi-supervised estimator which attains the\nlower bound. To address the second question, based on our proposed\nsemi-supervised estimator, we propose two additional estimators for\nsemi-supervised inference, the efficient estimator and the safe estimator. The\nformer is fully efficient if the unknown conditional mean function is estimated\nconsistently, but may not be more efficient than the supervised approach\notherwise. The latter usually does not aim to provide fully efficient\ninference, but is guaranteed to be no worse than the supervised approach, no\nmatter whether the linear model is correctly specified or the conditional mean\nfunction is consistently estimated.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:26:46 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Deng", "Siyi", ""], ["Ning", "Yang", ""], ["Zhao", "Jiwei", ""], ["Zhang", "Heping", ""]]}, {"id": "2011.14195", "submitter": "Abdelkader Mokkadem", "authors": "Abdelkader Mokkadem, Mariane Pelletier, Louis Raimbault", "title": "Recursive Association Rule Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mining frequent itemsets and association rules is an essential task within\ndata mining and data analysis. In this paper, we introduce PrefRec, a recursive\nalgorithm for finding frequent itemsets and association rules. Its main\nadvantage is its recursiveness with respect to the items. It is particularly\nefficient for updating the mining process when new items are added to the\ndatabase or when some are excluded. We present in a complete way the logic of\nthe algorithm as well as its various applications. Finally we present\nexperiments carried out in the R language comparing PrefRec with Apriori and\nEclat the two most powerful algorithms in this language. To achieve this we\nbuilt an R package to run our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:53:03 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 16:54:45 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mokkadem", "Abdelkader", ""], ["Pelletier", "Mariane", ""], ["Raimbault", "Louis", ""]]}, {"id": "2011.14219", "submitter": "Soonwoo Kwon", "authors": "Koohyun Kwon, Soonwoo Kwon", "title": "Adaptive Inference in Multivariate Nonparametric Regression Models Under\n  Monotonicity", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of adaptive inference on a regression function at a\npoint under a multivariate nonparametric regression setting. The regression\nfunction belongs to a H\\\"older class and is assumed to be monotone with respect\nto some or all of the arguments. We derive the minimax rate of convergence for\nconfidence intervals (CIs) that adapt to the underlying smoothness, and provide\nan adaptive inference procedure that obtains this minimax rate. The procedure\ndiffers from that of Cai and Low (2004), intended to yield shorter CIs under\npractically relevant specifications. The proposed method applies to general\nlinear functionals of the regression function, and is shown to have favorable\nperformance compared to existing inference procedures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 21:21:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kwon", "Koohyun", ""], ["Kwon", "Soonwoo", ""]]}, {"id": "2011.14425", "submitter": "Edoardo Mainini", "authors": "Emanuele Dolera, Stefano Favaro, Edoardo Mainini", "title": "A new approach to posterior contraction rates via Wasserstein dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new approach to the classical problem of quantifying\nposterior contraction rates (PCRs) in Bayesian statistics. Our approach relies\non Wasserstein distance, and it leads to two main contributions which improve\non the existing literature of PCRs. The first contribution exploits the dynamic\nformulation of Wasserstein distance, for short referred to as Wasserstein\ndynamics, in order to establish PCRs under dominated Bayesian statistical\nmodels. As a novelty with respect to existing approaches to PCRs, Wasserstein\ndynamics allows us to circumvent the use of sieves in both stating and proving\nPCRs, and it sets forth a natural connection between PCRs and three well-known\nclassical problems in statistics and probability theory: the speed of mean\nGlivenko-Cantelli convergence, the estimation of weighted Poincar\\'e-Wirtinger\nconstants and Sanov large deviation principle for Wasserstein distance. The\nsecond contribution combines the use of Wasserstein distance with a suitable\nsieve construction to establish PCRs under full Bayesian nonparametric models.\nAs a novelty with respect to existing literature of PCRs, our second result\nprovides with the first treatment of PCRs under non-dominated Bayesian models.\nApplications of our results are presented for some classical Bayesian\nstatistical models, e.g., regular parametric models, infinite-dimensional\nexponential families, linear regression in infinite dimension and nonparametric\nmodels under Dirichlet process priors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 19:24:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dolera", "Emanuele", ""], ["Favaro", "Stefano", ""], ["Mainini", "Edoardo", ""]]}, {"id": "2011.14455", "submitter": "Nicholas Witte", "authors": "N.S. Witte and P.E. Greenwood", "title": "On the Density arising from the Domain of Attraction between Sum and\n  Supremum: the $\\alpha$-Sun operator", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore the analytic properties of the density function $\nh(x;\\gamma,\\alpha) $, $ x \\in (0,\\infty) $, $ \\gamma > 0 $, $ 0 < \\alpha < 1 $\nwhich arises from the domain of attraction problem for a statistic\ninterpolating between the supremum and sum of random variables. The parameter $\n\\alpha $ controls the interpolation between these two cases, while $ \\gamma $\nparametrises the type of extreme value distribution from which the underlying\nrandom variables are drawn from. For $ \\alpha = 0 $ the Fr\\'echet density\napplies, whereas for $ \\alpha = 1 $ we identify a particular Fox H-function,\nwhich are a natural extension of hypergeometric functions into the realm of\nfractional calculus. In contrast for intermediate $ \\alpha $ an entirely new\nfunction appears, which is not one of the extensions to the hypergeometric\nfunction considered to date. We derive series, integral and continued fraction\nrepresentations of this latter function.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 22:29:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Witte", "N. S.", ""], ["Greenwood", "P. E.", ""]]}, {"id": "2011.14542", "submitter": "Kevin W. Lu", "authors": "Kevin W. Lu", "title": "Calibration for multivariate L\\'evy-driven Ornstein-Uhlenbeck processes\n  with applications to weak subordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a multivariate L\\'evy-driven Ornstein-Uhlenbeck process where the\nstationary distribution or background driving L\\'evy process is from a\nparametric family. We derive the likelihood function assuming that the\ninnovation term is absolutely continuous. Two examples are studied in detail:\nthe process where the stationary distribution or background driving L\\'evy\nprocess is given by a weak variance alpha-gamma process, which is a\nmultivariate generalisation of the variance gamma process created using weak\nsubordination. In the former case, we give an explicit representation of the\nbackground driving L\\'evy process, leading to an innovation term with a\nmixed-type distribution, allowing for the exact simulation of the process, and\na separate likelihood function. In the latter case, we show the innovation term\nis absolutely continuous. The results of a simulation study demonstrate that\nmaximum likelihood numerically computed using Fourier inversion can be applied\nto accurately estimate the parameters in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 04:28:30 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 02:21:58 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lu", "Kevin W.", ""]]}, {"id": "2011.14614", "submitter": "Guillaume Marrelec", "authors": "Guillaume Marrelec, Alain Giron and Laura Messio", "title": "Exponential decay of pairwise correlation in Gaussian graphical models\n  with an equicorrelational one-dimensional connection pattern", "comments": "Paper accepted for publication in Statistics and Probability Letters", "journal-ref": "Statistics and Probability Letters 171, 109016 (2021)", "doi": "10.1016/j.spl.2020.109016", "report-no": null, "categories": "math.ST cond-mat.stat-mech math-ph math.MP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider Gaussian graphical models associated with an equicorrelational\nand one-dimensional conditional independence graph. We show that pairwise\ncorrelation decays exponentially as a function of distance. We also provide a\nlimit when the number of variables tend to infinity and quantify the difference\nbetween the finite and infinite cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:33:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Marrelec", "Guillaume", ""], ["Giron", "Alain", ""], ["Messio", "Laura", ""]]}, {"id": "2011.14747", "submitter": "Shuhei Mano", "authors": "Masayo Y. Hirose, Shuhei Mano", "title": "A Bayesian construction of asymptotically unbiased estimators", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A differential geometric framework to construct an asymptotically unbiased\nestimator of a function of a parameter is presented. The derived estimator\nasymptotically coincides with the uniformly minimum variance unbiased\nestimator, if a complete sufficient statistic exists. The framework is based on\nthe maximum a posteriori estimation, where the prior is chosen such that the\nestimator is unbiased. The framework is demonstrated for the second-order\nasymptotic unbiasedness (unbiased up to $O(n^{-1})$ for a sample of size $n$).\nThe condition of the asymptotic unbiasedness leads the choice of the prior such\nthat the departure from a kind of harmonicity of the estimand is canceled out\nat each point of the model manifold. For a given estimand, the prior is given\nas an integral. On the other hand, for a given prior, we can address the bias\nof what estimator can be reduced by solving an elliptic partial differential\nequation. A family of invariant priors, which generalizes the Jeffreys prior,\nis mentioned as a specific example. Some illustrative examples of applications\nof the proposed framework are provided.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:01:13 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 11:57:15 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Hirose", "Masayo Y.", ""], ["Mano", "Shuhei", ""]]}, {"id": "2011.14762", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner", "title": "Testing for Uniqueness of Estimators", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniqueness of the population value of an estimated descriptor is a standard\nassumption in asymptotic theory. However, m-estimation problems often allow for\nlocal minima of the sample estimating function, which may stem from multiple\nglobal minima of the underlying population estimating function. In the present\narticle, we provide tools to systematically determine for a given sample\nwhether the underlying population estimating function may have multiple global\nminima. To achieve this goal, we develop asymptotic theory for non-unique\nminimizers and introduce asymptotic tests using the bootstrap. We discuss three\napplications of our tests to data, each of which presents a typical scenario in\nwhich non-uniqueness of descriptors may occur. These model scenarios are the\nmean on a non-euclidean space, non-linear regression and Gaussian mixture\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:18:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Eltzner", "Benjamin", ""]]}, {"id": "2011.14881", "submitter": "Amandine Dubois", "authors": "Cristina Butucea, Amandine Dubois, Adrien Saumard", "title": "Sharp phase transitions for exact support recovery under local\n  differential privacy", "comments": "financial supports added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of variable selection in the Gaussian mean model in\n$\\mathbb{R}^d$ under the additional constraint that only privatised data are\navailable for inference. For this purpose, we adopt a recent generalisation of\nclassical minimax theory to the framework of local $\\alpha$-differential\nprivacy. We provide lower and upper bounds on the rate of convergence for the\nexpected Hamming loss over classes of at most $s$-sparse vectors whose non-zero\ncoordinates are separated from $0$ by a constant $a>0$. As corollaries, we\nderive necessary and sufficient conditions (up to log factors) for exact\nrecovery and for almost full recovery. When we restrict our attention to\nnon-interactive mechanisms that act independently on each coordinate our lower\nbound shows that, contrary to the non-private setting, both exact and almost\nfull recovery are impossible whatever the value of $a$ in the high-dimensional\nregime such that $n \\alpha^2/ d^2\\lesssim 1$. However, in the regime\n$n\\alpha^2/d^2\\gg \\log(n\\alpha^2/d^2)\\log(d)$ we can exhibit a sharp critical\nvalue $a^*$ (up to a logarithmic factor) such that exact and almost full\nrecovery are possible for all $a\\gg a^*$ and impossible for $a\\leq a^*$. We\nshow that these results can be improved when allowing for all non-interactive\n(that act globally on all coordinates) locally $\\alpha-$differentially private\nmechanisms in the sense that phase transitions occur at lower levels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:22:39 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 13:52:41 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Butucea", "Cristina", ""], ["Dubois", "Amandine", ""], ["Saumard", "Adrien", ""]]}, {"id": "2011.14893", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Pierre Lafaye de Micheaux and Fr\\'ed\\'eric Ouimet", "title": "A study of seven asymmetric kernels for the estimation of cumulative\n  distribution functions", "comments": "38 pages, 2 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Mombeni et al. (2019), Birnbaum-Saunders and Weibull kernel estimators\nwere introduced for the estimation of cumulative distribution functions\n(c.d.f.s) supported on the half-line $[0,\\infty)$. They were the first authors\nto use asymmetric kernels in the context of c.d.f. estimation. Their estimators\nwere shown to perform better numerically than traditional methods such as the\nbasic kernel method and the boundary modified version from Tenreiro (2013). In\nthe present paper, we complement their study by introducing five new asymmetric\nkernel c.d.f. estimators, namely the Gamma, inverse Gamma, lognormal, inverse\nGaussian and reciprocal inverse Gaussian kernel c.d.f. estimators. For these\nfive new estimators, we prove the asymptotic normality and we find asymptotic\nexpressions for the following quantities: bias, variance, mean squared error\nand mean integrated squared error. A numerical study then compares the\nperformance of the five new c.d.f. estimators against traditional methods and\nthe Birnbaum-Saunders and Weibull kernel c.d.f. estimators from Mombeni et al.\n(2019). By using the same experimental design, we show that the lognormal and\nBirnbaum-Saunders kernel c.d.f. estimators perform the best overall, while the\nother asymmetric kernel estimators are sometimes better but always at least\ncompetitive against the boundary kernel method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:32:03 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["de Micheaux", "Pierre Lafaye", ""], ["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}]