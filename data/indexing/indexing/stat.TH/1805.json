[{"id": "1805.00194", "submitter": "Yimin Zhong", "authors": "Jennifer Bryson, Hongkai Zhao, Yimin Zhong", "title": "Intrinsic Complexity And Scaling Laws: From Random Fields to Random\n  Vectors", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random fields are commonly used for modeling of spatially (or timely)\ndependent stochastic processes. In this study, we provide a characterization of\nthe intrinsic complexity of a random field in terms of its second order\nstatistics, e.g., the covariance function, based on the Karhumen-Lo\\'{e}ve\nexpansion. We then show scaling laws for the intrinsic complexity of a random\nfield in terms of the correlation length as it goes to 0. In the discrete\nsetting, it becomes approximate embeddings of a set of random vectors. We\nprovide a precise scaling law when the random vectors have independent and\nidentically distributed entires using random matrix theory as well as when the\nrandom vectors has a specific covariance structure.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 05:48:26 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 03:24:57 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Bryson", "Jennifer", ""], ["Zhao", "Hongkai", ""], ["Zhong", "Yimin", ""]]}, {"id": "1805.00450", "submitter": "Levon Demirdjian", "authors": "Levon Demirdjian, Majid Mojirsheibani", "title": "Classification on convex sets in the presence of missing covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of results related to statistical classification on convex sets are\npresented. In particular, the focus is on the case where some of the covariates\nin the data and observation being classified can be missing. The form of the\noptimal classifier is derived when the class-conditional densities are uniform\nover convex regions. In practice, the underlying convex sets are often unknown\nand must be estimated with a set of data. In this case, the convex hull of a\nset of points is shown to be a consistent estimator of the underlying convex\nset. The problem of estimation is further complicated since the number of\npoints in each convex hull is itself a random variable. The corresponding\nplug-in version of the optimal classifier is derived and shown to be Bayes\nconsistent.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 17:38:12 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Demirdjian", "Levon", ""], ["Mojirsheibani", "Majid", ""]]}, {"id": "1805.01422", "submitter": "Lukas Steinberger", "authors": "Angelika Rohde and Lukas Steinberger", "title": "Geometrizing rates of convergence under local differential privacy\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a functional $\\theta(\\mathbb P)$ of an\nunknown probability distribution $\\mathbb P \\in\\mathcal P$ in which the\noriginal iid sample $X_1,\\dots, X_n$ is kept private even from the statistician\nvia an $\\alpha$-local differential privacy constraint. Let $\\omega_{TV}$ denote\nthe modulus of continuity of the functional $\\theta$ over $\\mathcal P$, with\nrespect to total variation distance. For a large class of loss functions $l$\nand a fixed privacy level $\\alpha$, we prove that the privatized minimax risk\nis equivalent to $l(\\omega_{TV}(n^{-1/2}))$ to within constants, under\nregularity conditions that are satisfied, in particular, if $\\theta$ is linear\nand $\\mathcal P$ is convex. Our results complement the theory developed by\nDonoho and Liu (1991) with the nowadays highly relevant case of privatized\ndata. Somewhat surprisingly, the difficulty of the estimation problem in the\nprivate case is characterized by $\\omega_{TV}$, whereas, it is characterized by\nthe Hellinger modulus of continuity if the original data $X_1,\\dots, X_n$ are\navailable. We also find that for locally private estimation of linear\nfunctionals over a convex model a simple sample mean estimator, based on\nindependently and binary privatized observations, always achieves the minimax\nrate. We further provide a general recipe for choosing the functional parameter\nin the optimal binary privatization mechanisms and illustrate the general\ntheory in numerous examples. Our theory allows to quantify the price to be paid\nfor local differential privacy in a large class of estimation problems. This\nprice appears to be highly problem specific.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:49:04 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 12:46:18 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Rohde", "Angelika", ""], ["Steinberger", "Lukas", ""]]}, {"id": "1805.01577", "submitter": "Adolfo Quiroz", "authors": "Mateo D\\'iaz, Adolfo J. Quiroz and Mauricio Velasco", "title": "Local angles and dimension estimation from data on manifolds", "comments": "1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data living in a manifold $M\\subseteq \\mathbb{R}^m$ and a point $p\\in M$\nwe consider a statistic $U_{k,n}$ which estimates the variance of the angle\nbetween pairs of vectors $X_i-p$ and $X_j-p$, for data points $X_i$, $X_j$,\nnear $p$, and evaluate this statistic as a tool for estimation of the intrinsic\ndimension of $M$ at $p$. Consistency of the local dimension estimator is\nestablished and the asymptotic distribution of $U_{k,n}$ is found under minimal\nregularity assumptions. Performance of the proposed methodology is compared\nagainst state-of-the-art methods on simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 00:05:55 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["D\u00edaz", "Mateo", ""], ["Quiroz", "Adolfo J.", ""], ["Velasco", "Mauricio", ""]]}, {"id": "1805.01720", "submitter": "Thomas Gallouet", "authors": "Thomas Gallou\\\"et (MOKAPLAN), Guillaume Mijoule, Yvik Swan", "title": "Regularity of solutions of the Stein equation and rates in the\n  multivariate central limit theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the multivariate Stein equation $\\Delta f - x\\cdot \\nabla f = h(x) -\nE h(Z)$, where $Z$ is a standard $d$-dimensional Gaussian random vector, and\nlet $f\\_h$ be the solution given by Barbour's generator approach. We prove\nthat, when $h$ is $\\alpha$-H\\\"older ($0<\\alpha\\leq1$), all derivatives of order\n$2$ of $f\\_h$ are $\\alpha$-H\\\"older {\\it up to a $\\log$ factor}; in particular\nthey are $\\beta$-H\\\"older for all $\\beta \\in (0, \\alpha)$, hereby improving\nexisting regularity results on the solution of the multivariate Gaussian Stein\nequation. For $\\alpha=1$, the regularity we obtain is optimal, as shown by an\nexample given by Rai\\v{c} \\cite{raivc2004multivariate}. As an application, we\nprove a near-optimal Berry-Esseen bound of the order $\\log n/\\sqrt n$ in the\nclassical multivariate CLT in $1$-Wasserstein distance, as long as the\nunderlying random variables have finite moment of order $3$. When only a finite\nmoment of order $2+\\delta$ is assumed ($0<\\delta<1$), we obtain the optimal\nrate in $\\mathcal O(n^{-\\frac{\\delta}{2}})$. All constants are explicit and\ntheir dependence on the dimension $d$ is studied when $d$ is large.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 11:45:23 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Gallou\u00ebt", "Thomas", "", "MOKAPLAN"], ["Mijoule", "Guillaume", ""], ["Swan", "Yvik", ""]]}, {"id": "1805.01729", "submitter": "Ilja Klebanov", "authors": "Ilja Klebanov", "title": "Axiomatic Approach to Variable Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable kernel density estimation allows the approximation of a probability\ndensity by the mean of differently stretched and rotated kernels centered at\ngiven sampling points $y_n\\in\\mathbb{R}^d,\\ n=1,\\dots,N$. Up to now, the choice\nof the corresponding bandwidth matrices $h_n$ has relied mainly on asymptotic\narguments, like the minimization of the asymptotic mean integrated squared\nerror (AMISE), which work well for large numbers of sampling points. However,\nin practice, one is often confronted with small to moderately sized sample sets\nfar below the asymptotic regime, which highly restricts the usability of such\nmethods. As an alternative to this asymptotic reasoning we suggest an axiomatic\napproach which guarantees invariance of the density estimate under linear\ntransformations of the original density (and the sampling points) as well as\nunder splitting of the density into several `well-separated' parts. In order to\nstill ensure proper asymptotic behavior of the estimate, we \\emph{postulate}\nthe typical dependence $h_n\\propto N^{-1/(d+4)}$. Further, we derive a new\nbandwidths selection rule which satisfies these axioms and performs\nconsiderably better than conventional ones in an artificially intricate\ntwo-dimensional example as well as in a real life example.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 11:58:33 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Klebanov", "Ilja", ""]]}, {"id": "1805.01820", "submitter": "Qian Lin", "authors": "Qian Lin, Zhigen Zhao, Jun S. Liu", "title": "Global testing under the sparse alternatives for single index models", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the single index model $y=f(\\beta^{\\tau}x,\\epsilon)$ with Gaussian\ndesign, %satisfying that rank $var(\\mathbb{E}[x\\mid y])=1$ where $f$ is unknown\nand $\\beta$ is a sparse $p$-dimensional unit vector with at most $s$ nonzero\nentries, we are interested in testing the null hypothesis that $\\beta$, when\nviewed as a whole vector, is zero against the alternative that some entries of\n$\\beta$ is nonzero. Assuming that $var(\\mathbb{E}[x \\mid y])$ is non-vanishing,\nwe define the generalized signal-to-noise ratio (gSNR) $\\lambda$ of the model\nas the unique non-zero eigenvalue of $var(\\mathbb{E}[x \\mid y])$. We show that\nif $s^{2}\\log^2(p)\\wedge p$ is of a smaller order of $n$, denoted as\n$s^{2}\\log^2(p)\\wedge p\\prec n$, where $n$ is the sample size, one can detect\nthe existence of signals if and only if gSNR$\\succ\\frac{p^{1/2}}{n}\\wedge\n\\frac{s\\log(p)}{n}$. Furthermore, if the noise is additive (i.e.,\n$y=f(\\beta^{\\tau}x)+\\epsilon$), one can detect the existence of the signal if\nand only if gSNR$\\succ\\frac{p^{1/2}}{n}\\wedge \\frac{s\\log(p)}{n} \\wedge\n\\frac{1}{\\sqrt{n}}$. It is rather surprising that the detection boundary for\nthe single index model with additive noise matches that for linear regression\nmodels.\n  These results pave the road for thorough theoretical analysis of\nsingle/multiple index models in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 15:21:59 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Lin", "Qian", ""], ["Zhao", "Zhigen", ""], ["Liu", "Jun S.", ""]]}, {"id": "1805.02234", "submitter": "Peter Harremo\\\"es", "authors": "Peter Harremo\\\"es", "title": "Statistical Inference and Exact Saddle Point Approximations", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference may follow a frequentist approach or it may follow a\nBayesian approach or it may use the minimum description length principle (MDL).\nOur goal is to identify situations in which these different approaches to\nstatistical inference coincide. It is proved that for exponential families MDL\nand Bayesian inference coincide if and only if the renormalized saddle point\napproximation for the conjugated exponential family is exact. For 1-dimensional\nexponential families the only families with exact renormalized saddle point\napproximations are the Gaussian location family, the Gamma family and the\ninverse Gaussian family. They are conjugated families of the Gaussian location\nfamily, the Gamma family and the Poisson-exponential family. The first two\nfamilies are self-conjugated implying that only for the two first families the\nBayesian approach is consistent with the frequentist approach. In higher\ndimensions there are more examples.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 15:30:01 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Harremo\u00ebs", "Peter", ""]]}, {"id": "1805.02257", "submitter": "Lingrui Gan", "authors": "Lingrui Gan, Naveen N. Narisetty, Feng Liang", "title": "Bayesian Regularization for Graphical Models with Unequal Shrinkage", "comments": "To appear in Journal of the American Statistical Association (Theory\n  & Methods)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian framework for estimating a high-dimensional sparse\nprecision matrix, in which adaptive shrinkage and sparsity are induced by a\nmixture of Laplace priors. Besides discussing our formulation from the Bayesian\nstandpoint, we investigate the MAP (maximum a posteriori) estimator from a\npenalized likelihood perspective that gives rise to a new non-convex penalty\napproximating the $\\ell_0$ penalty. Optimal error rates for estimation\nconsistency in terms of various matrix norms along with selection consistency\nfor sparse structure recovery are shown for the unique MAP estimator under mild\nconditions. For fast and efficient computation, an EM algorithm is proposed to\ncompute the MAP estimator of the precision matrix and (approximate) posterior\nprobabilities on the edges of the underlying sparse structure. Through\nextensive simulation studies and a real application to a call center data, we\nhave demonstrated the fine performance of our method compared with existing\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 18:16:21 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:31:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Gan", "Lingrui", ""], ["Narisetty", "Naveen N.", ""], ["Liang", "Feng", ""]]}, {"id": "1805.02422", "submitter": "Douge Lahcen", "authors": "Lahcen Douge", "title": "Nonparametric regression estimation for quasi-associated Hilbertian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the asymptotic normality of the kernel type estimator for the\nregression function constructed from quasi-associated data when the explanatory\nvariable takes its values in a separable Hilbert space.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 09:56:43 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Douge", "Lahcen", ""]]}, {"id": "1805.02542", "submitter": "Qiyang Han", "authors": "Qiyang Han, Jon A. Wellner", "title": "Robustness of shape-restricted regression estimators: an envelope\n  perspective", "comments": "44 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical least squares estimators are well-known to be robust with respect\nto moment assumptions concerning the error distribution in a wide variety of\nfinite-dimensional statistical problems; generally only a second moment\nassumption is required for least squares estimators to maintain the same rate\nof convergence that they would satisfy if the errors were assumed to be\nGaussian. In this paper, we give a geometric characterization of the robustness\nof shape-restricted least squares estimators (LSEs) to error distributions with\nan $L_{2,1}$ moment, in terms of the `localized envelopes' of the model.\n  This envelope perspective gives a systematic approach to proving oracle\ninequalities for the LSEs in shape-restricted regression problems in the random\ndesign setting, under a minimal $L_{2,1}$ moment assumption on the errors. The\ncanonical isotonic and convex regression models, and a more challenging\nadditive regression model with shape constraints are studied in detail.\nStrikingly enough, in the additive model both the adaptation and robustness\nproperties of the LSE can be preserved, up to error distributions with an\n$L_{2,1}$ moment, for estimating the shape-constrained proxy of the marginal\n$L_2$ projection of the true regression function. This holds essentially\nregardless of whether or not the additive model structure is correctly\nspecified.\n  The new envelope perspective goes beyond shape constrained models. Indeed, at\na general level, the localized envelopes give a sharp characterization of the\nconvergence rate of the $L_2$ loss of the LSE between the worst-case rate as\nsuggested by the recent work of the authors [25], and the best possible\nparametric rate.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 14:29:16 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Han", "Qiyang", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1805.02826", "submitter": "Yiqiao Zhong", "authors": "Jianqing Fan and Yiqiao Zhong", "title": "Optimal Subspace Estimation Using Overidentifying Vectors via\n  Generalized Method of Moments", "comments": "48 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical models seek relationship between variables via subspaces of\nreduced dimensions. For instance, in factor models, variables are roughly\ndistributed around a low dimensional subspace determined by the loading matrix;\nin mixed linear regression models, the coefficient vectors for different\nmixtures form a subspace that captures all regression functions; in multiple\nindex models, the effect of covariates is summarized by the effective dimension\nreduction space.\n  Such subspaces are typically unknown, and good estimates are crucial for data\nvisualization, dimension reduction, diagnostics and estimation of unknown\nparameters. Usually, we can estimate these subspaces by computing moments from\ndata. Often, there are many ways to estimate a subspace, by using moments of\ndifferent orders, transformed moments, etc. A natural question is: how can we\ncombine all these moment conditions and achieve optimality for subspace\nestimation?\n  In this paper, we formulate our problem as estimation of an unknown subspace\n$\\mathcal{S}$ of dimension $r$, given a set of overidentifying vectors $\\{\n\\mathrm{\\bf v}_\\ell \\}_{\\ell=1}^m$ (namely $m \\ge r$) that satisfy $\\mathbb{E}\n\\mathrm{\\bf v}_{\\ell} \\in \\mathcal{S}$ and have the form $$ \\mathrm{\\bf v}_\\ell\n= \\frac{1}{n} \\sum_{i=1}^n \\mathrm{\\bf f}_\\ell(\\mathbf{x}_i, y_i), $$ where\ndata are i.i.d. and each function $\\mathrm{\\bf f}_\\ell$ is known. By exploiting\ncertain covariance information related to $\\mathrm{\\bf v}_\\ell$, our estimator\nof $\\mathcal{S}$ uses an optimal weighting matrix and achieves the smallest\nasymptotic error, in terms of canonical angles. The analysis is based on the\ngeneralized method of moments that is tailored to our problem. Our method is\napplied to aforementioned models and distributed estimation of heterogeneous\ndatasets, and may be potentially extended to analyze matrix completion, neural\nnets, among others.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 04:14:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Fan", "Jianqing", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1805.03554", "submitter": "I-Hsiang Wang", "authors": "Wei-Ning Chen and I-Hsiang Wang", "title": "Anonymous Heterogeneous Distributed Detection: Optimal Decision Rules,\n  Error Exponents, and the Price of Anonymity", "comments": "Submitted to IEEE Transactions on Information Theory. Parts of this\n  paper was presented at ISIT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the fundamental limits of heterogeneous distributed detection in\nan anonymous sensor network with n sensors and a single fusion center. The\nfusion center collects the single observation from each of the n sensors to\ndetect a binary parameter. The sensors are clustered into multiple groups, and\ndifferent groups follow different distributions under a given hypothesis. The\nkey challenge for the fusion center is the anonymity of sensors - although it\nknows the exact number of sensors and the distribution of observations in each\ngroup, it does not know which group each sensor belongs to. It is hence natural\nto consider it as a composite hypothesis testing problem. First, we propose an\noptimal test called mixture likelihood ratio test based on the ratio of the\nuniform mixture of all distributions under one hypothesis to that under the\nother. Optimality is shown by first arguing that symmetric tests are optimal,\nwhich do not depend on the order of observations across the sensors, and then\nproving that the mixture likelihood ratio test is optimal among all symmetric\ntests. Second, we focus on the Neyman-Pearson setting and characterize the\nerror exponent of the worst-case type-II error probability as n tends to\ninfinity, assuming the number of sensors in each group is proportional to n.\nFinally, we generalize our result to find the collection of all achievable\ntype-I and type-II error exponents, where the boundary of the region can be\nobtained by solving a convex program. Our results elucidate the price of\nanonymity in heterogeneous distributed detection, and can be extended to M-ary\nhypothesis testing with heterogeneous observations generated according to\nhidden latent variables. The results are also applied to distributed detection\nunder Byzantine attacks, which hints that the conventional approach based on\nsimple hypothesis testing might be too pessimistic.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 14:29:09 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 23:39:07 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Wei-Ning", ""], ["Wang", "I-Hsiang", ""]]}, {"id": "1805.03579", "submitter": "Melisande Albert", "authors": "M\\'elisande Albert (IMT)", "title": "Concentration inequalities for randomly permuted sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially motivated by the study of the non-asymptotic properties of\nnon-parametric tests based on permutation methods, concentration inequalities\nfor uniformly permuted sums have been largely studied in the literature.\nRecently, Delyon et al. proved a new Bernstein-type concentration inequality\nbased on martingale theory. This work presents a new proof of this inequality\nbased on the fundamental inequalities for random permutations of Talagrand. The\nidea is to first obtain a rough inequality for the square root of the permuted\nsum, and then, iterate the previous analysis and plug this first inequality to\nobtain a general concentration of permuted sums around their median. Then,\nconcentration inequalities around the mean are deduced. This method allows us\nto obtain the Bernstein-type inequality up to constants, and, in particular, to\nrecovers the Gaussian behavior of such permuted sums under classical conditions\nencountered in the literature. Then, an application to the study of the second\nkind error rate of permutation tests of independence is presented.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:11:47 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Albert", "M\u00e9lisande", "", "IMT"]]}, {"id": "1805.03825", "submitter": "Ben Berckmoes", "authors": "Ben Berckmoes, Anna Ivanova, Geert Molenberghs", "title": "On asymptotic normality in estimation after a group sequential trial", "comments": "17 pages (and appendix with data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that in many realistic cases, the ordinary sample mean after a group\nsequential trial is asymptotically normal if the maximal number of observations\nincreases. We derive that it is often safe to use naive confidence intervals\nfor the mean of the collected observations, based on the ordinary sample mean.\nOur theoretical findings are confirmed by a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 05:28:38 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 20:15:53 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 09:45:37 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Berckmoes", "Ben", ""], ["Ivanova", "Anna", ""], ["Molenberghs", "Geert", ""]]}, {"id": "1805.03839", "submitter": "Matthias L\\\"offler", "authors": "Matthias L\\\"offler", "title": "Wald Statistics in high-dimensional PCA", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we consider PCA for Gaussian observations $X_1,\\dots, X_n$ with\ncovariance $\\Sigma=\\sum_i \\lambda_i P_i$ in the 'effective rank' setting with\nmodel complexity governed by $\\mathbf{r}(\\Sigma):=\\text{tr}(\\Sigma)/\\| \\Sigma\n\\|$. We prove a Berry-Essen type bound for a Wald Statistic of the spectral\nprojector $\\hat P_r$. This can be used to construct non-asymptotic confidence\nellipsoids and tests for spectral projectors $P_r$. Using higher order\npertubation theory we are able to show that our Theorem remains valid even when\n$\\mathbf{r}(\\Sigma) \\gg \\sqrt{n}$.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 06:19:46 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["L\u00f6ffler", "Matthias", ""]]}, {"id": "1805.03892", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti and Sukanta Pramanik", "title": "A Generalized Xgamma Generator Family of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new class of distributions, called Odds xgamma-G (OXG-G)\nfamily of distribu- tions is proposed for modeling lifetime data. A\ncomprehensive account of the mathematical proper- ties of the new class\nincluding estimation issue is presented. Two data sets have been analyzed to\nillustrate its applicability.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:54:40 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Pramanik", "Sukanta", ""]]}, {"id": "1805.04010", "submitter": "Mika Meitz", "authors": "Mika Meitz, Daniel Preve, Pentti Saikkonen", "title": "A mixture autoregressive model based on Student's $t$-distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new mixture autoregressive model based on Student's $t$-distribution is\nproposed. A key feature of our model is that the conditional $t$-distributions\nof the component models are based on autoregressions that have multivariate\n$t$-distributions as their (low-dimensional) stationary distributions. That\nautoregressions with such stationary distributions exist is not immediate. Our\nformulation implies that the conditional mean of each component model is a\nlinear function of past observations and the conditional variance is also time\nvarying. Compared to previous mixture autoregressive models our model may\ntherefore be useful in applications where the data exhibits rather strong\nconditional heteroskedasticity. Our formulation also has the theoretical\nadvantage that conditions for stationarity and ergodicity are always met and\nthese properties are much more straightforward to establish than is common in\nnonlinear autoregressive models. An empirical example employing a realized\nkernel series based on S&P 500 high-frequency data shows that the proposed\nmodel performs well in volatility forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:58:58 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Meitz", "Mika", ""], ["Preve", "Daniel", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1805.04345", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Rate optimal estimation of quadratic functionals in inverse problems\n  with partially unknown operator and application to testing problems", "comments": "Corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of quadratic functionals in a Gaussian sequence\nmodel where the eigenvalues are supposed to be unknown and accessible through\nnoisy observations only. Imposing smoothness assumptions both on the signal and\nthe sequence of eigenvalues, we develop a minimax theory for this problem. We\npropose a truncated series estimator and show that it attains the optimal rate\nof convergence if the truncation parameter is chosen appropriately.\nConsequences for testing problems in inverse problems are equally discussed: in\nparticular, the minimax rates of testing for signal detection and\ngoodness-of-fit testing are derived.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 12:03:04 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:23:21 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 13:59:18 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "1805.04620", "submitter": "Rafael Stern", "authors": "Victor Coscrato, Rafael Izbicki and Rafael Bassi Stern", "title": "Agnostic tests can control the type I and type II errors simultaneously", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its common practice, statistical hypothesis testing presents\nchallenges in interpretation. For instance, in the standard frequentist\nframework there is no control of the type II error. As a result, the\nnon-rejection of the null hypothesis cannot reasonably be interpreted as its\nacceptance. We propose that this dilemma can be overcome by using agnostic\nhypothesis tests, since they can control the type I and II errors\nsimultaneously. In order to make this idea operational, we show how to obtain\nagnostic hypothesis in typical models. For instance, we show how to build\n(unbiased) uniformly most powerful agnostic tests and how to obtain agnostic\ntests from standard p-values. Also, we present conditions such that the above\ntests can be made logically coherent. Finally, we present examples of\nconsistent agnostic hypothesis tests.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 23:53:30 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Coscrato", "Victor", ""], ["Izbicki", "Rafael", ""], ["Stern", "Rafael Bassi", ""]]}, {"id": "1805.04648", "submitter": "Rahul Mukerjee", "authors": "Jiayu Peng, Rahul Mukerjee and Dennis K. J. Lin", "title": "Design of Order-of-Addition Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an order-of-addition experiment, each treatment is a permutation of m\ncomponents. It is often unaffordable to test all the m! treatments, and the\ndesign problem arises. We consider a model that incorporates the order of each\npair of components and can also account for the distance between the two\ncomponents in every such pair. Under this model, the optimality of the uniform\ndesign measure is established, via the approximate theory, for a broad range of\ncriteria. Coupled with an eigen-analysis, this result serves as a benchmark\nthat paves the way for assessing the efficiency and robustness of any exact\ndesign. The closed-form construction of a class of robust optimal fractional\ndesigns is then explored and illustrated.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 04:19:55 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Peng", "Jiayu", ""], ["Mukerjee", "Rahul", ""], ["Lin", "Dennis K. J.", ""]]}, {"id": "1805.04682", "submitter": "Gerard Kerkyacharian", "authors": "G. Cleanthous, A. Georgiadis, G. Kerkyacharian, P. Petrushev, D.\n  Picard", "title": "Kernel and wavelet density estimators on manifolds and more general\n  metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the density of observations taking\nvalues in classical or nonclassical spaces such as manifolds and more general\nmetric spaces. Our setting is quite general but also sufficiently rich in\nallowing the development of smooth functional calculus with well localized\nspectral kernels, Besov regularity spaces, and wavelet type systems. Kernel and\nboth linear and nonlinear wavelet density estimators are introduced and\nstudied. Convergence rates for these estimators are established, which are\nanalogous to the existing results in the classical setting of real-valued\nvariables.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 07:58:08 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 15:47:11 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Cleanthous", "G.", ""], ["Georgiadis", "A.", ""], ["Kerkyacharian", "G.", ""], ["Petrushev", "P.", ""], ["Picard", "D.", ""]]}, {"id": "1805.04946", "submitter": "Alessio Figalli", "authors": "Alessio Figalli", "title": "On the Continuity of Center-Outward Distribution and Quantile Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To generalize the notion of distribution function to dimension $d\\geq 2$, in\nthe recent papers it was proposed a concept of center-outward distribution\nfunction based on optimal transportation ideas, and the inferential properties\nof the corresponding center-outward quantile function were studied. A crucial\ntool needed to derive the desired inferential properties is the continuity and\ninvertibility for the center-outward quantile function outside the origin, as\nthis ensures the existence of closed and nested quantile contours. The aim of\nthis paper is to prove such a continuity and invertibility result.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 21:06:14 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Figalli", "Alessio", ""]]}, {"id": "1805.05002", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis, G. Guillera-Arroita, RM Huggins, B J T Morgan", "title": "How can the score test be consistent?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The score test statistic using the observed information is easy to compute\nnumerically. Its large sample distribution under the null hypothesis is well\nknown and is equivalent to that of the score test based on the expected\ninformation, the likelihood-ratio test and the Wald test. However, several\nauthors have noted that under the alternative this no longer holds and in\nparticular the statistic can take negative values. Here we examine the score\ntest using the observed information in the context of comparing two binomial\nproportions under imperfect detection, a common problem in ecology when\nstudying occurrence of species. We demonstrate through a combination of\nsimulations and theoretical analysis that a new modified rule which we propose\nthat rejects the null hypothesis when the observed score statistic is larger\nthan the usual chi-square cut-off or is negative has power that is mostly\ngreater to any other test. In addition consistency is largely restored. Our new\ntest is easy to use and inference is always possible.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 03:35:02 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:05:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Guillera-Arroita", "G.", ""], ["Huggins", "RM", ""], ["Morgan", "B J T", ""]]}, {"id": "1805.05054", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif and Pierre Alquier", "title": "Consistency of Variational Bayes Inference for Estimation and Model\n  Selection in Mixtures", "comments": null, "journal-ref": "Electronic Journal of Statistics, 2018, vol. 12, no. 2, pp.\n  2995-3035", "doi": "10.1214/18-EJS1475", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are widely used in Bayesian statistics and machine learning,\nin particular in computational biology, natural language processing and many\nother fields. Variational inference, a technique for approximating intractable\nposteriors thanks to optimization algorithms, is extremely popular in practice\nwhen dealing with complex models such as mixtures. The contribution of this\npaper is two-fold. First, we study the concentration of variational\napproximations of posteriors, which is still an open problem for general\nmixtures, and we derive consistency and rates of convergence. We also tackle\nthe problem of model selection for the number of components: we study the\napproach already used in practice, which consists in maximizing a numerical\ncriterion (the Evidence Lower Bound). We prove that this strategy indeed leads\nto strong oracle inequalities. We illustrate our theoretical results by\napplications to Gaussian and multinomial mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 08:15:48 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 09:50:12 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1805.05071", "submitter": "Gilles Stoltz", "authors": "Aur\\'elien Garivier, H\\'edi Hadiji, Pierre Menard, Gilles Stoltz", "title": "KL-UCB-switch: optimal regret bounds for stochastic bandits from both a\n  distribution-dependent and a distribution-free viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of K-armed stochastic bandits with distribution only assumed\nto be supported by [0,1], we introduce the first algorithm, called\nKL-UCB-switch, that enjoys simultaneously a distribution-free regret bound of\noptimal order $\\sqrt{KT}$ and a distribution-dependent regret bound of optimal\norder as well, that is, matching the $\\kappa\\ln T$ lower bound by Lai & Robbins\n(1985) and Burnetas & Katehakis (1996). This self-contained contribution\nsimultaneously presents state-of-the-art techniques for regret minimization in\nbandit models, and an elementary construction of non-asymptotic confidence\nbounds based on the empirical likelihood method for bounded distributions.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 09:05:10 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:13:40 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Garivier", "Aur\u00e9lien", ""], ["Hadiji", "H\u00e9di", ""], ["Menard", "Pierre", ""], ["Stoltz", "Gilles", ""]]}, {"id": "1805.05199", "submitter": "Mahmoud El-Morshedy", "authors": "M. El- Morshedy and A. A. Khalil", "title": "Bivariate Discrete Exponentiated Weibull Distribution: Properties and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new bivariate discrete distribution is introduced which\ncalled bivariate discrete exponentiated Weibull (BDEW) distribution. Several of\nits mathematical statistical properties are derived such as the joint\ncumulative distribution function, the joint joint hazard rate function,\nprobability mass function, joint moment generating function, mathematical\nexpectation and reliability function for stress-strength model. Further, the\nparameters of the BDEW distribution are estimated by the maximum likelihood\nmethod. Two real data sets are analyzed, and it was found that the BDEW\ndistribution provides better fit than other discrete distributions.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:07:22 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Morshedy", "M. El-", ""], ["Khalil", "A. A.", ""]]}, {"id": "1805.05485", "submitter": "Mathias Drton", "authors": "Mathias Drton, Christopher Fox, Andreas K\\\"aufl, Guillaume Pouliot", "title": "The Maximum Likelihood Threshold of a Path Diagram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear structural equation models postulate noisy linear relationships\nbetween variables of interest. Each model corresponds to a path diagram, which\nis a mixed graph with directed edges that encode the domains of the linear\nfunctions and bidirected edges that indicate possible correlations among noise\nterms. Using this graphical representation, we determine the maximum likelihood\nthreshold, that is, the minimum sample size at which the likelihood function of\na Gaussian structural equation model is almost surely bounded. Our result\nallows the model to have feedback loops and is based on decomposing the path\ndiagram with respect to the connected components of its bidirected part. We\nalso prove that if the sample size is below the threshold, then the likelihood\nfunction is almost surely unbounded. Our work clarifies, in particular, that\nstandard likelihood inference is applicable to sparse high-dimensional models\neven if they feature feedback loops.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 22:45:13 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Drton", "Mathias", ""], ["Fox", "Christopher", ""], ["K\u00e4ufl", "Andreas", ""], ["Pouliot", "Guillaume", ""]]}, {"id": "1805.05816", "submitter": "Zacharie Naulet", "authors": "Zacharie Naulet", "title": "Adaptive Bayesian density estimation in sup-norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of deriving adaptive posterior rates of\ncontraction on $\\mathbb{L}^{\\infty}$ balls in density estimation. Although it\nis known that log-density priors can achieve optimal rates when the true\ndensity is sufficiently smooth, adaptive rates were still to be proven. Here we\nestablish that the so-called spike-and-slab prior can achieve adaptive and\noptimal posterior contraction rates. Along the way, we prove a generic\n$\\mathbb{L}^{\\infty}$ contraction result for log-density priors with\nindependent wavelet coefficients. Interestingly, our approach is different from\nprevious works on $\\mathbb{L}^{\\infty}$ contraction and is reminiscent of the\nclassical test-based approach used in Bayesian nonparametrics. Moreover, we\nrequire no lower bound on the smoothness of the true density, albeit the rates\nare deteriorated by an extra $\\log(n)$ factor in the case of low smoothness.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 14:40:32 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 19:15:13 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 16:41:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Naulet", "Zacharie", ""]]}, {"id": "1805.05857", "submitter": "Fabrizio Antenucci", "authors": "Fabrizio Antenucci, Silvio Franz, Pierfrancesco Urbani and Lenka\n  Zdeborov\\'a", "title": "Glassy nature of the hard phase in inference problems", "comments": "10 pages, 3 figures", "journal-ref": "Phys. Rev. X 9, 011020 (2019)", "doi": "10.1103/PhysRevX.9.011020", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithmically hard phase was described in a range of inference problems:\neven if the signal can be reconstructed with a small error from an information\ntheoretic point of view, known algorithms fail unless the noise-to-signal ratio\nis sufficiently small. This hard phase is typically understood as a metastable\nbranch of the dynamical evolution of message passing algorithms. In this work\nwe study the metastable branch for a prototypical inference problem, the\nlow-rank matrix factorization, that presents a hard phase. We show that for\nnoise-to-signal ratios that are below the information theoretic threshold, the\nposterior measure is composed of an exponential number of metastable glassy\nstates and we compute their entropy, called the complexity. We show that this\nglassiness extends even slightly below the algorithmic threshold below which\nthe well-known approximate message passing (AMP) algorithm is able to closely\nreconstruct the signal. Counter-intuitively, we find that the performance of\nthe AMP algorithm is not improved by taking into account the glassy nature of\nthe hard phase. This result provides further evidence that the hard phase in\ninference problems is algorithmically impenetrable for some deep computational\nreasons that remain to be uncovered.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 15:39:36 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 12:09:49 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 12:26:38 GMT"}, {"version": "v4", "created": "Wed, 9 Jan 2019 12:29:00 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Antenucci", "Fabrizio", ""], ["Franz", "Silvio", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1805.06098", "submitter": "Patrick L. Combettes", "authors": "Patrick L. Combettes and Christian L. M\\\"uller", "title": "Perspective Maximum Likelihood-Type Estimation via Proximal\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an optimization model for maximum likelihood-type estimation\n(M-estimation) that generalizes a large class of existing statistical models,\nincluding Huber's concomitant M-estimator, Owen's Huber/Berhu concomitant\nestimator, the scaled lasso, support vector machine regression, and penalized\nestimation with structured sparsity. The model, termed perspective\nM-estimation, leverages the observation that convex M-estimators with\nconcomitant scale as well as various regularizers are instances of perspective\nfunctions. Such functions are amenable to proximal analysis, which leads to\nprincipled and provably convergent optimization algorithms via proximal\nsplitting. Using a geometrical approach based on duality, we derive novel\nproximity operators for several perspective functions of interest. Numerical\nexperiments on synthetic and real-world data illustrate the broad applicability\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 02:35:55 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 19:05:24 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Combettes", "Patrick L.", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "1805.06144", "submitter": "Takayuki Kawashima", "authors": "Takayuki Kawashima and Hironori Fujisawa", "title": "On Difference Between Two Types of $\\gamma$-divergence for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\gamma$-divergence is well-known for having strong robustness against\nheavy contamination. By virtue of this property, many applications via the\n$\\gamma$-divergence have been proposed. There are two types of \\gd\\ for\nregression problem, in which the treatments of base measure are different. In\nthis paper, we compare them and pointed out a distinct difference between these\ntwo divergences under heterogeneous contamination where the outlier ratio\ndepends on the explanatory variable. One divergence has the strong robustness\nunder heterogeneous contamination. The other does not have in general, but has\nwhen the parametric model of the response variable belongs to a location-scale\nfamily in which the scale does not depend on the explanatory variables or under\nhomogeneous contamination where the outlier ratio does not depend on the\nexplanatory variable. \\citet{hung.etal.2017} discussed the strong robustness in\na logistic regression model with an additional assumption that the tuning\nparameter $\\gamma$ is sufficiently large. The results obtained in this paper\nhold for any parametric model without such an additional assumption.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 06:09:00 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Kawashima", "Takayuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1805.06278", "submitter": "Yuuya Yoshida", "authors": "Yuuya Yoshida, Man-Hong Yung, and Masahito Hayashi", "title": "Optimal Mechanism for Randomized Responses under Universally Composable\n  Security Measure", "comments": "12 pages, 6 figures; changed the title and revised the abstract,\n  introduction, and section 6", "journal-ref": "Proc. of 2019 IEEE International Symposium on Information Theory\n  (ISIT), pp. 547-551", "doi": "10.1109/ISIT.2019.8849268", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of analyzing a global property of private data through\nrandomized responses subject to a certain rule, where private data are used for\nanother cryptographic protocol, e.g., authentication. For this problem, the\nsecurity of private data was evaluated by a universally composable security\nmeasure, which can be regarded as $(0,\\delta)$-differential privacy. Here we\nfocus on the trade-off between the global accuracy and a universally composable\nsecurity measure, and derive an optimal solution to the trade-off problem. More\nprecisely, we adopt the Fisher information of a certain distribution family as\nthe estimation accuracy of a global property and impose\n$(0,\\delta)$-differential privacy on a randomization mechanism protecting\nprivate data. Finally, we maximize the Fisher information under the\n$(0,\\delta)$-differential privacy constraint and obtain an optimal mechanism\nexplicitly.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:47:55 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 15:41:15 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 15:52:38 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 04:43:04 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Yoshida", "Yuuya", ""], ["Yung", "Man-Hong", ""], ["Hayashi", "Masahito", ""]]}, {"id": "1805.06328", "submitter": "Panpan Zhang", "authors": "Panpan Zhang and Dipak K. Dey", "title": "The degree profile and Gini index of random caterpillar trees", "comments": null, "journal-ref": "Prob. Eng. Inf. Sci. 33 (2019) 511-527", "doi": "10.1017/S0269964818000475", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the degree profile and Gini index of random\ncaterpillar trees (RCTs). We consider RCTs which evolve in two different\nmanners: uniform and nonuniform. The degrees of the vertices on the central\npath (i.e., the degree profile) of a uniform RCT follow a multinomial\ndistribution. For nonuniform RCTs, we focus on those growing in the fashion of\npreferential attachment. We develop methods based on stochastic recurrences to\ncompute the exact expectations and the dispersion matrix of the degree\nvariables. A generalized P\\'{o}lya urn model is exploited to determine the\nexact joint distribution of these degree variables. We apply the methods from\ncombinatorics to prove that the asymptotic distribution is Dirichlet. In\naddition, we propose a new type of Gini index to quantitatively distinguish the\nevolutionary characteristics of the two classes of RCTs. We present the results\nvia several numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 17:47:04 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Zhang", "Panpan", ""], ["Dey", "Dipak K.", ""]]}, {"id": "1805.06364", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Adaptive elastic-net selection in a quantile model with diverging number\n  of variable groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications of the linear model, the explanatory variables are very\noften naturally grouped, the most common example being the multivariate\nvariance analysis. In the present paper, a quantile model with structure group\nis considered, the number of groups can diverge with sample size. We introduce\nand study the adaptive elastic-net group estimator, for improving the parameter\nestimation accuracy. This method allows automatic selection, with a probability\nconverging to one, of significant groups and further the non zero parameter\nestimators are asymptotically normal. The convergence rate of the adaptive\nelastic-net group quantile estimator is also obtained, rate which depends on\nthe number of groups. In order to put the estimation method into practice, an\nalgorithm based on the subgradient method is proposed and implemented. The\nMonte Carlo simulations show that the adaptive elastic-net group quantile\nestimations are more accurate that other existing group estimations in the\nliterature. Moreover, the numerical study confirms the theoretical results and\nthe usefulness of the proposed estimation method.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:14:21 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:17:28 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 21:34:01 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1805.06639", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:09 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06640", "submitter": "Ze Jin", "authors": "Ze Jin, Xiaohan Yan, David S. Matteson", "title": "Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:48 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Yan", "Xiaohan", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06663", "submitter": "Rahul Mukerjee", "authors": "Fatemah A. Alquallaf, S. Huda and Rahul Mukerjee", "title": "Causal Inference from Strip-Plot Designs in a Potential Outcomes\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strip-plot designs are very useful when the treatments have a factorial\nstructure and the factors levels are hard-to-change. We develop a\nrandomization-based theory of causal inference from such designs in a potential\noutcomes framework. For any treatment contrast, an unbiased estimator is\nproposed, an expression for its sampling variance is worked out, and a\nconservative estimator of the sampling variance is obtained. This conservative\nestimator has a nonnegative bias, and becomes unbiased under between-block\nadditivity, a condition milder than Neymannian strict additivity. A minimaxity\nproperty of this variance estimator is also established. Simulation results on\nthe coverage of resulting confidence intervals lend support to theoretical\nconsiderations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 09:16:55 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Alquallaf", "Fatemah A.", ""], ["Huda", "S.", ""], ["Mukerjee", "Rahul", ""]]}, {"id": "1805.06811", "submitter": "Salem Said", "authors": "Jialun Zhou and Salem Said", "title": "Fast, asymptotically efficient, recursive estimation in a Riemannian\n  manifold", "comments": "updated version of draft submitted for publication, currently under\n  review", "journal-ref": null, "doi": "10.3390/e21101021", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimisation in Riemannian manifolds, especially the Riemannian\nstochastic gradient method, has attracted much recent attention. The present\nwork applies stochastic optimisation to the task of recursive estimation of a\nstatistical parameter which belongs to a Riemannian manifold. Roughly, this\ntask amounts to stochastic minimisation of a statistical divergence function.\nThe following problem is considered : how to obtain fast, asymptotically\nefficient, recursive estimates, using a Riemannian stochastic optimisation\nalgorithm with decreasing step sizes? In solving this problem, several original\nresults are introduced. First, without any convexity assumptions on the\ndivergence function, it is proved that, with an adequate choice of step sizes,\nthe algorithm computes recursive estimates which achieve a fast non-asymptotic\nrate of convergence. Second, the asymptotic normality of these recursive\nestimates is proved, by employing a novel linearisation technique. Third, it is\nproved that, when the Fisher information metric is used to guide the algorithm,\nthese recursive estimates achieve an optimal asymptotic rate of convergence, in\nthe sense that they become asymptotically efficient. These results, while\nrelatively familiar in the Euclidean context, are here formulated and proved\nfor the first time, in the Riemannian context. In addition, they are\nillustrated with a numerical application to the recursive estimation of\nelliptically contoured distributions.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 15:09:10 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 11:13:58 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 16:51:29 GMT"}, {"version": "v4", "created": "Mon, 13 Aug 2018 21:25:37 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhou", "Jialun", ""], ["Said", "Salem", ""]]}, {"id": "1805.06833", "submitter": "Lidia  Rejt\\\"o K", "authors": "Mikl\\'os Arat\\'o, Vill\\\"o Csisz\\'ar, Bal\\'azs Gerencs\\'er, Gy\\\"orgy\n  Michaletzky, L\\'idia Rejt\\\"o, G\\'abor Sz\\'ekely, G\\'abor Tusn\\'ady and\n  Katalin Varga", "title": "An extension of the Plancherel measure", "comments": "10 pages + 13pages of figures, figures are contained in 3 pdf files,\n  the first one contains the 11 pages of Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a distribution in the unite square and having iid sample from it the\nfirst question what a statistician might do to test the hypothesis that the\nsample is iid. For this purpose an extension of the Plancherel measure is\nintroduced. Recent literature on asymptotic behavior of Plancherel measure is\ndiscussed with extension to the new set up. Models for random permutations are\ndescribed and the power of different tests is compared.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 15:54:51 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Arat\u00f3", "Mikl\u00f3s", ""], ["Csisz\u00e1r", "Vill\u00f6", ""], ["Gerencs\u00e9r", "Bal\u00e1zs", ""], ["Michaletzky", "Gy\u00f6rgy", ""], ["Rejt\u00f6", "L\u00eddia", ""], ["Sz\u00e9kely", "G\u00e1bor", ""], ["Tusn\u00e1dy", "G\u00e1bor", ""], ["Varga", "Katalin", ""]]}, {"id": "1805.06964", "submitter": "Rapha\\\"el Deswarte", "authors": "Rapha\\\"el Deswarte, Guillaume Lecu\\'e", "title": "Minimax regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approach to regularization is to design norms enhancing smoothness\nor sparsity and then to use this norm or some power of this norm as a\nregularization function. The choice of the regularization function (for\ninstance a power function) in terms of the norm is mostly dictated by\ncomputational purpose rather than theoretical considerations.\n  In this work, we design regularization functions that are motivated by\ntheoretical arguments. To that end we introduce a concept of optimal\nregularization called \"minimax regularization\" and, as a proof of concept, we\nshow how to construct such a regularization function for the $\\ell_1^d$ norm\nfor the random design setup. We develop a similar construction for the\ndeterministic design setup. It appears that the resulting regularized\nprocedures are different from the one used in the LASSO in both setups.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 20:49:31 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Deswarte", "Rapha\u00ebl", ""], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "1805.06970", "submitter": "Rong Ma", "authors": "Rong Ma, T. Tony Cai and Hongzhe Li", "title": "Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic\n  Regression Models", "comments": "Typos corrected", "journal-ref": "Journal of the American Statistical Association (2019)", "doi": "10.1080/01621459.2019.1699421", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional logistic regression is widely used in analyzing data with\nbinary outcomes. In this paper, global testing and large-scale multiple testing\nfor the regression coefficients are considered in both single- and\ntwo-regression settings. A test statistic for testing the global null\nhypothesis is constructed using a generalized low-dimensional projection for\nbias correction and its asymptotic null distribution is derived. A lower bound\nfor the global testing is established, which shows that the proposed test is\nasymptotically minimax optimal over some sparsity range. For testing the\nindividual coefficients simultaneously, multiple testing procedures are\nproposed and shown to control the false discovery rate (FDR) and falsely\ndiscovered variables (FDV) asymptotically. Simulation studies are carried out\nto examine the numerical performance of the proposed tests and their\nsuperiority over existing methods. The testing procedures are also illustrated\nby analyzing a data set of a metabolomics study that investigates the\nassociation between fecal metabolites and pediatric Crohn's disease and the\neffects of treatment on such associations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 21:11:34 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 19:32:51 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 23:22:57 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 22:05:12 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2020 19:08:45 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1805.07238", "submitter": "Luai Al Labadi Dr.", "authors": "Luai Al-Labadi", "title": "The Two-Sample Problem Via Relative Belief Ratio", "comments": "25 pages. arXiv admin note: text overlap with arXiv:1606.08106; text\n  overlap with arXiv:1411.3427, arXiv:1609.06418 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a new Bayesian approach to the two-sample problem. More\nspecifically, let $x=(x_1,\\ldots,x_{n_1})$ and $y=(y_1,\\ldots,y_{n_2})$ be two\nindependent samples coming from unknown distributions $F$ and $G$,\nrespectively. The goal is to test the null hypothesis $\\mathcal{H}_0:~F=G$\nagainst all possible alternatives. First, a Dirichlet process prior for $F$ and\n$G$ is considered. Then the change of their Cram\\'{e}r-von Mises distance from\na priori to a posteriori is compared through the relative belief ratio. Many\ntheoretical properties of the procedure have been developed and several\nexamples have been discussed, in which the proposed approach shows excellent\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 06:49:01 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Al-Labadi", "Luai", ""]]}, {"id": "1805.07418", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Le Li", "title": "Sequential Learning of Principal Curves: Summarizing Data Streams on the\n  Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When confronted with massive data streams, summarizing data with dimension\nreduction methods such as PCA raises theoretical and algorithmic pitfalls.\nPrincipal curves act as a nonlinear generalization of PCA and the present paper\nproposes a novel algorithm to automatically and sequentially learn principal\ncurves from data streams. We show that our procedure is supported by regret\nbounds with optimal sublinear remainder terms. A greedy local search\nimplementation (called \\texttt{slpc}, for Sequential Learning Principal Curves)\nthat incorporates both sleeping experts and multi-armed bandit ingredients is\npresented, along with its regret computation and performance on synthetic and\nreal-life data.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 19:49:13 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 20:03:35 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Guedj", "Benjamin", ""], ["Li", "Le", ""]]}, {"id": "1805.07495", "submitter": "Peng Zheng", "authors": "Jihun Yun, Peng Zheng, Eunho Yang, Aurelie Lozano, Aleksandr Aravkin", "title": "M-estimation with the Trimmed l1 Penalty", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional estimators with the trimmed $\\ell_1$ penalty, which\nleaves the $h$ largest parameter entries penalty-free. While optimization\ntechniques for this nonconvex penalty have been studied, the statistical\nproperties have not yet been analyzed. We present the first statistical\nanalyses for $M$-estimation and characterize support recovery, $\\ell_\\infty$\nand $\\ell_2$ error of the trimmed $\\ell_1$ estimates as a function of the\ntrimming parameter $h$. Our results show different regimes based on how $h$\ncompares to the true support size. Our second contribution is a new algorithm\nfor the trimmed regularization problem, which has the same theoretical\nconvergence rate as the difference of convex (DC) algorithms, but in practice\nis faster and finds lower objective values. Empirical evaluation of $\\ell_1$\ntrimming for sparse linear regression and graphical model estimation indicate\nthat trimmed $\\ell_1$ can outperform vanilla $\\ell_1$ and non-convex\nalternatives. Our last contribution is to show that the trimmed penalty is\nbeneficial beyond $M$-estimation, and yields promising results for two deep\nlearning tasks: input structures recovery and network sparsification.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 02:39:34 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 21:01:39 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Yun", "Jihun", ""], ["Zheng", "Peng", ""], ["Yang", "Eunho", ""], ["Lozano", "Aurelie", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1805.07682", "submitter": "Alnur Ali", "authors": "Alnur Ali and Ryan J. Tibshirani", "title": "The Generalized Lasso Problem and Uniqueness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study uniqueness in the generalized lasso problem, where the penalty is\nthe $\\ell_1$ norm of a matrix $D$ times the coefficient vector. We derive a\nbroad result on uniqueness that places weak assumptions on the predictor matrix\n$X$ and penalty matrix $D$; the implication is that, if $D$ is fixed and its\nnull space is not too large (the dimension of its null space is at most the\nnumber of samples), and $X$ and response vector $y$ jointly follow an\nabsolutely continuous distribution, then the generalized lasso problem has a\nunique solution almost surely, regardless of the number of predictors relative\nto the number of samples. This effectively generalizes previous uniqueness\nresults for the lasso problem (which corresponds to the special case $D=I$).\nFurther, we extend our study to the case in which the loss is given by the\nnegative log-likelihood from a generalized linear model. In addition to\nuniqueness results, we derive results on the local stability of generalized\nlasso solutions that might be of interest in their own right.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 00:11:56 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 13:35:19 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ali", "Alnur", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1805.07800", "submitter": "Takumi Saegusa", "authors": "Takumi Saegusa", "title": "Large Sample Theory for Merged Data from Multiple Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop large sample theory for merged data from multiple sources. Main\nstatistical issues treated in this paper are (1) the same unit potentially\nappears in multiple datasets from overlapping data sources, (2) duplicated\nitems are not identified, and (3) a sample from the same data source is\ndependent due to sampling without replacement. We propose and study a new\nweighted empirical process and extend empirical process theory to a dependent\nand biased sample with duplication. Specifically, we establish the uniform law\nof large numbers and uniform central limit theorem over a class of functions\nalong with several empirical process results under conditions identical to\nthose in the i.i.d. setting. As applications, we study infinite-dimensional\nM-estimation and develop its consistency, rates of convergence, and asymptotic\nnormality. Our theoretical results are illustrated with simulation studies and\na real data example.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 17:18:15 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Saegusa", "Takumi", ""]]}, {"id": "1805.07849", "submitter": "Andrew Ying", "authors": "Andrew Ying, Ronghui Xu and James Murphy", "title": "Two-Stage Residual Inclusion under the Additive Hazards Model - An\n  Instrumental Variable Approach with Application to SEER-Medicare Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable is an essential tool for addressing unmeasured\nconfounding in observational studies. Two stage predictor substitution (2SPS)\nestimator and two stage residual inclusion(2SRI) are two commonly used\napproaches in applying instrumental variables. Recently 2SPS was studied under\nthe additive hazards model in the presence of competing risks of time-to-events\ndata, where linearity was assumed for the relationship between the treatment\nand the instrument variable. This assumption may not be the most appropriate\nwhen we have binary treatments. In this paper, we consider the 2SRI estimator\nunder the additive hazards model for general survival data and in the presence\nof competing risks, which allows generalized linear models for the relation\nbetween the treatment and the instrumental variable. We derive the asymptotic\nproperties including a closed-form asymptotic variance estimate for the 2SRI\nestimator. We carry out numerical studies in finite samples, and apply our\nmethodology to the linked Surveillance, Epidemiology and End Results (SEER) -\nMedicare database comparing radical prostatectomy versus conservative treatment\nin early-stage prostate cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 00:20:42 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 00:26:34 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Ying", "Andrew", ""], ["Xu", "Ronghui", ""], ["Murphy", "James", ""]]}, {"id": "1805.08020", "submitter": "Philip Thompson", "authors": "Philip Thompson and Arnak S. Dalalyan", "title": "Restricted eigenvalue property for corrupted Gaussian designs", "comments": "21 pages. Some updates and corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the construction of tractable robust estimators via convex\nrelaxations, we present conditions on the sample size which guarantee an\naugmented notion of Restricted Eigenvalue-type condition for Gaussian designs.\nSuch a notion is suitable for high-dimensional robust inference in a Gaussian\nlinear model and a multivariate Gaussian model when samples are corrupted by\noutliers either in the response variable or in the design matrix. Our proof\ntechnique relies on simultaneous lower and upper bounds of two random bilinear\nforms with very different behaviors. Such simultaneous bounds are used for\nbalancing the interaction between the parameter vector and the estimated\ncorruption vector as well as for controlling the presence of corruption in the\ndesign. Our technique has the advantage of not relying on known bounds of the\nextreme singular values of the associated Gaussian ensemble nor on the use of\nmutual incoherence arguments. A relevant consequence of our analysis, compared\nto prior work, is that a significantly sharper restricted eigenvalue constant\ncan be obtained under weaker assumptions. In particular, the sparsity of the\nunknown parameter and the number of outliers are allowed to be completely\nindependent of each other.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 12:43:31 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 00:39:33 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 14:54:12 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 19:50:52 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Thompson", "Philip", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1805.08140", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Aryeh Kontorovich", "title": "A New Lower Bound for Agnostic Learning with Sample Compression Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a tight characterization of the worst-case rates for the excess\nrisk of agnostic learning with sample compression schemes and for uniform\nconvergence for agnostic sample compression schemes. In particular, we find\nthat the optimal rates of convergence for size-$k$ agnostic sample compression\nschemes are of the form $\\sqrt{\\frac{k \\log(n/k)}{n}}$, which contrasts with\nagnostic learning with classes of VC dimension $k$, where the optimal rates are\nof the form $\\sqrt{\\frac{k}{n}}$.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:47:31 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Hanneke", "Steve", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1805.08283", "submitter": "James M. Flegal", "authors": "Ying Liu and James M. Flegal", "title": "Weighted batch means estimators in Markov chain Monte Carlo", "comments": "52 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a family of weighted batch means variance estimators,\nwhich are computationally efficient and can be conveniently applied in\npractice. The focus is on Markov chain Monte Carlo simulations and estimation\nof the asymptotic covariance matrix in the Markov chain central limit theorem,\nwhere conditions ensuring strong consistency are provided. Finite sample\nperformance is evaluated through auto-regressive, Bayesian spatial-temporal,\nand Bayesian logistic regression examples, where the new estimators show\nsignificant computational gains with a minor sacrifice in variance compared\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:39:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Liu", "Ying", ""], ["Flegal", "James M.", ""]]}, {"id": "1805.08342", "submitter": "Jongha Jon Ryu", "authors": "J. Jon Ryu, Shouvik Ganguly, Young-Han Kim, Yung-Kyun Noh, Daniel D.\n  Lee", "title": "Nearest neighbor density functional estimation from inverse Laplace\n  transform", "comments": "52 pages, 4 figures. Submitted to the IEEE Transactions on\n  Information Theory. Minor fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to $L_2$-consistent estimation of a general density functional\nusing $k$-nearest neighbor distances is proposed, where the functional under\nconsideration is in the form of the expectation of some function $f$ of the\ndensities at each point. The estimator is designed to be asymptotically\nunbiased, using the convergence of the normalized volume of a $k$-nearest\nneighbor ball to a Gamma distribution in the large-sample limit, and naturally\ninvolves the inverse Laplace transform of a scaled version of the function $f.$\nSome instantiations of the proposed estimator recover existing $k$-nearest\nneighbor based estimators of Shannon and R\\'enyi entropies and\nKullback--Leibler and R\\'enyi divergences, and discover new consistent\nestimators for many other functionals such as logarithmic entropies and\ndivergences. The $L_2$-consistency of the proposed estimator is established for\na broad class of densities for general functionals, and the convergence rate in\nmean squared error is established as a function of the sample size for smooth,\nbounded densities.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 01:19:30 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 06:18:22 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 00:05:06 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ryu", "J. Jon", ""], ["Ganguly", "Shouvik", ""], ["Kim", "Young-Han", ""], ["Noh", "Yung-Kyun", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1805.08836", "submitter": "Shashank Singh", "authors": "Shashank Singh, Ananya Uppal, Boyue Li, Chun-Liang Li, Manzil Zaheer,\n  Barnab\\'as P\\'oczos", "title": "Nonparametric Density Estimation under Adversarial Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax convergence rates of nonparametric density estimation under\na large class of loss functions called \"adversarial losses\", which, besides\nclassical $\\mathcal{L}^p$ losses, includes maximum mean discrepancy (MMD),\nWasserstein distance, and total variation distance. These losses are closely\nrelated to the losses encoded by discriminator networks in generative\nadversarial networks (GANs). In a general framework, we study how the choice of\nloss and the assumed smoothness of the underlying density together determine\nthe minimax rate. We also discuss implications for training GANs based on deep\nReLU networks, and more general connections to learning implicit generative\nmodels in a minimax statistical sense.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:55:37 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:32:38 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Singh", "Shashank", ""], ["Uppal", "Ananya", ""], ["Li", "Boyue", ""], ["Li", "Chun-Liang", ""], ["Zaheer", "Manzil", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1805.08883", "submitter": "Yaroslav Mukhin", "authors": "Yaroslav Mukhin", "title": "Sensitivity of Regular Estimators", "comments": "35 pages, 5 figures, includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies local asymptotic relationship between two scalar\nestimates. We define sensitivity of a target estimate to a control estimate to\nbe the directional derivative of the target functional with respect to the\ngradient direction of the control functional. Sensitivity according to the\ninformation metric on the model manifold is the asymptotic covariance of\nregular efficient estimators. Sensitivity according to a general policy metric\non the model manifold can be obtained from influence functions of regular\nefficient estimators. Policy sensitivity has a local counterfactual\ninterpretation, where the ceteris paribus change to a counterfactual\ndistribution is specified by the combination of a control parameter and a\nRiemannian metric on the model manifold.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 21:57:42 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mukhin", "Yaroslav", ""]]}, {"id": "1805.08920", "submitter": "Tianyang Li", "authors": "Tianyang Li, Anastasios Kyrillidis, Liu Liu, Constantine Caramanis", "title": "Approximate Newton-based statistical inference using only stochastic\n  gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel statistical inference framework for convex empirical risk\nminimization, using approximate stochastic Newton steps. The proposed algorithm\nis based on the notion of finite differences and allows the approximation of a\nHessian-vector product from first-order information. In theory, our method\nefficiently computes the statistical error covariance in $M$-estimation, both\nfor unregularized convex learning problems and high-dimensional LASSO\nregression, without using exact second order information, or resampling the\nentire data set. We also present a stochastic gradient sampling scheme for\nstatistical inference in non-i.i.d. time series analysis, where we sample\ncontiguous blocks of indices. In practice, we demonstrate the effectiveness of\nour framework on large-scale machine learning problems, that go even beyond\nconvexity: as a highlight, our work can be used to detect certain adversarial\nattacks on neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:07:47 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:23:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Li", "Tianyang", ""], ["Kyrillidis", "Anastasios", ""], ["Liu", "Liu", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1805.08926", "submitter": "Hiroki Masuda", "authors": "Alexandre Brouste, Hiroki Masuda", "title": "Efficient estimation of stable Levy process with symmetric jumps", "comments": "Minor typos fixed in pages 5 and 7, specified in red; the original\n  version published from Statistical Inference for Stochastic Processes", "journal-ref": null, "doi": "10.1007/s11203-018-9181-0", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient estimation of a non-Gaussian stable Levy process with drift and\nsymmetric jumps observed at high frequency is considered. For this statistical\nexperiment, the local asymptotic normality of the likelihood is proved with a\nnon-singular Fisher information matrix through the use of a non-diagonal\nnorming matrix. The asymptotic normality and efficiency of a sequence of roots\nof the associated likelihood equation are shown as well. Moreover, we show that\na simple preliminary method of moments can be used as an initial estimator of a\nscoring procedure, thereby conveniently enabling us to bypass numerically\ndemanding likelihood optimization. Our simulation results show that the\none-step estimator can exhibit quite similar finite-sample performance as the\nmaximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:37:05 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 07:17:25 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Brouste", "Alexandre", ""], ["Masuda", "Hiroki", ""]]}, {"id": "1805.08929", "submitter": "Andrew Back Dr", "authors": "Andrew D. Back, Daniel Angus and Janet Wiles", "title": "Determining the Number of Samples Required to Estimate Entropy in\n  Natural Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating the Shannon entropy for symbolic sequences has been widely\nconsidered in many fields. For descriptive statistical problems such as\nestimating the N-gram entropy of English language text, a common approach is to\nuse as much data as possible to obtain progressively more accurate estimates.\nHowever in some instances, only short sequences may be available. This gives\nrise to the question of how many samples are needed to compute entropy. In this\npaper, we examine this problem and propose a method for estimating the number\nof samples required to compute Shannon entropy for a set of ranked symbolic\nnatural events. The result is developed using a modified Zipf-Mandelbrot law\nand the Dvoretzky-Kiefer-Wolfowitz inequality, and we propose an algorithm\nwhich yields an estimate for the minimum number of samples required to obtain\nan estimate of entropy with a given confidence level and degree of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:44:29 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Back", "Andrew D.", ""], ["Angus", "Daniel", ""], ["Wiles", "Janet", ""]]}, {"id": "1805.08956", "submitter": "Kwangjun Ahn", "authors": "Kwangjun Ahn, Kangwook Lee, and Changho Suh", "title": "Hypergraph Spectral Clustering in the Weighted Stochastic Block Model", "comments": "16 pages; 3 figures", "journal-ref": "October 2018 special issue on \"Information-Theoretic Methods in\n  Data Acquisition, Analysis, and Processing\" of the IEEE Journal of Selected\n  Topics in Signal Processing", "doi": "10.1109/JSTSP.2018.2837638", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a celebrated algorithm that partitions objects based\non pairwise similarity information. While this approach has been successfully\napplied to a variety of domains, it comes with limitations. The reason is that\nthere are many other applications in which only \\emph{multi}-way similarity\nmeasures are available. This motivates us to explore the multi-way measurement\nsetting. In this work, we develop two algorithms intended for such setting:\nHypergraph Spectral Clustering (HSC) and Hypergraph Spectral Clustering with\nLocal Refinement (HSCLR). Our main contribution lies in performance analysis of\nthe poly-time algorithms under a random hypergraph model, which we name the\nweighted stochastic block model, in which objects and multi-way measures are\nmodeled as nodes and weights of hyperedges, respectively. Denoting by $n$ the\nnumber of nodes, our analysis reveals the following: (1) HSC outputs a\npartition which is better than a random guess if the sum of edge weights (to be\nexplained later) is $\\Omega(n)$; (2) HSC outputs a partition which coincides\nwith the hidden partition except for a vanishing fraction of nodes if the sum\nof edge weights is $\\omega(n)$; and (3) HSCLR exactly recovers the hidden\npartition if the sum of edge weights is on the order of $n \\log n$. Our results\nimprove upon the state of the arts recently established under the model and\nthey firstly settle the order-wise optimal results for the binary edge weight\ncase. Moreover, we show that our results lead to efficient sketching algorithms\nfor subspace clustering, a computer vision application. Lastly, we show that\nHSCLR achieves the information-theoretic limits for a special yet practically\nrelevant model, thereby showing no computational barrier for the case.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 04:26:35 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ahn", "Kwangjun", ""], ["Lee", "Kangwook", ""], ["Suh", "Changho", ""]]}, {"id": "1805.08992", "submitter": "Joseph Mur\\'e", "authors": "Joseph Mur\\'e", "title": "Propriety of the reference posterior distribution in Gaussian Process\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal article, Berger, De Oliveira and Sans\\'o (2001) compare several\nobjective prior distributions for the parameters of Gaussian Process regression\nmodels with isotropic correlation kernel. The reference prior distribution\nstands out among them insofar as it always leads to a proper posterior. They\nprove this result for rough correlation kernels - Spherical, Exponential with\npower $q<2$, Mat\\'ern with smoothness $\\nu<1$. This paper provides a proof for\nsmooth correlation kernels - Exponential with power $q=2$, Mat\\'ern with\nsmoothness $\\nu \\geqslant 1$, Rational Quadratic - along with tail rates of the\nreference prior for these kernels.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 07:48:00 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 11:51:11 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 14:36:59 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mur\u00e9", "Joseph", ""]]}, {"id": "1805.09174", "submitter": "Pierre Gaillard", "authors": "Pierre Gaillard (SIERRA), Olivier Wintenberger (LPSM UMR 8001)", "title": "Efficient online algorithms for fast-rate regret bounds under sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online convex optimization problem. In the setting of\narbitrary sequences and finite set of parameters, we establish a new fast-rate\nquantile regret bound. Then we investigate the optimization into the L1-ball by\ndiscretizing the parameter space. Our algorithm is projection free and we\npropose an efficient solution by restarting the algorithm on adaptive\ndiscretization grids. In the adversarial setting, we develop an algorithm that\nachieves several rates of convergence with different dependencies on the\nsparsity of the objective. In the i.i.d. setting, we establish new risk bounds\nthat are adaptive to the sparsity of the problem and to the regularity of the\nrisk (ranging from a rate 1 / $\\sqrt T$ for general convex risk to 1 /T for\nstrongly convex risk). These results generalize previous works on sparse online\nlearning. They are obtained under a weak assumption on the risk\n({\\L}ojasiewicz's assumption) that allows multiple optima which is crucial when\ndealing with degenerate situations.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:49:11 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Gaillard", "Pierre", "", "SIERRA"], ["Wintenberger", "Olivier", "", "LPSM UMR 8001"]]}, {"id": "1805.09236", "submitter": "Jordan Awan", "authors": "Jordan Awan and Aleksandra Slavkovic", "title": "Differentially Private Uniformly Most Powerful Tests for Binomial Data", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive uniformly most powerful (UMP) tests for simple and one-sided\nhypotheses for a population proportion within the framework of Differential\nPrivacy (DP), optimizing finite sample performance. We show that in general, DP\nhypothesis tests for exchangeable data can always be expressed as a function of\nthe empirical distribution. Using this structure, we prove a `Neyman-Pearson\nlemma' for binomial data under DP, where the DP-UMP only depends on the sample\nsum. Our tests can also be stated as a post-processing of a random variable,\nwhose distribution we coin \"Truncated-Uniform-Laplace\" (Tulap), a\ngeneralization of the Staircase and discrete Laplace distributions.\nFurthermore, we obtain exact p-values, which are easily computed in terms of\nthe Tulap random variable. We show that our results also apply to\ndistribution-free hypothesis tests for continuous data. Our simulation results\ndemonstrate that our tests have exact type I error, and are more powerful than\ncurrent techniques.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:51:43 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Awan", "Jordan", ""], ["Slavkovic", "Aleksandra", ""]]}, {"id": "1805.09782", "submitter": "Justin Curry", "authors": "Justin Curry, Sayan Mukherjee, Katharine Turner", "title": "How Many Directions Determine a Shape and other Sufficiency Results for\n  Two Topological Transforms", "comments": "October 10, 2019 version is 30 pages with several additional details\n  for modified proposition and theorem statements. Formula for the bound on the\n  number of directions has changed slightly", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two topological transforms based on Euler calculus:\nthe Persistent Homology Transform (PHT) and the Euler Characteristic Transform\n(ECT). Both of these transforms are of interest for their mathematical\nproperties as well as their applications to science and engineering, because\nthey provide a way of summarizing shapes in a topological, yet quantitative,\nway. Both transforms take a shape, viewed as a tame subset $M$ of\n$\\mathbb{R}^d$, and associates to each direction $v\\in S^{d-1}$ a shape summary\nobtained by scanning $M$ in the direction $v$. These shape summaries are either\npersistence diagrams or piecewise constant integer valued functions called\nEuler curves. By using an inversion theorem of Schapira, we show that both\ntransforms are injective on the space of shapes---each shape has a unique\ntransform. We also introduce a notion of a \"generic shape\", which we prove can\nbe uniquely identified up to an element of $O(d)$ by using the pushforward of\nthe Lebesgue measure from the sphere to the space of Euler curves. Finally, our\nmain result proves that any shape in a certain uncountable set of non-axis\naligned shapes can be specified using only finitely many Euler curves.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:05:05 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:24:20 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Curry", "Justin", ""], ["Mukherjee", "Sayan", ""], ["Turner", "Katharine", ""]]}, {"id": "1805.09871", "submitter": "Dong Xia", "authors": "Dong Xia", "title": "Confidence Region of Singular Subspaces for Low-rank Matrix Regression", "comments": "typos are corrected and motivating examples are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix regression refers to the instances of recovering a low-rank\nmatrix based on specially designed measurements and the corresponding noisy\noutcomes. In the last decade, numerous statistical methodologies have been\ndeveloped for efficiently recovering the unknown low-rank matrices. However, in\nsome applications, the unknown singular subspace is scientifically more\nimportant than the low-rank matrix itself. In this article, we revisit the\nlow-rank matrix regression model and introduce a two-step procedure to\nconstruct confidence regions of the singular subspace. The procedure involves\nthe de-biasing for the typical low-rank estimators after which we calculate the\nempirical singular vectors. We investigate the distribution of the joint\nprojection distance between the empirical singular subspace and the unknown\ntrue singular subspace. We specifically prove the asymptotical normality of the\njoint projection distance with data-dependent centering and normalization when\n$r^{3/2}(m_1+m_2)^{3/2}=o(n/\\log n)$ where $m_1, m_2$ denote the matrix row and\ncolumn sizes, $r$ is the rank and $n$ is the number of independent random\nmeasurements. Consequently, we propose data-dependent confidence regions of the\ntrue singular subspace which attains any pre-determined confidence level\nasymptotically. In addition, non-asymptotical convergence rates are also\nestablished. Numerical results are presented to demonstrate the merits of our\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:53:58 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:04:17 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 14:16:22 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Xia", "Dong", ""]]}, {"id": "1805.09873", "submitter": "Charles R Doss", "authors": "Charles R. Doss", "title": "Concave regression: value-constrained estimation and likelihood\n  ratio-based inference", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a likelihood ratio statistic for forming hypothesis tests and\nconfidence intervals for a nonparametrically estimated univariate regression\nfunction, based on the shape restriction of concavity (alternatively,\nconvexity). Dealing with the likelihood ratio statistic requires studying an\nestimator satisfying a null hypothesis, that is, studying a concave\nleast-squares estimator satisfying a further equality constraint. We study this\nnull hypothesis least-squares estimator (NLSE) here, and use it to study our\nlikelihood ratio statistic. The NLSE is the solution to a convex program, and\nwe find a set of inequality and equality constraints that characterize the\nsolution. We also study a corresponding limiting version of the convex program\nbased on observing a Brownian motion with drift. The solution to the limit\nproblem is a stochastic process. We study the optimality conditions for the\nsolution to the limit problem and find that they match those we derived for the\nsolution to the finite sample problem. This allows us to show the limit\nstochastic process yields the limit distribution of the (finite sample) NLSE.\nWe conjecture that the likelihood ratio statistic is asymptotically pivotal,\nmeaning that it has a limit distribution with no nuisance parameters to be\nestimated, which makes it a very effective tool for this difficult inference\nproblem. We provide a partial proof of this conjecture, and we also provide\nsimulation evidence strongly supporting this conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:57:12 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 06:19:14 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Doss", "Charles R.", ""]]}, {"id": "1805.09937", "submitter": "Tatsushi Oka", "authors": "Dukpa Kim, Tatsushi Oka, Francisco Estrada, Pierre Perron", "title": "Inference Related to Common Breaks in a Multivariate System with Joined\n  Segmented Trends with Applications to Global and Hemispheric Temperatures", "comments": "42 pages, 8 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What transpires from recent research is that temperatures and radiative\nforcing seem to be characterized by a linear trend with two changes in the rate\nof growth. The first occurs in the early 60s and indicates a very large\nincrease in the rate of growth of both temperature and radiative forcing\nseries. This was termed as the \"onset of sustained global warming\". The second\nis related to the more recent so-called hiatus period, which suggests that\ntemperatures and total radiative forcing have increased less rapidly since the\nmid-90s compared to the larger rate of increase from 1960 to 1990. There are\ntwo issues that remain unresolved. The first is whether the breaks in the slope\nof the trend functions of temperatures and radiative forcing are common. This\nis important because common breaks coupled with the basic science of climate\nchange would strongly suggest a causal effect from anthropogenic factors to\ntemperatures. The second issue relates to establishing formally via a proper\ntesting procedure that takes into account the noise in the series, whether\nthere was indeed a `hiatus period' for temperatures since the mid 90s. This is\nimportant because such a test would counter the widely held view that the\nhiatus is the product of natural internal variability. Our paper provides tests\nrelated to both issues. The results show that the breaks in temperatures and\nradiative forcing are common and that the hiatus is characterized by a\nsignificant decrease in their rate of growth. The statistical results are of\nindependent interest and applicable more generally.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 00:13:42 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Kim", "Dukpa", ""], ["Oka", "Tatsushi", ""], ["Estrada", "Francisco", ""], ["Perron", "Pierre", ""]]}, {"id": "1805.09948", "submitter": "Guang Cheng", "authors": "Meimei Liu, Zuofeng Shang, Guang Cheng", "title": "How Many Machines Can We Use in Parallel Computing for Kernel Ridge\n  Regression?", "comments": "This work extends the work in arXiv:1512.09226 to random and\n  multivariate design", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to solve a basic problem in distributed statistical\ninference: how many machines can we use in parallel computing? In kernel ridge\nregression, we address this question in two important settings: nonparametric\nestimation and hypothesis testing. Specifically, we find a range for the number\nof machines under which optimal estimation/testing is achievable. The employed\nempirical processes method provides a unified framework, that allows us to\nhandle various regression problems (such as thin-plate splines and\nnonparametric additive regression) under different settings (such as\nunivariate, multivariate and diverging-dimensional designs). It is worth noting\nthat the upper bounds of the number of machines are proven to be un-improvable\n(upto a logarithmic factor) in two important cases: smoothing spline regression\nand Gaussian RKHS regression. Our theoretical findings are backed by thorough\nnumerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 02:00:53 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 05:01:25 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 03:10:17 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Liu", "Meimei", ""], ["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1805.09950", "submitter": "Guang Cheng", "authors": "Meimei Liu, Guang Cheng", "title": "Early Stopping for Nonparametric Testing", "comments": "To appear in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early stopping of iterative algorithms is an algorithmic regularization\nmethod to avoid over-fitting in estimation and classification. In this paper,\nwe show that early stopping can also be applied to obtain the minimax optimal\ntesting in a general non-parametric setup. Specifically, a Wald-type test\nstatistic is obtained based on an iterated estimate produced by functional\ngradient descent algorithms in a reproducing kernel Hilbert space. A notable\ncontribution is to establish a \"sharp\" stopping rule: when the number of\niterations achieves an optimal order, testing optimality is achievable;\notherwise, testing optimality becomes impossible. As a by-product, a similar\nsharpness result is also derived for minimax optimal estimation under early\nstopping studied in [11] and [19]. All obtained results hold for various kernel\nclasses, including Sobolev smoothness classes and Gaussian kernel classes.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 02:05:54 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 21:37:46 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 05:10:04 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Liu", "Meimei", ""], ["Cheng", "Guang", ""]]}, {"id": "1805.10074", "submitter": "Loucas Pillaud-Vivien", "authors": "Loucas Pillaud-Vivien (SIERRA, PSL), Alessandro Rudi (SIERRA, PSL),\n  Francis Bach (SIERRA, PSL)", "title": "Statistical Optimality of Stochastic Gradient Descent on Hard Learning\n  Problems through Multiple Passes", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS), Dec 2018,\n  Montr{\\'e}al, Canada. 2018", "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic gradient descent (SGD) for least-squares regression\nwith potentially several passes over the data. While several passes have been\nwidely reported to perform practically better in terms of predictive\nperformance on unseen data, the existing theoretical analysis of SGD suggests\nthat a single pass is statistically optimal. While this is true for\nlow-dimensional easy problems, we show that for hard problems, multiple passes\nlead to statistically optimal predictions while single pass does not; we also\nshow that in these hard models, the optimal number of passes over the data\nincreases with sample size. In order to define the notion of hardness and show\nthat our predictive performances are optimal, we consider potentially\ninfinite-dimensional models and notions typically associated to kernel methods,\nnamely, the decay of eigenvalues of the covariance matrix of the features and\nthe complexity of the optimal predictor as measured through the covariance\nmatrix. We illustrate our results on synthetic experiments with non-linear\nkernel methods and on a classical benchmark with a linear model.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 10:45:09 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 14:12:22 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 07:13:47 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Pillaud-Vivien", "Loucas", "", "SIERRA, PSL"], ["Rudi", "Alessandro", "", "SIERRA, PSL"], ["Bach", "Francis", "", "SIERRA, PSL"]]}, {"id": "1805.10353", "submitter": "David Koops", "authors": "A. Goldenshluger, D.T. Koops", "title": "Nonparametric estimation of service time characteristics in\n  infinite-server queues with nonstationary Poisson input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a mathematical framework for estimation of the service\ntime distribution and the expected service time of an infinite-server queueing\nsystem with a nonhomogeneous Poisson arrival process, in the case of partial\ninformation, where only the number of busy servers are observed over time. The\nproblem is reduced to a statistical deconvolution problem, which is solved by\nusing Laplace transform techniques and kernels for regularization. Upper bounds\non the mean squared error of the proposed estimators are derived. Some concrete\nsimulation experiments are performed to illustrate how the method can be\napplied and to provide some insight in the practical performance.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 20:22:46 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Goldenshluger", "A.", ""], ["Koops", "D. T.", ""]]}, {"id": "1805.10406", "submitter": "Simon Du", "authors": "Simon S. Du, Yining Wang, Sivaraman Balakrishnan, Pradeep Ravikumar,\n  Aarti Singh", "title": "Robust Nonparametric Regression under Huber's $\\epsilon$-contamination\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-parametric regression problem under Huber's\n$\\epsilon$-contamination model, in which an $\\epsilon$ fraction of observations\nare subject to arbitrary adversarial noise. We first show that a simple local\nbinning median step can effectively remove the adversary noise and this median\nestimator is minimax optimal up to absolute constants over the H\\\"{o}lder\nfunction class with smoothness parameters smaller than or equal to 1.\nFurthermore, when the underlying function has higher smoothness, we show that\nusing local binning median as pre-preprocessing step to remove the adversarial\nnoise, then we can apply any non-parametric estimator on top of the medians. In\nparticular we show local median binning followed by kernel smoothing and local\npolynomial regression achieve minimaxity over H\\\"{o}lder and Sobolev classes\nwith arbitrary smoothness parameters. Our main proof technique is a decoupled\nanalysis of adversary noise and stochastic noise, which can be potentially\napplied to other robust estimation problems. We also provide numerical results\nto verify the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 00:39:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Balakrishnan", "Sivaraman", ""], ["Ravikumar", "Pradeep", ""], ["Singh", "Aarti", ""]]}, {"id": "1805.10594", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Shirshendu Chatterjee", "title": "Spectral Clustering for Multiple Sparse Networks: I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much of the focus of statistical works on networks has been on\nstatic networks, multiple networks are currently becoming more common among\nnetwork data sets. Usually, a number of network data sets, which share some\nform of connection between each other are known as multiple or multi-layer\nnetworks. We consider the problem of identifying the common community\nstructures for multiple networks. We consider extensions of the spectral\nclustering methods for the multiple sparse networks, and give theoretical\nguarantee that the spectral clustering methods produce consistent community\ndetection in case of both multiple stochastic block model and multiple\ndegree-corrected block models. The methods are shown to work under sufficiently\nmild conditions on the number of multiple networks to detect associative\ncommunity structures, even if all the individual networks are sparse and most\nof the individual networks are below community detectability threshold. We\nreinforce the validity of the theoretical results via simulations too.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 08:01:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Chatterjee", "Shirshendu", ""]]}, {"id": "1805.10633", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Assessing transfer functions in control systems", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with control systems, it is useful and even necessary to assess\nthe performance of underlying transfer functions. The functions may or may not\nbe linear, may or may not be even monotonic. In addition, they may have\nstructural breaks and other abberations that require monitoring and\nquantification to aid decision making. The present paper develops such a\nmethodology, which is based on an index of increase that naturally arises as\nthe solution to an optimization problem. We show theoretically and illustrate\nnumerically that the empirical counterpart of the index needs to be used with\ngreat care and in-depth knowledge of the problem at hand in order to achieve\ndesired large-sample properties, such as consistency.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 14:35:07 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 15:02:52 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1805.10721", "submitter": "Bai Jiang", "authors": "Bai Jiang, Qiang Sun, Jianqing Fan", "title": "Bernstein's inequality for general Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish Bernstein inequalities for functions of general\n(general-state-space, not necessarily reversible) Markov chains. These\ninequalities achieve sharp variance proxies and recover the classical\nBernstein's inequality under independence. The key analysis lies in upper\nbounding the operator norm of a perturbed Markov transition kernel by the\nlimiting operator norm of a sequence of finite-rank and perturbed Markov\ntransition kernels. For each finite-rank and perturbed Markov kernel, we bound\nits norm by the sum of two convex functions. One coincides with what delivers\nthe classical Bernstein's inequality, and the other reflects the influence of\nthe Markov dependence. A convex analysis on conjugates of these two functions\nthen derives our Bernstein inequalities.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 01:00:07 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 00:18:00 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jiang", "Bai", ""], ["Sun", "Qiang", ""], ["Fan", "Jianqing", ""]]}, {"id": "1805.10736", "submitter": "Gene Yoo", "authors": "Gene Ryan Yoo and Houman Owhadi", "title": "De-noising by thresholding operator adapted wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Donoho and Johnstone proposed a method from reconstructing an unknown smooth\nfunction $u$ from noisy data $u+\\zeta$ by translating the empirical wavelet\ncoefficients of $u+\\zeta$ towards zero. We consider the situation where the\nprior information on the unknown function $u$ may not be the regularity of $u$\nbut that of $ \\L u$ where $\\L$ is a linear operator (such as a PDE or a graph\nLaplacian). We show that the approximation of $u$ obtained by thresholding the\ngamblet (operator adapted wavelet) coefficients of $u+\\zeta$ is near minimax\noptimal (up to a multiplicative constant), and with high probability, its\nenergy norm (defined by the operator) is bounded by that of $u$ up to a\nconstant depending on the amplitude of the noise. Since gamblets can be\ncomputed in $\\mathcal{O}(N \\operatorname{polylog} N)$ complexity and are\nlocalized both in space and eigenspace, the proposed method is of near-linear\ncomplexity and generalizable to non-homogeneous noise.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 02:23:57 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Yoo", "Gene Ryan", ""], ["Owhadi", "Houman", ""]]}, {"id": "1805.10767", "submitter": "Pan Zhou", "authors": "Pan Zhou and Jiashi Feng", "title": "Understanding Generalization and Optimization Performance of Deep CNNs", "comments": "This paper was accepted by ICML. It has 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work aims to provide understandings on the remarkable success of deep\nconvolutional neural networks (CNNs) by theoretically analyzing their\ngeneralization performance and establishing optimization guarantees for\ngradient descent based training algorithms. Specifically, for a CNN model\nconsisting of $l$ convolutional layers and one fully connected layer, we prove\nthat its generalization error is bounded by\n$\\mathcal{O}(\\sqrt{\\dt\\widetilde{\\varrho}/n})$ where $\\theta$ denotes freedom\ndegree of the network parameters and\n$\\widetilde{\\varrho}=\\mathcal{O}(\\log(\\prod_{i=1}^{l}\\rwi{i}\n(\\ki{i}-\\si{i}+1)/p)+\\log(\\rf))$ encapsulates architecture parameters including\nthe kernel size $\\ki{i}$, stride $\\si{i}$, pooling size $p$ and parameter\nmagnitude $\\rwi{i}$. To our best knowledge, this is the first generalization\nbound that only depends on $\\mathcal{O}(\\log(\\prod_{i=1}^{l+1}\\rwi{i}))$,\ntighter than existing ones that all involve an exponential term like\n$\\mathcal{O}(\\prod_{i=1}^{l+1}\\rwi{i})$. Besides, we prove that for an\narbitrary gradient descent algorithm, the computed approximate stationary point\nby minimizing empirical risk is also an approximate stationary point to the\npopulation risk. This well explains why gradient descent training algorithms\nusually perform sufficiently well in practice. Furthermore, we prove the\none-to-one correspondence and convergence guarantees for the non-degenerate\nstationary points between the empirical and population risks. It implies that\nthe computed local minimum for the empirical risk is also close to a local\nminimum for the population risk, thus ensuring the good generalization\nperformance of CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 04:39:24 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zhou", "Pan", ""], ["Feng", "Jiashi", ""]]}, {"id": "1805.10791", "submitter": "Olivier Collier", "authors": "Olivier Collier, La\\\"etitia Comminges and Alexandre B. Tsybakov", "title": "On estimation of nonsmooth functionals of sparse normal means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimation of the value N_gamma(\\theta) = sum(i=1)^d\n|\\theta_i|^gamma for 0 < gamma <= 1 based on the observations y_i = \\theta_i +\n\\epsilon\\xi_i, i = 1,...,d, where \\theta = (\\theta_1,...,\\theta_d) are unknown\nparameters, \\epsilon>0 is known, and \\xi_i are i.i.d. standard normal random\nvariables. We prove that the non-asymptotic minimax risk on the class B_0(s) of\ns-sparse vectors and we propose estimators achieving the minimax rate.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:15:45 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 07:05:16 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Collier", "Olivier", ""], ["Comminges", "La\u00ebtitia", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1805.10801", "submitter": "Markus Bachmayr", "authors": "Benjamin Arras, Markus Bachmayr and Albert Cohen", "title": "Sequential sampling for optimal weighted least squares approximations in\n  hierarchical spaces", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating an unknown function $u\\in\nL^2(D,\\rho)$ from its evaluations at given sampling points $x^1,\\dots,x^n\\in\nD$, where $D\\subset \\mathbb{R}^d$ is a general domain and $\\rho$ is a\nprobability measure. The approximation is picked in a linear space $V_m$ where\n$m=\\dim(V_m)$ and computed by a weighted least squares method. Recent results\nshow the advantages of picking the sampling points at random according to a\nwell-chosen probability measure $\\mu$ that depends both on $V_m$ and $\\rho$.\nWith such a random design, the weighted least squares approximation is proved\nto be stable with high probability, and having precision comparable to that of\nthe exact $L^2(D,\\rho)$-orthonormal projection onto $V_m$, in a near-linear\nsampling regime $n\\sim{m\\log m}$. The present paper is motivated by the\nadaptive approximation context, in which one typically generates a nested\nsequence of spaces $(V_m)_{m\\geq1}$ with increasing dimension. Although the\nmeasure $\\mu=\\mu_m$ changes with $V_m$, it is possible to recycle the\npreviously generated samples by interpreting $\\mu_m$ as a mixture between\n$\\mu_{m-1}$ and an update measure $\\sigma_m$. Based on this observation, we\ndiscuss sequential sampling algorithms that maintain the stability and\napproximation properties uniformly over all spaces $V_m$. Our main result is\nthat the total number of computed sample at step $m$ remains of the order\n$m\\log{m}$ with high probability. Numerical experiments confirm this analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:54:42 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Arras", "Benjamin", ""], ["Bachmayr", "Markus", ""], ["Cohen", "Albert", ""]]}, {"id": "1805.10843", "submitter": "Patr\\'icia Leone Espinheira", "authors": "Patr\\'icia Espinheira, Alisson de Oliveira Silva", "title": "Nonlinear Simplex Regression Models", "comments": "25 pg. 12 Fig", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simplex regression model in which both the mean\nand the dispersion parameters are related to covariates by nonlinear\npredictors. We provide closed-form expressions for the score function, for\nFisher's information matrix and its inverse. Some diagnostic measures are\nintroduced. We propose a residual, obtained using Fisher's scoring iterative\nscheme for the estimation of the parameters that index the regression nonlinear\npredictor to the mean response and numerically evaluate its behaviour. We also\nderive the appropriate matrices for assessing local influence on the parameter\nestimates under different perturbation schemes. We also proposed a scheme for\nthe choice of starting values for the Fisher's iterative scheme for nonlinear\nsimplex models. The diagnostic techniques were applied on actual data. The\nlocal influence analyses reveal that the simplex models can be a modeling\nalternative more robust to influential cases than the beta regression models,\nboth to linear and nonlinear models.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 09:47:53 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Espinheira", "Patr\u00edcia", ""], ["Silva", "Alisson de Oliveira", ""]]}, {"id": "1805.10939", "submitter": "Dmitry Kobak", "authors": "Dmitry Kobak, Jonathan Lomond, Benoit Sanchez", "title": "Optimal ridge penalty for real-world high-dimensional data can be zero\n  or negative due to the implicit ridge regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A conventional wisdom in statistical learning is that large models require\nstrong regularization to prevent overfitting. Here we show that this rule can\nbe violated by linear regression in the underdetermined $n\\ll p$ situation\nunder realistic conditions. Using simulations and real-life high-dimensional\ndata sets, we demonstrate that an explicit positive ridge penalty can fail to\nprovide any improvement over the minimum-norm least squares estimator.\nMoreover, the optimal value of ridge penalty in this situation can be negative.\nThis happens when the high-variance directions in the predictor space can\npredict the response variable, which is often the case in the real-world\nhigh-dimensional data. In this regime, low-variance directions provide an\nimplicit ridge regularization and can make any further positive ridge penalty\ndetrimental. We prove that augmenting any linear model with random covariates\nand using minimum-norm estimator is asymptotically equivalent to adding the\nridge penalty. We use a spiked covariance model as an analytically tractable\nexample and prove that the optimal ridge penalty in this case is negative when\n$n\\ll p$.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:25:37 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 20:17:07 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 20:21:54 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 23:02:28 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kobak", "Dmitry", ""], ["Lomond", "Jonathan", ""], ["Sanchez", "Benoit", ""]]}, {"id": "1805.11214", "submitter": "Liuhua Peng", "authors": "Song Xi Chen, Liuhua Peng", "title": "Distributed Statistical Inference for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers distributed statistical inference for general symmetric\nstatistics %that encompasses the U-statistics and the M-estimators in the\ncontext of massive data where the data can be stored at multiple platforms in\ndifferent locations. In order to facilitate effective computation and to avoid\nexpensive communication among different platforms, we formulate distributed\nstatistics which can be conducted over smaller data blocks. The statistical\nproperties of the distributed statistics are investigated in terms of the mean\nsquare error of estimation and asymptotic distributions with respect to the\nnumber of data blocks. In addition, we propose two distributed bootstrap\nalgorithms which are computationally effective and are able to capture the\nunderlying distribution of the distributed statistics. Numerical simulation and\nreal data applications of the proposed approaches are provided to demonstrate\nthe empirical performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 02:02:15 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Chen", "Song Xi", ""], ["Peng", "Liuhua", ""]]}, {"id": "1805.11268", "submitter": "Saeid Rezakhah", "authors": "Toktam Valizadeh and Saeid Rezakhah", "title": "Flexible Cholesky GARCH model with time dependent coefficients", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of instantaneous dependence among several variable is important in many\nof the high-dimensional sciences. Multivariate GARCH models are as a standard\napproach for modelling time-varying covariance matrix such phenomena. Cholesky\nGARCH is one of these approaches where the time-varying covariance matrix can\nbe written parsimoniously, containing variance components through a diagonal\nmatrix and dependency components through a unit lower triangular matrix with\nregression coefficients as entries. In this paper, we proposed a stochastic\nstructure for dependency components in Cholesky GARCH model by considering\nlinear regression model as a state-space model and using kalman filtering for\nestimating regression coefficients. We find that the MSE of stochastic Cholesky\nGARCH model is smaller than the MSE of other models also show that the\nstochastic Cholesky GARCH has better performance in compare to another models\nbased on MAE and MSE criterions for the real data.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 06:57:20 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Valizadeh", "Toktam", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1805.11311", "submitter": "Igor Kheifets", "authors": "Igor L. Kheifets and Pentti J. Saikkonen", "title": "Stationarity and ergodicity of vector STAR models", "comments": "Accepted to Econometric Reviews", "journal-ref": null, "doi": "10.1080/07474938.2019.1651489", "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth transition autoregressive models are widely used to capture\nnonlinearities in univariate and multivariate time series. Existence of\nstationary solution is typically assumed, implicitly or explicitly. In this\npaper we describe conditions for stationarity and ergodicity of vector STAR\nmodels. The key condition is that the joint spectral radius of certain matrices\nis below 1, which is not guaranteed if only separate spectral radii are below\n1. Our result allows to use recently introduced toolboxes from computational\nmathematics to verify the stationarity and ergodicity of vector STAR models.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 08:54:24 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 08:00:08 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 21:37:24 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kheifets", "Igor L.", ""], ["Saikkonen", "Pentti J.", ""]]}, {"id": "1805.11386", "submitter": "Gilles Stoltz", "authors": "Pierre Gaillard (SIERRA), S\\'ebastien Gerchinovitz (IMT), Malo Huard\n  (LMO), Gilles Stoltz (LMO)", "title": "Uniform regret bounds over $R^d$ for the sequential linear regression\n  problem with the square loss", "comments": "Proceedings of ALT'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of online linear regression for arbitrary\ndeterministic sequences, with the square loss. We are interested in the aim set\nby Bartlett et al. (2015): obtain regret bounds that hold uniformly over all\ncompetitor vectors. When the feature sequence is known at the beginning of the\ngame, they provided closed-form regret bounds of $2d B^2 \\ln T +\n\\mathcal{O}_T(1)$, where $T$ is the number of rounds and $B$ is a bound on the\nobservations. Instead, we derive bounds with an optimal constant of $1$ in\nfront of the $d B^2 \\ln T$ term. In the case of sequentially revealed features,\nwe also derive an asymptotic regret bound of $d B^2 \\ln T$ for any individual\nsequence of features and bounded observations. All our algorithms are variants\nof the online non-linear ridge regression forecaster, either with a\ndata-dependent regularization or with almost no regularization.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 12:26:14 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 11:39:00 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Gaillard", "Pierre", "", "SIERRA"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT"], ["Huard", "Malo", "", "LMO"], ["Stoltz", "Gilles", "", "LMO"]]}, {"id": "1805.11414", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Inference for ergodic diffusions plus noise", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.04462", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We research adaptive maximum likelihood-type estimation for an ergodic\ndiffusion process where the observation is contaminated by noise. This\nmethodology leads to the asymptotic independence of the estimators for the\nvariance of observation noise, the diffusion parameter and the drift one of the\nlatent diffusion process. Moreover, it can lessen the computational burden\ncompared to simultaneous maximum likelihood-type estimation. In addition to\nadaptive estimation, we propose a test to see if noise exists or not, and\nanalyse real data as the example such that data contains observation noise with\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 15:12:14 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1805.11505", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings, Yingying Fan and Richard J. Samworth", "title": "Classification with imperfect training labels", "comments": "44 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of imperfect training data labels on the performance of\nclassification methods. In a general setting, where the probability that an\nobservation in the training dataset is mislabelled may depend on both the\nfeature vector and the true label, we bound the excess risk of an arbitrary\nclassifier trained with imperfect labels in terms of its excess risk for\npredicting a noisy label. This reveals conditions under which a classifier\ntrained with imperfect labels remains consistent for classifying uncorrupted\ntest data points. Furthermore, under stronger conditions, we derive detailed\nasymptotic properties for the popular $k$-nearest neighbour ($k$nn), support\nvector machine (SVM) and linear discriminant analysis (LDA) classifiers. One\nconsequence of these results is that the knn and SVM classifiers are robust to\nimperfect training labels, in the sense that the rate of convergence of the\nexcess risks of these classifiers remains unchanged; in fact, our theoretical\nand empirical results even show that in some cases, imperfect labels may\nimprove the performance of these methods. On the other hand, the LDA classifier\nis shown to be typically inconsistent in the presence of label noise unless the\nprior probabilities of each class are equal. Our theoretical results are\nsupported by a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:36:06 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 10:58:08 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 10:53:29 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Fan", "Yingying", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1805.11556", "submitter": "Marcos Carreira", "authors": "Marcos Costa Santos Carreira", "title": "An exact solution for choosing the largest measurement from a sample\n  drawn from an uniform distribution", "comments": "25 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"Recognizing the Maximum of a Sequence\", Gilbert and Mosteller analyze a\nfull information game where n measurements from an uniform distribution are\ndrawn and a player (knowing n) must decide at each draw whether or not to\nchoose that draw. The goal is to maximize the probability of choosing the draw\nthat corresponds to the maximum of the sample. In their calculations of the\noptimal strategy, the optimal probability and the asymptotic probability, they\nassume that after a draw x the probability that the next i numbers are all\nsmaller than x is $x^i$; but this fails to recognize that continuing the game\n(not choosing a draw because it is lower than a cutoff and waiting for the next\ndraw) conditions the distribution of the following i numbers such that their\nexpected maximum is higher then i/(i+1). The problem is now redefined with each\ndraw leading to a win, a false positive loss, a false negative loss and a\ncontinuation. An exact formula for these probabilities is deduced, both for the\ngeneral case of n-1 different indifference numbers (assuming 0 as the last\ncutoff) and the particular case of the same indifference number for all cutoffs\nbut the last. An approximation is found that preserves the main characteristics\nof the optimal solution (slow decay of win probability, quick decay of false\npositives and linear decay of false negatives). This new solution and the\noriginal Gilbert and Mosteller formula are compared against simulations, and\ntheir asymptotic behavior is studied.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 16:05:03 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Carreira", "Marcos Costa Santos", ""]]}, {"id": "1805.11643", "submitter": "Liu Liu", "authors": "Liu Liu, Yanyao Shen, Tianyang Li, Constantine Caramanis", "title": "High Dimensional Robust Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel -- and to the best of our knowledge, the first --\nalgorithm for high dimensional sparse regression with constant fraction of\ncorruptions in explanatory and/or response variables. Our algorithm recovers\nthe true sparse parameters with sub-linear sample complexity, in the presence\nof a constant fraction of arbitrary corruptions. Our main contribution is a\nrobust variant of Iterative Hard Thresholding. Using this, we provide accurate\nestimators: when the covariance matrix in sparse regression is identity, our\nerror guarantee is near information-theoretically optimal. We then deal with\nrobust sparse regression with unknown structured covariance matrix. We propose\na filtering algorithm which consists of a novel randomized outlier removal\ntechnique for robust sparse mean estimation that may be of interest in its own\nright: the filtering algorithm is flexible enough to deal with unknown\ncovariance. Also, it is orderwise more efficient computationally than the\nellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves\nthe best known (and first) error guarantee. We demonstrate the effectiveness on\nlarge-scale sparse regression problems with arbitrary corruptions.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 18:33:23 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 06:04:34 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 19:34:15 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Liu", ""], ["Shen", "Yanyao", ""], ["Li", "Tianyang", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1805.11747", "submitter": "Igor Cialenco", "authors": "Ziteng Cheng, Igor Cialenco and Ruoting Gong", "title": "Bayesian Estimations for Diagonalizable Bilinear SPDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to study the parameter estimation problem,\nusing the Bayesian methodology, for the drift coefficient of some linear\n(parabolic) SPDEs driven by a multiplicative noise of special structure. We\ntake the spectral approach by assuming that one path of the first $N$ Fourier\nmodes of the solution is continuously observed over a finite time interval.\nFirst, we show that the model is regular and fits into classical local\nasymptotic normality framework, and thus the MLE and the Bayesian estimators\nare weakly consistent, asymptotically normal, efficient, and asymptotically\nequivalent in the class of loss functions with polynomial growth. Secondly, and\nmainly, we prove a Bernstein-Von Mises type result, that strengthens the\nexisting results in the literature, and that also allows to investigate the\nBayesian type estimators with respect to a larger class of priors and loss\nfunctions than that covered by classical asymptotic theory. In particular, we\nprove strong consistency and asymptotic normality of Bayesian estimators in the\nclass of loss functions of at most exponential growth. Finally, we present some\nnumerical examples that illustrate the obtained theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 23:53:40 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 20:14:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cheng", "Ziteng", ""], ["Cialenco", "Igor", ""], ["Gong", "Ruoting", ""]]}, {"id": "1805.11905", "submitter": "Andriy Olenko", "authors": "Huda Mohammed Alomari, Antoine Ayache, Myriam Fradon, Andriy Olenko", "title": "Estimation of seasonal long-memory parameters", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies seasonal long-memory processes with Gegenbauer-type\nspectral densities. Estimates for singularity location and long-memory\nparameters based on general filter transforms are proposed. It is proved that\nthe estimates are almost surely convergent to the true values of parameters.\nSolutions of the estimation equations are studied and adjusted statistics are\nproposed. Numerical results are presented to confirm the theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 11:26:37 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Alomari", "Huda Mohammed", ""], ["Ayache", "Antoine", ""], ["Fradon", "Myriam", ""], ["Olenko", "Andriy", ""]]}, {"id": "1805.12400", "submitter": "Anthea Monod", "authors": "Anthea Monod, Bo Lin, Ruriko Yoshida, and Qiwen Kang", "title": "Tropical Geometry of Phylogenetic Tree Space: A Statistical Perspective", "comments": "28 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG math.CO math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees are the fundamental mathematical representation of\nevolutionary processes in biology. As data objects, they are characterized by\nthe challenges associated with \"big data,\" as well as the complication that\ntheir discrete geometric structure results in a non-Euclidean phylogenetic tree\nspace, which poses computational and statistical limitations. We propose and\nstudy a novel framework to study sets of phylogenetic trees based on tropical\ngeometry. In particular, we focus on characterizing our framework for\nstatistical analyses of evolutionary biological processes represented by\nphylogenetic trees. Our setting exhibits analytic, geometric, and topological\nproperties that are desirable for theoretical studies in probability and\nstatistics, as well as increased computational efficiency over the current\nstate-of-the-art. We demonstrate our approach on seasonal influenza data.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 09:52:30 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 14:37:05 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2019 00:24:08 GMT"}, {"version": "v4", "created": "Sun, 15 Dec 2019 23:16:16 GMT"}, {"version": "v5", "created": "Sat, 21 Dec 2019 21:11:55 GMT"}, {"version": "v6", "created": "Thu, 13 Aug 2020 20:35:19 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Monod", "Anthea", ""], ["Lin", "Bo", ""], ["Yoshida", "Ruriko", ""], ["Kang", "Qiwen", ""]]}, {"id": "1805.12430", "submitter": "Eni Musta", "authors": "Hendrik P. Lopuha\\\"a and Eni Musta", "title": "Central limit theorems for the $L_p$-error of smooth isotonic estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic behavior of the $L_p$-distance between a\nmonotone function on a compact interval and a smooth estimator of this\nfunction. Our main result is a central limit theorem for the $L_p$-error of\nsmooth isotonic estimators obtained by smoothing a Grenander-type estimator or\nisotonizing the ordinary kernel estimator. As a preliminary result we establish\na similar result for ordinary kernel estimators. Our results are obtained in a\ngeneral setting, which includes estimation of a monotone density, regression\nfunction and hazard rate. We also perform a simulation study for testing\nmonotonicity on the basis of the $L_2$-distance between the kernel estimator\nand the smoothed Grenander-type estimator.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 12:00:09 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Lopuha\u00e4", "Hendrik P.", ""], ["Musta", "Eni", ""]]}, {"id": "1805.12472", "submitter": "Uri Hadar", "authors": "Uri Hadar and Ofer Shayevitz", "title": "Distributed Estimation of Gaussian Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed estimation problem in which two remotely located\nparties, Alice and Bob, observe an unlimited number of i.i.d. samples\ncorresponding to two different parts of a random vector. Alice can send $k$\nbits on average to Bob, who in turn wants to estimate the cross-correlation\nmatrix between the two parts of the vector. In the case where the parties\nobserve jointly Gaussian scalar random variables with an unknown correlation\n$\\rho$, we obtain two constructive and simple unbiased estimators attaining a\nvariance of $(1-\\rho^2)/(2k\\ln 2)$, which coincides with a known but\nnon-constructive random coding result of Zhang and Berger. We extend our\napproach to the vector Gaussian case, which has not been treated before, and\nconstruct an estimator that is uniformly better than the scalar estimator\napplied separately to each of the correlations. We then show that the Gaussian\nperformance can essentially be attained even when the distribution is\ncompletely unknown. This in particular implies that in the general problem of\ndistributed correlation estimation, the variance can decay at least as $O(1/k)$\nwith the number of transmitted bits. This behavior, however, is not tight: we\ngive an example of a rich family of distributions for which local samples\nreveal essentially nothing about the correlations, and where a slightly\nmodified estimator attains a variance of $2^{-\\Omega(k)}$.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 13:52:23 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 13:44:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Hadar", "Uri", ""], ["Shayevitz", "Ofer", ""]]}, {"id": "1805.12571", "submitter": "Felix Rios", "authors": "Jimmy Olsson, Tetyana Pavlenko and Felix L. Rios", "title": "Bayesian learning of weakly structural Markov graph laws using\n  sequential Monte Carlo methods", "comments": "34 pages, 8 figures", "journal-ref": "Electron. J. Statist., Volume 13, Number 2 (2019), 2865-2897", "doi": "10.1214/19-EJS1585", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequential sampling methodology for weakly structural Markov\nlaws, arising naturally in a Bayesian structure learning context for\ndecomposable graphical models. As a key component of our suggested approach, we\nshow that the problem of graph estimation, which in general lacks natural\nsequential interpretation, can be recast into a sequential setting by proposing\na recursive Feynman-Kac model that generates a flow of junction tree\ndistributions over a space of increasing dimensions. We focus on particle McMC\nmethods to provide samples on this space, in particular on particle Gibbs (PG),\nas it allows for generating McMC chains with global moves on an underlying\nspace of decomposable graphs. To further improve the PG mixing properties, we\nincorporate a systematic refreshment step implemented through direct sampling\nfrom a backward kernel. The theoretical properties of the algorithm are\ninvestigated, showing that the proposed refreshment step improves the\nperformance in terms of asymptotic variance of the estimated distribution. The\nsuggested sampling methodology is illustrated through a collection of numerical\nexamples demonstrating high accuracy in Bayesian graph structure learning in\nboth discrete and continuous graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 17:26:47 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 05:45:16 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 11:42:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Olsson", "Jimmy", ""], ["Pavlenko", "Tetyana", ""], ["Rios", "Felix L.", ""]]}]