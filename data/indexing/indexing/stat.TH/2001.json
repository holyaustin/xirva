[{"id": "2001.00093", "submitter": "Chi Zhang", "authors": "Gregory Rice and Chi Zhang (University of Waterloo)", "title": "Consistency of Binary Segmentation For Multiple Change-Points Estimation\n  With Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For sequentially observed functional data exhibiting multiple change points\nin the mean function, we establish consistency results for the estimated number\nand locations of the change points based on the norm of the functional CUSUM\nprocess and standard binary segmentation. In addition to extending similar\nresults in Venkatraman (1992) and Fryzlewicz (2014) for scalar data to the\ngeneral Hilbert space setting, our main results are established without\nassuming the Gaussianity of the data, and under general linear process\nconditions on the model errors.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 22:06:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Rice", "Gregory", "", "University of Waterloo"], ["Zhang", "Chi", "", "University of Waterloo"]]}, {"id": "2001.00152", "submitter": "Yan Wang", "authors": "Rui Tuo, Yan Wang and C. F. Jeff Wu", "title": "On the Improved Rates of Convergence for Mat\\'ern-type Kernel Ridge\n  Regression, with Application to Calibration of Computer Models", "comments": "24pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression is an important nonparametric method for estimating\nsmooth functions. We introduce a new set of conditions, under which the actual\nrates of convergence of the kernel ridge regression estimator under both the\nL_2 norm and the norm of the reproducing kernel Hilbert space exceed the\nstandard minimax rates. An application of this theory leads to a new\nunderstanding of the Kennedy-O'Hagan approach for calibrating model parameters\nof computer simulation. We prove that, under certain conditions, the\nKennedy-O'Hagan calibration estimator with a known covariance function\nconverges to the minimizer of the norm of the residual function in the\nreproducing kernel Hilbert space.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 07:02:25 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Tuo", "Rui", ""], ["Wang", "Yan", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2001.00220", "submitter": "Siddharth Vishwanath", "authors": "Siddharth Vishwanath, Kenji Fukumizu, Satoshi Kuriki and Bharath\n  Sriperumbudur", "title": "Statistical Invariance of Betti Numbers in the Thermodynamic Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) has become a promising tool to uncover\nlow-dimensional features from high-dimensional data. Notwithstanding the\nadvantages afforded by TDA, its adoption in statistical methodology has been\nlimited by several reasons. In this paper we study the framework of topological\ninference through the lens of classical parametric inference in statistics.\nSuppose $\\mathcal{P} = \\{\\mathbb{P}_\\theta : \\theta \\in \\Theta\\}$ is a\nparametric family of distributions indexed by a set $\\Theta$, and $\\mathbb{X}_n\n= \\{\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_n\\}$ is observed iid at\nrandom from a distribution $\\mathbb{P}_\\theta$. The asymptotic behaviour of the\nBetti numbers associated with the \\v{C}ech complex of $\\mathbb{X}_n$ contain\nboth the topological and parametric information about the distribution of\npoints. We investigate necessary and sufficient conditions under which\ntopological inference is possible in this parametric setup.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 15:13:41 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Vishwanath", "Siddharth", ""], ["Fukumizu", "Kenji", ""], ["Kuriki", "Satoshi", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "2001.00397", "submitter": "Jiang Hu Dr.", "authors": "Qiuyan Zhang, Jiang Hu, Zhidong Bai", "title": "Modified Pillai's trace statistics for two high-dimensional sample\n  covariance matrices", "comments": "31 pages-to appear in Journal of Statistical Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study was to test the equality of two covariance matrices by\nusing modified Pillai's trace statistics under a high-dimensional framework,\ni.e., the dimension and sample sizes go to infinity proportionally. In this\npaper, we introduce two modified Pillai's trace statistics and obtain their\nasymptotic distributions under the null hypothesis. The benefits of the\nproposed statistics include the following: (1) the sample size can be smaller\nthan the dimensions; (2) the limiting distributions of the proposed statistics\nare universal; and (3) we do not restrict the structure of the population\ncovariance matrices. The theoretical results are established under mild and\npractical assumptions, and their properties are demonstrated numerically by\nsimulations and a real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 11:27:29 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Zhang", "Qiuyan", ""], ["Hu", "Jiang", ""], ["Bai", "Zhidong", ""]]}, {"id": "2001.00419", "submitter": "Weichi Wu", "authors": "Holger Dette, Weichi Wu", "title": "Prediction in locally stationary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an estimator for the high-dimensional covariance matrix of a\nlocally stationary process with a smoothly varying trend and use this statistic\nto derive consistent predictors in non-stationary time series. In contrast to\nthe currently available methods for this problem the predictor developed here\ndoes not rely on fitting an autoregressive model and does not require a\nvanishing trend. The finite sample properties of the new methodology are\nillustrated by means of a simulation study and a financial indices study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 13:03:14 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:13:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dette", "Holger", ""], ["Wu", "Weichi", ""]]}, {"id": "2001.00519", "submitter": "Marco Chiani Dr.", "authors": "Marco Chiani and Alberto Zanella", "title": "On the Distribution of an Arbitrary Subset of the Eigenvalues for some\n  Finite Dimensional Random Matrices", "comments": null, "journal-ref": "Random Matrices: Theory and Applications (2019)", "doi": "10.1142/S2010326320400043", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some new results on the joint distribution of an arbitrary subset\nof the ordered eigenvalues of complex Wishart, double Wishart, and Gaussian\nhermitian random matrices of finite dimensions, using a tensor\npseudo-determinant operator. Specifically, we derive compact expressions for\nthe joint probability distribution function of the eigenvalues and the\nexpectation of functions of the eigenvalues, including joint moments, for the\ncase of both ordered and unordered eigenvalues.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 17:07:05 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chiani", "Marco", ""], ["Zanella", "Alberto", ""]]}, {"id": "2001.00801", "submitter": "Chao Shen", "authors": "Chao Shen and Hau-Tieng Wu", "title": "Scalability and robustness of spectral embedding: landmark diffusion is\n  all you need", "comments": "66 pages, 40 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While spectral embedding is a widely applied dimension reduction technique in\nvarious fields, so far it is still challenging to make it scalable and robust\nto handle \"big data\". Motivated by the need of handling such data, we propose a\nnovel spectral embedding algorithm, which we coined Robust and Scalable\nEmbedding via Landmark Diffusion (ROSELAND). In short, we measure the affinity\nbetween two points via a set of landmarks, which is composed of a small number\nof points, and \"diffuse\" on the dataset via the landmark set to achieve a\nspectral embedding. The embedding is not only scalable and robust, but also\npreserves the geometric properties under the manifold setup. The Roseland can\nbe viewed as a generalization of the commonly applied spectral embedding\nalgorithm, the diffusion map (DM), in the sense that it shares various\nproperties of the DM. In addition to providing a theoretical justification of\nthe Roseland under the manifold setup, including handling the U-statistics-like\nquantities, providing a $L^\\infty$ spectral convergence with a rate, and\noffering a high dimensional noise analysis, we show various numerical\nsimulations and compare the Roseland with other existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 12:27:26 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 13:29:15 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Shen", "Chao", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2001.01079", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Minimization Problems on Strictly Convex Divergences", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The divergence minimization problem plays an important role in various\nfields. In this note, we focus on differentiable and strictly convex\ndivergences. For some minimization problems, we show the minimizer conditions\nand the uniqueness of the minimizer without assuming a specific form of\ndivergences. Furthermore, we show geometric properties related to the\nminimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 13:53:11 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 04:34:28 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 13:14:08 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "2001.01130", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak, Sergii Myroshnychenko, Susanna Spektor", "title": "Analytic Permutation Testing via Kahane--Khintchine Inequalities", "comments": "35 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The permutation test is a versatile type of exact nonparametric significance\ntest that requires drastically fewer assumptions than similar parametric tests\nby considering the distribution of a test statistic over a discrete group of\ndistributionally invariant transformations. The main downfall of the\npermutation test is the high computational cost of running such a test making\nthis approach laborious for complex data and experimental designs and\ncompletely infeasible in any application requiring speedy results. We rectify\nthis problem through application of Kahane--Khintchine-type inequalities under\na weak dependence condition and thus propose a computation free permutation\ntest---i.e. a permutation-less permutation test. This general framework is\nstudied within both commutative and non-commutative Banach spaces. We further\nimprove these Kahane-Khintchine-type bounds via a transformation based on the\nincomplete beta function and Talagrand's concentration inequality. For\n$k$-sample testing, we extend the theory presented for Rademacher sums to\nweakly dependent Rademacher chaoses making use of modified decoupling\ninequalities. We test this methodology on classic functional data sets\nincluding the Berkeley growth curves and the phoneme dataset. We also consider\nhypothesis testing on speech samples under two experimental designs: the Latin\nsquare and the complete randomized block design.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 21:24:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:44:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kashlak", "Adam B", ""], ["Myroshnychenko", "Sergii", ""], ["Spektor", "Susanna", ""]]}, {"id": "2001.01138", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "Phase Transitions in the Edge/Concurrent Vertex Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it is well-known that some exponential family random graph model\n(ERGM) families exhibit phase transitions (in which small parameter changes\nlead to qualitative changes in graph structure), the behavior of other models\nis still poorly understood. Recently, Krivitsky and Morris have reported a\npreviously unobserved phase transition in the edge/concurrent vertex family (a\nsimple starting point for models of sexual contact networks). Here, we examine\nthis phase transition, showing it to be a first order transition with respect\nto an order parameter associated with the fraction of concurrent vertices. This\ntransition stems from weak cooperativity in the recruitment of vertices to the\nconcurrent phase, which may not be a desirable property in some applications.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 23:04:22 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "2001.01166", "submitter": "Israel Mart\\'inez Hern\\'andez", "authors": "Israel Mart\\'inez-Hern\\'andez and Marc G. Genton", "title": "Recent Developments in Complex and Spatially Correlated Functional Data", "comments": "Some typos fixed and new references added", "journal-ref": "Braz. J. Probab. Stat. 34 (2020) 204-229", "doi": "10.1214/20-BJPS466", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As high-dimensional and high-frequency data are being collected on a large\nscale, the development of new statistical models is being pushed forward.\nFunctional data analysis provides the required statistical methods to deal with\nlarge-scale and complex data by assuming that data are continuous functions,\ne.g., a realization of a continuous process (curves) or continuous random\nfields (surfaces), and that each curve or surface is considered as a single\nobservation. Here, we provide an overview of functional data analysis when data\nare complex and spatially correlated. We provide definitions and estimators of\nthe first and second moments of the corresponding functional random variable.\nWe present two main approaches: The first assumes that data are realizations of\na functional random field, i.e., each observation is a curve with a spatial\ncomponent. We call them 'spatial functional data'. The second approach assumes\nthat data are continuous deterministic fields observed over time. In this case,\none observation is a surface or manifold, and we call them 'surface time\nseries'. For the two approaches, we describe software available for the\nstatistical analysis. We also present a data illustration, using a\nhigh-resolution wind speed simulated dataset, as an example of the two\napproaches. The functional data approach offers a new paradigm of data\nanalysis, where the continuous processes or random fields are considered as a\nsingle entity. We consider this approach to be very valuable in the context of\nbig data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 04:31:51 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 08:07:56 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mart\u00ednez-Hern\u00e1ndez", "Israel", ""], ["Genton", "Marc G.", ""]]}, {"id": "2001.01194", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen and Yun Yang", "title": "Cutoff for exact recovery of Gaussian mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the information-theoretic cutoff value on separation of cluster\ncenters for exact recovery of cluster labels in a $K$-component Gaussian\nmixture model with equal cluster sizes. Moreover, we show that a semidefinite\nprogramming (SDP) relaxation of the $K$-means clustering method achieves such\nsharp threshold for exact recovery without assuming the symmetry of cluster\ncenters.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 08:57:04 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 19:44:07 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 11:14:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chen", "Xiaohui", ""], ["Yang", "Yun", ""]]}, {"id": "2001.01199", "submitter": "Vrettos Moulos", "authors": "Vrettos Moulos", "title": "A Hoeffding Inequality for Finite State Markov Chains and its\n  Applications to Markovian Bandits", "comments": "International Symposium on Information Theory (ISIT), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Hoeffding inequality for the partial sums $\\sum_{k=1}^n\nf (X_k)$, where $\\{X_k\\}_{k \\in \\mathbb{Z}_{> 0}}$ is an irreducible Markov\nchain on a finite state space $S$, and $f : S \\to [a, b]$ is a real-valued\nfunction. Our bound is simple, general, since it only assumes irreducibility\nand finiteness of the state space, and powerful. In order to demonstrate its\nusefulness we provide two applications in multi-armed bandit problems. The\nfirst is about identifying an approximately best Markovian arm, while the\nsecond is concerned with regret minimization in the context of Markovian\nbandits.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 09:28:10 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 16:56:28 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Moulos", "Vrettos", ""]]}, {"id": "2001.01297", "submitter": "Fang Han", "authors": "Yandi Shen, Fang Han, and Daniela Witten", "title": "Exponential inequalities for dependent V-statistics via random Fourier\n  features", "comments": "This is the first part of the arxiv preprint (arXiv:1902.02761), and\n  is to appear in Electronic Journal of Probability (EJP). The second part of\n  the arxiv preprint will be submitted to a statistical journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish exponential inequalities for a class of V-statistics under\nstrong mixing conditions. Our theory is developed via a novel kernel expansion\nbased on random Fourier features and the use of a probabilistic method. This\ntype of expansion is new and useful for handling many notorious classes of\nkernels.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 19:27:14 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Shen", "Yandi", ""], ["Han", "Fang", ""], ["Witten", "Daniela", ""]]}, {"id": "2001.01335", "submitter": "Satoru Tokuda", "authors": "Satoru Tokuda, Kenji Nagata, Masato Okada", "title": "Emergent limits of an indirect measurement from phase transitions of\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.dis-nn math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements are inseparable from inference, where the estimation of signals\nof interest from other observations is called an indirect measurement. While a\nvariety of measurement limits have been defined by the physical constraint on\neach setup, the fundamental limit of an indirect measurement is essentially the\nlimit of inference. Here, we propose the concept of statistical limits on\nindirect measurement: the bounds of distinction between signals and noise and\nbetween a signal and another signal. By developing the asymptotic theory of\nBayesian regression, we investigate the phenomenology of a typical indirect\nmeasurement and demonstrate the existence of these limits. Based on the\nconnection between inference and statistical physics, we also provide a unified\ninterpretation in which these limits emerge from phase transitions of\ninference. Our results could pave the way for novel experimental design,\nenabling assess to the required quality of observations according to the\nassumed ground truth before the concerned indirect measurement is actually\nperformed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 23:37:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Tokuda", "Satoru", ""], ["Nagata", "Kenji", ""], ["Okada", "Masato", ""]]}, {"id": "2001.01412", "submitter": "Min Dai", "authors": "Min Dai, Jinqiao Duan, Junjun Liao, Xiangjun Wang", "title": "Maximum Likelihood Estimation of Stochastic Differential Equations with\n  Random Effects Driven by Fractional Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations and stochastic dynamics are good models to\ndescribe stochastic phenomena in real world. In this paper, we study N\nindependent stochastic processes Xi(t) with real entries and the processes are\ndetermined by the stochastic differential equations with drift term relying on\nsome random effects. We obtain the Girsanov-type formula of the stochastic\ndifferential equation driven by Fractional Brownian Motion through kernel\ntransformation. Under some assumptions of the random effect, we estimate the\nparameter estimators by the maximum likelihood estimation and give some\nnumerical simulations for the discrete observations. Results show that for the\ndifferent H, the parameter estimator is closer to the true value as the amount\nof data increases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 05:50:29 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Dai", "Min", ""], ["Duan", "Jinqiao", ""], ["Liao", "Junjun", ""], ["Wang", "Xiangjun", ""]]}, {"id": "2001.01700", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Tyler Maunu, Philippe Rigollet, Austin J. Stromme", "title": "Gradient descent algorithms for Bures-Wasserstein barycenters", "comments": "28 pages, 5 figures, to appear in COLT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study first order methods to compute the barycenter of a probability\ndistribution $P$ over the space of probability measures with finite second\nmoment. We develop a framework to derive global rates of convergence for both\ngradient descent and stochastic gradient descent despite the fact that the\nbarycenter functional is not geodesically convex. Our analysis overcomes this\ntechnical hurdle by employing a Polyak-Lojasiewicz (PL) inequality and relies\non tools from optimal transport and metric geometry. In turn, we establish a PL\ninequality when $P$ is supported on the Bures-Wasserstein manifold of Gaussian\nprobability measures. It leads to the first global rates of convergence for\nfirst order methods in this context.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:25:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:24:13 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chewi", "Sinho", ""], ["Maunu", "Tyler", ""], ["Rigollet", "Philippe", ""], ["Stromme", "Austin J.", ""]]}, {"id": "2001.01890", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Jianqing Fan, Ellen Li", "title": "Statistical Inference for High-Dimensional Matrix-Variate Factor Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the estimation and inference of the low-rank components\nin high-dimensional matrix-variate factor models, where each dimension of the\nmatrix-variates ($p \\times q$) is comparable to or greater than the number of\nobservations ($T$). We propose an estimation method called $\\alpha$-PCA that\npreserves the matrix structure and aggregates mean and contemporary covariance\nthrough a hyper-parameter $\\alpha$. We develop an inferential theory,\nestablishing consistency, the rate of convergence, and the limiting\ndistributions, under general conditions that allow for correlations across\ntime, rows, or columns of the noise. We show both theoretical and empirical\nmethods of choosing the best $\\alpha$, depending on the use-case criteria.\nSimulation results demonstrate the adequacy of the asymptotic results in\napproximating the finite sample properties. The $\\alpha$-PCA compares favorably\nwith the existing ones. Finally, we illustrate its applications with a real\nnumeric data set and two real image data sets. In all applications, the\nproposed estimation procedure outperforms previous methods in the power of\nvariance explanation using out-of-sample 10-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 05:03:47 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:01:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Fan", "Jianqing", ""], ["Li", "Ellen", ""]]}, {"id": "2001.02229", "submitter": "Rahul Roy", "authors": "Rahul Roy, Subir Kumar Bhandari", "title": "Asymptotically optimal test for dependent multiple testing set up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the behaviour of dependent test statistics for\ntesting of multiple hypothesis . To keep simplicity, we have considered a\nmixture normal model with equicorrelated correlation set up. With a simple\nlinear transformation,the test statistics were decomposed into independent\ncomponents, which, when conditioned appropriately generated independent\nvariables. These were used to construct conditional tests , which were shown to\nbe asymptotically optimal with power as large as that obtained using N.P.Lemma.\nWe have pursued extensive simulation to support the claim.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 09:49:24 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Roy", "Rahul", ""], ["Bhandari", "Subir Kumar", ""]]}, {"id": "2001.02246", "submitter": "Dionissios Hristopulos Prof.", "authors": "Dionissios T. Hristopulos, Andreas Pavlides, Vasiliki D. Agou, and\n  Panagiota Gkafa", "title": "Stochastic Local Interaction Model: Geostatistics without Kriging", "comments": "42 pages, 15 figures", "journal-ref": "Mathematical Geosciences, pp.1-43 (2021)", "doi": "10.1007/s11004-021-09957-7", "report-no": null, "categories": "math.ST cs.LG physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical geostatistical methods face serious computational challenges if\nthey are confronted with large sets of spatially distributed data. We present a\nsimplified stochastic local interaction (SLI) model for computationally\nefficient spatial prediction that can handle large data. The SLI method\nconstructs a spatial interaction matrix (precision matrix) that accounts for\nthe data values, their locations, and the sampling density variations without\nuser input. We show that this precision matrix is strictly positive definite.\nThe SLI approach does not require matrix inversion for parameter estimation,\nspatial prediction, and uncertainty estimation, leading to computational\nprocedures that are significantly less intensive computationally than kriging.\nThe precision matrix involves compact kernel functions (spherical, quadratic,\netc.) which enable the application of sparse matrix methods, thus improving\ncomputational time and memory requirements. We investigate the proposed SLI\nmethod with a data set that includes approximately 11500 drill-hole data of\ncoal thickness from Campbell County (Wyoming, USA). We also compare SLI with\nordinary kriging (OK) in terms of estimation performance, using cross\nvalidation analysis, and computational time. According to the validation\nmeasures used, SLI performs slightly better in estimating seam thickness than\nOK in terms of cross-validation measures. In terms of computation time, SLI\nprediction is 3 to 25 times (depending on the size of the kriging neighborhood)\nfaster than OK for the same grid size.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 19:03:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hristopulos", "Dionissios T.", ""], ["Pavlides", "Andreas", ""], ["Agou", "Vasiliki D.", ""], ["Gkafa", "Panagiota", ""]]}, {"id": "2001.02466", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Toni Karvonen, Roland Hostettler, Simo S\\\"arkk\\\"a", "title": "Taylor Moment Expansion for Continuous-Discrete Gaussian Filtering and\n  Smoothing", "comments": "Submitted to IEEE Transactions on Automatic Control. Code is\n  available at (once accepted for publication)\n  https://github.com/zgbkdlm/TME-filter-smoother", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with non-linear Gaussian filtering and smoothing in\ncontinuous-discrete state-space models, where the dynamic model is formulated\nas an It\\^{o} stochastic differential equation (SDE), and the measurements are\nobtained at discrete time instants. We propose novel Taylor moment expansion\n(TME) Gaussian filter and smoother which approximate the moments of the SDE\nwith a temporal Taylor expansion. Differently from classical linearisation or\nIt\\^{o}--Taylor approaches, the Taylor expansion is formed for the moment\nfunctions directly and in time variable, not by using a Taylor expansion on the\nnon-linear functions in the model. We analyse the theoretical properties,\nincluding the positive definiteness of the covariance estimate and stability of\nthe TME Gaussian filter and smoother. By numerical experiments, we demonstrate\nthat the proposed TME Gaussian filter and smoother significantly outperform the\nstate-of-the-art methods in terms of estimation accuracy and numerical\nstability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 11:59:59 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhao", "Zheng", ""], ["Karvonen", "Toni", ""], ["Hostettler", "Roland", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2001.02488", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and Daniel Gaigall", "title": "Testing marginal homogeneity in Hilbert spaces with applications to\n  stock market returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a paired data framework and discuss the question of\nmarginal homogeneity of bivariate high dimensional or functional data. The\nrelated testing problem can be endowed into a more general setting for paired\nrandom variables taking values in a general Hilbert space. To address this\nproblem, a Cramer-von-Mises type test statistic is applied and a bootstrap\nprocedure is suggested to obtain critical values and finally a consistent test.\nThe desired properties of a bootstrap test can be derived, that are asymptotic\nexactness under the null hypothesis and consistency under alternatives.\nSimulations show the quality of the test in the finite sample case. A possible\napplication is the comparison of two possibly dependent stock market returns on\nthe basis of functional data. The approach is demonstrated on the basis of\nhistorical data for different stock market indices.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:04:52 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:44:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Gaigall", "Daniel", ""]]}, {"id": "2001.02579", "submitter": "Francois Roueff", "authors": "Paul Doukhan (AGM), Fran\\c{c}ois Roueff (IDS, S2A), Joseph Rynkiewicz\n  (SAMM)", "title": "Spectral estimation for non-linear long range dependent discrete time\n  trawl processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete time trawl processes constitute a large class of time series\nparameterized by a trawl sequence (a j) j$\\in$N and defined though a sequence\nof independent and identically distributed (i.i.d.) copies of a continuous time\nprocess ($\\gamma$(t)) t$\\in$R called the seed process. They provide a general\nframework for modeling linear or non-linear long range dependent time series.\nWe investigate the spectral estimation, either pointwise or broadband, of long\nrange dependent discrete-time trawl processes. The difficulty arising from the\nvariety of seed processes and of trawl sequences is twofold. First, the\nspectral density may take different forms, often including smooth additive\ncorrection terms. Second, trawl processes with similar spectral densities may\nexhibit very different statistical behaviors. We prove the consistency of our\nestimators under very general conditions and we show that a wide class of trawl\nprocesses satisfy them. This is done in particular by introducing a weighted\nweak dependence index that can be of independent interest. The broadband\nspectral estimator includes an estimator of the long memory parameter. We\ncomplete this work with numerical experiments to evaluate the finite sample\nsize performance of this estimator for various integer valued discrete time\ntrawl processes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 15:39:18 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Doukhan", "Paul", "", "AGM"], ["Roueff", "Fran\u00e7ois", "", "IDS, S2A"], ["Rynkiewicz", "Joseph", "", "SAMM"]]}, {"id": "2001.02719", "submitter": "Erin Gabriel", "authors": "Erin E Gabriel", "title": "A note on Horvitz-Thompson estimators for rare subgroup analysis in the\n  presence of interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is interference, a subject's outcome depends on the treatment of\nothers and treatment effects may take on several different forms. This\nsituation arises often, particularly in vaccine evaluation. In settings where\ninterference is likely, two-stage cluster randomized trials have been suggested\nas a means of estimating some of the causal contrast of interest. Working in\nthe finite population setting to investigate rare and unplanned subgroup\nanalyses using some of the estimators that have been suggested in the\nliterature, include Horvitz-Thompson, Hajek, and what might be called the\nnatural extension of the marginal estimators suggested in Hudgens and Halloran\n2008. I define the estimands of interest conditional on individual, group and\nboth individual and group baseline variables, giving unbiased Horvitz-Thompson\nstyle estimates for each. I also provide variance estimators for several\nestimators. I show that the Horvitz-Thompson (HT) type estimators are always\nunbiased provided at least one subject within the group or population, whatever\nthe level of interest for the estimator, is in the subgroup of interest. This\nis not true of the \"natural\" or the Hajek style estimators, which will often be\nundefined for rare subgroups.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:01:31 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Gabriel", "Erin E", ""]]}, {"id": "2001.03039", "submitter": "Matey Neykov", "authors": "Matey Neykov, Sivaraman Balakrishnan and Larry Wasserman", "title": "Minimax Optimal Conditional Independence Testing", "comments": "92 pages, 1 table, 6 figures. v4 major updates: fixed and error in\n  appendix G -- multivariate Z case", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conditional independence testing of $X$ and $Y$\ngiven $Z$ where $X,Y$ and $Z$ are three real random variables and $Z$ is\ncontinuous. We focus on two main cases - when $X$ and $Y$ are both discrete,\nand when $X$ and $Y$ are both continuous. In view of recent results on\nconditional independence testing (Shah and Peters, 2018), one cannot hope to\ndesign non-trivial tests, which control the type I error for all absolutely\ncontinuous conditionally independent distributions, while still ensuring power\nagainst interesting alternatives. Consequently, we identify various, natural\nsmoothness assumptions on the conditional distributions of $X,Y|Z=z$ as $z$\nvaries in the support of $Z$, and study the hardness of conditional\nindependence testing under these smoothness assumptions. We derive matching\nlower and upper bounds on the critical radius of separation between the null\nand alternative hypotheses in the total variation metric. The tests we consider\nare easily implementable and rely on binning the support of the continuous\nvariable $Z$. To complement these results, we provide a new proof of the\nhardness result of Shah and Peters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:06:01 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 18:24:54 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 03:10:58 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 21:52:09 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Neykov", "Matey", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2001.03403", "submitter": "Florian Hildebrandt", "authors": "Florian Hildebrandt", "title": "On generating fully discrete samples of the stochastic heat equation on\n  an interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing an idea of Davie and Gaines (2001), we present a method for the\nsimulation of fully discrete samples of the solution to the stochastic heat\nequation on an interval. We provide a condition for the validity of the\napproximation, which holds particularly when the number of temporal and spatial\nobservations tends to infinity. Hereby, the quality of the approximation is\nmeasured in total variation distance. In a simulation study we calculate\ntemporal and spatial quadratic variations from sample paths generated both via\nour method and via naive truncation of the Fourier series representation of the\nprocess. Hereby, the results provided by our method are more accurate at a\nconsiderably lower computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 12:18:12 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hildebrandt", "Florian", ""]]}, {"id": "2001.03527", "submitter": "Jaromir Sant", "authors": "Jaromir Sant, Paul A. Jenkins, Jere Koskela, Dario Spano", "title": "Convergence of Likelihood Ratios and Estimators for Selection in\n  non-neutral Wright-Fisher Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of discrete time, finite population size models in genetics\ndescribing the dynamics of allele frequencies are known to converge (subject to\nsuitable scaling) to a diffusion process in the infinite population limit,\ntermed the Wright-Fisher diffusion. In this article we show that the diffusion\nis ergodic uniformly in the selection and mutation parameters, and that the\nmeasures induced by the solution to the stochastic differential equation are\nuniformly locally asymptotically normal. Subsequently these two results are\nused to analyse the statistical properties of the Maximum Likelihood and\nBayesian estimators for the selection parameter, when both selection and\nmutation are acting on the population. In particular, it is shown that these\nestimators are uniformly over compact sets consistent, display uniform in the\nselection parameter asymptotic normality and convergence of moments over\ncompact sets, and are asymptotically efficient for a suitable class of loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:00:43 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 09:51:23 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sant", "Jaromir", ""], ["Jenkins", "Paul A.", ""], ["Koskela", "Jere", ""], ["Spano", "Dario", ""]]}, {"id": "2001.03710", "submitter": "Changlong Wu", "authors": "Changlong Wu and Narayana Santhanam", "title": "Prediction with eventual almost sure guarantees", "comments": "A preliminary version of this paper submitted to 2020 IEEE\n  International Symposium on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of predicting the properties of a probabilistic model\nand the next outcome of the model sequentially in the infinite horizon, so that\nthe perdition will make finitely many errors with probability 1. We introduce a\ngeneral framework that models such predication problems. We prove some general\nproperties of the framework, and show some concrete examples with the\napplication of such properties.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 04:17:39 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Wu", "Changlong", ""], ["Santhanam", "Narayana", ""]]}, {"id": "2001.03786", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis, Nicola Lunardon", "title": "Empirical bias-reducing adjustments to estimating functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel, general framework for reduced-bias $M$-estimation from\nasymptotically unbiased estimating functions. The framework relies on an\nempirical approximation of the bias by a function of derivatives of estimating\nfunction contributions. Reduced-bias $M$-estimation operates either implicitly,\nby solving empirically adjusted estimating equations, or explicitly, by\nsubtracting the estimated bias from the original $M$-estimates, and applies to\nmodels that are partially- or fully-specified, with either likelihoods or other\nsurrogate objectives. Automatic differentiation can be used to abstract away\nthe only algebra required to implement reduced-bias $M$-estimation. As a\nresult, the bias reduction methods we introduce have markedly broader\napplicability and more straightforward implementation than other established\nbias-reduction methods that require resampling or evaluation of expectations of\nproducts of log-likelihood derivatives. If $M$-estimation is by maximizing an\nobjective, then there always exists a bias-reducing penalized objective. That\npenalized objective relates closely to information criteria for model\nselection, and can be further enhanced with plug-in penalties to deliver\nreduced-bias $M$-estimates with extra properties, like finiteness in models for\ncategorical data. The reduced-bias $M$-estimators have the same asymptotic\ndistribution as the original $M$-estimators, and, hence, standard procedures\nfor inference and model selection apply unaltered with the improved estimates.\nWe demonstrate and assess the properties of reduced-bias $M$-estimation in\nwell-used, prominent modelling settings of varying complexity.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 19:19:36 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:00:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Lunardon", "Nicola", ""]]}, {"id": "2001.04130", "submitter": "Nived Rajaraman", "authors": "Nived Rajaraman, Prafulla Chandra, Andrew Thangaraj, Ananda Theertha\n  Suresh", "title": "Convergence of Chao Unseen Species Estimator", "comments": "20 pages, 1 figure, short version presented at International\n  Symposium on Information Theory (ISIT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support size estimation and the related problem of unseen species estimation\nhave wide applications in ecology and database analysis. Perhaps the most used\nsupport size estimator is the Chao estimator. Despite its wide spread use,\nlittle is known about its theoretical properties. We analyze the Chao estimator\nand show that its worst case mean squared error (MSE) is smaller than the MSE\nof the plug-in estimator by a factor of $\\mathcal{O} ((k/n)^4)$, where $k$ is\nthe maximum support size and $n$ is the number of samples. Our main technical\ncontribution is a new method to analyze rational estimators for discrete\ndistribution properties, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 10:07:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Rajaraman", "Nived", ""], ["Chandra", "Prafulla", ""], ["Thangaraj", "Andrew", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "2001.04295", "submitter": "Erwan Scornet", "authors": "Erwan Scornet (CMAP)", "title": "Trees, forests, and impurity-based variable importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensemble methods such as random forests [Breiman, 2001] are very popular\nto handle high-dimensional tabular data sets, notably because of their good\npredictive accuracy. However, when machine learning is used for decision-making\nproblems, settling for the best predictive procedures may not be reasonable\nsince enlightened decisions require an in-depth comprehension of the algorithm\nprediction process. Unfortunately, random forests are not intrinsically\ninterpretable since their prediction results from averaging several hundreds of\ndecision trees. A classic approach to gain knowledge on this so-called\nblack-box algorithm is to compute variable importances, that are employed to\nassess the predictive impact of each input variable. Variable importances are\nthen used to rank or select variables and thus play a great role in data\nanalysis. Nevertheless, there is no justification to use random forest variable\nimportances in such way: we do not even know what these quantities estimate. In\nthis paper, we analyze one of the two well-known random forest variable\nimportances, the Mean Decrease Impurity (MDI). We prove that if input variables\nare independent and in absence of interactions, MDI provides a variance\ndecomposition of the output, where the contribution of each variable is clearly\nidentified. We also study models exhibiting dependence between input variables\nor interaction, for which the variable importance is intrinsically ill-defined.\nOur analysis shows that there may exist some benefits to use a forest compared\nto a single tree.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:38:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 16:13:28 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Scornet", "Erwan", "", "CMAP"]]}, {"id": "2001.04598", "submitter": "Vincent Tan", "authors": "Yonglong Li, Vincent Y. F. Tan", "title": "Second-Order Asymptotics of Sequential Hypothesis Testing", "comments": "Accepted by the IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical sequential binary hypothesis testing problem in\nwhich there are two hypotheses governed respectively by distributions $P_0$ and\n$P_1$ and we would like to decide which hypothesis is true using a sequential\ntest. It is known from the work of Wald and Wolfowitz that as the expectation\nof the length of the test grows, the optimal type-I and type-II error exponents\napproach the relative entropies $D(P_1\\|P_0)$ and $D(P_0\\|P_1)$. We refine this\nresult by considering the optimal backoff---or second-order asymptotics---from\nthe corner point of the achievable exponent region $(D(P_1\\|P_0),D(P_0\\|P_1))$\nunder two different constraints on the length of the test (or the sample size).\nFirst, we consider a probabilistic constraint in which the probability that the\nlength of test exceeds a prescribed integer $n$ is less than a certain\nthreshold $0<\\varepsilon <1$. Second, the expectation of the sample size is\nbounded by $n$. In both cases, and under mild conditions, the second-order\nasymptotics is characterized exactly. Numerical examples are provided to\nillustrate our results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 02:48:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 04:15:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 15:24:06 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Yonglong", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2001.04620", "submitter": "Chen Cheng", "authors": "Chen Cheng, Yuting Wei, Yuxin Chen", "title": "Tackling small eigen-gaps: Fine-grained eigenvector estimation and\n  inference under heteroscedastic noise", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.NA eess.SP math.IT math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to address two fundamental challenges arising in eigenvector\nestimation and inference for a low-rank matrix from noisy observations: (1) how\nto estimate an unknown eigenvector when the eigen-gap (i.e. the spacing between\nthe associated eigenvalue and the rest of the spectrum) is particularly small;\n(2) how to perform estimation and inference on linear functionals of an\neigenvector -- a sort of \"fine-grained\" statistical reasoning that goes far\nbeyond the usual $\\ell_2$ analysis. We investigate how to address these\nchallenges in a setting where the unknown $n\\times n$ matrix is symmetric and\nthe additive noise matrix contains independent (and non-symmetric) entries.\nBased on eigen-decomposition of the asymmetric data matrix, we propose\nestimation and uncertainty quantification procedures for an unknown\neigenvector, which further allow us to reason about linear functionals of an\nunknown eigenvector. The proposed procedures and the accompanying theory enjoy\nseveral important features: (1) distribution-free (i.e. prior knowledge about\nthe noise distributions is not needed); (2) adaptive to heteroscedastic noise;\n(3) minimax optimal under Gaussian noise. Along the way, we establish optimal\nprocedures to construct confidence intervals for the unknown eigenvalues. All\nthis is guaranteed even in the presence of a small eigen-gap (up to\n$O(\\sqrt{n/\\mathrm{poly}\\log (n)})$ times smaller than the requirement in prior\ntheory), which goes significantly beyond what generic matrix perturbation\ntheory has to offer.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 04:26:10 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 08:55:58 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Cheng", "Chen", ""], ["Wei", "Yuting", ""], ["Chen", "Yuxin", ""]]}, {"id": "2001.04769", "submitter": "Kumar Vijay Mishra", "authors": "M. Ashok Kumar and Kumar Vijay Mishra", "title": "Cram\\'er-Rao Lower Bounds Arising from Generalized Csisz\\'ar Divergences", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the geometry of probability distributions with respect to a\ngeneralized family of Csisz\\'ar $f$-divergences. A member of this family is the\nrelative $\\alpha$-entropy which is also a R\\'enyi analog of relative entropy in\ninformation theory and known as logarithmic or projective power divergence in\nstatistics. We apply Eguchi's theory to derive the Fisher information metric\nand the dual affine connections arising from these generalized divergence\nfunctions. This enables us to arrive at a more widely applicable version of the\nCram\\'{e}r-Rao inequality, which provides a lower bound for the variance of an\nestimator for an escort of the underlying parametric probability distribution.\nWe then extend the Amari-Nagaoka's dually flat structure of the exponential and\nmixer models to other distributions with respect to the aforementioned\ngeneralized metric. We show that these formulations lead us to find unbiased\nand efficient estimators for the escort model. Finally, we compare our work\nwith prior results on generalized Cram\\'er-Rao inequalities that were derived\nfrom non-information-geometric frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:41:13 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 05:24:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kumar", "M. Ashok", ""], ["Mishra", "Kumar Vijay", ""]]}, {"id": "2001.04896", "submitter": "Vincent Divol", "authors": "Vincent Divol", "title": "Minimax adaptive estimation in manifold inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of manifold estimation: given a set of observations\nsampled close to some unknown submanifold $M$, one wants to recover information\nabout the geometry of $M$. Minimax estimators which have been proposed so far\nall depend crucially on the a priori knowledge of some parameters quantifying\nthe regularity of $M$ (such as its reach), whereas those quantities will be\nunknown in practice. Our contribution to the matter is twofold: first, we\nintroduce a one-parameter family of manifold estimators $(\\hat{M}_t)_{t\\geq\n0}$, and show that for some choice of $t$ (depending on the regularity\nparameters), the corresponding estimator is minimax on the class of models of\n$C^2$ manifolds introduced in [Genovese et al., Manifold estimation and\nsingular deconvolution under Hausdorff loss]. Second, we propose a completely\ndata-driven selection procedure for the parameter $t$, leading to a minimax\nadaptive manifold estimator on this class of models. This selection procedure\nactually allows to recover the sample rate of the set of observations, and can\ntherefore be used as an hyperparameter in other settings, such as tangent space\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:54:05 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 13:28:44 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Divol", "Vincent", ""]]}, {"id": "2001.04964", "submitter": "Johannes Heiny", "authors": "Johannes Heiny and Thomas Mikosch", "title": "The eigenstructure of the sample covariance matrices of high-dimensional\n  stochastic volatility models with heavy tails", "comments": "Bernoulli 25 (2019), no. 4B, 3590-3622", "journal-ref": null, "doi": "10.3150/18-BEJ1103", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a $p$-dimensional time series where the dimension $p$ increases\nwith the sample size $n$. The resulting data matrix $X$ follows a stochastic\nvolatility model: each entry consists of a positive random volatility term\nmultiplied by an independent noise term. The volatility multipliers introduce\ndependence in each row and across the rows. We study the asymptotic behavior of\nthe eigenvalues and eigenvectors of the sample covariance matrix $XX'$ under a\nregular variation assumption on the noise. In particular, we prove Poisson\nconvergence for the point process of the centered and normalized eigenvalues\nand derive limit theory for functionals acting on them, such as the trace. We\nprove related results for stochastic volatility models with additional linear\ndependence structure and for stochastic volatility models where the\ntime-varying volatility terms are extinguished with high probability when $n$\nincreases. We provide explicit approximations of the eigenvectors which are of\na strikingly simple structure. The main tools for proving these results are\nlarge deviation theorems for heavy-tailed time series, advocating a unified\napproach to the study of the eigenstructure of heavy-tailed random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:50:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Heiny", "Johannes", ""], ["Mikosch", "Thomas", ""]]}, {"id": "2001.05056", "submitter": "Johannes Heiny", "authors": "Johannes Heiny and Thomas Mikosch", "title": "Large sample autocovariance matrices of linear processes with heavy\n  tails", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide asymptotic theory for certain functions of the sample\nautocovariance matrices of a high-dimensional time series with infinite fourth\nmoment. The time series exhibits linear dependence across the coordinates and\nthrough time. Assuming that the dimension increases with the sample size, we\nprovide theory for the eigenvectors of the sample autocovariance matrices and\nfind explicit approximations of a simple structure, whose finite sample quality\nis illustrated for simulated data. We also obtain the limits of the normalized\neigenvalues of functions of the sample autocovariance matrices in terms of\ncluster Poisson point processes. In turn, we derive the distributional limits\nof the largest eigenvalues and functionals acting on them. In our proofs, we\nuse large deviation techniques for heavy-tailed processes, point process\ntechniques motivated by extreme value theory, and related continuous mapping\narguments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:51:15 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Heiny", "Johannes", ""], ["Mikosch", "Thomas", ""]]}, {"id": "2001.05057", "submitter": "Michael Kordovan", "authors": "Michael Kordovan, Stefan Rotter", "title": "Spike Train Cumulants for Linear-Nonlinear Poisson Cascade Models", "comments": "45 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.ST physics.bio-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking activity in cortical networks is nonlinear in nature. The\nlinear-nonlinear cascade model, some versions of which are also known as\npoint-process generalized linear model, can efficiently capture the nonlinear\ndynamics exhibited by such networks. Of particular interest in such models are\ntheoretical predictions of spike train statistics. However, due to the\nmoment-closure problem, approximations are inevitable. We suggest here a series\nexpansion that explains how higher-order moments couple to lower-order ones.\nOur approach makes predictions in terms of certain integrals, the so-called\nloop integrals. In previous studies these integrals have been evaluated\nnumerically, but numerical instabilities are sometimes encountered rendering\nthe results unreliable. Analytic solutions are presented here to overcome this\nproblem, and to arrive at more robust evaluations. We were able to deduce these\nanalytic solutions by switching to Fourier space and making use of complex\nanalysis, specifically Cauchy's residue theorem. We formalized the loop\nintegrals and explicitly solved them for specific response functions. To\nquantify the importance of these corrections for spike train cumulants, we\nnumerically simulated spiking networks and compared their sample statistics to\nour theoretical predictions. Our results demonstrate that the magnitude of the\nnonlinear corrections depends on the working point of the nonlinear network\ndynamics, and that it is related to the eigenvalues of the mean-field stability\nmatrix. For our example, the corrections for the firing rates are in the range\nbetween 4 % and 21 % on average. Precise and robust predictions of spike train\nstatistics accounting for nonlinear effects are, for example, highly relevant\nfor theories involving spike-timing dependent plasticity (STDP).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:56:13 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Kordovan", "Michael", ""], ["Rotter", "Stefan", ""]]}, {"id": "2001.05204", "submitter": "Ansgar Steland", "authors": "Nils Mause and Ansgar Steland", "title": "Detecting Changes in the Second Moment Structure of High-Dimensional\n  Sensor-Type Data in a $K$-Sample Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$ sample problem for high-dimensional vector time series is studied,\nespecially focusing on sensor data streams, in order to analyze the second\nmoment structure and detect changes across samples and/or across variables\ncumulated sum (CUSUM) statistics of bilinear forms of the sample covariance\nmatrix. In this model $K$ independent vector time series\n$\\mathbf{Y}_{T,1},\\dots,\\mathbf{Y}_{T,K}$ are observed over a time span $ [0,T]\n$, which may correspond to $K$ sensors (locations) yielding $d$-dimensional\ndata as well as $K$ locations where $d$ sensors emit univariate data. Unequal\nsample sizes are considered as arising when the sampling rate of the sensors\ndiffers. We provide large sample approximations and two related change-point\nstatistics, a sums of squares and a pooled variance statistic. The resulting\nprocedures are investigated by simulations and illustrated by analyzing a real\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 10:00:32 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Mause", "Nils", ""], ["Steland", "Ansgar", ""]]}, {"id": "2001.05454", "submitter": "Piet Groeneboom", "authors": "Fadoua Balabdaoui, Piet Groeneboom", "title": "Profile least squares estimators in the monotone single index model", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider least squares estimators of the finite dimensional regression\nparameter $\\alpha$ in the single index regression model\n$Y=\\psi(\\alpha^TX)+\\epsilon$, where $X$ is a $d$-dimensional random vector,\n$E(Y|X)=\\psi(\\alpha^TX)$, and where $\\psi$ is monotone. It has been suggested\nto estimate $\\alpha$ by a profile least squares estimator, minimizing\n$\\sum_{i=1}^n(Y_i-\\psi(\\alpha^TX_i))^2$ over monotone $\\psi$ and $\\alpha$ on\nthe boundary $S_{d-1}$of the unit ball. Although this suggestion has been\naround for a long time, it is still unknown whether the estimate is $\\sqrt{n}$\nconvergent. We show that a profile least squares estimator, using the same\npointwise least squares estimator for fixed $\\alpha$, but using a different\nglobal sum of squares, is $\\sqrt{n}$-convergent and asymptotically normal. We\nshow that a profile least squares estimator, using the same pointwise least\nsquares estimator for fixed $\\bma$, but using a different global sum of\nsquares, is $\\sqrt{n}$-convergent and asymptotically normal. The difference\nbetween the corresponding loss functions is studied and also a comparison with\nother methods is given.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:59:20 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 07:32:54 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Groeneboom", "Piet", ""]]}, {"id": "2001.05484", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Jianqing Fan, Cong Ma, Yuling Yan", "title": "Bridging Convex and Nonconvex Optimization in Robust PCA: Noise,\n  Outliers, and Missing Data", "comments": "accepted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper delivers improved theoretical guarantees for the convex\nprogramming approach in low-rank matrix estimation, in the presence of (1)\nrandom noise, (2) gross sparse outliers, and (3) missing data. This problem,\noften dubbed as robust principal component analysis (robust PCA), finds\napplications in various domains. Despite the wide applicability of convex\nrelaxation, the available statistical support (particularly the stability\nanalysis vis-\\`a-vis random noise) remains highly suboptimal, which we\nstrengthen in this paper. When the unknown matrix is well-conditioned,\nincoherent, and of constant rank, we demonstrate that a principled convex\nprogram achieves near-optimal statistical accuracy, in terms of both the\nEuclidean loss and the $\\ell_{\\infty}$ loss. All of this happens even when\nnearly a constant fraction of observations are corrupted by outliers with\narbitrary magnitudes. The key analysis idea lies in bridging the convex program\nin use and an auxiliary nonconvex optimization algorithm, and hence the title\nof this paper.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 18:54:29 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 20:05:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Yan", "Yuling", ""]]}, {"id": "2001.05513", "submitter": "Thomas Berrett", "authors": "Thomas B. Berrett, Ioannis Kontoyiannis, Richard J. Samworth", "title": "Optimal rates for independence testing via $U$-statistic permutation\n  tests", "comments": "58 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of independence testing given independent and\nidentically distributed pairs taking values in a $\\sigma$-finite, separable\nmeasure space. Defining a natural measure of dependence $D(f)$ as the squared\n$L^2$-distance between a joint density $f$ and the product of its marginals, we\nfirst show that there is no valid test of independence that is uniformly\nconsistent against alternatives of the form $\\{f: D(f) \\geq \\rho^2 \\}$. We\ntherefore restrict attention to alternatives that impose additional\nSobolev-type smoothness constraints, and define a permutation test based on a\nbasis expansion and a $U$-statistic estimator of $D(f)$ that we prove is\nminimax optimal in terms of its separation rates in many instances. Finally,\nfor the case of a Fourier basis on $[0,1]^2$, we provide an approximation to\nthe power function that offers several additional insights. Our methodology is\nimplemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 19:04:23 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 11:50:28 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Kontoyiannis", "Ioannis", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2001.05676", "submitter": "Ji Hyung Jung", "authors": "Ji Hyung Jung, Hye Won Chung, and Ji Oon Lee", "title": "Weak Detection in the Spiked Wigner Model with General Rank", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical decision process of detecting the signal from a\n`signal+noise' type matrix model with an additive Wigner noise. We propose a\nhypothesis test based on the linear spectral statistics of the data matrix,\nwhich does not depend on the distribution of the signal or the noise. The test\nis optimal under the Gaussian noise if the signal-to-noise ratio is small, as\nit minimizes the sum of the Type-I and Type-II errors. Under the non-Gaussian\nnoise, the test can be improved with an entrywise transformation to the data\nmatrix. We also introduce an algorithm that estimates the rank of the signal\nwhen it is not known a priori.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:40:24 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 08:29:54 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 05:33:32 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Jung", "Ji Hyung", ""], ["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""]]}, {"id": "2001.05889", "submitter": "Sebastiano Grazzi", "authors": "Joris Bierkens, Sebastiano Grazzi, Frank van der Meulen and Moritz\n  Schauer", "title": "A piecewise deterministic Monte Carlo method for diffusion bridges", "comments": "31 pages, 11 figures. Implementation at\n  https://github.com/SebaGraz/ZZDiffusionBridge/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the use of the Zig-Zag sampler to the problem of sampling\nconditional diffusion processes (diffusion bridges). The Zig-Zag sampler is a\nrejection-free sampling scheme based on a non-reversible continuous piecewise\ndeterministic Markov process. Similar to the L\\'evy-Ciesielski construction of\na Brownian motion, we expand the diffusion path in a truncated Faber-Schauder\nbasis. The coefficients within the basis are sampled using a Zig-Zag sampler. A\nkey innovation is the use of the fully local Algorithm for the Zig-Zag sampler\nthat allows to exploit the sparsity structure implied by the dependency graph\nof the coefficients and by the subsampling technique to reduce the complexity\nof the algorithm. We illustrate the performance of the proposed methods in a\nnumber of examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:29:28 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 07:24:26 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 09:02:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bierkens", "Joris", ""], ["Grazzi", "Sebastiano", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "2001.06002", "submitter": "Vilijandas Bagdonavi\\v{c}ius", "authors": "Vilijandas Bagdonavi\\v{c}ius, R\\=uta Levulien\\.e", "title": "Testing proportional hazards for specified covariates", "comments": "Published at https://doi.org/10.15559/19-VMSTA129 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2019, Vol. 6, No. 2,\n  209-225", "doi": "10.15559/19-VMSTA129", "report-no": "VTeX-VMSTA-VMSTA129", "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests for proportional hazards assumption concerning specified covariates or\ngroups of covariates are proposed. The class of alternatives is wide:\nlog-hazard rates under different values of covariates may cross, approach, go\naway. The data may be right censored. The limit distribution of the test\nstatistic is derived. Power of the test against approaching alternatives is\ngiven. Real data examples are considered.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:54:47 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Bagdonavi\u010dius", "Vilijandas", ""], ["Levulien\u0117", "R\u016bta", ""]]}, {"id": "2001.06027", "submitter": "David Benkeser", "authors": "David Benkeser", "title": "Nonparametric inference for interventional effects with multiple\n  mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the pathways whereby an intervention has an effect on an\noutcome is a common scientific goal. A rich body of literature provides various\ndecompositions of the total intervention effect into pathway specific effects.\nInterventional direct and indirect effects provide one such decomposition.\nExisting estimators of these effects are based on parametric models with\nconfidence interval estimation facilitated via the nonparametric bootstrap. We\nprovide theory that allows for more flexible, possibly machine learning-based,\nestimation techniques to be considered. In particular, we establish weak\nconvergence results that facilitate the construction of closed-form confidence\nintervals and hypothesis tests. Finally, we demonstrate multiple robustness\nproperties of the proposed estimators. Simulations show that inference based on\nlarge-sample theory has adequate small-sample performance. Our work thus\nprovides a means of leveraging modern statistical learning techniques in\nestimation of interventional mediation effects.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:05:00 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Benkeser", "David", ""]]}, {"id": "2001.06485", "submitter": "Boris Ndjia Njike", "authors": "Boris Ndjia Njike, Xavier Siebert", "title": "K-NN active learning under local smoothness assumption", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.03055", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large body of work on convergence rates either in passive or\nactive learning. Here we first outline some of the main results that have been\nobtained, more specifically in a nonparametric setting under assumptions about\nthe smoothness of the regression function (or the boundary between classes) and\nthe margin noise. We discuss the relative merits of these underlying\nassumptions by putting active learning in perspective with recent work on\npassive learning. We design an active learning algorithm with a rate of\nconvergence better than in passive learning, using a particular smoothness\nassumption customized for k-nearest neighbors. Unlike previous active learning\nalgorithms, we use a smoothness assumption that provides a dependence on the\nmarginal distribution of the instance space. Additionally, our algorithm avoids\nthe strong density assumption that supposes the existence of the density\nfunction of the marginal distribution of the instance space and is therefore\nmore generally applicable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 10:44:36 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 08:05:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Njike", "Boris Ndjia", ""], ["Siebert", "Xavier", ""]]}, {"id": "2001.06955", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Moulinath Banerjee, Ya'acov Ritov", "title": "Asymptotic normality of a linear threshold estimator in fixed dimension\n  with near-optimal rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear thresholding models postulate that the conditional distribution of a\nresponse variable in terms of covariates differs on the two sides of a\n(typically unknown) hyperplane in the covariate space. A key goal in such\nmodels is to learn about this separating hyperplane. Exact likelihood or least\nsquare methods to estimate the thresholding parameter involve an indicator\nfunction which make them difficult to optimize and are, therefore, often\ntackled by using a surrogate loss that uses a smooth approximation to the\nindicator. In this note, we demonstrate that the resulting estimator is\nasymptotically normal with a near optimal rate of convergence: $n^{-1}$ up to a\nlog factor, in a classification thresholding model. This is substantially\nfaster than the currently established convergence rates of smoothed estimators\nfor similar models in the statistics and econometrics literatures.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 03:22:10 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2001.06966", "submitter": "Junho Yang", "authors": "Suhasini Subba Rao and Junho Yang", "title": "Reconciling the Gaussian and Whittle Likelihood with an application to\n  estimation in the frequency domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time series analysis there is an apparent dichotomy between time and\nfrequency domain methods. The aim of this paper is to draw connections between\nfrequency and time domain methods. Our focus will be on reconciling the\nGaussian likelihood and the Whittle likelihood. We derive an exact,\ninterpretable, bound between the Gaussian and Whittle likelihood of a second\norder stationary time series. The derivation is based on obtaining the\ntransformation which is biorthogonal to the discrete Fourier transform of the\ntime series. Such a transformation yields a new decomposition for the inverse\nof a Toeplitz matrix and enables the representation of the Gaussian likelihood\nwithin the frequency domain. We show that the difference between the Gaussian\nand Whittle likelihood is due to the omission of the best linear predictions\noutside the domain of observation in the periodogram associated with the\nWhittle likelihood. Based on this result, we obtain an approximation for the\ndifference between the Gaussian and Whittle likelihoods in terms of the best\nfitting, finite order autoregressive parameters. These approximations are used\nto define two new frequency domain quasi-likelihoods criteria. We show that\nthese new criteria can yield a better approximation of the spectral divergence\ncriterion, as compared to both the Gaussian and Whittle likelihoods. In\nsimulations, we show that the proposed estimators have satisfactory finite\nsample properties.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:22:59 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 23:19:01 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 02:08:13 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rao", "Suhasini Subba", ""], ["Yang", "Junho", ""]]}, {"id": "2001.07042", "submitter": "Arjun Seshadri", "authors": "Arjun Seshadri, Johan Ugander", "title": "Fundamental Limits of Testing the Independence of Irrelevant\n  Alternatives in Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multinomial Logit (MNL) model and the axiom it satisfies, the\nIndependence of Irrelevant Alternatives (IIA), are together the most widely\nused tools of discrete choice. The MNL model serves as the workhorse model for\na variety of fields, but is also widely criticized, with a large body of\nexperimental literature claiming to document real-world settings where IIA\nfails to hold. Statistical tests of IIA as a modelling assumption have been the\nsubject of many practical tests focusing on specific deviations from IIA over\nthe past several decades, but the formal size properties of hypothesis testing\nIIA are still not well understood. In this work we replace some of the\nambiguity in this literature with rigorous pessimism, demonstrating that any\ngeneral test for IIA with low worst-case error would require a number of\nsamples exponential in the number of alternatives of the choice problem. A\nmajor benefit of our analysis over previous work is that it lies entirely in\nthe finite-sample domain, a feature crucial to understanding the behavior of\ntests in the common data-poor settings of discrete choice. Our lower bounds are\nstructure-dependent, and as a potential cause for optimism, we find that if one\nrestricts the test of IIA to violations that can occur in a specific collection\nof choice sets (e.g., pairs), one obtains structure-dependent lower bounds that\nare much less pessimistic. Our analysis of this testing problem is unorthodox\nin being highly combinatorial, counting Eulerian orientations of cycle\ndecompositions of a particular bipartite graph constructed from a data set of\nchoices. By identifying fundamental relationships between the comparison\nstructure of a given testing problem and its sample efficiency, we hope these\nrelationships will help lay the groundwork for a rigorous rethinking of the IIA\ntesting problem as well as other testing problems in discrete choice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 10:15:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seshadri", "Arjun", ""], ["Ugander", "Johan", ""]]}, {"id": "2001.07064", "submitter": "Qiyang Han", "authors": "Hang Deng, Qiyang Han, Cun-Hui Zhang", "title": "Confidence intervals for multiple isotonic regression and other monotone\n  models", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing pointwise confidence intervals in the\nmultiple isotonic regression model. Recently, [HZ19] obtained a pointwise limit\ndistribution theory for the so-called block max-min and min-max estimators\n[FLN17] in this model, but inference remains a difficult problem due to the\nnuisance parameter in the limit distribution that involves multiple unknown\npartial derivatives of the true regression function.\n  In this paper, we show that this difficult nuisance parameter can be\neffectively eliminated by taking advantage of information beyond point\nestimates in the block max-min and min-max estimators. Formally, let\n$\\hat{u}(x_0)$ (resp. $\\hat{v}(x_0)$) be the maximizing lower-left (resp.\nminimizing upper-right) vertex in the block max-min (resp. min-max) estimator,\nand $\\hat{f}_n$ be the average of the block max-min and min-max estimators. If\nall (first-order) partial derivatives of $f_0$ are non-vanishing at $x_0$, then\nthe following pivotal limit distribution theory holds: $$\n\\sqrt{n_{\\hat{u},\\hat{v}}(x_0)}\\big(\\hat{f}_n(x_0)-f_0(x_0)\\big)\\rightsquigarrow\n\\sigma\\cdot \\mathbb{L}_{1_d}. $$ Here $n_{\\hat{u},\\hat{v}}(x_0)$ is the number\nof design points in the block $[\\hat{u}(x_0),\\hat{v}(x_0)]$, $\\sigma$ is the\nstandard deviation of the errors, and $\\mathbb{L}_{1_d}$ is a universal limit\ndistribution free of nuisance parameters. This immediately yields confidence\nintervals for $f_0(x_0)$ with asymptotically exact confidence level and oracle\nlength. Notably, the construction of the confidence intervals, even new in the\nunivariate setting, requires no more efforts than performing an isotonic\nregression for once using the block max-min and min-max estimators, and can be\neasily adapted to other common monotone models. Extensive simulations are\ncarried out to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:32:12 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:12:03 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Deng", "Hang", ""], ["Han", "Qiyang", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2001.07212", "submitter": "Ping Li", "authors": "Xiao-Tong Yuan, Ping Li", "title": "Generalization Bounds for High-dimensional M-estimation under Sparsity\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\ell_0$-constrained empirical risk minimization ($\\ell_0$-ERM) is a\npromising tool for high-dimensional statistical estimation. The existing\nanalysis of $\\ell_0$-ERM estimator is mostly on parameter estimation and\nsupport recovery consistency. From the perspective of statistical learning,\nanother fundamental question is how well the $\\ell_0$-ERM estimator would\nperform on unseen samples. The answer to this question is important for\nunderstanding the learnability of such a non-convex (and also NP-hard)\nM-estimator but still relatively under explored.\n  In this paper, we investigate this problem and develop a generalization\ntheory for $\\ell_0$-ERM. We establish, in both white-box and black-box\nstatistical regimes, a set of generalization gap and excess risk bounds for\n$\\ell_0$-ERM to characterize its sparse prediction and optimization capability.\nOur theory mainly reveals three findings: 1) tighter generalization bounds can\nbe attained by $\\ell_0$-ERM than those of $\\ell_2$-ERM if the risk function is\n(with high probability) restricted strongly convex; 2) tighter uniform\ngeneralization bounds can be established for $\\ell_0$-ERM than the conventional\ndense ERM; and 3) sparsity level invariant bounds can be established by\nimposing additional strong-signal conditions to ensure the stability of\n$\\ell_0$-ERM. In light of these results, we further provide generalization\nguarantees for the Iterative Hard Thresholding (IHT) algorithm which serves as\none of the most popular greedy pursuit methods for approximately solving\n$\\ell_0$-ERM. Numerical evidence is provided to confirm our theoretical\npredictions when implied to sparsity-constrained linear regression and logistic\nregression models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 18:56:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Li", "Ping", ""]]}, {"id": "2001.07298", "submitter": "Ali Dolati", "authors": "M. Sanatgar, A. Dolati and M. Amini", "title": "A General Class of Weighted Rank Correlation Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we propose a class of weighted rank correlation coefficients\nextending the Spearman's rho. The proposed class constructed by giving suitable\nweights to the distance between two sets of ranks to place more emphasis on\nitems having low rankings than those have high rankings or vice versa. The\nasymptotic distribution of the proposed measures and properties of the\nparameters estimated by them are studied through the associated copula. A\nsimulation study is performed to compare the performance of the proposed\nstatistics for testing independence using asymptotic relative efficiency\ncalculations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 00:39:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sanatgar", "M.", ""], ["Dolati", "A.", ""], ["Amini", "M.", ""]]}, {"id": "2001.07383", "submitter": "Subhajit Dutta Dr.", "authors": "Soumya Das, Subhajit Dutta, Radhenduhska Srivastava", "title": "On Construction of Higher Order Kernels Using Fourier Transforms and\n  Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that a suitably chosen covariance function of a\ncontinuous time, second order stationary stochastic process can be viewed as a\nsymmetric higher order kernel. This leads to the construction of a higher order\nkernel by choosing an appropriate covariance function. An optimal choice of the\nconstructed higher order kernel that partially minimizes the mean integrated\nsquare error of the kernel density estimator is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 08:32:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Das", "Soumya", ""], ["Dutta", "Subhajit", ""], ["Srivastava", "Radhenduhska", ""]]}, {"id": "2001.07415", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on", "title": "Explicit agreement extremes for a $2\\times2$ table with given marginals", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of maximizing (or minimizing) the agreement between clusterings,\nsubject to given marginals, can be formally posed under a common framework for\nseveral agreement measures. Until now, it was possible to find its solution\nonly through numerical algorithms. Here, an explicit solution is shown for the\ncase where the two clusterings have two clusters each.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 09:47:57 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "2001.07422", "submitter": "Chiara Amorino", "authors": "Chiara Amorino (LaMME), Arnaud Gloter (LAMA)", "title": "Invariant density adaptive estimation for ergodic jump diffusion\n  processes over anisotropic classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the solution X = (Xt) t$\\ge$0 of a multivariate stochastic\ndifferential equation with Levy-type jumps and with unique invariant\nprobability measure with density $\\mu$. We assume that a continuous record of\nobservations X T = (Xt) 0$\\le$t$\\le$T is available. In the case without jumps,\nReiss and Dalalyan (2007) and Strauch (2018) have found convergence rates of\ninvariant density estimators, under respectively isotropic and anisotropic\nH{\\\"o}lder smoothness constraints, which are considerably faster than those\nknown from standard multivariate density estimation. We extend the previous\nworks by obtaining, in presence of jumps, some estimators which have the same\nconvergence rates they had in the case without jumps for d $\\ge$ 2 and a rate\nwhich depends on the degree of the jumps in the one-dimensional setting. We\npropose moreover a data driven bandwidth selection procedure based on the\nGoldensh-luger and Lepski (2011) method which leads us to an adaptive\nnon-parametric kernel estimator of the stationary density $\\mu$ of the jump\ndiffusion X. Adaptive bandwidth selection, anisotropic density estimation,\nergodic diffusion with jumps, L{\\'e}vy driven SDE\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 10:09:30 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Amorino", "Chiara", "", "LaMME"], ["Gloter", "Arnaud", "", "LAMA"]]}, {"id": "2001.07805", "submitter": "Banghua Zhu", "authors": "Banghua Zhu, Jiantao Jiao, Jacob Steinhardt", "title": "When does the Tukey median work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of the Tukey median estimator under total\nvariation (TV) distance corruptions. Previous results show that under Huber's\nadditive corruption model, the breakdown point is 1/3 for high-dimensional\nhalfspace-symmetric distributions. We show that under TV corruptions, the\nbreakdown point reduces to 1/4 for the same set of distributions. We also show\nthat a certain projection algorithm can attain the optimal breakdown point of\n1/2. Both the Tukey median estimator and the projection algorithm achieve\nsample complexity linear in dimension.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 23:04:39 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:46:43 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2001.07835", "submitter": "Zhimei Ren", "authors": "Zhimei Ren, Emmanuel Cand\\`es", "title": "Knockoffs with Side Information", "comments": "29 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assessing the importance of multiple variables or\nfactors from a dataset when side information is available. In principle, using\nside information can allow the statistician to pay attention to variables with\na greater potential, which in turn, may lead to more discoveries. We introduce\nan adaptive knockoff filter, which generalizes the knockoff procedure (Barber\nand Cand\\`es, 2015; Cand\\`es et al., 2018) in that it uses both the data at\nhand and side information to adaptively order the variables under study and\nfocus on those that are most promising. Adaptive knockoffs controls the\nfinite-sample false discovery rate (FDR) and we demonstrate its power by\ncomparing it with other structured multiple testing methods. We also apply our\nmethodology to real genetic data in order to find associations between genetic\nvariants and various phenotypes such as Crohn's disease and lipid levels. Here,\nadaptive knockoffs makes more discoveries than reported in previous studies on\nthe same datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 01:09:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Ren", "Zhimei", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2001.07883", "submitter": "Hao Liu", "authors": "Hao Liu, Wenjing Liao", "title": "Learning functions varying along an active subspace", "comments": "39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many functions of interest are in a high-dimensional space but exhibit\nlow-dimensional structures. This paper studies regression of a $s$-H\\\"{o}lder\nfunction $f$ in $\\mathbb{R}^D$ which varies along an active subspace of\ndimension $d$ while $d\\ll D$. A direct approximation of $f$ in $\\mathbb{R}^D$\nwith an $\\varepsilon$ accuracy requires the number of samples $n$ in the order\nof $\\varepsilon^{-(2s+D)/s}$. In this paper, we modify the Generalized Contour\nRegression (GCR) algorithm to estimate the active subspace and use piecewise\npolynomials for function approximation. GCR is among the best estimators for\nthe active subspace, but its sample complexity is an open question. Our\nmodified GCR improves the efficiency over the original GCR and leads to an mean\nsquared estimation error of $O(n^{-1})$ for the active subspace, when $n$ is\nsufficiently large. The mean squared regression error of $f$ is proved to be in\nthe order of $\\left(n/\\log n\\right)^{-\\frac{2s}{2s+d}}$ where the exponent\ndepends on the dimension of the active subspace $d$ instead of the ambient\nspace $D$. This result demonstrates that GCR is effective in learning\nlow-dimensional active subspaces. The convergence rate is validated through\nseveral numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 05:28:45 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 14:51:47 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Liu", "Hao", ""], ["Liao", "Wenjing", ""]]}, {"id": "2001.07932", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sudheesh Kattumannil", "title": "A new goodness of fit test for normal distribution based on Stein's\n  characterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new non-parametric test for testing normal distribution using\nStein's characterization. We study asymptotic properties of the test statistic.\nWe also develop jackknife empirical likelihood ratio test for testing\nnormality. Using Monte Carlo simulation study, we evaluate the finite sample\nperformance of the proposed JEL based test. Finally, we illustrate our test\nprocedure using two real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 09:43:11 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 07:21:04 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kattumannil", "Sudheesh", ""]]}, {"id": "2001.08006", "submitter": "John Harvey", "authors": "Cl\\'ement Berenfeld, John Harvey, Marc Hoffmann, Krishnan Shankar", "title": "Estimating the reach of a manifold via its convexity defect function", "comments": "35 pages, 4 figures. Various minor changes in v2 to correct minor\n  errors and/or improve clarity. Thanks to excellent work by peer reviewers, in\n  v3 an error in Lemma 4.9 was rectified, Section 4.2 was substantially revised\n  and other minor changes made throughout and the manuscript was accepted for\n  publication by Discrete & Computational Geometry. Extremely minor changes in\n  v4", "journal-ref": "Discrete & Computational Geometry, 2021", "doi": "10.1007/s00454-021-00290-8", "report-no": null, "categories": "math.ST cs.CG math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reach of a submanifold is a crucial regularity parameter for manifold\nlearning and geometric inference from point clouds. This paper relates the\nreach of a submanifold to its convexity defect function. Using the stability\nproperties of convexity defect functions, along with some new bounds and the\nrecent submanifold estimator of Aamari and Levrard [Ann. Statist. 47 177-204\n(2019)], an estimator for the reach is given. A uniform expected loss bound\nover a C^k model is found. Lower bounds for the minimax rate for estimating the\nreach over these models are also provided. The estimator almost achieves these\nrates in the C^3 and C^4 cases, with a gap given by a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 13:37:31 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 14:43:37 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 12:34:12 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 09:58:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Berenfeld", "Cl\u00e9ment", ""], ["Harvey", "John", ""], ["Hoffmann", "Marc", ""], ["Shankar", "Krishnan", ""]]}, {"id": "2001.08056", "submitter": "Jesse Van Oostrum", "authors": "Jesse van Oostrum", "title": "Bures-Wasserstein Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bures-Wasserstein distance is a Riemannian distance on the space of\npositive definite Hermitian matrices and is given by: $d(\\Sigma,T) =\n\\left[\\text{tr}(\\Sigma) + \\text{tr}(T) - 2 \\text{tr}\n\\left(\\Sigma^{1/2}T\\Sigma^{1/2}\\right)^{1/2}\\right]^{1/2}$. This distance\nfunction appears in the fields of optimal transport, quantum information, and\noptimisation theory. In this paper, the geometrical properties of this distance\nare studied using Riemannian submersions and quotient manifolds. The Riemannian\nmetric and geodesics are derived on both the whole space and the subspace of\ntrace-one matrices. In the first part of the paper a general framework is\nprovided, including different representations of the tangent bundle for the SLD\nFisher metric. The last part of the paper unifies up till now independent\narguments and results from quantum information theory and optimal transport.\nThe Bures-Wasserstein geometry is related to the Fubini-Study metric and the\nWigner-Yanase information.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 11:19:48 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["van Oostrum", "Jesse", ""]]}, {"id": "2001.08187", "submitter": "Paul Rohrbach", "authors": "Paul B. Rohrbach, Sergey Dolgov, Lars Grasedyck, Robert Scheichl", "title": "Rank Bounds for Approximating Gaussian Densities in the Tensor-Train\n  Format", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank tensor approximations have shown great potential for uncertainty\nquantification in high dimensions, for example, to build surrogate models that\ncan be used to speed up large-scale inference problems (Eigel et al., Inverse\nProblems 34, 2018; Dolgov et al., Statistics & Computing 30, 2020). The\nfeasibility and efficiency of such approaches depends critically on the rank\nthat is necessary to represent or approximate the underlying distribution. In\nthis paper, a-priori rank bounds for approximations in the functional\ntensor-train representation for the case of Gaussian models are developed. It\nis shown that under suitable conditions on the precision matrix, the Gaussian\ndensity can be approximated to high accuracy without suffering from an\nexponential growth of complexity as the dimension increases. These results\nprovide a rigorous justification of the suitability and the limitations of\nlow-rank tensor methods in a simple but important model case. Numerical\nexperiments confirm that the rank bounds capture the qualitative behavior of\nthe rank structure when varying the parameters of the precision matrix and the\naccuracy of the approximation. Finally, the practical relevance of the\ntheoretical results is demonstrated in the context of a Bayesian filtering\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:15:12 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 15:39:59 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rohrbach", "Paul B.", ""], ["Dolgov", "Sergey", ""], ["Grasedyck", "Lars", ""], ["Scheichl", "Robert", ""]]}, {"id": "2001.08336", "submitter": "Yang Chen", "authors": "Yang Chen, Ruobin Gong, Min-ge Xie", "title": "Geometric Conditions for the Discrepant Posterior Phenomenon and\n  Connections to Simpson's Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrepant posterior phenomenon (DPP) is a counterintuitive phenomenon\nthat occurs in the Bayesian analysis of multivariate parameters. It refers to\nwhen an estimate of a marginal parameter obtained from the posterior is more\nextreme than both of those obtained using either the prior or the likelihood\nalone. Inferential claims that exhibit DPP defy intuition, and the phenomenon\ncan be surprisingly ubiquitous in well-behaved Bayesian models. Using point\nestimation as an example, we derive conditions under which the DPP occurs in\nBayesian models with exponential quadratic likelihoods, including Gaussian\nmodels and those with local asymptotic normality property, with conjugate\nmultivariate Gaussian priors. We also examine the DPP for the Binomial model,\nin which the posterior mean is not a linear combination of that of the prior\nand the likelihood. We provide an intuitive geometric interpretation of the\nphenomenon and show that there exists a non-trivial space of marginal\ndirections such that the DPP occurs. We further relate the phenomenon to the\nSimpson's paradox and discover their deep-rooted connection that is associated\nwith marginalization. We also draw connections with Bayesian computational\nalgorithms when difficult geometry exists. Theoretical results are complemented\nby numerical illustrations. Scenarios covered in this study have implications\nfor parameterization, sensitivity analysis, and prior choice for Bayesian\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:46:59 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Chen", "Yang", ""], ["Gong", "Ruobin", ""], ["Xie", "Min-ge", ""]]}, {"id": "2001.08431", "submitter": "Thomas Yee", "authors": "Thomas William Yee", "title": "On the Hauck-Donner Effect in Wald Tests: Detection, Tipping Points, and\n  Parameter Space Characterization", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wald test remains ubiquitous in statistical practice despite shortcomings\nsuch as its inaccuracy in small samples and lack of invariance under\nreparameterization. This paper develops on another but lesser-known shortcoming\ncalled the Hauck--Donner effect (HDE) whereby a Wald test statistic is not\nmonotonely increasing as a function of increasing distance between the\nparameter estimate and the null value. Resulting in an upward biased $p$-value\nand loss of power, the aberration can lead to very damaging consequences such\nas in variable selection. The HDE afflicts many types of regression models and\ncorresponds to estimates near the boundary of the parameter space. This article\npresents several new results, and its main contributions are to (i) propose a\nvery general test for detecting the HDE, regardless of its underlying cause;\n(ii) fundamentally characterize the HDE by pairwise ratios of Wald and Rao\nscore and likelihood ratio test statistics for 1-parameter distributions; (iii)\nshow that the parameter space may be partitioned into an interior encased by 5\nHDE severity measures (faint, weak, moderate, strong, extreme); (iv) prove that\na necessary condition for the HDE in a 2 by 2 table is a log odds ratio of at\nleast 2; (v) give some practical guidelines about HDE-free hypothesis testing.\nOverall, practical post-fit tests can now be conducted potentially to any model\nestimated by iteratively reweighted least squares, such as the generalized\nlinear model (GLM) and Vector GLM (VGLM) classes, the latter which encompasses\nmany popular regression models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:15:50 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yee", "Thomas William", ""]]}, {"id": "2001.08512", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "A precise local limit theorem for the multinomial distribution and some\n  applications", "comments": "21 pages, 0 figure, v4: minor corrections", "journal-ref": "J. Statist. Plann. Inference, 215 (2021), 218-233", "doi": "10.1016/j.jspi.2021.03.006", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Siotani & Fujikoshi (1984), a precise local limit theorem for the\nmultinomial distribution is derived by inverting the Fourier transform, where\nthe error terms are explicit up to order $N^{-1}$. In this paper, we give an\nalternative (conceptually simpler) proof based on Stirling's formula and a\ncareful handling of Taylor expansions, and we show how the result can be used\nto approximate multinomial probabilities on most subsets of $\\mathbb{R}^d$.\nFurthermore, we discuss a recent application of the result to obtain asymptotic\nproperties of Bernstein estimators on the simplex, we improve the main result\nin Carter (2002) on the Le Cam distance bound between multinomial and\nmultivariate normal experiments while simultaneously simplifying the proof, and\nwe mention another potential application related to finely tuned continuity\ncorrections.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:56:22 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 23:44:10 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 13:43:48 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 20:39:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2001.08857", "submitter": "Adrian R\\\"ollin", "authors": "Xiao Fang, Han Liang Gan, Susan Holmes, Haiyan Huang, Erol Pek\\\"oz,\n  Adrian R\\\"ollin, Wenpin Tang", "title": "Arcsine laws for random walks generated from random permutations with\n  applications to genomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result for the simple symmetric random walk with $2n$ steps is\nthat the number of steps above the origin, the time of the last visit to the\norigin, and the time of the maximum height all have exactly the same\ndistribution and converge when scaled to the arcsine law. Motivated by\napplications in genomics, we study the distributions of these statistics for\nthe non-Markovian random walk generated from the ascents and descents of a\nuniform random permutation and a Mallows($q$) permutation and show that they\nhave the same asymptotic distributions as for the simple random walk. We also\ngive an unexpected conjecture, along with numerical evidence and a partial\nproof in special cases, for the result that the number of steps above the\norigin by step $2n$ for the uniform permutation generated walk has exactly the\nsame discrete arcsine distribution as for the simple random walk, even though\nthe other statistics for these walks have very different laws. We also give\nexplicit error bounds to the limit theorems using Stein's method for the\narcsine distribution, as well as functional central limit theorems and a strong\nembedding of the Mallows$(q)$ permutation which is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 01:25:59 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Fang", "Xiao", ""], ["Gan", "Han Liang", ""], ["Holmes", "Susan", ""], ["Huang", "Haiyan", ""], ["Pek\u00f6z", "Erol", ""], ["R\u00f6llin", "Adrian", ""], ["Tang", "Wenpin", ""]]}, {"id": "2001.08877", "submitter": "Hongji Wei", "authors": "T. Tony Cai, Hongji Wei", "title": "Distributed Gaussian Mean Estimation under Communication Constraints:\n  Optimal Rates and Communication-Efficient Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed estimation of a Gaussian mean under communication\nconstraints in a decision theoretical framework. Minimax rates of convergence,\nwhich characterize the tradeoff between the communication costs and statistical\naccuracy, are established in both the univariate and multivariate settings.\nCommunication-efficient and statistically optimal procedures are developed. In\nthe univariate case, the optimal rate depends only on the total communication\nbudget, so long as each local machine has at least one bit. However, in the\nmultivariate case, the minimax rate depends on the specific allocations of the\ncommunication budgets among the local machines.\n  Although optimal estimation of a Gaussian mean is relatively simple in the\nconventional setting, it is quite involved under the communication constraints,\nboth in terms of the optimal procedure design and lower bound argument. The\ntechniques developed in this paper can be of independent interest. An essential\nstep is the decomposition of the minimax estimation problem into two stages,\nlocalization and refinement. This critical decomposition provides a framework\nfor both the lower bound analysis and optimal procedure design.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 04:19:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "2001.09036", "submitter": "Rainer Schwabe", "authors": "Ulrike Gra{\\ss}hoff, Heiko Gro{\\ss}mann, Heinz Holling, Rainer Schwabe", "title": "Optimal Design for Probit Choice Models with Dependent Utilities", "comments": null, "journal-ref": "Statistics, Volume 55, 2021, Issue 1, Pages 173-194", "doi": "10.1080/02331888.2021.1888292", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive locally D-optimal designs for discrete choice\nexperiments based on multinomial probit models. These models include several\ndiscrete explanatory variables as well as a quantitative one. The commonly used\nmultinomial logit model assumes independent utilities for different choice\noptions. Thus, D-optimal optimal designs for such multinomial logit models may\ncomprise choice sets, e.g., consisting of alternatives which are identical in\nall discrete attributes but different in the quantitative variable. Obviously\nsuch designs are not appropriate for many empirical choice experiments. It will\nbe shown that locally D-optimal designs for multinomial probit models supposing\nindependent utilities consist of counterintuitive choice sets as well. However,\nlocally D-optimal designs for multinomial probit models allowing for dependent\nutilities turn out to be reasonable for analyzing decisions using discrete\nchoice studies.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:39:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gra\u00dfhoff", "Ulrike", ""], ["Gro\u00dfmann", "Heiko", ""], ["Holling", "Heinz", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2001.09040", "submitter": "Se Un Park", "authors": "Se Un Park", "title": "Estimation for Compositional Data using Measurements from Nonlinear\n  Systems using Artificial Neural Networks", "comments": "43 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our objective is to estimate the unknown compositional input from its output\nresponse through an unknown system after estimating the inverse of the original\nsystem with a training set. The proposed methods using artificial neural\nnetworks (ANNs) can compete with the optimal bounds for linear systems, where\nconvex optimization theory applies, and demonstrate promising results for\nnonlinear system inversions. We performed extensive experiments by designing\nnumerous different types of nonlinear systems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:50:13 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Park", "Se Un", ""]]}, {"id": "2001.09180", "submitter": "Kabir Chandrasekher", "authors": "Kabir Aladin Chandrasekher, Ahmed El Alaoui, Andrea Montanari", "title": "Imputation for High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional regression with missing entries in the covariates.\nA common strategy in practice is to \\emph{impute} the missing entries with an\nappropriate substitute and then implement a standard statistical procedure\nacting as if the covariates were fully observed. Recent literature on this\nsubject proposes instead to design a specific, often complicated or non-convex,\nalgorithm tailored to the case of missing covariates. We investigate a simpler\napproach where we fill-in the missing entries with their conditional mean given\nthe observed covariates. We show that this imputation scheme coupled with\nstandard off-the-shelf procedures such as the LASSO and square-root LASSO\nretains the minimax estimation rate in the random-design setting where the\ncovariates are i.i.d.\\ sub-Gaussian. We further show that the square-root LASSO\nremains \\emph{pivotal} in this setting.\n  It is often the case that the conditional expectation cannot be computed\nexactly and must be approximated from data. We study two cases where the\ncovariates either follow an autoregressive (AR) process, or are jointly\nGaussian with sparse precision matrix. We propose tractable estimators for the\nconditional expectation and then perform linear regression via LASSO, and show\nsimilar estimation rates in both cases. We complement our theoretical results\nwith simulations on synthetic and semi-synthetic examples, illustrating not\nonly the sharpness of our bounds, but also the broader utility of this strategy\nbeyond our theoretical assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 19:54:09 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chandrasekher", "Kabir Aladin", ""], ["Alaoui", "Ahmed El", ""], ["Montanari", "Andrea", ""]]}, {"id": "2001.09206", "submitter": "Ziv Goldfeld", "authors": "Ziv Goldfeld and Kristjan Greenewald", "title": "Gaussian-Smooth Optimal Transport: Metric Structure and Statistical\n  Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport (OT), and in particular the Wasserstein distance, has seen\na surge of interest and applications in machine learning. However, empirical\napproximation under Wasserstein distances suffers from a severe curse of\ndimensionality, rendering them impractical in high dimensions. As a result,\nentropically regularized OT has become a popular workaround. However, while it\nenjoys fast algorithms and better statistical properties, it looses the metric\nstructure that Wasserstein distances enjoy. This work proposes a novel\nGaussian-smoothed OT (GOT) framework, that achieves the best of both worlds:\npreserving the 1-Wasserstein metric structure while alleviating the empirical\napproximation curse of dimensionality. Furthermore, as the Gaussian-smoothing\nparameter shrinks to zero, GOT $\\Gamma$-converges towards classic OT (with\nconvergence of optimizers), thus serving as a natural extension. An empirical\nstudy that supports the theoretical results is provided, promoting\nGaussian-smoothed OT as a powerful alternative to entropic OT.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 21:48:07 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Goldfeld", "Ziv", ""], ["Greenewald", "Kristjan", ""]]}, {"id": "2001.09225", "submitter": "Ryan Martin", "authors": "Leonardo Cella and Ryan Martin", "title": "Strong validity, consonance, and conformal prediction", "comments": "34 pages, 3 figures, 2 tables. Comments welcome at\n  https://www.researchers.one/article/2020-01-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valid prediction of future observations is an important and challenging\nproblem. The two general approaches for quantifying uncertainty about the\nfuture value employ prediction regions and predictive distribution,\nrespectively, with the latter usually considered to be more informative because\nit performs other prediction-related tasks. Standard notions of validity focus\non the former, i.e., coverage probability bounds for prediction regions, but a\nnotion of validity relevant to the other prediction-related tasks performed by\nthe latter is lacking. In this paper, we present a new notion---strong\nprediction validity---relevant to these more general prediction tasks. We show\nthat strong validity is connected to more familiar notions of coherence, and\nargue that imprecise probability considerations are required in order to\nachieve it. We go on to show that strong prediction validity can be achieved by\ninterpreting the conformal prediction output as the contour function of a\nconsonant plausibility function. We also offer an alternative characterization,\nbased on a new nonparametric inferential model construction, wherein the\nappearance of consonance is more natural, and prove strong prediction validity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 23:24:15 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 22:10:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Cella", "Leonardo", ""], ["Martin", "Ryan", ""]]}, {"id": "2001.09266", "submitter": "Liam Hodgkinson", "authors": "Liam Hodgkinson, Robert Salomone, Fred Roosta", "title": "The reproducing Stein kernel approach for post-hoc corrected sampling", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein importance sampling is a widely applicable technique based on\nkernelized Stein discrepancy, which corrects the output of approximate sampling\nalgorithms by reweighting the empirical distribution of the samples. A general\nanalysis of this technique is conducted for the previously unconsidered setting\nwhere samples are obtained via the simulation of a Markov chain, and applies to\nan arbitrary underlying Polish space. We prove that Stein importance sampling\nyields consistent estimators for quantities related to a target distribution of\ninterest by using samples obtained from a geometrically ergodic Markov chain\nwith a possibly unknown invariant measure that differs from the desired target.\nThe approach is shown to be valid under conditions that are satisfied for a\nlarge number of unadjusted samplers, and is capable of retaining consistency\nwhen data subsampling is used. Along the way, a universal theory of reproducing\nStein kernels is established, which enables the construction of kernelized\nStein discrepancy on general Polish spaces, and provides sufficient conditions\nfor kernels to be convergence-determining on such spaces. These results are of\nindependent interest for the development of future methodology based on\nkernelized Stein discrepancies.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 05:33:05 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hodgkinson", "Liam", ""], ["Salomone", "Robert", ""], ["Roosta", "Fred", ""]]}, {"id": "2001.09351", "submitter": "Qian Zhao", "authors": "Qian Zhao, Pragya Sur, Emmanuel J. Cand\\`es", "title": "The Asymptotic Distribution of the MLE in High-dimensional Logistic\n  Models: Arbitrary Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of the maximum likelihood estimate (MLE) in\nhigh-dimensional logistic models, extending the recent results from Sur (2019)\nto the case where the Gaussian covariates may have an arbitrary covariance\nstructure. We prove that in the limit of large problems holding the ratio\nbetween the number $p$ of covariates and the sample size $n$ constant, every\nfinite list of MLE coordinates follows a multivariate normal distribution.\nConcretely, the $j$th coordinate $\\hat {\\beta}_j$ of the MLE is asymptotically\nnormally distributed with mean $\\alpha_\\star \\beta_j$ and standard deviation\n$\\sigma_\\star/\\tau_j$; here, $\\beta_j$ is the value of the true regression\ncoefficient, and $\\tau_j$ the standard deviation of the $j$th predictor\nconditional on all the others. The numerical parameters $\\alpha_\\star > 1$ and\n$\\sigma_\\star$ only depend upon the problem dimensionality $p/n$ and the\noverall signal strength, and can be accurately estimated. Our results imply\nthat the MLE's magnitude is biased upwards and that the MLE's standard\ndeviation is greater than that predicted by classical theory. We present a\nseries of experiments on simulated and real data showing excellent agreement\nwith the theory.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 19:07:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhao", "Qian", ""], ["Sur", "Pragya", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2001.09391", "submitter": "Shuang Zhou", "authors": "Shuang Zhou, Pallavi Ray, Debdeep Pati, Anirban Bhattacharya", "title": "Mass-shifting phenomenon of truncated multivariate normal priors", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that lower-dimensional marginal densities of dependent zero-mean\nnormal distributions truncated to the positive orthant exhibit a mass-shifting\nphenomenon. Despite the truncated multivariate normal density having a mode at\nthe origin, the marginal density assigns increasingly small mass near the\norigin as the dimension increases. The phenomenon accentuates with stronger\ncorrelation between the random variables. A precise quantification\ncharacterizing the role of the dimension as well as the dependence is provided.\nThis surprising behavior has serious implications towards Bayesian constrained\nestimation and inference, where the prior, in addition to having a full\nsupport, is required to assign a substantial probability near the origin to\ncapture at parts of the true function of interest. Without further\nmodification, we show that truncated normal priors are not suitable for\nmodeling at regions and propose a novel alternative strategy based on shrinking\nthe coordinates using a multiplicative scale parameter. The proposed shrinkage\nprior is empirically shown to guard against the mass shifting phenomenon while\nretaining computational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 02:54:41 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 03:29:18 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhou", "Shuang", ""], ["Ray", "Pallavi", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2001.09445", "submitter": "Nicolas Marie", "authors": "Fabienne Comte and Nicolas Marie", "title": "On a Nadaraya-Watson Estimator with Two Bandwidths", "comments": "43 pages", "journal-ref": "Electronic Journal of Statistics 15, 1, 2566-2607, 2021", "doi": "10.1214/21-EJS1849", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression model, we write the Nadaraya-Watson estimator of the\nregression function as the quotient of two kernel estimators, and propose a\nbandwidth selection method for both the numerator and the denominator. We prove\nrisk bounds for both data driven estimators and for the resulting ratio. The\nsimulation study confirms that both estimators have good performances, compared\nto the ones obtained by cross-validation selection of the bandwidth. However,\nunexpectedly, the single-bandwidth cross-validation estimator is found to be\nmuch better than the ratio of the previous two good estimators, in the small\nnoise context. However, the two methods have similar performances in models\nwith large noise.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 12:38:52 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 20:48:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Comte", "Fabienne", ""], ["Marie", "Nicolas", ""]]}, {"id": "2001.09602", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura and Tatsuya Kubokawa", "title": "Bayesian Shrinkage Estimation of Negative Multinomial Parameter Vectors", "comments": "31 pages; the code for numerical computation of the hierarchical\n  Bayes estimator in Section 4 has been corrected; Tables 2, 3, and 4 and the\n  second-to-the-last paragraph of Section 4 have been changed", "journal-ref": null, "doi": "10.1016/j.jmva.2020.104653", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The negative multinomial distribution is a multivariate generalization of the\nnegative binomial distribution. In this paper, we consider the problem of\nestimating an unknown matrix of probabilities on the basis of observations of\nnegative multinomial variables under the standardized squared error loss.\nFirst, a general sufficient condition for a shrinkage estimator to dominate the\nUMVU estimator is derived and an empirical Bayes estimator satisfying the\ncondition is constructed. Next, a hierarchical shrinkage prior is introduced,\nan associated Bayes estimator is shown to dominate the UMVU estimator under\nsome conditions, and some remarks about posterior computation are presented.\nFinally, shrinkage estimators and the UMVU estimator are compared by\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:42:17 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 14:47:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "2001.09817", "submitter": "Philippe Berthet", "authors": "Philippe Berthet and Jean-Claude Fort", "title": "Exact rate of convergence of the mean Wasserstein distance between the\n  empirical and true Gaussian distribution", "comments": null, "journal-ref": "Electron. J. Probab. 25 (2020)", "doi": "10.1214/19-EJP410", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Wasserstein distance $W_2$ for Gaussian samples. We establish\nthe exact rate of convergence $\\sqrt{\\log\\log n/n}$ of the expected value of\nthe $W_2$ distance between the empirical and true $c.d.f.$'s for the normal\ndistribution. We also show that the rate of weak convergence is unexpectedly\n$1/\\sqrt{n}$ in the case of two correlated Gaussian samples.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:26:52 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Berthet", "Philippe", ""], ["Fort", "Jean-Claude", ""]]}, {"id": "2001.09868", "submitter": "Matteo Ruggiero", "authors": "Filippo Ascolani, Antonio Lijoi and Matteo Ruggiero", "title": "Predictive inference with Fleming--Viot-driven dependent Dirichlet\n  processes", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider predictive inference using a class of temporally dependent\nDirichlet processes driven by Fleming--Viot diffusions, which have a natural\nbearing in Bayesian nonparametrics and lend the resulting family of random\nprobability measures to analytical posterior analysis. Formulating the implied\nstatistical model as a hidden Markov model, we fully describe the predictive\ndistribution induced by these Fleming--Viot-driven dependent Dirichlet\nprocesses, for a sequence of observations collected at a certain time given\nanother set of draws collected at several previous times. This is identified as\na mixture of P\\'olya urns, whereby the observations can be values from the\nbaseline distribution or copies of previous draws collected at the same time as\nin the usual P\\`olya urn, or can be sampled from a random subset of the data\ncollected at previous times. We characterise the time-dependent weights of the\nmixture which select such subsets and discuss the asymptotic regimes. We\ndescribe the induced partition by means of a Chinese restaurant process\nmetaphor with a conveyor belt, whereby new customers who do not sit at an\noccupied table open a new table by picking a dish either from the baseline\ndistribution or from a time-varying offer available on the conveyor belt. We\nlay out explicit algorithms for exact and approximate posterior sampling of\nboth observations and partitions, and illustrate our results on predictive\nproblems with synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:45:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ascolani", "Filippo", ""], ["Lijoi", "Antonio", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "2001.10025", "submitter": "Tim Barfoot", "authors": "Timothy D. Barfoot", "title": "Multivariate Gaussian Variational Inference by Natural Gradient Descent", "comments": "11 pages, 0 figures; second version fixed a single typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note reviews so-called Natural Gradient Descent (NGD) for\nmultivariate Gaussians. The Fisher Information Matrix (FIM) is derived for\nseveral different parameterizations of Gaussians. Careful attention is paid to\nthe symmetric nature of the covariance matrix when calculating derivatives. We\nshow that there are some advantages to choosing a parameterization comprising\nthe mean and inverse covariance matrix and provide a simple NGD update that\naccounts for the symmetric (and sparse) nature of the inverse covariance\nmatrix.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 19:20:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 13:54:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Barfoot", "Timothy D.", ""]]}, {"id": "2001.10137", "submitter": "Jonathan Scarlett", "authors": "Lan V. Truong, Matthew Aldridge, and Jonathan Scarlett", "title": "On the All-Or-Nothing Behavior of Bernoulli Group Testing", "comments": "(v2) Added section on non-i.i.d. test matrices, including optimal\n  approximate recovery threshold. (v3) Final version accepted to IEEE Journal\n  on Selected Areas in Information Theory (JSAIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of non-adaptive group testing, in which\none seeks to identify which items are defective given a set of\nsuitably-designed tests whose outcomes indicate whether or not at least one\ndefective item was included in the test. The most widespread recovery criterion\nseeks to exactly recover the entire defective set, and relaxed criteria such as\napproximate recovery and list decoding have also been considered. In this\npaper, we study the fundamental limits of group testing under the significantly\nrelaxed {\\em weak recovery} criterion, which only seeks to identify a small\nfraction (e.g., $0.01$) of the defective items. Given the near-optimality of\ni.i.d.~Bernoulli testing for exact recovery in sufficiently sparse scaling\nregimes, it is natural to ask whether this design additionally succeeds with\nmuch fewer tests under weak recovery. Our main negative result shows that this\nis not the case, and in fact, under i.i.d.~Bernoulli random testing in the\nsufficiently sparse regime, an {\\em all-or-nothing} phenomenon occurs: When the\nnumber of tests is slightly below a threshold, weak recovery is impossible,\nwhereas when the number of tests is slightly above the same threshold,\nhigh-probability exact recovery is possible. In establishing this result, we\nadditionally prove similar negative results under Bernoulli designs for the\nweak detection problem (distinguishing between the group testing model\nvs.~completely random outcomes) and the problem of identifying a single item\nthat is definitely defective. On the positive side, we show that all three\nrelaxed recovery criteria can be attained using considerably fewer tests under\nsuitably-chosen non-Bernoulli designs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 01:33:59 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 09:26:54 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 04:17:23 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 08:01:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Truong", "Lan V.", ""], ["Aldridge", "Matthew", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "2001.10164", "submitter": "Sayar Karmakar", "authors": "Sayar Karmakar and Wei Biao Wu", "title": "Optimal Gaussian Approximation for Multiple Time Series", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": "10.5705/ss.202017.0303", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain an optimal bound for a Gaussian approximation of a large class of\nvector-valued random processes. Our results provide a substantial\ngeneralization of earlier results that assume independence and/or stationarity.\nBased on the decay rate of the functional dependence measure, we quantify the\nerror bound of the Gaussian approximation using the sample size $n$ and the\nmoment condition. Under the assumption of $p$th finite moment, with $p>2$, this\ncan range from a worst case rate of $n^{1/2}$ to the best case rate of\n$n^{1/p}$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 04:24:06 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Karmakar", "Sayar", ""], ["Wu", "Wei Biao", ""]]}, {"id": "2001.10377", "submitter": "Chaonan Jiang", "authors": "Chaonan Jiang, Davide La Vecchia, Elvezio Ronchetti, Olivier Scaillet", "title": "Saddlepoint approximations for spatial panel data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new higher-order asymptotic techniques for the Gaussian maximum\nlikelihood estimator in a spatial panel data model, with fixed effects,\ntime-varying covariates, and spatially correlated errors. Our saddlepoint\ndensity and tail area approximation feature relative error of order\n$O(1/(n(T-1)))$ with $n$ being the cross-sectional dimension and $T$ the\ntime-series dimension. The main theoretical tool is the tilted-Edgeworth\ntechnique in a non-identically distributed setting. The density approximation\nis always non-negative, does not need resampling, and is accurate in the tails.\nMonte Carlo experiments on density approximation and testing in the presence of\nnuisance parameters illustrate the good performance of our approximation over\nfirst-order asymptotics and Edgeworth expansions. An empirical application to\nthe investment-saving relationship in OECD (Organisation for Economic\nCo-operation and Development) countries shows disagreement between testing\nresults based on first-order asymptotics and saddlepoint techniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 19:40:01 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 14:28:20 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 18:35:54 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Jiang", "Chaonan", ""], ["La Vecchia", "Davide", ""], ["Ronchetti", "Elvezio", ""], ["Scaillet", "Olivier", ""]]}, {"id": "2001.10391", "submitter": "Jeremie Bigot", "authors": "J\\'er\\'emie Bigot and Charles Deledalle", "title": "Low-rank matrix denoising for count data using unbiased Kullback-Leibler\n  risk estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned by the analysis of observations organized in a matrix\nform whose elements are count data assumed to follow a Poisson or a multinomial\ndistribution. We focus on the estimation of either the intensity matrix\n(Poisson case) or the compositional matrix (multinomial case) that is assumed\nto have a low rank structure. We propose to construct an estimator minimizing\nthe regularized negative log-likelihood by a nuclear norm penalty. Our approach\neasily yields a low-rank matrix-valued estimator with positive entries which\nbelongs to the set of row-stochastic matrices in the multinomial case. Then,\nour main contribution is to propose a data-driven way to select the\nregularization parameter in the construction of such estimators by minimizing\n(approximately) unbiased estimates of the Kullback-Leibler (KL) risk in such\nmodels, which generalize Stein's unbiased risk estimation originally proposed\nfor Gaussian data. The evaluation of these quantities is a delicate problem,\nand we introduce novel methods to obtain accurate numerical approximation of\nsuch unbiased estimates. Simulated data are used to validate this way of\nselecting regularizing parameters for low-rank matrix estimation from count\ndata. For data following a multinomial distribution, we also compare its\nperformances to K-fold cross-validation. Examples from a survey study and\nmetagenomics also illustrate the benefits of our approach for real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:01:50 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 07:57:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Deledalle", "Charles", ""]]}, {"id": "2001.10423", "submitter": "Arnaud Gloter", "authors": "Sylvain Delattre (LPMA), Arnaud Gloter (LaMME), Nakahiro Yoshida", "title": "Rate of Estimation for the Stationary Distribution of Stochastic Damping\n  Hamiltonian Systems with Continuous Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of the non-parametric estimation for the density $\\pi$\nof the stationary distribution of a stochastic two-dimensional damping\nHamiltonian system $(Z_t)_{t\\in[0,T]}=(X_t,Y_t)_{t \\in [0,T]}$. From the\ncontinuous observation of the sampling path on $[0,T]$, we study the rate of\nestimation for $\\pi(x_0,y_0)$ as $T \\to \\infty$. We show that kernel based\nestimators can achieve the rate $T^{-v}$ for some explicit exponent $v \\in\n(0,1/2)$. One finding is that the rate of estimation depends on the smoothness\nof $\\pi$ and is completely different with the rate appearing in the standard\ni.i.d.\\ setting or in the case of two-dimensional non degenerate diffusion\nprocesses. Especially, this rate depends also on $y_0$. Moreover, we obtain a\nminimax lower bound on the $L^2$-risk for pointwise estimation, with the same\nrate $T^{-v}$, up to $\\log(T)$ terms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:51:03 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Delattre", "Sylvain", "", "LPMA"], ["Gloter", "Arnaud", "", "LaMME"], ["Yoshida", "Nakahiro", ""]]}, {"id": "2001.10426", "submitter": "Jean-Baptiste Seby", "authors": "Elina Robeva and Jean-Baptiste Seby", "title": "Multi-trek separation in Linear Structural Equation Models", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the theory of causal discovery from observational data, we study\ninteractions between multiple (sets of) random variables in a linear structural\nequation model with non-Gaussian error terms. We give a correspondence between\nstructure in the higher order cumulants and combinatorial structure in the\ncausal graph. It has previously been shown that low rank of the covariance\nmatrix corresponds to trek separation in the graph. Generalizing this criterion\nto multiple sets of vertices, we characterize when determinants of subtensors\nof the higher order cumulant tensors vanish. This criterion applies when hidden\nvariables are present as well. For instance, it allows us to identify the\npresence of a hidden common cause of k of the observed variables.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:55:39 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 21:22:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Robeva", "Elina", ""], ["Seby", "Jean-Baptiste", ""]]}, {"id": "2001.10577", "submitter": "Julio Stern", "authors": "Julio Michael Stern and Carlos Alberto de Braganca Pereira", "title": "The e-value: A Fully Bayesian Significance Measure for Precise\n  Statistical Hypotheses and its Research Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a survey of the e-value, a statistical significance\nmeasure a.k.a. the evidence rendered by observational data, X, in support of a\nstatistical hypothesis, H, or, the other way around, the epistemic value of H\ngiven X. The $e$-value and the accompanying FBST, the Full Bayesian\nSignificance Test, constitute the core of a research program that was started\nat IME-USP, is being developed by over 20 researchers worldwide, and has, so\nfar, been referenced by over 200 publications.\n  The e-value and the FBST comply with the best principles of Bayesian\ninference, including the likelihood principle, complete invariance, asymptotic\nconsistency, etc. Furthermore, they exhibit powerful logic or algebraic\nproperties in situations where one needs to compare or compose distinct\nhypotheses that can be formulated either in the same or in different\nstatistical models. Moreover, they effortlessly accommodate the case of sharp\nor precise hypotheses, a situation where alternative methods often require ad\nhoc and convoluted procedures. Finally, the FBST has outstanding robustness and\nreliability characteristics, outperforming traditional tests of hypotheses in\nmany practical applications of statistical modeling and operations research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 20:27:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 20:59:11 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 11:41:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Stern", "Julio Michael", ""], ["Pereira", "Carlos Alberto de Braganca", ""]]}, {"id": "2001.10623", "submitter": "Nikita Zhivotovskiy", "authors": "Gergely Neu and Nikita Zhivotovskiy", "title": "Fast Rates for Online Prediction with Abstention", "comments": "19 pages, minor corrections, to appear in COLT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of sequential prediction of individual $\\{0, 1\\}$-sequences\nwith expert advice, we show that by allowing the learner to abstain from the\nprediction by paying a cost marginally smaller than $\\frac 12$ (say, $0.49$),\nit is possible to achieve expected regret bounds that are independent of the\ntime horizon $T$. We exactly characterize the dependence on the abstention cost\n$c$ and the number of experts $N$ by providing matching upper and lower bounds\nof order $\\frac{\\log N}{1-2c}$, which is to be contrasted with the best\npossible rate of $\\sqrt{T\\log N}$ that is available without the option to\nabstain. We also discuss various extensions of our model, including a setting\nwhere the sequence of abstention costs can change arbitrarily over time, where\nwe show regret bounds interpolating between the slow and the fast rates\nmentioned above, under some natural assumptions on the sequence of abstention\ncosts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 22:34:55 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 19:07:38 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Neu", "Gergely", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2001.10631", "submitter": "Xiaowei Li", "authors": "Halyun Jeong, Xiaowei Li, Yaniv Plan, \\\"Ozg\\\"ur Y{\\i}lmaz", "title": "Sub-Gaussian Matrices on Sets: Optimal Tail Dependence and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random linear mappings are widely used in modern signal processing,\ncompressed sensing and machine learning. These mappings may be used to embed\nthe data into a significantly lower dimension while at the same time preserving\nuseful information. This is done by approximately preserving the distances\nbetween data points, which are assumed to belong to $\\mathbb{R}^n$. Thus, the\nperformance of these mappings is usually captured by how close they are to an\nisometry on the data. Gaussian linear mappings have been the object of much\nstudy, while the sub-Gaussian settings is not yet fully understood. In the\nlatter case, the performance depends on the sub-Gaussian norm of the rows. In\nmany applications, e.g., compressed sensing, this norm may be large, or even\ngrowing with dimension, and thus it is important to characterize this\ndependence.\n  We study when a sub-Gaussian matrix can become a near isometry on a set, show\nthat previous best known dependence on the sub-Gaussian norm was sub-optimal,\nand present the optimal dependence. Our result not only answers a remaining\nquestion posed by Liaw, Mehrabian, Plan and Vershynin in 2017, but also\ngeneralizes their work. We also develop a new Bernstein type inequality for\nsub-exponential random variables, and a new Hanson-Wright inequality for\nquadratic forms of sub-Gaussian random variables, in both cases improving the\nbounds in the sub-Gaussian regime under moment constraints. Finally, we\nillustrate popular applications such as Johnson-Lindenstrauss embeddings, null\nspace property for 0-1 matrices, randomized sketches and blind demodulation,\nwhose theoretical guarantees can be improved by our results (in the\nsub-Gaussian case).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 23:06:20 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 21:30:18 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jeong", "Halyun", ""], ["Li", "Xiaowei", ""], ["Plan", "Yaniv", ""], ["Y\u0131lmaz", "\u00d6zg\u00fcr", ""]]}, {"id": "2001.10648", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi and Mohamed Ali Kaafar", "title": "Modelling and Quantifying Membership Information Leakage in Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have been shown to be vulnerable to membership\ninference attacks, i.e., inferring whether individuals' data have been used for\ntraining models. The lack of understanding about factors contributing success\nof these attacks motivates the need for modelling membership information\nleakage using information theory and for investigating properties of machine\nlearning models and training algorithms that can reduce membership information\nleakage. We use conditional mutual information leakage to measure the amount of\ninformation leakage from the trained machine learning model about the presence\nof an individual in the training dataset. We devise an upper bound for this\nmeasure of information leakage using Kullback--Leibler divergence that is more\namenable to numerical computation. We prove a direct relationship between the\nKullback--Leibler membership information leakage and the probability of success\nfor a hypothesis-testing adversary examining whether a particular data record\nbelongs to the training dataset of a machine learning model. We show that the\nmutual information leakage is a decreasing function of the training dataset\nsize and the regularization weight. We also prove that, if the sensitivity of\nthe machine learning model (defined in terms of the derivatives of the fitness\nwith respect to model parameters) is high, more membership information is\npotentially leaked. This illustrates that complex models, such as deep neural\nnetworks, are more susceptible to membership inference attacks in comparison to\nsimpler models with fewer degrees of freedom. We show that the amount of the\nmembership information leakage is reduced by\n$\\mathcal{O}(\\log^{1/2}(\\delta^{-1})\\epsilon^{-1})$ when using Gaussian\n$(\\epsilon,\\delta)$-differentially-private additive noises.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 00:42:08 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 00:43:42 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Farokhi", "Farhad", ""], ["Kaafar", "Mohamed Ali", ""]]}, {"id": "2001.10655", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Regularization Helps with Mitigating Poisoning Attacks:\n  Distributionally-Robust Machine Learning Using the Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR eess.SP math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use distributionally-robust optimization for machine learning to mitigate\nthe effect of data poisoning attacks. We provide performance guarantees for the\ntrained model on the original data (not including the poison records) by\ntraining the model for the worst-case distribution on a neighbourhood around\nthe empirical distribution (extracted from the training dataset corrupted by a\npoisoning attack) defined using the Wasserstein distance. We relax the\ndistributionally-robust machine learning problem by finding an upper bound for\nthe worst-case fitness based on the empirical sampled-averaged fitness and the\nLipschitz-constant of the fitness function (on the data for given model\nparameters) as regularizer. For regression models, we prove that this\nregularizer is equal to the dual norm of the model parameters. We use the Wine\nQuality dataset, the Boston Housing Market dataset, and the Adult dataset for\ndemonstrating the results of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 01:16:19 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "2001.10664", "submitter": "Yang Chen", "authors": "David E Jones, Robert N Trangucci, Yang Chen", "title": "Quantifying Observed Prior Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We distinguish two questions (i) how much information does the prior contain?\nand (ii) what is the effect of the prior? Several measures have been proposed\nfor quantifying effective prior sample size, for example Clarke [1996] and\nMorita et al. [2008]. However, these measures typically ignore the likelihood\nfor the inference currently at hand, and therefore address (i) rather than\n(ii). Since in practice (ii) is of great concern, Reimherr et al. [2014]\nintroduced a new class of effective prior sample size measures based on\nprior-likelihood discordance. We take this idea further towards its natural\nBayesian conclusion by proposing measures of effective prior sample size that\nnot only incorporate the general mathematical form of the likelihood but also\nthe specific data at hand. Thus, our measures do not average across datasets\nfrom the working model, but condition on the current observed data.\nConsequently, our measures can be highly variable, but we demonstrate that this\nis because the impact of a prior can be highly variable. Our measures are Bayes\nestimates of meaningful quantities and well communicate the extent to which\ninference is determined by the prior, or framed differently, the amount of\neffort saved due to having prior information. We illustrate our ideas through a\nnumber of examples including a Gaussian conjugate model (continuous\nobservations), a Beta-Binomial model (discrete observations), and a linear\nregression model (two unknown parameters). Future work on further developments\nof the methodology and an application to astronomy are discussed at the end.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:15:09 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Jones", "David E", ""], ["Trangucci", "Robert N", ""], ["Chen", "Yang", ""]]}, {"id": "2001.10679", "submitter": "Duzhe Wang", "authors": "Duzhe Wang, Po-Ling Loh", "title": "Adaptive Estimation and Statistical Inference for High-Dimensional\n  Graph-Based Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive estimation and statistical inference for\nhigh-dimensional graph-based linear models. In our model, the coordinates of\nregression coefficients correspond to an underlying undirected graph.\nFurthermore, the given graph governs the piecewise polynomial structure of the\nregression vector. In the adaptive estimation part, we apply graph-based\nregularization techniques and propose a family of locally adaptive estimators\ncalled the Graph-Piecewise-Polynomial-Lasso. We further study a one-step update\nof the Graph-Piecewise-Polynomial-Lasso for the problem of statistical\ninference. We develop the corresponding theory, which includes the fixed design\nand the sub-Gaussian random design. Finally, we illustrate the superior\nperformance of our approaches by extensive simulation studies and conclude with\nan application to an Arabidopsis thaliana microarray dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 03:25:10 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Wang", "Duzhe", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2001.10818", "submitter": "George Wynne", "authors": "George Wynne, Fran\\c{c}ois-Xavier Briol and Mark Girolami", "title": "Convergence Guarantees for Gaussian Process Means With Misspecified\n  Likelihoods and Smoothness", "comments": "Accepted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are ubiquitous in machine learning, statistics, and\napplied mathematics. They provide a flexible modelling framework for\napproximating functions, whilst simultaneously quantifying uncertainty.\nHowever, this is only true when the model is well-specified, which is often not\nthe case in practice. In this paper, we study the properties of Gaussian\nprocess means when the smoothness of the model and the likelihood function are\nmisspecified. In this setting, an important theoretical question of practial\nrelevance is how accurate the Gaussian process approximations will be given the\ndifficulty of the problem, our model and the extent of the misspecification.\nThe answer to this problem is particularly useful since it can inform our\nchoice of model and experimental design. In particular, we describe how the\nexperimental design and choice of kernel and kernel hyperparameters can be\nadapted to alleviate model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:28:27 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 15:52:29 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 12:33:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wynne", "George", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Girolami", "Mark", ""]]}, {"id": "2001.10877", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine and Joni Virta", "title": "On the behavior of extreme $d$-dimensional spatial quantiles under\n  minimal assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Spatial\" or \"geometric\" quantiles are the only multivariate quantiles coping\nwith both high-dimensional data and functional data, also in the framework of\nmultiple-output quantile regression. This work studies spatial quantiles in the\nfinite-dimensional case, where the spatial quantile $\\mu_{\\alpha,u}(P)$ of the\ndistribution $P$ taking values in $\\mathbb{R}^d $ is a point in $\\mathbb{R}^d$\nindexed by an order $\\alpha\\in[0,1)$ and a direction $u$ in the unit sphere\n$\\mathcal{S}^{d-1}$ of $\\mathbb{R}^d$ --- or equivalently by a vector $\\alpha\nu$ in the open unit ball of $\\mathbb{R}^d$. Recently, Girard and Stupfler\n(2017) proved that (i) the extreme quantiles $\\mu_{\\alpha,u}(P)$ obtained as\n$\\alpha\\to 1$ exit all compact sets of $\\mathbb{R}^d$ and that (ii) they do so\nin a direction converging to $u$. These results help understanding the nature\nof these quantiles: the first result is particularly striking as it holds even\nif $P$ has a bounded support, whereas the second one clarifies the delicate\ndependence of spatial quantiles on $u$. However, they were established under\nassumptions imposing that $P$ is non-atomic, so that it is unclear whether they\nhold for empirical probability measures. We improve on this by proving these\nresults under much milder conditions, allowing for the sample case. This\nprevents using gradient condition arguments, which makes the proofs very\nchallenging. We also weaken the well-known sufficient condition for uniqueness\nof finite-dimensional spatial quantiles.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:57:53 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Paindaveine", "Davy", ""], ["Virta", "Joni", ""]]}, {"id": "2001.10910", "submitter": "Sergio Brenner Miguel", "authors": "Sergio Brenner Miguel, Jan Johannes", "title": "Data-driven aggregation in non-parametric density estimation on the real\n  line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study non-parametric estimation of an unknown density with support in R\n(respectively R+). The proposed estimation procedure is based on the projection\non finite dimensional subspaces spanned by the Hermite (respectively the\nLaguerre) functions. The focus of this paper is to introduce a data-driven\naggregation approach in order to deal with the upcoming bias-variance\ntrade-off. Our novel procedure integrates the usual model selection method as a\nlimit case. We show the oracle- and the minimax-optimality of the data-driven\naggregated density estimator and hence its adaptivity. We present results of a\nsimulation study which allow to compare the finite sample performance of the\ndata-driven estimators using model selection compared to the new aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 15:53:48 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Miguel", "Sergio Brenner", ""], ["Johannes", "Jan", ""]]}, {"id": "2001.10917", "submitter": "Xiao Fang", "authors": "Xiao Fang, Yuta Koike", "title": "High-dimensional Central Limit Theorems by Stein's Method", "comments": "31 pages. Major update. Extension to the non-singular case", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain explicit error bounds for the $d$-dimensional normal approximation\non hyperrectangles for a random vector that has a Stein kernel, or admits an\nexchangeable pair coupling, or is a non-linear statistic of independent random\nvariables or a sum of $n$ locally dependent random vectors. We assume the\napproximating normal distribution has a non-singular covariance matrix. The\nerror bounds vanish even when the dimension $d$ is much larger than the sample\nsize $n$. We prove our main results using the approach of G\\\"otze (1991) in\nStein's method, together with modifications of an estimate of Anderson, Hall\nand Titterington (1998) and a smoothing inequality of Bhattacharya and Rao\n(1976). For sums of $n$ independent and identically distributed isotropic\nrandom vectors having a log-concave density, we obtain an error bound that is\noptimal up to a $\\log n$ factor. We also discuss an application to multiple\nWiener-It\\^{o} integrals.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 16:04:44 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 11:28:55 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Fang", "Xiao", ""], ["Koike", "Yuta", ""]]}, {"id": "2001.10965", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, George Wynne, Filip Tronarp, Chris J. Oates, Simo\n  S\\\"arkk\\\"a", "title": "Maximum likelihood estimation and uncertainty quantification for\n  Gaussian process approximation of deterministic functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the ubiquity of the Gaussian process regression model, few\ntheoretical results are available that account for the fact that parameters of\nthe covariance kernel typically need to be estimated from the dataset. This\narticle provides one of the first theoretical analyses in the context of\nGaussian process regression with a noiseless dataset. Specifically, we consider\nthe scenario where the scale parameter of a Sobolev kernel (such as a\nMat\\'{e}rn kernel) is estimated by maximum likelihood. We show that the maximum\nlikelihood estimation of the scale parameter alone provides significant\nadaptation against misspecification of the Gaussian process model in the sense\nthat the model can become \"slowly\" overconfident at worst, regardless of the\ndifference between the smoothness of the data-generating function and that\nexpected by the model. The analysis is based on a combination of techniques\nfrom nonparametric regression and scattered data interpolation. Empirical\nresults are provided in support of the theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:20:21 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:09:48 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 15:39:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Karvonen", "Toni", ""], ["Wynne", "George", ""], ["Tronarp", "Filip", ""], ["Oates", "Chris J.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2001.10973", "submitter": "Artyom Kovalevskii", "authors": "Mikhail Chebunin, Artyom Kovalevskii", "title": "Modifications of Simon text model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss probability text models and its modifications. We have proved a\ntheorem on the convergence of a multidimensional process of the number of urns\ncontaining a fixed number of balls in the Simon model to a multidimensional\nGaussian process. We have introduced and investigated three two-parameter urn\nschemes that guarantee the power tail asymptotics of the number of occupied\nurns. These models do not follow the restriction on the limitation of the ratio\nof the number of urns with exactly one ball to the number of occupied urns that\nappears in the classical Bahadur-Karlin model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:36:24 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""]]}, {"id": "2001.10982", "submitter": "Alex Dytso", "authors": "Alex Dytso and Michael Fau{\\ss} and H. Vincent Poor", "title": "A Cram\\'er-Rao Type Bound for Bayesian Risk with Bregman Loss", "comments": "This version contains some new examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general class of Bayesian lower bounds when the underlying loss function is\na Bregman divergence is demonstrated. This class can be considered as an\nextension of the Weinstein--Weiss family of bounds for the mean squared error\nand relies on finding a variational characterization of Bayesian risk. The\napproach allows for the derivation of a version of the Cram\\'er--Rao bound that\nis specific to a given Bregman divergence. The new generalization of the\nCram\\'er--Rao bound reduces to the classical one when the loss function is\ntaken to be the Euclidean norm. The effectiveness of the new bound is evaluated\nin the Poisson noise setting and the Binomial noise setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:47:12 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:17:53 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dytso", "Alex", ""], ["Fau\u00df", "Michael", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2001.10996", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock, David Preinerstorfer, Bezirgen Veliyev", "title": "Functional Sequential Treatment Allocation with Covariates", "comments": "The material in this paper replaces the material on covariates in\n  [v5] of \"Functional Sequential Treatment Allocation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-armed bandit problem with covariates. Given a realization\nof the covariate vector, instead of targeting the treatment with highest\nconditional expectation, the decision maker targets the treatment which\nmaximizes a general functional of the conditional potential outcome\ndistribution, e.g., a conditional quantile, trimmed mean, or a socio-economic\nfunctional such as an inequality, welfare or poverty measure. We develop\nexpected regret lower bounds for this problem, and construct a near minimax\noptimal assignment policy.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 18:08:53 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Preinerstorfer", "David", ""], ["Veliyev", "Bezirgen", ""]]}, {"id": "2001.11111", "submitter": "Wenda Zhou", "authors": "Morgane Austern and Wenda Zhou", "title": "Asymptotics of Cross-Validation", "comments": "62 pages, 3 tables; typos and minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation is a central tool in evaluating the performance of machine\nlearning and statistical models. However, despite its ubiquitous role, its\ntheoretical properties are still not well understood. We study the asymptotic\nproperties of the cross validated-risk for a large class of models. Under\nstability conditions, we establish a central limit theorem and Berry-Esseen\nbounds, which enable us to compute asymptotically accurate confidence\nintervals. Using our results, we paint a big picture for the statistical\nspeed-up of cross validation compared to a train-test split procedure. A\ncorollary of our results is that parametric M-estimators (or empirical risk\nminimizers) benefit from the \"full\" speed-up when performing cross-validation\nunder the training loss. In other common cases, such as when the training is\ndone using a surrogate loss or a regularizer, we show that the behavior of the\ncross-validated risk is complex with a variance reduction which may be smaller\nor larger than the \"full\" speed-up, depending on the model and the underlying\ndistribution. We allow the number of folds to grow with the number of\nobservations at any rate.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 22:06:21 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 11:06:45 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Austern", "Morgane", ""], ["Zhou", "Wenda", ""]]}, {"id": "2001.11168", "submitter": "Ryoya Yamasaki", "authors": "Ryoya Yamasaki, Toshiyuki Tanaka", "title": "Kernel Selection for Modal Linear Regression: Optimal Kernel and IRLS\n  Algorithm", "comments": "7 pages, 4 figures, published in the proceedings of the 18th IEEE\n  International Conference on Machine Learning and Applications - ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal linear regression (MLR) is a method for obtaining a conditional mode\npredictor as a linear model. We study kernel selection for MLR from two\nperspectives: \"which kernel achieves smaller error?\" and \"which kernel is\ncomputationally efficient?\". First, we show that a Biweight kernel is optimal\nin the sense of minimizing an asymptotic mean squared error of a resulting MLR\nparameter. This result is derived from our refined analysis of an asymptotic\nstatistical behavior of MLR. Secondly, we provide a kernel class for which\niteratively reweighted least-squares algorithm (IRLS) is guaranteed to\nconverge, and especially prove that IRLS with an Epanechnikov kernel terminates\nin a finite number of iterations. Simulation studies empirically verified that\nusing a Biweight kernel provides good estimation accuracy and that using an\nEpanechnikov kernel is computationally efficient. Our results improve MLR of\nwhich existing studies often stick to a Gaussian kernel and modal EM algorithm\nspecialized for it, by providing guidelines of kernel selection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 03:57:07 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Yamasaki", "Ryoya", ""], ["Tanaka", "Toshiyuki", ""]]}, {"id": "2001.11201", "submitter": "Vrettos Moulos", "authors": "Vrettos Moulos", "title": "Finite-Time Analysis of Round-Robin Kullback-Leibler Upper Confidence\n  Bounds for Optimal Adaptive Allocation with Multiple Plays and Markovian\n  Rewards", "comments": "31 pages, simulation results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extension of the classic stochastic multi-armed bandit problem\nwhich involves multiple plays and Markovian rewards in the rested bandits\nsetting. In order to tackle this problem we consider an adaptive allocation\nrule which at each stage combines the information from the sample means of all\nthe arms, with the Kullback-Leibler upper confidence bound of a single arm\nwhich is selected in round-robin way. For rewards generated from a\none-parameter exponential family of Markov chains, we provide a finite-time\nupper bound for the regret incurred from this adaptive allocation rule, which\nreveals the logarithmic dependence of the regret on the time horizon, and which\nis asymptotically optimal. For our analysis we devise several concentration\nresults for Markov chains, including a maximal inequality for Markov chains,\nthat may be of interest in their own right. As a byproduct of our analysis we\nalso establish asymptotically optimal, finite-time guarantees for the case of\nmultiple plays, and i.i.d. rewards drawn from a one-parameter exponential\nfamily of probability densities. Additionally, we provide simulation results\nthat illustrate that calculating Kullback-Leibler upper confidence bounds in a\nround-robin way, is significantly more efficient than calculating them for\nevery arm at each round, and that the expected regrets of those two approaches\nbehave similarly.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 08:09:01 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:28:37 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Moulos", "Vrettos", ""]]}, {"id": "2001.11213", "submitter": "Abderrazek Karoui", "authors": "Bilel Bousselmi, Jean-Fran\\c{c}ois Dupuy and Abderrazek Karoui", "title": "Reproducing kernels based schemes for nonparametric regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop and study an empirical projection operator scheme\nfor solving nonparametric regression problems. This scheme is based on an\napproximate projection of the regression function over a suitable reproducing\nkernel Hilbert space (RKHS). The RKHS considered in this paper are generated by\nthe Mercer kernels given by the Legendre Christoffel-Darboux and convolution\nSinc kernels. We provide error and convergence analysis of the proposed scheme\nunder the assumption that the regression function belongs to some suitable\nfunctional spaces. We also consider the popular RKHS regularized least square\nminimization for nonparametric regression. In particular, we check the\nnumerical stability of this second scheme and we provide its convergence rate\nin the special case of the Sinc kernel. Finally, we illustrate the proposed\nmethods by various numerical simulation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 08:42:28 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 05:25:37 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bousselmi", "Bilel", ""], ["Dupuy", "Jean-Fran\u00e7ois", ""], ["Karoui", "Abderrazek", ""]]}, {"id": "2001.11408", "submitter": "Johan Segers", "authors": "John H.J. Einmahl and Johan Segers", "title": "Empirical tail copulas for functional data", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multivariate distributions in the domain of attraction of a max-stable\ndistribution, the tail copula and the stable tail dependence function are\nequivalent ways to capture the dependence in the upper tail. The empirical\nversions of these functions are rank-based estimators whose inflated estimation\nerrors are known to converge weakly to a Gaussian process that is similar in\nstructure to the weak limit of the empirical copula process. We extend this\nmultivariate result to continuous functional data by establishing the\nasymptotic normality of the estimators of the tail copula, uniformly over all\nfinite subsets of at most $D$ points ($D$ fixed). An application for testing\ntail copula stationarity is presented. The main tool for deriving the result is\nthe uniform asymptotic normality of all the $D$-variate tail empirical\nprocesses. The proof of the main result is non-standard.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:41:13 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:34:17 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Einmahl", "John H. J.", ""], ["Segers", "Johan", ""]]}, {"id": "2001.11443", "submitter": "Huy Tuan Pham", "authors": "Phan-Minh Nguyen, Huy Tuan Pham", "title": "A Rigorous Framework for the Mean Field Limit of Multilayer Neural\n  Networks", "comments": "116 pages. This version incorporates the content of the companion\n  note arXiv:2006.09355 (June 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mathematically rigorous framework for multilayer neural networks\nin the mean field regime. As the network's widths increase, the network's\nlearning trajectory is shown to be well captured by a meaningful and\ndynamically nonlinear limit (the \\textit{mean field} limit), which is\ncharacterized by a system of ODEs. Our framework applies to a broad range of\nnetwork architectures, learning dynamics and network initializations. Central\nto the framework is the new idea of a \\textit{neuronal embedding}, which\ncomprises of a non-evolving probability space that allows to embed neural\nnetworks of arbitrary widths.\n  Using our framework, we prove several properties of large-width multilayer\nneural networks. Firstly we show that independent and identically distributed\ninitializations cause strong degeneracy effects on the network's learning\ntrajectory when the network's depth is at least four. Secondly we obtain\nseveral global convergence guarantees for feedforward multilayer networks under\na number of different setups. These include two-layer and three-layer networks\nwith independent and identically distributed initializations, and multilayer\nnetworks of arbitrary depths with a special type of correlated initializations\nthat is motivated by the new concept of \\textit{bidirectional diversity}.\nUnlike previous works that rely on convexity, our results admit non-convex\nlosses and hinge on a certain universal approximation property, which is a\ndistinctive feature of infinite-width neural networks and is shown to hold\nthroughout the training process. Aside from being the first known results for\nglobal convergence of multilayer networks in the mean field regime, they\ndemonstrate flexibility of our framework and incorporate several new ideas and\ninsights that depart from the conventional convex optimization wisdom.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:43:34 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:44:02 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nguyen", "Phan-Minh", ""], ["Pham", "Huy Tuan", ""]]}, {"id": "2001.11459", "submitter": "Johannes Heiny", "authors": "Johannes Heiny and Thomas Mikosch", "title": "Almost sure convergence of the largest and smallest eigenvalues of\n  high-dimensional sample correlation matrices", "comments": "Stochastic Process. Appl. 128 (2018), no. 8, 2779-2815", "journal-ref": null, "doi": "10.1016/j.spa.2017.10.002", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that the largest and smallest eigenvalues of a sample\ncorrelation matrix stemming from $n$ independent observations of a\n$p$-dimensional time series with iid components converge almost surely to\n$(1+\\sqrt{\\gamma})^2$ and $(1-\\sqrt{\\gamma})^2$, respectively, as $n \\to\n\\infty$, if $p/n\\to \\gamma \\in (0,1]$ and the truncated variance of the entry\ndistribution is 'almost slowly varying', a condition we describe via moment\nproperties of self-normalized sums. Moreover, the empirical spectral\ndistributions of these sample correlation matrices converge weakly, with\nprobability 1, to the Marchenko-Pastur law, which extends a result in Bai and\nZhou (2008). We compare the behavior of the eigenvalues of the sample\ncovariance and sample correlation matrices and argue that the latter seems more\nrobust, in particular in the case of infinite fourth moment. We briefly address\nsome practical issues for the estimation of extreme eigenvalues in a simulation\nstudy.\n  In our proofs we use the method of moments combined with a Path-Shortening\nAlgorithm, which efficiently uses the structure of sample correlation matrices,\nto calculate precise bounds for matrix norms. We believe that this new approach\ncould be of further use in random matrix theory.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 17:11:09 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Heiny", "Johannes", ""], ["Mikosch", "Thomas", ""]]}, {"id": "2001.11473", "submitter": "Gonzalo Rios", "authors": "Gonzalo Rios", "title": "Transport Gaussian Processes for Regression", "comments": "19 pages, 2 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) priors are non-parametric generative models with\nappealing modelling properties for Bayesian inference: they can model\nnon-linear relationships through noisy observations, have closed-form\nexpressions for training and inference, and are governed by interpretable\nhyperparameters. However, GP models rely on Gaussianity, an assumption that\ndoes not hold in several real-world scenarios, e.g., when observations are\nbounded or have extreme-value dependencies, a natural phenomenon in physics,\nfinance and social sciences. Although beyond-Gaussian stochastic processes have\ncaught the attention of the GP community, a principled definition and rigorous\ntreatment is still lacking. In this regard, we propose a methodology to\nconstruct stochastic processes, which include GPs, warped GPs, Student-t\nprocesses and several others under a single unified approach. We also provide\nformulas and algorithms for training and inference of the proposed models in\nthe regression problem. Our approach is inspired by layers-based models, where\neach proposed layer changes a specific property over the generated stochastic\nprocess. That, in turn, allows us to push-forward a standard Gaussian white\nnoise prior towards other more expressive stochastic processes, for which\nmarginals and copulas need not be Gaussian, while retaining the appealing\nproperties of GPs. We validate the proposed model through experiments with\nreal-world data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 17:44:21 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Rios", "Gonzalo", ""]]}, {"id": "2001.11624", "submitter": "Simon Clinet", "authors": "Simon Clinet", "title": "Quasi-likelihood analysis for marked point processes and application to\n  marked Hawkes processes", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a quasi-likelihood analysis procedure for a general class of\nmultivariate marked point processes. As a by-product of the general method, we\nestablish under stability and ergodicity conditions the local asymptotic\nnormality of the quasi-log likelihood, along with the convergence of moments of\nquasi-likelihood and quasi-Bayesian estimators. To illustrate the general\napproach, we then turn our attention to a class of multivariate marked Hawkes\nprocesses with generalized exponential kernels, comprising among others the\nso-called Erlang kernels. We provide explicit conditions on the kernel\nfunctions and the mark dynamics under which a certain transformation of the\noriginal process is Markovian and $V$-geometrically ergodic. We finally prove\nthat the latter result, which is of interest in its own right, constitutes the\nkey ingredient to show that the generalized exponential Hawkes process falls\nunder the scope of application of the quasi-likelihood analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 01:26:44 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Clinet", "Simon", ""]]}, {"id": "2001.11838", "submitter": "Boris Ryabko", "authors": "Boris Ryabko", "title": "The time-adaptive statistical testing for random number generators", "comments": "arXiv admin note: text overlap with arXiv:1912.06542", "journal-ref": null, "doi": "10.3390/e22060630", "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of constructing effective statistical tests for random number\ngenerators (RNG) is considered. Currently, there are hundreds of RNG\nstatistical tests that are often combined into so-called batteries, each\ncontaining from a dozen to more than one hundred tests.\n  When a battery test is used, it is applied to a sequence generated by the\nRNG, and the calculation time is determined by the length of the sequence and\nthe number of tests. Generally speaking, the longer the sequence, the smaller\ndeviations from randomness can be found by a specific test. So, when a battery\nis applied, on the one hand, the \"better\" tests are in the battery, the more\nchances to reject a \"bad\" RNG. On the other hand, the larger the battery, the\nless time can be spent on each test and, therefore, the shorter the test\nsequence. In turn, this reduces the ability to find small deviations from\nrandomness. To reduce this trade-off, we propose an adaptive way to use\nbatteries (and other sets) of tests, which requires less time but, in a certain\nsense, preserves the power of the original battery. We call this method\ntime-adaptive battery of tests.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 14:51:02 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 12:53:54 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ryabko", "Boris", ""]]}, {"id": "2001.11851", "submitter": "Tongseok Lim", "authors": "Tongseok Lim and Robert J. McCann", "title": "Geometrical bounds for the variance and recentered moments", "comments": "18 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1907.13593", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We bound the variance and other moments of a random vector based on the range\nof its realizations, thus generalizing inequalities of Popoviciu (1935) and\nBhatia and Davis (2000) concerning measures on the line to several dimensions.\nThis is done using convex duality and (infinite-dimensional) linear\nprogramming.\n  The following consequence of our bounds exhibits symmetry breaking, provides\na new proof of Jung's theorem (1901), and turns out to have applications to the\naggregation dynamics modelling attractive-repulsive interactions: among\nprobability measures on ${\\mathbf R}^n$ whose support has diameter at most\n$\\sqrt{2}$, we show that the variance around the mean is maximized precisely by\nthose measures which assign mass $1/(n+1)$ to each vertex of a standard\nsimplex.\n  For $1 \\le p <\\infty$, the $p$-th moment --- optimally centered --- is\nmaximized by the same measures among those satisfying the diameter constraint.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:02:06 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Lim", "Tongseok", ""], ["McCann", "Robert J.", ""]]}, {"id": "2001.11860", "submitter": "Sibo Cheng", "authors": "Sibo Cheng (EDF R&D PERICLES, LIMSI), Jean-Philippe Argaud (EDF R&D\n  PERICLES), Bertrand Iooss (EDF R&D PRISME, IMT), Ang\\'elique Pon\\c{c}ot (EDF\n  R&D PERICLES), Didier Lucor (LIMSI)", "title": "A graph clustering approach to localization for adaptive covariance\n  tuning in data assimilation based on state-observation mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An original graph clustering approach to efficient localization of error\ncovariances is proposed within an ensemble-variational data assimilation\nframework. Here the localization term is very generic and refers to the idea of\nbreaking up a global assimilation into subproblems. This unsupervised\nlocalization technique based on a linearizedstate-observation measure is\ngeneral and does not rely on any prior information such as relevant spatial\nscales, empirical cut-off radius or homogeneity assumptions. It automatically\nsegregates the state and observation variables in an optimal number of clusters\n(otherwise named as subspaces or communities), more amenable to scalable data\nassimilation.The application of this method does not require underlying\nblock-diagonal structures of prior covariance matrices. In order to deal with\ninter-cluster connectivity, two alternative data adaptations are proposed. Once\nthe localization is completed, an adaptive covariance diagnosis and tuning is\nperformed within each cluster. Numerical tests show that this approach is less\ncostly and more flexible than a global covariance tuning, and most often\nresults in more accurate background and observations error covariances.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:15:31 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Cheng", "Sibo", "", "EDF R&D PERICLES, LIMSI"], ["Argaud", "Jean-Philippe", "", "EDF R&D\n  PERICLES"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT"], ["Pon\u00e7ot", "Ang\u00e9lique", "", "EDF\n  R&D PERICLES"], ["Lucor", "Didier", "", "LIMSI"]]}, {"id": "2001.11861", "submitter": "Lioudmila Vostrikova", "authors": "Elena Boguslavskaya, Lioudmila Vostrikova (LAREMA)", "title": "Revisiting integral functionals of geometric Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.CP q-fin.GN q-fin.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the integral functional of geometric Brownian motion\n$I_t= \\int_0^t e^{-(\\mu s +\\sigma W_s)}ds$, where $\\mu\\in\\mathbb{R}$, $\\sigma >\n0$, and $(W_s )_s>0$ is a standard Brownian motion. Specifically, we calculate\nthe Laplace transform in $t$ of the cumulative distribution function and of the\nprobability density function of this functional.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:17:20 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Boguslavskaya", "Elena", "", "LAREMA"], ["Vostrikova", "Lioudmila", "", "LAREMA"]]}, {"id": "2001.11874", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen, Su-Yun Huang, Weichung Wang", "title": "A Consistency Theorem for Randomized Singular Value Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The singular value decomposition (SVD) and the principal component analysis\nare fundamental tools and probably the most popular methods for data dimension\nreduction. The rapid growth in the size of data matrices has lead to a need for\ndeveloping efficient large-scale SVD algorithms. Randomized SVD was proposed,\nand its potential was demonstrated for computing a low-rank SVD (Rokhlin et\nal., 2009). In this article, we provide a consistency theorem for the\nrandomized SVD algorithm and a numerical example to show how the random\nprojections to low dimension affect the consistency.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:44:54 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Chen", "Ting-Li", ""], ["Huang", "Su-Yun", ""], ["Wang", "Weichung", ""]]}, {"id": "2001.11942", "submitter": "Anirudh Sridhar", "authors": "Anirudh Sridhar, H. Vincent Poor", "title": "Bayes-optimal Methods for Finding the Source of a Cascade", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the source of a network cascade. The\ncascade starts from a single vertex at time 0 and spreads over time, but only a\nnoisy version of the propagation is observable. The goal is then to design a\nstopping time and estimator that will estimate the source well while ensuring\nthe cost of the cascade to the system is not too large. We rigorously formulate\na Bayesian approach to the problem. If vertices can be labelled by vectors in\nEuclidean space (which is natural in geo-spatial networks), the optimal\nestimator is the conditional mean estimator, and we derive an explicit form for\nthe optimal stopping time under minimal assumptions on the cascade dynamics. We\nstudy the performance of the optimal stopping time on lattices, and show that a\ncomputationally efficient but suboptimal stopping time which compares the\nposterior variance to a threshold has near-optimal performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:36:49 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:01:57 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 00:27:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sridhar", "Anirudh", ""], ["Poor", "H. Vincent", ""]]}]