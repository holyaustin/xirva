[{"id": "1503.00163", "submitter": "Matteo Ruggiero", "authors": "P. De Blasi, S. Favaro, A. Lijoi, R.H. Mena, I. Pruenster and M.\n  Ruggiero", "title": "Are Gibbs-type priors the most natural generalization of the Dirichlet\n  process?", "comments": "5 figures", "journal-ref": "IEEE Transactions Pattern Analysis and Machine Intelligence 2015,\n  Vol. 37, No. 2, pp. 212-229", "doi": "10.1109/TPAMI.2013.217", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete random probability measures and the exchangeable random partitions\nthey induce are key tools for addressing a variety of estimation and prediction\nproblems in Bayesian inference. Indeed, many popular nonparametric priors, such\nas the Dirichlet and the Pitman-Yor process priors, select discrete probability\ndistributions almost surely and, therefore, automatically induce exchangeable\nrandom partitions. Here we focus on the family of Gibbs-type priors, a recent\nand elegant generalization of the Dirichlet and the Pitman-Yor process priors.\nThese random probability measures share properties that are appealing both from\na theoretical and an applied point of view: (i) they admit an intuitive\ncharacterization in terms of their predictive structure justifying their use in\nterms of a precise assumption on the learning mechanism; (ii) they stand out in\nterms of mathematical tractability; (iii) they include several interesting\nspecial cases besides the Dirichlet and the Pitman-Yor processes. The goal of\nour paper is to provide a systematic and unified treatment of Gibbs-type priors\nand highlight their implications for Bayesian nonparametric inference. We will\ndeal with their distributional properties, the resulting estimators,\nfrequentist asymptotic validation and the construction of time-dependent\nversions. Applications, mainly concerning hierarchical mixture models and\nspecies sampling, will serve to convey the main ideas. The intuition inherent\nto this class of priors and the neat results that can be deduced for it lead\none to wonder whether it actually represents the most natural generalization of\nthe Dirichlet process.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 18:28:27 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["De Blasi", "P.", ""], ["Favaro", "S.", ""], ["Lijoi", "A.", ""], ["Mena", "R. H.", ""], ["Pruenster", "I.", ""], ["Ruggiero", "M.", ""]]}, {"id": "1503.00167", "submitter": "Vasily Vasilyev Olegovich", "authors": "Vasily Vasilyev and Alexander Dobrovidov", "title": "On estimation states of hidden markov models in condition of unknown\n  transition matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop methods of nonlinear filtering and prediction of an\nunobservable Markov chain with a finite set of states. This Markov chain\ncontrols coefficients of AR(p) model. Using observations generated by AR(p)\nmodel we have to estimate the state of Markov chain in the case of an unknown\nprobability transition matrix. Comparison of proposed non-parametric algorithms\nwith the optimal methods in the case of the known transition matrix is carried\nout by simulating.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 19:17:00 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 00:17:50 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Vasilyev", "Vasily", ""], ["Dobrovidov", "Alexander", ""]]}, {"id": "1503.00226", "submitter": "Sarah Lemler", "authors": "Agathe Guilloux (LSTA), Sarah Lemler (LaMME), Marie-Luce Taupin\n  (Unit\\'e MIAJ, LaMME)", "title": "Adaptive estimation of the baseline hazard function in the Cox model by\n  model selection, with high-dimensional covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to provide an adaptive estimator of the\nbaseline function in the Cox model with high-dimensional covariates. We\nconsider a two-step procedure : first, we estimate the regression parameter of\nthe Cox model via a Lasso procedure based on the partial log-likelihood,\nsecondly, we plug this Lasso estimator into a least-squares type criterion and\nthen perform a model selection procedure to obtain an adaptive penalized\ncontrast estimator of the baseline function. Using non-asymptotic estimation\nresults stated for the Lasso estimator of the regression parameter, we\nestablish a non-asymptotic oracle inequality for this penalized contrast\nestimator of the baseline function, which highlights the discrepancy of the\nrate of convergence when the dimension of the covariates increases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 07:22:08 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 14:14:24 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Guilloux", "Agathe", "", "LSTA"], ["Lemler", "Sarah", "", "LaMME"], ["Taupin", "Marie-Luce", "", "Unit\u00e9 MIAJ, LaMME"]]}, {"id": "1503.00378", "submitter": "Akimichi Takemura", "authors": "Tamio Koyama and Akimichi Takemura", "title": "Holonomic gradient method for distribution function of a weighted sum of\n  noncentral chi-square random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the holonomic gradient method to compute the distribution function\nof a weighted sum of independent noncentral chi-square random variables. It is\nthe distribution function of the squared length of a multivariate normal random\nvector. We treat this distribution as an integral of the normalizing constant\nof the Fisher-Bingham distribution on the unit sphere and make use of the\npartial differential equations for the Fisher-Bingham distribution.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 23:56:25 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 00:40:21 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2015 02:42:20 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Koyama", "Tamio", ""], ["Takemura", "Akimichi", ""]]}, {"id": "1503.00466", "submitter": "Jakub Chorowski", "authors": "Jakub Chorowski and Mathias Trabs", "title": "Spectral estimation for diffusions with random sampling times", "comments": "30 pages, 2 figures", "journal-ref": "Stochastic Processes and their Applications, 126 (10), 2976-3008,\n  2016", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonparametric estimation of the volatility and the drift coefficient of a\nscalar diffusion is studied when the process is observed at random time points.\nThe constructed estimator generalizes the spectral method by Gobet, Hoffmann\nand Rei{\\ss} [Ann. Statist. 32 (2006), 2223-2253]. The estimation procedure is\noptimal in the minimax sense and adaptive with respect to the sampling time\ndistribution and the regularity of the coefficients. The proofs are based on\nthe eigenvalue problem for the generalized transition operator. The finite\nsample performance is illustrated in a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 10:13:37 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 14:39:59 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2015 18:13:13 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Chorowski", "Jakub", ""], ["Trabs", "Mathias", ""]]}, {"id": "1503.00489", "submitter": "Cees de Valk", "authors": "Cees de Valk", "title": "Approximation and estimation of very small probabilities of multivariate\n  extreme events", "comments": null, "journal-ref": "Extremes 19(4) 687-717 (2016)", "doi": "10.1007/s10687-016-0252-6", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses modelling of the tail of a multivariate distribution\nfunction by means of a large deviation principle (LDP), and its application to\nthe estimation of the probability of a multivariate extreme event from a sample\nof n iid random vectors, with the probability bounded by powers of sample size\nwith exponents below -1. One way to view classical tail limits is as limits of\nprobability ratios. In contrast, the tail LDP provides asymptotic bounds or\nlimits for log-probability ratios. After standardising the marginals to\nstandard exponential, dependence is represented by a homogeneous rate function.\nFurthermore, the tail LDP can be extended to represent both dependence and\nmarginals, the latter implying marginal log-GW tail limits. A connection is\nestablished between the tail LDP and residual tail dependence (or hidden\nregular variation) and a recent extension of it. Under a smoothness assumption,\nthey are implied by the tail LDP. Based on the tail LDP, a simple estimator for\nvery small probabilities of extreme events is formulated. It avoids estimation\nof the rate function by making use of its homogeneity. Strong consistency in\nthe sense of convergence of log-probability ratios is proven. Simulations and\nan application illustrate the difference between the classical approach and the\nLDP-based approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 11:51:50 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 19:59:28 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["de Valk", "Cees", ""]]}, {"id": "1503.00677", "submitter": "Christopher Ferrie", "authors": "Richard Kueng and Christopher Ferrie", "title": "Near-optimal quantum tomography: estimators and bounds", "comments": "These guys submitted v1 with a clickbait title. You probably could\n  have guessed what happened next", "journal-ref": "New J. Phys. 17 (2015) 123013", "doi": "10.1088/1367-2630/17/12/123013", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give bounds on the average fidelity achievable by any quantum state\nestimator, which is arguably the most prominently used figure of merit in\nquantum state tomography. Moreover, these bounds can be computed online---that\nis, while the experiment is running. We show numerically that these bounds are\nquite tight for relevant distributions of density matrices. We also show that\nthe Bayesian mean estimator is ideal in the sense of performing close to the\nbound without requiring optimization. Our results hold for all finite\ndimensional quantum systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 19:54:01 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 22:25:10 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Kueng", "Richard", ""], ["Ferrie", "Christopher", ""]]}, {"id": "1503.00741", "submitter": "Gregory Rice", "authors": "Istv\\'an Berkes, Lajos Horv\\'ath, and Gregory Rice", "title": "On the asymptotic normality of kernel estimators of the long run\n  covariance of functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the asymptotic normality in $L^2$ of kernel estimators of the\nlong run covariance kernel of stationary functional time series. Our results\nare established assuming a weakly dependent Bernoulli shift structure for the\nunderlying observations, which contains most stationary functional time series\nmodels, under mild conditions. As a corollary, we obtain joint asymptotics for\nfunctional principal components computed from empirical long run covariance\noperators, showing that they have the favorable property of being\nasymptotically independent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 21:05:19 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 16:10:46 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Berkes", "Istv\u00e1n", ""], ["Horv\u00e1th", "Lajos", ""], ["Rice", "Gregory", ""]]}, {"id": "1503.00946", "submitter": "Claire Lacour", "authors": "Claire Lacour (LM-Orsay, SELECT), Pascal Massart (LM-Orsay, SELECT)", "title": "Minimal penalty for Goldenshluger-Lepski method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with adaptive nonparametric estimation using the\nGoldenshluger-Lepski selection method. This estimator selection method is based\non pairwise comparisons between estimators with respect to some loss function.\nThe method also involves a penalty term that typically needs to be large enough\nin order that the method works (in the sense that one can prove some oracle\ntype inequality for the selected estimator). In the case of density estimation\nwith kernel estimators and a quadratic loss, we show that the procedure fails\nif the penalty term is chosen smaller than some critical value for the penalty:\nthe minimal penalty. More precisely we show that the quadratic risk of the\nselected estimator explodes when the penalty is below this critical value while\nit stays under control when the penalty is above this critical value. This kind\nof phase transition phenomenon for penalty calibration has already been\nobserved and proved for penalized model selection methods in various contexts\nbut appears here for the first time for the Goldenshluger-Lepski pairwise\ncomparison method. Some simulations illustrate the theoretical results and lead\nto some hints on how to use the theory to calibrate the method in practice.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 14:00:27 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 11:58:30 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Lacour", "Claire", "", "LM-Orsay, SELECT"], ["Massart", "Pascal", "", "LM-Orsay, SELECT"]]}, {"id": "1503.00966", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Daniel M. Roy", "title": "Sequential Monte Carlo as Approximate Sampling: bounds, adaptive\n  resampling via $\\infty$-ESS, and an application to Particle Gibbs", "comments": "34 pages", "journal-ref": "Bernoulli, Volume 25, Number 1 (2019), 584-622", "doi": "10.3150/17-BEJ999", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) algorithms were originally designed for\nestimating intractable conditional expectations within state-space models, but\nare now routinely used to generate approximate samples in the context of\ngeneral-purpose Bayesian inference. In particular, SMC algorithms are often\nused as subroutines within larger Monte Carlo schemes, and in this context, the\ndemands placed on SMC are different: control of mean-squared error is\ninsufficient---one needs to control the divergence from the target distribution\ndirectly. Towards this goal, we introduce the conditional adaptive resampling\nparticle filter, building on the work of Gordon, Salmond, and Smith (1993),\nAndrieu, Doucet, and Holenstein (2010), and Whiteley, Lee, and Heine (2016). By\ncontrolling a novel notion of effective sample size, the $\\infty$-ESS, we\nestablish the efficiency of the resulting SMC sampling algorithm, providing an\nadaptive resampling extension of the work of Andrieu, Lee, and Vihola (2013).\nWe apply our results to arrive at new divergence bounds for SMC samplers with\nadaptive resampling as well as an adaptive resampling version of the Particle\nGibbs algorithm with the same geometric-ergodicity guarantees as its\nnonadaptive counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 15:03:31 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:15:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1503.01075", "submitter": "Robert Chen", "authors": "Robert W. Chen", "title": "Two Interesting Properties of the Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1, X_2,\\ldots, X_n$ be $n$ independent and identically distributed\nrandom variables, here $n \\geq 2.$ Let $X_{(1)}, X_{(2)}, \\ldots, X_{(n)}$ be\nthe order statistics of $X_1, X_2,..., X_n.$ In this note we proved that: (I)\nIf $X_1, X_2,..., X_n$ are exponential random variables with parameter $c > 0,$\nthen the \"correlation coefficient\" between $X_{(k)}$ and $X_{(k+t)}$ is\nstrictly increasing in $k$ from $1$ to $m,$ and then is strictly decreasing in\n$k$ from $m$ to $n - t,$ here $t$ is a fixed integer between $1$ and $n - 3,$\nand $m = (n - t)/2$ if $n - t$ is even, $m = (n - t + 1)/2$ if $n - t$ is odd.\nWe also proved that if $t = n - 2$, then the \"correlation coefficient\" between\n$X_{(1)}$ and $X_{(n-1)}$ is greater than the \"correlation coefficient\" between\n$X_{(2)}$ and$X_{(n)}.$ (II) The \"correlation coefficient\" between $X_{(k)}$\nand $X_{(k+t)}$ for the exponential random variables is always less than the\n\"correlation coefficient\" between $X_{(k)}$ and $X_{(k+t)}$ for the uniform\nrandom variables for all $k$ and $t$ such that $k + t \\leq n.$ A combinatorial\nidentity is also given as a bi-product. \\vs\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:40:07 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Chen", "Robert W.", ""]]}, {"id": "1503.01245", "submitter": "David Morales-Jimenez", "authors": "David Morales-Jimenez, Romain Couillet, Matthew R. McKay", "title": "Large Dimensional Analysis of Robust M-Estimators of Covariance with\n  Outliers", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2460225", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large dimensional characterization of robust M-estimators of covariance (or\nscatter) is provided under the assumption that the dataset comprises\nindependent (essentially Gaussian) legitimate samples as well as arbitrary\ndeterministic samples, referred to as outliers. Building upon recent random\nmatrix advances in the area of robust statistics, we specifically show that the\nso-called Maronna M-estimator of scatter asymptotically behaves similar to\nwell-known random matrices when the population and sample sizes grow together\nto infinity. The introduction of outliers leads the robust estimator to behave\nasymptotically as the weighted sum of the sample outer products, with a\nconstant weight for all legitimate samples and different weights for the\noutliers. A fine analysis of this structure reveals importantly that the\npropensity of the M-estimator to attenuate (or enhance) the impact of outliers\nis mostly dictated by the alignment of the outliers with the inverse population\ncovariance matrix of the legitimate samples. Thus, robust M-estimators can\nbring substantial benefits over more simplistic estimators such as the\nper-sample normalized version of the sample covariance matrix, which is not\ncapable of differentiating the outlying samples. The analysis shows that,\nwithin the class of Maronna's estimators of scatter, the Huber estimator is\nmost favorable for rejecting outliers. On the contrary, estimators more similar\nto Tyler's scale invariant estimator (often preferred in the literature) run\nthe risk of inadvertently enhancing some outliers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 07:28:27 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Morales-Jimenez", "David", ""], ["Couillet", "Romain", ""], ["McKay", "Matthew R.", ""]]}, {"id": "1503.01271", "submitter": "Pascal Vallet", "authors": "Pascal Vallet, Xavier Mestre, Philippe Loubaton", "title": "Performance analysis of an improved MUSIC DoA estimator", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper adresses the statistical performance of subspace DoA estimation\nusing a sensor array, in the asymptotic regime where the number of samples and\nsensors both converge to infinity at the same rate. Improved subspace DoA\nestimators were derived (termed as G-MUSIC) in previous works, and were shown\nto be consistent and asymptotically Gaussian distributed in the case where the\nnumber of sources and their DoA remain fixed. In this case, which models widely\nspaced DoA scenarios, it is proved in the present paper that the traditional\nMUSIC method also provides DoA consistent estimates having the same asymptotic\nvariances as the G-MUSIC estimates. The case of DoA that are spaced of the\norder of a beamwidth, which models closely spaced sources, is also considered.\nIt is shown that G-MUSIC estimates are still able to consistently separate the\nsources, while it is no longer the case for the MUSIC ones. The asymptotic\nvariances of G-MUSIC estimates are also evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 10:41:05 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 08:19:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Vallet", "Pascal", ""], ["Mestre", "Xavier", ""], ["Loubaton", "Philippe", ""]]}, {"id": "1503.01323", "submitter": "Rajesh Singh", "authors": "Viplav Kumar Singh and Rajesh Singh", "title": "Estimation of mean using dual-to-ratio and difference-type estimators\n  under measurement error model", "comments": "14 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sample survey, when data is collected, it is assumed that whatever is\nreported by respondent is correct. However, given the issues of prestige bias,\npersonal respect, respondents self reported data often produces over-or-under\nestimated values from true value. This causes measurement error to be present\nin sample values. In support of this study, we have considered some precise\nclasses using dual under measurement error model. The expressions for the bias\nand the mean square errors of proposed classes have been derived and compared\nwith, the mean per unit estimator, the Srivenkataramana 1980 estimator and\nSharma and Tailor 2010 estimator.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 05:35:25 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Singh", "Viplav Kumar", ""], ["Singh", "Rajesh", ""]]}, {"id": "1503.01504", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel", "title": "Uniform Behaviors of Random Polytopes under the Hausdorff Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Hausdorff distance between a random polytope, defined as the\nconvex hull of i.i.d. random points, and the convex hull of the support of\ntheir distribution. As particular examples, we consider uniform distributions\non convex bodies, densities that decay at a certain rate when approaching the\nboundary of a convex body, projections of uniform distributions on higher\ndimensional convex bodies and uniform distributions on the boundary of convex\nbodies. We essentially distinguish two types of convex bodies: those with a\nsmooth boundary and polytopes. In the case of uniform distributions, we prove\nthat, in some sense, the random polytope achieves its best statistical accuracy\nunder the Hausdorff metric when the support has a smooth boundary and its worst\nstatistical accuracy when the support is a polytope. This is somewhat\nsurprising, since the exact opposite is true under the Nikodym metric. We prove\nrate optimality of most our results in a minimax sense. In the case of uniform\ndistributions, we extend our results to a rescaled version of the Hausdorff\nmetric. We also tackle the estimation of functionals of the support of a\ndistribution such as its mean width and its diameter. Finally, we show that\nhigh dimensional random polytopes can be approximated with simple polyhedral\nrepresentations that significantly decrease their computational complexity\nwithout affecting their statistical accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 00:21:33 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 15:05:11 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1503.01587", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB), Nicolas Papadakis (IMB), Joseph Salmon", "title": "On debiasing restoration algorithms: applications to total-variation and\n  nonlocal-means", "comments": "Scale Space and Variational Methods in Computer Vision 2015, May\n  2015, L{\\`e}ge Cap Ferret, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias in image restoration algorithms can hamper further analysis, typically\nwhen the intensities have a physical meaning of interest, e.g., in medical\nimaging. We propose to suppress a part of the bias -- the method bias -- while\nleaving unchanged the other unavoidable part -- the model bias. Our debiasing\ntechnique can be used for any locally affine estimator including \\^a1\nregularization, anisotropic total-variation and some nonlocal filters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 09:46:07 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"], ["Papadakis", "Nicolas", "", "IMB"], ["Salmon", "Joseph", ""]]}, {"id": "1503.01727", "submitter": "Marcos Maruo", "authors": "Marcos H. Maruo and Jos\\'e C. M. Bermudez and Leonardo S. Resende", "title": "Statistical Analysis of a GSC-based Jointly Optimized\n  Beamformer-Assisted Acoustic Echo Canceler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a statistical analysis of a class of jointly optimized\nbeamformer-assisted acoustic echo cancelers (AEC) with the beamformer (BF)\nimplemented in the Generalized Sidelobe Canceler (GSC) form and using the\nleast-mean square (LMS) algorithm. The analysis considers the possibility of\nindependent convergence control for the BF and the AEC. The resulting models\npermit the study of system performance under typical handling of double-talk\nand channel changes. We show that the joint optimization of the BF-AEC is\nequivalent to a linearly-constrained minimum variance problem. Hence, the\nderived analytical model can be used to predict the transient performance of\ngeneral adaptive wideband beamformers. We study the transient and steady-state\nbehaviors of the residual mean echo power for stationary Gaussian inputs. A\nconvergence analysis leads to stability bounds for the step-size matrix and\ndesign guidelines are derived from the analytical models. Monte Carlo\nsimulations illustrate the accuracy of the theoretical models and the\napplicability of the proposed design guidelines. Examples include operation\nunder mild degrees of nonstationarity. Finally, we show how a high convergence\nrate can be achieved using a quasi-Newton adaptation scheme in which the\nstep-size matrix is designed to whiten the combined input vector.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 18:27:17 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Maruo", "Marcos H.", ""], ["Bermudez", "Jos\u00e9 C. M.", ""], ["Resende", "Leonardo S.", ""]]}, {"id": "1503.02106", "submitter": "David Donoho", "authors": "David L. Donoho and Andrea Montanari", "title": "Variance Breakdown of Huber (M)-estimators: $n/p \\rightarrow m \\in\n  (1,\\infty)$", "comments": "Based on a lecture delivered at a special colloquium honoring the\n  50th anniversary of the Seminar f\\\"ur Statistik (SfS) at ETH Z\\\"urich,\n  November 25, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A half century ago, Huber evaluated the minimax asymptotic variance in scalar\nlocation estimation, $ \\min_\\psi \\max_{F \\in {\\cal F}_\\epsilon} V(\\psi, F) =\n\\frac{1}{I(F_\\epsilon^*)} $, where $V(\\psi,F)$ denotes the asymptotic variance\nof the $(M)$-estimator for location with score function $\\psi$, and\n$I(F_\\epsilon^*)$ is the minimal Fisher information $ \\min_{{\\cal F}_\\epsilon}\nI(F)$ over the class of $\\epsilon$-Contaminated Normal distributions.\n  We consider the linear regression model $Y = X\\theta_0 + W$,\n$W_i\\sim_{\\text{i.i.d.}}F$, and iid Normal predictors $X_{i,j}$, working in the\nhigh-dimensional-limit asymptotic where the number $n$ of observations and $p$\nof variables both grow large, while $n/p \\rightarrow m \\in (1,\\infty)$; hence\n$m$ plays the role of `asymptotic number of observations per parameter\nestimated'. Let $V_m(\\psi,F)$ denote the per-coordinate asymptotic variance of\nthe $(M)$-estimator of regression in the $n/p \\rightarrow m$ regime. Then $V_m\n\\neq V$; however $V_m \\rightarrow V$ as $m \\rightarrow \\infty$.\n  In this paper we evaluate the minimax asymptotic variance of the Huber\n$(M)$-estimate. The statistician minimizes over the family\n$(\\psi_\\lambda)_{\\lambda > 0}$ of all tunings of Huber $(M)$-estimates of\nregression, and Nature maximizes over gross-error contaminations $F \\in {\\cal\nF}_\\epsilon$. Suppose that $I(F_\\epsilon^*) \\cdot m > 1$. Then $ \\min_\\lambda\n\\max_{F \\in {\\cal F}_\\epsilon} V_m(\\psi_\\lambda, F) = \\frac{1}{I(F_\\epsilon^*)\n- 1/m} $. Strikingly, if $I(F_\\epsilon^*) \\cdot m \\leq 1$, then the minimax\nasymptotic variance is $+\\infty$. The breakdown point is where the Fisher\ninformation per parameter equals unity.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 22:34:28 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Donoho", "David L.", ""], ["Montanari", "Andrea", ""]]}, {"id": "1503.02214", "submitter": "Vladimir Panov", "authors": "Vladimir Panov and Igor Sirotkin", "title": "Series representations for bivariate time-changed L{\\'e}vy models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a L{\\'e}vy model based on two popular concepts -\nsubordination and L{\\'e}vy copulas. More precisely, we consider a\ntwo-dimensional L{\\'e}vy process such that each component is a time-changed\n(subordinated) Brownian motion and the dependence between subordinators is\ndescribed via some L{\\'e}vy copula. We prove a series representation for our\nmodel, which can be efficiently used for simulation purposes, and provide some\npractical examples based on real data\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 21:26:05 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Panov", "Vladimir", ""], ["Sirotkin", "Igor", ""]]}, {"id": "1503.02339", "submitter": "Peter Gerstoft", "authors": "Peter Gerstoft, Angeliki Xenaki, and Christoph F. Mecklenbr\\\"auker", "title": "Multiple and single snapshot compressive beamforming", "comments": "In press Journal of Acoustical Society of America", "journal-ref": null, "doi": "10.1121/1.4929941", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a sound field observed on a sensor array, compressive sensing (CS)\nreconstructs the direction-of-arrival (DOA) of multiple sources using a\nsparsity constraint. The DOA estimation is posed as an underdetermined problem\nby expressing the acoustic pressure at each sensor as a phase-lagged\nsuperposition of source amplitudes at all hypothetical DOAs. Regularizing with\nan $\\ell_1$-norm constraint renders the problem solvable with convex\noptimization, and promoting sparsity gives high-resolution DOA maps. Here, the\nsparse source distribution is derived using maximum a posteriori (MAP)\nestimates for both single and multiple snapshots. CS does not require inversion\nof the data covariance matrix and thus works well even for a single snapshot\nwhere it gives higher resolution than conventional beamforming. For multiple\nsnapshots, CS outperforms conventional high-resolution methods, even with\ncoherent arrivals and at low signal-to-noise ratio. The superior resolution of\nCS is demonstrated with vertical array data from the SWellEx96 experiment for\ncoherent multi-paths.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 22:57:16 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 21:47:09 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gerstoft", "Peter", ""], ["Xenaki", "Angeliki", ""], ["Mecklenbr\u00e4uker", "Christoph F.", ""]]}, {"id": "1503.02390", "submitter": "Keisuke Yano", "authors": "Keisuke Yano, Fumiyasu Komaki", "title": "Information criteria for multistep ahead predictions", "comments": null, "journal-ref": "Statistica Sinica 27 (2017), 1205-1223", "doi": "10.5705/ss.202015.0380", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an information criterion for multistep ahead predictions. It is\nalso used for extrapolations. For the derivation, we consider multistep ahead\npredictions under local misspecification. In the prediction, we show that\nBayesian predictive distributions asymptotically have smaller Kullback--Leibler\nrisks than plug-in predictive distributions. From the results, we construct an\ninformation criterion for multistep ahead predictions by using an\nasymptotically unbiased estimator of the Kullback--Leibler risk of Bayesian\npredictive distributions. We show the effectiveness of the proposed information\ncriterion throughout the numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 08:13:24 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Yano", "Keisuke", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1503.02817", "submitter": "Ming Yuan", "authors": "Ming Yuan and Ding-Xuan Zhou", "title": "Minimax Optimal Rates of Estimation in High Dimensional Additive Models:\n  Universal Phase Transition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish minimax optimal rates of convergence for estimation in a high\ndimensional additive model assuming that it is approximately sparse. Our\nresults reveal an interesting phase transition behavior universal to this class\nof high dimensional problems. In the {\\it sparse regime} when the components\nare sufficiently smooth or the dimensionality is sufficiently large, the\noptimal rates are identical to those for high dimensional linear regression,\nand therefore there is no additional cost to entertain a nonparametric model.\nOtherwise, in the so-called {\\it smooth regime}, the rates coincide with the\noptimal rates for estimating a univariate function, and therefore they are\nimmune to the \"curse of dimensionality\".\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 08:56:34 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Yuan", "Ming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1503.02878", "submitter": "Alessio De Angelis", "authors": "Alessio De Angelis, Carlo Fischione", "title": "Mobile Node Localization via Pareto Optimization: Algorithm and\n  Fundamental Performance Limitations", "comments": "IEEE Journal on Selected Areas in Communications (To Appear), 2015", "journal-ref": null, "doi": "10.1109/JSAC.2015.2430151", "report-no": null, "categories": "cs.IT cs.RO math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the position of network nodes is essential, e.g., in\nlocalization, geographic routing, and vehicular networks. Unfortunately,\ntypical positioning techniques based on ranging or on velocity and angular\nmeasurements are inherently limited. To overcome the limitations of specific\npositioning techniques, the fusion of multiple and heterogeneous sensor\ninformation is an appealing strategy. In this paper, we investigate the\nfundamental performance of linear fusion of multiple measurements of the\nposition of mobile nodes, and propose a new distributed recursive position\nestimator. The Cram\\'er-Rao lower bounds for the parametric and a-posteriori\ncases are investigated. The proposed estimator combines information coming from\nranging, speed, and angular measurements, which is jointly fused by a Pareto\noptimization problem where the mean and the variance of the localization error\nare simultaneously minimized. A distinguished feature of the method is that it\nassumes a very simple dynamical model of the mobility and therefore it is\napplicable to a large number of scenarios providing good performance. The main\nchallenge is the characterization of the statistical information needed to\nmodel the Fisher information matrix and the Pareto optimization problem. The\nproposed analysis is validated by Monte Carlo simulations, and the performance\nis compared to several Kalman-based filters, commonly employed for localization\nand sensor fusion. Simulation results show that the proposed estimator\noutperforms the traditional approaches that are based on the extended Kalman\nfilter when no assumption on the model of motion is used. In such a scenario,\nbetter performance is achieved by the proposed method, but at the price of an\nincreased computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 12:12:57 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["De Angelis", "Alessio", ""], ["Fischione", "Carlo", ""]]}, {"id": "1503.02978", "submitter": "Junwei Lu", "authors": "Junwei Lu, Mladen Kolar and Han Liu", "title": "Kernel Meets Sieve: Post-Regularization Confidence Bands for Sparse\n  Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel procedure for constructing confidence bands for components\nof a sparse additive model. Our procedure is based on a new kernel-sieve hybrid\nestimator that combines two most popular nonparametric estimation methods in\nthe literature, the kernel regression and the spline method, and is of interest\nin its own right. Existing methods for fitting sparse additive model are\nprimarily based on sieve estimators, while the literature on confidence bands\nfor nonparametric models are primarily based upon kernel or local polynomial\nestimators. Our kernel-sieve hybrid estimator combines the best of both worlds\nand allows us to provide a simple procedure for constructing confidence bands\nin high-dimensional sparse additive models. We prove that the confidence bands\nare asymptotically honest by studying approximation with a Gaussian process.\nThorough numerical results on both synthetic data and real-world neuroscience\ndata are provided to demonstrate the efficacy of the theory.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 16:26:31 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 20:44:44 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Lu", "Junwei", ""], ["Kolar", "Mladen", ""], ["Liu", "Han", ""]]}, {"id": "1503.02991", "submitter": "Jos\\'e Luis Romero", "authors": "Lu\\'is Daniel Abreu and Jos\\'e Luis Romero", "title": "The bias-variance trade-off in Thomson's multitaper estimator", "comments": "Minor corrections. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of non-parametric spectral estimation, lies the dilemma known as\nthe bias-variance trade-off: low biased estimators tend to have high variance\nand low variance estimators tend to have high bias. In 1982, Thomson introduced\na multitaper method where this trade-off is made explicit by choosing a target\nbias resolution and obtaining a corresponding variance reduction. The method\nbecame the standard in many applications. Its favorable bias-variance trade-off\nis due to an empirical fact, conjectured by Thomson based on numerical\nevidence: assuming bandwidth W and N time domain observations, the average of\nthe square of the first $K=\\left\\lfloor 2NW\\right\\rfloor$ Slepian functions\napproaches, as K grows, an ideal band-pass kernel for the interval [-W,W]. We\nprovide an analytic proof of this fact and quantify the approximation error in\nthe L1 norm; the approximation error is then used to control the bias of the\nmultitaper estimator resulting from spectral leakage. This leads to new\nperformance bounds for the method, explicit in terms of the bandwidth W and the\nnumber N of time domain observations. Our method is flexible and can be\nextended to higher dimensions and different geometries.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:09:58 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2016 15:55:45 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 08:08:26 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Abreu", "Lu\u00eds Daniel", ""], ["Romero", "Jos\u00e9 Luis", ""]]}, {"id": "1503.03100", "submitter": "Christopher Ferrie", "authors": "Christopher Ferrie and Robin Blume-Kohout", "title": "Minimax quantum tomography: the ultimate bounds on accuracy", "comments": null, "journal-ref": "Phys. Rev. Lett. 116, 090407 (2016)", "doi": "10.1103/PhysRevLett.116.090407", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimax estimator has the minimum possible error (\"risk\") in the worst\ncase. We construct the first minimax estimators for quantum state tomography\nwith relative entropy risk. The minimax risk of non-adaptive tomography scales\nas $O(1/\\sqrt{N})$, in contrast to that of classical probability estimation\nwhich is $O(1/N)$. We trace this deficiency to sampling mismatch: future\nobservations that determine risk may come from a different sample space than\nthe past data that determine the estimate. This makes minimax estimators very\nbiased, and we propose a computationally tractable alternative with similar\nbehavior in the worst case, but superior accuracy on most states.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 21:20:38 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Ferrie", "Christopher", ""], ["Blume-Kohout", "Robin", ""]]}, {"id": "1503.03188", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan", "title": "Optimal prediction for sparse linear models? Lower bounds for\n  coordinate-separable M-estimators", "comments": "Add more coverage on related work; add a new lower bound for design\n  matrices satisfying the restricted eigenvalue condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the problem of high-dimensional sparse linear regression, it is known\nthat an $\\ell_0$-based estimator can achieve a $1/n$ \"fast\" rate on the\nprediction error without any conditions on the design matrix, whereas in\nabsence of restrictive conditions on the design matrix, popular polynomial-time\nmethods only guarantee the $1/\\sqrt{n}$ \"slow\" rate. In this paper, we show\nthat the slow rate is intrinsic to a broad class of M-estimators. In\nparticular, for estimators based on minimizing a least-squares cost function\ntogether with a (possibly non-convex) coordinate-wise separable regularizer,\nthere is always a \"bad\" local optimum such that the associated prediction error\nis lower bounded by a constant multiple of $1/\\sqrt{n}$. For convex\nregularizers, this lower bound applies to all global optima. The theory is\napplicable to many popular estimators, including convex $\\ell_1$-based methods\nas well as M-estimators based on nonconvex regularizers, including the SCAD\npenalty or the MCP regularizer. In addition, for a broad class of nonconvex\nregularizers, we show that the bad local optima are very common, in that a\nbroad class of local minimization algorithms with random initialization will\ntypically converge to a bad solution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 06:22:44 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 01:28:33 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhang", "Yuchen", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1503.03212", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "Multivariate Generalized Gram-Charlier Series in Vector Notations", "comments": "27 pages; reasons for arXiv update: corrected typos, modified\n  literature survey", "journal-ref": "JOURNAL OF MATHEMATICAL CHEMISTRY (2018) 56(6), 1631-1655", "doi": "10.1007/s10910-018-0878-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives multivariate Generalized Gram-Charlier (GGC) series that\nexpands an unknown joint probability density function (\\textit{pdf}) of a\nrandom vector in terms of the differentiations of the joint \\textit{pdf} of a\nreference random vector. Conventionally, the higher order differentiations of a\nmultivariate \\textit{pdf} in GGC series will require multi-element array or\ntensor representations. But, the current article derives the GGC series in\nvector notations. The required higher order differentiations of a multivariate\n\\textit{pdf} in vector notations are achieved through application of a specific\nKronecker product based differentiation operator. Overall, the article uses\nonly elementary calculus of several variables; instead Tensor calculus; to\nachieve the extension of an existing specific derivation for GGC series in\nunivariate to multivariate. The derived multivariate GGC expression is more\nelementary as using vector notations compare to the coordinatewise tensor\nnotations and more comprehensive as apparently more nearer to its counterpart\nfor univariate. The same advantages are shared by the other expressions\nobtained in the article; such as the mutual relations between cumulants and\nmoments of a random vector, integral form of a multivariate \\textit{pdf},\nintegral form of the multivariate Hermite polynomials, the multivariate\nGram-Charlier A (GCA) series and others.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 08:22:00 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 07:52:51 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 10:03:16 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1503.03393", "submitter": "Yuliana Linke Yu", "authors": "Yu.Yu. Linke", "title": "Asymptotic properties of one-step $M$-estimators based on nonidentically\n  distributed observations with applications to nonlinear regression problems", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic behavior of one-step $M$-estimators based on samples from\narrays of not necessarily identically distributed random variables and\nrepresenting explicit approximations to the corresponding consistent\n$M$-estimators. These estimators generalize Fisher's one-step approximations to\nconsistent maximum likelihood estimators. As a consequence, we consider some\nnonlinear regression problems where the procedure mentioned allow us to\nconstruct explicit asymptotically optimal estimators. We also consider the\nproblem of constructing initial estimators which are needed for one-step\nestimation procedures.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 15:57:25 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2015 16:05:22 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2015 19:00:14 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 16:17:52 GMT"}, {"version": "v5", "created": "Thu, 30 Apr 2015 18:06:11 GMT"}, {"version": "v6", "created": "Tue, 12 May 2015 08:31:49 GMT"}, {"version": "v7", "created": "Thu, 25 Jun 2015 16:39:05 GMT"}, {"version": "v8", "created": "Mon, 11 Apr 2016 18:12:08 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Linke", "Yu. Yu.", ""]]}, {"id": "1503.03414", "submitter": "Daniel Mortlock", "authors": "Daniel J. Mortlock (Imperial College London)", "title": "Bayesian model comparison in cosmology", "comments": "4 pages, 2 figures; to appear in Statistical Challenges in 21st\n  Century Cosmology, Proceedings IAU Symposium No. 306, A. H. Heavens & J.-L.\n  Starck, eds", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Bayesian model formalism comparison cannot be applied to most\ncosmological models as they lack well-motivated parameter priors. However, if\nthe data-set being used is separable then it is possible to use some of the\ndata to obtain the necessary parameter distributions, the rest of the data\nbeing retained for model comparison. While such methods are not fully\nprescriptive, they provide a route to applying Bayesian model comparison in\ncosmological situations where it could not otherwise be used.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 16:52:24 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Mortlock", "Daniel J.", "", "Imperial College London"]]}, {"id": "1503.03453", "submitter": "Brian Ji Dr.", "authors": "Edward Y. Ji and Brian L. Ji", "title": "On the Variability Estimation of Lognormal Distribution Based on Sample\n  Harmonic and Arithmetic Means", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the lognormal distribution, an unbiased estimator of the squared\ncoefficient of variation is derived from the relative ratio of sample\narithmetic to harmonic means. Analytical proofs and simulation results are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:15:21 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Ji", "Edward Y.", ""], ["Ji", "Brian L.", ""]]}, {"id": "1503.03467", "submitter": "Houman Owhadi", "authors": "Houman Owhadi", "title": "Multigrid with rough coefficients and Multiresolution operator\n  decomposition from Hierarchical Information Games", "comments": "Presented at SIAM CSE 15. Final (published) version.\n  http://epubs.siam.org/doi/abs/10.1137/15M1013894", "journal-ref": "SIAM Rev. 59-1, pp. 99-149 (2017)", "doi": null, "report-no": null, "categories": "math.NA cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a near-linear complexity (geometric and meshless/algebraic)\nmultigrid/multiresolution method for PDEs with rough ($L^\\infty$) coefficients\nwith rigorous a-priori accuracy and performance estimates. The method is\ndiscovered through a decision/game theory formulation of the problems of (1)\nidentifying restriction and interpolation operators (2) recovering a signal\nfrom incomplete measurements based on norm constraints on its image under a\nlinear operator (3) gambling on the value of the solution of the PDE based on a\nhierarchy of nested measurements of its solution or source term. The resulting\nelementary gambles form a hierarchy of (deterministic) basis functions of\n$H^1_0(\\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands\nwith respect to the scalar product induced by the energy norm of the PDE (2)\nenable sparse compression of the solution space in $H^1_0(\\Omega)$ (3) induce\nan orthogonal multiresolution operator decomposition. The operating diagram of\nthe multigrid method is that of an inverted pyramid in which gamblets are\ncomputed locally (by virtue of their exponential decay), hierarchically (from\nfine to coarse scales) and the PDE is decomposed into a hierarchy of\nindependent linear systems with uniformly bounded condition numbers. The\nresulting algorithm is parallelizable both in space (via localization) and in\nbandwith/subscale (subscales can be computed independently from each other).\nAlthough the method is deterministic it has a natural Bayesian interpretation\nunder the measure of probability emerging (as a mixed strategy) from the\ninformation game formulation and multiresolution approximations form a\nmartingale with respect to the filtration induced by the hierarchy of nested\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:52:13 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 19:27:55 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 01:09:11 GMT"}, {"version": "v4", "created": "Tue, 8 Mar 2016 05:57:10 GMT"}, {"version": "v5", "created": "Fri, 10 Feb 2017 07:02:43 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Owhadi", "Houman", ""]]}, {"id": "1503.03613", "submitter": "Mesrob Ohannessian", "authors": "Elchanan Mossel and Mesrob I. Ohannessian", "title": "On the Impossibility of Learning the Missing Mass", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that one cannot learn the probability of rare events without\nimposing further structural assumptions. The event of interest is that of\nobtaining an outcome outside the coverage of an i.i.d. sample from a discrete\ndistribution. The probability of this event is referred to as the \"missing\nmass\". The impossibility result can then be stated as: the missing mass is not\ndistribution-free PAC-learnable in relative error. The proof is\nsemi-constructive and relies on a coupling argument using a dithered geometric\ndistribution. This result formalizes the folklore that in order to predict rare\nevents, one necessarily needs distributions with \"heavy tails\".\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 07:27:24 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mossel", "Elchanan", ""], ["Ohannessian", "Mesrob I.", ""]]}, {"id": "1503.03626", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Alan Edelman", "title": "Integral geometry for Markov chain Monte Carlo: overcoming the curse of\n  search-subspace dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.DG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that uses the Cauchy-Crofton formula and a new\ncurvature formula from integral geometry to reweight the sampling probabilities\nof Metropolis-within-Gibbs algorithms in order to increase their convergence\nspeed. We consider algorithms that sample from a probability density\nconditioned on a manifold $\\mathcal{M}$. Our method exploits the symmetries of\nthe algorithms' isotropic random search-direction subspaces to analytically\naverage out the variance in the intersection volume caused by the orientation\nof the search-subspace with respect to the manifold $\\mathcal{M}$ it\nintersects. This variance can grow exponentially with the dimension of the\nsearch-subspace, greatly slowing down the algorithm. Eliminating this variance\nallows us to use search-subspaces of dimensions many times greater than would\notherwise be possible, allowing us to sample very rare events that a\nlower-dimensional search-subspace would be unlikely to intersect.\n  To extend this method to events that are rare for reasons other than their\nsupport $\\mathcal{M}$ having a lower dimension, we formulate and prove a new\ntheorem in integral geometry that makes use of the curvature form of the\nChern-Gauss-Bonnet theorem to reweight sampling probabilities. On the side, we\nalso apply our theorem to obtain new theoretical bounds for the volumes of real\nalgebraic manifolds.\n  Finally, we demonstrate the computational effectiveness and speedup of our\nmethod by numerically applying it to the conditional stochastic Airy operator\nsampling problem in random matrix theory.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 08:43:53 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mangoubi", "Oren", ""], ["Edelman", "Alan", ""]]}, {"id": "1503.03673", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen, Su-Yun Huang, Yanyuan Ma and I-Ping Tu", "title": "Functional Inverse Regression in an Enlarged Dimension Reduction Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider an enlarged dimension reduction space in functional inverse\nregression. Our operator and functional analysis based approach facilitates a\ncompact and rigorous formulation of the functional inverse regression problem.\nIt also enables us to expand the possible space where the dimension reduction\nfunctions belong. Our formulation provides a unified framework so that the\nclassical notions, such as covariance standardization, Mahalanobis distance,\nSIR and linear discriminant analysis, can be naturally and smoothly carried out\nin our enlarged space. This enlarged dimension reduction space also links to\nthe linear discriminant space of Gaussian measures on a separable Hilbert\nspace.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 11:14:38 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Chen", "Ting-Li", ""], ["Huang", "Su-Yun", ""], ["Ma", "Yanyuan", ""], ["Tu", "I-Ping", ""]]}, {"id": "1503.03786", "submitter": "Rajesh Sharma", "authors": "R. Sharma, R. Kumar, R. Saini and G. Kapoor", "title": "Complementary upper bounds for fourth central moment with extensions and\n  applications", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove some inequalities involving fourth central moment of a random\nvariable that takes values in a given finite interval. Both discrete and\ncontinuous cases are considered. Bounds for the spread are obtained when a\ngiven nxn complex matrix has real eigenvalues. Likewise, we discuss bounds for\nthe spans of polynomial equations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 15:55:41 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Sharma", "R.", ""], ["Kumar", "R.", ""], ["Saini", "R.", ""], ["Kapoor", "G.", ""]]}, {"id": "1503.03879", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri", "title": "Qualitative inequalities for squared partial correlations of a Gaussian\n  random vector", "comments": "21 pages, 13 figures", "journal-ref": "Annals of the Institute of Statistical Mathematics, 66(2),\n  345-367, 2014", "doi": "10.1007/s10463-013-0417-x", "report-no": "Department of Statistics and Applied Probability, National\n  University of Singapore technical report 201301", "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe various sets of conditional independence relationships,\nsufficient for qualitatively comparing non-vanishing squared partial\ncorrelations of a Gaussian random vector. These sufficient conditions are\nsatisfied by several graphical Markov models. Rules for comparing degree of\nassociation among the vertices of such Gaussian graphical models are also\ndeveloped. We apply these rules to compare conditional dependencies on Gaussian\ntrees. In particular for trees, we show that such dependence can be completely\ncharacterized by the length of the paths joining the dependent vertices to each\nother and to the vertices conditioned on. We also apply our results to\npostulate rules for model selection for polytree models. Our rules apply to\nmutual information of Gaussian random vectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:15:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhuri", "Sanjay", ""]]}, {"id": "1503.04022", "submitter": "Yuwei Zhao", "authors": "Thomas Mikosch, Yuwei Zhao", "title": "The integrated periodogram of a dependent extremal event sequence", "comments": null, "journal-ref": null, "doi": "10.1016/j.spa.2015.02.017", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic properties of the integrated periodogram\ncalculated from a sequence of indicator functions of dependent extremal events.\nAn event in Euclidean space is extreme if it occurs far away from the origin.\nWe use a regular variation condition on the underlying stationary sequence to\nmake these notions precise. Our main result is a functional central limit\ntheorem for the integrated periodogram of the indicator functions of dependent\nextremal events. The limiting process is a continuous Gaussian process whose\ncovari- ance structure is in general unfamiliar, but in the iid case a Brownian\nbridge appears. In the general case, we propose a stationary bootstrap\nprocedure for approximating the distribution of the limiting process. The\ndeveloped theory can be used to construct classical goodness-of-fit tests such\nas the Grenander- Rosenblatt and Cram\\'{e}r-von Mises tests which are based\nonly on the extremes in the sample. We apply the test statistics to simulated\nand real-life data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 11:02:08 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Mikosch", "Thomas", ""], ["Zhao", "Yuwei", ""]]}, {"id": "1503.04098", "submitter": "Cristiano Villa", "authors": "Cristiano Villa and Stephen Walker", "title": "On the Mathematics of the Jeffreys-Lindley Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the well known Jeffreys-Lindley paradox. In a\nBayesian set up, the so-called paradox arises when a point null hypothesis is\ntested and an objective prior is sought for the alternative hypothesis. In\nparticular, the posterior for the null hypothesis tends to one when the\nuncertainty, i.e. the variance, for the parameter value goes to infinity. We\nargue that the appropriate way to deal with the paradox is to use simple\nmathematics, and that any philosophical argument is to be regarded as\nirrelevant.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 15:17:25 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Villa", "Cristiano", ""], ["Walker", "Stephen", ""]]}, {"id": "1503.04161", "submitter": "Daniel Vogel", "authors": "Daniel Vogel, Martin Wendler", "title": "Studentized U-quantile processes under dependence with applications to\n  change-point analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular robust estimators are $U$-quantiles, most notably the\nHodges-Lehmann location estimator and the $Q_n$ scale estimator. We prove a\nfunctional central limit theorem for the sequential $U$-quantile process\nwithout any moment assumptions and under weak short-range dependence\nconditions. We further devise an estimator for the long-run variance and show\nits consistency, from which the convergence of the studentized version of the\nsequential $U$-quantile process to a standard Brownian motion follows. This\nresult can be used to construct CUSUM-type change-point tests based on\n$U$-quantiles, which do not rely on bootstrapping procedures. We demonstrate\nthis approach in detail at the example of the Hodges-Lehmann estimator for\nrobustly detecting changes in the central location. A simulation study confirms\nthe very good robustness and efficiency properties of the test. Two real-life\ndata sets are analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 17:38:49 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 14:21:25 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Vogel", "Daniel", ""], ["Wendler", "Martin", ""]]}, {"id": "1503.04304", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "Laplace's rule of succession in information geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laplace's \"add-one\" rule of succession modifies the observed frequencies in a\nsequence of heads and tails by adding one to the observed counts. This improves\nprediction by avoiding zero probabilities and corresponds to a uniform Bayesian\nprior on the parameter. The canonical Jeffreys prior corresponds to the\n\"add-one-half\" rule. We prove that, for exponential families of distributions,\nsuch Bayesian predictors can be approximated by taking the average of the\nmaximum likelihood predictor and the \\emph{sequential normalized maximum\nlikelihood} predictor from information theory. Thus in this case it is possible\nto approximate Bayesian predictors without the cost of integrating or sampling\nin parameter space.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 13:26:29 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1503.04427", "submitter": "Lucien Birg\\'e", "authors": "Yannick Baraud and Lucien Birg\\'e", "title": "Rates of convergence of rho-estimators for sets of densities satisfying\n  shape constraints", "comments": "24 pages", "journal-ref": "Stochastic Process. Appl., 126 (12):3888--3912 (2016)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to pursue our study of rho-estimators built from\ni.i.d. observations that we defined in Baraud et al. (2014). For a\n\\rho-estimator based on some model S (which means that the estimator belongs to\nS) and a true distribution of the observations that also belongs to S, the risk\n(with squared Hellinger loss) is bounded by a quantity which can be viewed as a\ndimension function of the model and is often related to the \"metric dimension\"\nof this model, as defined in Birg\\'e (2006). This is a minimax point of view\nand it is well-known that it is pessimistic. Typically, the bound is accurate\nfor most points in the model but may be very pessimistic when the true\ndistribution belongs to some specific part of it. This is the situation that we\nwant to investigate here. For some models, like the set of decreasing densities\non [0,1], there exist specific points in the model that we shall call\n\"extremal\" and for which the risk is substantially smaller than the typical\nrisk. Moreover, the risk at a non-extremal point of the model can be bounded by\nthe sum of the risk bound at a well-chosen extremal point plus the square of\nits distance to this point. This implies that if the true density is close\nenough to an extremal point, the risk at this point may be smaller than the\nminimax risk on the model and this actually remains true even if the true\ndensity does not belong to the model. The result is based on some refined\nbounds on the suprema of empirical processes that are established in Baraud\n(2016).\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 13:56:39 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 12:36:53 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2016 18:35:22 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2016 18:42:25 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Baraud", "Yannick", ""], ["Birg\u00e9", "Lucien", ""]]}, {"id": "1503.04443", "submitter": "Subrata Chakraborty", "authors": "Subrata Chakraborty, Tomoaki Imoto", "title": "Extended Conway-Maxwell-Poisson distribution and its properties and\n  applications", "comments": "14 pages, 2 figures, 3 tables, prepreprint, new sections added, new\n  references added,new coauthor added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new three parameter natural extension of the Conway-Maxwell-Poisson\n(COM-Poisson) distribution is proposed. This distribution includes the recently\nproposed COM-Poisson type negative binomial (COM-NB) distribution [Chakraborty,\nS. and Ong, S. H. (2014): A COM-type Generalization of the Negative Binomial\nDistribution, Accepted in Communications in Statistics-Theory and Methods] and\nthe generalized COM-Poisson (GCOMP) distribution [Imoto, T. :(2014) A\ngeneralized Conway-Maxwell-Poisson distribution which includes the negative\nbinomial distribution, Applied Mathematics and Computation, 247, 824-834]. The\nproposed distribution is derived from a queuing system with state dependent\narrival and service rates and also from an exponential combination of negative\nbinomial and COM-Poisson distribution. Some distributional, reliability and\nstochastic ordering properties are investigated. Computational asymptotic\napproximations, different characterizations, parameter estimation and data\nfitting example also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 16:04:04 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 17:54:53 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Chakraborty", "Subrata", ""], ["Imoto", "Tomoaki", ""]]}, {"id": "1503.04455", "submitter": "Gregory Rice", "authors": "Lajos Horv\\'ath, Marie Hu\\v{s}kov\\'a, Gregory Rice, and Jia Wang", "title": "Estimation of the time of change in panel data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the common time of a change in the mean\nparameters of panel data when dependence is allowed between the panels in the\nform of a common factor. A CUSUM type estimator is proposed, and we establish\nfirst and second order asymptotics that can be used to derive consistent\nconfidence intervals for the time of change. Our results improve upon existing\ntheory in two primary directions. Firstly, the conditions we impose on the\nmodel errors only pertain to the order of their long run moments, and hence our\nresults hold for nearly all stationary time series models of interest,\nincluding nonlinear time series like the ARCH and GARCH processes. Secondly, we\nstudy how the asymptotic distribution and norming sequences of the estimator\ndepend on the magnitude of the changes in each panel and the common factor\nloadings. The performance of our results in finite samples is demonstrated with\na Monte Carlo simulation study, and we consider applications to two real data\nsets: the exchange rates of 23 currencies with respect to the US dollar, and\nthe GDP per capita in 113 countries.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 17:32:44 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Horv\u00e1th", "Lajos", ""], ["Hu\u0161kov\u00e1", "Marie", ""], ["Rice", "Gregory", ""], ["Wang", "Jia", ""]]}, {"id": "1503.04493", "submitter": "Guang Cheng", "authors": "Yun Yang, Guang Cheng and David B. Dunson", "title": "Semiparametric Bernstein-von Mises Theorem: Second Order Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major goal of this paper is to study the second order frequentist\nproperties of the marginal posterior distribution of the parametric component\nin semiparametric Bayesian models, in particular, a second order semiparametric\nBernstein-von Mises (BvM) Theorem. Our first contribution is to discover an\ninteresting interference phenomenon between Bayesian estimation and frequentist\ninferential accuracy: more accurate Bayesian estimation on the nuisance\nfunction leads to higher frequentist inferential accuracy on the parametric\ncomponent. As the second contribution, we propose a new class of dependent\npriors under which Bayesian inference procedures for the parametric component\nare not only efficient but also adaptive (w.r.t. the smoothness of\nnonparametric component) up to the second order frequentist validity. However,\ncommonly used independent priors may even fail to produce a desirable root-n\ncontraction rate for the parametric component in this adaptive case unless some\nstringent assumption is imposed. Three important classes of semiparametric\nmodels are examined, and extensive simulations are also provided.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 00:28:25 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Yang", "Yun", ""], ["Cheng", "Guang", ""], ["Dunson", "David B.", ""]]}, {"id": "1503.04525", "submitter": "Kazuyoshi Yata", "authors": "Kazuyoshi Yata and Makoto Aoshima", "title": "Principal component analysis based clustering for high-dimension,\n  low-sample-size data", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider clustering based on principal component analysis\n(PCA) for high-dimension, low-sample-size (HDLSS) data. We give theoretical\nreasons why PCA is effective for clustering HDLSS data. First, we derive a\ngeometric representation of HDLSS data taken from a two-class mixture model.\nWith the help of the geometric representation, we give geometric consistency\nproperties of sample principal component scores in the HDLSS context. We\ndevelop ideas of the geometric representation and geometric consistency\nproperties to multiclass mixture models. We show that PCA can classify HDLSS\ndata under certain conditions in a surprisingly explicit way. Finally, we\ndemonstrate the performance of the clustering by using microarray data sets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 05:25:49 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Yata", "Kazuyoshi", ""], ["Aoshima", "Makoto", ""]]}, {"id": "1503.04530", "submitter": "Ester Mariucci", "authors": "Ester Mariucci", "title": "Asymptotic equivalence for pure jump L\\'evy processes with unknown\n  L\\'evy density and Gaussian white noise", "comments": "50 pages. The definition of the parameter space has changed and some\n  proofs have been expanded and corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to establish a global asymptotic equivalence between\nthe experiments generated by the discrete (high frequency) or continuous\nobservation of a path of a L{\\'e}vy process and a Gaussian white noise\nexperiment observed up to a time T, with T tending to $\\infty$. These\napproximations are given in the sense of the Le Cam distance, under some\nsmoothness conditions on the unknown L{\\'e}vy density. All the asymptotic\nequivalences are established by constructing explicit Markov kernels that can\nbe used to reproduce one experiment from the other.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 05:52:54 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 16:53:30 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Mariucci", "Ester", ""]]}, {"id": "1503.04549", "submitter": "Makoto Aoshima", "authors": "Makoto Aoshima, Kazuyoshi Yata", "title": "High-dimensional quadratic classifiers in non-sparse settings", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional quadratic classifiers in non-sparse settings.\nThe target of classification rules is not Bayes error rates in the context. The\nclassifier based on the Mahalanobis distance does not always give a preferable\nperformance even if the populations are normal distributions having known\ncovariance matrices. The quadratic classifiers proposed in this paper draw\ninformation about heterogeneity effectively through both the differences of\nexpanding mean vectors and covariance matrices. We show that they hold a\nconsistency property in which misclassification rates tend to zero as the\ndimension goes to infinity under non-sparse settings. We verify that they are\nasymptotically distributed as a normal distribution under certain conditions.\nWe also propose a quadratic classifier after feature selection by using both\nthe differences of mean vectors and covariance matrices. Finally, we discuss\nperformances of the classifiers in actual data analyses. The proposed\nclassifiers achieve highly accurate classification with very low computational\ncosts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 07:35:22 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 08:12:49 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Aoshima", "Makoto", ""], ["Yata", "Kazuyoshi", ""]]}, {"id": "1503.04985", "submitter": "Soutir Bandyopadhyay", "authors": "Soutir Bandyopadhyay, Soumendra N. Lahiri, Daniel J. Nordman", "title": "A frequency domain empirical likelihood method for irregularly spaced\n  spatial data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1291 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, 519-545", "doi": "10.1214/14-AOS1291", "report-no": "IMS-AOS-AOS1291", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops empirical likelihood methodology for irregularly spaced\nspatial data in the frequency domain. Unlike the frequency domain empirical\nlikelihood (FDEL) methodology for time series (on a regular grid), the\nformulation of the spatial FDEL needs special care due to lack of the usual\northogonality properties of the discrete Fourier transform for irregularly\nspaced data and due to presence of nontrivial bias in the periodogram under\ndifferent spatial asymptotic structures. A spatial FDEL is formulated in the\npaper taking into account the effects of these factors. The main results of the\npaper show that Wilks' phenomenon holds for a scaled version of the logarithm\nof the proposed empirical likelihood ratio statistic in the sense that it is\nasymptotically distribution-free and has a chi-squared limit. As a result, the\nproposed spatial FDEL method can be used to build nonparametric, asymptotically\ncorrect confidence regions and tests for covariance parameters that are defined\nthrough spectral estimating equations, for irregularly spaced spatial data. In\ncomparison to the more common studentization approach, a major advantage of our\nmethod is that it does not require explicit estimation of the standard error of\nan estimator, which is itself a very difficult problem as the asymptotic\nvariances of many common estimators depend on intricate interactions among\nseveral population quantities, including the spectral density of the spatial\nprocess, the spatial sampling density and the spatial asymptotic structure.\nResults from a numerical study are also reported to illustrate the methodology\nand its finite sample properties.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 10:44:42 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Bandyopadhyay", "Soutir", ""], ["Lahiri", "Soumendra N.", ""], ["Nordman", "Daniel J.", ""]]}, {"id": "1503.05033", "submitter": "Young K. Lee", "authors": "Young K. Lee, Enno Mammen, Jens P. Nielsen, Byeong U. Park", "title": "Asymptotics for in-sample density forecasting", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1288 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, 620-651", "doi": "10.1214/14-AOS1288", "report-no": "IMS-AOS-AOS1288", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes recent proposals of density forecasting models and it\ndevelops theory for this class of models. In density forecasting, the density\nof observations is estimated in regions where the density is not observed.\nIdentification of the density in such regions is guaranteed by structural\nassumptions on the density that allows exact extrapolation. In this paper, the\nstructural assumption is made that the density is a product of one-dimensional\nfunctions. The theory is quite general in assuming the shape of the region\nwhere the density is observed. Such models naturally arise when the time point\nof an observation can be written as the sum of two terms (e.g., onset and\nincubation period of a disease). The developed theory also allows for a\nmultiplicative factor of seasonal effects. Seasonal effects are present in many\nactuarial, biostatistical, econometric and statistical studies. Smoothing\nestimators are proposed that are based on backfitting. Full asymptotic theory\nis derived for them. A practical example from the insurance business is given\nproducing a within year budget of reported insurance claims. A small sample\nstudy supports the theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 13:25:26 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Lee", "Young K.", ""], ["Mammen", "Enno", ""], ["Nielsen", "Jens P.", ""], ["Park", "Byeong U.", ""]]}, {"id": "1503.05077", "submitter": "Maud Thomas", "authors": "St\\'ephane Boucheron and Maud Thomas", "title": "Tail index estimation, concentration and adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive version of the Hill estimator based on\nLespki's model selection method. This simple data-driven index selection method\nis shown to satisfy an oracle inequality and is checked to achieve the lower\nbound recently derived by Carpentier and Kim. In order to establish the oracle\ninequality, we derive non-asymptotic variance bounds and concentration\ninequalities for Hill estimators. These concentration inequalities are derived\nfrom Talagrand's concentration inequality for smooth functions of independent\nexponentially distributed random variables combined with three tools of Extreme\nValue Theory: the quantile transform, Karamata's representation of slowly\nvarying functions, and R\\'enyi's characterisation of the order statistics of\nexponential samples. The performance of this computationally and conceptually\nsimple method is illustrated using Monte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 14:43:28 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 13:34:44 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 10:46:32 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Boucheron", "St\u00e9phane", ""], ["Thomas", "Maud", ""]]}, {"id": "1503.05093", "submitter": "Yohann de Castro", "authors": "Jean-Marc Aza\\\"is, Yohann de Castro, St\\'ephane Mourareau", "title": "Power of the Spacing test for Least-Angle Regression", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Post-Selection Inference have shown that conditional\ntesting is relevant and tractable in high-dimensions. In the Gaussian linear\nmodel, further works have derived unconditional test statistics such as the\nKac-Rice Pivot for general penalized problems. In order to test the global\nnull, a prominent offspring of this breakthrough is the spacing test that\naccounts the relative separation between the first two knots of the celebrated\nleast-angle regression (LARS) algorithm. However, no results have been shown\nregarding the distribution of these test statistics under the alternative. For\nthe first time, this paper addresses this important issue for the spacing test\nand shows that it is unconditionally unbiased. Furthermore, we provide the\nfirst extension of the spacing test to the frame of unknown noise variance.\n  More precisely, we investigate the power of the spacing test for LARS and\nprove that it is unbiased: its power is always greater or equal to the\nsignificance level $\\alpha$. In particular, we describe the power of this test\nunder various scenarii: we prove that its rejection region is optimal when the\npredictors are orthogonal; as the level $\\alpha$ goes to zero, we show that the\nprobability of getting a true positive is much greater than $\\alpha$; and we\ngive a detailed description of its power in the case of two predictors.\nMoreover, we numerically investigate a comparison between the spacing test for\nLARS and the Pearson's chi-squared test (goodness of fit).\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 15:32:08 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 19:45:05 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 20:19:59 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2015 11:14:10 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Aza\u00efs", "Jean-Marc", ""], ["de Castro", "Yohann", ""], ["Mourareau", "St\u00e9phane", ""]]}, {"id": "1503.05160", "submitter": "Enayetur Raheem", "authors": "A. K. Md. Ehsanes Saleh and Enayetur Raheem", "title": "Improved LASSO", "comments": "17 pages, 12 figures, 24 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved LASSO estimation technique based on Stein-rule. We\nshrink classical LASSO estimator using preliminary test, shrinkage, and\npositive-rule shrinkage principle. Simulation results have been carried out for\nvarious configurations of correlation coefficients ($r$), size of the parameter\nvector ($\\beta$), error variance ($\\sigma^2$) and number of non-zero\ncoefficients ($k$) in the model parameter vector. Several real data examples\nhave been used to demonstrate the practical usefulness of the proposed\nestimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$\nStein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly\nin the divergence parameter $\\Delta^2$ as in the traditional case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 18:47:28 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Saleh", "A. K. Md. Ehsanes", ""], ["Raheem", "Enayetur", ""]]}, {"id": "1503.05289", "submitter": "Ting Zhang", "authors": "Ting Zhang, Wei Biao Wu", "title": "Time-varying nonlinear regression models: Nonparametric estimation and\n  model selection", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1299 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, 741-768", "doi": "10.1214/14-AOS1299", "report-no": "IMS-AOS-AOS1299", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a general class of nonparametric time series regression\nmodels where the regression function can be time-dependent. We establish an\nasymptotic theory for estimates of the time-varying regression functions. For\nthis general class of models, an important issue in practice is to address the\nnecessity of modeling the regression function as nonlinear and time-varying. To\ntackle this, we propose an information criterion and prove its selection\nconsistency property. The results are applied to the U.S. Treasury interest\nrate data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 06:13:33 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Ting", ""], ["Wu", "Wei Biao", ""]]}, {"id": "1503.05324", "submitter": "Yanrong Yang", "authors": "Yanrong Yang, Guangming Pan", "title": "Independence test for high dimensional data based on regularized\n  canonical correlation coefficients", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1284 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, 467-500", "doi": "10.1214/14-AOS1284", "report-no": "IMS-AOS-AOS1284", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new statistic to test independence between two high\ndimensional random vectors ${\\mathbf{X}}:p_1\\times1$ and\n${\\mathbf{Y}}:p_2\\times1$. The proposed statistic is based on the sum of\nregularized sample canonical correlation coefficients of ${\\mathbf{X}}$ and\n${\\mathbf{Y}}$. The asymptotic distribution of the statistic under the null\nhypothesis is established as a corollary of general central limit theorems\n(CLT) for the linear statistics of classical and regularized sample canonical\ncorrelation coefficients when $p_1$ and $p_2$ are both comparable to the sample\nsize $n$. As applications of the developed independence test, various types of\ndependent structures, such as factor models, ARCH models and a general\nuncorrelated but dependent case, etc., are investigated by simulations. As an\nempirical application, cross-sectional dependence of daily stock returns of\ncompanies between different sections in the New York Stock Exchange (NYSE) is\ndetected by the proposed test.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 10:07:02 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Yang", "Yanrong", ""], ["Pan", "Guangming", ""]]}, {"id": "1503.05392", "submitter": "Jana Jure\\v{c}kov\\'a", "authors": "Pranab Kumar Sen, Jana Jureckova, Jan Picek", "title": "Affine equivariant rank-weighted L-estimation of multivariate location", "comments": "16 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multivariate one-sample location model, we propose a class of flexible\nrobust, affine-equivariant L-estimators of location, for distributions invoking\naffine-invariance of Mahalanobis distances of individual observations. An\ninvolved iteration process for their computation is numerically illustrated.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 13:22:47 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 12:44:29 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Sen", "Pranab Kumar", ""], ["Jureckova", "Jana", ""], ["Picek", "Jan", ""]]}, {"id": "1503.05436", "submitter": "Damian Kozbur", "authors": "Damian Kozbur", "title": "Inference in Additively Separable Models With a High-Dimensional Set of\n  Conditioning Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies nonparametric series estimation and inference for the\neffect of a single variable of interest x on an outcome y in the presence of\npotentially high-dimensional conditioning variables z. The context is an\nadditively separable model E[y|x, z] = g0(x) + h0(z). The model is\nhigh-dimensional in the sense that the series of approximating functions for\nh0(z) can have more terms than the sample size, thereby allowing z to have\npotentially very many measured characteristics. The model is required to be\napproximately sparse: h0(z) can be approximated using only a small subset of\nseries terms whose identities are unknown. This paper proposes an estimation\nand inference method for g0(x) called Post-Nonparametric Double Selection which\nis a generalization of Post-Double Selection. Standard rates of convergence and\nasymptotic normality for the estimator are shown to hold uniformly over a large\nclass of sparse data generating processes. A simulation study illustrates\nfinite sample estimation properties of the proposed estimator and coverage\nproperties of the corresponding confidence intervals. Finally, an empirical\napplication to college admissions policy demonstrates the practical\nimplementation of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 14:50:15 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 13:13:19 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 13:32:40 GMT"}, {"version": "v4", "created": "Thu, 22 Feb 2018 14:16:19 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 09:48:46 GMT"}, {"version": "v6", "created": "Mon, 6 Apr 2020 16:38:10 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kozbur", "Damian", ""]]}, {"id": "1503.05459", "submitter": "Tingran Gao", "authors": "Tingran Gao", "title": "Hypoelliptic Diffusion Maps I: Tangent Bundles", "comments": "80 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a framework\ngeneralizing Diffusion Maps in the context of manifold learning and\ndimensionality reduction. Standard non-linear dimensionality reduction methods\n(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on mining\nmassive data sets using weighted affinity graphs; Orientable Diffusion Maps and\nVector Diffusion Maps enrich these graphs by attaching to each node also some\nlocal geometry. HDM likewise considers a scenario where each node possesses\nadditional structure, which is now itself of interest to investigate.\nVirtually, HDM augments the original data set with attached structures, and\nprovides tools for studying and organizing the augmented ensemble. The goal is\nto obtain information on individual structures attached to the nodes and on the\nrelationship between structures attached to nearby nodes, so as to study the\nunderlying manifold from which the nodes are sampled. In this paper, we analyze\nHDM on tangent bundles, revealing its intimate connection with sub-Riemannian\ngeometry and a family of hypoelliptic differential operators. In a later paper,\nwe shall consider more general fibre bundles.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 03:36:50 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gao", "Tingran", ""]]}, {"id": "1503.05509", "submitter": "Sebastien Marmin", "authors": "S\\'ebastien Marmin (I2M, IRSN, IMSV), Cl\\'ement Chevalier, David\n  Ginsbourger (IMSV)", "title": "Differentiating the multipoint Expected Improvement for optimal batch\n  design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with parallel optimization of expensive objective functions\nwhich are modeled as sample realizations of Gaussian processes. The study is\nformalized as a Bayesian optimization problem, or continuous multi-armed bandit\nproblem, where a batch of q > 0 arms is pulled in parallel at each iteration.\nSeveral algorithms have been developed for choosing batches by trading off\nexploitation and exploration. As of today, the maximum Expected Improvement\n(EI) and Upper Confidence Bound (UCB) selection rules appear as the most\nprominent approaches for batch selection. Here, we build upon recent work on\nthe multipoint Expected Improvement criterion, for which an analytic expansion\nrelying on Tallis' formula was recently established. The computational burden\nof this selection rule being still an issue in application, we derive a\nclosed-form expression for the gradient of the multipoint Expected Improvement,\nwhich aims at facilitating its maximization using gradient-based ascent\nalgorithms. Substantial computational savings are shown in application. In\naddition, our algorithms are tested numerically and compared to\nstate-of-the-art UCB-based batch-sequential algorithms. Combining starting\ndesigns relying on UCB with gradient-based EI local optimization finally\nappears as a sound option for batch design in distributed Gaussian Process\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 17:45:44 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 15:15:37 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 06:38:43 GMT"}, {"version": "v4", "created": "Mon, 2 Sep 2019 12:56:21 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Marmin", "S\u00e9bastien", "", "I2M, IRSN, IMSV"], ["Chevalier", "Cl\u00e9ment", "", "IMSV"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1503.05526", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Interpretable Aircraft Engine Diagnostic via Expert Indicator\n  Aggregation", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.6214,\n  arXiv:1409.4747, arXiv:1407.0880", "journal-ref": "Transactions on Machine Learning and Data Mining, 2014, 7 (2),\n  pp.39-64", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting early signs of failures (anomalies) in complex systems is one of\nthe main goal of preventive maintenance. It allows in particular to avoid\nactual failures by (re)scheduling maintenance operations in a way that\noptimizes maintenance costs. Aircraft engine health monitoring is one\nrepresentative example of a field in which anomaly detection is crucial.\nManufacturers collect large amount of engine related data during flights which\nare used, among other applications, to detect anomalies. This article\nintroduces and studies a generic methodology that allows one to build automatic\nearly signs of anomaly detection in a way that builds upon human expertise and\nthat remains understandable by human operators who make the final maintenance\ndecision. The main idea of the method is to generate a very large number of\nbinary indicators based on parametric anomaly scores designed by experts,\ncomplemented by simple aggregations of those scores. A feature selection method\nis used to keep only the most discriminant indicators which are used as inputs\nof a Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:30:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1503.05629", "submitter": "Bill Ralph Ph.D.", "authors": "William J. Ralph", "title": "Slide Statistics And Financial Returns", "comments": "6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1404.4339", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to financial returns based on an infinite family\nof statistics called slide statistics. The evidence these statistics provide\nsuggests that certain distributions such as the stable distributions are not\ngood models for the financial returns from various securities and indexes. The\nslide statistics are derived from a variant of differential entropy called the\ngenial entropy and can be computed for any sample in a metric space. We give\nexplicit formulas for the first two of these statistics that are easily\nevaluated by a computer and make this theory particularly suitable for\napplications. In simulations with a normal random variable, the first slide\nstatistic appears to converge to Pi/4 and for certain other random variables it\nappears to converge to the reciprocal of the Hausdorff dimension.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 02:17:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ralph", "William J.", ""]]}, {"id": "1503.05748", "submitter": "Mathieu Ribatet", "authors": "Cl\\'ement Dombry, Mathieu Ribatet and Stilian Stoev", "title": "Probabilities of concurrent extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical modelling of spatial extremes has recently made major\nadvances. Much of its focus so far has been on the modelling of the magnitudes\nof extreme events but little attention has been paid on the timing of extremes.\nTo address this gap, this paper introduces the notion of extremal concurrence.\nSuppose that one measures precipitation at several synoptic stations over\nmultiple days. We say that extremes are concurrent if the maximum precipitation\nover time at each station is achieved simultaneously, e.g., on a single day.\nUnder general conditions, we show that the finite sample concurrence\nprobability converges to an asymptotic quantity, deemed extremal concurrence\nprobability. Using Palm calculus, we establish general expressions for the\nextremal concurrence probability through the max-stable process emerging in the\nlimit of the componentwise maxima of the sample. Explicit forms of the extremal\nconcurrence probabilities are obtained for various max-stable models and\nseveral estimators are introduced. In particular, we prove that the pairwise\nextremal concurrence probability for max-stable vectors is precisely equal to\nthe Kendall's $\\tau$. The estimators are evaluated by using simulations and\napplied to study the concurrence patterns of temperature extremes in the United\nStates. The results demonstrate that concurrence probability can provide a\npowerful new perspective and tools for the analysis of the spatial structure\nand impact of extremes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 12:50:13 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dombry", "Cl\u00e9ment", ""], ["Ribatet", "Mathieu", ""], ["Stoev", "Stilian", ""]]}, {"id": "1503.05852", "submitter": "Tinghui Yu", "authors": "Tinghui Yu (FDA, Center for Devices and Radiological Health), Yabing\n  Mai (Merck Research Laboratories), Sherry Liu (FDA, Center for Devices and\n  Radiological Health), Xiaofei Hu (Merck Research Laboratories)", "title": "Combining Survival Trials Using Aggregate Data Based on Misspecified\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment effects of the same therapy observed from multiple clinical\ntrials can often be very different. Yet the patient characteristics accounting\nfor these differences may not be identifiable in real world practice. There\nneeds to be an unbiased way to combine the results from multiple trials and\nreport the overall treatment effect for the general population during the\ndevelopment and validation of a new therapy. The non-linear structure of the\nmaximum partial likelihood estimates for the (log) hazard ratio defined with a\nCox proportional hazard model leads to challenges in the statistical analyses\nfor combining such clinical trials. In this paper, we formulated the expected\noverall treatment effects using various modeling assumptions. Thus we are\nproposing efficient estimates and a version of Wald test for the combined\nhazard ratio using only aggregate data. Interpretation of the methods are\nprovided in the framework of robust data analyses involving misspecified\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 17:35:24 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yu", "Tinghui", "", "FDA, Center for Devices and Radiological Health"], ["Mai", "Yabing", "", "Merck Research Laboratories"], ["Liu", "Sherry", "", "FDA, Center for Devices and\n  Radiological Health"], ["Hu", "Xiaofei", "", "Merck Research Laboratories"]]}, {"id": "1503.05876", "submitter": "Todd Kuffner", "authors": "Thomas J. DiCiccio and Todd A. Kuffner and G. Alastair Young", "title": "Objective Bayes, conditional inference and the signed root likelihood\n  ratio statistic", "comments": null, "journal-ref": "Biometrika (2012), 99 (3), 675-686", "doi": "10.1093/biomet/ass028", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian properties of the signed root likelihood ratio statistic are\nanalysed. Conditions for first-order probability matching are derived by the\nexamination of the Bayesian posterior and frequentist means of this statistic.\nSecond-order matching conditions are shown to arise from matching of the\nBayesian posterior and frequentist variances of a mean-adjusted version of the\nsigned root statistic. Conditions for conditional probability matching in\nancillary statistic models are derived and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 18:38:54 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["DiCiccio", "Thomas J.", ""], ["Kuffner", "Todd A.", ""], ["Young", "G. Alastair", ""]]}, {"id": "1503.05890", "submitter": "Todd Kuffner", "authors": "Thomas J. DiCiccio, Todd A. Kuffner, G. Alastair Young and Russell\n  Zaretzki", "title": "Stability and uniqueness of $p$-values for likelihood-based inference", "comments": "Accepted for publication in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-based methods of statistical inference provide a useful general\nmethodology that is appealing, as a straightforward asymptotic theory can be\napplied for their implementation. It is important to assess the relationships\nbetween different likelihood-based inferential procedures in terms of accuracy\nand adherence to key principles of statistical inference, in particular those\nrelating to conditioning on relevant ancillary statistics. An analysis is given\nof the stability properties of a general class of likelihood-based statistics,\nincluding those derived from forms of adjusted profile likelihood, and\ncomparisons are made between inferences derived from different statistics. In\nparticular, we derive a set of sufficient conditions for agreement to\n$O_{p}(n^{-1})$, in terms of the sample size $n$, of inferences, specifically\n$p$-values, derived from different asymptotically standard normal pivots. Our\nanalysis includes inference problems concerning a scalar or vector interest\nparameter, in the presence of a nuisance parameter.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 19:07:21 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["DiCiccio", "Thomas J.", ""], ["Kuffner", "Todd A.", ""], ["Young", "G. Alastair", ""], ["Zaretzki", "Russell", ""]]}, {"id": "1503.05900", "submitter": "Todd Kuffner", "authors": "Thomas J. DiCiccio, Todd A. Kuffner and G. Alastair Young", "title": "Quantifying nuisance parameter effects via decompositions of asymptotic\n  refinements for likelihood-based statistics", "comments": "Accepted for publication in the Journal of Statistical Planning and\n  Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate inference on a scalar interest parameter in the presence of a\nnuisance parameter may be obtained using an adjusted version of the signed root\nlikelihood ratio statistic, in particular Barndorff-Nielsen's $R^*$ statistic.\nThe adjustment made by this statistic may be decomposed into a sum of two\nterms, interpreted as correcting respectively for the possible effect of\nnuisance parameters and the deviation from standard normality of the signed\nroot likelihood ratio statistic itself. We show that the adjustment terms are\ndetermined to second-order in the sample size by their means. Explicit\nexpressions are obtained for the leading terms in asymptotic expansions of\nthese means. These are easily calculated, allowing a simple way of quantifying\nand interpreting the respective effects of the two adjustments, in particular\nof the effect of a high dimensional nuisance parameter. Illustrations are given\nfor a number of examples, which provide theoretical insight to the effect of\nnuisance parameters on parametric inference. The analysis provides a\ndecomposition of the mean of the signed root statistic involving two terms: the\nfirst has the property of taking the same value whether there are no nuisance\nparameters or whether there is an orthogonal nuisance parameter, while the\nsecond is zero when there are no nuisance parameters. Similar decompositions\nare discussed for the Bartlett correction factor of the likelihood ratio\nstatistic, and for other asymptotically standard normal pivots.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 19:21:57 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["DiCiccio", "Thomas J.", ""], ["Kuffner", "Todd A.", ""], ["Young", "G. Alastair", ""]]}, {"id": "1503.05909", "submitter": "Alberto  Ohashi", "authors": "Alberto Ohashi, Alexandre B Simas", "title": "Principal Components Analysis for Semimartingales and Stochastic PDE", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.CP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a novel principal component analysis (PCA) for\nsemimartingales by introducing a suitable spectral analysis for the quadratic\nvariation operator. Motivated by high-dimensional complex systems typically\nfound in interest rate markets, we investigate correlation in high-dimensional\nhigh-frequency data generated by continuous semimartingales. In contrast to the\ntraditional PCA methodology, the directions of large variations are not\ndeterministic, but rather they are bounded variation adapted processes which\nmaximize quadratic variation almost surely. This allows us to reduce\ndimensionality from high-dimensional semimartingale systems in terms of\nquadratic covariation rather than the usual covariance concept.\n  The proposed methodology allows us to investigate space-time data driven by\nmulti-dimensional latent semimartingale state processes. The theory is applied\nto discretely-observed stochastic PDEs which admit finite-dimensional\nrealizations. In particular, we provide consistent estimators for\nfinite-dimensional invariant manifolds for Heath-Jarrow-Morton models. More\nimportantly, components of the invariant manifold associated to volatility and\ndrift dynamics are consistently estimated and identified. The proposed\nmethodology is illustrated with both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 19:47:07 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 04:04:43 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Ohashi", "Alberto", ""], ["Simas", "Alexandre B", ""]]}, {"id": "1503.05911", "submitter": "Thomas Sch\\\"urmann", "authors": "Thomas Sch\\\"urmann", "title": "A note on entropy estimation", "comments": "7 pages, including 4 figures; two references added", "journal-ref": "Neural Computation, October 2015, Vol. 27, No. 10 , Pages\n  2097-2106", "doi": "10.1162/NECO_a_00775", "report-no": null, "categories": "physics.data-an cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare an entropy estimator $H_z$ recently discussed in [10] with two\nestimators $H_1$ and $H_2$ introduced in [6][7]. We prove the identity $H_z\n\\equiv H_1$, which has not been taken into account in [10]. Then, we prove that\nthe statistical bias of $H_1$ is less than the bias of the ordinary likelihood\nestimator of entropy. Finally, by numerical simulation we verify that for the\nmost interesting regime of small sample estimation and large event spaces, the\nestimator $H_2$ has a significant smaller statistical error than $H_z$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 19:53:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 05:14:46 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Sch\u00fcrmann", "Thomas", ""]]}, {"id": "1503.05968", "submitter": "Mohamed Ali Belabbas", "authors": "M.-A. Belabbas", "title": "Geometric methods for optimal sensor design", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2015.0312", "report-no": null, "categories": "math.OC cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An observer is an estimator of the state of a dynamical system from noisy\nsensor measurements. The need for observers is ubiquitous, with applications in\nfields ranging from engineering to biology to economics. The most widely used\nobserver is the Kalman filter, which is known to be the optimal estimator of\nthe state when the noise is additive and Gaussian. Because its performance is\nlimited by the sensors to which it is paired, it is natural to seek an optimal\nsensor for the Kalman filter. The problem is however not convex and, as a\nconsequence, many ad hoc methods have been used over the years to design\nsensors. We show in this paper how to characterize and obtain the optimal\nsensor for the Kalman filter. Precisely, we exhibit a positive definite\noperator which optimal sensors have to commute with. We furthermore provide a\ngradient flow to find optimal sensors, and prove the convergence of this\ngradient flow to the unique minimum in a broad range of applications. This\noptimal sensor yields the lowest possible estimation error for measurements\nwith a fixed signal to noise ratio. The results presented here also apply to\nthe dual problem of optimal actuator design.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 23:42:30 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 15:39:43 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 15:42:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Belabbas", "M. -A.", ""]]}, {"id": "1503.06155", "submitter": "Min Wang", "authors": "Min Wang, Yuzo Maruyama", "title": "Consistency of Bayes factor for nonnested model selection when the model\n  dimension grows", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ720 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 4, 2080-2100", "doi": "10.3150/15-BEJ720", "report-no": "IMS-BEJ-BEJ720", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zellner's $g$-prior is a popular prior choice for the model selection\nproblems in the context of normal regression models. Wang and Sun [J. Statist.\nPlann. Inference 147 (2014) 95-105] recently adopt this prior and put a special\nhyper-prior for $g$, which results in a closed-form expression of Bayes factor\nfor nested linear model comparisons. They have shown that under very general\nconditions, the Bayes factor is consistent when two competing models are of\norder $O(n^{\\tau})$ for $\\tau <1$ and for $\\tau=1$ is almost consistent except\na small inconsistency region around the null hypothesis. In this paper, we\nstudy Bayes factor consistency for nonnested linear models with a growing\nnumber of parameters. Some of the proposed results generalize the ones of the\nBayes factor for the case of nested linear models. Specifically, we compare the\nasymptotic behaviors between the proposed Bayes factor and the intrinsic Bayes\nfactor in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 16:51:45 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 07:05:54 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Wang", "Min", ""], ["Maruyama", "Yuzo", ""]]}, {"id": "1503.06236", "submitter": "Rahul Agarwal", "authors": "Rahul Agarwal, Zhe Chen, Sridevi V. Sarma", "title": "Nonparametric Estimation of Band-limited Probability Density Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonparametric maximum likelihood (ML) estimator for\nband-limited (BL) probability density functions (pdfs) is proposed. The BLML\nestimator is consistent and computationally efficient. To compute the BLML\nestimator, three approximate algorithms are presented: a binary quadratic\nprogramming (BQP) algorithm for medium scale problems, a Trivial algorithm for\nlarge-scale problems that yields a consistent estimate if the underlying pdf is\nstrictly positive and BL, and a fast implementation of the Trivial algorithm\nthat exploits the band-limited assumption and the Nyquist sampling theorem\n(\"BLMLQuick\"). All three BLML estimators outperform kernel density estimation\n(KDE) algorithms (adaptive and higher order KDEs) with respect to the mean\nintegrated squared error for data generated from both BL and infinite-band\npdfs. Further, the BLMLQuick estimate is remarkably faster than the KD\nalgorithms. Finally, the BLML method is applied to estimate the conditional\nintensity function of a neuronal spike train (point process) recorded from a\nrat's entorhinal cortex grid cell, for which it outperforms state-of-the-art\nestimators used in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 21:51:39 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 21:39:46 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2015 21:08:39 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2015 01:40:31 GMT"}, {"version": "v5", "created": "Mon, 29 Jun 2015 01:32:21 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Agarwal", "Rahul", ""], ["Chen", "Zhe", ""], ["Sarma", "Sridevi V.", ""]]}, {"id": "1503.06284", "submitter": "Hiba Nassar", "authors": "Hiba Nassar", "title": "A consistent estimator of the smoothing operator in the functional\n  Hodrick-Prescott filter", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a version of the functional Hodrick-Prescott filter\nfor functional time series. We show that the associated optimal smoothing\noperator preserves the 'noise-to-signal' structure. Moreover, we propose a\nconsistent estimator of this optimal smoothing operator.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 10:41:38 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Nassar", "Hiba", ""]]}, {"id": "1503.06388", "submitter": "Stefan Wager", "authors": "Stefan Wager and Guenther Walther", "title": "Adaptive Concentration of Regression Trees, with Application to Random\n  Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence of the predictive surface of regression trees and\nforests. To support our analysis we introduce a notion of adaptive\nconcentration for regression trees. This approach breaks tree training into a\nmodel selection phase in which we pick the tree splits, followed by a model\nfitting phase where we find the best regression model consistent with these\nsplits. We then show that the fitted regression tree concentrates around the\noptimal predictor with the same splits: as d and n get large, the discrepancy\nis with high probability bounded on the order of sqrt(log(d) log(n)/k)\nuniformly over the whole regression surface, where d is the dimension of the\nfeature space, n is the number of training examples, and k is the minimum leaf\nsize for each tree. We also provide rate-matching lower bounds for this\nadaptive concentration statement. From a practical perspective, our result\nenables us to prove consistency results for adaptively grown forests in high\ndimensions, and to carry out valid post-selection inference in the sense of\nBerk et al. [2013] for subgroups defined by tree leaves.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 06:07:14 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 02:15:16 GMT"}, {"version": "v3", "created": "Sat, 30 Apr 2016 22:32:52 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Wager", "Stefan", ""], ["Walther", "Guenther", ""]]}, {"id": "1503.06401", "submitter": "Ching-Kang Ing", "authors": "Tzu-Chang F. Cheng, Ching-Kang Ing and Shu-Hui Yu", "title": "Toward optimal model averaging in regression models with time series\n  errors", "comments": "No", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a regression model with infinitely many parameters and time series\nerrors. We are interested in choosing weights for averaging across generalized\nleast squares (GLS) estimators obtained from a set of approximating models.\nHowever, GLS estimators, depending on the unknown inverse covariance matrix of\nthe errors, are usually infeasible. We therefore construct feasible generalized\nleast squares (FGLS) estimators using a consistent estimator of the unknown\ninverse matrix. Based on this inverse covariance matrix estimator and FGLS\nestimators, we develop a feasible autocovariance-corrected Mallows model\naveraging criterion to select weights, thereby providing an FGLS model\naveraging estimator of the true regression function. We show that the\ngeneralized squared error loss of our averaging estimator is asymptotically\nequivalent to the minimum one among those of GLS model averaging estimators\nwith the weight vectors belonging to a continuous set, which includes the\ndiscrete weight set used in Hansen (2007) as its proper subset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 08:08:22 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Cheng", "Tzu-Chang F.", ""], ["Ing", "Ching-Kang", ""], ["Yu", "Shu-Hui", ""]]}, {"id": "1503.06482", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Optimal binomial, Poisson, and normal left-tail domination for sums of\n  nonnegative random variables", "comments": "Version 2: fixed a typo (p. 17, line 2) and added a detail (p. 17,\n  line 9). Version 3: Added another proof of Lemma 3.2, using the Redlog\n  package of the computer algebra system Reduce (open-source and freely\n  distributed)", "journal-ref": "Electron. J. Probab., 21:1--19 (2016)", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,\\dots,X_n$ be independent nonnegative random variables (r.v.'s),\nwith $S_n:=X_1+\\dots+X_n$ and finite values of $s_i:=E X_i^2$ and $m_i:=E\nX_i>0$. Exact upper bounds on $E f(S_n)$ for all functions $f$ in a certain\nclass $\\mathcal{F}$ of nonincreasing functions are obtained, in each of the\nfollowing settings: (i) $n,m_1,\\dots,m_n,s_1,\\dots,s_n$ are fixed; (ii) $n$,\n$m:=m_1+\\dots+m_n$, and $s:=s_1+\\dots+s_n$ are fixed; (iii)~only $m$ and $s$\nare fixed. These upper bounds are of the form $E f(\\eta)$ for a certain r.v.\n$\\eta$. The r.v. $\\eta$ and the class $\\mathcal{F}$ depend on the choice of one\nof the three settings. In particular, $(m/s)\\eta$ has the binomial distribution\nwith parameters $n$ and $p:=m^2/(ns)$ in setting (ii) and the Poisson\ndistribution with parameter $\\lambda:=m^2/s$ in setting (iii). One can also let\n$\\eta$ have the normal distribution with mean $m$ and variance $s$ in any of\nthese three settings. In each of the settings, the class $\\mathcal{F}$\ncontains, and is much wider than, the class of all decreasing exponential\nfunctions. As corollaries of these results, optimal in a certain sense upper\nbounds on the left-tail probabilities $P(S_n\\le x)$ are presented, for any real\n$x$. In fact, more general settings than the ones described above are\nconsidered. Exact upper bounds on the exponential moments $E\\exp\\{hS_n\\}$ for\n$h<0$, as well as the corresponding exponential bounds on the left-tail\nprobabilities, were previously obtained by Pinelis and Utev. It is shown that\nthe new bounds on the tails are substantially better.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:33:48 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 14:29:33 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 22:04:36 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1503.06492", "submitter": "Kazuyoshi Yata", "authors": "Kazuyoshi Yata and Makoto Aoshima", "title": "High-dimensional inference on covariance structures via the extended\n  cross-data-matrix methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider testing the correlation coefficient matrix between\ntwo subsets of high-dimensional variables. We produce a test statistic by using\nthe extended cross-data-matrix (ECDM) methodology and show the unbiasedness of\nECDM estimator. We also show that the ECDM estimator has the consistency\nproperty and the asymptotic normality in high-dimensional settings. We propose\na test procedure by the ECDM estimator and evaluate its asymptotic size and\npower theoretically and numerically. We give several applications of the ECDM\nestimator. Finally, we demonstrate how the test procedure performs in actual\ndata analyses by using a microarray data set.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 23:02:03 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Yata", "Kazuyoshi", ""], ["Aoshima", "Makoto", ""]]}, {"id": "1503.06910", "submitter": "Enayetur Raheem", "authors": "Enayetur Raheem, A. K. Md. Ehsanes Saleh", "title": "Penalty, Shrinkage, and Preliminary Test Estimators under Full Model\n  Hypothesis", "comments": "28 pages, 4 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:1503.05160", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multiple regression model and compares, under full\nmodel hypothesis, analytically as well as by simulation, the performance\ncharacteristics of some popular penalty estimators such as ridge regression,\nLASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,\nrestricted estimator, preliminary test estimator, and Stein-type estimators\nwhen the dimension of the parameter space is smaller than the sample space\ndimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE while\nLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it is\nobserved that neither penalty estimators nor Stein-type estimator dominate one\nanother.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 04:40:53 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Raheem", "Enayetur", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1503.07003", "submitter": "Jana Jure\\v{c}kov\\'a", "authors": "Pranab K. Sen, Jana Jureckova, Jan Picek", "title": "Rank tests for corrupted linear models", "comments": "29 pages, 7 figures", "journal-ref": "Journal of the Indian Statistical Association, Vol. 51 No. 1,\n  2013, 201-229", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some variants of regression models, including partial, measurement error\nor error-in-variables, latent effects, semi-parametric and otherwise corrupted\nlinear models, the classical parametric tests generally do not perform well.\nVarious modifications and generalizations considered extensively in the\nliterature rests on stringent regularity assumptions which are not likely to be\ntenable in many applications. However, in such non-standard cases, rank based\ntests can be adapted better, and further, incorporation of rank analysis of\ncovariance tools enhance their power-efficiency. Numerical studies and a real\ndata illustration show the superiority of rank based inference in such\ncorrupted linear models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 12:16:03 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Sen", "Pranab K.", ""], ["Jureckova", "Jana", ""], ["Picek", "Jan", ""]]}, {"id": "1503.07187", "submitter": "Richard Herrmann", "authors": "Richard Herrmann", "title": "Generalization of the fractional Poisson distribution", "comments": "10 pages, 3 figures", "journal-ref": "Fract. Calc. Appl. Anal. (2016) 19(4) 832-842", "doi": "10.1515/fca-2016-0045", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalization of the Poisson distribution based on the generalized\nMittag-Leffler function $E_{\\alpha, \\beta}(\\lambda)$ is proposed and the raw\nmoments are calculated algebraically in terms of Bell polynomials. It is\ndemonstrated, that the proposed distribution function contains the standard\nfractional Poisson distribution as a subset. A possible interpretation of the\nadditional parameter $\\beta$ is suggested.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:26:24 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Herrmann", "Richard", ""]]}, {"id": "1503.07236", "submitter": "Christos Thrampoulidis", "authors": "Christos Thrampoulidis and Babak Hassibi", "title": "Isotropically Random Orthogonal Matrices: Performance of LASSO and\n  Minimum Conic Singular Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the precise performance of the Generalized LASSO algorithm for\nrecovering structured signals from compressed noisy measurements, obtained via\ni.i.d. Gaussian matrices, has been characterized. The analysis is based on a\nframework introduced by Stojnic and heavily relies on the use of Gordon's\nGaussian min-max theorem (GMT), a comparison principle on Gaussian processes.\nAs a result, corresponding characterizations for other ensembles of measurement\nmatrices have not been developed. In this work, we analyze the corresponding\nperformance of the ensemble of isotropically random orthogonal (i.r.o.)\nmeasurements. We consider the constrained version of the Generalized LASSO and\nderive a sharp characterization of its normalized squared error in the\nlarge-system limit. When compared to its Gaussian counterpart, our result\nanalytically confirms the superiority in performance of the i.r.o. ensemble.\nOur second result, derives an asymptotic lower bound on the minimum conic\nsingular values of i.r.o. matrices. This bound is larger than the corresponding\nbound on Gaussian matrices. To prove our results we express i.r.o. matrices in\nterms of Gaussians and show that, with some modifications, the GMT framework is\nstill applicable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 23:48:43 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Thrampoulidis", "Christos", ""], ["Hassibi", "Babak", ""]]}, {"id": "1503.07302", "submitter": "Aki Ishii", "authors": "Aki Ishii, Kazuyoshi Yata, Makoto Aoshima", "title": "Asymptotic properties of the first principal component and equality\n  tests of covariance matrices in high-dimension, low-sample-size context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common feature of high-dimensional data is that the data dimension is high,\nhowever, the sample size is relatively low. We call such data HDLSS data. In\nthis paper, we study asymptotic properties of the first principal component in\nthe HDLSS context and apply them to equality tests of covariance matrices for\nhigh dimensional data sets. We consider HDLSS asymptotic theories as the\ndimension grows for both the cases when the sample size is fixed and the sample\nsize goes to infinity. We introduce an eigenvalue estimator by the\nnoise-reduction methodology and provide asymptotic distributions of the largest\neigenvalue in the HDLSS context. We construct a confidence interval of the\nfirst contribution ratio. We give asymptotic properties both for the first PC\ndirection and PC score as well. We apply the findings to equality tests of two\ncovariance matrices in the HDLSS context. We provide numerical results and\ndiscussions about the performances both on the estimates of the first PC and\nthe equality tests of two covariance matrices.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 08:17:59 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Ishii", "Aki", ""], ["Yata", "Kazuyoshi", ""], ["Aoshima", "Makoto", ""]]}, {"id": "1503.07307", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad and H{\\aa}vard Rue", "title": "Improving the INLA approach for approximate Bayesian inference for\n  latent Gaussian models", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 9, Number 2 (2015),\n  2706-2731", "doi": "10.1214/15-EJS1092", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new copula-based correction for generalized linear mixed\nmodels (GLMMs) within the integrated nested Laplace approximation (INLA)\napproach for approximate Bayesian inference for latent Gaussian models. While\nINLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g.\nbinomial or Poisson data have been seen to be problematic. Inaccuracies can\noccur when there is a very low degree of smoothing or \"borrowing strength\"\nwithin the model, and we have therefore developed a correction aiming to push\nthe boundaries of the applicability of INLA. Our new correction has been\nimplemented as part of the R-INLA package, and adds only negligible\ncomputational cost. Empirical evaluations on both real and simulated data\nindicate that the method works well.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 09:00:22 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 17:14:30 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 14:24:17 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 11:21:11 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 09:45:06 GMT"}, {"version": "v6", "created": "Tue, 15 Dec 2015 09:23:47 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1503.07368", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu and John Lafferty", "title": "Quantized Nonparametric Estimation over Sobolev Ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the notion of minimax estimation under storage or communication\nconstraints, and prove an extension to Pinsker's theorem for nonparametric\nestimation over Sobolev ellipsoids. Placing limits on the number of bits used\nto encode any estimator, we give tight lower and upper bounds on the excess\nrisk due to quantization in terms of the number of bits, the signal size, and\nthe noise level. This establishes the Pareto optimal tradeoff between storage\nand risk under quantization constraints for Sobolev spaces. Our results and\nproof techniques combine elements of rate distortion theory and minimax\nanalysis. The proposed quantized estimation scheme, which shows achievability\nof the lower bounds, is adaptive in the usual statistical sense, achieving the\noptimal quantized minimax rate without knowledge of the smoothness parameter of\nthe Sobolev space. It is also adaptive in a computational sense, as it\nconstructs the code only after observing the data, to dynamically allocate more\ncodewords to blocks where the estimated signal size is large. Simulations are\nincluded that illustrate the effect of quantization on statistical risk.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 13:08:49 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 15:03:31 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 22:42:30 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Lafferty", "John", ""]]}, {"id": "1503.07423", "submitter": "Jesper M{\\o}ller", "authors": "Jesper M{\\o}ller, Farzaneh Safavimanesh and Jakob G. Rasmussen", "title": "The cylindrical K-function and Poisson line cluster point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing point patterns with linear structures has recently been of interest\nin e.g. neuroscience and geography. To detect anisotropy in such cases, we\nintroduce a functional summary statistic, called the cylindrical $K$-function,\nsince it is a directional $K$-function whose structuring element is a cylinder.\nFurther we introduce a class of anisotropic Cox point processes, called Poisson\nline cluster point processes. The points of such a process are random\ndisplacements of Poisson point processes defined on the lines of a Poisson line\nprocess. Parameter estimation based on moment methods or Bayesian inference for\nthis model is discussed when the underlying Poisson line process and the\ncluster memberships are treated as hidden processes. To illustrate the\nmethodologies, we analyze a two and a three-dimensional point pattern data set.\nThe 3D data set is of particular interest as it relates to the minicolumn\nhypothesis in neuroscience, claiming that pyramidal and other brain cells have\na columnar arrangement perpendicular to the pial surface of the brain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 15:26:56 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 12:52:36 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2015 09:56:53 GMT"}, {"version": "v4", "created": "Wed, 20 Apr 2016 06:47:31 GMT"}, {"version": "v5", "created": "Fri, 26 Aug 2016 11:25:28 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Safavimanesh", "Farzaneh", ""], ["Rasmussen", "Jakob G.", ""]]}, {"id": "1503.07642", "submitter": "Trevelyan J. McKinley", "authors": "Trevelyan J. McKinley, Michelle Morters, James L. N. Wood", "title": "Bayesian Model Choice in Cumulative Link Ordinal Regression Models", "comments": "Published at http://dx.doi.org/10.1214/14-BA884 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 1-30", "doi": "10.1214/14-BA884", "report-no": "VTeX-BA-BA884", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the proportional odds (PO) model for ordinal regression is\nubiquitous in the literature. If the assumption of parallel lines does not hold\nfor the data, then an alternative is to specify a non-proportional odds (NPO)\nmodel, where the regression parameters are allowed to vary depending on the\nlevel of the response. However, it is often difficult to fit these models, and\nchallenges regarding model choice and fitting are further compounded if there\nare a large number of explanatory variables. We make two contributions towards\ntackling these issues: firstly, we develop a Bayesian method for fitting these\nmodels, that ensures the stochastic ordering conditions hold for an arbitrary\nfinite range of the explanatory variables, allowing NPO models to be fitted to\nany observed data set. Secondly, we use reversible-jump Markov chain Monte\nCarlo to allow the model to choose between PO and NPO structures for each\nexplanatory variable, and show how variable selection can be incorporated.\nThese methods can be adapted for any monotonic increasing link functions. We\nillustrate the utility of these approaches on novel data from a longitudinal\nstudy of individual-level risk factors affecting body condition score in a dog\npopulation in Zenzele, South Africa.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 08:07:41 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["McKinley", "Trevelyan J.", ""], ["Morters", "Michelle", ""], ["Wood", "James L. N.", ""]]}, {"id": "1503.07643", "submitter": "Fumiyasu Komaki", "authors": "Fumiyasu Komaki", "title": "Asymptotic Properties of Bayesian Predictive Densities When the\n  Distributions of Data and Target Variables are Different", "comments": "Published at http://dx.doi.org/10.1214/14-BA886 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 31-51", "doi": "10.1214/14-BA886", "report-no": "VTeX-BA-BA886", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian predictive densities when the observed data $x$ and the target\nvariable $y$ to be predicted have different distributions are investigated by\nusing the framework of information geometry. The performance of predictive\ndensities is evaluated by the Kullback--Leibler divergence. The parametric\nmodels are formulated as Riemannian manifolds. In the conventional setting in\nwhich $x$ and $y$ have the same distribution, the Fisher--Rao metric and the\nJeffreys prior play essential roles. In the present setting in which $x$ and\n$y$ have different distributions, a new metric, which we call the predictive\nmetric, constructed by using the Fisher information matrices of $x$ and $y$,\nand the volume element based on the predictive metric play the corresponding\nroles. It is shown that Bayesian predictive densities based on priors\nconstructed by using non-constant positive superharmonic functions with respect\nto the predictive metric asymptotically dominate those based on the volume\nelement prior of the predictive metric.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 08:24:02 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Komaki", "Fumiyasu", ""]]}, {"id": "1503.07686", "submitter": "Giovanni Pistone", "authors": "Giovanni Pistone and Grazia Vicario", "title": "How to model the covariance structure in a spatial framework: variogram\n  or correlation function?", "comments": "This paper is the final revision of arXiv:1503.07686 [math.ST] with a\n  new title", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic Kriging's model assumes a Gaussian distribution with stationary\nmean and stationary variance. In such a setting, the joint distribution of the\nspatial process is characterized by the common variance and the correlation\nmatrix or, equivalently, by the common variance and the variogram matrix. We\ndiscuss in in detail the option to actually use the variogram as a\nparameterization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:09:58 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 13:14:48 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 15:41:27 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Pistone", "Giovanni", ""], ["Vicario", "Grazia", ""]]}, {"id": "1503.07791", "submitter": "Fernando V. Bonassi", "authors": "Fernando V. Bonassi, Mike West", "title": "Sequential Monte Carlo with Adaptive Weights for Approximate Bayesian\n  Computation", "comments": "Published at http://dx.doi.org/10.1214/14-BA891 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 171-187", "doi": "10.1214/14-BA891", "report-no": "VTeX-BA-BA891", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of approximate Bayesian computation (ABC) are increasingly used for\nanalysis of complex models. A major challenge for ABC is over-coming the often\ninherent problem of high rejection rates in the accept/reject methods based on\nprior:predictive sampling. A number of recent developments aim to address this\nwith extensions based on sequential Monte Carlo (SMC) strategies. We build on\nthis here, introducing an ABC SMC method that uses data-based adaptive weights.\nThis easily implemented and computationally trivial extension of ABC SMC can\nvery substantially improve acceptance rates, as is demonstrated in a series of\nexamples with simulated and real data sets, including a currently topical\nexample from dynamic modelling in systems biology applications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:12:57 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Bonassi", "Fernando V.", ""], ["West", "Mike", ""]]}, {"id": "1503.07940", "submitter": "Ananda Theertha Suresh", "authors": "Alon Orlitsky and Ananda Theertha Suresh", "title": "Competitive Distribution Estimation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating an unknown distribution from its samples is a fundamental problem\nin statistics. The common, min-max, formulation of this goal considers the\nperformance of the best estimator over all distributions in a class. It shows\nthat with $n$ samples, distributions over $k$ symbols can be learned to a KL\ndivergence that decreases to zero with the sample size $n$, but grows\nunboundedly with the alphabet size $k$.\n  Min-max performance can be viewed as regret relative to an oracle that knows\nthe underlying distribution. We consider two natural and modest limits on the\noracle's power. One where it knows the underlying distribution only up to\nsymbol permutations, and the other where it knows the exact distribution but is\nrestricted to use natural estimators that assign the same probability to\nsymbols that appeared equally many times in the sample.\n  We show that in both cases the competitive regret reduces to\n$\\min(k/n,\\tilde{\\mathcal{O}}(1/\\sqrt n))$, a quantity upper bounded uniformly\nfor every alphabet size. This shows that distributions can be estimated nearly\nas well as when they are essentially known in advance, and nearly as well as\nwhen they are completely known in advance but need to be estimated via a\nnatural estimator. We also provide an estimator that runs in linear time and\nincurs competitive regret of $\\tilde{\\mathcal{O}}(\\min(k/n,1/\\sqrt n))$, and\nshow that for natural estimators this competitive regret is inevitable. We also\ndemonstrate the effectiveness of competitive estimators using simulations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 01:41:48 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1503.08060", "submitter": "Guillaume Dehaene", "authors": "Guillaume Dehaene, Simon Barthelm\\'e", "title": "Expectation Propagation in the large-data limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Propagation (Minka, 2001) is a widely successful algorithm for\nvariational inference. EP is an iterative algorithm used to approximate\ncomplicated distributions, typically to find a Gaussian approximation of\nposterior distributions. In many applications of this type, EP performs\nextremely well. Surprisingly, despite its widespread use, there are very few\ntheoretical guarantees on Gaussian EP, and it is quite poorly understood.\n  In order to analyze EP, we first introduce a variant of EP: averaged-EP\n(aEP), which operates on a smaller parameter space. We then consider aEP and EP\nin the limit of infinite data, where the overall contribution of each\nlikelihood term is small and where posteriors are almost Gaussian. In this\nlimit, we prove that the iterations of both aEP and EP are simple: they behave\nlike iterations of Newton's algorithm for finding the mode of a function. We\nuse this limit behavior to prove that EP is asymptotically exact, and to obtain\nother insights into the dynamic behavior of EP: for example, that it may\ndiverge under poor initialization exactly like Newton's method. EP is a simple\nalgorithm to state, but a difficult one to study. Our results should facilitate\nfurther research into the theoretical properties of this important method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 13:15:42 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 11:32:47 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dehaene", "Guillaume", ""], ["Barthelm\u00e9", "Simon", ""]]}, {"id": "1503.08123", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Johanna F. Ziegel", "title": "Higher order elicitability and Osband's principle", "comments": "32 pages", "journal-ref": "The Annals of Statistics 2016, Vol. 44, No. 4, 1680-1707", "doi": "10.1214/16-AOS1439", "report-no": null, "categories": "math.ST q-fin.MF q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical functional, such as the mean or the median, is called\nelicitable if there is a scoring function or loss function such that the\ncorrect forecast of the functional is the unique minimizer of the expected\nscore. Such scoring functions are called strictly consistent for the\nfunctional. The elicitability of a functional opens the possibility to compare\ncompeting forecasts and to rank them in terms of their realized scores. In this\npaper, we explore the notion of elicitability for multi-dimensional functionals\nand give both necessary and sufficient conditions for strictly consistent\nscoring functions. We cover the case of functionals with elicitable components,\nbut we also show that one-dimensional functionals that are not elicitable can\nbe a component of a higher order elicitable functional. In the case of the\nvariance this is a known result. However, an important result of this paper is\nthat spectral risk measures with a spectral measure with finite support are\njointly elicitable if one adds the `correct' quantiles. A direct consequence of\napplied interest is that the pair (Value at Risk, Expected Shortfall) is\njointly elicitable under mild conditions that are usually fulfilled in risk\nmanagement applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 16:10:23 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 09:29:46 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 17:15:23 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Fissler", "Tobias", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1503.08195", "submitter": "Tilmann Gneiting", "authors": "Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Kr\\\"uger", "title": "Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n  Representations, and Forecast Rankings", "comments": "References updated; a few minor edits in response to initial comments\n  (merely for clarity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the practice of point prediction, it is desirable that forecasters receive\na directive in the form of a statistical functional, such as the mean or a\nquantile of the predictive distribution. When evaluating and comparing\ncompeting forecasts, it is then critical that the scoring function used for\nthese purposes be consistent for the functional at hand, in the sense that the\nexpected score is minimized when following the directive.\n  We show that any scoring function that is consistent for a quantile or an\nexpectile functional, respectively, can be represented as a mixture of extremal\nscoring functions that form a linearly parameterized family. Scoring functions\nfor the mean value and probability forecasts of binary events constitute\nimportant examples. The quantile and expectile functionals along with the\nrespective extremal scoring functions admit appealing economic interpretations\nin terms of thresholds in decision making.\n  The Choquet type mixture representations give rise to simple checks of\nwhether a forecast dominates another in the sense that it is preferable under\nany consistent scoring function. In empirical settings it suffices to compare\nthe average scores for only a finite number of extremal elements. Plots of the\naverage scores with respect to the extremal scoring functions, which we call\nMurphy diagrams, permit detailed comparisons of the relative merits of\ncompeting forecasts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 19:35:55 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 14:08:40 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Ehm", "Werner", ""], ["Gneiting", "Tilmann", ""], ["Jordan", "Alexander", ""], ["Kr\u00fcger", "Fabian", ""]]}, {"id": "1503.08337", "submitter": "Kean Ming Tan", "authors": "Rina Foygel Barber, Mathias Drton, and Kean Ming Tan", "title": "Laplace Approximation in High-dimensional Bayesian Regression", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian variable selection in sparse high-dimensional\nregression, where the number of covariates $p$ may be large relative to the\nsamples size $n$, but at most a moderate number $q$ of covariates are active.\nSpecifically, we treat generalized linear models. For a single fixed sparse\nmodel with well-behaved prior distribution, classical theory proves that the\nLaplace approximation to the marginal likelihood of the model is accurate for\nsufficiently large sample size $n$. We extend this theory by giving results on\nuniform accuracy of the Laplace approximation across all models in a\nhigh-dimensional scenario in which $p$ and $q$, and thus also the number of\nconsidered models, may increase with $n$. Moreover, we show how this connection\nbetween marginal likelihood and Laplace approximation can be used to obtain\nconsistency results for Bayesian approaches to variable selection in\nhigh-dimensional regression.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 19:10:47 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Drton", "Mathias", ""], ["Tan", "Kean Ming", ""]]}, {"id": "1503.08340", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan and Daniela Witten", "title": "Statistical Properties of Convex Clustering", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study the statistical properties of convex clustering.\nWe establish that convex clustering is closely related to single linkage\nhierarchical clustering and $k$-means clustering. In addition, we derive the\nrange of tuning parameter for convex clustering that yields a non-trivial\nsolution. We also provide an unbiased estimate of the degrees of freedom, and\nprovide a finite sample bound for the prediction error for convex clustering.\nWe compare convex clustering to some traditional clustering methods in\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 19:47:51 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 01:02:49 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Tan", "Kean Ming", ""], ["Witten", "Daniela", ""]]}, {"id": "1503.08357", "submitter": "Maria Terres", "authors": "Maria A. Terres and Alan E. Gelfand", "title": "Spatial Process Gradients and Their Use in Sensitivity Analysis for\n  Environmental Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops methodology for local sensitivity analysis based on\ndirectional derivatives associated with spatial processes. Formal gradient\nanalysis for spatial processes was elaborated in previous papers, focusing on\ndistribution theory for directional derivatives associated with a response\nvariable assumed to follow a Gaussian process model. In the current work, these\nideas are extended to additionally accommodate a continuous covariate whose\ndirectional derivatives are also of interest and to relate the behavior of the\ndirectional derivatives of the response surface to those of the covariate\nsurface. It is of interest to assess whether, in some sense, the gradients of\nthe response follow those of the explanatory variable. The joint Gaussian\nstructure of all variables, including the directional derivatives, allows for\nexplicit distribution theory and, hence, kriging across the spatial region\nusing multivariate normal theory. Working within a Bayesian hierarchical\nmodeling framework, posterior samples enable all gradient analysis to occur\npost model fitting. As a proof of concept, we show how our methodology can be\napplied to a standard geostatistical modeling setting using a simulation\nexample. For a real data illustration, we work with point pattern data,\ndeferring our gradient analysis to the intensity surface, adopting a\nlog-Gaussian Cox process model. In particular, we relate elevation data to\npoint patterns associated with several tree species in Duke Forest.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:08:53 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Terres", "Maria A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1503.08393", "submitter": "Weijie Su", "authors": "Weijie Su and Emmanuel Candes", "title": "SLOPE is Adaptive to Unknown Sparsity and Asymptotically Minimax", "comments": "To appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional sparse regression problems in which we observe\n$y = X \\beta + z$, where $X$ is an $n \\times p$ design matrix and $z$ is an\n$n$-dimensional vector of independent Gaussian errors, each with variance\n$\\sigma^2$. Our focus is on the recently introduced SLOPE estimator ((Bogdan et\nal., 2014)), which regularizes the least-squares estimates with the\nrank-dependent penalty $\\sum_{1 \\le i \\le p} \\lambda_i |\\hat \\beta|_{(i)}$,\nwhere $|\\hat \\beta|_{(i)}$ is the $i$th largest magnitude of the fitted\ncoefficients. Under Gaussian designs, where the entries of $X$ are\ni.i.d.~$\\mathcal{N}(0, 1/n)$, we show that SLOPE, with weights $\\lambda_i$ just\nabout equal to $\\sigma \\cdot \\Phi^{-1}(1-iq/(2p))$ ($\\Phi^{-1}(\\alpha)$ is the\n$\\alpha$th quantile of a standard normal and $q$ is a fixed number in $(0,1)$)\nachieves a squared error of estimation obeying \\[ \\sup_{\\| \\beta\\|_0 \\le k}\n\\,\\, \\mathbb{P} \\left(\\| \\hat{\\beta}_{\\text{SLOPE}} - \\beta \\|^2 > (1+\\epsilon)\n\\, 2\\sigma^2 k \\log(p/k) \\right) \\longrightarrow 0 \\] as the dimension $p$\nincreases to $\\infty$, and where $\\epsilon > 0$ is an arbitrary small constant.\nThis holds under a weak assumption on the $\\ell_0$-sparsity level, namely, $k/p\n\\rightarrow 0$ and $(k\\log p)/n \\rightarrow 0$, and is sharp in the sense that\nthis is the best possible error any estimator can achieve. A remarkable feature\nis that SLOPE does not require any knowledge of the degree of sparsity, and yet\nautomatically adapts to yield optimal total squared errors over a wide range of\n$\\ell_0$-sparsity classes. We are not aware of any other estimator with this\nproperty.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 06:49:40 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 06:43:27 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 20:43:06 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Su", "Weijie", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1503.08452", "submitter": "Kattumannil Sudheesh Dr", "authors": "K. K. Sudheesh", "title": "An exact test for renewal increasing mean residual life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an exact test for testing exponentiality against\nrenewal increasing mean residual life class. Pitman's asymptotic efficacy value\nshows that our test perform well. Some numerical results are presented to\ndemonstrate the performance of the testing method. We also discuss how the\nproposed method incorporates the right censored observations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 16:02:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 06:42:54 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 07:19:33 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Sudheesh", "K. K.", ""]]}, {"id": "1503.08458", "submitter": "S\\'andor Zolt\\'an N\\'emeth", "authors": "A. B. N\\'emeth and S. Z. N\\'emeth", "title": "Isotonic regression and isotonic projection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The note describes the cones in the Euclidean space admitting isotonic metric\nprojection with respect to the coordinate-wise ordering. As a consequence it is\nshowed that the metric projection onto the regression cone (the cone defined by\nthe general isotonic regression problem) admits a projection which is isotonic\nwith respect to the coordinate-wise ordering.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 16:59:19 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 16:02:08 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["N\u00e9meth", "A. B.", ""], ["N\u00e9meth", "S. Z.", ""]]}, {"id": "1503.08562", "submitter": "Clement Marteau", "authors": "Cl\\'ement Marteau (IMT), Theofanis Sapatinas", "title": "Minimax Goodness-of-Fit Testing in Ill-Posed Inverse Problems with\n  Partially Unknown Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Gaussian sequence model that contains ill-posed inverse\nproblems as special cases. We assume that the associated operator is partially\nunknown in the sense that its singular functions are known and the\ncorresponding singular values are unknown but observed with Gaussian noise. For\nthe considered model, we study the minimax goodness-of-fit testing problem.\nWorking with certain ellipsoids in the space of squared-summable sequences of\nreal numbers, with a ball of positive radius removed, we obtain lower and upper\nbounds for the minimax separation radius in the non-asymptotic framework, i.e.,\nfor fixed values of the involved noise levels. Examples of mildly and severely\nill-posed inverse problems with ellipsoids of ordinary-smooth and super-smooth\nsequences are examined in detail and minimax rates of goodness-of-fit testing\nare obtained for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 07:01:35 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 12:22:15 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Marteau", "Cl\u00e9ment", "", "IMT"], ["Sapatinas", "Theofanis", ""]]}, {"id": "1503.08686", "submitter": "Jacek Wesolowski", "authors": "Jacek Wesolowski and Robert Wieczorkowski", "title": "An eigenproblem approach to optimal equal-precision sample allocation in\n  subpopulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allocation of samples in stratified and/or multistage sampling is one of the\ncentral issues of sampling theory. In a survey of a population often the\nconstraints for precision of estimators of subpopulations parameters have to be\ntaken care of during the allocation of the sample. Such issues are often solved\nwith mathematical programming procedures. In many situations it is desirable to\nallocate the sample, in a way which forces the precision of estimates at the\nsubpopulations level to be both: optimal and identical, while the constraints\nof the total (expected) size of the sample (or samples, in two-stage sampling)\nare imposed. Here our main concern is related to two-stage sampling schemes. We\nshow that such problem in a wide class of sampling plans has an elegant\nmathematical and computational solution. This is done due to a suitable\ndefinition of the optimization problem, which enables to solve it through a\nlinear algebra setting involving eigenvalues and eigenvectors of matrices\ndefined in terms of some population quantities. As a final result we obtain a\nvery simple and relatively universal method for calculating the subpopulation\noptimal and equal-precision allocation which is based on one of the most\nstandard algorithms of linear algebra (available e.g. in R software).\nTheoretical solutions are illustrated through a numerical example based on the\nLabour Force Survey. Finally, we would like to stress that the method we\ndescribe, allows to accommodate quite automatically for different levels of\nprecision priority for subpopulations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 14:36:25 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Wesolowski", "Jacek", ""], ["Wieczorkowski", "Robert", ""]]}, {"id": "1503.08844", "submitter": "Thierry Klein", "authors": "Jean-Claude Fort and Thierry Klein", "title": "New Fr\\'echet features for random distributions and associated\n  sensitivity indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we define new Fr\\`Echet features for random cumulative\ndistribution functions using contrast. These contrasts allow to construct\nWasserstein costs and our new features minimize the average costs as the\nFr\\`Echet mean minimizes the mean square Wasserstein$_2$ distance. An example\nof new features is the median, and more generally the quantiles. From these\ndefinitions, we are able to define sensitivity indices when the random\ndistribution is the output of a stochastic code. Associated to the Fr\\`Echet\nmean we extend the Sobol indices, and in general the indices associated to a\ncontrast that we previously proposed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 20:22:24 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Fort", "Jean-Claude", ""], ["Klein", "Thierry", ""]]}, {"id": "1503.08854", "submitter": "Kyle Luh", "authors": "Kyle Luh, Van Vu", "title": "Dictionary Learning with Few Samples and Matrix Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A$ be an $n \\times n$ matrix, $X$ be an $n \\times p$ matrix and $Y =\nAX$. A challenging and important problem in data analysis, motivated by\ndictionary learning and other practical problems, is to recover both $A$ and\n$X$, given $Y$. Under normal circumstances, it is clear that this problem is\nunderdetermined. However, in the case when $X$ is sparse and random, Spielman,\nWang and Wright showed that one can recover both $A$ and $X$ efficiently from\n$Y$ with high probability, given that $p$ (the number of samples) is\nsufficiently large. Their method works for $p \\ge C n^2 \\log^ 2 n$ and they\nconjectured that $p \\ge C n \\log n$ suffices. The bound $n \\log n$ is sharp for\nan obvious information theoretical reason.\n  In this paper, we show that $p \\ge C n \\log^4 n$ suffices, matching the\nconjectural bound up to a polylogarithmic factor. The core of our proof is a\ntheorem concerning $l_1$ concentration of random matrices, which is of\nindependent interest.\n  Our proof of the concentration result is based on two ideas. The first is an\neconomical way to apply the union bound. The second is a refined version of\nBernstein's concentration inequality for the sum of independent variables. Both\nhave nothing to do with random matrices and are applicable in general settings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 21:13:29 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Luh", "Kyle", ""], ["Vu", "Van", ""]]}, {"id": "1503.08923", "submitter": "Prasenjit Ghosh", "authors": "Prasenjit Ghosh and Arijit Chakrabarti", "title": "A New Step-down Procedure for Simultaneous Hypothesis Testing Under\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of simultaneous testing of\nhypotheses when the individual test statistics are not necessarily independent.\nSpecifically, we consider the problem of simultaneous testing of point null\nhypotheses against two-sided alternatives for the mean parameters of normally\ndistributed random variables. We assume that conditionally given the vector of\nmeans, these random variables jointly follow a multivariate normal distribution\nwith a known but arbitrary covariance matrix. We consider a Bayesian framework\nwhere each unknown mean parameter is modeled through a two-component \"spike and\nslab\" mixture prior. This way, unconditionally the test statistics jointly have\na mixture of multivariate normal distributions. A new testing procedure is\ndeveloped that uses the dependence among the test statistics and works in a\n\"step-down\" manner. This procedure is general enough to be applied for\nnon-normal data. A decision theoretic justification in favor of the proposed\ntesting procedure has been provided by showing that unlike many traditional\np-value based stepwise procedures, this new method possesses a certain\n\"convexity property\" which makes it admissible with respect to a vector risk\nfunction that captures the risks for the individual testing problems. An\nalternative representation of the proposed test statistics has also been\nestablished resulting in great simplification in the computational complexity.\nIt is demonstrated through extensive simulations that for various forms of\ndependence and a wide range of sparsity levels, the proposed testing procedure\ncompares quite favorably with several existing multiple testing procedures\navailable in the literature in terms of overall misclassification probability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 05:41:53 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 22:07:58 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 19:38:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Prasenjit", ""], ["Chakrabarti", "Arijit", ""]]}, {"id": "1503.09011", "submitter": "Trisha Maitra Mrs", "authors": "Trisha Maitra and Sourabh Bhattacharya", "title": "Asymptotic Theory of Bayes Factor in Stochastic Differential Equations:\n  Part I", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on asymptotic model selection in the context of stochastic\ndifferential equations (SDEs) is almost non-existent in the literature. In\nparticular, when a collection of SDEs is considered, the problem of asymptotic\nmodel selection has not been hitherto investigated. Indeed, even though the\ndiffusion coefficients may be considered known, questions on appropriate choice\nof the drift functions constitute a non-trivial model selection problem. In\nthis article, we develop the asymptotic theory for comparisons between\ncollections of SDEs with respect to the choice of drift functions using Bayes\nfactors when the number of equations (individuals) in the collection of SDEs\ntend to infinity while the time domains remain bounded for each equation. Our\nasymptotic theory covers situations when the observed processes associated with\nthe SDEs are independently and identically distributed (iid), as well as when\nthey are independently but not identically distributed (non-iid). In\nparticular, we allow incorporation of available time-dependent covariate\ninformation into each SDE through a multiplicative factor of the drift\nfunction; we also permit different initial values and domains of observations\nfor the SDEs. Our model selection problem thus encompasses selection of a set\nof appropriate time-dependent covariates from a set of available time-dependent\ncovariates, besides selection of the part of the drift function free of\ncovariates. For both iid and non-iid set-ups we establish almost sure\nexponential convergence of the Bayes factor. Furthermore, we demonstrate with\nsimulation studies that even in non-asymptotic scenarios Bayes factor\nsuccessfully captures the right set of covariates.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 11:46:34 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 10:55:37 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 09:33:09 GMT"}, {"version": "v4", "created": "Wed, 11 May 2016 09:04:39 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 11:59:21 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}]