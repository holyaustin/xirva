[{"id": "1706.00289", "submitter": "Yulong Lu", "authors": "Yulong Lu", "title": "On the Bernstein-Von Mises Theorem for High Dimensional Nonlinear\n  Bayesian Inverse Problems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a Bernstein-von Mises theorem for a general class of high\ndimensional nonlinear Bayesian inverse problems in the vanishing noise limit.\nWe propose a sufficient condition on the growth rate of the number of unknown\nparameters under which the posterior distribution is asymptotically normal.\nThis growth condition is expressed explicitly in terms of the model dimension,\nthe degree of ill-posedness of the inverse problem and the noise parameter. The\ntheoretical results are applied to a Bayesian estimation of the medium\nparameter in an elliptic problem.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 13:32:04 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 16:14:28 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Lu", "Yulong", ""]]}, {"id": "1706.00342", "submitter": "Francois Malgouyres", "authors": "Francois Malgouyres (IMT)", "title": "On the stable recovery of deep structured linear networks under sparsity\n  constraints", "comments": null, "journal-ref": "Mathematical and Scientific Machine Learning, Jul 2020, Princeton,\n  United States", "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a deep structured linear network under sparsity constraints. We\nstudy sharp conditions guaranteeing the stability of the optimal parameters\ndefining the network. More precisely, we provide sharp conditions on the\nnetwork architecture and the sample under which the error on the parameters\ndefining the network scales linearly with the reconstruction error (i.e. the\nrisk). Therefore, under these conditions, the weights obtained with a\nsuccessful algorithms are well defined and only depend on the architecture of\nthe network and the sample. The features in the latent spaces are stably\ndefined. The stability property is required in order to interpret the features\ndefined in the latent spaces. It can also lead to a guarantee on the\nstatistical risk. This is what motivates this study. The analysis is based on\nthe recently proposed Tensorial Lifting. The particularity of this paper is to\nconsider a sparsity prior. This leads to a better stability constant. As an\nillustration, we detail the analysis and provide sharp stability guarantees for\nconvolutional linear network under sparsity prior. In this analysis, we\ndistinguish the role of the network architecture and the sample input. This\nhighlights the requirements on the data in connection to parameter stability.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:49:34 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 10:04:01 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 06:16:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Malgouyres", "Francois", "", "IMT"]]}, {"id": "1706.00378", "submitter": "Igor Kheifets", "authors": "Igor Kheifets and Carlos Velasco", "title": "New goodness-of-fit diagnostics for conditional discrete response models", "comments": null, "journal-ref": "Journal of Econometrics Volume 200, Issue 1, September 2017, Pages\n  135-149", "doi": "10.1016/j.jeconom.2017.05.017", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new specification tests for conditional models with\ndiscrete responses, which are key to apply efficient maximum likelihood\nmethods, to obtain consistent estimates of partial effects and to get\nappropriate predictions of the probability of future events. In particular, we\ntest the static and dynamic ordered choice model specifications and can cover\ninfinite support distributions for e.g. count data. The traditional approach\nfor specification testing of discrete response models is based on probability\nintegral transforms of a jittered discrete data which leads to continuous\nuniform iid series under the true conditional distribution. Then, standard\nspecification testing techniques for continuous variables could be applied to\nthe transformed series, but the extra randomness from jitters affects the power\nproperties of these methods. We investigate in this paper an alternative\ntransformation based only on original discrete data that avoids any\nrandomization. We analyze the asymptotic properties of goodness-of-fit tests\nbased on this new transformation and explore the properties in finite samples\nof a bootstrap algorithm to approximate the critical values of test statistics\nwhich are model and parameter dependent. We show analytically and in\nsimulations that our approach dominates the methods based on randomization in\nterms of power. We apply the new tests to models of the monetary policy\nconducted by the Federal Reserve.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:34:36 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Kheifets", "Igor", ""], ["Velasco", "Carlos", ""]]}, {"id": "1706.00666", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine, Germain Van Bever", "title": "Tyler shape depth", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems from multivariate analysis, the parameter of interest is a\nshape matrix, that is, a normalized version of the corresponding scatter or\ndispersion matrix. In this paper, we propose a depth concept for shape matrices\nthat involves data points only through their directions from the center of the\ndistribution. We use the terminology Tyler shape depth since the resulting\nestimator of shape, namely the deepest shape matrix, is the median-based\ncounterpart of the M-estimator of shape of Tyler (1987). Beyond estimation,\nshape depth, like its Tyler antecedent, also allows hypothesis testing on\nshape. Its main benefit, however, lies in the ranking of shape matrices it\nprovides, whose practical relevance is illustrated in principal component\nanalysis and in shape-based outlier detection. We study the invariance,\nquasi-concavity and continuity properties of Tyler shape depth, the topological\nand boundedness properties of the corresponding depth regions, existence of a\ndeepest shape matrix and prove Fisher consistency in the elliptical case.\nFinally, we derive a Glivenko-Cantelli-type result and establish almost sure\nconsistency of the deepest shape matrix estimator.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 12:54:41 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 12:08:10 GMT"}, {"version": "v3", "created": "Sat, 17 Nov 2018 16:56:26 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 09:29:58 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Paindaveine", "Davy", ""], ["Van Bever", "Germain", ""]]}, {"id": "1706.00679", "submitter": "Yohann De Castro", "authors": "Jean-Marc Aza\\\"is and Yohann De Castro and St\\'ephane Mourareau", "title": "Testing Gaussian Process with Applications to Super-Resolution", "comments": "Final version, 6 figures, Python code and Jupyter notebook available\n  at https://github.com/ydecastro/super-resolution-testing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces exact testing procedures on the mean of a Gaussian\nprocess $X$ derived from the outcomes of $\\ell_1$-minimization over the space\nof complex valued measures. The process $X$ can be thought as the sum of two\nterms: first, the convolution between some kernel and a target atomic measure\n(mean of the process); second, a random perturbation by an additive centered\nGaussian process. The first testing procedure considered is based on a dense\nsequence of grids on the index set of~$X$ and we establish that it converges\n(as the grid step tends to zero) to a randomized testing procedure: the\ndecision of the test depends on the observation $X$ and also on an independent\nrandom variable. The second testing procedure is based on the maxima and the\nHessian of $X$ in a grid-less manner. We show that both testing procedures can\nbe performed when the variance is unknown (and the correlation function of $X$\nis known). These testing procedures can be used for the problem of\ndeconvolution over the space of complex valued measures, and applications in\nframe of the Super-Resolution theory are presented. As a byproduct, numerical\ninvestigations may demonstrate that our grid-less method is more powerful\n(it~detects sparse alternatives) than tests based on very thin grids.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 13:17:03 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 21:37:50 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 13:07:55 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Aza\u00efs", "Jean-Marc", ""], ["De Castro", "Yohann", ""], ["Mourareau", "St\u00e9phane", ""]]}, {"id": "1706.00729", "submitter": "Daniel Hsu", "authors": "Arushi Gupta, Daniel Hsu", "title": "Parameter identification in Markov chain choice models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the parameter identification problem for the Markov chain\nchoice model of Blanchet, Gallego, and Goyal used in assortment planning. In\nthis model, the product selected by a customer is determined by a Markov chain\nover the products, where the products in the offered assortment are absorbing\nstates. The underlying parameters of the model were previously shown to be\nidentifiable from the choice probabilities for the all-products assortment,\ntogether with choice probabilities for assortments of all-but-one products.\nObtaining and estimating choice probabilities for such large assortments is not\ndesirable in many settings. The main result of this work is that the parameters\nmay be identified from assortments of sizes two and three, regardless of the\ntotal number of products. The result is obtained via a simple and efficient\nparameter recovery algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:47:17 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 18:21:52 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 21:03:33 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Gupta", "Arushi", ""], ["Hsu", "Daniel", ""]]}, {"id": "1706.00850", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Peter Chien", "title": "Minimax Optimal Rates of Estimation in Functional ANOVA Models with\n  Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish minimax optimal rates of convergence for nonparametric\nestimation in functional ANOVA models when data from first-order partial\nderivatives are available. Our results reveal that partial derivatives can\nimprove convergence rates for function estimation with deterministic or random\ndesigns. In particular, for full $d$-interaction models, the optimal rates with\nfirst-order partial derivatives on $p$ covariates are identical to those for\n$(d-p)$-interaction models without partial derivatives. For additive models,\nthe rates by using all first-order partial derivatives are root-$n$ to achieve\nthe \"parametric rate\". We also investigate the minimax optimal rates for\nfirst-order partial derivative estimations when derivative data are available.\nThose rates coincide with the optimal rate for estimating the first-order\nderivative of a univariate function.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 20:53:12 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 21:24:40 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Dai", "Xiaowu", ""], ["Chien", "Peter", ""]]}, {"id": "1706.00857", "submitter": "Xinghao Qiao", "authors": "Heng Lian, Xinghao Qiao, Wenyang Zhang", "title": "Homogeneity Pursuit in Single Index Models based Panel Data Analysis", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panel data analysis is an important topic in statistics and econometrics.\nTraditionally, in panel data analysis, all individuals are assumed to share the\nsame unknown parameters, e.g. the same coefficients of covariates when the\nlinear models are used, and the differences between the individuals are\naccounted for by cluster effects. This kind of modelling only makes sense if\nour main interest is on the global trend, this is because it would not be able\nto tell us anything about the individual attributes which are sometimes very\nimportant. In this paper, we proposed a modelling based on the single index\nmodels embedded with homogeneity for panel data analysis, which builds the\nindividual attributes in the model and is parsimonious at the same time. We\ndevelop a data driven approach to identify the structure of homogeneity, and\nestimate the unknown parameters and functions based on the identified\nstructure. Asymptotic properties of the resulting estimators are established.\nIntensive simulation studies conducted in this paper also show the resulting\nestimators work very well when sample size is finite. Finally, the proposed\nmodelling is applied to a public financial dataset and a UK climate dataset,\nthe results reveal some interesting findings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 21:19:21 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 22:33:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Lian", "Heng", ""], ["Qiao", "Xinghao", ""], ["Zhang", "Wenyang", ""]]}, {"id": "1706.00886", "submitter": "Wenjia Wang", "authors": "Wenjia Wang and Benjamin Haaland", "title": "Controlling Sources of Inaccuracy in Stochastic Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists and engineers commonly use simulation models to study real systems\nfor which actual experimentation is costly, difficult, or impossible. Many\nsimulations are stochastic in the sense that repeated runs with the same input\nconfiguration will result in different outputs. For expensive or time-consuming\nsimulations, stochastic kriging \\citep{ankenman} is commonly used to generate\npredictions for simulation model outputs subject to uncertainty due to both\nfunction approximation and stochastic variation. Here, we develop and justify a\nfew guidelines for experimental design, which ensure accuracy of stochastic\nkriging emulators. We decompose error in stochastic kriging predictions into\nnominal, numeric, parameter estimation and parameter estimation numeric\ncomponents and provide means to control each in terms of properties of the\nunderlying experimental design. The design properties implied for each source\nof error are weakly conflicting and broad principles are proposed. In brief,\nspace-filling properties \"small fill distance\" and \"large separation distance\"\nshould balance with replication at distinct input configurations, with number\nof replications depending on the relative magnitudes of stochastic and process\nvariability. Non-stationarity implies higher input density in more active\nregions, while regression functions imply a balance with traditional design\nproperties. A few examples are presented to illustrate the results.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 02:33:38 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 14:10:29 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Wang", "Wenjia", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1706.00961", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel, Ankur Moitra, Philippe Rigollet, John Urschel", "title": "Rates of estimation for determinantal point processes", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) have wide-ranging applications in\nmachine learning, where they are used to enforce the notion of diversity in\nsubset selection problems. Many estimators have been proposed, but surprisingly\nthe basic properties of the maximum likelihood estimator (MLE) have received\nlittle attention. In this paper, we study the local geometry of the expected\nlog-likelihood function to prove several rates of convergence for the MLE. We\nalso give a complete characterization of the case where the MLE converges at a\nparametric rate. Even in the latter case, we also exhibit a potential curse of\ndimensionality where the asymptotic variance of the MLE is exponentially large\nin the dimension of the problem.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 14:40:22 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 20:25:36 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""], ["Moitra", "Ankur", ""], ["Rigollet", "Philippe", ""], ["Urschel", "John", ""]]}, {"id": "1706.01031", "submitter": "Ivan Kojadinovic", "authors": "Axel B\\\"ucher and Ivan Kojadinovic", "title": "A note on conditional versus joint unconditional weak convergence in\n  bootstrap consistency results", "comments": "21 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency of a bootstrap or resampling scheme is classically validated\nby weak convergence of conditional laws. However, when working with stochastic\nprocesses in the space of bounded functions and their weak convergence in the\nHoffmann-J{\\o}rgensen sense, an obstacle occurs: due to possible\nnon-measurability, neither laws nor conditional laws are well-defined. Starting\nfrom an equivalent formulation of weak convergence based on the bounded\nLipschitz metric, a classical circumvent is to formulate bootstrap consistency\nin terms of the latter distance between what might be called a\n\\emph{conditional law} of the (non-measurable) bootstrap process and the law of\nthe limiting process. The main contribution of this note is to provide an\nequivalent formulation of bootstrap consistency in the space of bounded\nfunctions which is more intuitive and easy to work with. Essentially, the\nequivalent formulation consists of (unconditional) weak convergence of the\noriginal process jointly with two bootstrap replicates. As a by-product, we\nprovide two equivalent formulations of bootstrap consistency for statistics\ntaking values in separable metric spaces: the first in terms of (unconditional)\nweak convergence of the statistic jointly with its bootstrap replicates, the\nsecond in terms of convergence in probability of the empirical distribution\nfunction of the bootstrap replicates. Finally, the asymptotic validity of\nbootstrap-based confidence intervals and tests is briefly revisited, with\nparticular emphasis on the, in practice unavoidable, Monte Carlo approximation\nof conditional quantiles.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 05:50:26 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:00:54 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 18:10:22 GMT"}, {"version": "v4", "created": "Thu, 1 Mar 2018 19:43:09 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1706.01074", "submitter": "Lothar Heinrich", "authors": "Lothar Heinrich", "title": "Asymptotic Goodness-of-Fit Tests for Point Processes Based on Scaled\n  Empirical K-Functions", "comments": "33 pages, 36 references", "journal-ref": null, "doi": null, "report-no": "mpreprint 17 001, Institute for Mathematics, University Augsburg", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequences of scaled edge-corrected empirical (generalized)\nK-functions (modifying Ripley's K-function) each of them constructed from a\nsingle observation of a $d$-dimensional fourth-order stationary point process\nin a sampling window W_n which grows together with some scaling rate\nunboundedly as n --> infty. Under some natural assumptions it is shown that the\nnormalized difference between scaled empirical and scaled theoretical\nK-function converges weakly to a mean zero Gaussian process with simple\ncovariance function. This result suggests discrepancy measures between\nempirical and theoretical K-function with known limit distribution which allow\nto perform goodness-of-fit tests for checking a hypothesized point process\nbased only on its intensity and (generalized) K-function. Similar test\nstatistics are derived for testing the hypothesis that two independent point\nprocesses in W_n have the same distribution without explicit knowledge of their\nintensities and K-functions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 13:44:55 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Heinrich", "Lothar", ""]]}, {"id": "1706.01124", "submitter": "Nikita Zhivotovskiy", "authors": "Nikita Zhivotovskiy", "title": "Optimal learning via local entropies and sample compression", "comments": "25 pages. Extended and restructured version. Contains new results,\n  reflected in the new section 4 and section 5. Corrects Lemma 4 of the\n  previous version. Sample compression part remained unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide several novel upper bounds on the excess\nrisk with a primal focus on classification problems. We suggest two approaches\nand the obtained bounds are represented via the distribution dependent local\nentropies of the classes or the sizes of specific sample com- pression schemes.\nWe show that in some cases, our guarantees are optimal up to constant factors\nand outperform previously known results. As an application of our results, we\nprovide a new tight PAC bound for the hard-margin SVM, an extended analysis of\ncertain empirical risk minimizers under log-concave distributions, a new\nvariant of an online to batch conversion, and distribution dependent localized\nbounds in the aggregation framework. We also develop techniques that allow to\nreplace empirical covering number or covering numbers with bracketing by the\ncoverings with respect to the distribution of the data. The proofs for the\nsample compression schemes are based on the moment method combined with the\nanalysis of voting algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 18:39:43 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 14:37:31 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zhivotovskiy", "Nikita", ""]]}, {"id": "1706.01158", "submitter": "Qiang Sun", "authors": "Qiang Sun, Kean Ming Tan, Han Liu, Tong Zhang", "title": "Graphical Nonconvex Optimization for Optimal Estimation in Gaussian\n  Graphical Models", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning high-dimensional Gaussian graphical\nmodels. The graphical lasso is one of the most popular methods for estimating\nGaussian graphical models. However, it does not achieve the oracle rate of\nconvergence. In this paper, we propose the graphical nonconvex optimization for\noptimal estimation in Gaussian graphical models, which is then approximated by\na sequence of convex programs. Our proposal is computationally tractable and\nproduces an estimator that achieves the oracle rate of convergence. The\nstatistical error introduced by the sequential approximation using the convex\nprograms are clearly demonstrated via a contraction property. The rate of\nconvergence can be further improved using the notion of sparsity pattern. The\nproposed methodology is then extended to semiparametric graphical models. We\nshow through numerical studies that the proposed estimator outperforms other\npopular methods for estimating Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 23:20:13 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Sun", "Qiang", ""], ["Tan", "Kean Ming", ""], ["Liu", "Han", ""], ["Zhang", "Tong", ""]]}, {"id": "1706.01175", "submitter": "Min Xu", "authors": "Min Xu, Varun Jog, Po-Ling Loh", "title": "Optimal Rates for Community Estimation in the Weighted Stochastic Block\n  Model", "comments": "73 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community identification in a network is an important problem in fields such\nas social science, neuroscience, and genetics. Over the past decade, stochastic\nblock models (SBMs) have emerged as a popular statistical framework for this\nproblem. However, SBMs have an important limitation in that they are suited\nonly for networks with unweighted edges; in various scientific applications,\ndisregarding the edge weights may result in a loss of valuable information. We\nstudy a weighted generalization of the SBM, in which observations are collected\nin the form of a weighted adjacency matrix and the weight of each edge is\ngenerated independently from an unknown probability density determined by the\ncommunity membership of its endpoints. We characterize the optimal rate of\nmisclustering error of the weighted SBM in terms of the Renyi divergence of\norder 1/2 between the weight distributions of within-community and\nbetween-community edges, substantially generalizing existing results for\nunweighted SBMs. Furthermore, we present a computationally tractable algorithm\nbased on discretization that achieves the optimal error rate. Our method is\nadaptive in the sense that the algorithm, without assuming knowledge of the\nweight densities, performs as well as the best algorithm that knows the weight\ndensities.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 02:11:04 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 21:06:48 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Xu", "Min", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1706.01191", "submitter": "Pragya Sur", "authors": "Pragya Sur, Yuxin Chen, Emmanuel J. Cand\\`es", "title": "The Likelihood Ratio Test in High-Dimensional Logistic Regression Is\n  Asymptotically a Rescaled Chi-Square", "comments": "58 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is used thousands of times a day to fit data, predict\nfuture outcomes, and assess the statistical significance of explanatory\nvariables. When used for the purpose of statistical inference, logistic models\nproduce p-values for the regression coefficients by using an approximation to\nthe distribution of the likelihood-ratio test. Indeed, Wilks' theorem asserts\nthat whenever we have a fixed number $p$ of variables, twice the log-likelihood\nratio (LLR) $2\\Lambda$ is distributed as a $\\chi^2_k$ variable in the limit of\nlarge sample sizes $n$; here, $k$ is the number of variables being tested. In\nthis paper, we prove that when $p$ is not negligible compared to $n$, Wilks'\ntheorem does not hold and that the chi-square approximation is grossly\nincorrect; in fact, this approximation produces p-values that are far too small\n(under the null hypothesis). Assume that $n$ and $p$ grow large in such a way\nthat $p/n\\rightarrow\\kappa$ for some constant $\\kappa < 1/2$. We prove that for\na class of logistic models, the LLR converges to a rescaled chi-square, namely,\n$2\\Lambda~\\stackrel{\\mathrm{d}}{\\rightarrow}~\\alpha(\\kappa)\\chi_k^2$, where the\nscaling factor $\\alpha(\\kappa)$ is greater than one as soon as the\ndimensionality ratio $\\kappa$ is positive. Hence, the LLR is larger than\nclassically assumed. For instance, when $\\kappa=0.3$,\n$\\alpha(\\kappa)\\approx1.5$. In general, we show how to compute the scaling\nfactor by solving a nonlinear system of two equations with two unknowns. Our\nmathematical arguments are involved and use techniques from approximate message\npassing theory, non-asymptotic random matrix theory and convex geometry. We\nalso complement our mathematical study by showing that the new limiting\ndistribution is accurate for finite sample sizes. Finally, all the results from\nthis paper extend to some other regression models such as the probit regression\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 05:21:07 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Sur", "Pragya", ""], ["Chen", "Yuxin", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1706.01240", "submitter": "Guanhua Fang", "authors": "Guanhua Fang, Jingchen Liu, and Zhiliang Ying", "title": "On the Identifiability of Diagnostic Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes fundamental results for statistical inference of\ndiagnostic classification models (DCM). The results are developed at a high\nlevel of generality, applicable to essentially all diagnostic classification\nmodels. In particular, we establish identifiability results of various modeling\nparameters, notably item response probabilities, attribute distribution, and\nQ-matrix-induced partial information structure. Consistent estimators are\nconstructed. Simulation results show that these estimators perform well under\nvarious modeling settings. We also use a real example to illustrate the new\nmethod. The results are stated under the setting of general latent class\nmodels. For DCM with a specific parameterization, the conditions may be adapted\naccordingly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 08:56:25 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fang", "Guanhua", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1706.01252", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda and Fumiyasu Komaki", "title": "Empirical Bayes Matrix Completion", "comments": "15 pages", "journal-ref": "Computational Statistics & Data Analysis, Vol. 137, pp. 195--210,\n  2019", "doi": "10.1016/j.csda.2019.02.006", "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical Bayes (EB) algorithm for the matrix completion\nproblems. The EB algorithm is motivated from the singular value shrinkage\nestimator for matrix means by Efron and Morris (1972). Since the EB algorithm\nis essentially the EM algorithm applied to a simple model, it does not require\nheuristic parameter tuning other than tolerance. Numerical results demonstrated\nthat the EB algorithm achieves a good trade-off between accuracy and efficiency\ncompared to existing algorithms and that it works particularly well when the\ndifference between the number of rows and columns is large. Application to real\ndata also shows the practical utility of the EB algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 09:50:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 10:42:32 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Matsuda", "Takeru", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1706.01291", "submitter": "Ben Berckmoes", "authors": "Ben Berckmoes, Anna Ivanova, Geert Molenberghs", "title": "On the sample mean after a group sequential trial", "comments": "52 pages (supplementary data file included)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular setting in medical statistics is a group sequential trial with\nindependent and identically distributed normal outcomes, in which interim\nanalyses of the sum of the outcomes are performed. Based on a prescribed\nstopping rule, one decides after each interim analysis whether the trial is\nstopped or continued. Consequently, the actual length of the study is a random\nvariable. It is reported in the literature that the interim analyses may cause\nbias if one uses the ordinary sample mean to estimate the location parameter.\nFor a generic stopping rule, which contains many classical stopping rules as a\nspecial case, explicit formulas for the expected length of the trial, the bias,\nand the mean squared error (MSE) are provided. It is deduced that, for a fixed\nnumber of interim analyses, the bias and the MSE converge to zero if the first\ninterim analysis is performed not too early. In addition, optimal rates for\nthis convergence are provided. Furthermore, under a regularity condition,\nasymptotic normality in total variation distance for the sample mean is\nestablished. A conclusion for naive confidence intervals based on the sample\nmean is derived. It is also shown how the developed theory naturally fits in\nthe broader framework of likelihood theory in a group sequential trial setting.\nA simulation study underpins the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 12:35:25 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 08:11:23 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 15:59:34 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 10:10:15 GMT"}, {"version": "v5", "created": "Wed, 28 Mar 2018 14:09:22 GMT"}, {"version": "v6", "created": "Sat, 31 Mar 2018 18:57:57 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Berckmoes", "Ben", ""], ["Ivanova", "Anna", ""], ["Molenberghs", "Geert", ""]]}, {"id": "1706.01357", "submitter": "Roberto Fontana", "authors": "Roberto Fontana and Patrizia Semeraro", "title": "Characterization of multivariate Bernoulli distributions with given\n  margins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We express each Fr\\'echet class of multivariate Bernoulli distributions with\ngiven margins as the convex hull of a set of densities, which belong to the\nsame Fr\\'echet class. This characterisation allows us to establish whether a\ngiven correlation matrix is compatible with the assigned margins and, if it is,\nto easily construct one of the corresponding joint densities. % Such\n%representation is based on a polynomial expression of the distributions of a\nFr\\'echet class. We reduce the problem of finding a density belonging to a\nFr\\'echet class and with given correlation matrix to the solution of a linear\nsystem of equations. Our methodology also provides the bounds that each\ncorrelation must satisfy to be compatible with the assigned margins. An\nalgorithm and its use in some examples is shown.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:51:22 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fontana", "Roberto", ""], ["Semeraro", "Patrizia", ""]]}, {"id": "1706.01418", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Learning Whenever Learning is Possible: Universal Learning under General\n  Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work initiates a general study of learning and generalization without\nthe i.i.d. assumption, starting from first principles. While the traditional\napproach to statistical learning theory typically relies on standard\nassumptions from probability theory (e.g., i.i.d. or stationary ergodic), in\nthis work we are interested in developing a theory of learning based only on\nthe most fundamental and necessary assumptions implicit in the requirements of\nthe learning problem itself. We specifically study universally consistent\nfunction learning, where the objective is to obtain low long-run average loss\nfor any target function, when the data follow a given stochastic process. We\nare then interested in the question of whether there exist learning rules\nguaranteed to be universally consistent given only the assumption that\nuniversally consistent learning is possible for the given data process. The\nreasoning that motivates this criterion emanates from a kind of optimist's\ndecision theory, and so we refer to such learning rules as being optimistically\nuniversal. We study this question in three natural learning settings:\ninductive, self-adaptive, and online. Remarkably, as our strongest positive\nresult, we find that optimistically universal learning rules do indeed exist in\nthe self-adaptive learning setting. Establishing this fact requires us to\ndevelop new approaches to the design of learning algorithms. Along the way, we\nalso identify concise characterizations of the family of processes under which\nuniversally consistent learning is possible in the inductive and self-adaptive\nsettings. We additionally pose a number of enticing open problems, particularly\nfor the online learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:51:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:25:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1706.01419", "submitter": "Artyom Kovalevskii", "authors": "Mikhail Chebunin, Artyom Kovalevskii", "title": "Asymptotically normal estimators for Zipf's law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law states that sequential frequencies of words in a text correspond\nto a power function. Its probabilistic model is an infinite urn scheme with\nasymptotically power distribution. The exponent of this distribution must be\nestimated. We use the number of different words in a text and similar\nstatistics to construct asymptotically normal estimators of the exponent.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:57:22 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 16:11:31 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""]]}, {"id": "1706.01428", "submitter": "Colin LaMont", "authors": "Colin H. LaMont and Paul A. Wiggins", "title": "On the correspondence between thermodynamics and inference", "comments": "13 pages, 6 figures, 2 tables, and appendix", "journal-ref": "Phys. Rev. E 99, 052140 (2019)", "doi": "10.1103/PhysRevE.99.052140", "report-no": null, "categories": "math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand upon a natural analogy between Bayesian statistics and statistical\nphysics in which sample size corresponds to inverse temperature. This analogy\nmotivates the definition of two novel statistical quantities: a learning\ncapacity and a Gibbs entropy. The analysis of the learning capacity,\ncorresponding to the heat capacity in thermal physics, leads to new insight\ninto the mechanism of learning and explains why some models have\nanomalously-high learning performance. We explore the properties of the\nlearning capacity in a number of examples, including a sloppy model. Next, we\npropose that the Gibbs entropy provides a natural device for counting\ndistinguishable distributions in the context of Bayesian inference. We use this\ndevice to define a generalized principle of indifference (GPI) in which every\ndistinguishable model is assigned equal a priori probability. This principle\nresults in a new solution to a long-standing problem in Bayesian inference: the\ndefinition of an objective or uninformative prior. A key characteristic of this\nnew approach is that it can be applied to analyses where the model dimension is\nunknown and circumvents the automatic rejection of higher-dimensional models in\nBayesian inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:27:12 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 18:53:12 GMT"}, {"version": "v3", "created": "Tue, 22 Aug 2017 23:03:13 GMT"}, {"version": "v4", "created": "Sat, 7 Apr 2018 01:11:29 GMT"}, {"version": "v5", "created": "Thu, 4 Apr 2019 19:19:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["LaMont", "Colin H.", ""], ["Wiggins", "Paul A.", ""]]}, {"id": "1706.01534", "submitter": "Foad Shokrollahi", "authors": "Foad Shokrollahi and Tommi Sottinen", "title": "Hedging in fractional Black-Scholes model with transaction costs", "comments": null, "journal-ref": "Statistics & Probability Letters, Volume 130, November 2017", "doi": null, "report-no": null, "categories": "q-fin.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider conditional-mean hedging in a fractional Black-Scholes pricing\nmodel in the presence of proportional transaction costs. We develop an explicit\nformula for the conditional-mean hedging portfolio in terms of the recently\ndiscovered explicit conditional law of the fractional Brownian motion.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 08:22:40 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 19:35:00 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Shokrollahi", "Foad", ""], ["Sottinen", "Tommi", ""]]}, {"id": "1706.01778", "submitter": "Susan Athey", "authors": "Alberto Abadie, Susan Athey, Guido W. Imbens, and Jeffrey M.\n  Wooldridge", "title": "Sampling-based vs. Design-based Uncertainty in Regression Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a researcher estimating the parameters of a regression function\nbased on data for all 50 states in the United States or on data for all visits\nto a website. What is the interpretation of the estimated parameters and the\nstandard errors? In practice, researchers typically assume that the sample is\nrandomly drawn from a large population of interest and report standard errors\nthat are designed to capture sampling variation. This is common even in\napplications where it is difficult to articulate what that population of\ninterest is, and how it differs from the sample. In this article, we explore an\nalternative approach to inference, which is partly design-based. In a\ndesign-based setting, the values of some of the regressors can be manipulated,\nperhaps through a policy intervention. Design-based uncertainty emanates from\nlack of knowledge about the values that the regression outcome would have taken\nunder alternative interventions. We derive standard errors that account for\ndesign-based uncertainty instead of, or in addition to, sampling-based\nuncertainty. We show that our standard errors in general are smaller than the\nusual infinite-population sampling-based standard errors and provide conditions\nunder which they coincide.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 14:11:25 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 22:25:16 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Abadie", "Alberto", ""], ["Athey", "Susan", ""], ["Imbens", "Guido W.", ""], ["Wooldridge", "Jeffrey M.", ""]]}, {"id": "1706.01846", "submitter": "Xin Wang", "authors": "Xin Wang and Vivekananda Roy", "title": "Convergence analysis of the block Gibbs sampler for Bayesian probit\n  linear mixed models with improper priors", "comments": "27 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider Markov chain Monte Carlo(MCMC) algorithms for\nexploring the intractable posterior density associated with Bayesian probit\nlinear mixed models under improper priors on the regression coefficients and\nvariance components. In particular, we construct the two-block Gibbs sampler\nusing the data augmentation (DA) techniques. Furthermore, we prove geometric\nergodicity of the Gibbs sampler, which is the foundation for building central\nlimit theorems for MCMC based estimators and subsequent inferences. The\nconditions for geometric convergence are similar to those guaranteeing\nposterior propriety. We also provide conditions for posterior propriety when\nthe design matrices take commonly observed forms. In general, the Haar\nparameter expansion for DA (PX- DA) algorithm is an improvement of the DA\nalgorithm and it has been shown that it is theoretically at least as good as\nthe DA algorithm. Here we construct a Haar PX-DA algorithm, which has\nessentially the same computational cost as the two-block Gibbs sampler.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 16:30:51 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:08:45 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 00:21:31 GMT"}, {"version": "v4", "created": "Wed, 18 Apr 2018 21:40:14 GMT"}, {"version": "v5", "created": "Thu, 20 Sep 2018 02:19:36 GMT"}, {"version": "v6", "created": "Wed, 21 Nov 2018 02:42:26 GMT"}, {"version": "v7", "created": "Thu, 22 Nov 2018 18:34:39 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wang", "Xin", ""], ["Roy", "Vivekananda", ""]]}, {"id": "1706.01852", "submitter": "Fan Yang", "authors": "Fan Yang, Rina Foygel Barber", "title": "Contraction and uniform convergence of isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of isotonic regression, where the underlying signal\n$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the\ncone $\\{ x\\in\\mathbb{R}^n : x_1 \\leq \\dots \\leq x_n\\}$. We study the isotonic\nprojection operator (projection to this cone), and find a necessary and\nsufficient condition characterizing all norms with respect to which this\nprojection is contractive. This enables a simple and non-asymptotic analysis of\nthe convergence properties of isotonic regression, yielding uniform confidence\nbands that adapt to the local Lipschitz properties of the signal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:00:58 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 20:13:03 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 14:35:16 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Yang", "Fan", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1706.01968", "submitter": "Johan Segers", "authors": "Axel B\\\"ucher, Johan Segers", "title": "Inference for heavy tailed stationary time series based on sliding\n  blocks", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block maxima method in extreme value theory consists of fitting an\nextreme value distribution to a sample of block maxima extracted from a time\nseries. Traditionally, the maxima are taken over disjoint blocks of\nobservations. Alternatively, the blocks can be chosen to slide through the\nobservation period, yielding a larger number of overlapping blocks. Inference\nbased on sliding blocks is found to be more efficient than inference based on\ndisjoint blocks. The asymptotic variance of the maximum likelihood estimator of\nthe Fr\\'{e}chet shape parameter is reduced by more than 18%. Interestingly, the\namount of the efficiency gain is the same whatever the serial dependence of the\nunderlying time series: as for disjoint blocks, the asymptotic distribution\ndepends on the serial dependence only through the sequence of scaling\nconstants. The findings are illustrated by simulation experiments and are\napplied to the estimation of high return levels of the daily log-returns of the\nStandard & Poor's 500 stock market index.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 20:46:56 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 20:53:42 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Segers", "Johan", ""]]}, {"id": "1706.02104", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov, Thomas Jaki and Mauro Gasparini", "title": "Loss Functions in Restricted Parameter Spaces and Their Bayesian\n  Applications", "comments": "29 pages, 11 figures, 5 tables, references start at page 23", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Squared error loss remains the most commonly used loss function for\nconstructing a Bayes estimator of the parameter of interest. However, it can\nlead to sub-optimal solutions when a parameter is defined in a restricted\nspace. It can also be an inappropriate choice in the context when an extreme\noverestimation and/or underestimation results in severe consequences and a more\nconservative estimator is preferred. We advocate a class of loss functions for\nparameters defined on restricted spaces which infinitely penalize boundary\ndecisions like the squared error loss does on the real line. We also recall\nseveral properties of loss functions such as symmetry, convexity, and\ninvariance. We propose generalizations of the squared error loss function for\nparameters defined on the positive real line and on an interval. We provide\nexplicit solutions for corresponding Bayes estimators and discuss multivariate\nextensions. {Four} well-known Bayesian estimation problems are used to\ndemonstrate inferential benefits the novel Bayes estimators can provide in the\ncontext of restricted estimation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 09:43:37 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 16:13:01 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 18:11:11 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""], ["Gasparini", "Mauro", ""]]}, {"id": "1706.02142", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock and Anthony Lee", "title": "Variance bounding of delayed-acceptance kernels", "comments": "Added: summary table and four figures demonstration theoretical\n  results in practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A delayed-acceptance version of a Metropolis--Hastings algorithm can be\nuseful for Bayesian inference when it is computationally expensive to calculate\nthe true posterior, but a computationally cheap approximation is available; the\ndelayed-acceptance kernel targets the same posterior as its associated \"parent\"\nMetropolis-Hastings kernel. Although the asymptotic variance of the ergodic\naverage of any functional of the chain cannot be less than that obtained using\nits parent, the average computational time per iteration can be much smaller\nand so for a given computational budget the delayed-acceptance kernel can be\nmore efficient.\n  When the asymptotic variance of the ergodic averages of all $L^2$ functionals\nof the chain is finite, the kernel is said to be variance bounding. It has\nrecently been noted that a delayed-acceptance kernel need not be variance\nbounding even when its parent is. We provide sufficient conditions for\ninheritance: for global algorithms, such as the independence sampler, the error\nin the approximation should be bounded; for local algorithms, two alternative\nsets of conditions are provided.\n  As a by-product of our initial, general result we also supply sufficient\nconditions on any pair of proposals such that, for any shared target\ndistribution, if a Metropolis-Hastings kernel using one of the proposals is\nvariance bounding then so is the Metropolis-Hastings kernel using the other\nproposal.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 12:07:20 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 13:10:07 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 15:12:43 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sherlock", "Chris", ""], ["Lee", "Anthony", ""]]}, {"id": "1706.02353", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Li Zhang, Ivan Mizera, Bei Jiang, and Linglong Kong", "title": "Sparse Wavelet Estimation in Quantile Regression with Multiple\n  Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study quantile regression in partial functional linear\nmodel where response is scalar and predictors include both scalars and multiple\nfunctions. Wavelet basis are adopted to better approximate functional slopes\nwhile effectively detect local features. The sparse group lasso penalty is\nimposed to select important functional predictors while capture shared\ninformation among them. The estimation problem can be reformulated into a\nstandard second-order cone program and then solved by an interior point method.\nWe also give a novel algorithm by using alternating direction method of\nmultipliers (ADMM) which was recently employed by many researchers in solving\npenalized quantile regression problems. The asymptotic properties such as the\nconvergence rate and prediction error bound have been established. Simulations\nand a real data from ADHD-200 fMRI data are investigated to show the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:28:53 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 23:34:51 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yu", "Dengdeng", ""], ["Zhang", "Li", ""], ["Mizera", "Ivan", ""], ["Jiang", "Bei", ""], ["Kong", "Linglong", ""]]}, {"id": "1706.02410", "submitter": "Qiyang Han", "authors": "Qiyang Han, Jon A. Wellner", "title": "Convergence rates of least squares regression estimators with\n  heavy-tailed errors", "comments": "50 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of the Least Squares Estimator (LSE) in a general\nnonparametric regression model, when the errors are independent of the\ncovariates but may only have a $p$-th moment ($p\\geq 1$). In such a\nheavy-tailed regression setting, we show that if the model satisfies a standard\n`entropy condition' with exponent $\\alpha \\in (0,2)$, then the $L_2$ loss of\nthe LSE converges at a rate \\begin{align*}\n\\mathcal{O}_{\\mathbf{P}}\\big(n^{-\\frac{1}{2+\\alpha}} \\vee\nn^{-\\frac{1}{2}+\\frac{1}{2p}}\\big). \\end{align*} Such a rate cannot be improved\nunder the entropy condition alone.\n  This rate quantifies both some positive and negative aspects of the LSE in a\nheavy-tailed regression setting. On the positive side, as long as the errors\nhave $p\\geq 1+2/\\alpha$ moments, the $L_2$ loss of the LSE converges at the\nsame rate as if the errors are Gaussian. On the negative side, if\n$p<1+2/\\alpha$, there are (many) hard models at any entropy level $\\alpha$ for\nwhich the $L_2$ loss of the LSE converges at a strictly slower rate than other\nrobust estimators.\n  The validity of the above rate relies crucially on the independence of the\ncovariates and the errors. In fact, the $L_2$ loss of the LSE can converge\narbitrarily slowly when the independence fails.\n  The key technical ingredient is a new multiplier inequality that gives sharp\nbounds for the `multiplier empirical process' associated with the LSE. We\nfurther give an application to the sparse linear regression model with\nheavy-tailed covariates and errors to demonstrate the scope of this new\ninequality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 23:25:29 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 03:57:17 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Han", "Qiyang", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1706.02420", "submitter": "Khalifa Es-Sebaiy", "authors": "Soukaina Douissi, Khalifa Es-Sebaiy and Frederi G. Viens", "title": "Berry-Ess\\'een bounds for parameter estimation of general Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study rates of convergence in central limit theorems for the partial sum\nof squares of general Gaussian sequences, using tools from analysis on Wiener\nspace. No assumption of stationarity, asymptotically or otherwise, is made. The\nmain theoretical tool is the so-called Optimal Fourth Moment Theorem\n\\cite{NP2015}, which provides a sharp quantitative estimate of the total\nvariation distance on Wiener chaos to the normal law. The only assumptions made\non the sequence are the existence of an asymptotic variance, that a\nleast-squares-type estimator for this variance parameter has a bias and a\nvariance which can be controlled, and that the sequence's auto-correlation\nfunction, which may exhibit long memory, has a no-worse memory than that of\nfractional Brownian motion with Hurst parameter }$H<3/4$.{\\ \\ Our main result\nis explicit, exhibiting the trade-off between bias, variance, and memory. We\napply our result to study drift parameter estimation problems for subfractional\nOrnstein-Uhlenbeck and bifractional Ornstein-Uhlenbeck processes with\nfixed-time-step observations. These are processes which fail to be stationary\nor self-similar, but for which detailed calculations result in explicit\nformulas for the estimators' asymptotic normality.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 00:50:05 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Douissi", "Soukaina", ""], ["Es-Sebaiy", "Khalifa", ""], ["Viens", "Frederi G.", ""]]}, {"id": "1706.02429", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Snigdhansu Chatterjee", "title": "Fast and General Model Selection using Data Depth and Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique using data depth functions and resampling to perform\nbest subset variable selection for a wide range of statistical models. We do\nthis by assigning a score, called an $e$-value, to a candidate model, and use a\nfast bootstrap method to approximate sample versions of these $e$-values. Under\ngeneral conditions, $e$-values can separate statistical models that adequately\nexplain properties of the data from those that do not. This results in a fast\nalgorithm that fits only a single model and evaluates $p +1$ models, $p$ being\nthe number of predictors under consideration, as opposed to the traditional\nrequirement of fitting and evaluating $2^{p}$ models. We illustrate in\nsimulation experiments that our proposed method typically performs better than\nan array of currently used methods for variable selection in linear models and\nfixed effect selection in linear mixed models. As a real data application, we\nuse our procedure to elicit climatic drivers of Indian summer monsoon\nprecipitation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 02:15:46 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 21:30:38 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:39:29 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1706.02563", "submitter": "Clara Grazian", "authors": "Clara Grazian and Christian P. Robert", "title": "Jeffreys priors for mixture estimation: properties and alternatives", "comments": "arXiv admin note: substantial text overlap with arXiv:1511.03145", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Jeffreys priors usually are well-defined for the parameters of mixtures\nof distributions, they are not available in closed form. Furthermore, they\noften are improper priors. Hence, they have never been used to draw inference\non the mixture parameters. The implementation and the properties of Jeffreys\npriors in several mixture settings are studied. It is shown that the associated\nposterior distributions most often are improper. Nevertheless, the Jeffreys\nprior for the mixture weights conditionally on the parameters of the mixture\ncomponents will be shown to have the property of conservativeness with respect\nto the number of components, in case of overfitted mixture and it can be\ntherefore used as a default priors in this context.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:42:09 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 11:06:53 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 14:28:35 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Grazian", "Clara", ""], ["Robert", "Christian P.", ""]]}, {"id": "1706.02592", "submitter": "Paavo Sattler", "authors": "Paavo Sattler and Markus Pauly", "title": "Inference For High-Dimensional Split-Plot-Designs: A Unified Approach\n  for Small to Large Numbers of Factor Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians increasingly face the problem to reconsider the adaptability of\nclassical inference techniques. In particular, divers types of high-dimensional\ndata structures are observed in various research areas; disclosing the\nboundaries of conventional multivariate data analysis. Such situations occur,\ne.g., frequently in life sciences whenever it is easier or cheaper to\nrepeatedly generate a large number $d$ of observations per subject than\nrecruiting many, say $N$, subjects. In this paper we discuss inference\nprocedures for such situations in general heteroscedastic split-plot designs\nwith $a$ independent groups of repeated measurements. These will, e.g., be able\nto answer questions about the occurrence of certain time, group and\ninteractions effects or about particular profiles.\n  The test procedures are based on standardized quadratic forms involving\nsuitably symmetrized U-statistics-type estimators which are robust against an\nincreasing number of dimensions $d$ and/or groups $a$. We then discuss its\nlimit distributions in a general asymptotic framework and additionally propose\nimproved small sample approximations. Finally its small sample performance is\ninvestigated in simulations and the applicability is illustrated by a real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 14:00:02 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Sattler", "Paavo", ""], ["Pauly", "Markus", ""]]}, {"id": "1706.02858", "submitter": "Farkhondeh Sajadi Dr.", "authors": "Farkhondeh Alsadat Sajadi and Rahul Roy", "title": "On rumour propagation among sceptics", "comments": "21Pages- To appear in the Journal of Statistical Physics", "journal-ref": null, "doi": "10.1007/s10955-018-2191-x", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Junior, Machado and Zuluaga (2011) studied a model to understand the spread\nof a rumour. Their model consists of individuals situated at the integer points\nof the line $\\N$. An individual at the origin $0$ starts a rumour and passes it\nto all individuals in the interval $[0,R_0]$, where $R_0$ is a non-negative\nrandom variable. An individual located at $i$ in this interval receives the\nrumour and transmits it further among individuals in $[i, i+R_i]$ where $R_0$\nand $R_i$ are i.i.d. random variables. The rumour spreads in this manner. An\nalternate model considers individuals seeking to find the rumour from\nindividuals who have already heard it. For this s/he asks individuals to the\nleft of her/him and lying in an interval of a random size. We study these two\nmodels, when the individuals are more sceptical and they transmit or accept the\nrumour only if they receive it from at least two different sources.\n  In stochastic geometry the equivalent of this rumour process is the study of\ncoverage of the space $\\N^d$ by random sets. Our study here extends the study\nof coverage of space and considers the case when each vertex of $\\N^d$ is\ncovered by at least two distinct random sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 07:44:06 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 04:50:42 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 08:28:18 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sajadi", "Farkhondeh Alsadat", ""], ["Roy", "Rahul", ""]]}, {"id": "1706.02930", "submitter": "Peter Cameron", "authors": "R. A. Bailey, Peter J. Cameron and Tomas Nilson", "title": "Sesqui-arrays, a generalisation of triple arrays", "comments": "Paper in memory of Anne Penfold Street -- accepted version for\n  Australasian Journal of Combinatorics", "journal-ref": "Australas. J. Combinatorics 71(3) (2018), 427-451", "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A triple array is a rectangular array containing letters, each letter\noccurring equally often with no repeats in rows or columns, such that the\nnumber of letters common to two rows, two columns, or a row and a column are\n(possibly different) non-zero constants. Deleting the condition on the letters\ncommon to a row and a column gives a double array. We propose the term\n\\emph{sesqui-array} for such an array when only the condition on pairs of\ncolumns is deleted. Thus all triple arrays are sesqui-arrays.\n  In this paper we give three constructions for sesqui-arrays. The first gives\n$(n+1)\\times n^2$ arrays on $n(n+1)$ letters for $n\\geq 2$. (Such an array for\n$n=2$ was found by Bagchi.) This construction uses Latin squares. The second\nuses the \\emph{Sylvester graph}, a subgraph of the Hoffman--Singleton graph, to\nbuild a good block design for $36$ treatments in $42$ blocks of size~$6$, and\nthen uses this in a $7\\times 36$ sesqui-array for $42$ letters.\n  We also give a construction for $K\\times(K-1)(K-2)/2$ sesqui-arrays on\n$K(K-1)/2$ letters. This construction uses biplanes. It starts with a block of\na biplane and produces an array which satisfies the requirements for a\nsesqui-array except possibly that of having no repeated letters in a row or\ncolumn. We show that this condition holds if and only if the \\emph{Hussain\nchains} for the selected block contain no $4$-cycles. A sufficient condition\nfor the construction to give a triple array is that each Hussain chain is a\nunion of $3$-cycles; but this condition is not necessary, and we give a few\nfurther examples.\n  We also discuss the question of which of these arrays provide good designs\nfor experiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 12:45:06 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 13:10:56 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bailey", "R. A.", ""], ["Cameron", "Peter J.", ""], ["Nilson", "Tomas", ""]]}, {"id": "1706.03029", "submitter": "Norbert Henze", "authors": "Norbert Henze, Mar\\'ia Dolores Jim\\'enez-Gamero, Simos G. Meintanis", "title": "Characterizations of multinormality and corresponding tests of fit,\n  including for Garch models", "comments": "38 pages, 4 tables, 61 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel characterizations of multivariate normality that incorporate\nboth the characteristic function and the moment generating function, and we\nemploy these results to construct a class of affine invariant, consistent and\neasy-to-use goodness-of-fit tests for normality. The test statistics are\nsuitably weighted $L^2$-statistics, and we provide their asymptotic behavior\nboth for i.i.d. observations as well as in the context of testing that the\ninnovation distribution of a multivariate GARCH model is Gaussian. We also\nstudy the finite-sample behavior of the new tests and compare the new criteria\nwith alternative existing tests.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:39:16 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Henze", "Norbert", ""], ["Jim\u00e9nez-Gamero", "Mar\u00eda Dolores", ""], ["Meintanis", "Simos G.", ""]]}, {"id": "1706.03113", "submitter": "Daren Wang", "authors": "Daren Wang, Xinyang Lu and Alessandro Rinaldo", "title": "DBSCAN: Optimal Rates For Density Based Clustering", "comments": "55 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal estimation of the density cluster tree under\nvarious assumptions on the underlying density. Building up from the seminal\nwork of Chaudhuri et al. [2014], we formulate a new notion of clustering\nconsistency which is better suited to smooth densities, and derive minimax\nrates of consistency for cluster tree estimation for Holder smooth densities of\narbitrary degree \\alpha. We present a computationally efficient, rate optimal\ncluster tree estimator based on a straightforward extension of the popular\ndensity-based clustering algorithm DBSCAN by Ester et al. [1996]. The procedure\nrelies on a kernel density estimator with an appropriate choice of the kernel\nand bandwidth to produce a sequence of nested random geometric graphs whose\nconnected components form a hierarchy of clusters. The resulting optimal rates\nfor cluster tree estimation depend on the degree of smoothness of the\nunderlying density and, interestingly, match minimax rates for density\nestimation under the supremum norm. Our results complement and extend the\nanalysis of the DBSCAN algorithm in Sriperumbudur and Steinwart [2012].\nFinally, we consider level set estimation and cluster consistency for densities\nwith jump discontinuities, where the sizes of the jumps and the distance among\nclusters are allowed to vanish as the sample size increases. We demonstrate\nthat our DBSCAN-based algorithm remains minimax rate optimal in this setting as\nwell.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:18:31 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 18:36:03 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 16:32:25 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wang", "Daren", ""], ["Lu", "Xinyang", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1706.03214", "submitter": "Lionel Truquet", "authors": "Lionel Truquet", "title": "A perturbation analysis of some Markov chains models with time-varying\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study some regularity properties in locally stationary Markov models which\nare fundamental for controlling the bias of nonparametric kernel estimators. In\nparticular, we provide an alternative to the standard notion of derivative\nprocess developed in the literature and that can be used for studying a wide\nclass of Markov processes. To this end, for some families of V-geometrically\nergodic Markov kernels indexed by a real parameter u, we give conditions under\nwhich the invariant probability distribution is differentiable with respect to\nu, in the sense of signed measures. Our results also complete the existing\nliterature for the perturbation analysis of Markov chains, in particular when\nexponential moments are not finite. Our conditions are checked on several\noriginal examples of locally stationary processes such as integer-valued\nautoregressive processes, categorical time series or threshold autoregressive\nprocesses.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 09:53:28 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 20:02:57 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Truquet", "Lionel", ""]]}, {"id": "1706.03400", "submitter": "Anthony Hou", "authors": "Jiajie Chen, Anthony Hou, Thomas Y. Hou", "title": "A Prototype Knockoff Filter for Group Selection with FDR Control", "comments": "16 pages, 5 figures", "journal-ref": "Information and Inference: A Journal of the IMA, iaz012 (2019)", "doi": "10.1093/imaiai/iaz012", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, we need to study a linear regression model that\nconsists of a response variable and a large number of potential explanatory\nvariables and determine which variables are truly associated with the response.\nIn 2015, Barber and Candes introduced a new variable selection procedure called\nthe knockoff filter to control the false discovery rate (FDR) and proved that\nthis method achieves exact FDR control. In this paper, we propose a prototype\nknockoff filter for group selection by extending the Reid-Tibshirani prototype\nmethod. Our prototype knockoff filter improves the computational efficiency and\nstatistical power of the Reid-Tibshirani prototype method when it is applied\nfor group selection. In some cases when the group features are spanned by one\nor a few hidden factors, we demonstrate that the PCA prototype knockoff filter\noutperforms the Dai-Barber group knockoff filter. We present several numerical\nexperiments to compare our prototype knockoff filter with the Reid-Tibshirani\nprototype method and the group knockoff filter. We have also conducted some\nanalysis of the knockoff filter. Our analysis reveals that some knockoff path\nmethod statistics, including the Lasso path statistic, may lead to loss of\npower for certain design matrices and a specially designed response even if\ntheir signal strengths are still relatively strong.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 19:56:06 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 04:15:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Jiajie", ""], ["Hou", "Anthony", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "1706.03418", "submitter": "Randolf Altmeyer", "authors": "Randolf Altmeyer", "title": "Approximation of occupation time functionals", "comments": "Revised and corrected version. Changed title", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong $L^2$-approximation of occupation time functionals is studied with\nrespect to discrete observations of a $d$-dimensional c\\`adl\\`ag process. Upper\nbounds on the error are obtained under weak assumptions, generalizing previous\nresults in the literature considerably. The approach relies on regularity for\nthe marginals of the process and applies also to non-Markovian processes, such\nas fractional Brownian motion. The results are used to approximate occupation\ntimes and local times. For Brownian motion, the upper bounds are shown to be\nsharp up to a log-factor.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 22:53:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 12:57:11 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 13:19:30 GMT"}, {"version": "v4", "created": "Sun, 31 Jan 2021 23:53:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Altmeyer", "Randolf", ""]]}, {"id": "1706.03461", "submitter": "S\\\"oren R. K\\\"unzel", "authors": "S\\\"oren R. K\\\"unzel, Jasjeet S. Sekhon, Peter J. Bickel, Bin Yu", "title": "Meta-learners for Estimating Heterogeneous Treatment Effects using\n  Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1804597116", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in estimating and analyzing heterogeneous treatment\neffects in experimental and observational studies. We describe a number of\nmeta-algorithms that can take advantage of any supervised learning or\nregression method in machine learning and statistics to estimate the\nConditional Average Treatment Effect (CATE) function. Meta-algorithms build on\nbase algorithms---such as Random Forests (RF), Bayesian Additive Regression\nTrees (BART) or neural networks---to estimate the CATE, a function that the\nbase algorithms are not designed to estimate directly. We introduce a new\nmeta-algorithm, the X-learner, that is provably efficient when the number of\nunits in one treatment group is much larger than in the other, and can exploit\nstructural properties of the CATE function. For example, if the CATE function\nis linear and the response functions in treatment and control are Lipschitz\ncontinuous, the X-learner can still achieve the parametric rate under\nregularity conditions. We then introduce versions of the X-learner that use RF\nand BART as base learners. In extensive simulation studies, the X-learner\nperforms favorably, although none of the meta-learners is uniformly the best.\nIn two persuasion field experiments from political science, we demonstrate how\nour new X-learner can be used to target treatment regimes and to shed light on\nunderlying mechanisms. A software package is provided that implements our\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:10:09 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 16:44:35 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 07:18:59 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 02:13:30 GMT"}, {"version": "v5", "created": "Fri, 28 Sep 2018 04:31:26 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2019 03:01:02 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Sekhon", "Jasjeet S.", ""], ["Bickel", "Peter J.", ""], ["Yu", "Bin", ""]]}, {"id": "1706.03490", "submitter": "Martin Emil Jakobsen", "authors": "Martin Emil Jakobsen", "title": "Distance Covariance in Metric Spaces: Non-Parametric Independence\n  Testing in Metric Spaces (Master's thesis)", "comments": "Master's thesis, 160 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this thesis is to find a solution to the non-parametric\nindependence problem in separable metric spaces. Suppose we are given finite\ncollection of samples from an i.i.d. sequence of paired random elements, where\neach marginal has values in some separable metric space. The non-parametric\nindependence problem raises the question on how one can use these samples to\nreasonably draw inference on whether the marginal random elements are\nindependent or not. We will try to answer this question by utilizing the\nso-called distance covariance functional in metric spaces developed by Russell\nLyons. We show that, if the marginal spaces are so-called metric spaces of\nstrong negative type (e.g. seperable Hilbert spaces), then the distance\ncovariance functional becomes a direct indicator of independence. That is, one\ncan directly determine whether the marginals are independent or not based\nsolely on the value of this functional. As the functional formally takes the\nsimultaneous distribution as argument, its value is not known in the posed\nnon-parametric independence problem. Hence, we construct estimators of the\ndistance covariance functional, and show that they exhibit asymptotic\nproperties which can be used to construct asymptotically consistent statistical\ntests of independence. Finally, as the rejection thresholds of these\nstatistical tests are non-traceable we argue that they can be reasonably\nbootstrapped.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 07:16:12 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Jakobsen", "Martin Emil", ""]]}, {"id": "1706.03537", "submitter": "Elsa Dupraz", "authors": "Dominique Pastor, Elsa Dupraz, Fran\\c{c}ois-Xavier Socheleau", "title": "Decentralized Clustering based on Robust Estimation and Hypothesis\n  Testing", "comments": "35 pages, 10 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a network of sensors without fusion center that may be\ndifficult to set up in applications involving sensors embedded on autonomous\ndrones or robots. In this context, this paper considers that the sensors must\nperform a given clustering task in a fully decentralized setup. Standard\nclustering algorithms usually need to know the number of clusters and are very\nsensitive to initialization, which makes them difficult to use in a fully\ndecentralized setup. In this respect, this paper proposes a decentralized\nmodel-based clustering algorithm that overcomes these issues. The proposed\nalgorithm is based on a novel theoretical framework that relies on hypothesis\ntesting and robust M-estimation. More particularly, the problem of deciding\nwhether two data belong to the same cluster can be optimally solved via Wald's\nhypothesis test on the mean of a Gaussian random vector. The p-value of this\ntest makes it possible to define a new type of score function, particularly\nsuitable for devising an M-estimation of the centroids. The resulting\ndecentralized algorithm efficiently performs clustering without prior knowledge\nof the number of clusters. It also turns out to be less sensitive to\ninitialization than the already existing clustering algorithms, which makes it\nappropriate for use in a network of sensors without fusion center.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 09:45:15 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Pastor", "Dominique", ""], ["Dupraz", "Elsa", ""], ["Socheleau", "Fran\u00e7ois-Xavier", ""]]}, {"id": "1706.03559", "submitter": "Marco Singer", "authors": "Marco Singer, Tatyana Krivobokova, Axel Munk", "title": "Kernel partial least squares for stationary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the kernel partial least squares algorithm for non-parametric\nregression with stationary dependent data. Probabilistic convergence rates of\nthe kernel partial least squares estimator to the true regression function are\nestablished under a source and an effective dimensionality condition. It is\nshown both theoretically and in simulations that long range dependence results\nin slower convergence rates. A protein dynamics example shows high predictive\npower of kernel partial least squares.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:00:47 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Singer", "Marco", ""], ["Krivobokova", "Tatyana", ""], ["Munk", "Axel", ""]]}, {"id": "1706.03678", "submitter": "Stephen Page", "authors": "Stephen Page and Steffen Gr\\\"unew\\\"alder", "title": "Ivanov-Regularised Least-Squares Estimators over Large RKHSs and Their\n  Interpolation Spaces", "comments": "49 pages, submitted to The Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study kernel least-squares estimation under a norm constraint. This form\nof regularisation is known as Ivanov regularisation and it provides better\ncontrol of the norm of the estimator than the well-established Tikhonov\nregularisation. Ivanov regularisation can be studied under minimal assumptions.\nIn particular, we assume only that the RKHS is separable with a bounded and\nmeasurable kernel. We provide rates of convergence for the expected squared\n$L^2$ error of our estimator under the weak assumption that the variance of the\nresponse variables is bounded and the unknown regression function lies in an\ninterpolation space between $L^2$ and the RKHS. We then obtain faster rates of\nconvergence when the regression function is bounded by clipping the estimator.\nIn fact, we attain the optimal rate of convergence. Furthermore, we provide a\nhigh-probability bound under the stronger assumption that the response\nvariables have subgaussian errors and that the regression function lies in an\ninterpolation space between $L^\\infty$ and the RKHS. Finally, we derive\nadaptive results for the settings in which the regression function is bounded.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:03:42 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 17:18:04 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 17:28:10 GMT"}, {"version": "v4", "created": "Fri, 14 Jun 2019 15:07:20 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Page", "Stephen", ""], ["Gr\u00fcnew\u00e4lder", "Steffen", ""]]}, {"id": "1706.03862", "submitter": "Thiago VedoVatto", "authors": "Thiago VedoVatto and Abraao David Costa do Nascimento", "title": "Closed-form mathematical expressions for the exponentiated\n  Cauchy-Rayleigh distribution", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cauchy-Rayleigh (CR) distribution has been successfully used to describe\nasymmetric and heavy-tail events from radar imagery. Employing such model to\ndescribe lifetime data may then seem attractive, but some drawbacks arise: its\nprobability density function does not cover non-modal behavior as well as the\nCR hazard rate function (hrf) assumes only one form. To outperform this\ndifficulty, we introduce an extended CR model, called exponentiated\nCauchy-Rayleigh (ECR) distribution. This model has two parameters and hrf with\ndecreasing, decreasing-increasing-decreasing and upside-down bathtub forms. In\nthis paper, several closed-form mathematical expressions for the ECR model are\nproposed: median, mode, probability weighted, log-, incomplete and order\nstatistic moments and Fisher information matrix. We propose three estimation\nprocedures for the ECR parameters: maximum likelihood (ML), bias corrected ML\nand percentile-based methods. A simulation study is done to assess the\nperformance of estimators. An application to survival time of heart problem\npatients illustrates the usefulness of the ECR model. Results point out that\nthe ECR distribution may outperform classical lifetime models, such as the\ngamma, Birnbaun-Saunders, Weibull and log-normal laws, before heavy-tail data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:54:51 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["VedoVatto", "Thiago", ""], ["Nascimento", "Abraao David Costa do", ""]]}, {"id": "1706.03955", "submitter": "Agust\\'in G. Nogales", "authors": "A.G. Nogales and P. P\\'erez", "title": "A Note on the Relationship Between Conditional and Unconditional\n  Independence, and its Extensions for Markov Kernels", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two known results on the relationship between conditional and unconditional\nindependence are obtained as a consequence of the main result of this paper, a\ntheorem that uses independence of Markov kernels to obtain a minimal condition\nwhich added to conditional independence implies independence. Some\ncounterexamples and representation results are provided to clarify the concepts\nintroduced and the propositions of the statement of the main theorem. Moreover,\nconditional independence and the mentioned results are extended to the\nframework of Markov kernels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:34:43 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 12:47:56 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Nogales", "A. G.", ""], ["P\u00e9rez", "P.", ""]]}, {"id": "1706.04059", "submitter": "Yohann De Castro", "authors": "Yohann De Castro, Fabrice Gamboa, Didier Henrion, Roxana Hess,\n  Jean-Bernard Lasserre", "title": "Approximate Optimal Designs for Multivariate Polynomial Regression", "comments": "30 Pages, 8 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1703.01777", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.NA stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach aiming at computing approximate optimal designs\nfor multivariate polynomial regressions on compact (semi-algebraic) design\nspaces. We use the moment-sum-of-squares hierarchy of semidefinite programming\nproblems to solve numerically the approximate optimal design problem. The\ngeometry of the design is recovered via semidefinite programming duality\ntheory. This article shows that the hierarchy converges to the approximate\noptimal design as the order of the hierarchy increases. Furthermore, we provide\na dual certificate ensuring finite convergence of the hierarchy and showing\nthat the approximate optimal design can be computed numerically with our\nmethod. As a byproduct, we revisit the equivalence theorem of the experimental\ndesign theory: it is linked to the Christoffel polynomial and it characterizes\nfinite convergence of the moment-sum-of-square hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:19:36 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 13:03:59 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 12:50:56 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["De Castro", "Yohann", ""], ["Gamboa", "Fabrice", ""], ["Henrion", "Didier", ""], ["Hess", "Roxana", ""], ["Lasserre", "Jean-Bernard", ""]]}, {"id": "1706.04081", "submitter": "Giuseppe Notarstefano", "authors": "Francesco Sasso and Angelo Coluccia and Giuseppe Notarstefano", "title": "Interaction-Based Distributed Learning in Cyber-Physical and Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a network scenario in which agents can evaluate\neach other according to a score graph that models some physical or social\ninteraction. The goal is to design a distributed protocol, run by the agents,\nallowing them to learn their unknown state among a finite set of possible\nvalues. We propose a Bayesian framework in which scores and states are\nassociated to probabilistic events with unknown parameters and hyperparameters\nrespectively. We prove that each agent can learn its state by means of a local\nBayesian classifier and a (centralized) Maximum-Likelihood (ML) estimator of\nthe parameter-hyperparameter that combines plain ML and Empirical Bayes\napproaches. By using tools from graphical models, which allow us to gain\ninsight on conditional dependences of scores and states, we provide two relaxed\nprobabilistic models that ultimately lead to ML parameter-hyperparameter\nestimators amenable to distributed computation. In order to highlight the\nappropriateness of the proposed relaxations, we demonstrate the distributed\nestimators on a machine-to-machine testing set-up for anomaly detection and on\na social interaction set-up for user profiling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 14:06:41 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Sasso", "Francesco", ""], ["Coluccia", "Angelo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1706.04276", "submitter": "Billy Fang", "authors": "Billy Fang, Adityanand Guntuboyina", "title": "On the risk of convex-constrained least squares estimators under\n  misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean of a noisy vector. When the\nmean lies in a convex constraint set, the least squares projection of the\nrandom vector onto the set is a natural estimator. Properties of the risk of\nthis estimator, such as its asymptotic behavior as the noise tends to zero,\nhave been well studied. We instead study the behavior of this estimator under\nmisspecification, that is, without the assumption that the mean lies in the\nconstraint set. For appropriately defined notions of risk in the misspecified\nsetting, we prove a generalization of a low noise characterization of the risk\ndue to Oymak and Hassibi in the case of a polyhedral constraint set. An\ninteresting consequence of our results is that the risk can be much smaller in\nthe misspecified setting than in the well-specified setting. We also discuss\nconsequences of our result for isotonic regression.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 23:07:54 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Fang", "Billy", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "1706.04410", "submitter": "Ramji Venkataramanan", "authors": "Ramji Venkataramanan, Oliver Johnson", "title": "A strong converse bound for multiple hypothesis testing, with\n  applications to high-dimensional estimation", "comments": "In the latest version, the value of $\\lambda$ in the statements of\n  Lemma 4.1 and Proposition 4.2 is restricted to the interval $(0,1]$. This is\n  the correct condition, rather than $\\lambda>0$ stated in the journal version\n  below", "journal-ref": "Electronic Journal of Statistics, Vol. 12, No. 1, pp. 1126-1149,\n  2018", "doi": "10.1214/18-EJS1419", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical inference problems, we wish to obtain lower bounds on the\nminimax risk, that is to bound the performance of any possible estimator. A\nstandard technique to obtain risk lower bounds involves the use of Fano's\ninequality. In an information-theoretic setting, it is known that Fano's\ninequality typically does not give a sharp converse result (error lower bound)\nfor channel coding problems. Moreover, recent work has shown that an argument\nbased on binary hypothesis testing gives tighter results. We adapt this\ntechnique to the statistical setting, and argue that Fano's inequality can\nalways be replaced by this approach to obtain tighter lower bounds that can be\neasily computed and are asymptotically sharp. We illustrate our technique in\nthree applications: density estimation, active learning of a binary classifier,\nand compressed sensing, obtaining tighter risk lower bounds in each case.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 11:21:02 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 16:47:33 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 18:54:51 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Venkataramanan", "Ramji", ""], ["Johnson", "Oliver", ""]]}, {"id": "1706.04416", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi, Helene Massam, Gerard Letac", "title": "Accelerating Bayesian Structure Learning in Sparse Gaussian Graphical\n  Models", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are relevant tools to learn conditional\nindependence structure between variables. In this class of models, Bayesian\nstructure learning is often done by search algorithms over the graph space. The\nconjugate prior for the precision matrix satisfying graphical constraints is\nthe well-known G-Wishart. With this prior, the transition probabilities in the\nsearch algorithms necessitate evaluating the ratios of the prior normalizing\nconstants of G-Wishart. In moderate to high-dimensions, this ratio is often\napproximated using sampling-based methods as computationally expensive updates\nin the search algorithm. Calculating this ratio so far has been a major\ncomputational bottleneck. We overcome this issue by representing a search\nalgorithm in which the ratio of normalizing constant is carried out by an\nexplicit closed-form approximation. Using this approximation within our search\nalgorithm yields significant improvement in the scalability of structure\nlearning without sacrificing structure learning accuracy. We study the\nconditions under which the approximation is valid. We also evaluate the\nefficacy of our method with simulation studies. We show that the new search\nalgorithm with our approximation outperforms state-of-the-art methods in both\ncomputational efficiency and accuracy. The implementation of our work is\navailable in the R package BDgraph.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 11:41:06 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 12:26:11 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 14:38:50 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mohammadi", "Reza", ""], ["Massam", "Helene", ""], ["Letac", "Gerard", ""]]}, {"id": "1706.04668", "submitter": "Tobias Zwingmann", "authors": "Tobias Zwingmann and Hajo Holzmann", "title": "Weak convergence of quantile and expectile processes under general\n  assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show weak convergence of quantile and expectile processes to Gaussian\nlimit processes in the space of bounded functions endowed with an appropriate\nsemimetric which is based on the concepts of epi- and hypo convergence as\nintroduced in \\citet{buecher2014}. We impose assumptions for which it is known\nthat weak convergence with respect to the supremum norm or the Skorodhod metric\ngenerally fails to hold. For expectiles, we only require a distribution with\nfinite second moment but no further smoothness properties of distribution\nfunction, for quantiles, the distribution is assumed to be absolutely\ncontinuous with a version of its Lebesgue density which is strictly positive\nand has left- and right-sided limits. We also show consistency of the bootstrap\nfor this mode of convergence.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 21:06:04 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Zwingmann", "Tobias", ""], ["Holzmann", "Hajo", ""]]}, {"id": "1706.04677", "submitter": "Emmanuel Candes", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Gene Hunting with Knockoffs for Hidden Markov Models", "comments": "35 pages, 13 figues, 9 tables", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 1-18", "doi": "10.1093/biomet/asy033", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific studies often require the identification of a subset of\nrelevant explanatory variables, in the attempt to understand an interesting\nphenomenon. Several statistical methods have been developed to automate this\ntask, but only recently has the framework of model-free knockoffs proposed a\ngeneral solution that can perform variable selection under rigorous type-I\nerror control, without relying on strong modeling assumptions. In this paper,\nwe extend the methodology of model-free knockoffs to a rich family of problems\nwhere the distribution of the covariates can be described by a hidden Markov\nmodel (HMM). We develop an exact and efficient algorithm to sample knockoff\ncopies of an HMM. We then argue that combined with the knockoffs selective\nframework, they provide a natural and powerful tool for performing principled\ninference in genome-wide association studies with guaranteed FDR control.\nFinally, we apply our methodology to several datasets aimed at studying the\nCrohn's disease and several continuous phenotypes, e.g. levels of cholesterol.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 21:42:12 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1706.04729", "submitter": "Yao Xie", "authors": "Liyan Xie, Yao Xie", "title": "Sequential detection of low-rank changes using extreme eigenvalues", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting an abrupt change to the signal covariance\nmatrix. In particular, the covariance changes from a \"white\" identity matrix to\nan unknown spiked or low-rank matrix. Two sequential change-point detection\nprocedures are presented, based on the largest and the smallest eigenvalues of\nthe sample covariance matrix. To control false-alarm-rate, we present an\naccurate theoretical approximation to the average-run-length (ARL) and expected\ndetection delay (EDD) of the detection, leveraging the extreme eigenvalue\ndistributions from random matrix theory and by capturing a non-negligible\ntemporal correlation in the sequence of scan statistics due to the sliding\nwindow approach. Real data examples demonstrate the good performance of our\nmethod for detecting behavior change of a swarm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 03:42:02 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Xie", "Liyan", ""], ["Xie", "Yao", ""]]}, {"id": "1706.04857", "submitter": "Philip Dawid", "authors": "Rossella Murtas, Alexander Philip Dawid, Monica Musio", "title": "New bounds for the Probability of Causation in Mediation Analysis", "comments": "8 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individual has been subjected to some exposure and has developed some\noutcome. Using data on similar individuals, we wish to evaluate, for this case,\nthe probability that the outcome was in fact caused by the exposure. Even with\nthe best possible experimental data on exposure and outcome, we typically can\nnot identify this \"probability of causation\" exactly, but we can provide\ninformation in the form of bounds for it. Under appropriate assumptions, these\nbounds can be tightened if we can make other observations (e.g., on\nnon-experimental cases), measure additional variables (e.g., covariates) or\nmeasure complete mediators. In this work we propose new bounds for the case\nthat a third variable mediates partially the effect of the exposure on the\noutcome.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 13:19:11 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Murtas", "Rossella", ""], ["Dawid", "Alexander Philip", ""], ["Musio", "Monica", ""]]}, {"id": "1706.05173", "submitter": "Eni Musta", "authors": "Hendrik P. Lopuha\\\"a and Eni Musta", "title": "The distance between a naive cumulative estimator and its least concave\n  majorant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the process $\\widehat\\Lambda_n-\\Lambda_n$, where $\\Lambda_n$ is a\ncadlag step estimator for the primitive $\\Lambda$ of a nonincreasing function\n$\\lambda$ on $[0,1]$, and $\\widehat\\Lambda_n$ is the least concave majorant of\n$\\Lambda_n$. We extend the results in Kulikov and Lopuha\\\"a (2006, 2008) to the\ngeneral setting considered in Durot (2007). Under this setting we prove that a\nsuitably scaled version of $\\widehat\\Lambda_n-\\Lambda_n$ converges in\ndistribution to the corresponding process for two-sided Brownian motion with\nparabolic drift and we establish a central limit theorem for the $L_p$-distance\nbetween $\\widehat\\Lambda_n$ and $\\Lambda_n$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 08:06:37 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 13:18:50 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Lopuha\u00e4", "Hendrik P.", ""], ["Musta", "Eni", ""]]}, {"id": "1706.05331", "submitter": "Yao Xie", "authors": "Junzhuo Chen, Seong-Hee Kim and Yao Xie", "title": "$\\textsf{S}^3T$: An Efficient Score-Statistic for Spatio-Temporal\n  Surveillance", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient score statistic, called the $\\textsf{S}^3 \\textsf{T}$\nstatistic, to detect the emergence of a spatially and temporally correlated\nsignal from either fixed-sample or sequential data. The signal may cause a men\nshift and/or a change in the covariance structure. The score statistic can\ncapture both spatial and temporal structures of the change and hence is\nparticularly powerful in detecting weak signals. The score statistic is\ncomputationally efficient and statistically powerful. Our main theoretical\ncontribution are accurate analytical approximations on the false alarm rate of\nthe detection procedures, which can be used to calibrate the threshold\nanalytically. Numerical experiments on simulated and real data demonstrate the\ngood performance of our procedure for solar flame detection and water quality\nmonitoring.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 16:07:00 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 03:25:50 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 15:26:21 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Chen", "Junzhuo", ""], ["Kim", "Seong-Hee", ""], ["Xie", "Yao", ""]]}, {"id": "1706.05486", "submitter": "Lingfei Li", "authors": "Weiwei Guo, Lingfei Li", "title": "Parametric Inference for Discretely Observed Subordinate Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subordinate diffusions are constructed by time changing diffusion processes\nwith an independent L\\'{e}vy subordinator. This is a rich family of Markovian\njump processes which exhibit a variety of jump behavior and have found many\napplications. This paper studies parametric inference of discretely observed\nergodic subordinate diffusions. We solve the identifiability problem for these\nprocesses using spectral theory and propose a two-step estimation procedure\nbased on estimating functions. In the first step, we use an estimating function\nthat only involves diffusion parameters. In the second step, a martingale\nestimating function based on eigenvalues and eigenfunctions of the subordinate\ndiffusion is used to estimate the parameters of the L\\'{e}vy subordinator and\nthe problem of how to choose the weighting matrix is solved. When the\neigenpairs do not have analytical expressions, we apply the constant\nperturbation method with high order corrections to calculate them numerically\nand the martingale estimating function can be computed efficiently. Consistency\nand asymptotic normality of our estimator are established considering the\neffect of numerical approximation. Through numerical examples, we show that our\nmethod is both computationally and statistically efficient. A subordinate\ndiffusion model for VIX (CBOE volatility index) is developed which provides\ngood fit to the data.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 07:51:05 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 08:47:49 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Guo", "Weiwei", ""], ["Li", "Lingfei", ""]]}, {"id": "1706.05499", "submitter": "Chuancun Yin", "authors": "Chuancun Yin, Dan Zhu", "title": "Joint Mixability of Elliptical Distributions and Related Families", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we further develop the theory of complete mixability and joint\nmixability for some distribution families. We generalize a result of\nR\\\"uschendorf and Uckelmann (2002) related to complete mixability of continuous\ndistribution function having a symmetric and unimodal density. Two different\nproofs to a result of Wang and Wang (2016) which related to the joint\nmixability of elliptical distributions with the same characteristic generator\nare present. We solve the Open Problem 7 in Wang (2015) by constructing a\nbimodal-symmetric distribution. The joint mixability of slash-elliptical\ndistributions and skew-elliptical distributions is studied and the extension to\nmultivariate distributions is also investigated.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 09:18:04 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 02:40:08 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 08:52:05 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Yin", "Chuancun", ""], ["Zhu", "Dan", ""]]}, {"id": "1706.05510", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Statistical foundations for assessing the difference between the\n  classical and weighted-Gini betas", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The `beta' is one of the key quantities in the capital asset pricing model\n(CAPM). In statistical language, the beta can be viewed as the slope of the\nregression line fitted to financial returns on the market against the returns\non the asset under consideration. The insurance counterpart of CAPM, called the\nweighted insurance pricing model (WIPM), gives rise to the so-called\nweighted-Gini beta. The aforementioned two betas may or may not coincide,\ndepending on the form of the underlying regression function, and this has\nprofound implications when designing portfolios and allocating risk capital. To\nfacilitate these tasks, in this paper we develop large-sample statistical\ninference results that, in a straightforward fashion, imply confidence\nintervals for, and hypothesis tests about, the equality of the two betas.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 10:19:39 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 14:17:02 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 17:33:17 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1706.05566", "submitter": "Philip Dawid", "authors": "Philip Dawid, Monica Musio, Rossella Murtas", "title": "The Probability of Causation", "comments": "14 pages, 2 tables, 7 figures", "journal-ref": "Law, Probability and Risk (2017) 16, 163-179", "doi": "10.1093/lpr/mgx012", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many legal cases require decisions about causality, responsibility or blame,\nand these may be based on statistical data. However, causal inferences from\nsuch data are beset by subtle conceptual and practical difficulties, and in\ngeneral it is, at best, possible to identify the \"probability of causation\" as\nlying between certain empirically informed limits. These limits can be refined\nand improved if we can obtain additional information, from statistical or\nscientific data, relating to the internal workings of the causal processes. In\nthis paper we review and extend recent work in this area, where additional\ninformation may be available on covariate and/or mediating variables.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 17:41:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "Philip", ""], ["Musio", "Monica", ""], ["Murtas", "Rossella", ""]]}, {"id": "1706.05576", "submitter": "Youri Davydov", "authors": "Youri Davydov, Francesca Greselin", "title": "Inferential results for a new measure of inequality", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive inferential results for a new index of inequality,\nspecifically defined for capturing significant changes observed both in the\nleft and in the right tail of the income distributions. The latter shifts are\nan apparent fact for many countries like US, Germany, UK, and France in the\nlast decades, and are a concern for many policy makers. We propose two\nempirical estimators for the index, and show that they are asymptotically\nequivalent. Afterwards, we adopt one estimator and prove its consistency and\nasymptotic normality. Finally we introduce an empirical estimator for its\nvariance and provide conditions to show its convergence to the finite\ntheoretical value. An analysis of real data on net income from the Bank of\nItaly Survey of Income and Wealth is also presented, on the base of the\nobtained inferential results.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 19:38:01 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Davydov", "Youri", ""], ["Greselin", "Francesca", ""]]}, {"id": "1706.05738", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Alistair Stewart", "title": "Fourier-Based Testing for Families of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the general problem of testing whether an unknown distribution\nbelongs to a specified family of distributions. More specifically, given a\ndistribution family $\\mathcal{P}$ and sample access to an unknown discrete\ndistribution $\\mathbf{P}$, we want to distinguish (with high probability)\nbetween the case that $\\mathbf{P} \\in \\mathcal{P}$ and the case that\n$\\mathbf{P}$ is $\\epsilon$-far, in total variation distance, from every\ndistribution in $\\mathcal{P}$. This is the prototypical hypothesis testing\nproblem that has received significant attention in statistics and, more\nrecently, in theoretical computer science.\n  The sample complexity of this general inference task depends on the\nunderlying family $\\mathcal{P}$. The gold standard in distribution property\ntesting is to design sample-optimal and computationally efficient algorithms\nfor this task. The main contribution of this work is a simple and general\ntesting technique that is applicable to all distribution families whose Fourier\nspectrum satisfies a certain approximate sparsity property. To the best of our\nknowledge, ours is the first use of the Fourier transform in the context of\ndistribution testing.\n  We apply our Fourier-based framework to obtain near sample-optimal and\ncomputationally efficient testers for the following fundamental distribution\nfamilies: Sums of Independent Integer Random Variables (SIIRVs), Poisson\nMultinomial Distributions (PMDs), and Discrete Log-Concave Distributions. For\nthe first two, ours are the first non-trivial testers in the literature, vastly\ngeneralizing previous work on testing Poisson Binomial Distributions. For the\nthird, our tester improves on prior work in both sample and time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 22:28:20 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 03:05:23 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Stewart", "Alistair", ""]]}, {"id": "1706.05800", "submitter": "Muneya Matsui", "authors": "Ewa Damek, Muneya Matsui and Witold \\'Swi\\k{a}tkowski", "title": "Componentwise different tail solutions for bivariate stochastic\n  recurrence equations -- with application to GARCH(1,1) processes --", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study bivariate stochastic recurrence equations (SREs) motivated by\napplications to GARCH(1,1) processes. If coefficient matrices of SREs have\nstrictly positive entries, then the Kesten result applies and it gives\nsolutions with regularly varying tails. Moreover, the tail indices are the same\nfor all coordinates. However, for applications, this framework is too\nrestrictive. We study SREs when coefficients are triangular matrices and prove\nthat the coordinates of the solution may exhibit regularly varying tails with\ndifferent indices. We also specify each tail index together with its constant.\nThe results are used to characterize regular variations of bivariate stationary\nGARCH(1,1) processes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 06:35:00 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Damek", "Ewa", ""], ["Matsui", "Muneya", ""], ["\u015awi\u0105tkowski", "Witold", ""]]}, {"id": "1706.05940", "submitter": "Samuel Perreault", "authors": "Samuel Perreault, Thierry Duchesne and Johanna G. Ne\\v{s}lehov\\'a", "title": "Detection of Block-Exchangeable Structure in Large-Scale Correlation\n  Matrices", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2018.10.009", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation matrices are omnipresent in multivariate data analysis. When the\nnumber d of variables is large, the sample estimates of correlation matrices\nare typically noisy and conceal underlying dependence patterns. We consider the\ncase when the variables can be grouped into K clusters with exchangeable\ndependence; this assumption is often made in applications, e.g., in finance and\neconometrics. Under this partial exchangeability condition, the corresponding\ncorrelation matrix has a block structure and the number of unknown parameters\nis reduced from d(d-1)/2 to at most K(K+1)/2. We propose a robust algorithm\nbased on Kendall's rank correlation to identify the clusters without assuming\nthe knowledge of K a priori or anything about the margins except continuity.\nThe corresponding block-structured estimator performs considerably better than\nthe sample Kendall rank correlation matrix when K < d. The new estimator can\nalso be much more efficient in finite samples even in the unstructured case K =\nd, although there is no gain asymptotically. When the distribution of the data\nis elliptical, the results extend to linear correlation matrices and their\ninverses. The procedure is illustrated on financial stock returns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 13:45:21 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 16:39:19 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 15:01:54 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Perreault", "Samuel", ""], ["Duchesne", "Thierry", ""], ["Ne\u0161lehov\u00e1", "Johanna G.", ""]]}, {"id": "1706.06006", "submitter": "Ville Satopaa", "authors": "Ville A. Satop\\\"a\\\"a", "title": "Combining Information from Multiple Forecasters: Inefficiency of Central\n  Tendency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the forecasting literature agrees that aggregating multiple\npredictions of some future outcome typically outperforms the individual\npredictions, there is no general consensus about the right way to do this. Most\ncommon aggregators are means, defined loosely as aggregators that always remain\nbetween the smallest and largest predictions. Examples include the arithmetic\nmean, trimmed means, median, mid-range, and many other measures of central\ntendency. If the forecasters use different information, the aggregator ideally\ncombines their information into a consensus without losing or distorting any of\nit. An aggregator that achieves this is considered efficient. Unfortunately,\nour results show that if the forecasters use their information accurately, an\naggregator that always remains strictly between the smallest and largest\npredictions is never efficient in practice. A similar result holds even if the\nideal predictions are distorted with random error that is centered at zero. If\nthese noisy predictions are aggregated with a similar notion of centrality,\nthen, under some mild conditions, the aggregator is asymptotically inefficient.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:18:54 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 17:49:43 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Satop\u00e4\u00e4", "Ville A.", ""]]}, {"id": "1706.06182", "submitter": "Nevena Maric", "authors": "Mark Huber and Nevena Maric", "title": "Bernoulli Correlations and Cut Polytopes", "comments": "15 pages; minor changes compared to the previous version, mostly\n  stylistic", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.MG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ symmetric Bernoulli variables, what can be said about their\ncorrelation matrix viewed as a vector? We show that the set of those vectors\n$R(\\mathcal{B}_n)$ is a polytope and identify its vertices. Those extreme\npoints correspond to correlation vectors associated to the discrete uniform\ndistributions on diagonals of the cube $[0,1]^n$. We also show that the\npolytope is affinely isomorphic to a well-known cut polytope ${\\rm CUT}(n)$\nwhich is defined as a convex hull of the cut vectors in a complete graph with\nvertex set $\\{1,\\ldots,n\\}$. The isomorphism is obtained explicitly as\n$R(\\mathcal{B}_n)= {\\mathbf{1}}-2~{\\rm CUT}(n)$. As a corollary of this work,\nit is straightforward using linear programming to determine if a particular\ncorrelation matrix is realizable or not. Furthermore, a sampling method for\nmultivariate symmetric Bernoullis with given correlation is obtained. In some\ncases the method can also be used for general, not exclusively Bernoulli,\nmarginals.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 21:19:50 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 19:51:56 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Huber", "Mark", ""], ["Maric", "Nevena", ""]]}, {"id": "1706.06274", "submitter": "Raghu Meka", "authors": "Adam Klivans, Raghu Meka", "title": "Learning Graphical Models Using Multiplicative Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, multiplicative-weight update algorithm for learning\nundirected graphical models or Markov random fields (MRFs). The approach is\nnew, and for the well-studied case of Ising models or Boltzmann machines, we\nobtain an algorithm that uses a nearly optimal number of samples and has\nquadratic running time (up to logarithmic factors), subsuming and improving on\nall prior work. Additionally, we give the first efficient algorithm for\nlearning Ising models over general alphabets.\n  Our main application is an algorithm for learning the structure of t-wise\nMRFs with nearly-optimal sample complexity (up to polynomial losses in\nnecessary terms that depend on the weights) and running time that is\n$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the\nparameters of the model and generate a hypothesis that is close in statistical\ndistance to the true MRF. All prior work runs in time $n^{\\Omega(d)}$ for\ngraphs of bounded degree d and does not generate a hypothesis close in\nstatistical distance even for t=3. We observe that our runtime has the correct\ndependence on n and t assuming the hardness of learning sparse parities with\nnoise.\n  Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)\nand holds in the on-line setting. Its analysis applies a regret bound from\nFreund and Schapire's classic Hedge algorithm. It also gives the first solution\nto the problem of learning sparse Generalized Linear Models (GLMs).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 05:41:31 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1706.06288", "submitter": "Javier \\'Alvarez  Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana", "title": "A review and comparative study on functional time series techniques", "comments": "38 pages with 16 figures. Supplementary material is also included (30\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.HO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper reviews the main estimation and prediction results derived in the\ncontext of functional time series, when Hilbert and Banach spaces are\nconsidered, specially, in the context of autoregressive processes of order one\n(ARH(1) and ARB(1) processes, for H and B being a Hilbert and Banach space,\nrespectively). Particularly, we pay attention to the estimation and prediction\nresults, and statistical tests, derived in both parametric and non-parametric\nframeworks. A comparative study between different ARH(1) prediction approaches\nis developed in the simulation study undertaken.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:09:19 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1706.06289", "submitter": "J\\\"urgen Kampf", "authors": "J\\\"urgen Kampf, Georgiy Shevchenko, Evgeny Spodarev", "title": "Nonparametric estimation of the kernel function of symmetric stable\n  moving average random functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the kernel function of a symmetric alpha stable ($S\\alpha S$)\nmoving average random function which is observed on a regular grid of points.\nThe proposed estimator relies on the empirical normalized (smoothed)\nperiodogram. It is shown to be weakly consistent for positive definite kernel\nfunctions, when the grid mesh size tends to zero and at the same time the\nobservation horizon tends to infinity (high frequency observations). A\nsimulation study shows that the estimator performs well at finite sample sizes,\nwhen the integrator measure of the moving average random function is $S\\alpha\nS$ and for some other infinitely divisible integrators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:11:45 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 09:26:08 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kampf", "J\u00fcrgen", ""], ["Shevchenko", "Georgiy", ""], ["Spodarev", "Evgeny", ""]]}, {"id": "1706.06296", "submitter": "Bharath Sriperumbudur", "authors": "Bharath Sriperumbudur and Nicholas Sterge", "title": "Approximate Kernel PCA Using Random Features: Computational vs.\n  Statistical Trade-off", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are powerful learning methodologies that provide a simple way\nto construct nonlinear algorithms from linear ones. Despite their popularity,\nthey suffer from poor scalability in big data scenarios. Various approximation\nmethods, including random feature approximation, have been proposed to\nalleviate the problem. However, the statistical consistency of most of these\napproximate kernel methods is not well understood except for kernel ridge\nregression wherein it has been shown that the random feature approximation is\nnot only computationally efficient but also statistically consistent with a\nminimax optimal rate of convergence. In this paper, we investigate the efficacy\nof random feature approximation in the context of kernel principal component\nanalysis (KPCA) by studying the trade-off between computational and statistical\nbehaviors of approximate KPCA. We show that the approximate KPCA is both\ncomputationally and statistically efficient compared to KPCA in terms of the\nerror associated with reconstructing a kernel function based on its projection\nonto the corresponding eigenspaces. The analysis hinges on Bernstein-type\ninequalities for the operator and Hilbert-Schmidt norms of a self-adjoint\nHilbert-Schmidt operator-valued U-statistics, which is of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:36:13 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 13:19:25 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 20:51:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Sriperumbudur", "Bharath", ""], ["Sterge", "Nicholas", ""]]}, {"id": "1706.06351", "submitter": "Annika  Betken", "authors": "Annika Betken and Rafa{\\l} Kulik", "title": "Testing for Change in Stochastic Volatility with Long Range Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, change-point problems for long memory stochastic volatility\nmodels are considered. A general testing problem which includes various\nalternative hypotheses is discussed. Under the hypothesis of stationarity the\nlimiting behavior of CUSUM- and Wilcoxon-type test statistics is derived. In\nthis context, a limit theorem for the two-parameter empirical process of long\nmemory stochastic volatility time series is proved. In particular, it is shown\nthat the asymptotic distribution of CUSUM test statistics may not be affected\nby long memory, unlike Wilcoxon test statistics which are typically influenced\nby long range dependence. To avoid the estimation of nuisance parameters in\napplications, the usage of self-normalized test statistics is proposed. The\ntheoretical results are accompanied by simulation studies which characterize\nthe finite sample behavior of the considered testing procedures when testing\nfor changes in mean, in variance, and in the tail index.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:08:59 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Betken", "Annika", ""], ["Kulik", "Rafa\u0142", ""]]}, {"id": "1706.06354", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, D. Bosq and M. D. Ruiz-Medina", "title": "Consistency of the plug-in functional predictor of the\n  Ornstein-Uhlenbeck process in Hilbert and Banach spaces", "comments": "30 pages with 10 figures. Supplementary material (8 pages) is also\n  included", "journal-ref": "Statistics & Probability Letters, 117, pp. 12-22 (2016)", "doi": "10.1016/j.spl.2016.04.023", "report-no": null, "categories": "math.ST math.FA stat.AP stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New results on functional prediction of the Ornstein-Uhlenbeck process in an\nautoregressive Hilbert-valued and Banach-valued frameworks are derived.\nSpecifically, consistency of the maximum likelihood estimator of the\nautocorrelation operator, and of the associated plug-in predictor is obtained\nin both frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:18:14 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 09:54:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Bosq", "D.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "1706.06498", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, D. Bosq and M. Dolores Ruiz-Medina", "title": "Asymptotic properties of a componentwise ARH(1) plug-in predictor", "comments": "50 pages (with 4 figures)", "journal-ref": "Journal of Multivariate Analysis, 155, pp. 12-34 (2017)", "doi": "10.1016/j.jmva.2016.11.009", "report-no": null, "categories": "math.ST math.FA stat.AP stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents new results on prediction of linear processes in function\nspaces. The autoregressive Hilbertian process framework of order one (ARH(1)\nprocess framework) is adopted. A componentwise estimator of the autocorrelation\noperator is formulated, from the moment-based estimation of its diagonal\ncoefficients, with respect to the orthogonal eigenvectors of the\nauto-covariance operator, which are assumed to be known. Mean-square\nconvergence to the theoretical autocorrelation operator, in the space of\nHilbert-Schmidt operators, is proved. Consistency then follows in that space.\nFor the associated ARH(1) plug-in predictor, mean absolute convergence to the\ncorresponding conditional expectation, in the considered Hilbert space, is\nobtained. Hence, consistency in that space also holds. A simulation study is\nundertaken to illustrate the finite-large sample behavior of the formulated\ncomponentwise estimator and predictor. The performance of the presented\napproach is compared with alternative approaches in the previous and current\nARH(1) framework literature, including the case of unknown eigenvectors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:50:55 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:24:01 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Bosq", "D.", ""], ["Ruiz-Medina", "M. Dolores", ""]]}, {"id": "1706.06632", "submitter": "S\\'ev\\'erien Nkurunziza", "authors": "S\\'ev\\'erien Nkurunziza, and Youzhi Yu", "title": "On the joint asymptotic distribution of the restricted estimators in\n  multivariate regression model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main Theorem of Jain et al.[Jain, K., Singh, S., and Sharma, S. (2011),\nRe- stricted estimation in multivariate measurement error regression model;\nJMVA, 102, 2, 264-280] is established in its full generality. Namely, we derive\nthe joint asymp- totic normality of the unrestricted estimator (UE) and the\nrestricted estimators of the matrix of the regression coefficients. The derived\nresult holds under the hypothesized restriction as well as under the sequence\nof alternative restrictions. In addition, we establish Asymptotic\nDistributional Risk for the estimators and compare their relative performance.\nIt is established that near the restriction, the restricted estimators (REs)\nperform better than the UE. But the REs perform worse than the unrestricted\nestimator when one moves far away from the restriction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:15:22 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Nkurunziza", "S\u00e9v\u00e9rien", ""], ["Yu", "Youzhi", ""]]}, {"id": "1706.06638", "submitter": "S\\'ev\\'erien Nkurunziza", "authors": "S\\'ev\\'erien Nkurunziza, and Yueleng Wang", "title": "On convergence of the sample correlation matrices in high-dimensional\n  data", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an estimation problem concerning the matrix of\ncorrelation coefficients in context of high dimensional data settings. In\nparticular, we revisit some results in Li and Rolsalsky [Li, D. and Rolsalsky,\nA. (2006). Some strong limit theorems for the largest entries of sample\ncorrelation matrices, The Annals of Applied Probability, 16, 1, 423-447]. Four\nof the main theorems of Li and Rolsalsky (2006) are established in their full\ngeneralities and we simplify substantially some proofs of the quoted paper.\nFurther, we generalize a theorem which is useful in deriving the existence of\nthe pth moment as well as in studying the convergence rates in law of large\nnumbers.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:29:39 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Nkurunziza", "S\u00e9v\u00e9rien", ""], ["Wang", "Yueleng", ""]]}, {"id": "1706.06770", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen", "title": "Nonlinear probability. A theory with incompatible stochastic variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1991 J.F. Aarnes introduced the concept of quasi-measures in a compact\ntopological space $\\Omega$ and established the connection between quasi-states\non $C (\\Omega)$ and quasi-measures in $\\Omega$. This work solved the linearity\nproblem of quasi-states on $C^*$-algebras formulated by R.V. Kadison in 1965.\nThe answer is that a quasi-state need not be linear, so a quasi-state need not\nbe a state. We introduce nonlinear measures in a space $\\Omega$ which is a\ngeneralization of a measurable space. In this more general setting we are still\nable to define integration and establish a representation theorem for the\ncorresponding functionals. A probabilistic language is choosen since we feel\nthat the subject should be of some interest to probabilists. In particular we\npoint out that the theory allows for incompatible stochastic variables. The\nneed for incompatible variables is well known in quantum mechanics, but the\nneed seems natural also in other contexts as we try to explain by a questionary\nexample.\n  Keywords and phrases: Epistemic probability, Integration with respect to mea-\nsures and other set functions, Banach algebras of continuous functions, Set\nfunc- tions and measures on topological spaces, States, Logical foundations of\nquantum mechanics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 07:42:17 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Taraldsen", "Gunnar", ""]]}, {"id": "1706.06774", "submitter": "Jiang Hu Dr.", "authors": "Qiuyan Zhang, Jiang Hu and Zhidong Bai", "title": "Optimal modification of the LRT for the equality of two high-dimensional\n  covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the optimal modification of the likelihood ratio test\n(LRT) for the equality of two high-dimensional covariance matrices. The\nclassical LRT is not well defined when the dimensions are larger than or equal\nto one of the sample sizes. In this paper, an optimally modified test that\nworks well in cases where the dimensions may be larger than the sample sizes is\nproposed. In addition, the test is established under the weakest conditions on\nthe moments and the dimensions of the samples. We also present weakly\nconsistent estimators of the fourth moments, which are necessary for the\nproposed test, when they are not equal to 3. From the simulation results and\nreal data analysis, we find that the performances of the proposed statistics\nare robust against affine transformations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 07:48:58 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 13:54:13 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 15:08:35 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Zhang", "Qiuyan", ""], ["Hu", "Jiang", ""], ["Bai", "Zhidong", ""]]}, {"id": "1706.06869", "submitter": "Tigran Nagapetyan", "authors": "Mike B. Giles, Tigran Nagapetyan, Klaus Ritter", "title": "Adaptive Multilevel Monte Carlo Approximation of Distribution Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse a multilevel Monte Carlo method for the approximation of\ndistribution functions of univariate random variables. Since, by assumption,\nthe target distribution is not known explicitly, approximations have to be\nused. We provide an asymptotic analysis of the error and the cost of the\nalgorithm. Furthermore we construct an adaptive version of the algorithm that\ndoes not require any a priori knowledge on weak or strong convergence rates. We\napply the adaptive algorithm to smooth path-independent and path-dependent\nfunctionals and to stopped exit times of SDEs.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 12:46:47 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Giles", "Mike B.", ""], ["Nagapetyan", "Tigran", ""], ["Ritter", "Klaus", ""]]}, {"id": "1706.06976", "submitter": "Javier \\'Alvarez-Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana, M. D. Ruiz-Medina", "title": "The effect of the spatial domain in FANOVA models with ARH(1) error term", "comments": "56 pages (with 11 figures). Supplementary material is also included", "journal-ref": "Statistics and Its Interface, 10, pp. 607-628 (2017)", "doi": "10.4310/SII.2017.v10.n4.a7", "report-no": null, "categories": "stat.AP math.FA math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional Analysis of Variance (FANOVA) from Hilbert-valued correlated data\nwith spatial rectangular or circular supports is analyzed, when Dirichlet\nconditions are assumed on the boundary. Specifically, a Hilbert-valued fixed\neffect model with error term defined from an Autoregressive Hilbertian process\nof order one (ARH(1) process) is considered, extending the formulation given in\nRuiz-Medina (2016). A new statistical test is also derived to contrast the\nsignificance of the functional fixed effect parameters. The Dirichlet\nconditions established at the boundary affect the dependence range of the\ncorrelated error term. While the rate of convergence to zero of the eigenvalues\nof the covariance kernels, characterizing the Gaussian functional error\ncomponents, directly affects the stability of the generalized least-squares\nparameter estimation problem. A simulation study and a real-data application\nrelated to fMRI analysis are undertaken to illustrate the performance of the\nparameter estimator and statistical test derived.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:01:54 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:44:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "1706.06977", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec, Joseph Salmon, Samuel Vaiter", "title": "A sharp oracle inequality for Graph-Slope", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent success on the analysis of the Slope estimator, we provide a\nsharp oracle inequality in term of prediction error for Graph-Slope, a\ngeneralization of Slope to signals observed over a graph. In addition to\nimproving upon best results obtained so far for the Total Variation denoiser\n(also referred to as Graph-Lasso or Generalized Lasso), we propose an efficient\nalgorithm to compute Graph-Slope. The proposed algorithm is obtained by\napplying the forward-backward method to the dual formulation of the Graph-Slope\noptimization problem. We also provide experiments showing the interest of the\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:04:35 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 19:51:51 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bellec", "Pierre C", ""], ["Salmon", "Joseph", ""], ["Vaiter", "Samuel", ""]]}, {"id": "1706.06991", "submitter": "Qiang Sun", "authors": "Qiang Sun, Wenxin Zhou, and Jianqing Fan", "title": "Adaptive Huber Regression", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data can easily be contaminated by outliers or contain variables with\nheavy-tailed distributions, which makes many conventional methods inadequate.\nTo address this challenge, we propose the adaptive Huber regression for robust\nestimation and inference. The key observation is that the robustification\nparameter should adapt to the sample size, dimension and moments for optimal\ntradeoff between bias and robustness. Our theoretical framework deals with\nheavy-tailed distributions with bounded $(1+\\delta)$-th moment for any $\\delta\n> 0$. We establish a sharp phase transition for robust estimation of regression\nparameters in both low and high dimensions: when $\\delta \\geq 1$, the estimator\nadmits a sub-Gaussian-type deviation bound without sub-Gaussian assumptions on\nthe data, while only a slower rate is available in the regime $0<\\delta< 1$.\nFurthermore, this transition is smooth and optimal. In addition, we extend the\nmethodology to allow both heavy-tailed predictors and observation noise.\nSimulation studies lend further support to the theory. In a genetic study of\ncancer cell lines that exhibit heavy-tailedness, the proposed methods are shown\nto be more robust and predictive.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:28:15 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 04:54:05 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Sun", "Qiang", ""], ["Zhou", "Wenxin", ""], ["Fan", "Jianqing", ""]]}, {"id": "1706.07034", "submitter": "Sim\\'eon Val\\`ere Bitseki Penda", "authors": "S Valere Bitseki Penda and Angelina Roche", "title": "Local bandwidth selection for kernel density estimation in bifurcating\n  Markov chain model", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an adaptive estimator for the stationary distribution of a\nbifurcating Markov Chain on $\\mathbb R^d$. Bifurcating Markov chains (BMC for\nshort) are a class of stochastic processes indexed by regular binary trees. A\nkernel estimator is proposed whose bandwidth is selected by a method inspired\nby the works of Goldenshluger and Lepski [18]. Drawing inspiration from\ndimension jump methods for model selection, we also provide an algorithm to\nselect the best constant in the penalty.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 17:55:21 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Penda", "S Valere Bitseki", ""], ["Roche", "Angelina", ""]]}, {"id": "1706.07136", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia", "title": "Multiscale Information Decomposition: Exact Computation for Multivariate\n  Gaussian Processes", "comments": null, "journal-ref": "Entropy, 19, 408 (2017)", "doi": "10.3390/e19080408", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the theory of state space models, we derive the exact expressions\nof the information transfer, as well as redundant and synergistic transfer, for\ncoupled Gaussian processes observed at multiple temporal scales. All of the\nterms, constituting the frameworks known as interaction information\ndecomposition and partial information decomposition, can thus be analytically\nobtained for different time scales from the parameters of the VAR model that\nfits the processes. We report the application of the proposed methodology\nfirstly to benchmark Gaussian systems, showing that this class of systems may\ngenerate patterns of information decomposition characterized by mainly\nredundant or synergistic information transfer persisting across multiple time\nscales or even by the alternating prevalence of redundant and synergistic\nsource interaction depending on the time scale. Then, we apply our method to an\nimportant topic in neuroscience, i.e., the detection of causal interactions in\nhuman epilepsy networks, for which we show the relevance of partial information\ndecomposition to the detection of multiscale information transfer spreading\nfrom the seizure onset zone.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 23:02:11 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 08:02:15 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Faes", "Luca", ""], ["Marinazzo", "Daniele", ""], ["Stramaglia", "Sebastiano", ""]]}, {"id": "1706.07180", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (PANAMA, DANTE), Gilles Blanchard (DATASHAPE, LMO),\n  Nicolas Keriven (PANAMA, GIPSA-GAIA), Yann Traonmilin (PANAMA, IMB)", "title": "Compressive Statistical Learning with Random Feature Moments", "comments": "Main novelties between version 1 and version 2: improved\n  concentration bounds, improved sketch sizes for compressive k-means and\n  compressive GMM that now scale linearly with the ambient dimensionMain\n  novelties of version 3: all content on compressive clustering and compressive\n  GMM is now developed in the companion paper hal-02536818; improved\n  statistical guarantees in a generic framework with illustration of the\n  improvements on compressive PCA. Mathematical Statistics and Learning, EMS\n  Publishing House, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general framework -- compressive statistical learning -- for\nresource-efficient large-scale learning: the training collection is compressed\nin one pass into a low-dimensional sketch (a vector of random empirical\ngeneralized moments) that captures the information relevant to the considered\nlearning task. A near-minimizer of the risk is computed from the sketch through\nthe solution of a nonlinear least squares problem. We investigate sufficient\nsketch sizes to control the generalization error of this procedure. The\nframework is illustrated on compressive PCA, compressive clustering, and\ncompressive Gaussian mixture Modeling with fixed known variance. The latter two\nare further developed in a companion paper.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 06:59:19 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 09:38:07 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 15:25:47 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 08:26:13 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "PANAMA, DANTE"], ["Blanchard", "Gilles", "", "DATASHAPE, LMO"], ["Keriven", "Nicolas", "", "PANAMA, GIPSA-GAIA"], ["Traonmilin", "Yann", "", "PANAMA, IMB"]]}, {"id": "1706.07193", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos and Daniel Sanz-Alonso", "title": "Continuum Limit of Posteriors in Graph Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AP math.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a function input of a differential\nequation formulated on an unknown domain $M$. We assume to have access to a\ndiscrete domain $M_n=\\{x_1, \\dots, x_n\\} \\subset M$, and to noisy measurements\nof the output solution at $p\\le n$ of those points. We introduce a graph-based\nBayesian inverse problem, and show that the graph-posterior measures over\nfunctions in $M_n$ converge, in the large $n$ limit, to a posterior over\nfunctions in $M$ that solves a Bayesian inverse problem with known domain.\n  The proofs rely on the variational formulation of the Bayesian update, and on\na new topology for the study of convergence of measures over functions on point\nclouds to a measure over functions on the continuum. Our framework, techniques,\nand results may serve to lay the foundations of robust uncertainty\nquantification of graph-based tasks in machine learning. The ideas are\npresented in the concrete setting of recovering the initial condition of the\nheat equation on an unknown manifold.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 07:39:51 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "1706.07237", "submitter": "Johannes Tewes", "authors": "Johannes Tewes, Daniel J. Nordman and Dimitris N. Politis", "title": "Convolved subsampling estimation with applications to block bootstrap", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block bootstrap approximates sampling distributions from dependent data\nby resampling data blocks. A fundamental problem is establishing its\nconsistency for the distribution of a sample mean, as a prototypical statistic.\nWe use a structural relationship with subsampling to characterize the bootstrap\nin a new and general manner. While subsampling and block bootstrap differ, the\nblock bootstrap distribution of a sample mean equals that of a $k$-fold\nself-convolution of a subsampling distribution. Motivated by this, we provide\nsimple necessary and sufficient conditions for a convolved subsampling\nestimator to produce a normal limit that matches the target of bootstrap\nestimation. These conditions may be linked to consistency properties of an\noriginal subsampling distribution, which are often obtainable under minimal\nassumptions. Through several examples, the results are shown to validate the\nblock bootstrap for means under significantly weakened assumptions in many\nexisting (and some new) dependence settings, which also addresses a standing\nconjecture of Politis, Romano and Wolf(1999). Beyond sample means, the\nconvolved subsampling estimator may not match the block bootstrap, but instead\nprovides a hybrid-resampling estimator of interest in its own right. For\ngeneral statistics with normal limits, results also establish the consistency\nof convolved subsampling under minimal dependence conditions, including\nnon-stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 10:09:25 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Tewes", "Johannes", ""], ["Nordman", "Daniel J.", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1706.07328", "submitter": "Damian Kozbur", "authors": "Christian Hansen, Damian Kozbur, Sanjog Misra", "title": "Targeted Undersmoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a post-model selection inference procedure, called\ntargeted undersmoothing, designed to construct uniformly valid confidence sets\nfor a broad class of functionals of sparse high-dimensional statistical models.\nThese include dense functionals, which may potentially depend on all elements\nof an unknown high-dimensional parameter. The proposed confidence sets are\nbased on an initially selected model and two additionally selected models, an\nupper model and a lower model, which enlarge the initially selected model. We\nillustrate application of the procedure in two empirical examples. The first\nexample considers estimation of heterogeneous treatment effects using data from\nthe Job Training Partnership Act of 1982, and the second example looks at\nestimating profitability from a mailing strategy based on estimated\nheterogeneous treatment effects in a direct mail marketing campaign. We also\nprovide evidence on the finite sample performance of the proposed targeted\nundersmoothing procedure through a series of simulation experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 14:03:52 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 12:43:42 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Hansen", "Christian", ""], ["Kozbur", "Damian", ""], ["Misra", "Sanjog", ""]]}, {"id": "1706.07408", "submitter": "Aur\\'elien Bibaut", "authors": "Aurelien F. Bibaut and Mark J. van der Laan", "title": "Data-adaptive smoothing for optimal-rate estimation of possibly\n  non-regular parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric inference of finite dimensional, potentially\nnon-pathwise differentiable target parameters. In a nonparametric model, some\nexamples of such parameters that are always non pathwise differentiable target\nparameters include probability density functions at a point, or regression\nfunctions at a point. In causal inference, under appropriate causal\nassumptions, mean counterfactual outcomes can be pathwise differentiable or\nnot, depending on the degree at which the positivity assumption holds.\n  In this paper, given a potentially non-pathwise differentiable target\nparameter, we introduce a family of approximating parameters, that are pathwise\ndifferentiable. This family is indexed by a scalar. In kernel regression or\ndensity estimation for instance, a natural choice for such a family is obtained\nby kernel smoothing and is indexed by the smoothing level. For the\ncounterfactual mean outcome, a possible approximating family is obtained\nthrough truncation of the propensity score, and the truncation level then plays\nthe role of the index.\n  We propose a method to data-adaptively select the index in the family, so as\nto optimize mean squared error. We prove an asymptotic normality result, which\nallows us to derive confidence intervals. Under some conditions, our estimator\nachieves an optimal mean squared error convergence rate. Confidence intervals\nare data-adaptive and have almost optimal width.\n  A simulation study demonstrates the practical performance of our estimators\nfor the inference of a causal dose-response curve at a given treatment dose.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:23:39 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 23:29:12 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Bibaut", "Aurelien F.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1706.07449", "submitter": "Moritz Schauer", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian estimation of a H\\\"older continuous diffusion\n  coefficient", "comments": null, "journal-ref": "Braz. J. Probab. Stat., 34(3): 537-579. 2020", "doi": "10.1214/19-BJPS433", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonparametric Bayesian approach to estimate the diffusion\ncoefficient of a stochastic differential equation given discrete time\nobservations over a fixed time interval. As a prior on the diffusion\ncoefficient, we employ a histogram-type prior with piecewise constant\nrealisations on bins forming a partition of the time interval. Specifically,\nthese constants are realizations of independent inverse Gamma distributed\nrandoma variables. We justify our approach by deriving the rate at which the\ncorresponding posterior distribution asymptotically concentrates around the\ndata-generating diffusion coefficient. This posterior contraction rate turns\nout to be optimal for estimation of a H\\\"older-continuous diffusion coefficient\nwith smoothness parameter $0<\\lambda\\leq 1.$ Our approach is straightforward to\nimplement, as the posterior distributions turn out to be inverse Gamma again,\nand leads to good practical results in a wide range of simulation examples.\nFinally, we apply our method on exchange rate data sets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:16:50 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 12:32:40 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 18:43:54 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 12:24:55 GMT"}, {"version": "v5", "created": "Wed, 30 Jan 2019 13:49:04 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1706.07550", "submitter": "Jose Zubizarreta", "authors": "Luke W. Miratrix, Stefan Wager, Jose R. Zubizarreta", "title": "Shape-constrained partial identification of a population mean under\n  unknown probabilities of sample selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prevailing challenge in the biomedical and social sciences is to estimate a\npopulation mean from a sample obtained with unknown selection probabilities.\nUsing a well-known ratio estimator, Aronow and Lee (2013) proposed a method for\npartial identification of the mean by allowing the unknown selection\nprobabilities to vary arbitrarily between two fixed extreme values. In this\npaper, we show how to leverage auxiliary shape constraints on the population\noutcome distribution, such as symmetry or log-concavity, to obtain tighter\nbounds on the population mean. We use this method to estimate the performance\nof Aymara students---an ethnic minority in the north of Chile---in a national\neducational standardized test. We implement this method in the new statistical\nsoftware package scbounds for R.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 03:04:57 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Miratrix", "Luke W.", ""], ["Wager", "Stefan", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "1706.07584", "submitter": "Mu-Fa Chen", "authors": "Mu-Fa Chen", "title": "Global algorithms for maximal eigenpair", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.OC math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a continuation of \\ct{cmf16} where an efficient algorithm for\ncomputing the maximal eigenpair was introduced first for tridiagonal matrices\nand then extended to the irreducible matrices with nonnegative off-diagonal\nelements. This paper introduces two global algorithms for computing the maximal\neigenpair in a rather general setup, including even a class of real (with some\nnegative off-diagonal elements) or complex matrices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 07:03:45 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Chen", "Mu-Fa", ""]]}, {"id": "1706.07586", "submitter": "Hock Peng Chan", "authors": "Hock-Peng Chan, Hao Chen", "title": "Multi-sequence segmentation via score and higher-criticism tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local segmentation of multiple sequences sharing a common time- or\nlocation-index, building upon the single sequence local segmentation methods of\nNiu and Zhang (2012) and Fang, Li and Siegmund (2016). We also propose reverse\nsegmentation of multiple sequences that is new even in the single sequence\ncontext. We show that local segmentation estimates change-points consistently\nfor both single and multiple sequences, and that both methods proposed here\ndetect signals well, with the reverse segmentation method outperforming a large\nnumber of known segmentation methods on a commonly used single sequence test\nscenario. We show that on a recent allele-specific copy number study involving\nmultiple cancer patients, the simultaneous segmentations of the DNA sequences\nof all the patients provide information beyond that obtained by segmentation of\nthe sequences one at a time.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 07:16:37 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Chan", "Hock-Peng", ""], ["Chen", "Hao", ""]]}, {"id": "1706.07607", "submitter": "Fengnan Gao", "authors": "Fengnan Gao, Aad van der Vaart, Rui Castro, Remco van der Hofstad", "title": "Consistent Estimation in General Sublinear Preferential Attachment Trees", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical estimator of the preferential attachment function $f$\nin the setting of general preferential attachment trees. Using a supercritical\ncontinuous-time branching process framework, we prove the almost sure\nconsistency of the proposed estimator. We perform simulations to study the\nempirical properties of our estimators.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 09:17:26 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Gao", "Fengnan", ""], ["van der Vaart", "Aad", ""], ["Castro", "Rui", ""], ["van der Hofstad", "Remco", ""]]}, {"id": "1706.07712", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead", "title": "Asymptotics of ABC", "comments": "This document is due to appear as a chapter of the forthcoming\n  Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y.\n  Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an informal review of recent work on the asymptotics of\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\nhave more data? The results we review show that ABC can perform well in terms\nof point estimation, but standard implementations will over-estimate the\nuncertainty about the parameters. If we use the regression correction of\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\ntheoretical results also have practical implications for how to implement ABC.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:01:34 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Fearnhead", "Paul", ""]]}, {"id": "1706.07766", "submitter": "Alfredo Alegr\\'ia", "authors": "Alfredo Alegr\\'ia, Emilio Porcu and Reinhard Furrer", "title": "Asymmetric Matrix-Valued Covariances for Multivariate Random Fields on\n  Spheres", "comments": null, "journal-ref": null, "doi": "10.1080/00949655.2017.1406488", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-valued covariance functions are crucial to geostatistical modeling of\nmultivariate spatial data. The classical assumption of symmetry of a\nmultivariate covariance function is overlay restrictive and has been considered\nas unrealistic for most of real data applications. Despite of that, the\nliterature on asymmetric covariance functions has been very sparse. In\nparticular, there is some work related to asymmetric covariances on Euclidean\nspaces, depending on the Euclidean distance. However, for data collected over\nlarge portions of planet Earth, the most natural spatial domain is a sphere,\nwith the corresponding geodesic distance being the natural metric. In this\nwork, we propose a strategy based on spatial rotations to generate asymmetric\ncovariances for multivariate random fields on the $d$-dimensional unit sphere.\nWe illustrate through simulations as well as real data analysis that our\nproposal allows to achieve improvements in the predictive performance in\ncomparison to the symmetric counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 16:16:34 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Alegr\u00eda", "Alfredo", ""], ["Porcu", "Emilio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1706.07840", "submitter": "Neil Shephard", "authors": "Iavor Bojinov and Neil Shephard", "title": "Time series experiments and causal estimands: exact randomization tests\n  and trading", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1527225", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define causal estimands for experiments on single time series, extending\nthe potential outcome framework to dealing with temporal data. Our approach\nallows the estimation of some of these estimands and exact randomization based\np-values for testing causal effects, without imposing stringent assumptions. We\ntest our methodology on simulated \"potential autoregressions,\"which have a\ncausal interpretation. Our methodology is partially inspired by data from a\nlarge number of experiments carried out by a financial company who compared the\nimpact of two different ways of trading equity futures contracts. We use our\nmethodology to make causal statements about their trading methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 19:14:38 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 17:35:30 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bojinov", "Iavor", ""], ["Shephard", "Neil", ""]]}, {"id": "1706.07899", "submitter": "Yuefeng Han", "authors": "Yuefeng Han and Ruey S. Tsay", "title": "High-dimensional Linear Regression for Dependent Observations with\n  Application to Nowcasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, an extensive literature has been focused on the\n$\\ell_1$ penalized least squares (Lasso) estimators of high dimensional linear\nregression when the number of covariates $p$ is considerably larger than the\nsample size $n$. However, there is limited attention paid to the properties of\nthe estimators when the errors or/and the covariates are serially dependent. In\nthis study, we investigate the theoretical properties of the Lasso estimators\nfor linear regression with random design under serially dependent and/or\nnon-sub-Gaussian errors and covariates. In contrast to the traditional case in\nwhich the errors are i.i.d and have finite exponential moments, we show that\n$p$ can at most be a power of $n$ if the errors have only polynomial moments.\nIn addition, the rate of convergence becomes slower due to the serial\ndependencies in errors and the covariates. We also consider sign consistency\nfor model selection via Lasso when there are serial correlations in the errors\nor the covariates or both. Adopting the framework of functional dependence\nmeasure, we provide a detailed description on how the rates of convergence and\nthe selection consistencies of the estimators depend on the dependence measures\nand moment conditions of the errors and the covariates. Simulation results show\nthat Lasso regression can be substantially more powerful than the mixed\nfrequency data sampling regression (MIDAS) in the presence of irrelevant\nvariables. We apply the results obtained for the Lasso method to nowcasting\nmixing frequency data in which serially correlated errors and a large number of\ncovariates are common. In real examples, the Lasso procedure outperforms the\nMIDAS in both forecasting and nowcasting.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 03:01:10 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 03:15:51 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Han", "Yuefeng", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1706.08020", "submitter": "John Goes", "authors": "John Goes, Gilad Lerman, Boaz Nadler", "title": "Robust Sparse Covariance Estimation by Thresholding Tyler's M-Estimator", "comments": null, "journal-ref": "Annals of Statistics, 48(1):86-110, 2020", "doi": "10.1214/18-AOS1793", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a high-dimensional sparse covariance matrix from a limited number\nof samples is a fundamental problem in contemporary data analysis. Most\nproposals to date, however, are not robust to outliers or heavy tails. Towards\nbridging this gap, in this work we consider estimating a sparse shape matrix\nfrom $n$ samples following a possibly heavy tailed elliptical distribution. We\npropose estimators based on thresholding either Tyler's M-estimator or its\nregularized variant. We derive bounds on the difference in spectral norm\nbetween our estimators and the shape matrix in the joint limit as the dimension\n$p$ and sample size $n$ tend to infinity with $p/n\\to\\gamma>0$. These bounds\nare minimax rate-optimal. Results on simulated data support our theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 01:46:47 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 01:35:18 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 09:53:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Goes", "John", ""], ["Lerman", "Gilad", ""], ["Nadler", "Boaz", ""]]}, {"id": "1706.08058", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Peter B\\\"uhlmann and Jonas Peters", "title": "Invariant Causal Prediction for Sequential Data", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of inferring the causal predictors of a response\n$Y$ from a set of $d$ explanatory variables $(X^1,\\dots,X^d)$. Classical\nordinary least squares regression includes all predictors that reduce the\nvariance of $Y$. Using only the causal predictors instead leads to models that\nhave the advantage of remaining invariant under interventions, loosely speaking\nthey lead to invariance across different \"environments\" or \"heterogeneity\npatterns\". More precisely, the conditional distribution of $Y$ given its causal\npredictors remains invariant for all observations. Recent work exploits such a\nstability to infer causal relations from data with different but known\nenvironments. We show that even without having knowledge of the environments or\nheterogeneity pattern, inferring causal relations is possible for time-ordered\n(or any other type of sequentially ordered) data. In particular, this allows\ndetecting instantaneous causal relations in multivariate linear time series\nwhich is usually not the case for Granger causality. Besides novel methodology,\nwe provide statistical confidence bounds and asymptotic detection results for\ninferring causal predictors, and present an application to monetary policy in\nmacroeconomics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 08:25:25 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 15:35:51 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Pfister", "Niklas", ""], ["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""]]}, {"id": "1706.08167", "submitter": "Teng Zhang", "authors": "Teng Zhang", "title": "Phase retrieval using alternating minimization in a batch setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of phase retrieval, where the goal is to\nrecover a signal $z\\in C^n$ from the observations $y_i=|a_i^* z|$,\n$i=1,2,\\cdots,m$. While many algorithms have been proposed, the alternating\nminimization algorithm has been one of the most commonly used methods, and it\nis very simple to implement. Current work has proved that when the observation\nvectors $\\{a_i\\}_{i=1}^m$ are sampled from a complex Gaussian distribution\n$N(0, I)$, it recovers the underlying signal with a good initialization when\n$m=O(n)$, or with random initialization when $m=O(n^2)$, and it conjectured\nthat random initialization succeeds with $m=O(n)$. This work proposes a\nmodified alternating minimization method in a batch setting, and proves that\nwhen $m=O(n\\log^{3}n)$, the proposed algorithm with random initialization\nrecovers the underlying signal with high probability. The proof is based on the\nobservation that after each iteration of alternating minimization, with high\nprobability, the angle between the estimated signal and the underlying signal\nis reduced.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 20:28:32 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 20:21:35 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Zhang", "Teng", ""]]}, {"id": "1706.08170", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen", "title": "Image transformations on locally compact spaces", "comments": "Keywords: Banach algebras of continuous functions, Integration with\n  respect to measures and other set functions, Set functions and measures on\n  topological spaces, States, Logical foundations of quantum mechanics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An image is here defined to be a set which is either open or closed and an\nimage transformation is structure preserving in the following sense: It\ncorresponds to an algebra homomorphism for each singly generated algebra. The\nresults extend parts of results of J.F. Aarnes on quasi-measures, -states,\n-homomorphisms, and image-transformations from the setting compact Hausdorff\nspaces to locally compact Hausdorff spaces.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 20:47:17 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Taraldsen", "Gunnar", ""]]}, {"id": "1706.08244", "submitter": "Veronique Maume-Deschamps", "authors": "Ahmed Manaf (ICJ), V\\'eronique Maume-Deschamps (ICJ), Pierre Ribereau\n  (ICJ), C\\'eline Vial (ICJ, DRACULA)", "title": "Spatial Risk Measure for Max-Stable and Max-Mixture Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider isotropic and stationary max-stable, inverse\nmax-stable and max-mixture processes $X=(X(s))\\_{s\\in\\bR^2}$ and the damage\nfunction $\\cD\\_X^{\\nu}= |X|^\\nu$ with $0<\\nu<1/2$. We study the quantitative\nbehavior of a risk measure which is the variance of the average of\n$\\cD\\_X^{\\nu}$ over a region $\\mathcal{A}\\subset \\bR^2$.} This kind of risk\nmeasure has already been introduced and studied for \\vero{some} max-stable\nprocesses in \\cite{koch2015spatial}. %\\textcolor{red}{In this study, we\ngeneralised this risk measure to be applicable for several models: asymptotic\ndependence represented by max-stable, asymptotic independence represented by\ninverse max-stable and mixing between of them.} We evaluated the proposed risk\nmeasure by a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 06:33:09 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Manaf", "Ahmed", "", "ICJ"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ"], ["Ribereau", "Pierre", "", "ICJ"], ["Vial", "C\u00e9line", "", "ICJ, DRACULA"]]}, {"id": "1706.08250", "submitter": "Etienne Roquain", "authors": "Sebastian D\\\"ohler, Guillermo Durand (1), Etienne Roquain (1)((1)\n  LPMA)", "title": "Improving the Benjamini-Hochberg Procedure for Discrete Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find interesting items in genome-wide association studies or next\ngeneration sequencing data, a crucial point is to design powerful false\ndiscovery rate (FDR) controlling procedures that suitably combine discrete\ntests (typically binomial or Fisher tests). In particular, recent research has\nbeen striving for appropriate modifications of the classical Benjamini-Hochberg\n(BH) step-up procedure that accommodate discreteness. However, despite an\nimportant number of attempts, these procedures did not come with theoretical\nguarantees. The present paper contributes to fill the gap: it presents new\nmodifications of the BH procedure that incorporate the discrete structure of\nthe data and provably control the FDR for any fixed number of null hypotheses\n(under independence). Markedly, our FDR controlling methodology allows to\nincorporate simultaneously the discreteness and the quantity of signal of the\ndata (corresponding therefore to a so-called $\\pi\\_0$-adaptive procedure). The\npower advantage of the new methods is demonstrated in a numerical experiment\nand for some appropriate real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 07:05:38 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 14:06:18 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["D\u00f6hler", "Sebastian", ""], ["Durand", "Guillermo", ""], ["Roquain", "Etienne", ""]]}, {"id": "1706.08277", "submitter": "Luc Lehericy", "authors": "Luc Leh\\'ericy (LMO)", "title": "State-by-state Minimax Adaptive Estimation for Nonparametric Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new estimator for the emission densities of a\nnonparametric hidden Markov model. It is adaptive and minimax with respect to\neach state's regularity--as opposed to globally minimax estimators, which adapt\nto the worst regularity among the emission densities. Our method is based on\nGoldenshluger and Lepski's methodology. It is computationally efficient and\nonly requires a family of preliminary estimators, without any restriction on\nthe type of estimators considered. We present two such estimators that allow to\nreach minimax rates up to a logarithmic term: a spectral estimator and a least\nsquares estimator. We show how to calibrate it in practice and assess its\nperformance on simulations and on real data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 08:42:37 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 15:06:27 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 14:27:03 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Leh\u00e9ricy", "Luc", "", "LMO"]]}, {"id": "1706.08344", "submitter": "Felix Abramovich", "authors": "Felix Abramovich and Vadim Grinshtein", "title": "High-dimensional classification by sparse logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional binary classification by sparse logistic\nregression. We propose a model/feature selection procedure based on penalized\nmaximum likelihood with a complexity penalty on the model size and derive the\nnon-asymptotic bounds for the resulting misclassification excess risk. The\nbounds can be reduced under the additional low-noise condition. The proposed\ncomplexity penalty is remarkably related to the VC-dimension of a set of sparse\nlinear classifiers. Implementation of any complexity penalty-based criterion,\nhowever, requires a combinatorial search over all possible models. To find a\nmodel selection procedure computationally feasible for high-dimensional data,\nwe extend the Slope estimator for logistic regression and show that under an\nadditional weighted restricted eigenvalue condition it is rate-optimal in the\nminimax sense.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 12:42:42 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 08:49:45 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 10:18:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Abramovich", "Felix", ""], ["Grinshtein", "Vadim", ""]]}, {"id": "1706.08557", "submitter": "Carlo Orsi", "authors": "Carlo Orsi", "title": "New insights into non-central beta distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta family owes its privileged status within unit interval distributions\nto several relevant features such as, for example, easyness of interpretation\nand versatility in modeling different types of data. However, its flexibility\nat the unit interval endpoints is poor enough to prevent from properly modeling\nthe portions of data having values next to zero and one. Such a drawback can be\novercome by resorting to the class of the non-central beta distributions.\nIndeed, the latter allows the density to take on arbitrary positive and finite\nlimits which have a really simple form. That said, new insights into such class\nare provided in this paper. In particular, new representations and moments\nexpressions are derived. Moreover, its potential with respect to alternative\nmodels is highlighted through applications to real data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 18:41:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Orsi", "Carlo", ""]]}, {"id": "1706.08561", "submitter": "Andrea Montanari", "authors": "Emmanuel Abbe, Laurent Massoulie, Andrea Montanari, Allan Sly, Nikhil\n  Srivastava", "title": "Group Synchronization on Grids", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group synchronization requires to estimate unknown elements\n$({\\theta}_v)_{v\\in V}$ of a compact group ${\\mathfrak G}$ associated to the\nvertices of a graph $G=(V,E)$, using noisy observations of the group\ndifferences associated to the edges. This model is relevant to a variety of\napplications ranging from structure from motion in computer vision to graph\nlocalization and positioning, to certain families of community detection\nproblems.\n  We focus on the case in which the graph $G$ is the $d$-dimensional grid.\nSince the unknowns ${\\boldsymbol \\theta}_v$ are only determined up to a global\naction of the group, we consider the following weak recovery question. Can we\ndetermine the group difference ${\\theta}_u^{-1}{\\theta}_v$ between far apart\nvertices $u, v$ better than by random guessing? We prove that weak recovery is\npossible (provided the noise is small enough) for $d\\ge 3$ and, for certain\nfinite groups, for $d\\ge 2$. Viceversa, for some continuous groups, we prove\nthat weak recovery is impossible for $d=2$. Finally, for strong enough noise,\nweak recovery is always impossible.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 18:56:53 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Massoulie", "Laurent", ""], ["Montanari", "Andrea", ""], ["Sly", "Allan", ""], ["Srivastava", "Nikhil", ""]]}, {"id": "1706.08565", "submitter": "Ryan Martin", "authors": "Michael S. Balch and Ryan Martin and Scott Ferson", "title": "Satellite conjunction analysis and the false confidence theorem", "comments": "18 pages, 3 figures", "journal-ref": "Proceedings of the Royal Society Series A, volume 475, paper\n  20180565, 2019", "doi": "10.1098/rspa.2018.0565", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite conjunction analysis is the assessment of collision risk during a\nclose encounter between a satellite and another object in orbit. A\ncounterintuitive phenomenon has emerged in the conjunction analysis literature,\nnamely, probability dilution, in which lower quality data paradoxically appear\nto reduce the risk of collision. We show that probability dilution is a symptom\nof a fundamental deficiency in probabilistic representations of statistical\ninference, in which there are propositions that will consistently be assigned a\nhigh degree of belief, regardless of whether or not they are true. We call this\ndeficiency false confidence. In satellite conjunction analysis, it results in a\nsevere and persistent underestimate of collision risk exposure.\n  We introduce the Martin--Liu validity criterion as a benchmark by which to\nidentify statistical methods that are free from false confidence. Such\ninferences will necessarily be non-probabilistic. In satellite conjunction\nanalysis, we show that uncertainty ellipsoids satisfy the validity criterion.\nPerforming collision avoidance maneuvers based on ellipsoid overlap will ensure\nthat collision risk is capped at the user-specified level. Further, this\ninvestigation into satellite conjunction analysis provides a template for\nrecognizing and resolving false confidence issues as they occur in other\nproblems of statistical inference.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:09:21 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 15:06:33 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 22:46:47 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 14:28:31 GMT"}, {"version": "v5", "created": "Wed, 24 Jul 2019 13:58:28 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Balch", "Michael S.", ""], ["Martin", "Ryan", ""], ["Ferson", "Scott", ""]]}, {"id": "1706.08567", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "Empirical priors and posterior concentration rates for a monotone\n  density", "comments": "14 pages; 3 figures; 2 tables", "journal-ref": "Sankhya A, 2019, volume 81, pages 493--509", "doi": "10.1007/s13171-018-0147-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Bayesian context, prior specification for inference on monotone\ndensities is conceptually straightforward, but proving posterior convergence\ntheorems is complicated by the fact that desirable prior concentration\nproperties often are not satisfied. In this paper, I first develop a new prior\ndesigned specifically to satisfy an empirical version of the prior\nconcentration property, and then I give sufficient conditions on the prior\ninputs such that the corresponding empirical Bayes posterior concentrates\naround the true monotone density at nearly the optimal minimax rate. Numerical\nillustrations also reveal the practical benefits of the proposed empirical\nBayes approach compared to Dirichlet process mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:12:52 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 14:24:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1706.08648", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou", "title": "Laplace deconvolution in the presence of indirect long-memory data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating a function $f$ based on observations\nfrom its noisy convolution when the noise exhibits long-range dependence. We\nconstruct an adaptive estimator based on the kernel method, derive minimax\nlower bound for the $L^2$-risk when $f$ belongs to Sobolev space and show that\nsuch estimator attains optimal rates that deteriorate as the LRD worsens.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 02:17:26 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Benhaddou", "Rida", ""]]}, {"id": "1706.08921", "submitter": "Giuseppe Pica", "authors": "Giuseppe Pica, Eugenio Piasini, Daniel Chicharro, Stefano Panzeri", "title": "Invariant components of synergy, redundancy, and unique information\n  among three variables", "comments": null, "journal-ref": "Entropy 2017, 19(9), 451", "doi": "10.3390/e19090451", "report-no": null, "categories": "cs.IT cs.NE math.IT math.ST physics.bio-ph physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a system of three stochastic variables, the Partial Information\nDecomposition (PID) of Williams and Beer dissects the information that two\nvariables (sources) carry about a third variable (target) into nonnegative\ninformation atoms that describe redundant, unique, and synergistic modes of\ndependencies among the variables. However, the classification of the three\nvariables into two sources and one target limits the dependency modes that can\nbe quantitatively resolved, and does not naturally suit all systems. Here, we\nextend the PID to describe trivariate modes of dependencies in full generality,\nwithout introducing additional decomposition axioms or making assumptions about\nthe target/source nature of the variables. By comparing different PID lattices\nof the same system, we unveil a finer PID structure made of seven nonnegative\ninformation subatoms that are invariant to different target/source\nclassifications and that are sufficient to construct any PID lattice. This\nfiner structure naturally splits redundant information into two nonnegative\ncomponents: the source redundancy, which arises from the pairwise correlations\nbetween the source variables, and the non-source redundancy, which does not,\nand relates to the synergistic information the sources carry about the target.\nThe invariant structure is also sufficient to construct the system's entropy,\nhence it characterizes completely all the interdependencies in the system.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:09:37 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Pica", "Giuseppe", ""], ["Piasini", "Eugenio", ""], ["Chicharro", "Daniel", ""], ["Panzeri", "Stefano", ""]]}, {"id": "1706.09231", "submitter": "Benjamin Stucky", "authors": "Benjamin Stucky, Sara van de Geer", "title": "Asymptotic Confidence Regions for High-dimensional Structured Sparsity", "comments": "28 pages, 4 figures, 1 table", "journal-ref": null, "doi": "10.1109/TSP.2018.2807399", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of high-dimensional linear regression models, we propose two\nframeworks for constructing pointwise and group confidence sets for penalized\nestimators which incorporate prior knowledge about the organization of the\nnon-zero coefficients. This is done by desparsifying the estimator as in van de\nGeer et al. [18] and van de Geer and Stucky [17], then using an appropriate\nestimator for the precision matrix $\\Theta$. In order to estimate the precision\nmatrix a corresponding structured matrix norm penalty has to be introduced.\n  After normalization the result is an asymptotic pivot.\n  The asymptotic behavior is studied and simulations are added to study the\ndifferences between the two schemes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 12:01:17 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Stucky", "Benjamin", ""], ["van de Geer", "Sara", ""]]}, {"id": "1706.09233", "submitter": "Alfredo Alegr\\'ia", "authors": "Emilio Porcu, Alfredo Alegr\\'ia and Reinhard Furrer", "title": "Modeling Temporally Evolving and Spatially Globally Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decades have seen an unprecedented increase in the availability of\ndata sets that are inherently global and temporally evolving, from remotely\nsensed networks to climate model ensembles. This paper provides a view of\nstatistical modeling techniques for space-time processes, where space is the\nsphere representing our planet. In particular, we make a distintion between (a)\nsecond order-based, and (b) practical approaches to model temporally evolving\nglobal processes. The former are based on the specification of a class of\nspace-time covariance functions, with space being the two-dimensional sphere.\nThe latter are based on explicit description of the dynamics of the space-time\nprocess, i.e., by specifying its evolution as a function of its past history\nwith added spatially dependent noise.\n  We especially focus on approach (a), where the literature has been sparse. We\nprovide new models of space-time covariance functions for random fields defined\non spheres cross time. Practical approaches, (b), are also discussed, with\nspecial emphasis on models built directly on the sphere, without projecting the\nspherical coordinate on the plane.\n  We present a case study focused on the analysis of air pollution from the\n2015 wildfires in Equatorial Asia, an event which was classified as the year's\nworst environmental disaster. The paper finishes with a list of the main\ntheoretical and applied research problems in the area, where we expect the\nstatistical community to engage over the next decade.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 12:07:22 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Porcu", "Emilio", ""], ["Alegr\u00eda", "Alfredo", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1706.09293", "submitter": "James Ridgway", "authors": "Pierre Alquier and James Ridgway", "title": "Concentration of tempered posteriors and of their variational\n  approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are extremely popular in statistics and machine\nlearning, their application to massive datasets is often challenging, when\npossible at all. Indeed, the classical MCMC algorithms are prohibitively slow\nwhen both the model dimension and the sample size are large. Variational\nBayesian methods aim at approximating the posterior by a distribution in a\ntractable family. Thus, MCMC are replaced by an optimization algorithm which is\norders of magnitude faster. VB methods have been applied in such\ncomputationally demanding applications as including collaborative filtering,\nimage and video processing, NLP and text processing... However, despite very\nnice results in practice, the theoretical properties of these approximations\nare usually not known. In this paper, we propose a general approach to prove\nthe concentration of variational approximations of fractional posteriors. We\napply our theory to two examples: matrix completion, and Gaussian VB.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:58:56 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 17:27:01 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 08:30:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Alquier", "Pierre", ""], ["Ridgway", "James", ""]]}, {"id": "1706.09811", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Agn\\`es Lagnoux, Thi Mong Ngoc Nguyen, Fr\\'ed\\'eric Pro\\\"ia", "title": "On the Bickel-Rosenblatt test of goodness-of-fit for the residuals of\n  autoregressive processes", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate in this paper a Bickel-Rosenblatt test of goodness-of-fit for\nthe density of the noise in an autoregressive model. Since the seminal work of\nBickel and Rosenblatt, it is well-known that the integrated squared error of\nthe Parzen-Rosenblatt density estimator, once correctly renormalized, is\nasymptotically Gaussian for independent and identically distributed (i.i.d.)\nsequences. We show that the result still holds when the statistic is built from\nthe residuals of general stable and explosive autoregressive processes. In the\nunivariate unstable case, we prove that the result holds when the unit root is\nlocated at $-1$ whereas we give further results when the unit root is located\nat $1$. In particular, we establish that except for some particular asymmetric\nkernels leading to a non-Gaussian limiting distribution and a slower\nconvergence, the statistic has the same order of magnitude. We also study some\ncommon unstable cases, like the integrated seasonal process. Finally we build a\ngoodness-of-fit Bickel-Rosenblatt test for the true density of the noise\ntogether with its empirical properties on the basis of a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:37:50 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 19:29:25 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 16:33:36 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Lagnoux", "Agn\u00e8s", ""], ["Nguyen", "Thi Mong Ngoc", ""], ["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1706.09993", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan, Roman Vershynin", "title": "Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees", "comments": "Revised after comments from referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of phase retrieval, i.e. that of solving systems of\nquadratic equations. A simple variant of the randomized Kaczmarz method was\nrecently proposed for phase retrieval, and it was shown numerically to have a\ncomputational edge over state-of-the-art Wirtinger flow methods. In this paper,\nwe provide the first theoretical guarantee for the convergence of the\nrandomized Kaczmarz method for phase retrieval. We show that it is sufficient\nto have as many Gaussian measurements as the dimension, up to a constant\nfactor. Along the way, we introduce a sufficient condition on measurement sets\nfor which the randomized Kaczmarz method is guaranteed to work. We show that\nGaussian sampling vectors satisfy this property with high probability; this is\nproved using a chaining argument coupled with bounds on VC dimension and metric\nentropy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 01:21:55 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 20:02:48 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Tan", "Yan Shuo", ""], ["Vershynin", "Roman", ""]]}, {"id": "1706.10003", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan and Larry Wasserman", "title": "Hypothesis Testing For Densities and High-Dimensional Multinomials:\n  Sharp Local Minimax Rates", "comments": "60 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the goodness-of-fit testing problem of distinguishing whether the\ndata are drawn from a specified distribution, versus a composite alternative\nseparated from the null in the total variation metric. In the discrete case, we\nconsider goodness-of-fit testing when the null distribution has a possibly\ngrowing or unbounded number of categories. In the continuous case, we consider\ntesting a Lipschitz density, with possibly unbounded support, in the\nlow-smoothness regime where the Lipschitz parameter is not assumed to be\nconstant. In contrast to existing results, we show that the minimax rate and\ncritical testing radius in these settings depend strongly, and in a precise\nway, on the null distribution being tested and this motivates the study of the\n(local) minimax rate as a function of the null distribution. For multinomials\nthe local minimax rate was recently studied in the work of Valiant and Valiant.\nWe re-visit and extend their results and develop two modifications to the\nchi-squared test whose performance we characterize. For testing Lipschitz\ndensities, we show that the usual binning tests are inadequate in the\nlow-smoothness regime and we design a spatially adaptive partitioning scheme\nthat forms the basis for our locally minimax optimal tests. Furthermore, we\nprovide the first local minimax lower bounds for this problem which yield a\nsharp characterization of the dependence of the critical radius on the null\nhypothesis being tested. In the low-smoothness regime we also provide adaptive\ntests, that adapt to the unknown smoothness parameter. We illustrate our\nresults with a variety of simulations that demonstrate the practical utility of\nour proposed tests.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 02:34:12 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1706.10125", "submitter": "Loic Devilliers", "authors": "Lo\\\"ic Devilliers (ASCLEPIOS, UCA), St\\'ephanie Allassonni\\`ere (CRC),\n  Alain Trouv\\'e (CMLA), Xavier Pennec (ASCLEPIOS, UCA)", "title": "Inconsistency of Template Estimation by Minimizing of the\n  Variance/Pre-Variance in the Quotient Space", "comments": "arXiv admin note: text overlap with arXiv:1703.01232", "journal-ref": "Entropy, MDPI, 19 (6), pp.28 (2017)", "doi": "10.3390/e19060288", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of template estimation when data have been randomly\ndeformed under a group action in the presence of noise. In order to estimate\nthe template, one often minimizes the variance when the influence of the\ntransformations have been removed (computation of the Fr{\\'e}chet mean in the\nquotient space). The consistency bias is defined as the distance (possibly\nzero) between the orbit of the template and the orbit of one element which\nminimizes the variance. In the first part, we restrict ourselves to isometric\ngroup action, in this case the Hilbertian distance is invariant under the group\naction. We establish an asymptotic behavior of the consistency bias which is\nlinear with respect to the noise level. As a result the inconsistency is\nunavoidable as soon as the noise is enough. In practice, template estimation\nwith a finite sample is often done with an algorithm called \"max-max\". In the\nsecond part, also in the case of isometric group finite, we show the\nconvergence of this algorithm to an empirical Karcher mean. Our numerical\nexperiments show that the bias observed in practice can not be attributed to\nthe small sample size or to a convergence problem but is indeed due to the\npreviously studied inconsistency. In a third part, we also present some\ninsights of the case of a non invariant distance with respect to the group\naction. We will see that the inconsistency still holds as soon as the noise\nlevel is large enough. Moreover we prove the inconsistency even when a\nregularization term is added.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 11:20:01 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Devilliers", "Lo\u00efc", "", "ASCLEPIOS, UCA"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC"], ["Trouv\u00e9", "Alain", "", "CMLA"], ["Pennec", "Xavier", "", "ASCLEPIOS, UCA"]]}]