[{"id": "2102.00082", "submitter": "Sophie H. Yu", "authors": "Yihong Wu and Jiaming Xu and Sophie H. Yu", "title": "Settling the Sharp Reconstruction Thresholds of Random Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the problem of recovering the hidden vertex correspondence\nbetween two edge-correlated random graphs. We focus on the Gaussian model where\nthe two graphs are complete graphs with correlated Gaussian weights and the\nErd\\H{o}s-R\\'enyi model where the two graphs are subsampled from a common\nparent Erd\\H{o}s-R\\'enyi graph $\\mathcal{G}(n,p)$. For dense graphs with\n$p=n^{-o(1)}$, we prove that there exists a sharp threshold, above which one\ncan correctly match all but a vanishing fraction of vertices and below which\ncorrectly matching any positive fraction is impossible, a phenomenon known as\nthe \"all-or-nothing\" phase transition. Even more strikingly, in the Gaussian\nsetting, above the threshold all vertices can be exactly matched with high\nprobability. In contrast, for sparse Erd\\H{o}s-R\\'enyi graphs with\n$p=n^{-\\Theta(1)}$, we show that the all-or-nothing phenomenon no longer holds\nand we determine the thresholds up to a constant factor. Along the way, we also\nderive the sharp threshold for exact recovery, sharpening the existing results\nin Erd\\H{o}s-R\\'enyi graphs.\n  The proof of the negative results builds upon a tight characterization of the\nmutual information based on the truncated second-moment computation and an\n\"area theorem\" that relates the mutual information to the integral of the\nreconstruction error. The positive results follows from a tight analysis of the\nmaximum likelihood estimator that takes into account the cycle structure of the\ninduced permutation on the edges.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 21:49:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wu", "Yihong", ""], ["Xu", "Jiaming", ""], ["Yu", "Sophie H.", ""]]}, {"id": "2102.00102", "submitter": "Ivana Malenica", "authors": "Ivana Malenica, Aurelien Bibaut and Mark J. van der Laan", "title": "Adaptive Sequential Design for a Single Time-Series", "comments": "arXiv admin note: text overlap with arXiv:1809.00734", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The current work is motivated by the need for robust statistical methods for\nprecision medicine; as such, we address the need for statistical methods that\nprovide actionable inference for a single unit at any point in time. We aim to\nlearn an optimal, unknown choice of the controlled components of the design in\norder to optimize the expected outcome; with that, we adapt the randomization\nmechanism for future time-point experiments based on the data collected on the\nindividual over time. Our results demonstrate that one can learn the optimal\nrule based on a single sample, and thereby adjust the design at any point t\nwith valid inference for the mean target parameter. This work provides several\ncontributions to the field of statistical precision medicine. First, we define\na general class of averages of conditional causal parameters defined by the\ncurrent context for the single unit time-series data. We define a nonparametric\nmodel for the probability distribution of the time-series under few\nassumptions, and aim to fully utilize the sequential randomization in the\nestimation procedure via the double robust structure of the efficient influence\ncurve of the proposed target parameter. We present multiple\nexploration-exploitation strategies for assigning treatment, and methods for\nestimating the optimal rule. Lastly, we present the study of the data-adaptive\ninference on the mean under the optimal treatment rule, where the target\nparameter adapts over time in response to the observed context of the\nindividual. Our target parameter is pathwise differentiable with an efficient\ninfluence function that is doubly robust - which makes it easier to estimate\nthan previously proposed variations. We characterize the limit distribution of\nour estimator under a Donsker condition expressed in terms of a notion of\nbracketing entropy adapted to martingale settings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:51:45 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 15:16:45 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Malenica", "Ivana", ""], ["Bibaut", "Aurelien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2102.00185", "submitter": "Alexey Naumov", "authors": "Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Hoi-To\n  Wai", "title": "On the Stability of Random Matrix Product with Markovian Noise:\n  Application to Linear Stochastic Approximation and TD Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the exponential stability of random matrix products driven\nby a general (possibly unbounded) state space Markov chain. It is a cornerstone\nin the analysis of stochastic algorithms in machine learning (e.g. for\nparameter tracking in online learning or reinforcement learning). The existing\nresults impose strong conditions such as uniform boundedness of the\nmatrix-valued functions and uniform ergodicity of the Markov chains. Our main\ncontribution is an exponential stability result for the $p$-th moment of random\nmatrix product, provided that (i) the underlying Markov chain satisfies a\nsuper-Lyapunov drift condition, (ii) the growth of the matrix-valued functions\nis controlled by an appropriately defined function (related to the drift\ncondition). Using this result, we give finite-time $p$-th moment bounds for\nconstant and decreasing stepsize linear stochastic approximation schemes with\nMarkovian noise on general state space. We illustrate these findings for linear\nvalue-function estimation in reinforcement learning. We provide finite-time\n$p$-th moment bound for various members of temporal difference (TD) family of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 08:39:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Naumov", "Alexey", ""], ["Samsonov", "Sergey", ""], ["Wai", "Hoi-To", ""]]}, {"id": "2102.00199", "submitter": "Nikita Puchkin", "authors": "Denis Belomestny, Eric Moulines, Alexey Naumov, Nikita Puchkin, and\n  Sergey Samsonov", "title": "Rates of convergence for density estimation with GANs", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We undertake a precise study of the non-asymptotic properties of vanilla\ngenerative adversarial networks (GANs) and derive theoretical guarantees in the\nproblem of estimating an unknown $d$-dimensional density $p^*$ under a proper\nchoice of the class of generators and discriminators. We prove that the\nresulting density estimate converges to $p^*$ in terms of Jensen-Shannon (JS)\ndivergence at the rate $(\\log n/n)^{2\\beta/(2\\beta+d)}$ where $n$ is the sample\nsize and $\\beta$ determines the smoothness of $p^*.$ This is the first result\nin the literature on density estimation using vanilla GANs with JS rates faster\nthan $n^{-1/2}$ in the regime $\\beta>d/2.$\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 09:59:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Belomestny", "Denis", ""], ["Moulines", "Eric", ""], ["Naumov", "Alexey", ""], ["Puchkin", "Nikita", ""], ["Samsonov", "Sergey", ""]]}, {"id": "2102.00327", "submitter": "Ming Zhong", "authors": "Mauro Maggioni, Jason Miller, Hongda Qiu, Ming Zhong", "title": "Learning Interaction Kernels for Agent Systems on Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting agent and particle systems are extensively used to model complex\nphenomena in science and engineering. We consider the problem of learning\ninteraction kernels in these dynamical systems constrained to evolve on\nRiemannian manifolds from given trajectory data. The models we consider are\nbased on interaction kernels depending on pairwise Riemannian distances between\nagents, with agents interacting locally along the direction of the shortest\ngeodesic connecting them. We show that our estimators converge at a rate that\nis independent of the dimension of the state space, and derive bounds on the\ntrajectory estimation error, on the manifold, between the observed and\nestimated dynamics. We demonstrate the performance of our estimator on two\nclassical first order interacting systems: Opinion Dynamics and a\nPredator-Swarm system, with each system constrained on two prototypical\nmanifolds, the $2$-dimensional sphere and the Poincar\\'e disk model of\nhyperbolic space.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 22:15:50 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:38:19 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 18:11:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Maggioni", "Mauro", ""], ["Miller", "Jason", ""], ["Qiu", "Hongda", ""], ["Zhong", "Ming", ""]]}, {"id": "2102.00356", "submitter": "Johannes Wiesel", "authors": "Johannes Wiesel", "title": "Measuring association with Wasserstein distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\pi\\in \\Pi(\\mu,\\nu)$ be a coupling between two probability measures\n$\\mu$ and $\\nu$ on a Polish space. In this article we propose and study a class\nof nonparametric measures of association between $\\mu$ and $\\nu$. The analysis\nis based on the Wasserstein distance between $\\nu$ and the disintegration\n$\\pi_{x_1}$ of $\\pi$ with respect to the first coordinate. We also establish\nbasic statistical properties of this new class of measures: we develop a\nstatistical theory for strongly consistent estimators and determine their\nconvergence rate. Throughout our analysis we make use of the so-called\nadapted/causal Wasserstein distance, in particular we rely on results\nestablished in [Backhoff, Bartl, Beiglb\\\"ock, Wiesel. Estimating processes in\nadapted Wasserstein distance. 2020]. Our class of measures offers on\nalternative to the correlation coefficient proposed by [Dette, Siburg and\nStoimenov (2013). A copula-based non-parametric measure of regression\ndependence. Scandinavian Journal of Statistics 40(1), 21-41] and [Chatterjee\n(2020). A new coefficient of correlation. Journal of the American Statistical\nAssociation, 1-21]. In contrast to these works, our approach also applies to\nprobability laws on general Polish spaces.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 02:28:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wiesel", "Johannes", ""]]}, {"id": "2102.00366", "submitter": "John O'Leary", "authors": "John O'Leary, Guanyang Wang", "title": "Transition kernel couplings of the Metropolis-Hastings algorithm", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Couplings play a central role in the analysis of Markov chain convergence to\nstationarity and in the construction of novel Markov chain Monte Carlo\ndiagnostics, estimators, and variance reduction techniques. The quality of the\nresulting bounds or methods typically depends on how quickly the coupling\ninduces meeting between chains, a property sometimes referred to as its\nefficiency. The design of efficient Markovian couplings remains a difficult\nopen question, especially for discrete time processes. In pursuit of this goal,\nin this paper we fully characterize the couplings of the Metropolis--Hastings\n(MH) transition kernel, providing necessary and sufficient conditions in terms\nof the underlying proposal and acceptance distributions. We apply these results\nto characterize the set of maximal couplings of the MH kernel, resolving open\nquestions posed in O'Leary et al. [2020] on the structure and properties of\nthese couplings. These results represent an advance in the understanding of the\nMH kernel and a step toward the formulation of efficient couplings for this\npopular family of algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 03:45:57 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 18:45:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["O'Leary", "John", ""], ["Wang", "Guanyang", ""]]}, {"id": "2102.00384", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Miaoyan Wang", "title": "Beyond the Signs: Nonparametric Tensor Completion via Sign Series", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tensor estimation from noisy observations with\npossibly missing entries. A nonparametric approach to tensor completion is\ndeveloped based on a new model which we coin as sign representable tensors. The\nmodel represents the signal tensor of interest using a series of structured\nsign tensors. Unlike earlier methods, the sign series representation\neffectively addresses both low- and high-rank signals, while encompassing many\nexisting tensor models -- including CP models, Tucker models, single index\nmodels, several hypergraphon models -- as special cases. We show that the sign\ntensor series is theoretically characterized, and computationally estimable,\nvia classification tasks with carefully-specified weights. Excess risk bounds,\nestimation error rates, and sample complexities are established. We demonstrate\nthe outperformance of our approach over previous methods on two datasets, one\non human brain connectivity networks and the other on topic data mining.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 05:27:01 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lee", "Chanwoo", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2102.00451", "submitter": "Nikita Zhivotovskiy", "authors": "Nikita Puchkin and Nikita Zhivotovskiy", "title": "Exponential Savings in Agnostic Active Learning through Abstention", "comments": "27 pages, additional clarifications added and several typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that in pool-based active classification without assumptions on the\nunderlying distribution, if the learner is given the power to abstain from some\npredictions by paying the price marginally smaller than the average loss $1/2$\nof a random guess, exponential savings in the number of label requests are\npossible whenever they are possible in the corresponding realizable problem. We\nextend this result to provide a necessary and sufficient condition for\nexponential savings in pool-based active classification under the model\nmisspecification.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 13:43:14 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:24:12 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Puchkin", "Nikita", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2102.00618", "submitter": "Omer Tamuz", "authors": "Xiaosheng Mu, Luciano Pomatto, Philipp Strack, Omer Tamuz", "title": "Monotone additive statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation is an example of a descriptive statistic that is monotone\nwith respect to stochastic dominance, and additive for sums of independent\nrandom variables. We provide a complete characterization of such statistics,\nand explore a number of applications to models of individual and group\ndecision-making. These include a representation of stationary, monotone time\npreferences, extending the work of Fishburn and Rubinstein (1982) to time\nlotteries, as well as a characterization of risk-averse preferences over\nmonetary gambles that are invariant to mean-zero background risks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 03:43:06 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:21:09 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 14:49:27 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Mu", "Xiaosheng", ""], ["Pomatto", "Luciano", ""], ["Strack", "Philipp", ""], ["Tamuz", "Omer", ""]]}, {"id": "2102.00630", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Johannes Ruf, Martin Larsson, Wouter Koolen", "title": "Testing exchangeability: fork-convexity, supermartingales, and\n  e-processes", "comments": "34 pages, 7 figures, accepted at the International Journal of\n  Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we observe an infinite series of coin flips $X_1,X_2,\\ldots$, and\nwish to sequentially test the null that these binary random variables are\nexchangeable. Nonnegative supermartingales (NSMs) are a workhorse of sequential\ninference, but we prove that they are powerless for this problem. First,\nutilizing a geometric concept called fork-convexity (a sequential analog of\nconvexity), we show that any process that is an NSM under a set of\ndistributions, is also necessarily an NSM under their \"fork-convex hull\".\nSecond, we demonstrate that the fork-convex hull of the exchangeable null\nconsists of all possible laws over binary sequences; this implies that any NSM\nunder exchangeability is necessarily nonincreasing, hence always yields a\npowerless test for any alternative. Since testing arbitrary deviations from\nexchangeability is information theoretically impossible, we focus on Markovian\nalternatives. We combine ideas from universal inference and the method of\nmixtures to derive a \"safe e-process\", which is a nonnegative process with\nexpectation at most one under the null at any stopping time, and is upper\nbounded by a martingale, but is not itself an NSM. This in turn yields a level\n$\\alpha$ sequential test that is consistent; regret bounds from universal\ncoding also demonstrate rate-optimal power. We present ways to extend these\nresults to any finite alphabet and to Markovian alternatives of any order using\na \"double mixture\" approach. We provide an array of simulations, and give\ngeneral approaches based on betting for unstructured or ill-specified\nalternatives. Finally, inspired by Shafer, Vovk, and Ville, we provide\ngame-theoretic interpretations of our e-processes and pathwise results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:31:44 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 23:31:09 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 14:51:43 GMT"}, {"version": "v4", "created": "Fri, 23 Jul 2021 17:02:11 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Ruf", "Johannes", ""], ["Larsson", "Martin", ""], ["Koolen", "Wouter", ""]]}, {"id": "2102.00720", "submitter": "Amedeo Esposito", "authors": "Amedeo Roberto Esposito, Diyuan Wu, Michael Gastpar", "title": "On conditional Sibson's $\\alpha$-Mutual Information", "comments": "Accepted for Publication by ISIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyse how to define a conditional version of Sibson's\n$\\alpha$-Mutual Information. Several such definitions can be advanced and they\nall lead to different information measures with different (but similar)\noperational meanings. We will analyse in detail one such definition, compute a\nclosed-form expression for it and endorse it with an operational meaning while\nalso considering some applications. The alternative definitions will also be\nmentioned and compared.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:30:42 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 08:42:34 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 10:15:02 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 07:32:10 GMT"}, {"version": "v5", "created": "Wed, 12 May 2021 06:51:45 GMT"}, {"version": "v6", "created": "Tue, 8 Jun 2021 11:27:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Esposito", "Amedeo Roberto", ""], ["Wu", "Diyuan", ""], ["Gastpar", "Michael", ""]]}, {"id": "2102.00725", "submitter": "Yi Yu", "authors": "Anne Gael Manegueu, Alexandra Carpentier and Yi Yu", "title": "Generalized non-stationary bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a non-stationary stochastic bandit problem, which\ngeneralizes the switching bandit problem. On top of the switching bandit\nproblem (\\textbf{Case a}), we are interested in three concrete examples:\n(\\textbf{b}) the means of the arms are local polynomials, (\\textbf{c}) the\nmeans of the arms are locally smooth, and (\\textbf{d}) the gaps of the arms\nhave a bounded number of inflexion points and where the highest arm mean cannot\nvary too much in a short range. These three settings are very different, but\nhave in common the following: (i) the number of similarly-sized level sets of\nthe logarithm of the gaps can be controlled, and (ii) the highest mean has a\nlimited number of abrupt changes, and otherwise has limited variations. We\npropose a single algorithm in this general setting, that in particular solves\nin an efficient and unified way the four problems (a)-(d) mentioned.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:34:44 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 08:13:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Manegueu", "Anne Gael", ""], ["Carpentier", "Alexandra", ""], ["Yu", "Yi", ""]]}, {"id": "2102.00729", "submitter": "Olivier Wintenberger", "authors": "Olivier Wintenberger (LPSM (UMR\\_8001))", "title": "Stochastic Online Convex Optimization; Application to probabilistic time\n  series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic regret bounds for online algorithms are usually derived from an\n''online to batch'' conversion. Inverting the reasoning, we start our analyze\nby a ''batch to online'' conversion that applies in any Stochastic Online\nConvex Optimization problem under stochastic exp-concavity condition. We obtain\nfast rate stochastic regret bounds with high probability for non-convex loss\nfunctions. Based on this approach, we provide prediction and probabilistic\nforecasting methods for non-stationary unbounded time series.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 09:49:15 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 10:02:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2102.00760", "submitter": "Vivien Cabannes", "authors": "Vivien Cabannes and Alessandro Rudi and Francis Bach", "title": "Fast rates in structured prediction", "comments": "14 main pages, 3 main figures, 43 pages, 4 figures (with appendix)", "journal-ref": "Conference on Learning Theory, PMLR 134, 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discrete supervised learning problems such as classification are often\ntackled by introducing a continuous surrogate problem akin to regression.\nBounding the original error, between estimate and solution, by the surrogate\nerror endows discrete problems with convergence rates already shown for\ncontinuous instances. Yet, current approaches do not leverage the fact that\ndiscrete problems are essentially predicting a discrete output when continuous\nproblems are predicting a continuous value. In this paper, we tackle this issue\nfor general structured prediction problems, opening the way to \"super fast\"\nrates, that is, convergence rates for the excess risk faster than $n^{-1}$,\nwhere $n$ is the number of observations, with even exponential rates with the\nstrongest assumptions. We first illustrate it for predictors based on nearest\nneighbors, generalizing rates known for binary classification to any discrete\nproblem within the framework of structured prediction. We then consider kernel\nridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast\nrates, depending on a parameter characterizing the hardness of the problem,\nthus allowing, under smoothness assumptions, to bypass the curse of\ndimensionality.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 10:50:04 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 13:02:31 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 15:04:41 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Cabannes", "Vivien", ""], ["Rudi", "Alessandro", ""], ["Bach", "Francis", ""]]}, {"id": "2102.00929", "submitter": "Kweku Abraham", "authors": "Kweku Abraham, Ismael Castillo, Etienne Roquain", "title": "Empirical Bayes cumulative $\\ell$-value multiple testing procedure for\n  sparse sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the sparse sequence model, we consider a popular Bayesian multiple testing\nprocedure and investigate for the first time its behaviour from the frequentist\npoint of view. Given a spike-and-slab prior on the high-dimensional sparse\nunknown parameter, one can easily compute posterior probabilities of coming\nfrom the spike, which correspond to the well known local-fdr values, also\ncalled $\\ell$-values. The spike-and-slab weight parameter is calibrated in an\nempirical Bayes fashion, using marginal maximum likelihood. The multiple\ntesting procedure under study, called here the cumulative $\\ell$-value\nprocedure, ranks coordinates according to their empirical $\\ell$-values and\nthresholds so that the cumulative ranked sum does not exceed a user-specified\nlevel $t$.\n  We validate the use of this method from the multiple testing perspective: for\nalternatives of appropriately large signal strength, the false discovery rate\n(FDR) of the procedure is shown to converge to the target level $t$, while its\nfalse negative rate (FNR) goes to $0$. We complement this study by providing\nconvergence rates for the method. Additionally, we prove that the $q$-value\nmultiple testing procedure shares similar convergence rates in this model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 15:57:47 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 10:52:33 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Abraham", "Kweku", ""], ["Castillo", "Ismael", ""], ["Roquain", "Etienne", ""]]}, {"id": "2102.00968", "submitter": "Jonathan Berrisch", "authors": "Jonathan Berrisch, Florian Ziel", "title": "CRPS Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combination and aggregation techniques can improve forecast accuracy\nsubstantially. This also holds for probabilistic forecasting methods where full\npredictive distributions are combined. There are several time-varying and\nadaptive weighting schemes like Bayesian model averaging (BMA). However, the\nperformance of different forecasters may vary not only over time but also in\nparts of the distribution. So one may be more accurate in the center of the\ndistributions, and other ones perform better in predicting the distribution's\ntails. Consequently, we introduce a new weighting procedure that considers both\nvarying performance across time and the distribution. We discuss pointwise\nonline aggregation algorithms that optimize with respect to the continuous\nranked probability score (CRPS). After analyzing the theoretical properties of\na fully adaptive Bernstein online aggregation (BOA) method, we introduce\nsmoothing procedures for pointwise CRPS learning. The properties are confirmed\nand discussed using simulation studies. Additionally, we illustrate the\nperformance in a forecasting study for carbon markets. In detail, we predict\nthe distribution of European emission allowance prices.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:54:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Berrisch", "Jonathan", ""], ["Ziel", "Florian", ""]]}, {"id": "2102.00995", "submitter": "Guillaume Lecu\\'e", "authors": "Jules Depersin and Guillaume Lecu\\'e", "title": "Optimal robust mean and location estimation via convex programs with\n  respect to any pseudo-norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust mean and location estimation w.r.t. any\npseudo-norm of the form $x\\in\\mathbb{R}^d\\to ||x||_S = \\sup_{v\\in S}<v,x>$\nwhere $S$ is any symmetric subset of $\\mathbb{R}^d$. We show that the\ndeviation-optimal minimax subgaussian rate for confidence $1-\\delta$ is $$\n  \\max\\left(\\frac{l^*(\\Sigma^{1/2}S)}{\\sqrt{N}}, \\sup_{v\\in\nS}||\\Sigma^{1/2}v||_2\\sqrt{\\frac{\\log(1/\\delta)}{N}}\\right) $$where\n$l^*(\\Sigma^{1/2}S)$ is the Gaussian mean width of $\\Sigma^{1/2}S$ and $\\Sigma$\nthe covariance of the data (in the benchmark i.i.d. Gaussian case). This\nimproves the entropic minimax lower bound from [Lugosi and Mendelson, 2019] and\ncloses the gap characterized by Sudakov's inequality between the entropy and\nthe Gaussian mean width for this problem. This shows that the right statistical\ncomplexity measure for the mean estimation problem is the Gaussian mean width.\nWe also show that this rate can be achieved by a solution to a convex\noptimization problem in the adversarial and $L_2$ heavy-tailed setup by\nconsidering minimum of some Fenchel-Legendre transforms constructed using the\nMedian-of-means principle. We finally show that this rate may also be achieved\nin situations where there is not even a first moment but a location parameter\nexists.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 17:19:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Depersin", "Jules", ""], ["Lecu\u00e9", "Guillaume", ""]]}, {"id": "2102.01037", "submitter": "Xavier Loizeau", "authors": "Jan Johannes, Xavier Loizeau", "title": "Data-driven aggregation in circular deconvolution", "comments": "19 pages, 4 figures, 27 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a circular deconvolution model we consider the fully data driven density\nestimation of a circular random variable where the density of the additive\nindependent measurement error is unknown. We have at hand two independent iid\nsamples, one of the contaminated version of the variable of interest, and the\nother of the additive noise. We show optimality,in an oracle and minimax sense,\nof a fully data-driven weighted sum of orthogonal series density estimators.\nTwo shapes of random weights are considered, one motivated by a Bayesian\napproach and the other by a well known model selection method. We derive\nnon-asymptotic upper bounds for the quadratic risk and the maximal quadratic\nrisk over Sobolev-like ellipsoids of the fully data-driven estimator. We\ncompute rates which can be obtained in different configurations for the\nsmoothness of the density of interest and the error density. The rates\n(strictly) match the optimal oracle or minimax rates for a large variety of\ncases, and feature otherwise at most a deterioration by a logarithmic factor.\nWe illustrate the performance of the fully data-driven weighted sum of\northogonal series estimators by a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:14:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Johannes", "Jan", ""], ["Loizeau", "Xavier", ""]]}, {"id": "2102.01121", "submitter": "Arya Akhavan", "authors": "Arya Akhavan, Massimiliano Pontil, Alexandre B. Tsybakov", "title": "Distributed Zero-Order Optimization under Adversarial Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distributed zero-order optimization for a class of\nstrongly convex functions. They are formed by the average of local objectives,\nassociated to different nodes in a prescribed network of connections. We\npropose a distributed zero-order projected gradient descent algorithm to solve\nthis problem. Exchange of information within the network is permitted only\nbetween neighbouring nodes. A key feature of the algorithm is that it can query\nonly function values, subject to a general noise model, that does not require\nzero mean or independent errors. We derive upper bounds for the average\ncumulative regret and optimization error of the algorithm which highlight the\nrole played by a network connectivity parameter, the number of variables, the\nnoise level, the strong convexity parameter of the global objective and certain\nsmoothness properties of the local objectives. When the bound is specified to\nthe standard undistributed setting, we obtain an improvement over the\nstate-of-the-art bounds, due to the novel gradient estimation procedure\nproposed here. We also comment on lower bounds and observe that the dependency\nover certain function parameters in the bound is nearly optimal.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 19:27:54 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 17:23:10 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 09:57:19 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 22:08:19 GMT"}, {"version": "v5", "created": "Mon, 7 Jun 2021 17:14:27 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 13:25:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Akhavan", "Arya", ""], ["Pontil", "Massimiliano", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "2102.01291", "submitter": "Pedro H. C. Sant'Anna", "authors": "Jonathan Roth and Pedro H. C. Sant'Anna", "title": "Efficient Estimation for Staggered Rollout Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies efficient estimation of causal effects when treatment is\n(quasi-) randomly rolled out to units at different points in time. We solve for\nthe most efficient estimator in a class of estimators that nests two-way fixed\neffects models and other popular generalized difference-in-differences methods.\nA feasible plug-in version of the efficient estimator is asymptotically\nunbiased with efficiency (weakly) dominating that of existing approaches. We\nprovide both $t$-based and permutation-test based methods for inference. We\nillustrate the performance of the plug-in efficient estimator in simulations\nand in an application to Wood et al. (2020a)'s study of the staggered rollout\nof a procedural justice training program for police officers. We find that\nconfidence intervals based on the plug-in efficient estimator have good\ncoverage and can be as much as five times shorter than confidence intervals\nbased on existing state-of-the-art methods. As an empirical contribution of\nindependent interest, our application provides the most precise estimates to\ndate on the effectiveness of procedural justice training programs for police\nofficers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:04:18 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 22:13:19 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 01:49:24 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Roth", "Jonathan", ""], ["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "2102.01306", "submitter": "Alexander Tartakovsky", "authors": "Alexander G. Tartakovsky", "title": "An Asymptotic Theory of Joint Sequential Changepoint Detection and\n  Identification for General Stochastic Models", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper addresses a joint sequential changepoint detection and\nidentification/isolation problem for a general stochastic model, assuming that\nthe observed data may be dependent and non-identically distributed, the prior\ndistribution of the change point is arbitrary, and the post-change hypotheses\nare composite. The developed detection-identification theory generalizes the\nchangepoint detection theory developed by Tartakovsky (2019) to the case of\nmultiple composite post-change hypotheses when one has not only to detect a\nchange as quickly as possible but also to identify (or isolate) the true\npost-change distribution. We propose a multi-hypothesis change\ndetection-identification rule and show that it is nearly optimal, minimizing\nmoments of the delay to detection as the probability of a false alarm and the\nprobabilities of misidentification go to zero.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:36:39 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 03:35:37 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Tartakovsky", "Alexander G.", ""]]}, {"id": "2102.01310", "submitter": "Alexander Tartakovsky", "authors": "Alexander G. Tartakovsky, Nikita R. Berenkov, Alexei E. Kolessa, and\n  Igor V. Nikiforov", "title": "Optimal Sequential Detection of Signals with Unknown Appearance and\n  Disappearance Points in Time", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2021.3071016", "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper addresses a sequential changepoint detection problem, assuming that\nthe duration of change may be finite and unknown. This problem is of importance\nfor many applications, e.g., for signal and image processing where signals\nappear and disappear at unknown points in time or space. In contrast to the\nconventional optimality criterion in quickest change detection that requires\nminimization of the expected delay to detection for a given average run length\nto a false alarm, we focus on a reliable maximin change detection criterion of\nmaximizing the minimal probability of detection in a given time (or space)\nwindow for a given local maximal probability of false alarm in the prescribed\nwindow. We show that the optimal detection procedure is a modified CUSUM\nprocedure. We then compare operating characteristics of this optimal procedure\nwith popular in engineering the Finite Moving Average (FMA) detection algorithm\nand the ordinary CUSUM procedure using Monte Carlo simulations, which show that\ntypically the later algorithms have almost the same performance as the optimal\none. At the same time, the FMA procedure has a substantial advantage --\nindependence to the intensity of the signal, which is usually unknown. Finally,\nthe FMA algorithm is applied to detecting faint streaks of satellites in\noptical images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 04:58:57 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 03:18:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tartakovsky", "Alexander G.", ""], ["Berenkov", "Nikita R.", ""], ["Kolessa", "Alexei E.", ""], ["Nikiforov", "Igor V.", ""]]}, {"id": "2102.01541", "submitter": "Tatiana Brailovskaya", "authors": "Tatiana Brailovskaya, Mikl\\'os Z. R\\'acz", "title": "Tree trace reconstruction using subtraces", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree trace reconstruction aims to learn the binary node labels of a tree,\ngiven independent samples of the tree passed through an appropriately defined\ndeletion channel. In recent work, Davies, R\\'acz, and Rashtchian used\ncombinatorial methods to show that $\\exp(\\mathcal{O}(k \\log_{k} n))$ samples\nsuffice to reconstruct a complete $k$-ary tree with $n$ nodes with high\nprobability. We provide an alternative proof of this result, which allows us to\ngeneralize it to a broader class of tree topologies and deletion models. In our\nproofs, we introduce the notion of a subtrace, which enables us to connect with\nand generalize recent mean-based complex analytic algorithms for string trace\nreconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 15:13:04 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Brailovskaya", "Tatiana", ""], ["R\u00e1cz", "Mikl\u00f3s Z.", ""]]}, {"id": "2102.01616", "submitter": "Nakahiro Yoshida", "authors": "Yuliya Mishura and Nakahiro Yoshida", "title": "Divergence of an integral of a process with small ball estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper contains sufficient conditions on the function $f$ and the\nstochastic process $X$ that supply the rate of divergence of the integral\nfunctional $\\int_0^Tf(X_t)^2dt$ at the rate $T^{1-\\epsilon}$ as $T\\to\\infty$\nfor every $\\epsilon>0$. These conditions include so called small ball estimates\nwhich are discussed in detail. Statistical applications are provided.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:18:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Mishura", "Yuliya", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "2102.01784", "submitter": "Scott Bruce", "authors": "Pramita Bagchi and Scott A. Bruce", "title": "Adaptive Frequency Band Analysis for Functional Time Series", "comments": "33 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The frequency-domain properties of nonstationary functional time series often\ncontain valuable information. These properties are characterized through its\ntime-varying power spectrum. Practitioners seeking low-dimensional summary\nmeasures of the power spectrum often partition frequencies into bands and\ncreate collapsed measures of power within bands. However, standard frequency\nbands have largely been developed through manual inspection of time series data\nand may not adequately summarize power spectra. In this article, we propose a\nframework for adaptive frequency band estimation of nonstationary functional\ntime series that optimally summarizes the time-varying dynamics of the series.\nWe develop a scan statistic and search algorithm to detect changes in the\nfrequency domain. We establish theoretical properties of this framework and\ndevelop a computationally-efficient implementation. The validity of our method\nis also justified through numerous simulation studies and an application to\nanalyzing electroencephalogram data in participants alternating between eyes\nopen and eyes closed conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 22:33:37 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:19:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Bagchi", "Pramita", ""], ["Bruce", "Scott A.", ""]]}, {"id": "2102.01938", "submitter": "Prafulla Chandra Mr", "authors": "Prafulla Chandra, Andrew Thangaraj and Nived Rajaraman", "title": "Missing Mass of Rank-2 Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimation of missing mass with the popular Good-Turing (GT) estimator is\nwell-understood in the case where samples are independent and identically\ndistributed (iid). In this article, we consider the same problem when the\nsamples come from a stationary Markov chain with a rank-2 transition matrix,\nwhich is one of the simplest extensions of the iid case. We develop an upper\nbound on the absolute bias of the GT estimator in terms of the spectral gap of\nthe chain and a tail bound on the occupancy of states. Borrowing tail bounds\nfrom known concentration results for Markov chains, we evaluate the bound using\nother parameters of the chain. The analysis, supported by simulations, suggests\nthat, for rank-2 irreducible chains, the GT estimator has bias and mean-squared\nerror falling with number of samples at a rate that depends loosely on the\nconnectivity of the states in the chain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:38:21 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 09:22:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chandra", "Prafulla", ""], ["Thangaraj", "Andrew", ""], ["Rajaraman", "Nived", ""]]}, {"id": "2102.01943", "submitter": "Efstathios Paparoditis", "authors": "Marco Meyer and Efstathios Paparoditis", "title": "A Frequency Domain Bootstrap for General Multivariate Stationary\n  Processes", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many relevant statistics of multivariate time series, no valid frequency\ndomain bootstrap procedures exist. This is mainly due to the fact that the\ndistribution of such statistics depends on the fourth-order moment structure of\nthe underlying process in nearly every scenario, except for some special cases\nlike Gaussian time series. In contrast to the univariate case, even additional\nstructural assumptions such as linearity of the multivariate process or a\nstandardization of the statistic of interest do not solve the problem. This\npaper focuses on integrated periodogram statistics as well as functions thereof\nand presents a new frequency domain bootstrap procedure for multivariate time\nseries, the multivariate frequency domain hybrid bootstrap (MFHB), to fill this\ngap. Asymptotic validity of the MFHB procedure is established for general\nclasses of periodogram-based statistics and for stationary multivariate\nprocesses satisfying rather weak dependence conditions. A simulation study is\ncarried out which compares the finite sample performance of the MFHB with that\nof the moving block bootstrap.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 08:43:35 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Meyer", "Marco", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2102.01977", "submitter": "Tommaso Cesari", "authors": "Fran\\c{c}ois Bachoc (IMT, GdR MASCOT-NUM), Tommaso Cesari (TSE),\n  S\\'ebastien Gerchinovitz (IMT)", "title": "Instance-Dependent Bounds for Zeroth-order Lipschitz Optimization with\n  Error Certificates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of zeroth-order (black-box) optimization of a Lipschitz\nfunction $f$ defined on a compact subset $\\mathcal X$ of $\\mathbb R^d$, with\nthe additional constraint that algorithms must certify the accuracy of their\nrecommendations. We characterize the optimal number of evaluations of any\nLipschitz function $f$ to find and certify an approximate maximizer of $f$ at\naccuracy $\\varepsilon$. Under a weak assumption on $\\mathcal X$, this optimal\nsample complexity is shown to be nearly proportional to the integral\n$\\int_{\\mathcal X} \\mathrm{d}\\boldsymbol x/( \\max(f) - f(\\boldsymbol x) +\n\\varepsilon )^d$. This result, which was only (and partially) known in\ndimension $d=1$, solves an open problem dating back to 1991. In terms of\ntechniques, our upper bound relies on a slightly improved analysis of the DOO\nalgorithm that we adapt to the certified setting and then link to the above\nintegral. Our instance-dependent lower bound differs from traditional\nworst-case lower bounds in the Lipschitz setting and relies on a local\nworst-case analysis that could likely prove useful for other learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 09:51:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 09:01:28 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 14:16:24 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 08:19:36 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT, GdR MASCOT-NUM"], ["Cesari", "Tommaso", "", "TSE"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT"]]}, {"id": "2102.02171", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart and Yuxin\n  Sun", "title": "Outlier-Robust Learning of Ising Models Under Dobrushin's Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Ising models satisfying Dobrushin's\ncondition in the outlier-robust setting where a constant fraction of the\nsamples are adversarially corrupted. Our main result is to provide the first\ncomputationally efficient robust learning algorithm for this problem with\nnear-optimal error guarantees. Our algorithm can be seen as a special case of\nan algorithm for robustly learning a distribution from a general exponential\nfamily. To prove its correctness for Ising models, we establish new\nanti-concentration results for degree-$2$ polynomials of Ising models that may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 18:00:57 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""], ["Sun", "Yuxin", ""]]}, {"id": "2102.02339", "submitter": "Wenpin Tang", "authors": "Wenpin Tang and Xun Yu Zhou", "title": "Simulated annealing from continuum to discretization: a convergence\n  analysis via the Eyring--Kramers law", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the convergence rate of continuous-time simulated annealing $(X_t;\n\\, t \\ge 0)$ and its discretization $(x_k; \\, k =0,1, \\ldots)$ for\napproximating the global optimum of a given function $f$. We prove that the\ntail probability $\\mathbb{P}(f(X_t) > \\min f +\\delta)$ (resp.\n$\\mathbb{P}(f(x_k) > \\min f +\\delta)$) decays polynomial in time (resp. in\ncumulative step size), and provide an explicit rate as a function of the model\nparameters. Our argument applies the recent development on functional\ninequalities for the Gibbs measure at low temperatures -- the Eyring-Kramers\nlaw. In the discrete setting, we obtain a condition on the step size to ensure\nthe convergence.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:45:39 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 06:12:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Tang", "Wenpin", ""], ["Zhou", "Xun Yu", ""]]}, {"id": "2102.02450", "submitter": "Haoyu Wei", "authors": "Huiming Zhang, Haoyu Wei", "title": "Sharper Sub-Weibull Concentrations: Non-asymptotic Bai-Yin Theorem", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arising in high-dimensional probability, non-asymptotic concentration\ninequalities play an essential role in the finite-sample theory of machine\nlearning and high-dimensional statistics. In this article, we obtain a sharper\nand constants-specified concentration inequality for the summation of\nindependent sub-Weibull random variables, which leads to a mixture of two\ntails: sub-Gaussian for small deviations and sub-Weibull for large deviations\n(from mean). These bounds improve existing bounds with sharper constants. In\nthe application of random matrices, we derive non-asymptotic versions of\nBai-Yin's theorem for sub-Weibull entries and it extends the previous result in\nterms of sub-Gaussian entries. In the application of negative binomial\nregressions, we gives the $\\ell_2$-error of the estimated coefficients when\ncovariate vector $X$ is sub-Weibull distributed with sparse structures, which\nis a new result for negative binomial regressions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 07:16:27 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 16:10:12 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhang", "Huiming", ""], ["Wei", "Haoyu", ""]]}, {"id": "2102.02504", "submitter": "Pierre Alquier", "authors": "Dimitri Meunier and Pierre Alquier", "title": "Meta-strategy for Learning Tuning Parameters with Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online gradient methods, like the online gradient algorithm (OGA), often\ndepend on tuning parameters that are difficult to set in practice. We consider\nan online meta-learning scenario, and we propose a meta-strategy to learn these\nparameters from past tasks. Our strategy is based on the minimization of a\nregret bound. It allows to learn the initialization and the step size in OGA\nwith guarantees. We provide a regret analysis of the strategy in the case of\nconvex losses. It suggests that, when there are parameters\n$\\theta_1,\\dots,\\theta_T$ solving well tasks $1,\\dots,T$ respectively and that\nare close enough one to each other, our strategy indeed improves on learning\neach task in isolation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:32:22 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:03:58 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Meunier", "Dimitri", ""], ["Alquier", "Pierre", ""]]}, {"id": "2102.02572", "submitter": "Juan A. Cuesta-Albertos", "authors": "E. del Barrio, J.A. Cuesta-Albertos and C. Matran", "title": "The complex behaviour of Galton rank order statistic", "comments": "35 pages. No figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Galton's rank order statistic is one of the oldest statistical tools for\ntwo-sample comparisons. It is also a very natural index to measure departures\nfrom stochastic dominance. Yet, its asymptotic behaviour has been investigated\nonly partially, under restrictive assumptions. This work provides a\ncomprehensive {study} of this behaviour, based on the analysis of the so-called\ncontact set (a modification of the set in which the quantile functions\ncoincide). We show that a.s. convergence to the population counterpart holds if\nand only if {the} contact set has zero Lebesgue measure. When this set is\nfinite we show that the asymptotic behaviour is determined by the local\nbehaviour of a suitable reparameterization of the quantile functions in a\nneighbourhood of the contact points. Regular crossings result in standard rates\nand Gaussian limiting distributions, but higher order contacts (in the sense\nintroduced in this work) or contacts at the extremes of the supports may result\nin different rates and non-Gaussian limits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:24:04 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matran", "C.", ""]]}, {"id": "2102.02580", "submitter": "Yuan Gao Dr.", "authors": "Yuan Gao, Han Lin Shang, Yanrong Yang", "title": "Factor-augmented Smoothing Model for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose modeling raw functional data as a mixture of a smooth function and\na highdimensional factor component. The conventional approach to retrieving the\nsmooth function from the raw data is through various smoothing techniques.\nHowever, the smoothing model is not adequate to recover the smooth curve or\ncapture the data variation in some situations. These include cases where there\nis a large amount of measurement error, the smoothing basis functions are\nincorrectly identified, or the step jumps in the functional mean levels are\nneglected. To address these challenges, a factor-augmented smoothing model is\nproposed, and an iterative numerical estimation approach is implemented in\npractice. Including the factor model component in the proposed method solves\nthe aforementioned problems since a few common factors often drive the\nvariation that cannot be captured by the smoothing model. Asymptotic theorems\nare also established to demonstrate the effects of including factor structures\non the smoothing results. Specifically, we show that the smoothing coefficients\nprojected on the complement space of the factor loading matrix is\nasymptotically normal. As a byproduct of independent interest, an estimator for\nthe population covariance matrix of the raw data is presented based on the\nproposed model. Extensive simulation studies illustrate that these factor\nadjustments are essential in improving estimation accuracy and avoiding the\ncurse of dimensionality. The superiority of our model is also shown in modeling\nCanadian weather data and Australian temperature data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 12:49:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Gao", "Yuan", ""], ["Shang", "Han Lin", ""], ["Yang", "Yanrong", ""]]}, {"id": "2102.02685", "submitter": "Luca Ganassali", "authors": "Luca Ganassali, Laurent Massouli\\'e, Marc Lelarge", "title": "Impossibility of Partial Recovery in the Graph Alignment Problem", "comments": "23 pages, 8 figures. Accepted for publication at COLT21", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graph alignment refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges. This can be\nviewed as an average-case and noisy version of the well-known graph isomorphism\nproblem. For the correlated Erd\\\"os-R\\'enyi model, we prove an impossibility\nresult for partial recovery in the sparse regime, with constant average degree\nand correlation, as well as a general bound on the maximal reachable overlap.\nOur bound is tight in the noiseless case (the graph isomorphism problem) and we\nconjecture that it is still tight with noise. Our proof technique relies on a\ncareful application of the probabilistic method to build automorphisms between\ntree components of a subcritical Erd\\\"os-R\\'enyi graph.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:26:48 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:23:47 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ganassali", "Luca", ""], ["Massouli\u00e9", "Laurent", ""], ["Lelarge", "Marc", ""]]}, {"id": "2102.02870", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop, William Kengne", "title": "Inference and model selection in general causal time series with\n  exogenous covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study a general class of causal processes with exogenous\ncovariates, including many classical processes such as the ARMA-GARCH, APARCH,\nARMAX, GARCH-X and APARCH-X processes.\n  Under some Lipschitz-type conditions, the existence of a $\\tau$-weakly\ndependent strictly stationary and ergodic solution is established.\n  We provide conditions for the strong consistency and derive the asymptotic\ndistribution of the quasi-maximum likelihood estimator (QMLE), both when the\ntrue parameter is an interior point of the parameter's space and when it\nbelongs to the boundary.\n  A significance Wald-type test of parameter is developed. This test is quite\nextensive and includes the test of nullity of the parameter's components, which\nin particular, allows us to assess the relevance of the exogenous covariates.\n  Relying on the QMLE of the model, we also propose a penalized criterion to\naddress the problem of the model selection for this class. The weak and the\nstrong consistency of the procedure are established.\n  Finally, Monte Carlo simulations are conducted to numerically illustrate the\nmain results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 20:13:54 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2102.02981", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun,\n  Tengyang Xie", "title": "Finite Sample Analysis of Minimax Offline Reinforcement Learning:\n  Completeness, Fast Rates and First-Order Efficiency", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We offer a theoretical characterization of off-policy evaluation (OPE) in\nreinforcement learning using function approximation for marginal importance\nweights and $q$-functions when these are estimated using recent minimax\nmethods. Under various combinations of realizability and completeness\nassumptions, we show that the minimax approach enables us to achieve a fast\nrate of convergence for weights and quality functions, characterized by the\ncritical inequality \\citep{bartlett2005}. Based on this result, we analyze\nconvergence rates for OPE. In particular, we introduce novel alternative\ncompleteness conditions under which OPE is feasible and we present the first\nfinite-sample result with first-order efficiency in non-tabular environments,\ni.e., having the minimal coefficient in the leading term.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:20:39 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Imaizumi", "Masaaki", ""], ["Jiang", "Nan", ""], ["Kallus", "Nathan", ""], ["Sun", "Wen", ""], ["Xie", "Tengyang", ""]]}, {"id": "2102.03073", "submitter": "Pedro Jos\\'e Chocano Feito", "authors": "Elena Castilla and Pedro J. Chocano", "title": "A new robust approach for multinomial logistic regression with complex\n  design model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimators and Wald-type tests are developed for the multinomial\nlogistic regression based on $\\phi$-divergence measures. The robustness of the\nproposed estimators and tests is proved through the study of their influence\nfunctions and it is also illustrated with two numerical examples and an\nextensive simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 09:37:36 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Castilla", "Elena", ""], ["Chocano", "Pedro J.", ""]]}, {"id": "2102.03262", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Mehmet Niyazi \\c{C}ankaya", "title": "On the estimating equations and objective functions for parameters of\n  exponential power distribution: Application for disorder", "comments": "37 pages,8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The efficient modeling for disorder in a phenomena depends on the chosen\nscore and objective functions. The main parameters in modeling are location,\nscale and shape. The exponential power distribution known as generalized\nGaussian is extensively used in modeling. In real world, the observations are\nmember of different parametric models or disorder in a data set exists. In this\nstudy, estimating equations for the parameters of exponential power\ndistribution are derived to have robust and also efficient M-estimators when\nthe data set includes disorder or contamination. The robustness property of\nM-estimators for the parameters is examined. Fisher information matrices based\non the derivative of score functions from $\\log$, $\\log_q$ and distorted\nlog-likelihoods are proposed by use of Tsallis $q$-entropy in order to have\nvariances of M-estimators. It is shown that matrices derived by score functions\nare positive semidefinite if conditions are satisfied. Information criteria\ninspired by Akaike and Bayesian are arranged by taking the absolute value of\nscore functions. Fitting performances of score functions from estimating\nequations and objective functions are tested by applying volume, information\ncriteria and mean absolute error which are essential tools in modeling to\nassess the fitting competence of the proposed functions. Applications from\nsimulation and real data sets are carried out to compare the performance of\nestimating equations and objective functions. It is generally observed that the\ndistorted log-likelihood for the estimations of parameters of exponential power\ndistribution has superior performance than other score and objective functions\nfor the contaminated data sets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:10:05 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["\u00c7ankaya", "Mehmet Niyazi", ""]]}, {"id": "2102.03306", "submitter": "Lars Lau Raket", "authors": "Lars Lau Raket", "title": "Differential equations, splines and Gaussian processes", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore the connections between Green's functions for certain differential\nequations, covariance functions for Gaussian processes, and the smoothing\nsplines problem. Conventionally, the smoothing spline problem is considered in\na setting of reproducing kernel Hilbert spaces, but here we present a more\ndirect approach. With this approach, some choices that are implicit in the\nreproducing kernel Hilbert space setting stand out, one example being choice of\nboundary conditions and more elaborate shape restrictions.\n  The paper first explores the Laplace operator and the Poisson equation and\nstudies the corresponding Green's functions under various boundary conditions\nand constraints. Explicit functional forms are derived in a range of examples.\nThese examples include several novel forms of the Green's function that, to the\nauthor's knowledge, have not previously been presented. Next we present a\nsmoothing spline problem where we penalize the integrated squared derivative of\nthe function to be estimated. We then show how the solution can be explicitly\ncomputed using the Green's function for the Laplace operator. In the last part\nof the paper, we explore the connection between Gaussian processes and\ndifferential equations, and show how the Laplace operator is related to\nBrownian processes and how processes that arise due to boundary conditions and\nshape constraints can be viewed as conditional Gaussian processes. The\npresented connection between Green's functions for the Laplace operator and\ncovariance functions for Brownian processes allows us to introduce several new\nnovel Brownian processes with specific behaviors. Finally, we consider the\nconnection between Gaussian process priors and smoothing splines.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:01:17 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Raket", "Lars Lau", ""]]}, {"id": "2102.03389", "submitter": "He Li", "authors": "Xi Chen, Zehua Lai, He Li, Yichen Zhang", "title": "Online Statistical Inference for Gradient-free Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As gradient-free stochastic optimization gains emerging attention for a wide\nrange of applications recently, the demand for uncertainty quantification of\nparameters obtained from such approaches arises. In this paper, we investigate\nthe problem of statistical inference for model parameters based on\ngradient-free stochastic optimization methods that use only function values\nrather than gradients. We first present central limit theorem results for\nPolyak-Ruppert-averaging type gradient-free estimators. The asymptotic\ndistribution reflects the trade-off between the rate of convergence and\nfunction query complexity. We next construct valid confidence intervals for\nmodel parameters through the estimation of the covariance matrix in a fully\nonline fashion. We further give a general gradient-free framework for\ncovariance estimation and analyze the role of function query complexity in the\nconvergence rate of the covariance estimator. This provides a one-pass\ncomputationally efficient procedure for simultaneously obtaining an estimator\nof model parameters and conducting statistical inference. Finally, we provide\nnumerical experiments to verify our theoretical results and illustrate some\nextensions of our method for various machine learning and deep learning\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:22:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chen", "Xi", ""], ["Lai", "Zehua", ""], ["Li", "He", ""], ["Zhang", "Yichen", ""]]}, {"id": "2102.03403", "submitter": "Saptarshi Chakraborty", "authors": "Debolina Paul, Saptarshi Chakraborty and Swagatam Das", "title": "Robust Principal Component Analysis: A Median of Means Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a fundamental tool for data\nvisualization, denoising, and dimensionality reduction. It is widely popular in\nStatistics, Machine Learning, Computer Vision, and related fields. However, PCA\nis well known to fall prey to the presence of outliers and often fails to\ndetect the true underlying low-dimensional structure within the dataset. Recent\nsupervised learning methods, following the Median of Means (MoM) philosophy,\nhave shown great success in dealing with outlying observations without much\ncompromise to their large sample theoretical properties. In this paper, we\npropose a PCA procedure based on the MoM principle. Called the Median of Means\nPrincipal Component Analysis (MoMPCA), the proposed method is not only\ncomputationally appealing but also achieves optimal convergence rates under\nminimal assumptions. In particular, we explore the non-asymptotic error bounds\nof the obtained solution via the aid of Vapnik-Chervonenkis theory and\nRademacher complexity, while granting absolutely no assumption on the outlying\nobservations. The efficacy of the proposal is also thoroughly showcased through\nsimulations and real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:59:05 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Paul", "Debolina", ""], ["Chakraborty", "Saptarshi", ""], ["Das", "Swagatam", ""]]}, {"id": "2102.03426", "submitter": "Benjamin Hollering", "authors": "Benjamin Hollering and Seth Sullivant", "title": "Discrete Max-Linear Bayesian Networks", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete max-linear Bayesian networks are directed graphical models specified\nby the same recursive structural equations as max-linear models but with\ndiscrete innovations. When all of the random variables in the model are binary,\nthese models are isomorphic to the conjunctive Bayesian network (CBN) models of\nBeerenwinkel, Eriksson, and Sturmfels. Many of the techniques used to study CBN\nmodels can be extended to discrete max-linear models and similar results can be\nobtained. In particular, we extend the fact that CBN models are toric varieties\nafter linear change of coordinates to all discrete max-linear models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 21:34:20 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hollering", "Benjamin", ""], ["Sullivant", "Seth", ""]]}, {"id": "2102.03589", "submitter": "Friedrich G\\\"otze", "authors": "Friedrich G\\\"otze and Mindaugas Bloznelis", "title": "Edgeworth approximations for distributions of symmetric statistics", "comments": "This paper dates back to 2005 including literature up to this year", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the distribution of a general class of asymptoticallylinear\nstatistics which are symmetric functions of $N$ independent observations. The\ndistribution functions of these statistics are approximated by an Edgeworth\nexpansion with a remainder of order $o(N^{-1})$. The Edgeworth expansion is\nbased on Hoeffding's decomposition which provides a stochastic expansion into a\nlinear part, a quadratic part as well as smaller higher order parts. The\nvalidity of this Edgeworth expansion is proved under Cram\\'er's condition on\nthe linear part, moment assumptions for all parts of the statistic and an\noptimal dimensionality requirement for the non linear part.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 14:44:09 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["G\u00f6tze", "Friedrich", ""], ["Bloznelis", "Mindaugas", ""]]}, {"id": "2102.03594", "submitter": "Oleksandr Zadorozhnyi", "authors": "Oleksandr Zadorozhnyi, Pierre Gaillard, Sebastien Gerschinovitz,\n  Alessandro Rudi", "title": "Online nonparametric regression with Sobolev kernels", "comments": "40 pages, 5 figures, 3 tables (version 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we investigate the variation of the online kernelized ridge\nregression algorithm in the setting of $d-$dimensional adversarial\nnonparametric regression. We derive the regret upper bounds on the classes of\nSobolev spaces $W_{p}^{\\beta}(\\mathcal{X})$, $p\\geq 2, \\beta>\\frac{d}{p}$. The\nupper bounds are supported by the minimax regret analysis, which reveals that\nin the cases $\\beta> \\frac{d}{2}$ or $p=\\infty$ these rates are (essentially)\noptimal. Finally, we compare the performance of the kernelized ridge regression\nforecaster to the known non-parametric forecasters in terms of the regret rates\nand their computational complexity as well as to the excess risk rates in the\nsetting of statistical (i.i.d.) nonparametric regression.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 15:05:14 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 09:20:16 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zadorozhnyi", "Oleksandr", ""], ["Gaillard", "Pierre", ""], ["Gerschinovitz", "Sebastien", ""], ["Rudi", "Alessandro", ""]]}, {"id": "2102.03607", "submitter": "Botao Hao", "authors": "Botao Hao, Xiang Ji, Yaqi Duan, Hao Lu, Csaba Szepesv\\'ari, Mengdi\n  Wang", "title": "Bootstrapping Statistical Inference for Off-Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping provides a flexible and effective approach for assessing the\nquality of batch reinforcement learning, yet its theoretical property is less\nunderstood. In this paper, we study the use of bootstrapping in off-policy\nevaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE)\nthat is known to be minimax-optimal in the tabular and linear-model cases. We\npropose a bootstrapping FQE method for inferring the distribution of the policy\nevaluation error and show that this method is asymptotically efficient and\ndistributionally consistent for off-policy statistical inference. To overcome\nthe computation limit of bootstrapping, we further adapt a subsampling\nprocedure that improves the runtime by an order of magnitude. We numerically\nevaluate the bootrapping method in classical RL environments for confidence\ninterval estimation, estimating the variance of off-policy evaluator, and\nestimating the correlation between multiple off-policy evaluators.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 16:45:33 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 11:19:15 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hao", "Botao", ""], ["Ji", "Xiang", ""], ["Duan", "Yaqi", ""], ["Lu", "Hao", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Wang", "Mengdi", ""]]}, {"id": "2102.03802", "submitter": "Mark Kozdoba", "authors": "Mark Kozdoba and Shie Mannor", "title": "Dimension Free Generalization Bounds for Non Linear Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study generalization guarantees for the metric learning\nproblem, where the metric is induced by a neural network type embedding of the\ndata. Specifically, we provide uniform generalization bounds for two regimes --\nthe sparse regime, and a non-sparse regime which we term \\emph{bounded\namplification}. The sparse regime bounds correspond to situations where\n$\\ell_1$-type norms of the parameters are small. Similarly to the situation in\nclassification, solutions satisfying such bounds can be obtained by an\nappropriate regularization of the problem. On the other hand, unregularized SGD\noptimization of a metric learning loss typically does not produce sparse\nsolutions. We show that despite this lack of sparsity, by relying on a\ndifferent, new property of the solutions, it is still possible to provide\ndimension free generalization guarantees. Consequently, these bounds can\nexplain generalization in non sparse real experimental situations. We\nillustrate the studied phenomena on the MNIST and 20newsgroups datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 14:47:00 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "2102.03885", "submitter": "Yordan Raykov", "authors": "Yazan Qarout and Yordan P. Raykov and Max A. Little", "title": "Few-shot time series segmentation using prototype-defined infinite\n  hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust framework for interpretable, few-shot analysis of\nnon-stationary sequential data based on flexible graphical models to express\nthe structured distribution of sequential events, using prototype radial basis\nfunction (RBF) neural network emissions. A motivational link is demonstrated\nbetween prototypical neural network architectures for few-shot learning and the\nproposed RBF network infinite hidden Markov model (RBF-iHMM). We show that RBF\nnetworks can be efficiently specified via prototypes allowing us to express\ncomplex nonstationary patterns, while hidden Markov models are used to infer\nprincipled high-level Markov dynamics. The utility of the framework is\ndemonstrated on biomedical signal processing applications such as automated\nseizure detection from EEG data where RBF networks achieve state-of-the-art\nperformance using a fraction of the data needed to train long-short-term memory\nvariational autoencoders.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 19:02:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Qarout", "Yazan", ""], ["Raykov", "Yordan P.", ""], ["Little", "Max A.", ""]]}, {"id": "2102.04259", "submitter": "Mathieu Even", "authors": "Mathieu Even and Laurent Massouli\\'e", "title": "Concentration of Non-Isotropic Random Tensors with Applications to\n  Learning and Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 17:13:03 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 08:47:50 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Even", "Mathieu", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2102.04363", "submitter": "Bart Paul Gerard Van Parys", "authors": "Bart P.G. Van Parys", "title": "Optimal Transport in the Face of Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport distances are popular and theoretically well understood in\nthe context of data-driven prediction. A flurry of recent work has popularized\nthese distances for data-driven decision-making as well although their merits\nin this context are far less well understood. This in contrast to the more\nclassical entropic distances which are known to enjoy optimal statistical\nproperties. This begs the question when, if ever, optimal transport distances\nenjoy similar statistical guarantees. Optimal transport methods are shown here\nto enjoy optimal statistical guarantees for decision problems faced with noisy\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:12:30 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Van Parys", "Bart P. G.", ""]]}, {"id": "2102.04401", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis", "title": "The Optimality of Polynomial Regression for Agnostic Learning under\n  Gaussian Marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agnostic learning under the Gaussian distribution. We\ndevelop a method for finding hard families of examples for a wide class of\nproblems by using LP duality. For Boolean-valued concept classes, we show that\nthe $L^1$-regression algorithm is essentially best possible, and therefore that\nthe computational difficulty of agnostically learning a concept class is\nclosely related to the polynomial degree required to approximate any function\nfrom the class in $L^1$-norm. Using this characterization along with additional\nanalytic tools, we obtain optimal SQ lower bounds for agnostically learning\nlinear threshold functions and the first non-trivial SQ lower bounds for\npolynomial threshold functions and intersections of halfspaces. We also develop\nan analogous theory for agnostically learning real-valued functions, and as an\napplication prove near-optimal SQ lower bounds for agnostically learning ReLUs\nand sigmoids.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:06:32 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Pittas", "Thanasis", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2102.04423", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "Prepivoted permutation tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach to constructing permutation tests that are both\nexact for the null hypothesis of equality of distributions and asymptotically\ncorrect for testing equality of parameters of distributions while allowing the\ndistributions themselves to differ. These robust permutation tests transform a\ngiven test statistic by a consistent estimator of its limiting distribution\nfunction before enumerating its permutation distribution. This transformation,\nknown as prepivoting, aligns the unconditional limiting distribution for the\ntest statistic with the probability limit of its permutation distribution.\nThrough prepivoting, the tests permute one minus an asymptotically valid\n$p$-value for testing the null of equality of parameters. We describe two\napproaches for prepivoting within permutation tests, one directly using\nasymptotic normality and the other using the bootstrap. We further illustrate\nthat permutation tests using bootstrap prepivoting can provide improvements to\nthe order of the error in rejection probability relative to competing\ntransformations when testing equality of parameters, while maintaining\nexactness under equality of distributions. Simulation studies highlight the\nversatility of the proposal, illustrating the restoration of asymptotic\nvalidity to a wide range of permutation tests conducted when only the\nparameters of distributions are equal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:35:08 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 19:29:28 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "2102.04451", "submitter": "Michael Gnewuch", "authors": "Michael Gnewuch and Nils Hebbinghaus", "title": "Discrepancy Bounds for a Class of Negatively Dependent Random Points\n  Including Latin Hypercube Samples", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.NA math.NA math.NT math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a class of $\\gamma$-negatively dependent random samples. We\nprove that this class includes, apart from Monte Carlo samples, in particular\nLatin hypercube samples and Latin hypercube samples padded by Monte Carlo.\n  For a $\\gamma$-negatively dependent $N$-point sample in dimension $d$ we\nprovide probabilistic upper bounds for its star discrepancy with explicitly\nstated dependence on $N$, $d$, and $\\gamma$. These bounds generalize the\nprobabilistic bounds for Monte Carlo samples from [Heinrich et al., Acta Arith.\n96 (2001), 279--302] and [C.~Aistleitner, J.~Complexity 27 (2011), 531--540],\nand they are optimal for Monte Carlo and Latin hypercube samples. In the\nspecial case of Monte Carlo samples the constants that appear in our bounds\nimprove substantially on the constants presented in the latter paper and in\n[C.~Aistleitner, M.~T.~Hofer, Math. Comp.~83 (2014), 1373--1381].\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 17:56:23 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Gnewuch", "Michael", ""], ["Hebbinghaus", "Nils", ""]]}, {"id": "2102.04462", "submitter": "Stefano Favaro", "authors": "Emanuele Dolera, Stefano Favaro, Stefano Peluchetti", "title": "Learning-augmented count-min sketches via Bayesian nonparametrics", "comments": "40 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2102.03743", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The count-min sketch (CMS) is a time and memory efficient randomized data\nstructure that provides estimates of tokens' frequencies in a data stream, i.e.\npoint queries, based on random hashed data. Learning-augmented CMSs improve the\nCMS by learning models that allow to better exploit data properties. In this\npaper, we focus on the learning-augmented CMS of Cai, Mitzenmacher and Adams\n(\\textit{NeurIPS} 2018), which relies on Bayesian nonparametric (BNP) modeling\nof a data stream via Dirichlet process (DP) priors. This is referred to as the\nCMS-DP, and it leads to BNP estimates of a point query as posterior means of\nthe point query given the hashed data. While BNPs is proved to be a powerful\ntool for developing robust learning-augmented CMSs, ideas and methods behind\nthe CMS-DP are tailored to point queries under DP priors, and they can not be\nused for other priors or more general queries. In this paper, we present an\nalternative, and more flexible, derivation of the CMS-DP such that: i) it\nallows to make use of the Pitman-Yor process (PYP) prior, which is arguably the\nmost popular generalization of the DP prior; ii) it can be readily applied to\nthe more general problem of estimating range queries. This leads to develop a\nnovel learning-augmented CMS under power-law data streams, referred to as the\nCMS-PYP, which relies on BNP modeling of the stream via PYP priors.\nApplications to synthetic and real data show that the CMS-PYP outperforms the\nCMS and the CMS-DP in the estimation of low-frequency tokens; this known to be\na critical feature in natural language processing, where it is indeed common to\nencounter power-law data streams.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:02:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Dolera", "Emanuele", ""], ["Favaro", "Stefano", ""], ["Peluchetti", "Stefano", ""]]}, {"id": "2102.04543", "submitter": "Jacob Dorn", "authors": "Jacob Dorn and Kevin Guo", "title": "Sharp Sensitivity Analysis for Inverse Propensity Weighting via Quantile\n  Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inverse propensity weighting (IPW) is a popular method for estimating\ntreatment effects from observational data. However, its correctness relies on\nthe untestable (and frequently implausible) assumption that all confounders\nhave been measured. This paper introduces a robust sensitivity analysis for IPW\nthat estimates the range of treatment effects compatible with a given amount of\nunobserved confounding. The estimated range converges to the narrowest possible\ninterval (under the given assumptions) that must contain the true treatment\neffect. Our proposal is a refinement of the influential sensitivity analysis by\nZhao, Small, and Bhattacharya (2019), which we show gives bounds that are too\nwide even asymptotically. This analysis is based on new partial identification\nresults for Tan (2006)'s marginal sensitivity model.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:47:23 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 18:27:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Dorn", "Jacob", ""], ["Guo", "Kevin", ""]]}, {"id": "2102.04810", "submitter": "Ivan Nourdin", "authors": "Soukaina Douissi and Khalifa Es-Sebaiy and George Kerchev and Ivan\n  Nourdin", "title": "Berry-Esseen bounds of second moment estimators for Gaussian processes\n  observed at high frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $Z:=\\{Z_t,t\\geq0\\}$ be a stationary Gaussian process. We study two\nestimators of $\\mathbb{E}[Z_0^2]$, namely $\\widehat{f}_T(Z):= \\frac{1}{T}\n\\int_{0}^{T} Z_{t}^{2}dt$, and $\\widetilde{f}_n(Z) :=\\frac{1}{n} \\sum_{i\n=1}^{n} Z_{t_{i}}^{2}$, where $ t_{i} = i \\Delta_{n}$, $ i=0,1,\\ldots, n $,\n$\\Delta_{n}\\rightarrow 0$ and $ T_{n} := n \\Delta_{n}\\rightarrow \\infty$. We\nprove that the two estimators are strongly consistent and establish\nBerry-Esseen bounds for a central limit theorem involving $\\widehat{f}_T(Z)$\nand $\\widetilde{f}_n(Z)$. We apply these results to asymptotically stationary\nGaussian processes and estimate the drift parameter for Gaussian\nOrnstein-Uhlenbeck processes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 13:26:54 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Douissi", "Soukaina", ""], ["Es-Sebaiy", "Khalifa", ""], ["Kerchev", "George", ""], ["Nourdin", "Ivan", ""]]}, {"id": "2102.04923", "submitter": "Zhuo-Song Zhang", "authors": "Qi-Man Shao and Zhuo-Song Zhang", "title": "Berry--Esseen Bounds for Multivariate Nonlinear Statistics with\n  Applications to M-estimators and Stochastic Gradient Descent Algorithms", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a Berry--Esseen bound for general multivariate nonlinear\nstatistics by developing a new multivariate-type randomized concentration\ninequality. The bound is the best possible for many known statistics. As\napplications, Berry--Esseen bounds for M-estimators and averaged stochastic\ngradient descent algorithms are obtained.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:29:52 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 04:26:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shao", "Qi-Man", ""], ["Zhang", "Zhuo-Song", ""]]}, {"id": "2102.05307", "submitter": "Nakahiro Yoshida", "authors": "Haruhiko Inatsugu and Nakahiro Yoshida", "title": "Global jump filters and realized volatility", "comments": "Some modifications and corrections have been done", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a semimartingale with jumps, we propose a new estimation method for\nintegrated volatility, i.e., the quadratic variation of the continuous\nmartingale part, based on the global jump filter proposed by Inatsugu and\nYoshida [8]. To decide whether each increment of the process has jumps, the\nglobal jump filter adopts the upper $\\alpha$-quantile of the absolute\nincrements as the threshold. This jump filter is called global since it uses\nall the observations to classify one increment. We give a rate of convergence\nand prove asymptotic mixed normality of the global realized volatility and its\nvariant \"Winsorized global volatility\". By simulation studies, we show that our\nestimators outperform previous realized volatility estimators that use a few\nadjacent increments to mitigate the effects of jumps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:07:38 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 05:53:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Inatsugu", "Haruhiko", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "2102.05314", "submitter": "Yohann de Castro", "authors": "Yohann de Castro (ICJ, CERMICS), Luca Mencarelli (CERMICS)", "title": "Forecasting Nonnegative Time Series via Sliding Mask Method (SMM) and\n  Latent Clustered Forecast (LCF)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonnegative time series forecasting framework. Based on recent\nadvances in Nonnegative Matrix Factorization (NMF) and Archetypal Analysis, we\nintroduce two procedures referred to as Sliding Mask Method (SMM) and Latent\nClustered Forecast (LCF). SMM is a simple and powerful method based on time\nwindow prediction using Completion of Nonnegative Matrices. This new procedure\ncombines low nonnegative rank decomposition and matrix completion where the\nhidden values are to be forecasted. LCF is two stage: it leverages archetypal\nanalysis for dimension reduction and clustering of time series, then it uses\nany black-box supervised forecast solver on the clustered latent\nrepresentation. Theoretical guarantees on uniqueness and robustness of the\nsolution of NMF Completion-type problems are also provided for the first time.\nFinally, numerical experiments on real-world and synthetic data-set confirms\nforecasting accuracy for both the methodologies.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:29:55 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["de Castro", "Yohann", "", "ICJ, CERMICS"], ["Mencarelli", "Luca", "", "CERMICS"]]}, {"id": "2102.05386", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Shyamal Ghosh and Prajamitra Bhuyan and Maxim Finkelstein", "title": "On a Bivariate Copula for Modeling Negative Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new bivariate copula is proposed for modeling negative dependence between\ntwo random variables. We show that it complies with most of the popular notions\nof negative dependence reported in the literature and study some of its basic\nproperties. Specifically, the Spearman's rho and the Kendall's tau for the\nproposed copula have a simple one-parameter form with negative values in the\nfull range. Some important ordering properties comparing the strength of\nnegative dependence with respect to the parameter involved are considered.\nSimple examples of the corresponding bivariate distributions with popular\nmarginals are presented. Application of the proposed copula is illustrated\nusing a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 11:35:38 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Ghosh", "Shyamal", ""], ["Bhuyan", "Prajamitra", ""], ["Finkelstein", "Maxim", ""]]}, {"id": "2102.05472", "submitter": "Marina Garrote-L\\'opez", "authors": "Marta Casanellas, Marina Garrote-L\\'opez and Piotr Zwiernik", "title": "Robust estimation of tree structured models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of learning undirected graphical models on trees from\ncorrupted data. Recently Katiyar et al. showed that it is possible to recover\ntrees from noisy binary data up to a small equivalence class of possible trees.\nTheir other paper on the Gaussian case follows a similar pattern. By framing\nthis as a special phylogenetic recovery problem we largely generalize these two\nsettings. Using the framework of linear latent tree models we discuss tree\nidentifiability for binary data under a continuous corruption model. For the\nIsing and the Gaussian tree model we also provide a characterisation of when\nthe Chow-Liu algorithm consistently learns the underlying tree from the noisy\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:58:40 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Casanellas", "Marta", ""], ["Garrote-L\u00f3pez", "Marina", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "2102.05569", "submitter": "Sander Greenland", "authors": "Sander Greenland", "title": "There are natural scores: Full comment on Shafer, \"Testing by betting: A\n  strategy for statistical and scientific communication\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Shafer (2021) offers a betting perspective on statistical testing which may\nbe useful for foundational debates, given that disputes over such testing\ncontinue to be intense. To be helpful for researchers, however, this\nperspective will need more elaboration using real examples in which (a) the\nbetting score has a justification and interpretation in terms of study goals\nthat distinguishes it from the uncountable mathematical possibilities, and (b)\nthe assumptions in the sampling model are uncertain. On justification, Shafer\nsays 'No one has made a convincing case for any particular choice' of a score\nderived from a P-value and then states that 'the choice is fundamentally\narbitrary'. Yet some (but not most) scores can be motivated by study goals\n(e.g., information measurement; decision making). The one I have seen\nrepeatedly in information statistics and data mining is the surprisal, logworth\nor S-value s = -log(p), where the log base determines the scale. The present\ncomment explains the rationale for this choice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:02:50 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Greenland", "Sander", ""]]}, {"id": "2102.05629", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Christos Tzamos,\n  Nikos Zarifis", "title": "Agnostic Proper Learning of Halfspaces under Gaussian Marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of agnostically learning halfspaces under the Gaussian\ndistribution. Our main result is the {\\em first proper} learning algorithm for\nthis problem whose sample complexity and computational complexity qualitatively\nmatch those of the best known improper agnostic learner. Building on this\nresult, we also obtain the first proper polynomial-time approximation scheme\n(PTAS) for agnostically learning homogeneous halfspaces. Our techniques\nnaturally extend to agnostically learning linear models with respect to other\nnon-linear activations, yielding in particular the first proper agnostic\nalgorithm for ReLU regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 18:40:44 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2102.05761", "submitter": "Benjamin Cooper Boniece", "authors": "Patrice Abry, B. Cooper Boniece, Gustavo Didier, Herwig Wendt", "title": "On high-dimensional wavelet eigenanalysis", "comments": "Shortened version; applications for statistical inference to appear\n  in a separate publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we characterize the asymptotic and large scale behavior of the\neigenvalues of wavelet random matrices in high dimensions. We assume that\npossibly non-Gaussian, finite-variance $p$-variate measurements are made of a\nlow-dimensional $r$-variate ($r \\ll p$) fractional stochastic process with\nnon-canonical scaling coordinates and in the presence of additive\nhigh-dimensional noise. The measurements are correlated both time-wise and\nbetween rows. We show that the $r$ largest eigenvalues of the wavelet random\nmatrices, when appropriately rescaled, converge to scale invariant functions in\nthe high-dimensional limit. By contrast, the remaining $p-r$ eigenvalues remain\nbounded. Under additional assumptions, we show that, up to a log\ntransformation, the $r$ largest eigenvalues of wavelet random matrices exhibit\nasymptotically Gaussian distributions. The results have direct consequences for\nstatistical inference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 22:32:03 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 20:54:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Abry", "Patrice", ""], ["Boniece", "B. Cooper", ""], ["Didier", "Gustavo", ""], ["Wendt", "Herwig", ""]]}, {"id": "2102.05764", "submitter": "Qiyang Han", "authors": "Qiyang Han", "title": "Multiplier U-processes: sharp bounds and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory for multiplier empirical processes has been one of the central\ntopics in the development of the classical theory of empirical processes, due\nto its wide applicability to various statistical problems. In this paper, we\ndevelop theory and tools for studying multiplier $U$-processes, a natural\nhigher-order generalization of the multiplier empirical processes. To this end,\nwe develop a multiplier inequality that quantifies the moduli of continuity of\nthe multiplier $U$-process in terms of that of the (decoupled) symmetrized\n$U$-process. The new inequality finds a variety of applications including (i)\nmultiplier and bootstrap central limit theorems for $U$-processes, (ii) general\ntheory for bootstrap $M$-estimators based on $U$-statistics, and (iii) theory\nfor $M$-estimation under general complex sampling designs, again based on\n$U$-statistics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 22:43:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Han", "Qiyang", ""]]}, {"id": "2102.05802", "submitter": "Leighton Barnes", "authors": "Leighton Pate Barnes and Ayfer Ozgur", "title": "Fisher Information and Mutual Information Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the processing of statistical samples $X\\sim P_\\theta$ by a\nchannel $p(y|x)$, and characterize how the statistical information from the\nsamples for estimating the parameter $\\theta\\in\\mathbb{R}^d$ can scale with the\nmutual information or capacity of the channel. We show that if the statistical\nmodel has a sub-Gaussian score function, then the trace of the Fisher\ninformation matrix for estimating $\\theta$ from $Y$ can scale at most linearly\nwith the mutual information between $X$ and $Y$. We apply this result to obtain\nminimax lower bounds in distributed statistical estimation problems, and obtain\na tight preconstant for Gaussian mean estimation. We then show how our Fisher\ninformation bound can also imply mutual information or Jensen-Shannon\ndivergence based distributed strong data processing inequalities.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 01:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 22:28:26 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Barnes", "Leighton Pate", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2102.05821", "submitter": "Liyan Xie", "authors": "Liyan Xie and Yao Xie", "title": "Optimality of Graph Scanning Statistic for Online Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential change-point detection for graphs is a fundamental problem for\nstreaming network data types and has wide applications in social networks and\npower systems. Given fixed vertices and a sequence of random graphs, the\nobjective is to detect the change-point where the underlying distribution of\nthe random graph changes. In particular, we focus on the local change that only\naffects a subgraph. We adopt the classical Erdos-Renyi model and revisit the\ngeneralized likelihood ratio (GLR) detection procedure. The scan statistic is\ncomputed by sequentially estimating the most-likely subgraph where the change\nhappens. We provide theoretical analysis for the asymptotic optimality of the\nproposed procedure based on the GLR framework. We demonstrate the efficiency of\nour detection algorithm using simulations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 02:58:41 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Xie", "Liyan", ""], ["Xie", "Yao", ""]]}, {"id": "2102.05832", "submitter": "Rahul Singh", "authors": "Rahul Singh and Neeraj Misra", "title": "Some parametric tests based on sample spacings", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that we have a random sample from an absolutely continuous\ndistribution (univariate or multivariate) with a known functional form and some\nunknown parameters. In this paper, we have studied several parametric tests\nsymmetrically based on sample spacings. Asymptotic properties of these tests\nhave been investigated under the simple null hypothesis and the sequence of\nlocal alternatives converging to the null hypothesis. The asymptotic properties\nof the proposed tests have also been studied under composite null hypothesis.\nIt is observed that these tests have similar properties as the likelihood ratio\ntest. For assessment of finite sample performance of the proposed tests, we\nhave performed an extensive numerical study. The proposed tests can be used in\nsome situations where likelihood ratio tests do not exist due to unboundedness\nof likelihood function.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 03:31:50 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Singh", "Rahul", ""], ["Misra", "Neeraj", ""]]}, {"id": "2102.06059", "submitter": "Stefan Franssen", "authors": "S.E.M.P. Franssen, A.W. van der Vaart", "title": "The Bernstein-von Mises theorem for the Pitman-Yor process of\n  nonnegative type", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Pitman-Yor process is a nonparametric species sampling prior with number\nof different species of the order of $n^\\sigma$ for some $\\sigma>0$. In case of\nan atomless true distribution, the asymptotic distribution of the posterior of\nthe Pitman-Yor process was known but typically inconsistent. In this paper, we\nextend this result into a general theorem for arbitrary true distributions. For\ndiscrete distributions the posterior is consistent, but it turns out that there\ncan be a bias which does not converge to zero at the $\\sqrt{n}$ rate. We\npropose a bias correction and show that after correcting for the bias, the\nposterior distribution will be asymptotically normal. Without the bias\ncorrection, the coverage of the credible sets can be arbitrarily low, and we\nillustrate this finding with simulations where we compare the coverage of\ncorrected and uncorrected credible sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 15:06:53 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Franssen", "S. E. M. P.", ""], ["van der Vaart", "A. W.", ""]]}, {"id": "2102.06287", "submitter": "Pierre-Yves Louis", "authors": "Irene Crimaldi, Pierre-Yves Louis, Ida Germana Minelli", "title": "Urn models with random multiple drawing and random addition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider an urn model with multiple drawing and random time-dependent\naddition matrix. The model is very general with respect to previous literature:\nthe number of sampled balls at each time-step is random, the addition matrix\nhas general random entries. For the proportion of balls of a given color, we\nprove almost sure convergence results and fluctuation theorems (through CLTs in\nthe sense of stable convergence and of almost sure conditional convergence,\nwhich are stronger than convergence in distribution). Asymptotic confidence\nintervals are given for the limit proportion, whose distribution is generally\nunknown.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 22:16:07 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 10:32:33 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 09:22:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Crimaldi", "Irene", ""], ["Louis", "Pierre-Yves", ""], ["Minelli", "Ida Germana", ""]]}, {"id": "2102.06379", "submitter": "Jean-Michel Loubes", "authors": "Eustasio del Barrio (IMUVA), Alberto Gonz\\'alez-Sanz (IMT, IMUVA),\n  Jean-Michel Loubes (IMT)", "title": "Central Limit Theorems for General Transportation Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimal transportation with general cost between a\nempirical measure and a general target probability on R d , with d $\\ge$ 1. We\nextend results in [19] and prove asymptotic stability of both optimal transport\nmaps and potentials for a large class of costs in R d. We derive a central\nlimit theorem (CLT) towards a Gaussian distribution for the empirical\ntransportation cost under minimal assumptions, with a new proof based on the\nEfron-Stein inequality and on the sequential compactness of the closed unit\nball in L 2 (P) for the weak topology. We provide also CLTs for empirical\nWassertsein distances in the special case of potential costs | $\\bullet$ | p ,\np > 1.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 07:53:22 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 09:58:01 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["del Barrio", "Eustasio", "", "IMUVA"], ["Gonz\u00e1lez-Sanz", "Alberto", "", "IMT, IMUVA"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "2102.06449", "submitter": "Jie Wang", "authors": "Jie Wang, Rui Gao, Yao Xie", "title": "Two-sample Test with Kernel Projected Wasserstein Distance", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a kernel projected Wasserstein distance for the two-sample test,\nan essential building block in statistics and machine learning: given two sets\nof samples, to determine whether they are from the same distribution. This\nmethod operates by finding the nonlinear mapping in the data space which\nmaximizes the distance between projected distributions. In contrast to existing\nworks about projected Wasserstein distance, the proposed method circumvents the\ncurse of dimensionality more efficiently. We present practical algorithms for\ncomputing this distance function together with the non-asymptotic uncertainty\nquantification of empirical estimates. Numerical examples validate our\ntheoretical results and demonstrate good performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 11:14:56 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 22:10:54 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Jie", ""], ["Gao", "Rui", ""], ["Xie", "Yao", ""]]}, {"id": "2102.06548", "submitter": "Yuejie Chi", "authors": "Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, Yuejie Chi", "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis", "comments": "v2 added a matching lower bound, and removed the finite-horizon\n  setting for brevity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Q-learning, which seeks to learn the optimal Q-function of a Markov decision\nprocess (MDP) in a model-free fashion, lies at the heart of reinforcement\nlearning. When it comes to the synchronous setting (such that independent\nsamples for all state-action pairs are drawn from a generative model in each\niteration), substantial progress has been made recently towards understanding\nthe sample efficiency of Q-learning. Take a $\\gamma$-discounted\ninfinite-horizon MDP with state space $\\mathcal{S}$ and action space\n$\\mathcal{A}$: to yield an entrywise $\\varepsilon$-accurate estimate of the\noptimal Q-function, state-of-the-art theory for Q-learning proves that a sample\nsize on the order of\n$\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^5\\varepsilon^{2}}$ is sufficient,\nwhich, however, fails to match with the existing minimax lower bound. This\ngives rise to natural questions: what is the sharp sample complexity of\nQ-learning? Is Q-learning provably sub-optimal? In this work, we settle these\nquestions by (1) demonstrating that the sample complexity of Q-learning is at\nmost on the order of\n$\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^4\\varepsilon^2}$ (up to some log\nfactor) for any $0<\\varepsilon <1$, and (2) developing a matching lower bound\nto confirm the sharpness of our result. Our findings unveil both the\neffectiveness and limitation of Q-learning: its sample complexity matches that\nof speedy Q-learning without requiring extra computation and storage, albeit\nstill being considerably higher than the minimax lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:22:05 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 13:56:01 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Gen", ""], ["Cai", "Changxiao", ""], ["Chen", "Yuxin", ""], ["Gu", "Yuantao", ""], ["Wei", "Yuting", ""], ["Chi", "Yuejie", ""]]}, {"id": "2102.06736", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva and Alfred Kume", "title": "Multivariate Max-Stable Processes and Homogeneous Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate max-stable processes are important for both theoretical\ninvestigations and various statistical applications motivated by the fact that\nthese are limiting processes, for instance of stationary multivariate regularly\nvarying time series, [1]. In this contribution we explore the relation between\nhomogeneous functionals and multivariate max-stable processes and discuss the\nconnections between multivariate max-stable process and zonoid / max-zonoid\nequivalence. We illustrate our results considering Brown-Resnick and Smith\nprocesses.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 19:37:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Kume", "Alfred", ""]]}, {"id": "2102.06817", "submitter": "Nayel Bettache", "authors": "Nayel Bettache, Cristina Butucea, Marianne Sorba", "title": "Fast Non-Asymptotic Testing And Support Recovery For Large Sparse\n  Toeplitz Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $n$ independent $p$-dimensional Gaussian vectors with covariance\nmatrix having Toeplitz structure. We test that these vectors have independent\ncomponents against a stationary distribution with sparse Toeplitz covariance\nmatrix, and also select the support of non-zero entries. We assume that the\nnon-zero values can occur in the recent past (time-lag less than $p/2$). We\nbuild test procedures that combine a sum and a scan-type procedures, but are\ncomputationally fast, and show their non-asymptotic behaviour in both one-sided\n(only positive correlations) and two-sided alternatives, respectively. We also\nexhibit a selector of significant lags and bound the Hamming-loss risk of the\nestimated support. These results can be extended to the case of nearly Toeplitz\ncovariance structure and to sub-Gaussian vectors. Numerical results illustrate\nthe excellent behaviour of both test procedures and support selectors - larger\nthe dimension $p$, faster are the rates.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 23:40:44 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bettache", "Nayel", ""], ["Butucea", "Cristina", ""], ["Sorba", "Marianne", ""]]}, {"id": "2102.06851", "submitter": "Juan Domingo Gonzalez", "authors": "Juan D. Gonzalez, Ricardo Maronna, Victor J. Yohai and Ruben H.Zamar", "title": "Robust Model-Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of robust and Fisher-consistent estimators for mixture\nmodels. These estimators can be used to construct robust model-based clustering\nprocedures. We study in detail the case of multivariate normal mixtures and\npropose a procedure that uses S estimators of multivariate location and\nscatter. We develop an algorithm to compute the estimators and to build the\nclusters which is quite similar to the EM algorithm. An extensive Monte Carlo\nsimulation study shows that our proposal compares favorably with other robust\nand non robust model-based clustering procedures. We apply ours and alternative\nprocedures to a real data set and again find that the best results are obtained\nusing our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 02:51:42 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 21:17:49 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 16:30:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gonzalez", "Juan D.", ""], ["Maronna", "Ricardo", ""], ["Yohai", "Victor J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "2102.06871", "submitter": "Yozo Tonaki", "authors": "Yozo Tonaki, Yusuke Kaino and Masayuki Uchida", "title": "Estimation for change point of discretely observed ergodic diffusion\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We treat the change point problem in ergodic diffusion processes from\ndiscrete observations. Tonaki et al. (2020) proposed adaptive tests for\ndetecting changes in the diffusion and drift parameters in ergodic diffusion\nmodels. When any changes are detected by this method, the next question to be\nconsidered is where the change point is. Therefore, we propose the method to\nestimate the change point of the parameter for two cases: the case where there\nis a change in the diffusion parameter, and the case where there is no change\nin the diffusion parameter but a change in the drift parameter. Furthermore, we\npresent rates of convergence and distributional results of the change point\nestimators. Some examples and simulation results are also given.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 06:34:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tonaki", "Yozo", ""], ["Kaino", "Yusuke", ""], ["Uchida", "Masayuki", ""]]}, {"id": "2102.07093", "submitter": "David Azriel", "authors": "David Azriel, Yosef Rinott and Martin Posch", "title": "Optimal designs for the development of personalized treatment rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present paper, personalized treatment means choosing the best\ntreatment for a patient while taking into account certain relevant personal\ncovariate values. We study the design of trials whose goal is to find the best\ntreatment for a given patient with given covariates. We assume that the\nsubjects in the trial represent a random sample from the population, and\nconsider the allocation, possibly with randomization, of these subjects to the\ndifferent treatment groups in a way that depends on their covariates.\n  We derive approximately optimal allocations, aiming to minimize expected\nregret, assuming that future patients will arrive from the same population as\nthe trial subjects. We find that, for the case of two treatments, an\napproximately optimal allocation design does not depend on the value of the\ncovariates but only on the variances of the responses. In contrast, for the\ncase of three treatments the optimal allocation design does depend on the\ncovariates as we show for specific scenarios. Another finding is that the\noptimal allocation can vary a lot as a function of the sample size, and that\nrandomized allocations are relevant for relatively small samples, and may not\nbe needed for very large studies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 07:35:29 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Azriel", "David", ""], ["Rinott", "Yosef", ""], ["Posch", "Martin", ""]]}, {"id": "2102.07162", "submitter": "Alexander Ly", "authors": "Alexander Ly and Eric-Jan Wagenmakers", "title": "Bayes Factors for Peri-Null Hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A perennial objection against Bayes factor point-null hypothesis tests is\nthat the point-null hypothesis is known to be false from the outset. Following\nMorey and Rouder (2011) we examine the consequences of approximating the sharp\npoint-null hypothesis by a hazy `peri-null' hypothesis instantiated as a narrow\nprior distribution centered on the point of interest. The peri-null Bayes\nfactor then equals the point-null Bayes factor multiplied by a correction term\nwhich is itself a Bayes factor. For moderate sample sizes, the correction term\nis relatively inconsequential; however, for large sample sizes the correction\nterm becomes influential and causes the peri-null Bayes factor to be\ninconsistent and approach a limit that depends on the ratio of prior ordinates\nevaluated at the maximum likelihood estimate. We characterize the asymptotic\nbehavior of the peri-null Bayes factor and discuss how to construct peri-null\nBayes factor hypothesis tests that are also consistent.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 14:44:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ly", "Alexander", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "2102.07203", "submitter": "Ilan Livne Mr", "authors": "Ilan Livne, David Azriel, Yair Goldberg", "title": "Improved Estimators for Semi-supervised High-dimensional Regression\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study a linear high-dimensional regression model in a semi-supervised\nsetting, where for many observations only the vector of covariates $X$ is given\nwith no response $Y$. We do not make any sparsity assumptions on the vector of\ncoefficients, and aim at estimating Var$(Y|X)$. We propose an estimator, which\nis unbiased, consistent, and asymptotically normal. This estimator can be\nimproved by adding zero-estimators arising from the unlabelled data. Adding\nzero-estimators does not affect the bias and potentially can reduce variance.\n  In order to achieve optimal improvement, many zero-estimators should be used,\nbut this raises the problem of estimating many parameters. Therefore, we\nintroduce covariate selection algorithms that identify which zero-estimators\nshould be used in order to improve the above estimator.\n  We further illustrate our approach for other estimators, and present an\nalgorithm that improves estimation for any given variance estimator. Our\ntheoretical results are demonstrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 17:31:04 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Livne", "Ilan", ""], ["Azriel", "David", ""], ["Goldberg", "Yair", ""]]}, {"id": "2102.07234", "submitter": "Conrad Slagle", "authors": "CNP Slagle", "title": "One Hundred Probability and Statistics Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herein we present one hundred inequalities culled from various corners of the\nprobability, statistics, and combinatorics literature. We welcome new\nsuggestions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 20:24:32 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Slagle", "CNP", ""]]}, {"id": "2102.07378", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee", "title": "Horseshoe shrinkage methods for Bayesian fusion estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimation and structure learning of high\ndimensional signals via a normal sequence model, where the underlying parameter\nvector is piecewise constant, or has a block structure. We develop a Bayesian\nfusion estimation method by using the Horseshoe prior to induce a strong\nshrinkage effect on successive differences in the mean parameters,\nsimultaneously imposing sufficient prior concentration for non-zero values of\nthe same. The proposed method thus facilitates consistent estimation and\nstructure recovery of the signal pieces. We provide theoretical justifications\nof our approach by deriving posterior convergence rates and establishing\nselection consistency under suitable assumptions. We also extend our proposed\nmethod to signal de-noising over arbitrary graphs and develop efficient\ncomputational methods along with providing theoretical guarantees. We\ndemonstrate the superior performance of the Horseshoe based Bayesian fusion\nestimation method through extensive simulations and two real-life examples on\nsignal de-noising in biological and geophysical applications. We also\ndemonstrate the estimation performance of our method on a real-world large\nnetwork for the graph signal de-noising problem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 07:38:57 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 05:37:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Banerjee", "Sayantan", ""]]}, {"id": "2102.07501", "submitter": "Michael Arbel", "authors": "Michael Arbel, Alexander G. D. G. Matthews, Arnaud Doucet", "title": "Annealed Flow Transport Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC)\nextensions are state-of-the-art methods for estimating normalizing constants of\nprobability distributions. We propose here a novel Monte Carlo algorithm,\nAnnealed Flow Transport (AFT), that builds upon AIS and SMC and combines them\nwith normalizing flows (NFs) for improved performance. This method transports a\nset of particles using not only importance sampling (IS), Markov chain Monte\nCarlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are\nlearned sequentially to push particles towards the successive annealed targets.\nWe provide limit theorems for the resulting Monte Carlo estimates of the\nnormalizing constant and expectations with respect to the target distribution.\nAdditionally, we show that a continuous-time scaling limit of the population\nversion of AFT is given by a Feynman--Kac measure which simplifies to the law\nof a controlled diffusion for expressive NFs. We demonstrate experimentally the\nbenefits and limitations of our methodology on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:05:56 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 12:32:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Arbel", "Michael", ""], ["Matthews", "Alexander G. D. G.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "2102.07556", "submitter": "Cyrus Mostajeran Dr", "authors": "Simon Heuveline, Salem Said, Cyrus Mostajeran", "title": "Gaussian distributions on Riemannian symmetric spaces in the large N\n  limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Gaussian distributions on certain Riemannian symmetric spaces. In\ncontrast to the Euclidean case, it is challenging to compute the normalization\nfactors of such distributions, which we refer to as partition functions. In\nsome cases, such as the space of Hermitian positive definite matrices or\nhyperbolic space, it is possible to compute them exactly using techniques from\nrandom matrix theory. However, in most cases which are important to\napplications, such as the space of symmetric positive definite (SPD) matrices\nor the Siegel domain, this is only possible numerically. Moreover, when we\nconsider, for instance, high-dimensional SPD matrices, the known algorithms for\ncomputing partition functions can become exceedingly slow. Motivated by notions\nfrom theoretical physics, we will discuss how to approximate the partition\nfunctions in the large $N$ limit: an approximation that gets increasingly\nbetter as the dimension of the underlying symmetric space (more precisely, its\nrank) gets larger. We will give formulas for leading order terms in the case of\nSPD matrices and related spaces. Furthermore, we will characterize the large\n$N$ limit of the Siegel domain through a singular integral equation arising as\na saddle-point equation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:52:12 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 16:27:05 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Heuveline", "Simon", ""], ["Said", "Salem", ""], ["Mostajeran", "Cyrus", ""]]}, {"id": "2102.07595", "submitter": "Vincent Divol", "authors": "Vincent Divol", "title": "Reconstructing measures on manifolds: an optimal transport approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assume that we observe i.i.d. points lying close to some unknown\n$d$-dimensional $\\mathcal{C}^k$ submanifold $M$ in a possibly high-dimensional\nspace. We study the problem of reconstructing the probability distribution\ngenerating the sample. After remarking that this problem is degenerate for a\nlarge class of standard losses ($L_p$, Hellinger, total variation, etc.), we\nfocus on the Wasserstein loss, for which we build an estimator, based on kernel\ndensity estimation, whose rate of convergence depends on $d$ and the regularity\n$s\\leq k-1$ of the underlying density, but not on the ambient dimension. In\nparticular, we show that the estimator is minimax and matches previous rates in\nthe literature in the case where the manifold $M$ is a $d$-dimensional cube.\nThe related problem of the estimation of the volume measure of $M$ for the\nWasserstein loss is also considered, for which a minimax estimator is\nexhibited.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:20:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Divol", "Vincent", ""]]}, {"id": "2102.07856", "submitter": "Yu Bai", "authors": "Yu Bai, Song Mei, Huan Wang, Caiming Xiong", "title": "Don't Just Blame Over-parametrization for Over-confidence: Theoretical\n  Analysis of Calibration in Binary Classification", "comments": "Appearing at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning models with high accuracy are often miscalibrated --\nthe predicted top probability does not reflect the actual accuracy, and tends\nto be over-confident. It is commonly believed that such over-confidence is\nmainly due to over-parametrization, in particular when the model is large\nenough to memorize the training data and maximize the confidence.\n  In this paper, we show theoretically that over-parametrization is not the\nonly reason for over-confidence. We prove that logistic regression is\ninherently over-confident, in the realizable, under-parametrized setting where\nthe data is generated from the logistic model, and the sample size is much\nlarger than the number of parameters. Further, this over-confidence happens for\ngeneral well-specified binary classification problems as long as the activation\nis symmetric and concave on the positive part. Perhaps surprisingly, we also\nshow that over-confidence is not always the case -- there exists another\nactivation function (and a suitable loss function) under which the learned\nclassifier is under-confident at some probability values. Overall, our theory\nprovides a precise characterization of calibration in realizable binary\nclassification, which we verify on simulations and real data experiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:38:09 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:21:37 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bai", "Yu", ""], ["Mei", "Song", ""], ["Wang", "Huan", ""], ["Xiong", "Caiming", ""]]}, {"id": "2102.07858", "submitter": "Leonid Sirota", "authors": "M.R.Formica, E.Ostrovsky, and L.Sirota", "title": "Signed variable optimal kernel for non-parametric density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the optimal signed variable in general case kernels for the\nclassical statistic density estimation, which are some generalization of the\nfamous Epanechnikov's ones.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:42:23 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Formica", "M. R.", ""], ["Ostrovsky", "E.", ""], ["Sirota", "L.", ""]]}, {"id": "2102.07867", "submitter": "Leonid Sirota", "authors": "M.R.Formica, E.Ostrovsky, and L.Sirota", "title": "Exponential confidence interval based on the recursive Wolverton-Wagner\n  density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the exponential non improvable Grand Lebesgue Space norm decreasing\nestimations for tail of distribution for exact normed deviation for the famous\nrecursive Wolverton-Wagner multivariate statistical density estimation. We\nconsider pointwise as well as Lebesgue-Riesz norm error of statistical density\nof measurement.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:15:53 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Formica", "M. R.", ""], ["Ostrovsky", "E.", ""], ["Sirota", "L.", ""]]}, {"id": "2102.07967", "submitter": "Dhruv Medarametla", "authors": "Dhruv Medarametla, Emmanuel J. Cand\\`es", "title": "Distribution-Free Conditional Median Inference", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing confidence intervals for the median\nof a response $Y \\in \\mathbb{R}$ conditional on features $X = x \\in\n\\mathbb{R}^d$ in a situation where we are not willing to make any assumption\nwhatsoever on the underlying distribution of the data $(X,Y)$. We propose a\nmethod based upon ideas from conformal prediction and establish a theoretical\nguarantee of coverage while also going over particular distributions where its\nperformance is sharp. Further, we provide a lower bound on the length of any\npossible conditional median confidence interval. This lower bound is\nindependent of sample size and holds for all distributions with no point\nmasses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 05:38:43 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Medarametla", "Dhruv", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2102.08057", "submitter": "James Hodgson", "authors": "James Hodgson, Adam M. Johansen, Murray Pollock", "title": "Unbiased simulation of rare events in continuous time", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For rare events described in terms of Markov processes, truly unbiased\nestimation of the rare event probability generally requires the avoidance of\nnumerical approximations of the Markov process. Recent work in the exact and\n$\\varepsilon$-strong simulation of diffusions, which can be used to almost\nsurely constrain sample paths to a given tolerance, suggests one way to do\nthis. We specify how such algorithms can be combined with the classical\nmultilevel splitting method for rare event simulation. This provides unbiased\nestimations of the probability in question. We discuss the practical\nfeasibility of the algorithm with reference to existing $\\varepsilon$-strong\nmethods and provide proof-of-concept numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 10:10:53 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Hodgson", "James", ""], ["Johansen", "Adam M.", ""], ["Pollock", "Murray", ""]]}, {"id": "2102.08127", "submitter": "Bruno Loureiro", "authors": "Bruno Loureiro, C\\'edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent\n  Krzakala, Marc M\\'ezard, Lenka Zdeborov\\'a", "title": "Learning curves of generic features maps for realistic datasets with a\n  teacher-student model", "comments": "main: 11 pages, 5 figures; appendix: 50 pages, 4 figures; v2: revised\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teacher-student models provide a framework in which the typical-case\nperformance of high-dimensional supervised learning can be described in closed\nform. The assumptions of Gaussian i.i.d. input data underlying the canonical\nteacher-student model may, however, be perceived as too restrictive to capture\nthe behaviour of realistic data sets. In this paper, we introduce a Gaussian\ncovariate generalisation of the model where the teacher and student can act on\ndifferent spaces, generated with fixed, but generic feature maps. While still\nsolvable in a closed form, this generalization is able to capture the learning\ncurves for a broad range of realistic data sets, thus redeeming the potential\nof the teacher-student framework. Our contribution is then two-fold: First, we\nprove a rigorous formula for the asymptotic training loss and generalisation\nerror. Second, we present a number of situations where the learning curve of\nthe model captures the one of a realistic data set learned with kernel\nregression and classification, with out-of-the-box feature maps such as random\nprojections or scattering transforms, or with pre-learned ones - such as the\nfeatures learned by training multi-layer neural networks. We discuss both the\npower and the limitations of the framework.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 12:49:15 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 15:19:46 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Loureiro", "Bruno", ""], ["Gerbelot", "C\u00e9dric", ""], ["Cui", "Hugo", ""], ["Goldt", "Sebastian", ""], ["Krzakala", "Florent", ""], ["M\u00e9zard", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2102.08178", "submitter": "Nicolas Marie", "authors": "Pierre Alquier, Nicolas Marie and Am\\'elie Rosier", "title": "Tight Risk Bound for High Dimensional Time Series Completion", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initially designed for independent datas, low-rank matrix completion was\nsuccessfully applied in many domains to the reconstruction of partially\nobserved high-dimensional time series. However, there is a lack of theory to\nsupport the application of these methods to dependent datas. In this paper, we\npropose a general model for multivariate, partially observed time series. We\nshow that the least-square method with a rank penalty leads to reconstruction\nerror of the same order as for independent datas. Moreover, when the time\nseries has some additional properties such as periodicity or smoothness, the\nrate can actually be faster than in the independent case.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:49:14 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 11:06:05 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Alquier", "Pierre", ""], ["Marie", "Nicolas", ""], ["Rosier", "Am\u00e9lie", ""]]}, {"id": "2102.08184", "submitter": "Assaf Ben-Yishai", "authors": "Assaf Ben-Yishai and Or Ordentlich", "title": "Constructing Multiclass Classifiers using Binary Classifiers Under\n  Log-Loss", "comments": "A shorter version of this contribution was submitted to ISIT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The construction of multiclass classifiers from binary classifiers is studied\nin this paper, and performance is quantified by the regret, defined with\nrespect to the Bayes optimal log-loss. We start by proving that the regret of\nthe well known One vs. All (OVA) method is upper bounded by the sum of the\nregrets of its constituent binary classifiers. We then present a new method\ncalled Conditional OVA (COVA), and prove that its regret is given by the\nweighted sum of the regrets corresponding to the constituent binary\nclassifiers. Lastly, we present a method termed Leveraged COVA (LCOVA),\ndesignated to reduce the regret of a multiclass classifier by breaking it down\nto independently optimized binary classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 14:34:59 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ben-Yishai", "Assaf", ""], ["Ordentlich", "Or", ""]]}, {"id": "2102.08373", "submitter": "Phan-Minh Nguyen", "authors": "Phan-Minh Nguyen", "title": "Analysis of feature learning in weight-tied autoencoders via the mean\n  field lens", "comments": "121 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders are among the earliest introduced nonlinear models for\nunsupervised learning. Although they are widely adopted beyond research, it has\nbeen a longstanding open problem to understand mathematically the feature\nextraction mechanism that trained nonlinear autoencoders provide.\n  In this work, we make progress in this problem by analyzing a class of\ntwo-layer weight-tied nonlinear autoencoders in the mean field framework. Upon\na suitable scaling, in the regime of a large number of neurons, the models\ntrained with stochastic gradient descent are shown to admit a mean field\nlimiting dynamics. This limiting description reveals an asymptotically precise\npicture of feature learning by these models: their training dynamics exhibit\ndifferent phases that correspond to the learning of different principal\nsubspaces of the data, with varying degrees of nonlinear shrinkage dependent on\nthe $\\ell_{2}$-regularization and stopping time. While we prove these results\nunder an idealized assumption of (correlated) Gaussian data, experiments on\nreal-life data demonstrate an interesting match with the theory.\n  The autoencoder setup of interests poses a nontrivial mathematical challenge\nto proving these results. In this setup, the \"Lipschitz\" constants of the\nmodels grow with the data dimension $d$. Consequently an adaptation of previous\nanalyses requires a number of neurons $N$ that is at least exponential in $d$.\nOur main technical contribution is a new argument which proves that the\nrequired $N$ is only polynomial in $d$. We conjecture that $N\\gg d$ is\nsufficient and that $N$ is necessarily larger than a data-dependent intrinsic\ndimension, a behavior that is fundamentally different from previously studied\nsetups.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:58:37 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Nguyen", "Phan-Minh", ""]]}, {"id": "2102.08483", "submitter": "Jin H. An", "authors": "J. An", "title": "Sample variance of rounded variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST astro-ph.IM physics.data-an stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If the rounding errors are assumed to be distributed independently from the\nintrinsic distribution of the random variable, the sample variance $s^2$ of the\nrounded variable is given by the sum of the true variance $\\sigma^2$ and the\nvariance of the rounding errors (which is equal to $w^2/12$ where $w$ is the\nsize of the rounding window). Here the exact expressions for the sample\nvariance of the rounded variables are examined and it is also discussed when\nthe simple approximation $s^2=\\sigma^2+w^2/12$ can be considered valid. In\nparticular, if the underlying distribution $f$ belongs to a family of symmetric\nnormalizable distributions such that $f(x)=\\sigma^{-1}F(u)$ where\n$u=(x-\\mu)/\\sigma$, and $\\mu$ and $\\sigma^2$ are the mean and variance of the\ndistribution, then the rounded sample variance scales like\n$s^2-(\\sigma^2+w^2/12)\\sim\\sigma\\Phi'(\\sigma)$ as $\\sigma\\to\\infty$ where\n$\\Phi(\\tau)=\\int_{-\\infty}^\\infty{\\rm d}u\\,e^{iu\\tau}F(u)$ is the\ncharacteristic function of $F(u)$. It follows that, roughly speaking, the\napproximation is valid for a slowly-varying symmetric underlying distribution\nwith its variance sufficiently larger than the size of the rounding unit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:52:18 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["An", "J.", ""]]}, {"id": "2102.08644", "submitter": "Alberto Gonzalez-Sanz", "authors": "Lucas de Lara (IMT), Alberto Gonz\\'alez-Sanz (IMT), Jean-Michel Loubes\n  (IMT)", "title": "A Consistent Extension of Discrete Optimal Transport Maps for Machine\n  Learning Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport maps define a one-to-one correspondence between probability\ndistributions, and as such have grown popular for machine learning\napplications. However, these maps are generally defined on empirical\nobservations and cannot be generalized to new samples while preserving\nasymptotic properties. We extend a novel method to learn a consistent estimator\nof a continuous optimal transport map from two empirical distributions. The\nconsequences of this work are two-fold: first, it enables to extend the\ntransport plan to new observations without computing again the discrete optimal\ntransport map; second, it provides statistical guarantees to machine learning\napplications of optimal transport. We illustrate the strength of this approach\nby deriving a consistent framework for transport-based counterfactual\nexplanations in fairness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 09:22:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["de Lara", "Lucas", "", "IMT"], ["Gonz\u00e1lez-Sanz", "Alberto", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "2102.08782", "submitter": "Efstathia Bura", "authors": "Lukas Fertl and Efstathia Bura", "title": "Conditional Variance Estimator for Sufficient Dimension Reduction", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Variance Estimation (CVE) is a novel sufficient dimension\nreduction (SDR) method for additive error regressions with continuous\npredictors and link function. It operates under the assumption that the\npredictors can be replaced by a lower dimensional projection without loss of\ninformation. In contrast to the majority of moment based sufficient dimension\nreduction methods, Conditional Variance Estimation is fully data driven, does\nnot require the restrictive linearity and constant variance conditions, and is\nnot based on inverse regression. CVE is shown to be consistent and its\nobjective function to be uniformly convergent. CVE outperforms the mean average\nvariance estimation, (MAVE), its main competitor, in several simulation\nsettings, remains on par under others, while it always outperforms the usual\ninverse regression based linear SDR methods, such as Sliced Inverse Regression.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:17:54 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Fertl", "Lukas", ""], ["Bura", "Efstathia", ""]]}, {"id": "2102.09053", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng", "title": "Estimating The Proportion of Signal Variables Under Arbitrary Covariance\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the proportion of signals hidden in a large amount of noise\nvariables is of interest in many scientific inquires. In this paper, we\nconsider realistic but theoretically challenging settings with arbitrary\ncovariance dependence between variables. We define mean absolute correlation\n(MAC) to measure the overall dependence level and investigate a family of\nestimators for their performances in the full range of MAC. We explicit the\njoint effect of MAC dependence and signal sparsity on the performances of the\nfamily of estimators and discover that no single estimator in the family is\nmost powerful under different MAC dependence levels. Informed by the\ntheoretical insight, we propose a new estimator to better adapt to arbitrary\ncovariance dependence. The proposed method compares favorably to several\nexisting methods in extensive finite-sample settings with strong to weak\ncovariance dependence and real dependence structures from genetic association\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 22:20:13 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:45:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Jeng", "X. Jessie", ""]]}, {"id": "2102.09204", "submitter": "Hugo Lavenant", "authors": "Hugo Lavenant, Stephen Zhang, Young-Heon Kim, Geoffrey Schiebinger", "title": "Towards a mathematical theory of trajectory inference", "comments": "The first two authors contributed equally to this work; 62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise a theoretical framework and a numerical method to infer\ntrajectories of a stochastic process from snapshots of its temporal marginals.\nThis problem arises in the analysis of single cell RNA-sequencing data, which\nprovide high dimensional measurements of cell states but cannot track the\ntrajectories of the cells over time. We prove that for a class of stochastic\nprocesses it is possible to recover the ground truth trajectories from limited\nsamples of the temporal marginals at each time-point, and provide an efficient\nalgorithm to do so in practice. The method we develop, Global Waddington-OT\n(gWOT), boils down to a smooth convex optimization problem posed globally over\nall time-points involving entropy-regularized optimal transport. We demonstrate\nthat this problem can be solved efficiently in practice and yields good\nreconstructions, as we show on several synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 07:58:47 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lavenant", "Hugo", ""], ["Zhang", "Stephen", ""], ["Kim", "Young-Heon", ""], ["Schiebinger", "Geoffrey", ""]]}, {"id": "2102.09293", "submitter": "Charles Arnal", "authors": "Charles Arnal", "title": "Convolution of a symmetric log-concave distribution and a symmetric\n  bimodal distribution can have any number of modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we show that the convolution of a discrete symmetric\nlog-concave distribution and a discrete symmetric bimodal distribution can have\nany strictly positive number of modes. A similar result is proved for smooth\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:14:34 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Arnal", "Charles", ""]]}, {"id": "2102.09385", "submitter": "Steffen Dereich", "authors": "Steffen Dereich and Sebastian Kassing", "title": "Convergence of stochastic gradient descent schemes for\n  Lojasiewicz-landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider convergence of stochastic gradient descent\nschemes (SGD) under weak assumptions on the underlying landscape. More\nexplicitly, we show that on the event that the SGD stays local we have\nconvergence of the SGD if there is only a countable number of critical points\nor if the target function/landscape satisfies Lojasiewicz-inequalities around\nall critical levels as all analytic functions do. In particular, we show that\nfor neural networks with analytic activation function such as softplus, sigmoid\nand the hyperbolic tangent, SGD converges on the event of staying local, if the\nrandom variables modeling the signal and response in the training are compactly\nsupported.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 12:42:25 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:00:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Dereich", "Steffen", ""], ["Kassing", "Sebastian", ""]]}, {"id": "2102.09448", "submitter": "Xinwei Deng", "authors": "Xiaoning Kang, Lulu Kang, Wei Chen and Xinwei Deng", "title": "A Generative Approach to Joint Modeling of Quantitative and Qualitative\n  Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many scientific areas, data with quantitative and qualitative (QQ)\nresponses are commonly encountered with a large number of predictors. By\nexploring the association between QQ responses, existing approaches often\nconsider a joint model of QQ responses given the predictor variables. However,\nthe dependency among predictive variables also provides useful information for\nmodeling QQ responses. In this work, we propose a generative approach to model\nthe joint distribution of the QQ responses and predictors. The proposed\ngenerative model provides efficient parameter estimation under a penalized\nlikelihood framework. It achieves accurate classification for qualitative\nresponse and accurate prediction for quantitative response with efficient\ncomputation. Because of the generative approach framework, the asymptotic\noptimality of classification and prediction of the proposed method can be\nestablished under some regularity conditions. The performance of the proposed\nmethod is examined through simulations and real case studies in material\nscience and genetics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 16:10:45 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Kang", "Xiaoning", ""], ["Kang", "Lulu", ""], ["Chen", "Wei", ""], ["Deng", "Xinwei", ""]]}, {"id": "2102.09497", "submitter": "Alina Kumukova", "authors": "Miguel de Carvalho, Gon\\c{c}alo dos Reis, Alina Kumukova", "title": "Regression-type analysis for block maxima on block maxima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper devises a regression-type model for the situation where both the\nresponse and covariates are extreme. The proposed approach is designed for the\nsetting where both the response and covariates are themselves block maxima, and\nthus contrarily to standard regression methods it takes into account the key\nfact that the limiting distribution of suitably standardized componentwise\nmaxima is an extreme value copula. An important target in the proposed\nframework is the regression manifold, which consists of a family of regression\nlines obeying the latter asymptotic result. To learn about the proposed model\nfrom data, we employ a Bernstein polynomial prior on the space of angular\ndensities which leads to an induced prior on the space of regression manifolds.\nNumerical studies suggest a good performance of the proposed methods, and a\nfinance real-data illustration reveals interesting aspects on the comovements\nof extreme losses between two leading stock markets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:27:44 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:00:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Reis", "Gon\u00e7alo dos", ""], ["Kumukova", "Alina", ""]]}, {"id": "2102.09504", "submitter": "David Obst", "authors": "David Obst and Badih Ghattas and Jairo Cugliari and Georges Oppenheim\n  and Sandra Claudel and Yannig Goude", "title": "Transfer Learning for Linear Regression: a Statistical Test of Gain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning, also referred as knowledge transfer, aims at reusing\nknowledge from a source dataset to a similar target one. While many empirical\nstudies illustrate the benefits of transfer learning, few theoretical results\nare established especially for regression problems. In this paper a theoretical\nframework for the problem of parameter transfer for the linear model is\nproposed. It is shown that the quality of transfer for a new input vector $x$\ndepends on its representation in an eigenbasis involving the parameters of the\nproblem. Furthermore a statistical test is constructed to predict whether a\nfine-tuned model has a lower prediction quadratic risk than the base target\nmodel for an unobserved sample. Efficiency of the test is illustrated on\nsynthetic data as well as real electricity consumption data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 17:46:26 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Obst", "David", ""], ["Ghattas", "Badih", ""], ["Cugliari", "Jairo", ""], ["Oppenheim", "Georges", ""], ["Claudel", "Sandra", ""], ["Goude", "Yannig", ""]]}, {"id": "2102.09526", "submitter": "Tapio Helin", "authors": "Tatiana A. Bubba and Martin Burger and Tapio Helin and Luca Ratti", "title": "Convex regularization in statistical inverse learning problems", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical inverse learning problem, where the task is to\nestimate a function $f$ based on noisy point evaluations of $Af$, where $A$ is\na linear operator. The function $Af$ is evaluated at i.i.d. random design\npoints $u_n$, $n=1,...,N$ generated by an unknown general probability\ndistribution. We consider Tikhonov regularization with general convex and\n$p$-homogeneous penalty functionals and derive concentration rates of the\nregularized solution to the ground truth measured in the symmetric Bregman\ndistance induced by the penalty functional. We derive concrete rates for Besov\nnorm penalties and numerically demonstrate the correspondence with the observed\nrates in the context of X-ray tomography.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:12:08 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 07:29:17 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bubba", "Tatiana A.", ""], ["Burger", "Martin", ""], ["Helin", "Tapio", ""], ["Ratti", "Luca", ""]]}, {"id": "2102.09540", "submitter": "Nikos Karampatziakis", "authors": "Nikos Karampatziakis, Paul Mineiro, Aaditya Ramdas", "title": "Off-policy Confidence Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop confidence bounds that hold uniformly over time for off-policy\nevaluation in the contextual bandit setting. These confidence sequences are\nbased on recent ideas from martingale analysis and are non-asymptotic,\nnon-parametric, and valid at arbitrary stopping times. We provide algorithms\nfor computing these confidence sequences that strike a good balance between\ncomputational and statistical efficiency. We empirically demonstrate the\ntightness of our approach in terms of failure probability and width and apply\nit to the \"gated deployment\" problem of safely upgrading a production\ncontextual bandit system.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:40:30 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Karampatziakis", "Nikos", ""], ["Mineiro", "Paul", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2102.09552", "submitter": "Bo Waggoner", "authors": "Bo Waggoner", "title": "Linear Functions to the Extended Reals", "comments": "working draft; 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.GT stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This note investigates functions from $\\mathbb{R}^d$ to $\\mathbb{R} \\cup\n\\{\\pm \\infty\\}$ that satisfy axioms of linearity wherever allowed by\nextended-value arithmetic. They have a nontrivial structure defined inductively\non $d$, and unlike finite linear functions, they require $\\Omega(d^2)$\nparameters to uniquely identify. In particular they can capture vertical\ntangent planes to epigraphs: a function (never $-\\infty$) is convex if and only\nif it has an extended-valued subgradient at every point in its effective\ndomain, if and only if it is the supremum of a family of \"affine extended\"\nfunctions. These results are applied to the well-known characterization of\nproper scoring rules, for the finite-dimensional case: it is carefully and\nrigorously extended here to a more constructive form. In particular it is\ninvestigated when proper scoring rules can be constructed from a given convex\nfunction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 18:53:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Waggoner", "Bo", ""]]}, {"id": "2102.09606", "submitter": "Lorenz Richter", "authors": "Carsten Hartmann, Lorenz Richter", "title": "Nonasymptotic bounds for suboptimal importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Importance sampling is a popular variance reduction method for Monte Carlo\nestimation, where a notorious question is how to design good proposal\ndistributions. While in most cases optimal (zero-variance) estimators are\ntheoretically possible, in practice only suboptimal proposal distributions are\navailable and it can often be observed numerically that those can reduce\nstatistical performance significantly, leading to large relative errors and\ntherefore counteracting the original intention. In this article, we provide\nnonasymptotic lower and upper bounds on the relative error in importance\nsampling that depend on the deviation of the actual proposal from optimality,\nand we thus identify potential robustness issues that importance sampling may\nhave, especially in high dimensions. We focus on path sampling problems for\ndiffusion processes, for which generating good proposals comes with additional\ntechnical challenges, and we provide numerous numerical examples that support\nour findings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 20:41:43 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Hartmann", "Carsten", ""], ["Richter", "Lorenz", ""]]}, {"id": "2102.09637", "submitter": "Artur O. Lopes", "authors": "M.J. Karling, A.O. Lopes and S.R.C. Lopes", "title": "Explicit Bivariate Rate Functions for Large Deviations in AR(1) and\n  MA(1) Processes with Gaussian Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate large deviations properties for centered stationary AR(1) and\nMA(1) processes with independent Gaussian innovations, by giving the explicit\nbivariate rate functions for the sequence of random vectors\n$(\\boldsymbol{S}_n)_{n \\in \\N} = \\left(n^{-1}(\\sum_{k=1}^n X_k, \\sum_{k=1}^n\nX_k^2)\\right)_{n \\in \\N}$. In the AR(1) case, we also give the explicit rate\nfunction for the bivariate random sequence $(\\W_n)_{n \\geq 2} =\n\\left(n^{-1}(\\sum_{k=1}^n X_k^2, \\sum_{k=2}^n X_k X_{k+1})\\right)_{n \\geq\n  2}$. Via Contraction Principle, we provide explicit rate functions for the\nsequences $(n^{-1} \\sum_{k=1}^n X_k)_{n \\in \\N}$, $(n^{-1} \\sum_{k=1}^n\nX_k^2)_{n \\geq 2}$ and $(n^{-1} \\sum_{k=2}^n X_k X_{k+1})_{n \\geq 2}$, as well.\nIn the AR(1) case, we present a new proof for an already known result on the\nexplicit deviation function for the Yule-Walker estimator.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:55:26 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Karling", "M. J.", ""], ["Lopes", "A. O.", ""], ["Lopes", "S. R. C.", ""]]}, {"id": "2102.09879", "submitter": "Jonathan Larson", "authors": "Jonathan Larson and Jukka-Pekka Onnela", "title": "Inferring the minimum spanning tree from a sample network", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum spanning trees (MSTs) are used in a variety of fields, from computer\nscience to geography. Infectious disease researchers have used them to infer\nthe transmission pathway of certain pathogens. However, these are often the\nMSTs of sample networks, not population networks, and surprisingly little is\nknown about what can be inferred about a population MST from a sample MST. We\nprove that if $n$ nodes (the sample) are selected uniformly at random from a\ncomplete graph with $N$ nodes and unique edge weights (the population), the\nprobability that an edge is in the population graph's MST given that it is in\nthe sample graph's MST is $\\frac{n}{N}$. We use simulation to investigate this\nconditional probability for $G(N,p)$ graphs, Barab\\'{a}si-Albert (BA) graphs,\ngraphs whose nodes are distributed in $\\mathbb{R}^2$ according to a bivariate\nstandard normal distribution, and an empirical HIV genetic distance network.\nBroadly, results for the complete, $G(N,p)$, and normal graphs are similar, and\nresults for the BA and empirical HIV graphs are similar. We recommend that\nresearchers use an edge-weighted random walk to sample nodes from the\npopulation so that they maximize the probability that an edge is in the\npopulation MST given that it is in the sample MST.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 11:41:21 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 15:55:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Larson", "Jonathan", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2102.09912", "submitter": "Jan O. Bauer", "authors": "Jan O. Bauer", "title": "Correlation Based Principal Loading Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Principal loading analysis is a dimension reduction method that discards\nvariables which have only a small distorting effect on the covariance matrix.\nWe complement principal loading analysis and propose to rather use a mix of\nboth, the correlation and covariance matrix instead. Further, we suggest to use\nrescaled eigenvectors and provide updated algorithms for all proposed changes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:08:30 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bauer", "Jan O.", ""]]}, {"id": "2102.09924", "submitter": "Adrian Riekert", "authors": "Patrick Cheridito, Arnulf Jentzen, Adrian Riekert, Florian Rossmannek", "title": "A proof of convergence for gradient descent in the training of\n  artificial neural networks for constant target functions", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent optimization algorithms are the standard ingredients that\nare used to train artificial neural networks (ANNs). Even though a huge number\nof numerical simulations indicate that gradient descent optimization methods do\nindeed convergence in the training of ANNs, until today there is no rigorous\ntheoretical analysis which proves (or disproves) this conjecture. In\nparticular, even in the case of the most basic variant of gradient descent\noptimization algorithms, the plain vanilla gradient descent method, it remains\nan open problem to prove or disprove the conjecture that gradient descent\nconverges in the training of ANNs. In this article we solve this problem in the\nspecial situation where the target function under consideration is a constant\nfunction. More specifically, in the case of constant target functions we prove\nin the training of rectified fully-connected feedforward ANNs with one-hidden\nlayer that the risk function of the gradient descent method does indeed\nconverge to zero. Our mathematical analysis strongly exploits the property that\nthe rectifier function is the activation function used in the considered ANNs.\nA key contribution of this work is to explicitly specify a Lyapunov function\nfor the gradient flow system of the ANN parameters. This Lyapunov function is\nthe central tool in our convergence proof of the gradient descent method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:33:03 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Cheridito", "Patrick", ""], ["Jentzen", "Arnulf", ""], ["Riekert", "Adrian", ""], ["Rossmannek", "Florian", ""]]}, {"id": "2102.10039", "submitter": "Miao-jung Ou", "authors": "Chuan Bi, Miao-jung Yvonne Ou, Wenshu Qian, Kenneth W. Fishbein,\n  Mustapha Bouhrara, Richard G. Spencer", "title": "Multi-Regularization Reconstruction of One-Dimensional $T_2$\n  Distributions in Magnetic Resonance Relaxometry with a Gaussian Basis", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the inverse problem of recovering the probability distribution\nfunction of $T_2$ relaxation times from NMR transverse relaxometry experiments.\nThis problem is a variant of the inverse Laplace transform and hence ill-posed.\nWe cast this within the framework of a Gaussian mixture model to obtain a\nleast-square problem with an $L_2$ regularization term. We propose a new method\nfor incorporating regularization into the solution; rather than seeking to\nreplace the native problem with a suitable mathematically close, regularized,\nversion, we instead augment the native formulation with regularization. We term\nthis new approach 'multi-regularization'; it avoids the treacherous process of\nselecting a single best regularization parameter $\\lambda$ and instead permits\nincorporation of several degrees of regularization into the solution. We\nillustrate the method with extensive simulation results as well as application\nto real experimental data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:17:40 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:01:16 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Bi", "Chuan", ""], ["Ou", "Miao-jung Yvonne", ""], ["Qian", "Wenshu", ""], ["Fishbein", "Kenneth W.", ""], ["Bouhrara", "Mustapha", ""], ["Spencer", "Richard G.", ""]]}, {"id": "2102.10080", "submitter": "Guang Cheng", "authors": "Yang Yu, Shih-Kang Chao, Guang Cheng", "title": "Distributed Bootstrap for Simultaneous Inference Under High\n  Dimensionality", "comments": "arXiv admin note: text overlap with arXiv:2002.08443", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a distributed bootstrap method for simultaneous inference on\nhigh-dimensional massive data that are stored and processed with many machines.\nThe method produces a $\\ell_\\infty$-norm confidence region based on a\ncommunication-efficient de-biased lasso, and we propose an efficient\ncross-validation approach to tune the method at every iteration. We\ntheoretically prove a lower bound on the number of communication rounds\n$\\tau_{\\min}$ that warrants the statistical accuracy and efficiency.\nFurthermore, $\\tau_{\\min}$ only increases logarithmically with the number of\nworkers and intrinsic dimensionality, while nearly invariant to the nominal\ndimensionality. We test our theory by extensive simulation studies, and a\nvariable screening task on a semi-synthetic dataset based on the US Airline\nOn-time Performance dataset. The code to reproduce the numerical results is\navailable at GitHub: https://github.com/skchao74/Distributed-bootstrap.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:28:29 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yu", "Yang", ""], ["Chao", "Shih-Kang", ""], ["Cheng", "Guang", ""]]}, {"id": "2102.10166", "submitter": "Ezzedine Mliki", "authors": "Ezzedine Mliki and Shaykhah Alajmi", "title": "Mixed Generalized Fractional Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To extend several known centered Gaussian processes, we introduce a new\ncentered mixed self-similar Gaussian process called the mixed generalized\nfractional Brownian motion, which could serve as a good model for a larger\nclass of natural phenomena. This process generalizes both the well known mixed\nfractional Brownian motion introduced by Cheridito [10] and the generalized\nfractional Brownian motion introduced by Zili [31]. We study its main\nstochastic properties, its non-Markovian and non-stationarity characteristics\nand the conditions under which it is not a semimartingale. We prove the long\nrange dependence properties of this process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:36:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mliki", "Ezzedine", ""], ["Alajmi", "Shaykhah", ""]]}, {"id": "2102.10180", "submitter": "Ezzedine Mliki", "authors": "Ezzedine Mliki and Shaykhah Alajmi", "title": "On the long range dependence of time-changed mixed fractional Brownian\n  motion model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A time-changed mixed fractional Brownian motion is an iterated process\nconstructed as the superposition of mixed fractional Brownian motion and other\nprocess. In this paper we consider mixed fractional Brownian motion of\nparameters a, b and H\\in(0, 1) time-changed by two processes, gamma and\ntempered stable subordinators. We present their main properties paying main\nattention to the long range dependence. We deduce that the fractional Brownian\nmotion time-changed by gamma and tempered stable subordinators has long range\ndependence property for all H\\in(0, 1).\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:14:04 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mliki", "Ezzedine", ""], ["Alajmi", "Shaykhah", ""]]}, {"id": "2102.10196", "submitter": "Romain Cosson", "authors": "Romain Cosson, Devavrat Shah", "title": "Approximating the Log-Partition Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approximation, such as mean-field (MF) and tree-reweighted (TRW),\nprovide a computationally efficient approximation of the log-partition function\nfor a generic graphical model. TRW provably provides an upper bound, but the\napproximation ratio is generally not quantified.\n  As the primary contribution of this work, we provide an approach to quantify\nthe approximation ratio through the property of the underlying graph structure.\nSpecifically, we argue that (a variant of) TRW produces an estimate that is\nwithin factor $\\frac{1}{\\sqrt{\\kappa(G)}}$ of the true log-partition function\nfor any discrete pairwise graphical model over graph $G$, where $\\kappa(G) \\in\n(0,1]$ captures how far $G$ is from tree structure with $\\kappa(G) = 1$ for\ntrees and $2/N$ for the complete graph over $N$ vertices. As a consequence, the\napproximation ratio is $1$ for trees, $\\sqrt{(d+1)/2}$ for any graph with\nmaximum average degree $d$, and $\\stackrel{\\beta\\to\\infty}{\\approx}\n1+1/(2\\beta)$ for graphs with girth (shortest cycle) at least $\\beta \\log N$.\nIn general, $\\kappa(G)$ is the solution of a max-min problem associated with\n$G$ that can be evaluated in polynomial time for any graph.\n  Using samples from the uniform distribution over the spanning trees of G, we\nprovide a near linear-time variant that achieves an approximation ratio equal\nto the inverse of square-root of minimal (across edges) effective resistance of\nthe graph. We connect our results to the graph partition-based approximation\nmethod and thus provide a unified perspective.\n  Keywords: variational inference, log-partition function, spanning tree\npolytope, minimum effective resistance, min-max spanning tree, local inference\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:57:32 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cosson", "Romain", ""], ["Shah", "Devavrat", ""]]}, {"id": "2102.10226", "submitter": "Teng Zhang", "authors": "Xing Fan, Marianna Pensky, Feng Yu, Teng Zhang", "title": "ALMA: Alternating Minimization Algorithm for Clustering Mixture\n  Multilayer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a Mixture Multilayer Stochastic Block Model (MMLSBM),\nwhere layers can be partitioned into groups of similar networks, and networks\nin each group are equipped with a distinct Stochastic Block Model. The goal is\nto partition the multilayer network into clusters of similar layers, and to\nidentify communities in those layers. Jing et al. (2020) introduced the MMLSBM\nand developed a clustering methodology, TWIST, based on regularized tensor\ndecomposition.\n  The present paper proposes a different technique, an alternating minimization\nalgorithm (ALMA), that aims at simultaneous recovery of the layer partition,\ntogether with estimation of the matrices of connection probabilities of the\ndistinct layers. Compared to TWIST, ALMA achieves higher accuracy both\ntheoretically and numerically.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 01:26:55 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 17:15:18 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 19:52:03 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Fan", "Xing", ""], ["Pensky", "Marianna", ""], ["Yu", "Feng", ""], ["Zhang", "Teng", ""]]}, {"id": "2102.10324", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Necessary and sufficient graphical conditions for optimal adjustment\n  sets in causal graphical models with hidden variables", "comments": "35 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of selecting optimal valid backdoor adjustment sets to estimate\ncausal effects in graphical models with hidden and conditioned variables is\naddressed. Previous work has defined optimality as achieving the smallest\nasymptotic variance compared to other adjustment sets and identified a\ngraphical criterion for an optimal set for the case without hidden variables.\nFor the case with hidden variables currently a sufficient graphical criterion\nand a corresponding construction algorithm exists. Here optimality is\ncharacterized by an information-theoretic approach based on the conditional\nmutual informations among cause, effect, adjustment set, and conditioned\nvariables. This characterization allows to derive the main contributions of\nthis paper: A necessary and sufficient graphical criterion for the existence of\nan optimal adjustment set and a definition and algorithm to construct it.\nFurther, the optimal set is valid if and only if a valid adjustment set exists\nand has smaller (or equal) asymptotic variance compared to the Adjust-set\nproposed in Perkovic et al. (2018) (arXiv:1606.06903) for any graph, whether\ngraphical optimality holds or not. The results are valid for a class of\nestimators whose asymptotic variance follows a certain information-theoretic\nrelation. Numerical experiments indicate that the asymptotic results also hold\nfor relatively small sample sizes. For estimators outside of the class studied\nhere none of the considered adjustment sets outperforms all others, but a\nminimized variant of the optimal set proposed here tends to have lower\nvariance. Surprisingly, among the randomly created setups more than 80\\%\nfulfill the optimality conditions indicating that also in many real-world\nscenarios graphical optimality may hold. Code is available as part of the\npython package \\url{https://github.com/jakobrunge/tigramite}.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 12:25:06 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 15:17:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "2102.10429", "submitter": "Yifan Yang", "authors": "Yifan Yang and Xiaoyu Zhou", "title": "A Note on Taylor's Expansion and Mean Value Theorem With Respect to a\n  Random Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a stochastic version of Taylor's expansion and Mean Value\nTheorem, originally proved by Aliprantis and Border (1999), and extend them to\na multivariate case. For a univariate case, the theorem asserts that \"suppose a\nreal-valued function $f$ has a continuous derivative $f'$ on a closed interval\n$I$ and $X$ is a random variable on a probability space $(\\Omega, \\mathcal{F},\nP)$. Fix $a \\in I$, there exists a \\textit{random variable} $\\xi$ such that\n$\\xi(\\omega) \\in I$ for every $\\omega \\in \\Omega$ and $f(X(\\omega)) = f(a) +\nf'(\\xi(\\omega))(X(\\omega) - a)$.\" The proof is not trivial. By applying these\nresults in statistics, one may simplify some details in the proofs of the Delta\nmethod or the asymptotic properties for a maximum likelihood estimator. In\nparticular, when mentioning \"there exists $\\theta ^ *$ between $\\hat{\\theta}$\n(a maximum likelihood estimator) and $\\theta_0$ (the true value)\", a stochastic\nversion of Mean Value Theorem guarantees $\\theta ^ *$ is a random variable (or\na random vector).\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 20:02:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Yifan", ""], ["Zhou", "Xiaoyu", ""]]}, {"id": "2102.10558", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "Kolos Csaba \\'Agoston and L\\'aszl\\'o Csat\\'o", "title": "Inconsistency thresholds for incomplete pairwise comparison matrices", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI math.OC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison matrices are increasingly used in settings where some\npairs are missing. However, there exist few inconsistency indices for similar\nincomplete data sets and no reasonable measure has an associated threshold.\nThis paper generalises the famous rule of thumb for the acceptable level of\ninconsistency, proposed by Saaty, to incomplete pairwise comparison matrices.\nThe extension is based on choosing the missing elements such that the maximal\neigenvalue of the incomplete matrix is minimised. Consequently, the\nwell-established values of the random index cannot be adopted: the\ninconsistency of random matrices is found to be the function of matrix size and\nthe number of missing elements, with a nearly linear dependence in the case of\nthe latter variable. Our results can be directly built into decision-making\nsoftware and used by practitioners as a statistical criterion for accepting or\nrejecting an incomplete pairwise comparison matrix.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:39:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:09:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["\u00c1goston", "Kolos Csaba", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2102.10630", "submitter": "Antonio Di Crescenzo", "authors": "Antonio Di Crescenzo, Suchandan Kayal, Alessandra Meoli", "title": "Fractional generalized cumulative entropy and its dynamic version", "comments": "25 pages, 8 figures, accepted for publication on Communications in\n  Nonlinear Science and Numerical Simulation", "journal-ref": null, "doi": "10.1016/j.cnsns.2021.105899", "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the theory of information measures based on the cumulative\ndistribution function, we propose the fractional generalized cumulative\nentropy, and its dynamic version. These entropies are particularly suitable to\ndeal with distributions satisfying the proportional reversed hazard model. We\nstudy the connection with fractional integrals, and some bounds and comparisons\nbased on stochastic orderings, that allow to show that the proposed measure is\nactually a variability measure. The investigation also involves various notions\nof reliability theory, since the considered dynamic measure is a suitable\nextension of the mean inactivity time. We also introduce the empirical\ngeneralized fractional cumulative entropy as a non-parametric estimator of the\nnew measure. It is shown that the empirical measure converges to the proposed\nnotion almost surely. Then, we address the stability of the empirical measure\nand provide an example of application to real data. Finally, a central limit\ntheorem is established under the exponential distribution.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 15:59:43 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 20:42:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Di Crescenzo", "Antonio", ""], ["Kayal", "Suchandan", ""], ["Meoli", "Alessandra", ""]]}, {"id": "2102.10669", "submitter": "Xueheng Shi", "authors": "Colin Gallagher, Rebecca Killick, Robert Lund, Xueheng Shi", "title": "Autocovariance Estimation in the Presence of Changepoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article studies estimation of a stationary autocovariance structure in\nthe presence of an unknown number of mean shifts. Here, a Yule-Walker moment\nestimator for the autoregressive parameters in a dependent time series\ncontaminated by mean shift changepoints is proposed and studied. The estimator\nis based on first order differences of the series and is proven consistent and\nasymptotically normal when the number of changepoints $m$ and the series length\n$N$ satisfies $m/N \\rightarrow 0$ as $N \\rightarrow \\infty$\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 19:52:36 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 22:14:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gallagher", "Colin", ""], ["Killick", "Rebecca", ""], ["Lund", "Robert", ""], ["Shi", "Xueheng", ""]]}, {"id": "2102.10724", "submitter": "Yu Man Tam", "authors": "Raymond C. W. Leung and Yu-Man Tam", "title": "A Small-Uniform Statistic for the Inference of Functional Linear\n  Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a \"small-uniform\" statistic for the inference of the functional\nPCA estimator in a functional linear regression model. The literature has shown\ntwo extreme behaviors: on the one hand, the FPCA estimator does not converge in\ndistribution in its norm topology; but on the other hand, the FPCA estimator\ndoes have a pointwise asymptotic normal distribution. Our statistic takes a\nmiddle ground between these two extremes: after a suitable rate normalization,\nour small-uniform statistic is constructed as the maximizer of a fractional\nprogramming problem of the FPCA estimator over a finite-dimensional subspace,\nand whose dimensions will grow with sample size. We show the rate for which our\nscalar statistic converges in probability to the supremum of a Gaussian\nprocess. The small-uniform statistic has applications in hypothesis testing.\nSimulations show our statistic has comparable to slightly better power\nproperties for hypothesis testing than the two statistics of Cardot, Ferraty,\nMas and Sarda (2003).\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 00:44:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Leung", "Raymond C. W.", ""], ["Tam", "Yu-Man", ""]]}, {"id": "2102.11076", "submitter": "Rahul Singh", "authors": "Rahul Singh", "title": "Debiased Kernel Methods", "comments": "32 pages. arXiv admin note: text overlap with arXiv:2010.04855", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I propose a practical procedure based on bias correction and sample splitting\nto calculate confidence intervals for functionals of generic kernel methods,\ni.e. nonparametric estimators learned in a reproducing kernel Hilbert space\n(RKHS). For example, an analyst may desire confidence intervals for functionals\nof kernel ridge regression. I propose a bias correction that mirrors kernel\nridge regression. The framework encompasses (i) evaluations over discrete\ndomains, (ii) derivatives over continuous domains, (iii) treatment effects of\ndiscrete treatments, and (iv) incremental treatment effects of continuous\ntreatments. For the target quantity, whether it is (i)-(iv), I prove root-n\nconsistency, Gaussian approximation, and semiparametric efficiency by finite\nsample arguments. I show that the classic assumptions of RKHS learning theory\nalso imply inference.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 14:46:23 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 22:22:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Singh", "Rahul", ""]]}, {"id": "2102.11120", "submitter": "Takeyuki Sasai", "authors": "Takeyuki Sasai and Hironori Fujisawa", "title": "Adversarial robust weighted Huber regression", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to estimate the coefficients of linear regression\nwhen outputs and inputs are contaminated by malicious outliers. Our method\nconsists of two-step: (i) Make appropriate weights\n$\\left\\{\\hat{w}_i\\right\\}_{i=1}^n$ such that the weighted sample mean of\nregression covariates robustly estimates the population mean of the regression\ncovariate, (ii) Process Huber regression using\n$\\left\\{\\hat{w}_i\\right\\}_{i=1}^n$. When (a-1) the regression covariate is a\nsequence with i.i.d. random vectors drawn from sub-Gaussian distribution\nsatisfying $L_4$-$L_2$ norm equivalence with unknown mean and known identity\ncovariance and (a-2) the absolute moment of the random noise is finite, our\nmethod attains a convergence rate, which is information theoretically optimal\nup to constant factor about noise term. When (b-1) the regression covariate is\na sequence with i.i.d. random vectors drawn from heavy tailed distribution\nsatisfying $L_4$-$L_2$ norm equivalence with unknown mean and (b-2) the\nabsolute moment of the random noise is finite, our method attains a convergence\nrate, which is information theoretically optimal up to constant factor.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:50:34 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 15:38:56 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sasai", "Takeyuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "2102.11224", "submitter": "Catherine Matias", "authors": "Suzana de Siqueira Santos and Andr\\'e Fujita and Catherine Matias", "title": "Spectral density of random graphs: convergence properties and\n  application in model fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Random graph models are used to describe the complex structure of real-world\nnetworks in diverse fields of knowledge. Studying their behavior and fitting\nproperties are still critical challenges, that in general, require model\nspecific techniques. An important line of research is to develop generic\nmethods able to fit and select the best model among a collection. Approaches\nbased on spectral density (i.e., distribution of the graph adjacency matrix\neigenvalues) are appealing for that purpose: they apply to different random\ngraph models. Also, they can benefit from the theoretical background of random\nmatrix theory. This work investigates the convergence properties of model\nfitting procedures based on the graph spectral density and the corresponding\ncumulative distribution function. We also review results on the convergence of\nthe spectral density for the most widely used random graph models. Moreover, we\nexplore through simulations the limits of these graph spectral density\nconvergence results, particularly in the case of the block model, where only\npartial results have been established.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:55:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Santos", "Suzana de Siqueira", ""], ["Fujita", "Andr\u00e9", ""], ["Matias", "Catherine", ""]]}, {"id": "2102.11229", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Moulinath Banerjee and Ya'acov Ritov", "title": "Regression discontinuity design: estimating the treatment effect with\n  standard parametric rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regression discontinuity design models are widely used for the assessment of\ntreatment effects in psychology, econometrics and biomedicine, specifically in\nsituations where treatment is assigned to an individual based on their\ncharacteristics (e.g. scholarship is allocated based on merit) instead of being\nallocated randomly, as is the case, for example, in randomized clinical trials.\nPopular methods that have been largely employed till date for estimation of\nsuch treatment effects suffer from slow rates of convergence (i.e. slower than\n$\\sqrt{n}$). In this paper, we present a new model and method that allows\nestimation of the treatment effect at $\\sqrt{n}$ rate in the presence of fairly\ngeneral forms of confoundedness. Moreover, we show that our estimator is also\nsemi-parametrically efficient in certain situations. We analyze two real\ndatasets via our method and compare our results with those obtained by using\nprevious approaches. We conclude this paper with a discussion on some possible\nextensions of our method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:59:48 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 18:38:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2102.11253", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Xu Chen, Eugene Katsevich, Jelle Goeman, Aaditya Ramdas", "title": "Large-scale simultaneous inference under dependence", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous, post-hoc inference is desirable in large-scale hypotheses\ntesting as it allows for exploration of data while deciding on criteria for\nproclaiming discoveries. It was recently proved that all admissible post-hoc\ninference methods for the number of true discoveries must be based on closed\ntesting. In this paper we investigate tractable and efficient closed testing\nwith local tests of different properties, such as monotonicty, symmetry and\nseparability, meaning that the test thresholds a monotonic or symmetric\nfunction or a function of sums of test scores for the individual hypotheses.\nThis class includes well-known global null tests by Fisher, Stouffer and\nRuschendorf, as well as newly proposed ones based on harmonic means and Cauchy\ncombinations. Under monotonicity, we propose a new linear time statistic\n(\"coma\") that quantifies the cost of multiplicity adjustments. If the tests are\nalso symmetric and separable, we develop several fast (mostly linear-time)\nalgorithms for post-hoc inference, making closed testing tractable. Paired with\nrecent advances in global null tests based on generalized means, our work\nimmediately instantiates a series of simultaneous inference methods that can\nhandle many complex dependence structures and signal compositions. We provide\nguidance on choosing from these methods via theoretical investigation of the\nconservativeness and sensitivity for different local tests, as well as\nsimulations that find analogous behavior for local tests and full closed\ntesting. One result of independent interest is the following: if\n$P_1,\\dots,P_d$ are $p$-values from a multivariate Gaussian with arbitrary\ncovariance, then their arithmetic average P satisfies $Pr(P \\leq t) \\leq t$ for\n$t \\leq \\frac{1}{2d}$.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:36:37 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Tian", "Jinjin", ""], ["Chen", "Xu", ""], ["Katsevich", "Eugene", ""], ["Goeman", "Jelle", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2102.11575", "submitter": "Juan Kuntz", "authors": "Juan Kuntz, Francesca R. Crucinio, Adam M. Johansen", "title": "Product-form estimators: exploiting independence to scale up Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of Monte Carlo estimators for product-form target\ndistributions that aim to overcome the rapid growth of variance with dimension\noften observed for standard estimators. We identify them with a class of\ngeneralized U-Statistics, and thus establish their unbiasedness, consistency,\nand asymptotic normality. Moreover, we show that they achieve lower variances\nthan their conventional counterparts given the same number of samples drawn\nfrom the target, investigate the gap in variance via several examples, and\nidentify the situations in which the difference is most, and least, pronounced.\nWe further study the estimators' computational cost and delineate the settings\nin which they are most efficient. We illustrate their utility beyond the\nsetting of product-form distributions by detailing two simple extensions (one\nto targets that are mixtures of product-form distributions and another to\ntargets that are absolutely continuous with respect to product-form\ndistributions) and conclude by discussing further possible uses.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:27:30 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 17:54:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kuntz", "Juan", ""], ["Crucinio", "Francesca R.", ""], ["Johansen", "Adam M.", ""]]}, {"id": "2102.11681", "submitter": "Vicky Fasen-Hartmann", "authors": "Vicky Fasen-Hartmann and Markus Scholz", "title": "Factorization and discrete-time representation of multivariate CARMA\n  processes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we show that stationary and non-stationary multivariate\ncontinuous-time ARMA (MCARMA) processes have the representation as a sum of\nmultivariate complex-valued Ornstein-Uhlenbeck processes under some mild\nassumptions. The proof benefits from properties of rational matrix polynomials.\nA conclusion is an alternative description of the autocovariance function of a\nstationary MCARMA process. Moreover, that representation is used to show that\nthe discrete-time sampled MCARMA(p,q) process is a weak VARMA(p,p-1) process if\nsecond moments exist. That result complements the weak VARMA(p,p-1)\nrepresentation derived in Chambers and Thornton (2012). In particular, it\nrelates the right solvents of the autoregressive polynomial of the MCARMA\nprocess to the right solvents of the autoregressive polynomial of the VARMA\nprocess; in the one-dimensional case the right solvents are the zeros of the\nautoregressive polynomial. Finally, a factorization of the sample\nautocovariance function of the noise sequence is presented which is useful for\nstatistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 13:10:17 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Fasen-Hartmann", "Vicky", ""], ["Scholz", "Markus", ""]]}, {"id": "2102.11800", "submitter": "Merle Behr", "authors": "Merle Behr, Yu Wang, Xiao Li, and Bin Yu", "title": "Provable Boolean Interaction Recovery from Tree Ensemble obtained via\n  Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests (RF) are at the cutting edge of supervised machine learning in\nterms of prediction performance, especially in genomics. Iterative Random\nForests (iRF) use a tree ensemble from iteratively modified RF to obtain\npredictive and stable non-linear high-order Boolean interactions of features.\nThey have shown great promise for high-order biological interaction discovery\nthat is central to advancing functional genomics and precision medicine.\nHowever, theoretical studies into how tree-based methods discover high-order\nfeature interactions are missing. In this paper, to enable such theoretical\nstudies, we first introduce a novel discontinuous nonlinear regression model,\ncalled Locally Spiky Sparse (LSS) model, which is inspired by the thresholding\nbehavior in many biological processes. Specifically, LSS model assumes that the\nregression function is a linear combination of piece-wise constant Boolean\ninteraction terms. We define a quantity called depth-weighted prevalence (DWP)\nfor a set of signed features S and a given RF tree ensemble. We prove that,\nwith high probability under the LSS model, DWP of S attains a universal upper\nbound that does not involve any model coefficients, if and only if S\ncorresponds to a union of Boolean interactions in the LSS model. As a\nconsequence, we show that RF yields consistent interaction discovery under the\nLSS model. Simulation results show that DWP can recover the interactions under\nthe LSS model even when some assumptions such as the uniformity assumption are\nviolated.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 17:10:21 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 16:37:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Behr", "Merle", ""], ["Wang", "Yu", ""], ["Li", "Xiao", ""], ["Yu", "Bin", ""]]}, {"id": "2102.11976", "submitter": "Dana Yang", "authors": "Jiaming Xu, Kuang Xu and Dana Yang", "title": "Learner-Private Online Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online convex optimization is a framework where a learner sequentially\nqueries an external data source in order to arrive at the optimal solution of a\nconvex function. The paradigm has gained significant popularity recently thanks\nto its scalability in large-scale optimization and machine learning. The\nrepeated interactions, however, expose the learner to privacy risks from\neavesdropping adversary that observe the submitted queries. In this paper, we\nstudy how to optimally obfuscate the learner's queries in first-order online\nconvex optimization, so that their learned optimal value is provably difficult\nto estimate for the eavesdropping adversary. We consider two formulations of\nlearner privacy: a Bayesian formulation in which the convex function is drawn\nrandomly, and a minimax formulation in which the function is fixed and the\nadversary's probability of error is measured with respect to a minimax\ncriterion. We show that, if the learner wants to ensure the probability of\naccurate prediction by the adversary be kept below $1/L$, then the overhead in\nquery complexity is additive in $L$ in the minimax formulation, but\nmultiplicative in $L$ in the Bayesian formulation. Compared to existing\nlearner-private sequential learning models with binary feedback, our results\napply to the significantly richer family of general convex functions with\nfull-gradient feedback. Our proofs are largely enabled by tools from the theory\nof Dirichlet processes, as well as more sophisticated lines of analysis aimed\nat measuring the amount of information leakage under a full-gradient oracle.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 23:00:44 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Xu", "Jiaming", ""], ["Xu", "Kuang", ""], ["Yang", "Dana", ""]]}, {"id": "2102.12034", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Sivaraman Balakrishnan, Larry Wasserman", "title": "Semiparametric counterfactual density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal effects are often characterized with averages, which can give an\nincomplete picture of the underlying counterfactual distributions. Here we\nconsider estimating the entire counterfactual density and generic functionals\nthereof. We focus on two kinds of target parameters. The first is a density\napproximation, defined by a projection onto a finite-dimensional model using a\ngeneralized distance metric, which includes f-divergences as well as $L_p$\nnorms. The second is the distance between counterfactual densities, which can\nbe used as a more nuanced effect measure than the mean difference, and as a\ntool for model selection. We study nonparametric efficiency bounds for these\ntargets, giving results for smooth but otherwise generic models and distances.\nImportantly, we show how these bounds connect to means of particular\nnon-trivial functions of counterfactuals, linking the problems of density and\nmean estimation. We go on to propose doubly robust-style estimators for the\ndensity approximations and distances, and study their rates of convergence,\nshowing they can be optimally efficient in large nonparametric models. We also\ngive analogous methods for model selection and aggregation, when many models\nmay be available and of interest. Our results all hold for generic models and\ndistances, but throughout we highlight what happens for particular choices,\nsuch as $L_2$ projections on linear models, and KL projections on exponential\nfamilies. Finally we illustrate by estimating the density of CD4 count among\npatients with HIV, had all been treated with combination therapy versus\nzidovudine alone, as well as a density effect. Our results suggest combination\ntherapy may have increased CD4 count most for high-risk patients. Our methods\nare implemented in the freely available R package npcausal on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 02:32:32 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2102.12066", "submitter": "Gil Kur", "authors": "Gil Kur, Alexander Rakhlin", "title": "On the Minimal Error of Empirical Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimal error of the Empirical Risk Minimization (ERM) procedure\nin the task of regression, both in the random and the fixed design settings.\nOur sharp lower bounds shed light on the possibility (or impossibility) of\nadapting to simplicity of the model generating the data. In the fixed design\nsetting, we show that the error is governed by the global complexity of the\nentire class. In contrast, in random design, ERM may only adapt to simpler\nmodels if the local neighborhoods around the regression function are nearly as\ncomplex as the class itself, a somewhat counter-intuitive conclusion. We\nprovide sharp lower bounds for performance of ERM for both Donsker and\nnon-Donsker classes. We also discuss our results through the lens of recent\nstudies on interpolation in overparameterized models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:47:55 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kur", "Gil", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "2102.12079", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama, William E. Strawderman", "title": "On admissible estimation of a mean vector when the scale is unknown", "comments": "32 pages, 1figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider admissibility of generalized Bayes estimators of the mean of a\nmultivariate normal distribution when the scale is unknown under quadratic\nloss. The priors considered put the improper invariant prior on the scale while\nthe prior on the mean has a hierarchical normal structure conditional on the\nscale. This conditional hierarchical prior is essentially that of Maruyama and\nStrawderman (2021, Biometrika) (MS21) which is indexed by a hyperparameter $a$.\nIn that paper $a$ is chosen so this conditional prior is proper which\ncorresponds to $a>-1$. This paper extends MS21 by considering improper\nconditional priors with $a$ in the closed interval $[-2, -1]$, and establishing\nadmissibility for such $a$. The authors, in Maruyama and Strawderman (2017,\nJMVA), have earlier shown that such conditional priors with $a < -2$ lead to\ninadmissible estimators. This paper therefore completes the determination of\nadmissibility/inadmissibility for this class of priors. It establishes the the\nboundary as $a = -2$, with admissibility holding for $a\\geq -2$ and\ninadmissibility for $a < -2$. This boundary corresponds exactly to that in the\nknown scale case for these conditional priors, and which follows from Brown\n(1971, AOMS). As a notable benefit of this enlargement of the class of\nadmissible generalized Bayes estimators, we give admissible and minimax\nestimators in all dimensions greater than $2$ as opposed to MS21 which required\nthe dimension to be greater than $4$. In one particularly interesting special\ncase, we establish that the joint Stein prior for the unknown scale case leads\nto a minimax admissible estimator in all dimensions greater than $2$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 05:57:03 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "2102.12225", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Valid Instrumental Variables Selection Methods using Auxiliary Variable\n  and Constructing Efficient Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, we are usually interested in estimating causal\neffects between treatments and outcomes. When some covariates are not observed,\nan unbiased estimator usually cannot be obtained. In this paper, we focus on\ninstrumental variable (IV) methods. By using IVs, an unbiased estimator for\ncausal effects can be estimated even if there exists some unmeasured\ncovariates. Constructing a linear combination of IVs solves weak IV problems,\nhowever, there are risks estimating biased causal effects by including some\ninvalid IVs. In this paper, we use Negative Control Outcomes as auxiliary\nvariables to select valid IVs. By using NCOs, there are no necessity to specify\nnot only the set of valid IVs but also invalid one in advance: this point is\ndifferent from previous methods. We prove that the estimated causal effects has\nthe same asymptotic variance as the estimator using Generalized Method of\nMoments that has the semiparametric efficiency. Also, we confirm properties of\nour method and previous methods through simulations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 11:34:25 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2102.12282", "submitter": "Elena Castilla", "authors": "Elena Castilla, Maria Jaenada and Leandro Pardo", "title": "Estimation and testing on independent not identically distributed\n  observations based on R\\'enyi's pseudodistances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real life we often deal with independent but not identically distributed\nobservations (i.n.i.d.o), for which the most well-known statistical model is\nthe multiple linear regression model (MLRM) without random covariates. While\nthe classical methods are based on the maximum likelihood estimator (MLE), it\nis well known its lack of robustness to small deviations from the assumed\nconditions. In this paper, and based on the R\\'enyi's pseudodistance (RP), we\nintroduce a new family of estimators in case our information about the unknown\nparameter is given for i.n.i.d.o.. This family of estimators, let say minimum\nRP estimators (as they are obtained by minimizing the RP between the assumed\ndistribution and the empirical distribution of the data), contains the MLE as a\nparticular case and can be applied, among others, to the MLRM without random\ncovariates. Based on these estimators, we introduce Wald-type tests for testing\nsimple and composite null hypotheses, as an extension of the classical\nMLE-based Wald test. Influence functions for the estimators and Wald-type tests\nare also obtained and analysed. Finally, a simulation study is developed in\norder to asses the performance of the proposed methods and some real-life data\nare analysed for illustrative purpose.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 13:44:05 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Castilla", "Elena", ""], ["Jaenada", "Maria", ""], ["Pardo", "Leandro", ""]]}, {"id": "2102.12352", "submitter": "Andr\\'e Martin Timpanaro", "authors": "Andr\\'e M. Timpanaro", "title": "A convex analysis approach to tight expectation inequalities", "comments": "26 pages, 13 figures (to be submitted to 'Probability Theory and\n  Related Fields')", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the question of how knowledge about expectations\n$\\mathbb{E}(f_i(X))$ of a random vector $X$ translate into inequalities for\n$\\mathbb{E}(g(X))$ for given functions $f_i$, $g$ and a random vector $X$ whose\nsupport is contained in some set $S\\subseteq \\mathbb{R}^n$. We show that there\nis a connection between the problem of obtaining tight expectation inequalities\nin this context and properties of convex hulls, allowing us to rewrite it as an\noptimization problem. The results of these optimization problems not only\narrive at sharp bounds for $\\mathbb{E}(g(X))$ but in some cases also yield\ndiscrete probability measures where equality holds.\n  We develop an analytical approach that is particularly suited for studying\nthe Jensen gap problem when the known information are the average and variance,\nas well as a numerical approach for the general case, that reduces the problem\nto a convex optimization; which in a sense extends known results about the\nmoment problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:37:52 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 21:27:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Timpanaro", "Andr\u00e9 M.", ""]]}, {"id": "2102.12422", "submitter": "Ilias Zadik", "authors": "Jonathan Niles-Weed, Ilias Zadik", "title": "It was \"all\" for \"nothing\": sharp phase transitions for noiseless\n  discrete channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a phase transition known as the \"all-or-nothing\" phenomenon for\nnoiseless discrete channels. This class of models includes the Bernoulli group\ntesting model and the planted Gaussian perceptron model. Previously, the\nexistence of the all-or-nothing phenomenon for such models was only known in a\nlimited range of parameters. Our work extends the results to all signals with\narbitrary sublinear sparsity.\n  Over the past several years, the all-or-nothing phenomenon has been\nestablished in various models as an outcome of two seemingly disjoint results:\none positive result establishing the \"all\" half of all-or-nothing, and one\nimpossibility result establishing the \"nothing\" half. Our main technique in the\npresent work is to show that for noiseless discrete channels, the \"all\" half\nimplies the \"nothing\" half, that is a proof of \"all\" can be turned into a proof\nof \"nothing.\" Since the \"all\" half can often be proven by straightforward means\n-- for instance, by the first-moment method -- our equivalence gives a powerful\nand general approach towards establishing the existence of this phenomenon in\nother contexts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 17:31:37 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Niles-Weed", "Jonathan", ""], ["Zadik", "Ilias", ""]]}, {"id": "2102.12451", "submitter": "Camilla Cal\\`i", "authors": "Camilla Cal\\`i, Maria Longobardi, Claudio Macci, Barbara Pacchiarotti", "title": "Asymptotic results for linear combinations of spacings generated by\n  i.i.d. exponential random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We prove large (and moderate) deviations for a class of linear combinations\nof spacings generated by i.i.d. exponentially distributed random variables. We\nallow a wide class of coefficients which can be expressed in terms of\ncontinuous functions defined on [0, 1] which satisfy some suitable conditions.\nIn this way we generalize some recent results by Giuliano et al. (2015) which\nconcern the empirical cumulative entropies defined in Di Crescenzo and\nLongobardi (2009a).\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:33:55 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 18:10:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Cal\u00ec", "Camilla", ""], ["Longobardi", "Maria", ""], ["Macci", "Claudio", ""], ["Pacchiarotti", "Barbara", ""]]}, {"id": "2102.12460", "submitter": "Nakahiro Yoshida", "authors": "Nakahiro Yoshida", "title": "Simplified quasi-likelihood analysis for a locally asymptotically\n  quadratic random field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic decision theory by Le Cam and Hajek has been given a lucid\nperspective by the Ibragimov-Hasminskii theory on convergence of the likelihood\nrandom field. Their scheme has been applied to stochastic processes by\nKutoyants, and today this plot is called the IHK program. This scheme ensures\nthat asymptotic properties of an estimator follow directly from the convergence\nof the random field if a large deviation estimate exists. The quasi-likelihood\nanalysis (QLA) proved a polynomial type large deviation (PLD) inequality to go\nthrough a bottleneck of the program. A conclusion of the QLA is that if the\nquasi-likelihood random field is asymptotically quadratic and if a key index\nreflecting identifiability the random field has is non-degenerate, then the PLD\ninequality is always valid, and as a result, the IHK program can run. Many\nstudies already took advantage of the QLA theory. However, not a few of them\nare using it in an inefficient way yet. The aim of this paper is to provide a\nreformed and simplified version of the QLA and to improve accessibility to the\ntheory. As an example of the effects of the theory based on the PLD, the user\ncan obtain asymptotic properties of the quasi-Bayesian estimator by only\nverifying non-degeneracy of the key index.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:40:17 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Yoshida", "Nakahiro", ""]]}, {"id": "2102.12512", "submitter": "Robert Salzmann", "authors": "Robert Salzmann, Nilanjana Datta, Gilad Gour, Xin Wang, Mark M. Wilde", "title": "Symmetric distinguishability as a quantum resource", "comments": "59 pages main text + 25 pages of appendices, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a resource theory of symmetric distinguishability, the fundamental\nobjects of which are elementary quantum information sources, i.e., sources that\nemit one of two possible quantum states with given prior probabilities. Such a\nsource can be represented by a classical-quantum state of a composite system\n$XA$, corresponding to an ensemble of two quantum states, with $X$ being\nclassical and $A$ being quantum. We study the resource theory for two different\nclasses of free operations: $(i)$ ${\\rm{CPTP}}_A$, which consists of quantum\nchannels acting only on $A$, and $(ii)$ conditional doubly stochastic (CDS)\nmaps acting on $XA$. We introduce the notion of symmetric distinguishability of\nan elementary source and prove that it is a monotone under both these classes\nof free operations. We study the tasks of distillation and dilution of\nsymmetric distinguishability, both in the one-shot and asymptotic regimes. We\nprove that in the asymptotic regime, the optimal rate of converting one\nelementary source to another is equal to the ratio of their quantum Chernoff\ndivergences, under both these classes of free operations. This imparts a new\noperational interpretation to the quantum Chernoff divergence. We also obtain\ninteresting operational interpretations of the Thompson metric, in the context\nof the dilution of symmetric distinguishability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:05:02 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Salzmann", "Robert", ""], ["Datta", "Nilanjana", ""], ["Gour", "Gilad", ""], ["Wang", "Xin", ""], ["Wilde", "Mark M.", ""]]}, {"id": "2102.12528", "submitter": "Aymeric Dieuleveut", "authors": "Constantin Philippenko and Aymeric Dieuleveut", "title": "Preserved central model for faster bidirectional compression in\n  distributed settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new approach to tackle communication constraints in a\ndistributed learning problem with a central server. We propose and analyze a\nnew algorithm that performs bidirectional compression and achieves the same\nconvergence rate as algorithms using only uplink (from the local workers to the\ncentral server) compression. To obtain this improvement, we design MCM, an\nalgorithm such that the downlink compression only impacts local models, while\nthe global model is preserved. As a result, and contrary to previous works, the\ngradients on local servers are computed on perturbed models. Consequently,\nconvergence proofs are more challenging and require a precise control of this\nperturbation. To ensure it, MCM additionally combines model compression with a\nmemory mechanism. This analysis opens new doors, e.g. incorporating worker\ndependent randomized-models and partial participation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:48:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Philippenko", "Constantin", ""], ["Dieuleveut", "Aymeric", ""]]}, {"id": "2102.12676", "submitter": "Jiangtao Duan", "authors": "Jiangtao Duan, Wei Gao, Yanyuan Ma and Hon Keung Tony Ng", "title": "Efficient computational algorithms for approximate optimal designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose two simple yet efficient computational algorithms\nto obtain approximate optimal designs for multi-dimensional linear regression\non a large variety of design spaces. We focus on the two commonly used optimal\ncriteria, $D$- and $A$-optimal criteria. For $D$-optimality, we provide an\nalternative proof for the monotonic convergence for $D$-optimal criterion and\npropose an efficient computational algorithm to obtain the approximate\n$D$-optimal design. We further show that the proposed algorithm converges to\nthe $D$-optimal design, and then prove that the approximate $D$-optimal design\nconverges to the continuous $D$-optimal design under certain conditions. For\n$A$-optimality, we provide an efficient algorithm to obtain approximate\n$A$-optimal design and conjecture the monotonicity of the proposed algorithm.\nNumerical comparisons suggest that the proposed algorithms perform well and\nthey are comparable or superior to some existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 04:21:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Duan", "Jiangtao", ""], ["Gao", "Wei", ""], ["Ma", "Yanyuan", ""], ["Ng", "Hon Keung Tony", ""]]}, {"id": "2102.12752", "submitter": "Minseok Shin", "authors": "Minseok Shin, Donggyu Kim, Jianqing Fan", "title": "Adaptive Robust Large Volatility Matrix Estimation Based on\n  High-Frequency Financial Data", "comments": "51 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several novel statistical methods have been developed to estimate large\nintegrated volatility matrices based on high-frequency financial data. To\ninvestigate their asymptotic behaviors, they require a sub-Gaussian or finite\nhigh-order moment assumption for observed log-returns, which cannot account for\nthe heavy tail phenomenon of stock returns. Recently, a robust estimator was\ndeveloped to handle heavy-tailed distributions with some bounded fourth-moment\nassumption. However, we often observe that log-returns have heavier tail\ndistribution than the finite fourth-moment and that the degrees of heaviness of\ntails are heterogeneous over the asset and time period. In this paper, to deal\nwith the heterogeneous heavy-tailed distributions, we develop an adaptive\nrobust integrated volatility estimator that employs pre-averaging and\ntruncation schemes based on jump-diffusion processes. We call this an adaptive\nrobust pre-averaging realized volatility (ARP) estimator. We show that the ARP\nestimator has a sub-Weibull tail concentration with only finite 2$\\alpha$-th\nmoments for any $\\alpha>1$. In addition, we establish matching upper and lower\nbounds to show that the ARP estimation procedure is optimal. To estimate large\nintegrated volatility matrices using the approximate factor model, the ARP\nestimator is further regularized using the principal orthogonal complement\nthresholding (POET) method. The numerical study is conducted to check the\nfinite sample performance of the ARP estimator.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 09:42:53 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shin", "Minseok", ""], ["Kim", "Donggyu", ""], ["Fan", "Jianqing", ""]]}, {"id": "2102.12919", "submitter": "Nikita Zhivotovskiy", "authors": "Jaouad Mourtada and Tomas Va\\v{s}kevi\\v{c}ius and Nikita Zhivotovskiy", "title": "Distribution-Free Robust Linear Regression", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study random design linear regression with no assumptions on the\ndistribution of the covariates and with a heavy-tailed response variable. When\nlearning without assumptions on the covariates, we establish boundedness of the\nconditional second moment of the response variable as a necessary and\nsufficient condition for achieving deviation-optimal excess risk rate of\nconvergence. In particular, combining the ideas of truncated least squares,\nmedian-of-means procedures and aggregation theory, we construct a non-linear\nestimator achieving excess risk of order $d/n$ with the optimal sub-exponential\ntail. While the existing approaches to learning linear classes under\nheavy-tailed distributions focus on proper estimators, we highlight that the\nimproperness of our estimator is necessary for attaining non-trivial guarantees\nin the distribution-free setting considered in this work. Finally, as a\nbyproduct of our analysis, we prove an optimal version of the classical bound\nfor the truncated least squares estimator due to Gy\\\"{o}rfi, Kohler, Krzyzak,\nand Walk.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:10:41 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Va\u0161kevi\u010dius", "Tomas", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2102.12938", "submitter": "Nilabja Guha", "authors": "Nilabja Guha and Jyotishka Datta", "title": "On Posterior consistency of Bayesian Changepoint models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there have been a lot of recent developments in the context of Bayesian\nmodel selection and variable selection for high dimensional linear models,\nthere is not much work in the presence of change point in literature, unlike\nthe frequentist counterpart. We consider a hierarchical Bayesian linear model\nwhere the active set of covariates that affects the observations through a mean\nmodel can vary between different time segments. Such structure may arise in\nsocial sciences/ economic sciences, such as sudden change of house price based\non external economic factor, crime rate changes based on social and\nbuilt-environment factors, and others. Using an appropriate adaptive prior, we\noutline the development of a hierarchical Bayesian methodology that can select\nthe true change point as well as the true covariates, with high probability. We\nprovide the first detailed theoretical analysis for posterior consistency with\nor without covariates, under suitable conditions. Gibbs sampling techniques\nprovide an efficient computational strategy. We also consider small sample\nsimulation study as well as application to crime forecasting applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 15:34:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Guha", "Nilabja", ""], ["Datta", "Jyotishka", ""]]}, {"id": "2102.12952", "submitter": "L\\'aszl\\'o Gy\\\"orfi", "authors": "Luc Devroye, L\\'aszl\\'o Gy\\\"orfi", "title": "On the consistency of the Kozachenko-Leonenko entropy estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We revisit the problem of the estimation of the differential entropy $H(f)$\nof a random vector $X$ in $R^d$ with density $f$, assuming that $H(f)$ exists\nand is finite. In this note, we study the consistency of the popular nearest\nneighbor estimate $H_n$ of Kozachenko and Leonenko. Without any smoothness\ncondition we show that the estimate is consistent ($E\\{|H_n - H(f)|\\} \\to 0$ as\n$n \\to \\infty$) if and only if $\\mathbb{E} \\{ \\log ( \\| X \\| + 1 )\\} < \\infty$.\nFurthermore, if $X$ has compact support, then $H_n \\to H(f)$ almost surely.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:02:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Devroye", "Luc", ""], ["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""]]}, {"id": "2102.12956", "submitter": "Nikolas Nuesken", "authors": "Nikolas N\\\"usken, D.R. Michiel Renger", "title": "Stein Variational Gradient Descent: many-particle and long-time\n  asymptotics", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.AP math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient descent (SVGD) refers to a class of methods for\nBayesian inference based on interacting particle systems. In this paper, we\nconsider the originally proposed deterministic dynamics as well as a stochastic\nvariant, each of which represent one of the two main paradigms in Bayesian\ncomputational statistics: variational inference and Markov chain Monte Carlo.\nAs it turns out, these are tightly linked through a correspondence between\ngradient flow structures and large-deviation principles rooted in statistical\nphysics. To expose this relationship, we develop the cotangent space\nconstruction for the Stein geometry, prove its basic properties, and determine\nthe large-deviation functional governing the many-particle limit for the\nempirical measure. Moreover, we identify the Stein-Fisher information (or\nkernelised Stein discrepancy) as its leading order contribution in the\nlong-time and many-particle regime in the sense of $\\Gamma$-convergence,\nshedding some light on the finite-particle properties of SVGD. Finally, we\nestablish a comparison principle between the Stein-Fisher information and\nRKHS-norms that might be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 16:03:04 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["N\u00fcsken", "Nikolas", ""], ["Renger", "D. R. Michiel", ""]]}, {"id": "2102.13083", "submitter": "\\'Eric Marchand", "authors": "Lahoucine Hobbad, \\'Eric Marchand and Idir Ouassou", "title": "On shrinkage estimation of a spherically symmetric distribution for\n  balanced loss functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating the mean vector $\\theta$ of a\n$d$-dimensional spherically symmetric distributed $X$ based on balanced loss\nfunctions of the forms: {\\bf (i)} $\\omega \\rho(\\|\\de-\\de_{0}\\|^{2})\n+(1-\\omega)\\rho(\\|\\de - \\theta\\|^{2})$ and {\\bf (ii)} $\\ell\\left(\\omega \\|\\de -\n\\de_{0}\\|^{2} +(1-\\omega)\\|\\de - \\theta\\|^{2}\\right)$, where $\\delta_0$ is a\ntarget estimator, and where $\\rho$ and $\\ell$ are increasing and concave\nfunctions. For $d\\geq 4$ and the target estimator $\\delta_0(X)=X$, we provide\nBaranchik-type estimators that dominate $\\delta_0(X)=X$ and are minimax. The\nfindings represent extensions of those of Marchand \\& Strawderman\n(\\cite{ms2020}) in two directions: {\\bf (a)} from scale mixture of normals to\nthe spherical class of distributions with Lebesgue densities and {\\bf (b)} from\ncompletely monotone to concave $\\rho'$ and $\\ell'$.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:53:30 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Hobbad", "Lahoucine", ""], ["Marchand", "\u00c9ric", ""], ["Ouassou", "Idir", ""]]}, {"id": "2102.13135", "submitter": "Nafiseh Ghoroghchian Ms.", "authors": "Nafiseh Ghoroghchian, Gautam Dasarathy, and Stark C. Draper", "title": "Graph Community Detection from Coarse Measurements: Recovery Conditions\n  for the Coarsened Weighted Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of community recovery from coarse measurements of a\ngraph. In contrast to the problem of community recovery of a fully observed\ngraph, one often encounters situations when measurements of a graph are made at\nlow-resolution, each measurement integrating across multiple graph nodes. Such\nlow-resolution measurements effectively induce a coarse graph with its own\ncommunities. Our objective is to develop conditions on the graph structure, the\nquantity, and properties of measurements, under which we can recover the\ncommunity organization in this coarse graph. In this paper, we build on the\nstochastic block model by mathematically formalizing the coarsening process,\nand characterizing its impact on the community members and connections. Through\nthis novel setup and modeling, we characterize an error bound for community\nrecovery. The error bound yields simple and closed-form asymptotic conditions\nto achieve the perfect recovery of the coarse graph communities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 19:24:33 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ghoroghchian", "Nafiseh", ""], ["Dasarathy", "Gautam", ""], ["Draper", "Stark C.", ""]]}, {"id": "2102.13219", "submitter": "Theodor Misiakiewicz Mr.", "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Learning with invariances in random features and kernel models", "comments": "63 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of machine learning tasks entail a high degree of invariance: the\ndata distribution does not change if we act on the data with a certain group of\ntransformations. For instance, labels of images are invariant under\ntranslations of the images. Certain neural network architectures -- for\ninstance, convolutional networks -- are believed to owe their success to the\nfact that they exploit such invariance properties. With the objective of\nquantifying the gain achieved by invariant architectures, we introduce two\nclasses of models: invariant random features and invariant kernel methods. The\nlatter includes, as a special case, the neural tangent kernel for convolutional\nnetworks with global average pooling. We consider uniform covariates\ndistributions on the sphere and hypercube and a general invariant target\nfunction. We characterize the test error of invariant methods in a\nhigh-dimensional regime in which the sample size and number of hidden units\nscale as polynomials in the dimension, for a class of groups that we call\n`degeneracy $\\alpha$', with $\\alpha \\leq 1$. We show that exploiting invariance\nin the architecture saves a $d^\\alpha$ factor ($d$ stands for the dimension) in\nsample size and number of hidden units to achieve the same test error as for\nunstructured architectures.\n  Finally, we show that output symmetrization of an unstructured kernel\nestimator does not give a significant statistical improvement; on the other\nhand, data augmentation with an unstructured kernel estimator is equivalent to\nan invariant kernel estimator and enjoys the same improvement in statistical\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 23:06:21 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2102.13232", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li, and Changbao Wu", "title": "Semiparametric empirical likelihood inference with estimating equations\n  under density ratio models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density ratio model (DRM) provides a flexible and useful platform for\ncombining information from multiple sources. In this paper, we consider\nstatistical inference under two-sample DRMs with additional parameters defined\nthrough and/or additional auxiliary information expressed as estimating\nequations. We examine the asymptotic properties of the maximum empirical\nlikelihood estimators (MELEs) of the unknown parameters in the DRMs and/or\ndefined through estimating equations, and establish the chi-square limiting\ndistributions for the empirical likelihood ratio (ELR) statistics. We show that\nthe asymptotic variance of the MELEs of the unknown parameters does not\ndecrease if one estimating equation is dropped. Similar properties are obtained\nfor inferences on the cumulative distribution function and quantiles of each of\nthe populations involved. We also propose an ELR test for the validity and\nusefulness of the auxiliary information. Simulation studies show that correctly\nspecified estimating equations for the auxiliary information result in more\nefficient estimators and shorter confidence intervals. Two real-data examples\nare used for illustrations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 23:47:25 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2102.13415", "submitter": "Florian Hildebrandt", "authors": "Florian Hildebrandt and Mathias Trabs", "title": "Nonparametric calibration for stochastic reaction-diffusion equations\n  based on discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation for semilinear SPDEs, namely stochastic\nreaction-diffusion equations in one space dimension, is studied. We consider\nobservations of the solution field on a discrete grid in time and space with\ninfill asymptotics in both coordinates. Firstly, based on a precise analysis of\nthe H\\\"older regularity of the solution process and its nonlinear component, we\nshow that the asymptotic properties of diffusivity and volatility estimators\nderived from realized quadratic variations in the linear setup generalize to\nthe semilinear SPDE. In particular, we obtain a rate-optimal joint estimator of\nthe two parameters. Secondly, we derive a nonparametric estimator for the\nreaction function specifying the underlying equation. The estimate is chosen\nfrom a finite-dimensional function space based on a simple least squares\ncriterion. Oracle inequalities with respect to both the empirical and usual\n$L^2$-risk provide conditions for the estimator to achieve the usual\nnonparametric rate of convergence. Adaptivity is provided via model selection.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:44:53 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Hildebrandt", "Florian", ""], ["Trabs", "Mathias", ""]]}, {"id": "2102.13638", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha, EunYi Chung", "title": "Permutation Tests at Nonparametric Rates", "comments": "One file contains main paper (30 pages) and supplement (47 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical two-sample permutation tests for equality of distributions have\nexact size in finite samples, but they fail to control size for testing\nequality of parameters that summarize each distribution. This paper proposes\npermutation tests for equality of parameters that are estimated at root-n or\nslower rates. Our general framework applies to both parametric and\nnonparametric models, with two samples or one sample split into two subsamples.\nOur tests have correct size asymptotically while preserving exact size in\nfinite samples when distributions are equal. They have no loss in\nlocal-asymptotic power compared to tests that use asymptotic critical values.\nWe propose confidence sets with correct coverage in large samples that also\nhave exact coverage in finite samples if distributions are equal up to a\ntransformation. We apply our theory to four commonly-used hypothesis tests of\nnonparametric functions evaluated at a point. Lastly, simulations show good\nfinite sample properties of our tests.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:30:22 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bertanha", "Marinho", ""], ["Chung", "EunYi", ""]]}]