[{"id": "1602.00136", "submitter": "Qian Qin", "authors": "Qian Qin, James P. Hobert", "title": "Trace-class Monte Carlo Markov Chains for Bayesian Multivariate Linear\n  Regression with Non-Gaussian Errors", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\pi$ denote the intractable posterior density that results when the\nlikelihood from a multivariate linear regression model with errors from a scale\nmixture of normals is combined with the standard non-informative prior. There\nis a simple data augmentation algorithm (based on latent data from the mixing\ndensity) that can be used to explore $\\pi$. Let $h(\\cdot)$ and $d$ denote the\nmixing density and the dimension of the regression model, respectively. Hobert\net al. (2016) [arXiv:1506.03113v2] have recently shown that, if $h$ converges\nto 0 at the origin at an appropriate rate, and $\\int_0^\\infty u^{\\frac{d}{2}}\n\\, h(u) \\, du < \\infty$, then the Markov chains underlying the DA algorithm and\nan alternative Haar PX-DA algorithm are both geometrically ergodic. In fact,\nsomething much stronger than geometric ergodicity often holds. Indeed, it is\nshown in this paper that, under simple conditions on $h$, the Markov operators\ndefined by the DA and Haar PX-DA Markov chains are trace-class, i.e., compact\nwith summable eigenvalues. Many of the mixing densities that satisfy Hobert et\nal.'s (2016) conditions also satisfy the new conditions developed in this\npaper. Thus, for this set of mixing densities, the new results provide a\nsubstantial strengthening of Hobert et al.'s (2016) conclusion without any\nadditional assumptions. For example, Hobert et al. (2016) showed that the DA\nand Haar PX-DA Markov chains are geometrically ergodic whenever the mixing\ndensity is generalized inverse Gaussian, log-normal, Fr\\'{e}chet (with shape\nparameter larger than $d/2$), or inverted gamma (with shape parameter larger\nthan $d/2$). The results in this paper show that, in each of these cases, the\nDA and Haar PX-DA Markov operators are, in fact, trace-class.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 16:28:26 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:22:58 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""]]}, {"id": "1602.00175", "submitter": "Leonid Sirota", "authors": "E.Ostrovsky, L.Sirota", "title": "Sharp moment and exponential tail estimates for U-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain in this paper a non-asymptotic non-improvable up to multiplicative\nconstant moment and exponential tail estimates for distribution for\nU-statistics by means of martingale representation.\n  We show also the exactness of obtained estimations in one way or another by\nproviding appropriate examples.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 00:14:36 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Ostrovsky", "E.", ""], ["Sirota", "L.", ""]]}, {"id": "1602.00180", "submitter": "Sonja Petrovic", "authors": "Nicolas Kim, Dane Wilburne, Sonja Petrovi\\'c and Alessandro Rinaldo", "title": "On the Geometry and Extremal Properties of the Edge-Degeneracy Model", "comments": "9 pages, 4 figures. This version differs ever so slightly from the\n  published one; several typos have been fixed and clarifying comments by J.\n  Rauh incorporated in the update", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edge-degeneracy model is an exponential random graph model that uses the\ngraph degeneracy, a measure of the graph's connection density, and number of\nedges in a graph as its sufficient statistics. We show this model is relatively\nwell-behaved by studying the statistical degeneracy of this model through the\ngeometry of the associated polytope.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 01:23:24 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 09:08:41 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Kim", "Nicolas", ""], ["Wilburne", "Dane", ""], ["Petrovi\u0107", "Sonja", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1602.00197", "submitter": "Reyhaneh Hosseini", "authors": "Reyhaneh Hosseini and Mahmoud Zarepour", "title": "A Bayesian nonparametric chi-squared goodness-of-fit test", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian nonparametric inference and Dirichlet process are popular tools\nin statistical methodologies. In this paper, we employ the Dirichlet process in\nhypothesis testing to propose a Bayesian nonparametric chi-squared\ngoodness-of-fit test. In our Bayesian nonparametric approach, we consider the\nDirichlet process as the prior for the distribution of data and carry out the\ntest based on the Kullback-Leibler distance between the updated Dirichlet\nprocess and the hypothesized distribution F0. We prove that this distance\nasymptotically converges to the same chi-squared distribution as the\nchi-squared test does. Similarly, a Bayesian nonparametric chi-squared test of\nindependence for a contingency table is provided. Also, by computing the\nKullback-Leibler distance between the Dirichlet process and the hypothesized\ndistribution, a method to obtain an appropriate concentration parameter for the\nDirichlet process is suggested.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 04:00:35 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 16:24:08 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 21:18:32 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hosseini", "Reyhaneh", ""], ["Zarepour", "Mahmoud", ""]]}, {"id": "1602.00199", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen", "title": "Gaussian approximation for the sup-norm of high-dimensional\n  matrix-variate U-statistics and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Gaussian approximation of high-dimensional and\nnon-degenerate U-statistics of order two under the supremum norm. We propose a\ntwo-step Gaussian approximation procedure that does not impose structural\nassumptions on the data distribution. Specifically, subject to mild moment\nconditions on the kernel, we establish the explicit rate of convergence that\ndecays polynomially in sample size for a high-dimensional scaling limit, where\nthe dimension can be much larger than the sample size. We also supplement a\npractical Gaussian wild bootstrap method to approximate the quantiles of the\nmaxima of centered U-statistics and prove its asymptotic validity. The wild\nbootstrap is demonstrated on statistical applications for high-dimensional\nnon-Gaussian data including: (i) principled and data-dependent tuning parameter\nselection for regularized estimation of the covariance matrix and its related\nfunctionals; (ii) simultaneous inference for the covariance and rank\ncorrelation matrices. In particular, for the thresholded covariance matrix\nestimator with the bootstrap selected tuning parameter, we show that the\nGaussian-like convergence rates can be achieved for heavy-tailed data, which\nare less conservative than those obtained by the Bonferroni technique that\nignores the dependency in the underlying data distribution. In addition, we\nalso show that even for subgaussian distributions, error bounds of the\nbootstrapped thresholded covariance matrix estimator can be much tighter than\nthose of the minimax estimator with a universal threshold.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 04:29:54 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 03:44:21 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 21:04:40 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Chen", "Xiaohui", ""]]}, {"id": "1602.00354", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park", "title": "Active Learning Algorithms for Graphical Model Selection", "comments": "26 pages, 3 figures. Preliminary version to appear in AI & Statistics\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning the structure of a high dimensional graphical model\nfrom data has received considerable attention in recent years. In many\napplications such as sensor networks and proteomics it is often expensive to\nobtain samples from all the variables involved simultaneously. For instance,\nthis might involve the synchronization of a large number of sensors or the\ntagging of a large number of proteins. To address this important issue, we\ninitiate the study of a novel graphical model selection problem, where the goal\nis to optimize the total number of scalar samples obtained by allowing the\ncollection of samples from only subsets of the variables. We propose a general\nparadigm for graphical model selection where feedback is used to guide the\nsampling to high degree vertices, while obtaining only few samples from the\nones with the low degrees. We instantiate this framework with two specific\nactive learning algorithms, one of which makes mild assumptions but is\ncomputationally expensive, while the other is more computationally efficient\nbut requires stronger (nevertheless standard) assumptions. Whereas the sample\ncomplexity of passive algorithms is typically a function of the maximum degree\nof the graph, we show that the sample complexity of our algorithms is provable\nsmaller and that it depends on a novel local complexity measure that is akin to\nthe average degree of the graph. We finally demonstrate the efficacy of our\nframework via simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:04:34 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 05:49:56 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Singh", "Aarti", ""], ["Balcan", "Maria-Florina", ""], ["Park", "Jong Hyuk", ""]]}, {"id": "1602.00359", "submitter": "Forrest Crawford", "authors": "Peter M. Aronow and Forrest W. Crawford and Jos\\'e R. Zubizarreta", "title": "Confidence intervals for means under constrained dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for conducting inference on the mean of\ndependent random variables given constraints on their dependency graph. We\nestablish the consistency of an oracle variance estimator of the mean when the\ndependency graph is known, along with an associated central limit theorem. We\nderive an integer linear program for finding an upper bound for the estimated\nvariance when the graph is unknown, but topological and degree-based\nconstraints are available. We develop alternative bounds, including a\nclosed-form bound, under an additional homoskedasticity assumption. We\nestablish a basis for Wald-type confidence intervals for the mean that are\nguaranteed to have asymptotically conservative coverage. We apply the approach\nto inference from a social network link-tracing study and provide statistical\nsoftware implementing the approach.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:48:00 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Aronow", "Peter M.", ""], ["Crawford", "Forrest W.", ""], ["Zubizarreta", "Jos\u00e9 R.", ""]]}, {"id": "1602.00366", "submitter": "Tan Le Thanh", "authors": "Tan Le Thanh, Long Bao Le", "title": "Multi-Channel MAC Protocol for Full-Duplex Cognitive Radio Networks with\n  Optimized Access Control and Load Balancing", "comments": "To appear in 2016 IEEE International Conference on Communications\n  (IEEE ICC 2016). arXiv admin note: text overlap with arXiv:1512.03839", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-channel full-duplex Medium Access Control\n(MAC) protocol for cognitive radio networks (MFDC-MAC). Our design exploits the\nfact that full-duplex (FD) secondary users (SUs) can perform spectrum sensing\nand access simultaneously, and we employ the randomized dynamic channel\nselection for load balancing among channels and the standard backoff mechanism\nfor contention resolution on each available channel. Then, we develop a\nmathematical model to analyze the throughput performance of the proposed\nMFDC-MAC protocol. Furthermore, we study the protocol configuration\noptimization to maximize the network throughput where we show that this\noptimization can be performed in two steps, namely optimization of access and\ntransmission parameters on each channel and optimization of channel selection\nprobabilities of the users. Such optimization aims at achieving efficient\nself-interference management for FD transceivers, sensing overhead control, and\nload balancing among the channels. Numerical results demonstrate the impacts of\ndifferent protocol parameters and the importance of parameter optimization on\nthe throughput performance as well as the significant performance gain of the\nproposed design compared to traditional design.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 02:50:13 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Thanh", "Tan Le", ""], ["Le", "Long Bao", ""]]}, {"id": "1602.00382", "submitter": "Anit Kumar Sahu", "authors": "Anit Kumar Sahu, Soummya Kar, Jose' M. F. Moura and H. Vincent Poor", "title": "Distributed Constrained Recursive Nonlinear Least-Squares Estimation:\n  Algorithms and Asymptotics", "comments": "28 pages. Initial Submission: Feb. 2016, Revised: July 2016,\n  Accepted: September 2016, To appear in IEEE Transactions on Signal and\n  Information Processing over Networks: Special Issue on Inference and Learning\n  over Networks", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2618318", "report-no": null, "categories": "math.OC cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of recursive nonlinear least squares\nparameter estimation in multi-agent networks, in which the individual agents\nobserve sequentially over time an independent and identically distributed\n(i.i.d.) time-series consisting of a nonlinear function of the true but unknown\nparameter corrupted by noise. A distributed recursive estimator of the\n\\emph{consensus} + \\emph{innovations} type, namely $\\mathcal{CIWNLS}$, is\nproposed, in which the agents update their parameter estimates at each\nobservation sampling epoch in a collaborative way by simultaneously processing\nthe latest locally sensed information~(\\emph{innovations}) and the parameter\nestimates from other agents~(\\emph{consensus}) in the local neighborhood\nconforming to a pre-specified inter-agent communication topology. Under rather\nweak conditions on the connectivity of the inter-agent communication and a\n\\emph{global observability} criterion, it is shown that at every network agent,\nthe proposed algorithm leads to consistent parameter estimates. Furthermore,\nunder standard smoothness assumptions on the local observation functions, the\ndistributed estimator is shown to yield order-optimal convergence rates, i.e.,\nas far as the order of pathwise convergence is concerned, the local parameter\nestimates at each agent are as good as the optimal centralized nonlinear least\nsquares estimator which would require access to all the observations across all\nthe agents at all times. In order to benchmark the performance of the proposed\ndistributed $\\mathcal{CIWNLS}$ estimator with that of the centralized nonlinear\nleast squares estimator, the asymptotic normality of the estimate sequence is\nestablished and the asymptotic covariance of the distributed estimator is\nevaluated. Finally, simulation results are presented which illustrate and\nverify the analytical findings.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 03:47:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 01:26:30 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 15:21:56 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Sahu", "Anit Kumar", ""], ["Kar", "Soummya", ""], ["Moura", "Jose' M. F.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1602.00520", "submitter": "Hanne Kekkonen", "authors": "Martin Burger, Tapio Helin and Hanne Kekkonen", "title": "Large Noise in Variational Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider variational regularization methods for inverse\nproblems with large noise that is in general unbounded in the image space of\nthe forward operator. We introduce a Banach space setting that allows to define\na reasonable notion of solutions for more general noise in a larger space\nprovided one has sufficient mapping properties of the forward operators.\n  A key observation, which guides us through the subsequent analysis, is that\nsuch a general noise model can be understood with the same setting as\napproximate source conditions (while a standard model of bounded noise is\nrelated directly to classical source conditions). Based on this insight we\nobtain a quite general existence result for regularized variational problems\nand derive error estimates in terms of Bregman distances. The latter are\nspecialized for the particularly important cases of one- and p-homogeneous\nregularization functionals.\n  As a natural further step we study stochastic noise models and in particular\nwhite noise, for which we derive error estimates in terms of the expectation of\nthe Bregman distance. The finiteness of certain expectations leads to a novel\nclass of abstract smoothness conditions on the forward operator, which can be\neasily interpreted in the Hilbert space case. We finally exemplify the approach\nand in particular the conditions for popular examples of regularization\nfunctionals given by squared norm, Besov norm and total variation,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 13:44:01 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 11:22:58 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 10:24:18 GMT"}, {"version": "v4", "created": "Thu, 8 Feb 2018 11:17:46 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Burger", "Martin", ""], ["Helin", "Tapio", ""], ["Kekkonen", "Hanne", ""]]}, {"id": "1602.00522", "submitter": "Benjamin Guedj", "authors": "Le Li and Benjamin Guedj and S\\'ebastien Loustau", "title": "A Quasi-Bayesian Perspective to Online Clustering", "comments": null, "journal-ref": "Electronic Journal of Statistics (2018), vol. 12(2), 3071--3113", "doi": "10.1214/18-EJS1479", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When faced with high frequency streams of data, clustering raises theoretical\nand algorithmic pitfalls. We introduce a new and adaptive online clustering\nalgorithm relying on a quasi-Bayesian approach, with a dynamic (i.e.,\ntime-dependent) estimation of the (unknown and changing) number of clusters. We\nprove that our approach is supported by minimax regret bounds. We also provide\nan RJMCMC-flavored implementation (called PACBO, see\nhttps://cran.r-project.org/web/packages/PACBO/index.html) for which we give a\nconvergence guarantee. Finally, numerical experiments illustrate the potential\nof our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 13:49:06 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 08:35:09 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 15:51:09 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Li", "Le", ""], ["Guedj", "Benjamin", ""], ["Loustau", "S\u00e9bastien", ""]]}, {"id": "1602.00531", "submitter": "Nicolas Asin", "authors": "Nicolas Asin and Jan Johannes", "title": "Adaptive non-parametric estimation in the presence of dependence", "comments": "39 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider non-parametric estimation problems in the presence of dependent\ndata, notably non-parametric regression with random design and non-parametric\ndensity estimation. The proposed estimation procedure is based on a dimension\nreduction. The minimax optimal rate of convergence of the estimator is derived\nassuming a sufficiently weak dependence characterized by fast decreasing mixing\ncoefficients. We illustrate these results by considering classical smoothness\nassumptions. However, the proposed estimator requires an optimal choice of a\ndimension parameter depending on certain characteristics of the function of\ninterest, which are not known in practice. The main issue addressed in our work\nis an adaptive choice of this dimension parameter combining model selection and\nLepski's method. It is inspired by the recent work of Goldenshluger and Lepski\n(2011). We show that this data-driven estimator can attain the lower risk bound\nup to a constant provided a fast decay of the mixing coefficients.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 14:01:42 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Asin", "Nicolas", ""], ["Johannes", "Jan", ""]]}, {"id": "1602.00542", "submitter": "Ramji Venkataramanan", "authors": "K. Pavan Srinath and Ramji Venkataramanan", "title": "Cluster-Seeking James-Stein Estimators", "comments": "Appeared in IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, vol. 64, no. 2, pp.\n  853-874, February 2018", "doi": "10.1109/TIT.2017.2783543", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating a high-dimensional vector of\nparameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ from a noisy observation. The\nnoise vector is i.i.d. Gaussian with known variance. For a squared-error loss\nfunction, the James-Stein (JS) estimator is known to dominate the simple\nmaximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The\nJS-estimator shrinks the observed vector towards the origin, and the risk\nreduction over the ML-estimator is greatest for $\\boldsymbol{\\theta}$ that lie\nclose to the origin. JS-estimators can be generalized to shrink the data\ntowards any target subspace. Such estimators also dominate the ML-estimator,\nbut the risk reduction is significant only when $\\boldsymbol{\\theta}$ lies\nclose to the subspace. This leads to the question: in the absence of prior\ninformation about $\\boldsymbol{\\theta}$, how do we design estimators that give\nsignificant risk reduction over the ML-estimator for a wide range of\n$\\boldsymbol{\\theta}$?\n  In this paper, we propose shrinkage estimators that attempt to infer the\nstructure of $\\boldsymbol{\\theta}$ from the observed data in order to construct\na good attracting subspace. In particular, the components of the observed\nvector are separated into clusters, and the elements in each cluster shrunk\ntowards a common attractor. The number of clusters and the attractor for each\ncluster are determined from the observed vector. We provide concentration\nresults for the squared-error loss and convergence results for the risk of the\nproposed estimators. The results show that the estimators give significant risk\nreduction over the ML-estimator for a wide range of $\\boldsymbol{\\theta}$,\nparticularly for large $n$. Simulation results are provided to support the\ntheoretical claims.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 14:37:20 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 14:09:44 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 17:19:46 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 10:52:10 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Srinath", "K. Pavan", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1602.00853", "submitter": "Le Riche", "authors": "Hossein Mohammadi (LIMOS, DEMO-ENSMSE), Rodolphe Le Riche (LIMOS,\n  DEMO-ENSMSE), Nicolas Durrande (DEMO-ENSMSE, LIMOS), Eric Touboul (LIMOS,\n  DEMO-ENSMSE), Xavier Bay (LIMOS, DEMO-ENSMSE)", "title": "An analytic comparison of regularization methods for Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are a popular approach to predict the output of a\nparameterized experiment. They have many applications in the field of Computer\nExperiments, in particular to perform sensitivity analysis, adaptive design of\nexperiments and global optimization. Nearly all of the applications of GPs\nrequire the inversion of a covariance matrix that, in practice, is often\nill-conditioned. Regularization methodologies are then employed with\nconsequences on the GPs that need to be better understood.The two principal\nmethods to deal with ill-conditioned covariance matrices are i) pseudoinverse\nand ii) adding a positive constant to the diagonal (the so-called nugget\nregularization).The first part of this paper provides an algebraic comparison\nof PI and nugget regularizations. Redundant points, responsible for covariance\nmatrix singularity, are defined. It is proven that pseudoinverse\nregularization, contrarily to nugget regularization, averages the output values\nand makes the variance zero at redundant points. However, pseudoinverse and\nnugget regularizations become equivalent as the nugget value vanishes. A\nmeasure for data-model discrepancy is proposed which serves for choosing a\nregularization technique.In the second part of the paper, a distribution-wise\nGP is introduced that interpolates Gaussian distributions instead of data\npoints. Distribution-wise GP can be seen as an improved regularization method\nfor GPs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:46:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 08:30:20 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 12:29:59 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Mohammadi", "Hossein", "", "LIMOS, DEMO-ENSMSE"], ["Riche", "Rodolphe Le", "", "LIMOS,\n  DEMO-ENSMSE"], ["Durrande", "Nicolas", "", "DEMO-ENSMSE, LIMOS"], ["Touboul", "Eric", "", "LIMOS,\n  DEMO-ENSMSE"], ["Bay", "Xavier", "", "LIMOS, DEMO-ENSMSE"]]}, {"id": "1602.00856", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi and Roberto Casarin and Bertrand Maillet and Lea\n  Petrella", "title": "Dynamic Model Averaging for Bayesian Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general dynamic model averaging (DMA) approach based on\nMarkov-Chain Monte Carlo for the sequential combination and estimation of\nquantile regression models with time-varying parameters. The efficiency and the\neffectiveness of the proposed DMA approach and the MCMC algorithm are shown\nthrough simulation studies and applications to macro-economics and finance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 09:49:42 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Bernardi", "Mauro", ""], ["Casarin", "Roberto", ""], ["Maillet", "Bertrand", ""], ["Petrella", "Lea", ""]]}, {"id": "1602.00869", "submitter": "Ritwik Chaudhuri", "authors": "Ritwik Chaudhuri, Vladas Pipiras", "title": "Non-Gaussian semi-stable laws arising in sampling of finite point\n  processes", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ686 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1055-1092", "doi": "10.3150/14-BEJ686", "report-no": "IMS-BEJ-BEJ686", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite point process is characterized by the distribution of the number of\npoints (the size) of the process. In some applications, for example, in the\ncontext of packet flows in modern communication networks, it is of interest to\ninfer this size distribution from the observed sizes of sampled point\nprocesses, that is, processes obtained by sampling independently the points of\ni.i.d. realizations of the original point process. A standard nonparametric\nestimator of the size distribution has already been suggested in the\nliterature, and has been shown to be asymptotically normal under suitable but\nrestrictive assumptions. When these assumptions are not satisfied, it is shown\nhere that the estimator can be attracted to a semi-stable law. The assumptions\nare discussed in the case of several concrete examples. A major theoretical\ncontribution of this work are new and quite general sufficient conditions for a\nsequence of i.i.d. random variables to be attracted to a semi-stable law.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 10:34:37 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Chaudhuri", "Ritwik", ""], ["Pipiras", "Vladas", ""]]}, {"id": "1602.00879", "submitter": "Joni Virta", "authors": "Joni Virta, Bing Li, Klaus Nordhausen and Hannu Oja", "title": "Independent component analysis for tensor-valued data", "comments": "26 pages, 4 figures", "journal-ref": "Journal of Multivariate Analysis, 162, 172-192, 2017", "doi": "10.1016/j.jmva.2017.09.008", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In preprocessing tensor-valued data, e.g. images and videos, a common\nprocedure is to vectorize the observations and subject the resulting vectors to\none of the many methods used for independent component analysis (ICA). However,\nthe tensor structure of the original data is lost in the vectorization and, as\na more suitable alternative, we propose the matrix- and tensor fourth order\nblind identification (MFOBI and TFOBI). In these tensorial extensions of the\nclassic fourth order blind identification (FOBI) we assume a Kronecker\nstructure for the mixing and perform FOBI simultaneously on each direction of\nthe observed tensors. We discuss the theory and assumptions behind MFOBI and\nTFOBI and provide two different algorithms and related estimates of the\nunmixing matrices along with their asymptotic properties. Finally, simulations\nare used to compare the method's performance with that of classical FOBI for\nvectorized data and we end with a real data clustering example.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 11:03:13 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 17:11:18 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Virta", "Joni", ""], ["Li", "Bing", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "1602.00886", "submitter": "S{\\o}ren Johansen", "authors": "S{\\o}ren Johansen, Bent Nielsen", "title": "Analysis of the Forward Search using some new results for martingales\n  and empirical processes", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ689 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1131-1183", "doi": "10.3150/14-BEJ689", "report-no": "IMS-BEJ-BEJ689", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Forward Search is an iterative algorithm for avoiding outliers in a\nregression analysis suggested by Hadi and Simonoff (J. Amer. Statist. Assoc. 88\n(1993) 1264-1272), see also Atkinson and Riani (Robust Diagnostic Regression\nAnalysis (2000) Springer). The algorithm constructs subsets of \"good\"\nobservations so that the size of the subsets increases as the algorithm\nprogresses. It results in a sequence of regression estimators and forward\nresiduals. Outliers are detected by monitoring the sequence of forward\nresiduals. We show that the sequences of regression estimators and forward\nresiduals converge to Gaussian processes. The proof involves a new iterated\nmartingale inequality, a theory for a new class of weighted and marked\nempirical processes, the corresponding quantile process theory, and a fixed\npoint argument to describe the iterative aspect of the procedure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 11:27:26 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Johansen", "S\u00f8ren", ""], ["Nielsen", "Bent", ""]]}, {"id": "1602.01051", "submitter": "Zijian Guo", "authors": "Zijian Guo, Dylan Small", "title": "Control Function Instrumental Variable Estimation of Nonlinear Causal\n  Effect Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instrumental variable method consistently estimates the effect of a\ntreatment when there is unmeasured confounding and a valid instrumental\nvariable. A valid instrumental variable is a variable that is independent of\nunmeasured confounders and affects the treatment but does not have a direct\neffect on the outcome beyond its effect on the treatment. Two commonly used\nestimators for using an instrumental variable to estimate a treatment effect\nare the two stage least squares estimator and the control function estimator.\nFor linear causal effect models, these two estimators are equivalent, but for\nnonlinear causal effect models, the estimators are different. We provide a\nsystematic comparison of these two estimators for nonlinear causal effect\nmodels and develop an approach to combing the two estimators that generally\nperforms better than either one alone. We show that the control function\nestimator is a two stage least squares estimator with an augmented set of\ninstrumental variables. If these augmented instrumental variables are valid,\nthen the control function estimator can be much more efficient than usual two\nstage least squares without the augmented instrumental variables while if the\naugmented instrumental variables are not valid, then the control function\nestimator may be inconsistent while the usual two stage least squares remains\nconsistent. We apply the Hausman test to test whether the augmented\ninstrumental variables are valid and construct a pretest estimator based on\nthis test. The pretest estimator is shown to work well in a simulation study.\nAn application to the effect of exposure to violence on time preference is\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:24:18 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Guo", "Zijian", ""], ["Small", "Dylan", ""]]}, {"id": "1602.01132", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Tom Hess", "title": "Interactive algorithms: from pool to stream", "comments": "Appearing in COLT 2016", "journal-ref": "S. Sabato and T. Hess, \"Interactive Algorithms: from Pool to\n  Stream\", Proceedings of the 29th Annual Conference on Learning Theory (COLT),\n  JMLR Workshop and Conference Proceedings 49:1419-1439, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 22:06:02 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 09:27:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 13:40:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""], ["Hess", "Tom", ""]]}, {"id": "1602.01196", "submitter": "Peng Ding", "authors": "Peng Ding, Jiannan Lu", "title": "Principal stratification analysis using principal scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners are interested in not only the average causal effect of the\ntreatment on the outcome but also the underlying causal mechanism in the\npresence of an intermediate variable between the treatment and outcome.\nHowever, in many cases we cannot randomize the intermediate variable, resulting\nin sample selection problems even in randomized experiments. Therefore, we view\nrandomized experiments with intermediate variables as semi-observational\nstudies. In parallel with the analysis of observational studies, we provide a\ntheoretical foundation for conducting objective causal inference with an\nintermediate variable under the principal stratification framework, with\nprincipal strata defined as the joint potential values of the intermediate\nvariable. Our strategy constructs weighted samples based on principal scores,\ndefined as the conditional probabilities of the latent principal strata given\ncovariates, without access to any outcome data. This principal stratification\nanalysis yields robust causal inference without relying on any model\nassumptions on the outcome distributions. We also propose approaches to\nconducting sensitivity analysis for violations of the ignorability and\nmonotonicity assumptions, the very crucial but untestable identification\nassumptions in our theory. When the assumptions required by the classical\ninstrumental variable analysis cannot be justified by background knowledge or\ncannot be made because of scientific questions of interest, our strategy serves\nas a useful alternative tool to deal with intermediate variables. We illustrate\nour methodologies by using two real data examples, and find scientifically\nmeaningful conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 05:56:30 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Ding", "Peng", ""], ["Lu", "Jiannan", ""]]}, {"id": "1602.01262", "submitter": "Bikramjit Das", "authors": "Bikramjit Das and Sidney I. Resnick", "title": "Hidden Regular Variation under Full and Strong Asymptotic Dependence", "comments": "25 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exhibiting heavy-tails in one or more dimensions is often studied using\nthe framework of regular variation. In a multivariate setting this requires\nidentifying specific forms of dependence in the data; this means identifying\nthat the data tends to concentrate along particular directions and does not\ncover the full space. This is observed in various data sets from finance,\ninsurance, network traffic, social networks, etc. In this paper we discuss the\nnotions of full and strong asymptotic dependence for bivariate data along with\nthe idea of hidden regular variation in these cases. In a risk analysis\nsetting, this leads to improved risk estimation accuracy when regular methods\nprovide a zero estimate of risk. Analyses of both real and simulated data sets\nillustrate concepts of generation and detection of such models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:51:47 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 17:29:36 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 01:14:13 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Das", "Bikramjit", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "1602.01269", "submitter": "Emanuele Dolera", "authors": "Donato Michele Cifarelli, Emanuele Dolera, Eugenio Regazzini", "title": "Frequentistic approximations to Bayesian prevision of exchangeable\n  random elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence \\xi_1, \\xi_2,... of X-valued, exchangeable random elements,\nlet q(\\xi^(n)) and p_m(\\xi^(n)) stand for posterior and predictive\ndistribution, respectively, given \\xi^(n) = (\\xi_1,..., \\xi_n). We provide an\nupper bound for limsup b_n d_[[X]](q(\\xi^(n)), \\delta_\\empiricn) and limsup b_n\nd_[X^m](p_m(\\xi^(n)), \\empiricn^m), where \\empiricn is the empirical measure,\nb_n is a suitable sequence of positive numbers increasing to +\\infty, d_[[X]]\nand d_[X^m] denote distinguished weak probability distances on [[X]] and [X^m],\nrespectively, with the proviso that [S] denotes the space of all probability\nmeasures on S. A characteristic feature of our work is that the aforesaid\nbounds are established under the law of the \\xi_n's, unlike the more common\nliterature on Bayesian consistency, where they are studied with respect to\nproduct measures (p_0)^\\infty, as p_0 varies among the admissible\ndeterminations of a random probability measure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 11:28:31 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Cifarelli", "Donato Michele", ""], ["Dolera", "Emanuele", ""], ["Regazzini", "Eugenio", ""]]}, {"id": "1602.01355", "submitter": "Arkadi Nemirovski", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "Near-Optimality of Linear Recovery in Gaussian Observation Scheme under\n  $\\|\\cdot\\|_2^2$-Loss", "comments": null, "journal-ref": "The Annals of Statistics 46:4 (2018), 1603-1629", "doi": "10.1214/17-AOS1596", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering linear image $Bx$ of a signal $x$ known\nto belong to a given convex compact set $X$ from indirect observation\n$\\omega=Ax+\\sigma\\xi$ of $x$ corrupted by Gaussian noise $\\xi$. It is shown\nthat under some assumptions on $X$ (satisfied, e.g., when $X$ is the\nintersection of $K$ concentric ellipsoids/elliptic cylinders), an\neasy-to-compute linear estimate is near-optimal, in certain precise sense, in\nterms of its worst-case, over $x\\in X$, expected $\\|\\cdot\\|_2^2$-error. The\nmain novelty here is that our results impose no restrictions on $A$ and $B$, to\nthe best of our knowledge, preceding results on optimality of linear estimates\ndealt either with the case of direct observations $A=I$ and $B=I$, or with the\n\"diagonal case\" where $A$, $B$ are diagonal and $X$ is given by a \"separable\"\nconstraint like $X=\\{x:\\sum_ia_i^2x_i^2\\leq 1\\}$ or\n$X=\\{x:\\max_i|a_ix_i|\\leq1\\}$, or with estimating a linear form (i.e., the case\none-dimensional $Bx$).\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 16:09:44 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 15:21:44 GMT"}, {"version": "v3", "created": "Wed, 15 Feb 2017 16:11:39 GMT"}, {"version": "v4", "created": "Tue, 28 Feb 2017 11:06:23 GMT"}, {"version": "v5", "created": "Wed, 5 Apr 2017 02:33:34 GMT"}, {"version": "v6", "created": "Thu, 15 Jun 2017 16:10:03 GMT"}, {"version": "v7", "created": "Sun, 20 Oct 2019 19:54:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1602.01942", "submitter": "Francois Roueff", "authors": "Fran\\c{c}ois Roueff (LTCI), Andres Sanchez-Perez (LTCI)", "title": "Prediction of weakly locally stationary processes by auto-regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we introduce weakly locally stationary time series\nthrough the local approximation of the non-stationary covariance structure by a\nstationary one. This allows us to define autoregression coefficients in a\nnon-stationary context, which, in the particular case of a locally stationary\nTime Varying Autoregressive (TVAR) process, coincide with the generating\ncoefficients. We provide and study an estimator of the time varying\nautoregression coefficients in a general setting. The proposed estimator of\nthese coefficients enjoys an optimal minimax convergence rate under limited\nsmoothness conditions. In a second step, using a bias reduction technique, we\nderive a minimax-rate estimator for arbitrarily smooth time-evolving\ncoefficients, which outperforms the previous one for large data sets. In turn,\nfor TVAR processes, the predictor derived from the estimator exhibits an\noptimal minimax prediction rate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 07:47:11 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 10:44:47 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 08:41:25 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Roueff", "Fran\u00e7ois", "", "LTCI"], ["Sanchez-Perez", "Andres", "", "LTCI"]]}, {"id": "1602.01951", "submitter": "Alessio Sancetta", "authors": "Alessio Sancetta", "title": "Greedy algorithms for prediction", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ691 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1227-1277", "doi": "10.3150/14-BEJ691", "report-no": "IMS-BEJ-BEJ691", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many prediction problems, it is not uncommon that the number of variables\nused to construct a forecast is of the same order of magnitude as the sample\nsize, if not larger. We then face the problem of constructing a prediction in\nthe presence of potentially large estimation error. Control of the estimation\nerror is either achieved by selecting variables or combining all the variables\nin some special way. This paper considers greedy algorithms to solve this\nproblem. It is shown that the resulting estimators are consistent under weak\nconditions. In particular, the derived rates of convergence are either minimax\nor improve on the ones given in the literature allowing for dependence and\nunbounded regressors. Some versions of the algorithms provide fast solution to\nproblems such as Lasso.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 08:27:27 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Sancetta", "Alessio", ""]]}, {"id": "1602.01955", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai, Steffen Schenk, Matthias Scherer", "title": "Exchangeable exogenous shock models", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ693 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1278-1299", "doi": "10.3150/14-BEJ693", "report-no": "IMS-BEJ-BEJ693", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize a comprehensive family of $d$-variate exogenous shock models.\nAnalytically, we consider a family of multivariate distribution functions that\narises from ordering, idiosyncratically distorting, and finally multiplying the\narguments. Necessary and sufficient conditions on the involved distortions to\nyield a multivariate distribution function are given. Probabilistically, the\nattainable set of distribution functions corresponds to a large class of\nexchangeable exogenous shock models. Besides, the vector of exceedance times of\nan increasing additive stochastic process across independent exponential\ntrigger variables is shown to constitute an interesting subclass of the\nconsidered distributions and yields a second probabilistic model. The\nalternative construction is illustrated in terms of two examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 08:48:09 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Mai", "Jan-Frederik", ""], ["Schenk", "Steffen", ""], ["Scherer", "Matthias", ""]]}, {"id": "1602.02092", "submitter": "Adrien Richou", "authors": "Bernard Bercu and Adrien Richou", "title": "Large deviations and concentration inequalities for the\n  Ornstein-Uhlenbeck process without tears", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to establish large deviations and concentration inequalities for\nthe maximum likelihood estimator of the drift parameter of the\nOrnstein-Uhlenbeck process without tears. We propose a new strategy to\nestablish large deviation results which allows us, via a suitable\ntransformation, to circumvent the classical difficulty of non-steepness. Our\napproach holds in the stable case where the process is positive recurrent as\nwell as in the unstable and explosive cases where the process is respectively\nnull recurrent and transient. Notwithstanding of this trichotomy, we also\nprovide new concentration inequalities for the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 16:43:31 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 08:23:52 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bercu", "Bernard", ""], ["Richou", "Adrien", ""]]}, {"id": "1602.02210", "submitter": "Aaditya Ramdas", "authors": "Ilmun Kim, Aaditya Ramdas, Aarti Singh, Larry Wasserman", "title": "Classification accuracy as a proxy for two sample testing", "comments": "71 pages, 4 figures. Accepted for publication at the Annals of\n  Statistics (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data analysts train a classifier and check if its accuracy is\nsignificantly different from chance, they are implicitly performing a\ntwo-sample test. We investigate the statistical properties of this flexible\napproach in the high-dimensional setting. We prove two results that hold for\nall classifiers in any dimensions: if its true error remains $\\epsilon$-better\nthan chance for some $\\epsilon>0$ as $d,n \\to \\infty$, then (a) the\npermutation-based test is consistent (has power approaching to one), (b) a\ncomputationally efficient test based on a Gaussian approximation of the null\ndistribution is also consistent. To get a finer understanding of the rates of\nconsistency, we study a specialized setting of distinguishing Gaussians with\nmean-difference $\\delta$ and common (known or unknown) covariance $\\Sigma$,\nwhen $d/n \\to c \\in (0,\\infty)$. We study variants of Fisher's linear\ndiscriminant analysis (LDA) such as \"naive Bayes\" in a nontrivial regime when\n$\\epsilon \\to 0$ (the Bayes classifier has true accuracy approaching 1/2), and\ncontrast their power with corresponding variants of Hotelling's test.\nSurprisingly, the expressions for their power match exactly in terms of\n$n,d,\\delta,\\Sigma$, and the LDA approach is only worse by a constant factor,\nachieving an asymptotic relative efficiency (ARE) of $1/\\sqrt{\\pi}$ for\nbalanced samples. We also extend our results to high-dimensional elliptical\ndistributions with finite kurtosis. Other results of independent interest\ninclude minimax lower bounds, and the optimality of Hotelling's test when\n$d=o(n)$. Simulation results validate our theory, and we present practical\ntakeaway messages along with natural open problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 03:48:04 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:36:47 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 21:29:08 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 17:56:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kim", "Ilmun", ""], ["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1602.02279", "submitter": "Svetoslav Kostov", "authors": "Svetoslav Kostov, Nick Whiteley", "title": "An algorithm for approximating the second moment of the normalizing\n  constant estimate from a particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for approximating the non-asymptotic second moment\nof the marginal likelihood estimate, or normalizing constant, provided by a\nparticle filter. The computational cost of the new method is $O(M)$ per time\nstep, independently of the number of particles $N$ in the particle filter,\nwhere $M$ is a parameter controlling the quality of the approximation. This is\nin contrast to $O(MN)$ for a simple averaging technique using $M$ i.i.d.\nreplicates of a particle filter with $N$ particles. We establish that the\napproximation delivered by the new algorithm is unbiased, strongly consistent\nand, under standard regularity conditions, increasing $M$ linearly with time is\nsufficient to prevent growth of the relative variance of the approximation,\nwhereas for the simple averaging technique it can be necessary to increase $M$\nexponentially with time in order to achieve the same effect. Numerical examples\nillustrate performance in the context of a stochastic Lotka\\textendash Volterra\nsystem and a simple AR(1) model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:01:24 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 12:11:56 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Kostov", "Svetoslav", ""], ["Whiteley", "Nick", ""]]}, {"id": "1602.02330", "submitter": "Tingran Gao", "authors": "Tingran Gao", "title": "The Diffusion Geometry of Fibre Bundles: Horizontal Diffusion Maps", "comments": "65 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1503.05459 (an earlier version of this paper). To appear in Applied and\n  Computational Harmonic Analysis (2019+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based non-linear dimensionality reduction methods, such as Local\nLinear Embedding (LLE) and Laplacian Eigenmaps, rely heavily upon pairwise\ndistances or similarity scores, with which one can construct and study a\nweighted graph associated with the dataset. When each individual data object\ncarries additional structural details, however, the correspondence relations\nbetween these structures provide extra information that can be leveraged for\nstudying the dataset using the graph. Based on this observation, we generalize\nDiffusion Maps (DM) in manifold learning and introduce the framework of\nHorizontal Diffusion Maps (HDM). We model a dataset with pairwise structural\ncorrespondences as a fibre bundle equipped with a connection. We demonstrate\nthe advantage of incorporating such additional information and study the\nasymptotic behavior of HDM on general fibre bundles. In a broader context, HDM\nreveals the sub-Riemannian structure of high-dimensional datasets, and provides\na nonparametric learning framework for datasets with structural\ncorrespondences.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 02:37:52 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 04:23:42 GMT"}, {"version": "v3", "created": "Sun, 31 Mar 2019 15:27:24 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 21:22:10 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Gao", "Tingran", ""]]}, {"id": "1602.02345", "submitter": "Wenge Guo", "authors": "Anjana Grandhi, Wenge Guo, Joseph P. Romano", "title": "Control of Directional Errors in Fixed Sequence Multiple Testing", "comments": "40 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of simultaneously testing many\ntwo-sided hypotheses when rejections of null hypotheses are accompanied by\nclaims of the direction of the alternative. The fundamental goal is to\nconstruct methods that control the mixed directional familywise error rate,\nwhich is the probability of making any type 1 or type 3 (directional) error. In\nparticular, attention is focused on cases where the hypotheses are ordered as\n$H_1 , \\ldots, H_n$, so that $H_{i+1}$ is tested only if $H_1 , \\ldots, H_i$\nhave all been previously rejected. In this situation, one can control the usual\nfamilywise error rate under arbitrary dependence by the basic procedure which\ntests each hypothesis at level $\\alpha$, and no other multiplicity adjustment\nis needed. However, we show that this is far too liberal if one also accounts\nfor directional errors. But, by imposing certain dependence assumptions on the\ntest statistics, one can retain the basic procedure. Through a simulation study\nand a clinical trial example, we numerically illustrate good performance of the\nproposed procedures compared to the existing mdFWER controlling procedures. The\nproposed procedures are also implemented in the R-package FixSeqMTP.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 07:35:18 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 18:14:39 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Grandhi", "Anjana", ""], ["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1602.02408", "submitter": "Marta Garcia-Barzana M.A", "authors": "Marta Garc\\'ia B\\'arzana, Ana Colubi, Erricos John Kontoghiorghes", "title": "Lasso Estimation of an Interval-Valued Multiple Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple interval-valued linear regression model considering all the\ncross-relationships between the mids and spreads of the intervals has been\nintroduced recently. A least-squares estimation of the regression parameters\nhas been carried out by transforming a quadratic optimization problem with\ninequality constraints into a linear complementary problem and using Lemke's\nalgorithm to solve it. Due to the irrelevance of certain cross-relationships,\nan alternative estimation process, the LASSO (Least Absolut Shrinkage and\nSelection Operator), is developed. A comparative study showing the differences\nbetween the proposed estimators is provided.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 18:53:02 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["B\u00e1rzana", "Marta Garc\u00eda", ""], ["Colubi", "Ana", ""], ["Kontoghiorghes", "Erricos John", ""]]}, {"id": "1602.02466", "submitter": "Zo\\'e van Havre PhD", "authors": "Zo\\'e van Havre, Judith Rousseau, Nicole White, and Kerrie Mengersen", "title": "Overfitting hidden Markov models with an unknown number of states", "comments": "Submitted to Bayesian Analysis on 04-August-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new theory and methodology for the Bayesian estimation of\noverfitted hidden Markov models, with finite state space. The goal is then to\nachieve posterior emptying of extra states. A prior configuration is\nconstructed which favours configurations where the hidden Markov chain remains\nergodic although it empties out some of the states. Asymptotic posterior\nconvergence rates are proven theoretically, and demonstrated with a large\nsample simulation. The problem of overfitted HMMs is then considered in the\ncontext of smaller sample sizes, and due to computational and mixing issues two\nalternative prior structures are studied, one commonly used in practice, and a\nmixture of the two priors. The Prior Parallel Tempering approach of van Havre\n(2015) is also extended to HMMs to allow MCMC estimation of the complex\nposterior space. A replicate simulation study and an in-depth exploration is\nperformed to compare the three priors with hyperparameters chosen according to\nthe asymptotic constraints alongside less informative alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 05:23:26 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["van Havre", "Zo\u00e9", ""], ["Rousseau", "Judith", ""], ["White", "Nicole", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1602.02491", "submitter": "Makoto Aoshima", "authors": "Makoto Aoshima, Kazuyoshi Yata", "title": "Two-sample tests for high-dimension, strongly spiked eigenvalue models", "comments": "48 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-sample tests for high-dimensional data under two disjoint\nmodels: the strongly spiked eigenvalue (SSE) model and the non-SSE (NSSE)\nmodel. We provide a general test statistic as a function of a\npositive-semidefinite matrix. We give sufficient conditions for the test\nstatistic to satisfy a consistency property and to be asymptotically normal. We\ndiscuss an optimality of the test statistic under the NSSE model. We also\ninvestigate the test statistic under the SSE model by considering strongly\nspiked eigenstructures and create a new effective test procedure for the SSE\nmodel. Finally, we discuss the performance of the classifiers numerically.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 08:23:23 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 06:50:53 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 00:52:11 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Aoshima", "Makoto", ""], ["Yata", "Kazuyoshi", ""]]}, {"id": "1602.02605", "submitter": "Abdelhakim Necir", "authors": "Louiza Soltane, Djamel Meraghni, Abdelhakim Necir", "title": "Statistical estimate of the proportional hazard premium of loss under\n  random censoring", "comments": "arXiv admin note: text overlap with arXiv:1507.03178, arXiv:1302.1666", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many insurance premium principles are defined and various estimation\nprocedures introduced in the literature. In this paper, we focus on the\nestimation of the excess-of-loss reinsurance premium when the risks are\nrandomly right-censored. The asymptotic normality of the proposed estimator is\nestablished under suitable conditions and its performance evaluated through\nsets of simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:13:19 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 08:39:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Soltane", "Louiza", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1602.02616", "submitter": "Thomas Bonis", "authors": "Thomas Bonis", "title": "Guarantees in Wasserstein Distance for the Langevin Monte Carlo\n  Algorithm", "comments": "Updated and merged with arXiv article 1506.06966", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a distribution $\\target$ using the\nLangevin Monte Carlo algorithm and provide rate of convergences for this\nalgorithm in terms of Wasserstein distance of order $2$. Our result holds as\nlong as the continuous diffusion process associated with the algorithm\nconverges exponentially fast to the target distribution along with some\ntechnical assumptions. While such an exponential convergence holds for example\nin the log-concave measure case, it also holds for the more general case of\nasymptoticaly log-concave measures. Our results thus extends the known rates of\nconvergence in total variation and Wasserstein distances which have only been\nobtained in the log-concave case. Moreover, using a sharper approximation bound\nof the continuous process, we obtain better asymptotic rates than traditional\nresults. We also look into variations of the Langevin Monte Carlo algorithm\nusing other discretization schemes. In a first time, we look into the use of\nthe Ozaki's discretization but are unable to obtain any significative\nimprovement in terms of convergence rates compared to the Euler's scheme. We\nthen provide a (sub-optimal) way to study more general schemes, however our\napproach only holds for the log-concave case.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:49:07 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 11:19:23 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bonis", "Thomas", ""]]}, {"id": "1602.02854", "submitter": "Wenge Guo", "authors": "Wenge Guo, Joseph P. Romano", "title": "On Stepwise Control of Directional Errors under Independence and Some\n  Dependence", "comments": "31 pages", "journal-ref": "Journal of Statistical Planning and Inference 2015, Vol. 163,\n  21-33", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of error control of stepwise multiple testing\nprocedures is considered. For two-sided hypotheses, control of both type 1 and\ntype 3 (or directional) errors is required, and thus mixed directional\nfamilywise error rate control and mixed directional false discovery rate\ncontrol are each considered by incorporating both types of errors in the error\nrate. Mixed directional familywise error rate control of stepwise methods in\nmultiple testing has proven to be a challenging problem, as demonstrated in\nShaffer (1980). By an appropriate formulation of the problem, some new stepwise\nprocedures are developed that control type 1 and directional errors under\nindependence and various dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 04:03:10 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1602.02861", "submitter": "Ra\\'ul Fierro", "authors": "Ra\\'ul Fierro and V\\'ictor Leiva", "title": "A model for risk assessment of a large earthquake with application to\n  Chilean data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic distribution for the occurrence time of the next\nlarge earthquake, by knowing the last large seismic event occurred a long time\nago. We prove that, under reasonable conditions, such a distribution is\nasymptotically exponential with a rate depending on the asymptotic slope of the\ncumulative intensity function corresponding to a non-homogeneous Poisson\nprocess. Moreover, as it is not possible to obtain an empirical cumulative\ndistribution function for the waiting time of the next large earthquake, a\nrandom cumulative function based on existing data is stated. We demonstrate\nthat analogous results to the theorems of Glivenko-Cantelli and Kolmogorov are\nsatisfied by this random cumulative function. We conduct a simulation study for\ndetecting in what scenario the approximate distribution of the studied elapsed\ntime performs well. Finally, a real-world data analysis is carried out to\nillustrate the potential applications of our proposal.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 05:06:15 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Fierro", "Ra\u00fal", ""], ["Leiva", "V\u00edctor", ""]]}, {"id": "1602.02882", "submitter": "Francois Bachoc", "authors": "Fran\\c{c}ois Bachoc (GdR MASCOT-NUM, IMT), Reinhard Furrer (UZH)", "title": "On the smallest eigenvalues of covariance matrices of multivariate\n  spatial processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in providing models for multivariate\nspatial processes. A majority of these models specify a parametric matrix\ncovariance function. Based on observations, the parameters are estimated by\nmaximum likelihood or variants thereof. While the asymptotic properties of\nmaximum likelihood estimators for univariate spatial processes have been\nanalyzed in detail, maximum likelihood estimators for multivariate spatial\nprocesses have not received their deserved attention yet. In this article we\nconsider the classical increasing-domain asymptotic setting restricting the\nminimum distance between the locations. Then, one of the main components to be\nstudied from a theoretical point of view is the asymptotic positive\ndefiniteness of the underlying covariance matrix. Based on very weak\nassumptions on the matrix covariance function we show that the smallest\neigenvalue of the covariance matrix is asymptotically bounded away from zero.\nSeveral practical implications are discussed as well.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 07:50:13 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "GdR MASCOT-NUM, IMT"], ["Furrer", "Reinhard", "", "UZH"]]}, {"id": "1602.02889", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Ergodicity of Markov chain Monte Carlo with reversible proposal", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ergodic properties of some Metropolis-Hastings (MH) algorithms\nfor heavy-tailed target distributions. The analysis usually falls into\nsub-geometric ergodicity framework but we prove that the mixed preconditioned\nCrank-Nicolson (MpCN) algorithm has geometric ergodicity even for heavy-tailed\ntarget distributions. This useful property comes from the fact that the MpCN\nalgorithm becomes a random-walk Metropolis algorithm under suitable\ntransformation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:20:23 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1602.02905", "submitter": "Patrick Cattiaux", "authors": "Patrick Cattiaux, Myriam Fradon, Alexei M. Kulik, Sylvie Roelly", "title": "Long time behavior of stochastic hard ball systems", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ672 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 681-710", "doi": "10.3150/14-BEJ672", "report-no": "IMS-BEJ-BEJ672", "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the long time behavior of a system of $n=2,3$ Brownian hard balls,\nliving in $\\mathbb{R}^d$ for $d\\ge2$, submitted to a mutual attraction and to\nelastic collisions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:51:46 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Cattiaux", "Patrick", ""], ["Fradon", "Myriam", ""], ["Kulik", "Alexei M.", ""], ["Roelly", "Sylvie", ""]]}, {"id": "1602.02907", "submitter": "Fred Espen Benth", "authors": "Fred Espen Benth, Heidar Eyjolfsson", "title": "Simulation of volatility modulated Volterra processes using hyperbolic\n  stochastic partial differential equations", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ675 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 774-793", "doi": "10.3150/14-BEJ675", "report-no": "IMS-BEJ-BEJ675", "categories": "math.ST math.PR q-fin.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a finite difference scheme to simulate solutions to a certain type\nof hyperbolic stochastic partial differential equation (HSPDE). These solutions\ncan in turn estimate so called volatility modulated Volterra (VMV) processes\nand L\\'{e}vy semistationary (LSS) processes, which is a class of processes that\nhave been employed to model turbulence, tumor growth and electricity forward\nand spot prices. We will see that our finite difference scheme converges to the\nsolution of the HSPDE as we take finer and finer partitions for our finite\ndifference scheme in both time and space. Finally, we demonstrate our method\nwith an example from the energy finance literature.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 09:07:28 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Benth", "Fred Espen", ""], ["Eyjolfsson", "Heidar", ""]]}, {"id": "1602.03061", "submitter": "Matthew Reyes", "authors": "Matthew G. Reyes and David L. Neuhoff", "title": "Minimum Conditional Description Length Estimation for Markov Random\n  Fields", "comments": "Information Theory and Applications (ITA) workshop, February 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss a method, which we call Minimum Conditional\nDescription Length (MCDL), for estimating the parameters of a subset of sites\nwithin a Markov random field. We assume that the edges are known for the entire\ngraph $G=(V,E)$. Then, for a subset $U\\subset V$, we estimate the parameters\nfor nodes and edges in $U$ as well as for edges incident to a node in $U$, by\nfinding the exponential parameter for that subset that yields the best\ncompression conditioned on the values on the boundary $\\partial U$. Our\nestimate is derived from a temporally stationary sequence of observations on\nthe set $U$. We discuss how this method can also be applied to estimate a\nspatially invariant parameter from a single configuration, and in so doing,\nderive the Maximum Pseudo-Likelihood (MPL) estimate.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 16:36:51 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 07:24:23 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Reyes", "Matthew G.", ""], ["Neuhoff", "David L.", ""]]}, {"id": "1602.03090", "submitter": "Ryan Rogers", "authors": "Marco Gaboardi, Hyun woo Lim, Ryan Rogers, Salil Vadhan", "title": "Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit\n  and Independence Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is a useful statistical tool in determining whether a\ngiven model should be rejected based on a sample from the population. Sample\ndata may contain sensitive information about individuals, such as medical\ninformation. Thus it is important to design statistical tests that guarantee\nthe privacy of subjects in the data. In this work, we study hypothesis testing\nsubject to differential privacy, specifically chi-squared tests for goodness of\nfit for multinomial data and independence between two categorical variables.\n  We propose new tests for goodness of fit and independence testing that like\nthe classical versions can be used to determine whether a given model should be\nrejected or not, and that additionally can ensure differential privacy. We give\nboth Monte Carlo based hypothesis tests as well as hypothesis tests that more\nclosely follow the classical chi-squared goodness of fit test and the Pearson\nchi-squared test for independence. Crucially, our tests account for the\ndistribution of the noise that is injected to ensure privacy in determining\nsignificance.\n  We show that these tests can be used to achieve desired significance levels,\nin sharp contrast to direct applications of classical tests to differentially\nprivate contingency tables which can result in wildly varying significance\nlevels. Moreover, we study the statistical power of these tests. We empirically\nshow that to achieve the same level of power as the classical non-private tests\nour new tests need only a relatively modest increase in sample size.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 20:05:13 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 15:21:36 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Gaboardi", "Marco", ""], ["Lim", "Hyun woo", ""], ["Rogers", "Ryan", ""], ["Vadhan", "Salil", ""]]}, {"id": "1602.03427", "submitter": "Pierre C. Bellec", "authors": "Pierre C. Bellec", "title": "Aggregation of supports along the Lasso path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear regression with fixed design, we propose two procedures that\naggregate a data-driven collection of supports. The collection is a subset of\nthe $2^p$ possible supports and both its cardinality and its elements can\ndepend on the data. The procedures satisfy oracle inequalities with no\nassumption on the design matrix. Then we use these procedures to aggregate the\nsupports that appear on the regularization path of the Lasso in order to\nconstruct an estimator that mimics the best Lasso estimator. If the restricted\neigenvalue condition on the design matrix is satisfied, then this estimator\nachieves optimal prediction bounds. Finally, we discuss the computational cost\nof these procedures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 16:05:33 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 11:46:21 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Bellec", "Pierre C.", ""]]}, {"id": "1602.03436", "submitter": "Martin Genzel", "authors": "Martin Genzel", "title": "High-Dimensional Estimation of Structured Signals from Non-Linear\n  Observations with General Convex Loss Functions", "comments": null, "journal-ref": "IEEE Trans. Inf. Theory 64.3 (2017), 1601-1619", "doi": "10.1109/TIT.2016.2642993", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the issue of estimating a structured signal $x_0 \\in\n\\mathbb{R}^n$ from non-linear and noisy Gaussian observations. Supposing that\n$x_0$ is contained in a certain convex subset $K \\subset \\mathbb{R}^n$, we\nprove that accurate recovery is already feasible if the number of observations\nexceeds the effective dimension of $K$, which is a common measure for the\ncomplexity of signal classes. It will turn out that the possibly unknown\nnon-linearity of our model affects the error rate only by a multiplicative\nconstant. This achievement is based on recent works by Plan and Vershynin, who\nhave suggested to treat the non-linearity rather as noise which perturbs a\nlinear measurement process. Using the concept of restricted strong convexity,\nwe show that their results for the generalized Lasso can be extended to a\nfairly large class of convex loss functions. Moreover, we shall allow for the\npresence of adversarial noise so that even deterministic model inaccuracies can\nbe coped with. These generalizations particularly give further evidence of why\nmany standard estimators perform surprisingly well in practice, although they\ndo not rely on any knowledge of the underlying output rule. To this end, our\nresults provide a unified and general framework for signal reconstruction in\nhigh dimensions, covering various challenges from the fields of compressed\nsensing, signal processing, and statistical learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 16:35:36 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 08:13:04 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 12:52:57 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Genzel", "Martin", ""]]}, {"id": "1602.03471", "submitter": "Matthew Aldridge", "authors": "Matthew Aldridge, Oliver Johnson, Jonathan Scarlett", "title": "Improved group testing rates with constant column weight designs", "comments": "5 pages, 2 figures; to be presented at ISIT 2016", "journal-ref": "Proceedings of the International Symposium on Information Theory\n  (ISIT 2016), p1381-1385", "doi": "10.1109/ISIT.2016.7541525", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonadaptive group testing where each item is placed in a constant\nnumber of tests. The tests are chosen uniformly at random with replacement, so\nthe testing matrix has (almost) constant column weights. We show that\nperformance is improved compared to Bernoulli designs, where each item is\nplaced in each test independently with a fixed probability. In particular, we\nshow that the rate of the practical COMP detection algorithm is increased by\n31% in all sparsity regimes. In dense cases, this beats the best possible\nalgorithm with Bernoulli tests, and in sparse cases is the best proven\nperformance of any practical algorithm. We also give an algorithm-independent\nupper bound for the constant column weight case; for dense cases this is again\na 31% increase over the analogous Bernoulli result.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:18:35 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 10:19:46 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Aldridge", "Matthew", ""], ["Johnson", "Oliver", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "1602.03574", "submitter": "Rina Barber", "authors": "Rina Foygel Barber and Emmanuel J. Candes", "title": "A knockoff filter for high-dimensional selective inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a framework for testing for associations in a possibly\nhigh-dimensional linear model where the number of features/variables may far\nexceed the number of observational units. In this framework, the observations\nare split into two groups, where the first group is used to screen for a set of\npotentially relevant variables, whereas the second is used for inference over\nthis reduced set of variables; we also develop strategies for leveraging\ninformation from the first part of the data at the inference step for greater\npower. In our work, the inferential step is carried out by applying the\nrecently introduced knockoff filter, which creates a knockoff copy-a fake\nvariable serving as a control-for each screened variable. We prove that this\nprocedure controls the directional false discovery rate (FDR) in the reduced\nmodel controlling for all screened variables; this says that our\nhigh-dimensional knockoff procedure 'discovers' important variables as well as\nthe directions (signs) of their effects, in such a way that the expected\nproportion of wrongly chosen signs is below the user-specified level (thereby\ncontrolling a notion of Type S error averaged over the selected set). This\nresult is non-asymptotic, and holds for any distribution of the original\nfeatures and any values of the unknown regression coefficients, so that\ninference is not calibrated under hypothesized values of the effect sizes. We\ndemonstrate the performance of our general and flexible approach through\nnumerical studies, showing more power than existing alternatives. Finally, we\napply our method to a genome-wide association study to find locations on the\ngenome that are possibly associated with a continuous phenotype.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:36:37 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 19:26:34 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 13:34:24 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1602.03727", "submitter": "Maria Umlauft", "authors": "Markus Pauly, Maria Umlauft and Ali \\\"Unl\\\"u", "title": "Resampling-based inference methods for comparing two coefficient alpha", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-sample problem for Cronbach's coefficient $\\alpha_C$, as an estimate\nof test or composite score reliability, has attracted little attention,\ncompared to the extensive treatment of the one-sample case. It is necessary to\ncompare the reliability of a test for different subgroups, for different tests\nor the short and long forms of a test. In this paper, we study statistically\nhow to compare two coefficients $\\alpha_{C,1}$ and $\\alpha_{C,2}$. The null\nhypothesis of interest is $H_0 : \\alpha_{C,1} = \\alpha_{C,2}$, which we test\nagainst one-or two-sided alternatives. For this purpose, resampling-based\npermutation and bootstrap tests are proposed. These statistical tests ensure a\nbetter control of the type I error, in finite or very small sample sizes, when\nthe state-of-affairs \\textit{asymptotically distribution-free} (ADF)\nlarge-sample test may fail to properly attain the nominal significance level.\nWe introduce the permutation and bootstrap tests for the two-group multivariate\nnon-normal models under the general ADF setting, thereby improving on the small\nsample properties of the well-known ADF asymptotic test. By proper choice of a\nstudentized test statistic, the resampling tests are modified such that they\nare still asymptotically valid, if the data may not be exchangeable. The\nusefulness of the proposed resampling-based testing strategies is demonstrated\nin an extensive simulation study and illustrated by real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 14:01:25 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 09:49:52 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Pauly", "Markus", ""], ["Umlauft", "Maria", ""], ["\u00dcnl\u00fc", "Ali", ""]]}, {"id": "1602.03794", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Andrey Pepelyshev, Anatoly Zhigljavsky", "title": "Optimal designs for regression models with autoregressive errors\n  structure", "comments": "Keywords and Phrases: linear regression; correlated observatio ns;\n  signed measures; optimal design; BLUE; AR processes; continuous\n  autoregressive model AMS Subject classification: Primary 62K05; Secondary\n  31A10", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the one-parameter regression model with AR(1) and AR(2) errors we find\nexplicit expressions and a continuous approximation of the optimal discrete\ndesign for the signed least square estimator. The results are used to derive\nthe optimal variance of the best linear estimator in the continuous time model\nand to construct efficient estimators and corresponding optimal designs for\nfinite samples. The resulting procedure (estimator and design) provides nearly\nthe same efficiency as the weighted least squares and its variance is close to\nthe optimal variance in the continuous time model. The results are illustrated\nby several examples demonstrating the feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 17:14:28 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1602.03828", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Govinda Kamath, Changho Suh, David Tse", "title": "Community Recovery in Graphs with Locality", "comments": "accepted in part to International Conference on Machine Learning\n  (ICML), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in domains such as social networks and\ncomputational biology, we study the problem of community recovery in graphs\nwith locality. In this problem, pairwise noisy measurements of whether two\nnodes are in the same community or different communities come mainly or\nexclusively from nearby nodes rather than uniformly sampled between all nodes\npairs, as in most existing models. We present an algorithm that runs nearly\nlinearly in the number of measurements and which achieves the information\ntheoretic limit for exact recovery.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 19:13:20 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 19:26:32 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 18:18:04 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Chen", "Yuxin", ""], ["Kamath", "Govinda", ""], ["Suh", "Changho", ""], ["Tse", "David", ""]]}, {"id": "1602.03861", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Unified Statistical Theory of Spectral Graph Analysis", "comments": "Major changes have been done in terms of contents and structure of\n  the paper. New set of motivations for GraField, Expanding Section 4,\n  Connections with Diffusion map and Google's PageRank method etc", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to show that there exists a simple, yet universal\nstatistical logic of spectral graph analysis by recasting it into a\nnonparametric function estimation problem. The prescribed viewpoint appears to\nbe good enough to accommodate most of the existing spectral graph techniques as\na consequence of just one single formalism and algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 20:05:38 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 01:24:51 GMT"}, {"version": "v3", "created": "Sun, 20 Mar 2016 13:13:13 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 21:35:31 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1602.03969", "submitter": "Wenyi Zhang", "authors": "Wenyi Zhang, George V. Moustakides, H. Vincent Poor", "title": "Opportunistic Detection Rules: Finite and Asymptotic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opportunistic detection rules (ODRs) are variants of fixed-sample-size\ndetection rules in which the statistician is allowed to make an early decision\non the alternative hypothesis opportunistically based on the sequentially\nobserved samples. From a sequential decision perspective, ODRs are also\nmixtures of one-sided and truncated sequential detection rules. Several results\nregarding ODRs are established in this paper. In the finite regime, the maximum\nsample size is modeled either as a fixed finite number, or a geometric random\nvariable with a fixed finite mean. For both cases, the corresponding Bayesian\nformulations are investigated. The former case is a slight variation of the\nwell-known finite-length sequential hypothesis testing procedure in the\nliterature, whereas the latter case is new, for which the Bayesian optimal ODR\nis shown to be a sequence of likelihood ratio threshold tests with two\ndifferent thresholds: a running threshold, which is determined by solving a\nstationary state equation, is used when future samples are still available, and\na terminal threshold (simply the ratio between the priors scaled by costs) is\nused when the statistician reaches the final sample and thus has to make a\ndecision immediately. In the asymptotic regime, the tradeoff among the\nexponents of the (false alarm and miss) error probabilities and the normalized\nexpected stopping time under the alternative hypothesis is completely\ncharacterized and proved to be tight, via an information-theoretic argument.\nWithin the tradeoff region, one noteworthy fact is that the performance of the\nStein-Chernoff Lemma is attainable by ODRs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 07:34:33 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Zhang", "Wenyi", ""], ["Moustakides", "George V.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1602.04102", "submitter": "Nicolas Garcia Trillos", "authors": "Nicol\\'as Garc\\'ia Trillos, Dejan Slep\\v{c}ev, James von Brecht", "title": "Estimating perimeter using graph cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the estimation of the perimeter of a set by a graph cut of a\nrandom geometric graph. For $\\Omega \\subset D = (0,1)^d$, with $d \\geq 2$, we\nare given $n$ random i.i.d. points on $D$ whose membership in $\\Omega$ is\nknown. We consider the sample as a random geometric graph with connection\ndistance $\\varepsilon>0$. We estimate the perimeter of $\\Omega$ (relative to\n$D$) by the, appropriately rescaled, graph cut between the vertices in $\\Omega$\nand the vertices in $D \\backslash \\Omega$. We obtain bias and variance\nestimates on the error, which are optimal in scaling with respect to $n$ and\n$\\varepsilon$. We consider two scaling regimes: the dense (when the average\ndegree of the vertices goes to $\\infty$) and the sparse one (when the degree\ngoes to $0$). In the dense regime there is a crossover in the nature of\napproximation at dimension $d=5$: we show that in low dimensions $d=2,3,4$ one\ncan obtain confidence intervals for the approximation error, while in higher\ndimensions one can only obtain error estimates for testing the hypothesis that\nthe perimeter is less than a given number.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:20:23 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 17:31:31 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Trillos", "Nicol\u00e1s Garc\u00eda", ""], ["Slep\u010dev", "Dejan", ""], ["von Brecht", "James", ""]]}, {"id": "1602.04117", "submitter": "Florian Kelma", "authors": "Thomas Hotz, Florian Kelma", "title": "Non-asymptotic Confidence Sets for Extrinsic Means on Spheres and\n  Projective Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence sets from i.i.d. data are constructed for the extrinsic mean of a\nprobabilty measure P on spheres, real projective spaces, and complex projective\nspaces, as well as Grassmann manifolds, with the latter three embedded by the\nVeronese-Whitney embedding. When the data are sufficiently concentrated, these\nare projections of a ball around the corresponding Euclidean sample mean.\nFurthermore, these confidence sets are rate-optimal. The usefulness of this\napproach is illustrated for projective shape data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:56:08 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Hotz", "Thomas", ""], ["Kelma", "Florian", ""]]}, {"id": "1602.04310", "submitter": "Rania Zgheib", "authors": "Cristina Butucea, Rania Zgheib", "title": "Adaptive test for large covariance matrices with missing observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe $n$ independent $p-$dimensional Gaussian vectors with missing\ncoordinates, that is each value (which is assumed standardized) is observed\nwith probability $a>0$. We investigate the problem of minimax nonparametric\ntesting that the high-dimensional covariance matrix $\\Sigma$ of the underlying\nGaussian distribution is the identity matrix, using these partially observed\nvectors. Here, $n$ and $p$ tend to infinity and $a>0$ tends to 0,\nasymptotically.\n  We assume that $\\Sigma$ belongs to a Sobolev-type ellipsoid with parameter\n$\\alpha >0$. When $\\alpha$ is known, we give asymptotically minimax consistent\ntest procedure and find the minimax separation rates $\\tilde \\varphi_{n,p}=\n(a^2n \\sqrt{p})^{- \\frac{2 \\alpha}{4 \\alpha +1}}$, under some additional\nconstraints on $n,\\, p$ and $a$. We show that, in the particular case of\nToeplitz covariance matrices,the minimax separation rates are faster, $\\tilde\n\\phi_{n,p}= (a^2n p)^{- \\frac{2 \\alpha}{4 \\alpha +1}}$. We note how the\n\"missingness\" parameter $a$ deteriorates the rates with respect to the case of\nfully observed vectors ($a=1$).\n  We also propose adaptive test procedures, that is free of the parameter\n$\\alpha$ in some interval, and show that the loss of rate is $(\\ln \\ln (a^2\nn\\sqrt{p}))^{\\alpha/(4 \\alpha +1)}$ and $(\\ln \\ln (a^2 n p))^{\\alpha/(4 \\alpha\n+1)}$ for Toeplitz covariance matrices, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 10:08:01 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Butucea", "Cristina", ""], ["Zgheib", "Rania", ""]]}, {"id": "1602.04330", "submitter": "Florian Kelma", "authors": "Thomas Hotz, Florian Kelma and John T. Kent", "title": "Manifolds of Projective Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV math.GT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The projective shape of a configuration of k points or \"landmarks\" in RP(d)\nconsists of the information that is invariant under projective transformations\nand hence is reconstructable from uncalibrated camera views. Mathematically,\nthe space of projective shapes for these k landmarks can be described as the\nquotient space of k copies of RP(d) modulo the action of the projective linear\ngroup PGL(d). Using homogeneous coordinates, such configurations can be\ndescribed as real k-times-(d+1)-dimensional matrices given up to\nleft-multiplication of non-singular diagonal matrices, while the group PGL(d)\nacts as GL(d+1) from the right. The main purpose of this paper is to give a\ndetailed examination of the topology of projective shape space, and, using\nmatrix notation, it is shown how to derive subsets that are in a certain sense\nmaximal, differentiable Hausdorff manifolds which can be provided with a\nRiemannian metric. A special subclass of the projective shapes consists of the\nTyler regular shapes, for which geometrically motivated pre-shapes can be\ndefined, thus allowing for the construction of a natural Riemannian metric.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 13:32:22 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 08:32:00 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 11:12:13 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 17:04:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hotz", "Thomas", ""], ["Kelma", "Florian", ""], ["Kent", "John T.", ""]]}, {"id": "1602.04361", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin, Bharath Sriperumbudur and Krikamol Muandet", "title": "Minimax Estimation of Kernel Mean Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the minimax estimation of the Bochner integral\n$$\\mu_k(P):=\\int_{\\mathcal{X}} k(\\cdot,x)\\,dP(x),$$ also called as the kernel\nmean embedding, based on random samples drawn i.i.d.~from $P$, where\n$k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow\\mathbb{R}$ is a positive definite\nkernel. Various estimators (including the empirical estimator),\n$\\hat{\\theta}_n$ of $\\mu_k(P)$ are studied in the literature wherein all of\nthem satisfy $\\bigl\\|\n\\hat{\\theta}_n-\\mu_k(P)\\bigr\\|_{\\mathcal{H}_k}=O_P(n^{-1/2})$ with\n$\\mathcal{H}_k$ being the reproducing kernel Hilbert space induced by $k$. The\nmain contribution of the paper is in showing that the above mentioned rate of\n$n^{-1/2}$ is minimax in $\\|\\cdot\\|_{\\mathcal{H}_k}$ and\n$\\|\\cdot\\|_{L^2(\\mathbb{R}^d)}$-norms over the class of discrete measures and\nthe class of measures that has an infinitely differentiable density, with $k$\nbeing a continuous translation-invariant kernel on $\\mathbb{R}^d$. The\ninteresting aspect of this result is that the minimax rate is independent of\nthe smoothness of the kernel and the density of $P$ (if it exists). This result\nhas practical consequences in statistical applications as the mean embedding\nhas been widely employed in non-parametric hypothesis testing, density\nestimation, causal inference and feature selection, through its relation to\nenergy distance (and distance covariance).\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 17:53:48 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 19:37:03 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Sriperumbudur", "Bharath", ""], ["Muandet", "Krikamol", ""]]}, {"id": "1602.04387", "submitter": "Luca Weihs", "authors": "Preetam Nandy, Luca Weihs, and Mathias Drton", "title": "Large-Sample Theory for the Bergsma-Dassios Sign Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bergsma-Dassios sign covariance is a recently proposed extension of\nKendall's tau. In contrast to tau or also Spearman's rho, the new sign\ncovariance $\\tau^*$ vanishes if and only if the two considered random variables\nare independent. Specifically, this result has been shown for continuous as\nwell as discrete variables. We develop large-sample distribution theory for the\nempirical version of $\\tau^*$. In particular, we use theory for degenerate\nU-statistics to derive asymptotic null distributions under independence and\ndemonstrate in simulations that the limiting distributions give useful\napproximations.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 22:03:05 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nandy", "Preetam", ""], ["Weihs", "Luca", ""], ["Drton", "Mathias", ""]]}, {"id": "1602.04435", "submitter": "Denis Sidorov", "authors": "A. Zhukov, D. Sidorov and A. Foley", "title": "Random Forest Based Approach for Concept Drift Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift has potential in smart grid analysis because the socio-economic\nbehaviour of consumers is not governed by the laws of physics. Likewise there\nare also applications in wind power forecasting. In this paper we present\ndecision tree ensemble classification method based on the Random Forest\nalgorithm for concept drift. The weighted majority voting ensemble aggregation\nrule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.\nBase learner weight in our case is computed for each sample evaluation using\nbase learners accuracy and intrinsic proximity measure of Random Forest. Our\nalgorithm exploits both temporal weighting of samples and ensemble pruning as a\nforgetting strategy. We present results of empirical comparison of our method\nwith original random forest with incorporated \"replace-the-looser\" forgetting\nandother state-of-the-art concept-drfit classifiers like AWE2.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 09:58:39 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhukov", "A.", ""], ["Sidorov", "D.", ""], ["Foley", "A.", ""]]}, {"id": "1602.04484", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "Surprising properties of dropout in deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze dropout in deep networks with rectified linear units and the\nquadratic loss. Our results expose surprising differences between the behavior\nof dropout and more traditional regularizers like weight decay. For example, on\nsome simple data sets dropout training produces negative weights even though\nthe output is the sum of the inputs. This provides a counterpoint to the\nsuggestion that dropout discourages co-adaptation of weights. We also show that\nthe dropout penalty can grow exponentially in the depth of the network while\nthe weight-decay penalty remains essentially linear, and that dropout is\ninsensitive to various re-scalings of the input features, outputs, and network\nweights. This last insensitivity implies that there are no isolated local\nminima of the dropout training criterion. Our work uncovers new properties of\ndropout, extends our understanding of why dropout succeeds, and lays the\nfoundation for further progress.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 18:20:29 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 23:00:10 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 23:24:17 GMT"}, {"version": "v4", "created": "Thu, 3 Nov 2016 16:39:19 GMT"}, {"version": "v5", "created": "Wed, 19 Apr 2017 21:15:15 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1602.04589", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (CRIStAL, SEQUEL)", "title": "Optimal Best Arm Identification with Fixed Confidence", "comments": "Conference on Learning Theory (COLT), Jun 2016, New York, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of the complexity of best-arm\nidentification in one-parameter bandit problems. We prove a new, tight lower\nbound on the sample complexity. We propose the `Track-and-Stop' strategy, which\nwe prove to be asymptotically optimal. It consists in a new sampling rule\n(which tracks the optimal proportions of arm draws highlighted by the lower\nbound) and in a stopping rule named after Chernoff, for which we give a new\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 08:25:02 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:29:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"]]}, {"id": "1602.04676", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (CRIStAL, SEQUEL), Wouter\n  Koolen (CWI)", "title": "Maximin Action Identification: A New Bandit Framework for Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.GT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an original problem of pure exploration in a strategic bandit model\nmotivated by Monte Carlo Tree Search. It consists in identifying the best\naction in a game, when the player may sample random outcomes of sequentially\nchosen pairs of actions. We propose two strategies for the fixed-confidence\nsetting: Maximin-LUCB, based on lower-and upper-confidence bounds; and\nMaximin-Racing, which operates by successively eliminating the sub-optimal\nactions. We discuss the sample complexity of both methods and compare their\nperformance empirically. We sketch a lower bound analysis, and possible\nconnections to an optimal algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 13:55:45 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"], ["Koolen", "Wouter", "", "CWI"]]}, {"id": "1602.04850", "submitter": "Yongli Sang", "authors": "Hailin Sang and Yongli Sang", "title": "Memory properties of transformations of linear processes", "comments": "accepted by Statistical Inference for Stochastic Processes, 28 pages", "journal-ref": null, "doi": "10.1007/s11203-016-9134-4", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we study the memory properties of transformations of linear\nprocesses. Dittmann and Granger (2002) studied the polynomial transformations\nof Gaussian FARIMA(0,d,0) processes by applying the orthonormality of the\nHermite polynomials under the measure for the standard normal distribution.\nNevertheless, the orthogonality does not hold for transformations of\nnon-Gaussian linear processes. Instead, we use the decomposition developed by\nHo and Hsing (1996, 1997) to study the memory properties of nonlinear\ntransformations of linear processes, which include the FARIMA(p,d,q) processes,\nand obtain consistent results as in the Gaussian case. In particular, for\nstationary processes, the transformations of short-memory time series still\nhave short-memory and the transformation of long-memory time series may have\ndifferent weaker memory parameters which depend on the power rank of the\ntransformation. On the other hand, the memory properties of transformations of\nnon-stationary time series may not depend on the power ranks of the\ntransformations. This study has application in econometrics and financial data\nanalysis when the time series observations have non-Gaussian heavy tails. As an\nexample, the memory properties of call option processes at different strike\nprices are discussed in details.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 21:51:45 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 18:30:29 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Sang", "Hailin", ""], ["Sang", "Yongli", ""]]}, {"id": "1602.04912", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Athina P. Petropulu", "title": "Uniform {\\varepsilon}-Stability of Distributed Nonlinear Filtering over\n  DNAs: Gaussian-Finite HMMs", "comments": "30 pages, to appear in the IEEE Transactions on Signal & Information\n  Processing over Networks, in the upcoming Special Issue on Inference &\n  Learning over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study stability of distributed filtering of Markov chains\nwith finite state space, partially observed in conditionally Gaussian noise. We\nconsider a nonlinear filtering scheme over a Distributed Network of Agents\n(DNA), which relies on the distributed evaluation of the likelihood part of the\ncentralized nonlinear filter and is based on a particular specialization of the\nAlternating Direction Method of Multipliers (ADMM) for fast average consensus.\nAssuming the same number of consensus steps between any two consecutive noisy\nmeasurements for each sensor in the network, we fully characterize a minimal\nnumber of such steps, such that the distributed filter remains uniformly stable\nwith a prescribed accuracy level, {\\varepsilon} \\in (0,1], within a finite\noperational horizon, T, and across all sensors. Stability is in the sense of\nthe \\ell_1-norm between the centralized and distributed versions of the\nposterior at each sensor, and at each time within T. Roughly speaking, our main\nresult shows that uniform {\\varepsilon}-stability of the distributed filtering\nprocess depends only loglinearly on T and (roughly) the size of the network,\nand only logarithmically on 1/{\\varepsilon}. If this total loglinear bound is\nfulfilled, any additional consensus iterations will incur a fully quantified\nfurther exponential decay in the consensus error. Our bounds are universal, in\nthe sense that they are independent of the particular structure of the Gaussian\nHidden Markov Model (HMM) under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:23:13 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 20:55:19 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 16:29:28 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 18:39:58 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1602.04941", "submitter": "Juan A. Cuesta-Albertos", "authors": "Juan A. Cuesta-Albertos and Subhajit Dutta", "title": "On Perfect Classification and Clustering for Gaussian Processes", "comments": "52 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by singularity of a certain class of Gaussian measures, we propose\na data based transformation for infinite-dimensional data. For a classification\nproblem, an appropriate joint transformation induces complete separation among\nthe associated Gaussian processes. The misclassification probability of a\nsimple classifier when applied on this transformed data asymptotically\nconverges to zero. In a clustering problem, an appropriate modification of this\ntransformation asymptotically leads to perfect separation of the populations.\nTheoretical properties are studied for the usual $k$-means clustering method\nwhen used on the transformed data.\n  Good performance of the proposed methodology is demonstrated using simulated\nas well as benchmark data sets, when compared with some popular parametric and\nnonparametric classifiers for such functional data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 08:33:56 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 14:41:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Cuesta-Albertos", "Juan A.", ""], ["Dutta", "Subhajit", ""]]}, {"id": "1602.05063", "submitter": "Robin Ince", "authors": "Robin A. A. Ince", "title": "Measuring multivariate redundant information with pointwise common\n  change in surprisal", "comments": "v3: revisions based on review process at Entropy (expand game-theory\n  and max-ent motivation), v2: add game-theoretic operational definition for\n  maximum entropy constraints; remove thresholding and normalisation of values\n  on lattice", "journal-ref": "Entropy 2017, 19(7), 318", "doi": "10.3390/e19070318", "report-no": null, "categories": "cs.IT math.IT math.ST q-bio.NC q-bio.QM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The problem of how to properly quantify redundant information is an open\nquestion that has been the subject of much recent research. Redundant\ninformation refers to information about a target variable S that is common to\ntwo or more predictor variables Xi. It can be thought of as quantifying\noverlapping information content or similarities in the representation of S\nbetween the Xi. We present a new measure of redundancy which measures the\ncommon change in surprisal shared between variables at the local or pointwise\nlevel. We provide a game-theoretic operational definition of unique\ninformation, and use this to derive constraints which are used to obtain a\nmaximum entropy distribution. Redundancy is then calculated from this maximum\nentropy distribution by counting only those local co-information terms which\nadmit an unambiguous interpretation as redundant information. We show how this\nredundancy measure can be used within the framework of the Partial Information\nDecomposition (PID) to give an intuitive decomposition of the multivariate\nmutual information into redundant, unique and synergistic contributions. We\ncompare our new measure to existing approaches over a range of example systems,\nincluding continuous Gaussian variables. Matlab code for the measure is\nprovided, including all considered examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 15:57:36 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 10:32:28 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 16:54:30 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ince", "Robin A. A.", ""]]}, {"id": "1602.05125", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft, Michael Eichler", "title": "Locally Stationary Functional Time Series", "comments": null, "journal-ref": "Electronic Journal of Statistics Volume 12, Number 1 (2018),\n  107-170", "doi": "10.1214/17-EJS1384", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on time series of functional data has focused on processes of\nwhich the probabilistic law is either constant over time or constant up to its\nsecond-order structure. Especially for long stretches of data it is desirable\nto be able to weaken this assumption. This paper introduces a framework that\nwill enable meaningful statistical inference of functional data of which the\ndynamics change over time. We put forward the concept of local stationarity in\nthe functional setting and establish a class of processes that have a\nfunctional time-varying spectral representation. Subsequently, we derive\nconditions that allow for fundamental results from nonstationary multivariate\ntime series to carry over to the function space. In particular, time-varying\nfunctional ARMA processes are investigated and shown to be functional locally\nstationary according to the proposed definition. As a side-result, we establish\na Cram\\'er representation for an important class of weakly stationary\nfunctional processes. Important in our context is the notion of a time-varying\nspectral density operator of which the properties are studied and uniqueness is\nderived. Finally, we provide a consistent nonparametric estimator of this\noperator and show it is asymptotically Gaussian using a weaker tightness\ncriterion than what is usually deemed necessary.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:29:17 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:28:57 GMT"}, {"version": "v3", "created": "Sun, 10 Dec 2017 16:03:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["van Delft", "Anne", ""], ["Eichler", "Michael", ""]]}, {"id": "1602.05162", "submitter": "Tri Le", "authors": "Tri Le and Bertrand Clarke", "title": "A Bayes interpretation of stacking for M-complete and M-open settings", "comments": "37 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In M-open problems where no true model can be conceptualized, it is common to\nback off from modeling and merely seek good prediction. Even in M-complete\nproblems, taking a predictive approach can be very useful. Stacking is a model\naveraging procedure that gives a composite predictor by combining individual\npredictors from a list of models using weights that optimize a cross-validation\ncriterion. We show that the stacking weights also asymptotically minimize a\nposterior expected loss. Hence we formally provide a Bayesian justification for\ncross-validation. Often the weights are constrained to be positive and sum to\none. For greater generality, we omit the positivity constraint and relax the\n`sum to one' constraint.\n  A key question is `What predictors should be in the average?' We first verify\nthat the stacking error depends only on the span of the models. Then we propose\nusing bootstrap samples from the data to generate empirical basis elements that\ncan be used to form models. We use this in two computed examples to give\nstacking predictors that are (i) data driven, (ii) optimal with respect to the\nnumber of component predictors, and (iii) optimal with respect to the weight\neach predictor gets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:14:40 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Le", "Tri", ""], ["Clarke", "Bertrand", ""]]}, {"id": "1602.05394", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton, Jim Huang, Dominik Csiba, Cedric Archambeau", "title": "Online optimization and regret guarantees for non-additive long-term\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online optimization in the 1-lookahead setting, where the\nobjective does not decompose additively over the rounds of the online game. The\nresulting formulation enables us to deal with non-stationary and/or long-term\nconstraints , which arise, for example, in online display advertising problems.\nWe propose an on-line primal-dual algorithm for which we obtain dynamic\ncumulative regret guarantees. They depend on the convexity and the smoothness\nof the non-additive penalty, as well as terms capturing the smoothness with\nwhich the residuals of the non-stationary and long-term constraints vary over\nthe rounds. We conduct experiments on synthetic data to illustrate the benefits\nof the non-additive penalty and show vanishing regret convergence on live\ntraffic data collected by a display advertising platform in production.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:57:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 09:04:18 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jenatton", "Rodolphe", ""], ["Huang", "Jim", ""], ["Csiba", "Dominik", ""], ["Archambeau", "Cedric", ""]]}, {"id": "1602.05455", "submitter": "Ziwei Zhu", "authors": "Jianqing Fan, Han Liu, Weichen Wang and Ziwei Zhu", "title": "Heterogeneity Adjustment with Applications to Graphical Model Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is an unwanted variation when analyzing aggregated datasets\nfrom multiple sources. Though different methods have been proposed for\nheterogeneity adjustment, no systematic theory exists to justify these methods.\nIn this work, we propose a generic framework named ALPHA (short for Adaptive\nLow-rank Principal Heterogeneity Adjustment) to model, estimate, and adjust\nheterogeneity from the original data. Once the heterogeneity is adjusted, we\nare able to remove the biases of batch effects and to enhance the inferential\npower by aggregating the homogeneous residuals from multiple sources. Under a\npervasive assumption that the latent heterogeneity factors simultaneously\naffect a large fraction of observed variables, we provide a rigorous theory to\njustify the proposed framework. Our framework also allows the incorporation of\ninformative covariates and appeals to the \"Bless of Dimensionality\". As an\nillustrative application of this generic framework, we consider a problem of\nestimating high-dimensional precision matrix for graphical model inference\nbased on multiple datasets. We also provide thorough numerical studies on both\nsynthetic datasets and a brain imaging dataset to demonstrate the efficacy of\nthe developed theory and methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 15:50:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Liu", "Han", ""], ["Wang", "Weichen", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1602.05522", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "Taras Bodnar, Stepan Mazur, Nestor Parolya", "title": "Central limit theorems for functionals of large sample covariance matrix\n  and mean vector in matrix-variate location mixture of normal distributions", "comments": "30 pages, 8 figures, 1st revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the asymptotic distributions of functionals of the\nsample covariance matrix and the sample mean vector obtained under the\nassumption that the matrix of observations has a matrix-variate location\nmixture of normal distributions. The central limit theorem is derived for the\nproduct of the sample covariance matrix and the sample mean vector. Moreover,\nwe consider the product of the inverse sample covariance matrix and the mean\nvector for which the central limit theorem is established as well. All results\nare obtained under the large-dimensional asymptotic regime where the dimension\n$p$ and the sample size $n$ approach to infinity such that $p/n\\to c\\in[0 ,\n+\\infty)$ when the sample covariance matrix does not need to be invertible and\n$p/n\\to c\\in [0, 1)$ otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 18:32:15 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:22:09 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Bodnar", "Taras", ""], ["Mazur", "Stepan", ""], ["Parolya", "Nestor", ""]]}, {"id": "1602.05761", "submitter": "Ivan Vuja\\v{c}i\\'c", "authors": "Ivan Vuja\\v{c}i\\'c, Itai Dattner", "title": "Consistency of direct integral estimator for partially observed systems\n  of ordinary differential equations linear in the parameters", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic systems are ubiquitous in nature and are used to model many processes\nin biology, chemistry, physics, medicine, and engineering. In particular,\nsystems of ordinary differential equations are commonly used for the\nmathematical modelling of the rate of change of dynamic processes. In many\npractical applications, the process can only be partially measured, a fact that\nrenders estimation of parameters of the system extremely challenging. Recently,\na 'direct integral estimator' for partially observed systems of ordinary\ndifferential equations was introduced. The practical performance of the\nintegral estimator was demonstrated, but its theoretical properties were not\nderived. In this paper we use the sieve framework to prove that the estimator\nis consistent.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 11:32:48 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Vuja\u010di\u0107", "Ivan", ""], ["Dattner", "Itai", ""]]}, {"id": "1602.05801", "submitter": "Lukas Steinberger", "authors": "Lukas Steinberger and Hannes Leeb", "title": "Leave-one-out prediction intervals in linear regression models with many\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study prediction intervals based on leave-one-out residuals in a linear\nregression model where the number of explanatory variables can be large\ncompared to sample size. We establish uniform asymptotic validity (conditional\non the training sample) of the proposed interval under minimal assumptions on\nthe unknown error distribution and the high dimensional design. Our intervals\nare generic in the sense that they are valid for a large class of linear\npredictors used to obtain a point forecast, such as robust M-estimators,\nJames-Stein type estimators and penalized estimators like the LASSO. These\nresults show that despite the serious problems of resampling procedures for\ninference on the unknown parameters, leave-one-out methods can be successfully\napplied to obtain reliable predictive inference even in high dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 13:52:17 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Steinberger", "Lukas", ""], ["Leeb", "Hannes", ""]]}, {"id": "1602.05807", "submitter": "Wolfgang Trutschnig", "authors": "Thomas Mroz, Wolfgang Trutschnig, Juan Fern\\'andez S\\'anchez", "title": "Distributions with fixed marginals maximizing the mass of the endograph\n  of a function", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of maximizing the probability that $X$ does not default\nbefore $Y$ within the class of all random variables $X,Y$ with given\ndistribution functions $F$ and $G$ respectively, and construct a dependence\nstructure attaining the maximum. After translating the maximization problem to\nthe copula setting we generalize it and prove that for each (not necessarily\nmonotonic) transformation $T:[0,1] \\rightarrow [0,1]$ there exists a completely\ndependent copula maximizing the mass of the endograph $\\Gamma^\\leq(T)$ of $T$\nand derive a simple and easily calculable formula for the maximum. Analogous\nexpressions for the minimal mass are given. Several examples and graphics\nillustrate the main results and falsify some natural conjectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 14:12:02 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Mroz", "Thomas", ""], ["Trutschnig", "Wolfgang", ""], ["S\u00e1nchez", "Juan Fern\u00e1ndez", ""]]}, {"id": "1602.06028", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "Generalized Gaussian Mechanism for Differential Privacy", "comments": "23 pages; 12 figures", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering Volume: 31 ,\n  Issue: 4 , April 1 2019", "doi": "10.1109/TKDE.2018.2845388", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessment of disclosure risk is of paramount importance in the research and\napplications of data privacy techniques. The concept of differential privacy\n(DP) formalizes privacy in probabilistic terms and provides a robust concept\nfor privacy protection without making assumptions about the background\nknowledge of adversaries. Practical applications of DP involve development of\nDP mechanisms to release results at a pre-specified privacy budget. In this\npaper, we generalize the widely used Laplace mechanism to the family of\ngeneralized Gaussian (GG) mechanism based on the $l_p$ global sensitivity of\nstatistical queries. We explore the theoretical requirement for the GG\nmechanism to reach DP at prespecified privacy parameters, and investigate the\nconnections and differences between the GG mechanism and the Exponential\nmechanism based on the GG distribution We also present a lower bound on the\nscale parameter of the Gaussian mechanism of $(\\epsilon,\\delta)$-probabilistic\nDP as a special case of the GG mechanism, and compare the statistical utility\nof the sanitized results in the tail probability and dispersion in the Gaussian\nand Laplace mechanisms. Lastly, we apply the GG mechanism in 3 experiments (the\nmildew, Czech, adult data), and compare the accuracy of sanitized results via\nthe $l_1$ distance and Kullback-Leibler divergence and examine how sanitization\naffects the prediction power of a classifier constructed with the sanitized\ndata in the adult experiment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:17:46 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 00:08:05 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 01:58:34 GMT"}, {"version": "v4", "created": "Tue, 8 Nov 2016 19:53:52 GMT"}, {"version": "v5", "created": "Wed, 31 May 2017 06:55:25 GMT"}, {"version": "v6", "created": "Sat, 23 Dec 2017 17:12:56 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "1602.06217", "submitter": "Irene Crimaldi", "authors": "Irene Crimaldi, Paolo Dai Pra, Pierre-Yves Louis, Ida Germana Minelli", "title": "Synchronization and functional central limit theorems for interacting\n  reinforced random walks", "comments": "28 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain Central Limit Theorems in Functional form for a class of\ntime-inhomogeneous interacting random walks on the simplex of probability\nmeasures over a finite set. Due to a reinforcement mechanism, the increments of\nthe walks are correlated, forcing their convergence to the same, possibly\nrandom, limit. Random walks of this form have been introduced in the context of\nurn models and in stochastic approximation. We also propose an application to\nopinion dynamics in a random network evolving via preferential attachment. We\nstudy, in particular, random walks interacting through a mean-field rule and\ncompare the rate they converge to their limit with the rate of synchronization,\ni.e. the rate at which their mutual distances converge to zero. Under certain\nconditions, synchronization is faster than convergence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 16:45:39 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 14:14:24 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Crimaldi", "Irene", ""], ["Pra", "Paolo Dai", ""], ["Louis", "Pierre-Yves", ""], ["Minelli", "Ida Germana", ""]]}, {"id": "1602.06276", "submitter": "Milad Kharratzadeh", "authors": "Milad Kharratzadeh, Mark Coates", "title": "Semi-parametric Order-based Generalized Multivariate Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a generalized multivariate regression problem\nwhere the responses are monotonic functions of linear transformations of\npredictors. We propose a semi-parametric algorithm based on the ordering of the\nresponses which is invariant to the functional form of the transformation\nfunction. We prove that our algorithm, which maximizes the rank correlation of\nresponses and linear transformations of predictors, is a consistent estimator\nof the true coefficient matrix. We also identify the rate of convergence and\nshow that the squared estimation error decays with a rate of $o(1/\\sqrt{n})$.\nWe then propose a greedy algorithm to maximize the highly non-smooth objective\nfunction of our model and examine its performance through extensive\nsimulations. Finally, we compare our algorithm with traditional multivariate\nregression algorithms over synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 20:20:56 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Kharratzadeh", "Milad", ""], ["Coates", "Mark", ""]]}, {"id": "1602.06318", "submitter": "Tatyana Krivobokova", "authors": "Katsiaryna Schwarz and Tatyana Krivobokova", "title": "A unified framework for spline estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a unified framework to study the asymptotic properties\nof all periodic spline-based estimators, that is, of regression, penalized and\nsmoothing splines. The explicit form of the periodic Demmler-Reinsch basis in\nterms of exponential splines allows the derivation of an expression for the\nasymptotic equivalent kernel on the real line for all spline estimators\nsimultaneously. The corresponding bandwidth, which drives the asymptotic\nbehavior of spline estimators, is shown to be a function of the number of knots\nand the smoothing parameter. Strategies for the selection of the optimal\nbandwidth and other model parameters are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 21:35:32 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Schwarz", "Katsiaryna", ""], ["Krivobokova", "Tatyana", ""]]}, {"id": "1602.06383", "submitter": "Daniel Gaigall", "authors": "Daniel Gaigall", "title": "Testing hypotheses about mixture distributions using not identically\n  distributed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing hypotheses of goodness-of-fit about mixture distributions on the\nbasis of independent but not necessarily identically distributed random vectors\nis considered. The hypotheses are given by a specific distribution or by a\nfamily of distributions. Moreover, testing hypotheses formulated by Hadamard\ndifferentiable functionals is discussed in this situation, in particular the\nhypothesis of central symmetry, homogeneity and independence.\nKolmogorov-Smirnov or Cram\\'er-von-Mises type statistics are suggested as well\nas methods to determine critical values. The focus of the investigation is on\nasymptotic properties of the test statistics. Further, outcomes of simulations\nfor finite sample sizes are given. Applications to models with not identically\ndistributed errors are presented. The results imply that the tests are of\nasymptotically exact size and consistent.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 08:45:01 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 17:19:31 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Gaigall", "Daniel", ""]]}, {"id": "1602.06410", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Semidefinite Programs for Exact Recovery of a Hidden Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a semidefinite programming (SDP) relaxation of the maximum\nlikelihood estimation for exactly recovering a hidden community of cardinality\n$K$ from an $n \\times n$ symmetric data matrix $A$, where for distinct indices\n$i,j$, $A_{ij} \\sim P$ if $i, j$ are both in the community and $A_{ij} \\sim Q$\notherwise, for two known probability distributions $P$ and $Q$. We identify a\nsufficient condition and a necessary condition for the success of SDP for the\ngeneral model. For both the Bernoulli case ($P={{\\rm Bern}}(p)$ and $Q={{\\rm\nBern}}(q)$ with $p>q$) and the Gaussian case ($P=\\mathcal{N}(\\mu,1)$ and\n$Q=\\mathcal{N}(0,1)$ with $\\mu>0$), which correspond to the problem of planted\ndense subgraph recovery and submatrix localization respectively, the general\nresults lead to the following findings: (1) If $K=\\omega( n /\\log n)$, SDP\nattains the information-theoretic recovery limits with sharp constants; (2) If\n$K=\\Theta(n/\\log n)$, SDP is order-wise optimal, but strictly suboptimal by a\nconstant factor; (3) If $K=o(n/\\log n)$ and $K \\to \\infty$, SDP is order-wise\nsuboptimal. The same critical scaling for $K$ is found to hold, up to constant\nfactors, for the performance of SDP on the stochastic block model of $n$\nvertices partitioned into multiple communities of equal size $K$. A key\ningredient in the proof of the necessary condition is a construction of a\nprimal feasible solution based on random perturbation of the true cluster\nmatrix.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 14:15:59 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 15:54:06 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1602.06606", "submitter": "Igor Melnyk", "authors": "Igor Melnyk and Arindam Banerjee", "title": "Estimating Structured Vector Autoregressive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While considerable advances have been made in estimating high-dimensional\nstructured models from independent data using Lasso-type models, limited\nprogress has been made for settings when the samples are dependent. We consider\nestimating structured VAR (vector auto-regressive models), where the structure\ncan be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted\nLasso, sparse group Lasso, etc. In VAR setting with correlated noise, although\nthere is strong dependence over time and covariates, we establish bounds on the\nnon-asymptotic estimation error of structured VAR parameters. Surprisingly, the\nestimation error is of the same order as that of the corresponding Lasso-type\nestimator with independent samples, and the analysis holds for any norm. Our\nanalysis relies on results in generic chaining, sub-exponential martingales,\nand spectral representation of VAR models. Experimental results on synthetic\ndata with a variety of structures as well as real aviation data are presented,\nvalidating theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 23:47:36 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 22:06:48 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1602.06612", "submitter": "Soledad Villar", "authors": "Dustin G. Mixon, Soledad Villar, Rachel Ward", "title": "Clustering subgaussian mixtures by semidefinite programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-free relax-and-round algorithm for k-means clustering\nbased on a semidefinite relaxation due to Peng and Wei. The algorithm\ninterprets the SDP output as a denoised version of the original data and then\nrounds this output to a hard clustering. We provide a generic method for\nproving performance guarantees for this algorithm, and we analyze the algorithm\nin the context of subgaussian mixture models. We also study the fundamental\nlimits of estimating Gaussian centers by k-means clustering in order to compare\nour approximation guarantee to the theoretically optimal k-means clustering\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:19:20 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 19:38:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1602.06770", "submitter": "Gleb Oshanin", "authors": "O. Benichou, P. L. Krapivsky, C. Mejia-Monasterio and G. Oshanin", "title": "Temporal correlations of the running maximum of a Brownian trajectory", "comments": "5 pages, 5 figures", "journal-ref": "Phys. Rev. Lett. 117, 080601 (2016)", "doi": "10.1103/PhysRevLett.117.080601", "report-no": null, "categories": "cond-mat.stat-mech math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the correlations between the maxima $m$ and $M$ of a Brownian motion\n(BM) on the time intervals $[0,t_1]$ and $[0,t_2]$, with $t_2>t_1$. We\ndetermine exact forms of the distribution functions $P(m,M)$ and $P(G = M -\nm)$, and calculate the moments $\\mathbb{E}\\{\\left(M - m\\right)^k\\}$ and the\ncross-moments $\\mathbb{E}\\{m^l M^k\\}$ with arbitrary integers $l$ and $k$. We\nshow that correlations between $m$ and $M$ decay as $\\sqrt{t_1/t_2}$ when\n$t_2/t_1 \\to \\infty$, revealing strong memory effects in the statistics of the\nBM maxima. We also compute the Pearson correlation coefficient $\\rho(m,M)$, the\npower spectrum of $M_t$, and we discuss a possibility of extracting the\nensemble-averaged diffusion coefficient in single-trajectory experiments using\na single realization of the maximum process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 13:41:26 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 18:24:45 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Benichou", "O.", ""], ["Krapivsky", "P. L.", ""], ["Mejia-Monasterio", "C.", ""], ["Oshanin", "G.", ""]]}, {"id": "1602.06896", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Sharp detection in PCA under correlations: all eigenvalues matter", "comments": "46 pages, 9 figures", "journal-ref": "Ann. Statist. Volume 45, Number 4 (2017), 1810-1833", "doi": "10.1214/16-AOS1514", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used method for dimension\nreduction. In high dimensional data, the \"signal\" eigenvalues corresponding to\nweak principal components (PCs) do not necessarily separate from the bulk of\nthe \"noise\" eigenvalues. Therefore, popular tests based on the largest\neigenvalue have little power to detect weak PCs. In the special case of the\nspiked model, certain tests asymptotically equivalent to linear spectral\nstatistics (LSS)---averaging effects over all eigenvalues---were recently shown\nto achieve some power.\n  We consider a nonparametric, non-Gaussian generalization of the spiked model\nto the setting of Marchenko and Pastur (1967). This allows a general bulk of\nthe noise eigenvalues, accomodating correlated variables even under the null\nhypothesis of no significant PCs.\n  We develop new tests based on LSS to detect weak PCs in this model. We show\nusing the CLT for LSS that the optimal LSS satisfy a Fredholm integral equation\nof the first kind. We develop algorithms to solve it, building on our recent\nmethod for computing the limit empirical spectrum. In contrast to the standard\nspiked model, we find that under \"widely spread\" null eigenvalue distributions,\nthe new tests have a lot of power.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 19:11:28 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "1602.07182", "submitter": "Gilles Stoltz", "authors": "Aur\\'elien Garivier (IMT), Pierre M\\'enard (IMT), Gilles Stoltz\n  (GREGH)", "title": "Explore First, Exploit Next: The True Shape of Regret in Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit lower bounds on the regret in the case of multi-armed bandit\nproblems. We obtain non-asymptotic, distribution-dependent bounds and provide\nstraightforward proofs based only on well-known properties of Kullback-Leibler\ndivergences. These bounds show in particular that in an initial phase the\nregret grows almost linearly, and that the well-known logarithmic growth of the\nregret only holds in a final phase. The proof techniques come to the essence of\nthe information-theoretic arguments used and they are deprived of all\nunnecessary complications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:04:13 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 11:25:23 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 12:25:47 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["M\u00e9nard", "Pierre", "", "IMT"], ["Stoltz", "Gilles", "", "GREGH"]]}, {"id": "1602.07440", "submitter": "Sylvain Delattre Mr", "authors": "Nicolas Fournier and Sylvain Delattre", "title": "On the Kozachenko-Leonenko entropy estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in details the bias and variance of the entropy estimator proposed\nby Kozachenko and Leonenko for a large class of densities on $\\mathbb{R}^d$. We\nthen use the work of Bickel and Breiman to prove a central limit theorem in\ndimensions $1$ and $2$. In higher dimensions, we provide a development of the\nbias in terms of powers of $N^{-2/d}$. This allows us to use a Richardson\nextrapolation to build, in any dimension, an estimator satisfying a central\nlimit theorem and for which we can give some some explicit (asymptotic)\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 09:06:00 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Fournier", "Nicolas", ""], ["Delattre", "Sylvain", ""]]}, {"id": "1602.07586", "submitter": "Edward J. Green", "authors": "Fatemeh Borhani and Edward J. Green", "title": "A parsimonious theory of evidence-based choice", "comments": "33 pages. Exposition has been thoroughly revised. To afford more\n  simple proofs, the weakly-more-specific-than relation is now assumed to be\n  antisymmetric", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  That an agent's possible evidential states form a Boolean algebra (on which\nit is natural to define a probability measure) is an assertion that ideally\nshould be proved, rather than assumed, in justifying rational choice as a\nrepresentation of expected utility. A more parsimonious, axiomatic\ncharacterization of evidence is provided here. Two primitive entities are\nevidential states and a relation, more specific than, between evidential\nstates. The axioms specify that more-specific-than is a partial order, there is\na minimally specific e-state, and more-specific-than is a separative order.\nChoice alternatives are another primitive entity. A plan is an assignment of a\nchoice alternative to each evidential state. In general, plans satisfying a\nversion of the sure-thing principle cannot be rationalized by expected utility.\nBut there is such a rationalization if the evidential structure is a tree.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 16:37:15 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 17:34:47 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 19:24:37 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Borhani", "Fatemeh", ""], ["Green", "Edward J.", ""]]}, {"id": "1602.07666", "submitter": "Felix Nagel", "authors": "Felix Nagel", "title": "Swap-invariant and exchangeable random measures", "comments": "30 pages; v2: variant of ergodic theorem and example added, minor\n  changes in text; v3: structure changed, theorems slightly improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyze the concept of swap-invariance, which is a weaker\nvariant of exchangeability. A random vector $\\xi$ in $\\mathbb{R}^n$ is called\nswap-invariant if $\\,{\\mathbf E}\\,\\big| \\!\\sum_j u_j \\xi_j \\big|\\,$ is\ninvariant under all permutations of $(\\xi_1, \\ldots, \\xi_n)$ for each $u \\in\n\\mathbb{R}^n$. We extend this notion to random measures. For a swap-invariant\nrandom measure $\\xi$ on a measure space $(S,\\mathcal{S},\\mu)$ the vector\n$(\\xi(A_1), \\ldots, \\xi(A_n))$ is swap-invariant for all disjoint $A_j \\in\n\\mathcal{S}$ with equal $\\mu$-measure. Various characterizations of\nswap-invariant random measures and connections to exchangeable ones are\nestablished. We prove the ergodic theorem for swap-invariant random measures\nand derive a representation in terms of the ergodic limit and an exchangeable\nrandom measure. Moreover we show that diffuse swap-invariant random measures on\na Borel space are trivial. As for random sequences two new representations are\nobtained using different ergodic limits.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 20:29:28 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2016 10:48:38 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 16:07:23 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Nagel", "Felix", ""]]}, {"id": "1602.08048", "submitter": "John Lafferty", "authors": "Sabyasachi Chatterjee and John Lafferty", "title": "Denoising Flows on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of flows on trees, a structured generalization of\nisotonic regression. A tree flow is defined recursively as a positive flow\nvalue into a node that is partitioned into an outgoing flow to the children\nnodes, with some amount of the flow possibly leaking outside. We study the\nbehavior of the least squares estimator for flows, and the associated minimax\nlower bounds. We characterize the risk of the least squares estimator in two\nregimes. In the first regime the diameter of the tree grows at most\nlogarithmically with the number of nodes. In the second regime, the tree\ncontains long paths. The results are compared with known risk bounds for\nisotonic regression.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 19:27:39 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 19:23:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Chatterjee", "Sabyasachi", ""], ["Lafferty", "John", ""]]}, {"id": "1602.08062", "submitter": "Junxian Geng", "authors": "Junxian Geng and Anirban Bhattacharya and Debdeep Pati", "title": "Probabilistic community detection with unknown number of communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in network analysis is clustering the nodes into groups\nwhich share a similar connectivity pattern. Existing algorithms for community\ndetection assume the knowledge of the number of clusters or estimate it a\npriori using various selection criteria and subsequently estimate the community\nstructure. Ignoring the uncertainty in the first stage may lead to erroneous\nclustering, particularly when the community structure is vague. We instead\npropose a coherent probabilistic framework for simultaneous estimation of the\nnumber of communities and the community structure, adapting recently developed\nBayesian nonparametric techniques to network models. An efficient Markov chain\nMonte Carlo (MCMC) algorithm is proposed which obviates the need to perform\nreversible jump MCMC on the number of clusters. The methodology is shown to\noutperform recently developed community detection algorithms in a variety of\nsynthetic data examples and in benchmark real-datasets. Using an appropriate\nmetric on the space of all configurations, we develop non-asymptotic Bayes risk\nbounds even when the number of clusters is unknown. Enroute, we develop\nconcentration properties of non-linear functions of Bernoulli random variables,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 20:01:22 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:16:26 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 00:24:39 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 02:40:19 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Geng", "Junxian", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1602.08181", "submitter": "Natesh Pillai", "authors": "Natesh S. Pillai", "title": "Ratios and Cauchy Distribution", "comments": "Generalization of a recent conjecture on Cauchy distribution; added a\n  Remark and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the ratio of two independent standard Gaussian random\nvariables follows a Cauchy distribution. Any convex combination of independent\nstandard Cauchy random variables also follows a Cauchy distribution. In a\nrecent joint work, the author proved a surprising multivariate generalization\nof the above facts. Fix $m > 1$ and let $\\Sigma$ be a $m\\times m$ positive\nsemi-definite matrix. Let $X,Y \\sim \\mathrm{N}(0,\\Sigma)$ be independent\nvectors. Let $\\vec{w}=(w_1, \\dots, w_m)$ be a vector of non-negative numbers\nwith $\\sum_{j=1}^m w_j = 1.$ The author proved recently that the random\nvariable $$ Z = \\sum_{j=1}^m w_j\\frac{X_j}{Y_j}\\; $$ also has the standard\nCauchy distribution. In this note, we provide some more understanding of this\nresult and give a number of natural generalizations. In particular, we observe\nthat if $(X,Y)$ have the same marginal distribution, they need neither be\nindependent nor be jointly normal for $Z$ to be Cauchy distributed. In fact,\nour calculations suggest that joint normality of $(X,Y)$ may be the only\ninstance in which they can be independent. Our results also give a method to\nconstruct copulas of Cauchy distributions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 02:47:13 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 20:32:38 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Pillai", "Natesh S.", ""]]}, {"id": "1602.08296", "submitter": "Valentina Cammarota", "authors": "Dan Cheng, Valentina Cammarota, Yabebal Fantaye, Domenico Marinucci,\n  Armin Schwartzman", "title": "Multiple testing of local maxima for detection of peaks on the\n  (celestial) sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a topological multiple testing scheme for detecting peaks on the\nsphere under isotropic Gaussian noise, where tests are performed at local\nmaxima of the observed field filtered by the spherical needlet transform. Our\nsetting is different from the standard Euclidean/large same asymptotic\nframework, yet highly relevant to realistic experimental circumstances for some\nimportant areas of application in astronomy. More precisely, we focus on cases\nwhere a single realization of a smooth isotropic Gaussian random field on the\nsphere is observed, and a number of well-localized signals are superimposed on\nsuch background field. The proposed algorithms, combined with the\nBenjamini-Hochberg procedure for thresholding p-values, provide asymptotic\nstrong control of the False Discovery Rate (FDR) and power consistency as the\nsignal strength and the frequency of the needlet transform get large. This\nnovel multiple testing method is illustrated in a simulation of point-source\ndetection in Cosmic Microwave Background radiation (CMB) data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 12:44:24 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:52:36 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Cheng", "Dan", ""], ["Cammarota", "Valentina", ""], ["Fantaye", "Yabebal", ""], ["Marinucci", "Domenico", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1602.08307", "submitter": "Dimitra Kosta", "authors": "Dimitra Kosta", "title": "A note on Maximum Likelihood Estimation for cubic and quartic canonical\n  toric del Pezzo Surfaces", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the study of toric algebraic statistical models which\ncorrespond to toric Del Pezzo surfaces with Du Val singularities. A closed-form\nfor the Maximum Likelihood Estimate of algebraic statistical models which\ncorrespond to cubic and quartic toric Del Pezzo surfaces with Du Val singular\npoints is given. Also, we calculate the ML degrees of some toric Del Pezzo\nsurfaces of degree less than or equal to six, which equals the degree of the\nsurface in all the case but one, namely the quintic with two points of type\n$\\mathbb{A}_1$.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 13:11:34 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 13:29:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kosta", "Dimitra", ""]]}, {"id": "1602.08529", "submitter": "David Gamarnik", "authors": "David Gamarnik and Quan Li", "title": "Finding a Large Submatrix of a Gaussian Random Matrix", "comments": "38 pages 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding a $k\\times k$ submatrix of an $n\\times n$\nmatrix with i.i.d. standard Gaussian entries, which has a large average entry.\nIt was shown earlier by Bhamidi et al. that the largest average value of such a\nmatrix is $2\\sqrt{\\log n/k}$ with high probability. In the same paper an\nevidence was provided that a natural greedy algorithm called Largest Average\nSubmatrix ($\\LAS$) should produce a matrix with average entry approximately\n$\\sqrt{2}$ smaller.\n  In this paper we show that the matrix produced by the $\\LAS$ algorithm is\nindeed $\\sqrt{2\\log n/k}$ w.h.p. Then by drawing an analogy with the problem of\nfinding cliques in random graphs, we propose a simple greedy algorithm which\nproduces a $k\\times k$ matrix with asymptotically the same average value. Since\nthe greedy algorithm is the best known algorithm for finding cliques in random\ngraphs, it is tempting to believe that beating the factor $\\sqrt{2}$\nperformance gap suffered by both algorithms might be very challenging.\nSurprisingly, we show the existence of a very simple algorithm which produces a\nmatrix with average value $(4/3)\\sqrt{2\\log n/k}$.\n  To get an insight into the algorithmic hardness of this problem, and\nmotivated by methods originating in the theory of spin glasses, we conduct the\nso-called expected overlap analysis of matrices with average value\nasymptotically $\\alpha\\sqrt{2\\log n/k}$. The overlap corresponds to the number\nof common rows and common columns for pairs of matrices achieving this value.\nWe discover numerically an intriguing phase transition at $\\alpha^*\\approx\n1.3608..$: when $\\alpha<\\alpha^*$ the space of overlaps is a continuous subset\nof $[0,1]^2$, whereas $\\alpha=\\alpha^*$ marks the onset of discontinuity, and\nthe model exhibits the Overlap Gap Property when $\\alpha>\\alpha^*$. We\nconjecture that $\\alpha>\\alpha^*$ marks the onset of the algorithmic hardness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 23:08:56 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Gamarnik", "David", ""], ["Li", "Quan", ""]]}, {"id": "1602.08558", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty and Kshitij Khare (Department of Statistics,\n  University of Florida)", "title": "Convergence properties of Gibbs samplers for Bayesian probit regression\n  with proper priors", "comments": "40 pages, 6 figures; typos corrected", "journal-ref": "Electron. J. Statist. 11, no. 1, 177--210 (2017)", "doi": "10.1214/16-EJS1219", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian probit regression model (Albert and Chib (1993)) is popular and\nwidely used for binary regression. While the improper flat prior for the\nregression coefficients is an appropriate choice in the absence of any prior\ninformation, a proper normal prior is desirable when prior information is\navailable or in modern high dimensional settings where the number of\ncoefficients ($p$) is greater than the sample size ($n$). For both choices of\npriors, the resulting posterior density is intractable and a Data Dugmentation\n(DA) Markov chain is used to generate approximate samples from the posterior\ndistribution. Establishing geometric ergodicity for this DA Markov chain is\nimportant as it provides theoretical guarantees for constructing standard\nerrors for Markov chain based estimates of posterior quantities. In this paper,\nwe first show that in case of proper normal priors, the DA Markov chain is\ngeometrically ergodic *for all* choices of the design matrix $X$, $n$ and $p$\n(unlike the improper prior case, where $n \\geq p$ and another condition on $X$\nare required for posterior propriety itself). We also derive sufficient\nconditions under which the DA Markov chain is trace-class, i.e., the\neigenvalues of the corresponding operator are summable. In particular, this\nallows us to conclude that the Haar PX-DA sandwich algorithm (obtained by\ninserting an inexpensive extra step in between the two steps of the DA\nalgorithm) is strictly better than the DA algorithm in an appropriate sense.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 05:52:23 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 18:33:08 GMT"}, {"version": "v3", "created": "Sat, 5 Mar 2016 13:49:19 GMT"}, {"version": "v4", "created": "Tue, 3 Jan 2017 04:48:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Chakraborty", "Saptarshi", "", "Department of Statistics,\n  University of Florida"], ["Khare", "Kshitij", "", "Department of Statistics,\n  University of Florida"]]}, {"id": "1602.08595", "submitter": "Felix Lucka", "authors": "Felix Lucka", "title": "Fast Gibbs sampling for high-dimensional Bayesian inversion", "comments": "submitted to \"Inverse Problems\"", "journal-ref": null, "doi": "10.1088/0266-5611/32/11/115019", "report-no": null, "categories": "math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving ill-posed inverse problems by Bayesian inference has recently\nattracted considerable attention. Compared to deterministic approaches, the\nprobabilistic representation of the solution by the posterior distribution can\nbe exploited to explore and quantify its uncertainties. In applications where\nthe inverse solution is subject to further analysis procedures, this can be a\nsignificant advantage. Alongside theoretical progress, various new\ncomputational techniques allow to sample very high dimensional posterior\ndistributions: In [Lucka2012], a Markov chain Monte Carlo (MCMC) posterior\nsampler was developed for linear inverse problems with $\\ell_1$-type priors. In\nthis article, we extend this single component Gibbs-type sampler to a wide\nrange of priors used in Bayesian inversion, such as general $\\ell_p^q$ priors\nwith additional hard constraints. Besides a fast computation of the\nconditional, single component densities in an explicit, parameterized form, a\nfast, robust and exact sampling from these one-dimensional densities is key to\nobtain an efficient algorithm. We demonstrate that a generalization of slice\nsampling can utilize their specific structure for this task and illustrate the\nperformance of the resulting slice-within-Gibbs samplers by different computed\nexamples. These new samplers allow us to perform sample-based Bayesian\ninference in high-dimensional scenarios with certain priors for the first time,\nincluding the inversion of computed tomography (CT) data with the popular\nisotropic total variation (TV) prior.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 14:31:58 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 13:36:33 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lucka", "Felix", ""]]}, {"id": "1602.08654", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop and William Kengne", "title": "Testing for parameter change in general integer-valued time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the structural change in a class of discrete valued time series\nthat the conditional distribution follows a one-parameter exponential family.\nWe propose a change-point test based on the maximum likelihood estimator of the\nparameter of the model. Under the null hypothesis (of no change), the test\nstatistics converges to a well known distribution, allowing for the calculation\nof the critical values of the test. The test statistic diverges to infinity\nunder the alternative, that is, the test asymptotically has power one. Some\nsimulation results and real data applications are reported to show the\napplicability of the test procedure.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 01:07:22 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "1602.08770", "submitter": "Esra Russell Dr.", "authors": "Jean-Renaud Pycke and Esra Russell (New York University Abu Dhabi)", "title": "New Statistical Perspective to The Cosmic Void Distribution", "comments": "Accepted in ApJ main journal, 7 pages, 3 figures", "journal-ref": null, "doi": "10.3847/0004-637X/821/2/110", "report-no": null, "categories": "astro-ph.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we obtain the size distribution of voids as a 3-parameter\nredshift independent log-normal void probability function (VPF) directly from\nthe Cosmic Void Catalog (CVC). Although many statistical models of void\ndistributions are based on the counts in randomly placed cells, the log-normal\nVPF that we here obtain is independent of the shape of the voids due to the\nparameter-free void finder of the CVC. We use three void populations drawn from\nthe CVC generated by the Halo Occupation Distribution (HOD) Mocks which are\ntuned to three mock SDSS samples to investigate the void distribution\nstatistically and the effects of the environments on the size distribution. As\na result, it is shown that void size distributions obtained from the HOD Mock\nsamples are satisfied by the 3-parameter log-normal distribution. In addition,\nwe find that there may be a relation between hierarchical formation, skewness\nand kurtosis of the log-normal distribution for each catalog. We also show that\nthe shape of the 3-parameter distribution from the samples is strikingly\nsimilar to the galaxy log-normal mass distribution obtained from numerical\nstudies. This similarity of void size and galaxy mass distributions may\npossibly indicate evidence of nonlinear mechanisms affecting both voids and\ngalaxies, such as large scale accretion and tidal effects. Considering in this\nstudy all voids are generated by galaxy mocks and show hierarchical structures\nin different levels, it may be possible that the same nonlinear mechanisms of\nmass distribution affect the void size distribution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 21:10:18 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Pycke", "Jean-Renaud", "", "New York University Abu Dhabi"], ["Russell", "Esra", "", "New York University Abu Dhabi"]]}, {"id": "1602.08780", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Does quantification without adjustments work?", "comments": "20 pages, 2 figures, major update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is the task of predicting the class labels of objects based on\nthe observation of their features. In contrast, quantification has been defined\nas the task of determining the prevalences of the different sorts of class\nlabels in a target dataset. The simplest approach to quantification is Classify\n& Count where a classifier is optimised for classification on a training set\nand applied to the target dataset for the prediction of class labels. In the\ncase of binary quantification, the number of predicted positive labels is then\nused as an estimate of the prevalence of the positive class in the target\ndataset. Since the performance of Classify & Count for quantification is known\nto be inferior its results typically are subject to adjustments. However, some\nresearchers recently have suggested that Classify & Count might actually work\nwithout adjustments if it is based on a classifer that was specifically trained\nfor quantification. We discuss the theoretical foundation for this claim and\nexplore its potential and limitations with a numerical example based on the\nbinormal model with equal variances. In order to identify an optimal quantifier\nin the binormal setting, we introduce the concept of local Bayes optimality. As\na side remark, we present a complete proof of a theorem by Ye et al. (2012).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 22:29:25 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 16:24:05 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1602.08793", "submitter": "Meng Li", "authors": "Meng Li, Kehui Wang, Arnab Maity and Ana-Maria Staicu", "title": "Inference in Functional Linear Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical inference in functional quantile\nregression for scalar response and a functional covariate. Specifically, we\nconsider a functional linear quantile regression model where the effect of the\ncovariate on the quantile of the response is modeled through the inner product\nbetween the functional covariate and an unknown smooth regression parameter\nfunction that varies with the level of quantile. The objective is to test that\nthe regression parameter is constant across several quantile levels of\ninterest. The parameter function is estimated by combining ideas from\nfunctional principal component analysis and quantile regression. An adjusted\nWald testing procedure is proposed for this hypothesis of interest, and its\nchi-square asymptotic null distribution is derived. The testing procedure is\ninvestigated numerically in simulations involving sparse and noisy functional\ncovariates and in a capital bike share data application. The proposed approach\nis easy to implement and the {\\tt R} code is published online at\n\\url{https://github.com/xylimeng/fQR-testing}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 01:09:26 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 02:36:47 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Li", "Meng", ""], ["Wang", "Kehui", ""], ["Maity", "Arnab", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1602.08927", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "High-Dimensional $L_2$Boosting: Rate of Convergence", "comments": "19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,\n  62J07, 41A25; secondary 49M15, 68Q32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 12:05:53 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 14:35:38 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1602.09100", "submitter": "Libo Wang", "authors": "Libo Wang, Yuanyuan Tang, Debajyoti Sinha, Debdeep Pati, and Stuart\n  Lipsitz", "title": "Bayesian Variable Selection for Skewed Heteroscedastic Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose new Bayesian methods for selecting and estimating\na sparse coefficient vector for skewed heteroscedastic response. Our novel\nBayesian procedures effectively estimate the median and other quantile\nfunctions, accommodate non-local prior for regression effects without\ncompromising ease of implementation via sampling based tools, and\nasymptotically select the true set of predictors even when the number of\ncovariates increases in the same order of the sample size. We also extend our\nmethod to deal with some observations with very large errors. Via simulation\nstudies and a re-analysis of a medical cost study with large number of\npotential predictors, we illustrate the ease of implementation and other\npractical advantages of our approach compared to existing methods for such\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 19:02:51 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 07:17:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Wang", "Libo", ""], ["Tang", "Yuanyuan", ""], ["Sinha", "Debajyoti", ""], ["Pati", "Debdeep", ""], ["Lipsitz", "Stuart", ""]]}, {"id": "1602.09120", "submitter": "Peter Gerstoft", "authors": "Peter Gerstoft, Christoph F. Mecklenbr\\\"auker, Angeliki Xenaki", "title": "Multi Snapshot Sparse Bayesian Learning for DOA Estimation", "comments": "submitted to Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2016.2598550", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The directions of arrival (DOA) of plane waves are estimated from\nmulti-snapshot sensor array data using Sparse Bayesian Learning (SBL). The\nprior source amplitudes is assumed independent zero-mean complex Gaussian\ndistributed with hyperparameters the unknown variances (i.e. the source\npowers). For a complex Gaussian likelihood with hyperparameter the unknown\nnoise variance, the corresponding Gaussian posterior distribution is derived.\nFor a given number of DOAs, the hyperparameters are automatically selected by\nmaximizing the evidence and promote sparse DOA estimates. The SBL scheme for\nDOA estimation is discussed and evaluated competitively against LASSO\n($\\ell_1$-regularization), conventional beamforming, and MUSIC\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 20:02:13 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Gerstoft", "Peter", ""], ["Mecklenbr\u00e4uker", "Christoph F.", ""], ["Xenaki", "Angeliki", ""]]}]