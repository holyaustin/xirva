[{"id": "1908.00414", "submitter": "Xiye Yang", "authors": "Jungjun Choi and Xiye Yang", "title": "Bias Correction and Robust Inference in Semiparametric Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes several different biases that emerge from the (possibly)\nlow-precision nonparametric ingredient in a semiparametric model. We show that\nboth the variance part and the bias part of the nonparametric ingredient can\nlead to some biases in the semiparametric estimator, under conditions weaker\nthan typically required in the literature. We then propose two bias-robust\ninference procedures, based on multi-scale jackknife and analytical bias\ncorrection, respectively. We also extend our framework to the case where the\nsemiparametric estimator is constructed by some discontinuous functionals of\nthe nonparametric ingredient. Simulation study shows that both bias-correction\nmethods have good finite-sample performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:09:36 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 14:50:13 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 15:15:48 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Choi", "Jungjun", ""], ["Yang", "Xiye", ""]]}, {"id": "1908.00428", "submitter": "Jan Vrbik", "authors": "Jan Vrbik", "title": "General proof of a limit related to AR(k) model of Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing moments of various parameter estimators related to an\nautoregressive model of Statistics, one needs to evaluate several non-trivial\nlimits. This was done by arXiv:1506.03131 for the case of two, three and four\ndimensions; in this article, we present a proof of a fully general formula,\nbased on an ingenious solution of\nhttps://mathoverflow.net/users/4312/fedor-petrov.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:19:39 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Vrbik", "Jan", ""]]}, {"id": "1908.00583", "submitter": "Yusi Fang", "authors": "Yusi Fang, Shaowu Tang, Zhiguang Huo, George C. Tseng and Yongseok\n  Park", "title": "Properties of adaptively weighted Fisher's method", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is a statistical method to combine results from multiple\nclinical or genomic studies with the same or similar research problems. It has\nbeen widely use to increase statistical power in finding clinical or genomic\ndifferences among different groups. One major category of meta-analysis is\ncombining p-values from independent studies and the Fisher's method is one of\nthe most commonly used statistical methods. However, due to heterogeneity of\nstudies, particularly in the field of genomic research, with thousands of\nfeatures such as genes to assess, researches often desire to discover this\nheterogeneous information when identify differentially expressed genomic\nfeatures. To address this problem, \\citet{Li2011adaptively} proposed very\ninteresting statistical method, adaptively weighted (AW) Fisher's method, where\nbinary weights, $0$ or $1$, are assigned to the studies for each feature to\ndistinguish potential zero or none-zero effect sizes. \\citet{Li2011adaptively}\nhas shown some good properties of AW fisher's method such as the admissibility.\nIn this paper, we further explore some asymptotic properties of AW-Fisher's\nmethod including consistency of the adaptive weights and the asymptotic Bahadur\noptimality of the test.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 19:04:56 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Fang", "Yusi", ""], ["Tang", "Shaowu", ""], ["Huo", "Zhiguang", ""], ["Tseng", "George C.", ""], ["Park", "Yongseok", ""]]}, {"id": "1908.00828", "submitter": "Thibaut Le Gouic", "authors": "Thibaut Le Gouic, Quentin Paris, Philippe Rigollet, Austin J. Stromme", "title": "Fast convergence of empirical barycenters in Alexandrov spaces and the\n  Wasserstein space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.MG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work establishes fast rates of convergence for empirical barycenters\nover a large class of geodesic spaces with curvature bounds in the sense of\nAlexandrov. More specifically, we show that parametric rates of convergence are\nachievable under natural conditions that characterize the bi-extendibility of\ngeodesics emanating from a barycenter. These results largely advance the\nstate-of-the-art on the subject both in terms of rates of convergence and the\nvariety of spaces covered. In particular, our results apply to\ninfinite-dimensional spaces such as the 2-Wasserstein space, where\nbi-extendibility of geodesics translates into regularity of Kantorovich\npotentials.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 12:46:08 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 22:31:20 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 23:47:02 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 20:10:45 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gouic", "Thibaut Le", ""], ["Paris", "Quentin", ""], ["Rigollet", "Philippe", ""], ["Stromme", "Austin J.", ""]]}, {"id": "1908.00845", "submitter": "Lionel Truquet", "authors": "Max Zinsou Debaly and Lionel Truquet", "title": "Iterations of dependent random maps and exogeneity in nonlinear dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss existence and uniqueness of stationary and ergodic nonlinear\nautoregressive processes when exogenous regressors are incorporated in the\ndynamic. To this end, we consider the convergence of the backward iterations of\ndependent random maps. In particular, we give a new result when the classical\ncondition of contraction on average is replaced with a contraction in\nconditional expectation. Under some conditions, we also derive an explicit\ncontrol of the functional dependence of Wu (2005) which guarantees a wide range\nof statistical applications. Our results are illustrated with CHARN models,\nGARCH processes, count time series, binary choice models and categorical time\nseries for which we provide many extensions of existing results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:29:57 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 08:09:05 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 12:22:44 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Debaly", "Max Zinsou", ""], ["Truquet", "Lionel", ""]]}, {"id": "1908.00947", "submitter": "Kevin Vanslette", "authors": "Kevin Vanslette, Abdullatif Al Alsheikh, and Kamal Youcef-Toumi", "title": "Why Simple Quadrature is just as good as Monte Carlo", "comments": null, "journal-ref": null, "doi": "10.1515/mcma-2020-2055", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motive and calculate Newton--Cotes quadrature integration variance and\ncompare it directly with Monte Carlo (MC) integration variance. We find an\nequivalence between deterministic quadrature sampling and random MC sampling by\nnoting that MC random sampling is statistically indistinguishable from a method\nthat uses deterministic sampling on a randomly shuffled (permuted) function. We\nuse this statistical equivalence to regularize the form of permissible Bayesian\nquadrature integration priors such that they are guaranteed to be objectively\ncomparable with MC. This leads to the proof that simple quadrature methods have\nexpected variances that are less than or equal to their corresponding\ntheoretical MC integration variances. Separately, using Bayesian probability\ntheory, we find that the theoretical standard deviations of the unbiased errors\nof simple Newton--Cotes composite quadrature integrations improve over their\nworst case errors by an extra dimension independent factor $\\propto N^{-1/2}$.\nThis dimension independent factor is validated in our simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:38:39 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 15:02:26 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 14:16:55 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Vanslette", "Kevin", ""], ["Alsheikh", "Abdullatif Al", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1908.01034", "submitter": "Vasilis Kontonis", "authors": "Vasilis Kontonis, Christos Tzamos, Manolis Zampetakis", "title": "Efficient Truncated Statistics with Unknown Truncation", "comments": "to appear at 60th Annual IEEE Symposium on Foundations of Computer\n  Science (FOCS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Gaussian distribution\nwhen samples are only shown if they fall in some (unknown) subset $S \\subseteq\n\\R^d$. This core problem in truncated statistics has long history going back to\nGalton, Lee, Pearson and Fisher. Recent work by Daskalakis et al. (FOCS'18),\nprovides the first efficient algorithm that works for arbitrary sets in high\ndimension when the set is known, but leaves as an open problem the more\nchallenging and relevant case of unknown truncation set.\n  Our main result is a computationally and sample efficient algorithm for\nestimating the parameters of the Gaussian under arbitrary unknown truncation\nsets whose performance decays with a natural measure of complexity of the set,\nnamely its Gaussian surface area. Notably, this algorithm works for large\nfamilies of sets including intersections of halfspaces, polynomial threshold\nfunctions and general convex sets. We show that our algorithm closely captures\nthe tradeoff between the complexity of the set and the number of samples needed\nto learn the parameters by exhibiting a set with small Gaussian surface area\nfor which it is information theoretically impossible to learn the true Gaussian\nwith few samples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 20:05:52 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1908.01131", "submitter": "Changqing Xu", "authors": "Changqing Xu and Ziming Zhang", "title": "Random Tensors and their Normal Distributions", "comments": "29 pages, addressed at 2019 international conference on Matrices and\n  Operators, Chern Institute of Mathematics, Nankai Univ., Tianjin, China, July\n  24, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to introduce the random tensor with normal\ndistribution, which promotes the matrix normal distribution to a higher order\ncase. Some basic knowledge on tensors are introduced before we focus on the\nrandom tensors whose entries follow normal distribution. The random tensor with\nstandard normal distribution(SND) is introduced as an extension of random\nnormal matrices. As a random multi-array deduced from an affine transformation\non a SND tensor, the general normal random tensor is initialised in the paper.\nWe then investigate some equivalent definitions of a normal tensor and present\nthe description of the density function, characteristic function, moments, and\nsome other functions related to a random matrix. A general form of an\neven-order multi-variance tensor is also introduced to tackle a random tensor.\nAlso presented are some equivalent definitions for the tensor normal\ndistribution. We initialize the definition of high order standard Gaussian\ntensors, general Gaussian tensors, deduce some properties and their\ncharacteristic functions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 07:58:19 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 14:59:06 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xu", "Changqing", ""], ["Zhang", "Ziming", ""]]}, {"id": "1908.01162", "submitter": "Mikhail Zhitlukhin", "authors": "Alexey Muravlev, Mikhail Urusov, Mikhail Zhitlukhin", "title": "Sequential tracking of an unobservable two-state Markov process under\n  Brownian noise", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an optimal control problem, where a Brownian motion with drift is\nsequentially observed, and the sign of the drift coefficient changes at jump\ntimes of a symmetric two-state Markov process. The Markov process itself is not\nobservable, and the problem consist in finding a {-1,1}-valued process that\ntracks the unobservable process as close as possible. We present an explicit\nconstruction of such a process.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 12:45:37 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Muravlev", "Alexey", ""], ["Urusov", "Mikhail", ""], ["Zhitlukhin", "Mikhail", ""]]}, {"id": "1908.01251", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, Suofei Wu, Thomas C. M. Lee", "title": "Measuring the Algorithmic Convergence of Randomized Ensembles: The\n  Regression Setting", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When randomized ensemble methods such as bagging and random forests are\nimplemented, a basic question arises: Is the ensemble large enough? In\nparticular, the practitioner desires a rigorous guarantee that a given ensemble\nwill perform nearly as well as an ideal infinite ensemble (trained on the same\ndata). The purpose of the current paper is to develop a bootstrap method for\nsolving this problem in the context of regression --- which complements our\ncompanion paper in the context of classification (Lopes 2019). In contrast to\nthe classification setting, the current paper shows that theoretical guarantees\nfor the proposed bootstrap can be established under much weaker assumptions. In\naddition, we illustrate the flexibility of the method by showing how it can be\nadapted to measure algorithmic convergence for variable selection. Lastly, we\nprovide numerical results demonstrating that the method works well in a range\nof situations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 00:45:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lopes", "Miles E.", ""], ["Wu", "Suofei", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1908.01252", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Liao", "title": "Learning Latent Factors from Diversified Projections and its\n  Applications to Over-Estimated and Weak Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimations and applications of factor models often rely on the crucial\ncondition that the number of latent factors is consistently estimated, which in\nturn also requires that factors be relatively strong, data are stationary and\nweak serial dependence, and the sample size be fairly large, although in\npractical applications, one or several of these conditions may fail. In these\ncases it is difficult to analyze the eigenvectors of the data matrix. To\naddress this issue, we propose simple estimators of the latent factors using\ncross-sectional projections of the panel data, by weighted averages with\npre-determined weights. These weights are chosen to diversify away the\nidiosyncratic components, resulting in \"diversified factors\". Because the\nprojections are conducted cross-sectionally, they are robust to serial\nconditions, easy to analyze and work even for finite length of time series. We\nformally prove that this procedure is robust to over-estimating the number of\nfactors, and illustrate it in several applications, including post-selection\ninference, big data forecasts, large covariance estimation and factor\nspecification tests. We also recommend several choices for the diversified\nweights.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 01:09:01 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 21:41:46 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 00:51:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""]]}, {"id": "1908.01253", "submitter": "Fei Wang", "authors": "Fei Wang, Ling Zhou, Lu Tang, and Peter X.-K. Song", "title": "Method of Contraction-Expansion (MOCE) for Simultaneous Inference in\n  Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous inference after model selection is of critical importance to\naddress scientific hypotheses involving a set of parameters. In this paper, we\nconsider high-dimensional linear regression model in which a regularization\nprocedure such as LASSO is applied to yield a sparse model. To establish a\nsimultaneous post-model selection inference, we propose a method of contraction\nand expansion (MOCE) along the line of debiasing estimation that enables us to\nbalance the bias-and-variance trade-off so that the super-sparsity assumption\nmay be relaxed. We establish key theoretical results for the proposed MOCE\nprocedure from which the expanded model can be selected with theoretical\nguarantees and simultaneous confidence regions can be constructed by the joint\nasymptotic normal distribution. In comparison with existing methods, our\nproposed method exhibits stable and reliable coverage at a nominal significance\nlevel with substantially less computational burden, and thus it is trustworthy\nfor its application in solving real-world problems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 01:35:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Wang", "Fei", ""], ["Zhou", "Ling", ""], ["Tang", "Lu", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "1908.01260", "submitter": "Pengfei Li", "authors": "Yukun Liu, Pengfei Li, and Jing Qin", "title": "Full-semiparametric-likelihood-based inference for non-ignorable missing\n  data", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past few decades, missing-data problems have been studied\nextensively, with a focus on the ignorable missing case, where the missing\nprobability depends only on observable quantities. By contrast, research into\nnon-ignorable missing data problems is quite limited. The main difficulty in\nsolving such problems is that the missing probability and the regression\nlikelihood function are tangled together in the likelihood presentation, and\nthe model parameters may not be identifiable even under strong parametric model\nassumptions. In this paper we discuss a semiparametric model for non-ignorable\nmissing data and propose a maximum full semiparametric likelihood estimation\nmethod, which is an efficient combination of the parametric conditional\nlikelihood and the marginal nonparametric biased sampling likelihood. The extra\nmarginal likelihood contribution can not only produce efficiency gain but also\nidentify the underlying model parameters without additional assumptions. We\nfurther show that the proposed estimators for the underlying parameters and the\nresponse mean are semiparametrically efficient. Extensive simulations and a\nreal data analysis demonstrate the advantage of the proposed method over\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 02:30:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Yukun", ""], ["Li", "Pengfei", ""], ["Qin", "Jing", ""]]}, {"id": "1908.01388", "submitter": "Cheuk Ting Li", "authors": "Cheuk Ting Li and Venkat Anantharam", "title": "Pairwise Multi-marginal Optimal Transport and Embedding for Earth\n  Mover's Distance", "comments": "91 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of pairwise multi-marginal optimal transport, that\nis, given a collection of probability distributions $\\{P_\\alpha\\}$ on a Polish\nspace $\\mathcal{X}$, to find a coupling $\\{X_\\alpha\\}$, $X_\\alpha\\sim\nP_\\alpha$, such that $\\mathbf{E}[c(X_\\alpha,X_\\beta)]\\le r\\inf_{X\\sim\nP_\\alpha,Y\\sim P_\\beta}\\mathbf{E}[c(X,Y)]$ for all $\\alpha,\\beta$, where $c$ is\na cost function and $r\\ge1$. In other words, every pair $(X_\\alpha,X_\\beta)$\nhas an expected cost at most a factor of $r$ from its lowest possible value.\nThis can be regarded as a locality sensitive hash function for probability\ndistributions, and has applications such as robust and distributed computation\nof transport plans. It can also be considered as a bi-Lipschitz embedding of\nthe collection of probability distributions into the space of random variables\ntaking values on $\\mathcal{X}$. For $c(x,y)=\\Vert x-y\\Vert_2^q$ on\n$\\mathbb{R}^n$, where $q>0$, we show that a finite $r$ is attainable if and\nonly if either $n=1$ or $0<q<1$. As $n\\to\\infty$, the growth rate of the\nsmallest possible $r$ is exactly $\\Theta(n^{q/2})$ if $0<q<1$. Hence, the\nmetric space of probability distributions on $\\mathbb{R}^n$ with finite $q$-th\nabsolute moments, $0<q<1$, with the earth mover's distance (or 1-Wasserstein\ndistance) with respect to the snowflake metric $c(x,y)=\\Vert x-y\\Vert_2^q$, is\nbi-Lipschitz embeddable into $L_1$ with distortion $O(n^{q/2})$. If we consider\n$c(x,y)=\\Vert x-y\\Vert_2$ (i.e., $q=1$) on the grid $[0..s]^n$ instead of\n$\\mathbb{R}^n$, then $r=O(\\sqrt{n}\\log s)$ is attainable, which implies the\nembeddability of the space of probability distributions on $[0..s]^n$ into\n$L_1$ with distortion $O(\\sqrt{n}\\log s)$, and improves upon the $O(n\\log s)$\nresult by Indyk and Thaper. The case of the discrete metric cost\n$c(x,y)=\\mathbf{1}\\{x\\neq y\\}$ and more general metric and ultrametric costs\nare also investigated.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 19:09:09 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 06:30:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Li", "Cheuk Ting", ""], ["Anantharam", "Venkat", ""]]}, {"id": "1908.01628", "submitter": "Hanzhong Liu", "authors": "Hanzhong Liu and Yuehan Yang", "title": "Regression-adjusted average treatment effect estimates in stratified\n  randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often use linear regression to analyse randomized experiments to\nimprove treatment effect estimation by adjusting for imbalances of covariates\nin the treatment and control groups. Our work offers a randomization-based\ninference framework for regression adjustment in stratified randomized\nexperiments. Under mild conditions, we re-establish the finite population\ncentral limit theorem for a stratified experiment. We prove that both the\nstratified difference-in-means and the regression-adjusted average treatment\neffect estimators are consistent and asymptotically normal. The asymptotic\nvariance of the latter is no greater and is typically lesser than that of the\nformer. We also provide conservative variance estimators to construct\nlarge-sample confidence intervals for the average treatment effect.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 14:03:35 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 08:44:42 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Liu", "Hanzhong", ""], ["Yang", "Yuehan", ""]]}, {"id": "1908.01823", "submitter": "Lizhen Lin", "authors": "Zifeng Zhao, Li Chen and Lizhen Lin", "title": "Change-point detection in dynamic networks via graphon estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach for change-point detection in dynamic networks.\nThe proposed method is model-free and covers a wide range of dynamic networks.\nThe key idea behind our approach is to effectively utilize the network\nstructure in designing change-point detection algorithms. This is done via an\ninitial step of graphon estimation, where we propose a modified neighborhood\nsmoothing~(MNBS) algorithm for estimating the link probability matrices of a\ndynamic network. Based on the initial graphon estimation, we then develop a\nscreening and thresholding algorithm for multiple change-point detection in\ndynamic networks. The convergence rate and consistency for the change-point\ndetection procedure are derived as well as those for MNBS. When the number of\nnodes is large~(e.g., exceeds the number of temporal points), our approach\nyields a faster convergence rate in detecting change-points comparing with an\nalgorithm that simply employs averaged information of the dynamic network\nacross time. Numerical experiments demonstrate robust performance of the\nproposed algorithm for change-point detection under various types of dynamic\nnetworks, and superior performance over existing methods is observed. A real\ndata example is provided to illustrate the effectiveness and practical impact\nof the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:50:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhao", "Zifeng", ""], ["Chen", "Li", ""], ["Lin", "Lizhen", ""]]}, {"id": "1908.01865", "submitter": "Lev B Klebanov", "authors": "Lev Klebanov", "title": "Characterizations of Two-Points and Other Related Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new characterizations of two-points and some related\ndistributions. We use properties of independence and/or identity of the\ndistributions of suitable linear forms of random variables.\n  Keywords: characterization of a distribution; two-points distribution;\nuniform distribution; squared hyperbolic secant distribution; independent or\nidentically distributed linear forms\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:18:02 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Klebanov", "Lev", ""]]}, {"id": "1908.01943", "submitter": "Chuancun Yin", "authors": "Chuancun Yin", "title": "Stochastic ordering of Gini indexes for multivariate elliptical random\n  variables", "comments": "14pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish the stochastic ordering of the Gini indexes for\nmultivariate elliptical risks which generalized the corresponding results for\nmultivariate normal risks. It is shown that several conditions on dispersion\nmatrices and the components of dispersion matrices of multivariate normal risks\nfor the monotonicity of the Gini index in the usual stochastic order proposed\nby Samanthi, Wei and Brazauskas (2016) and Kim and Kim (2019) are also suitable\nfor multivariate elliptical risks. We also study the tail probability of Gini\nindex for multivariate elliptical risks and revised a large deviation result\nfor the Gini indexes of multivariate normal risks in Kim and Kim (2019).\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 03:45:41 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 13:14:23 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 11:00:21 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Yin", "Chuancun", ""]]}, {"id": "1908.01976", "submitter": "Jing Zhang", "authors": "Jing Zhang, Jin Xu, Kai Jia, Yimin Yin and Zhengming Wang", "title": "Optimal Sliced Latin Hypercube Designs with Slices of Arbitrary Run\n  Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced Latin hypercube designs (SLHDs) are widely used in computer\nexperiments with both quantitative and qualitative factors and in batches.\nOptimal SLHDs achieve better space-filling property on the whole experimental\nregion. However, most existing methods for constructing optimal SLHDs have\nrestriction on the run sizes. In this paper, we propose a new method for\nconstructing SLHDs with arbitrary run sizes, and a new combined space-filling\nmeasurement describing the space-filling property for both the whole design and\nits slices. Furthermore, we develop general algorithms to search the optimal\nSLHD with arbitrary run sizes under the proposed measurement. Examples are\npresented to illustrate that effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 06:25:59 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhang", "Jing", ""], ["Xu", "Jin", ""], ["Jia", "Kai", ""], ["Yin", "Yimin", ""], ["Wang", "Zhengming", ""]]}, {"id": "1908.02143", "submitter": "Guy Martial Nkiet", "authors": "St\\'ephane Bouka and Sophie Dabo-Niang and Guy Martial Nkiet", "title": "On estimation and prediction in spatial functional linear regression\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a spatial functional linear regression, where a scalar response\nis related to a square integrable spatial functional process. We use a\nsmoothing spline estimator for the functional slope parameter and establish a\nfinite sample bound for variance of this estimator under mixing spatial\ndependence. Then, we give a bound of the prediction error. Finally, we\nillustrate our results by simulations\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 11:10:22 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Bouka", "St\u00e9phane", ""], ["Dabo-Niang", "Sophie", ""], ["Nkiet", "Guy Martial", ""]]}, {"id": "1908.02193", "submitter": "Nabaneet Das", "authors": "Nabaneet Das, Subir K.Bhandari", "title": "Bound on FWER for correlated normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper,our main focus is to obtain an asymptotic bound on the family\nwise error rate (FWER) for Bonferroni-type procedure in the simultaneous\nhypotheses testing problem when the observations corresponding to individual\nhypothesis are correlated. In particular, we have considered the sequence of\nnull hypotheses H_{0i} : X_i follows N(0,1) , (i=1,2,....,n) and equicorrelated\nstructure of the sequence (X_1,....,X_n). Distribution free bound on FWER under\nequicorrelated setup can be found in Tong(2014). But the upper bound provided\nin Tong(2014) is not a bounded quantity as the no. of hypotheses(n) gets larger\nand larger and as a result,FWER is highly overestimated for the choice of a\nparticular distribution (e.g.- normal). In the equicorrelated normal setup, we\nhave shown that FWER asymptotically is a convex function (as a function of\ncorrelation (rho)) and hence an upper bound on the FWER of Bonferroni-(alpha)\nprocedure is alpha(1-\\rho).This implies,Bonferroni's method actually controls\nthe FWER at a much smaller level than the desired level of significance under\nthe positively correlated case and necessitates a correlation correction.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 14:58:13 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 16:25:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Das", "Nabaneet", ""], ["Bhandari", "Subir K.", ""]]}, {"id": "1908.02493", "submitter": "Fabian Telschow J. E.", "authors": "Fabian Telschow, Armin Schwartzman, Dan Cheng and Pratyush Pranav", "title": "Estimation of Expected Euler Characteristic Curves of Nonstationary\n  Smooth Gaussian Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected Euler characteristic (EEC) curve of excursion sets of a Gaussian\nrandom field is used to approximate the distribution of its supremum for high\nthresholds. Viewed as a function of the excursion threshold, the EEC is\nexpressed by the Gaussian kinematic formula (GKF) as a linear function of the\nLipschitz-Killing curvatures (LKCs) of the field, which solely depend on the\ndomain and covariance function of the field. So far its use for non-stationary\nGaussian fields over non-trivial domains has been limited because in this case\nthe LKCs are difficult to estimate. In this paper, consistent estimators of the\nLKCs are proposed as linear projections of \"pinned\" observed Euler\ncharacteristic curves and a linear parametric estimator of the EEC curve is\nobtained, which is more efficient than its nonparametric counterpart for\nrepeated observations. A multiplier bootstrap modification reduces the variance\nof the estimator, and allows estimation of LKCs and EEC of the limiting field\nof non-Gaussian fields satisfying a functional CLT. The proposed methods are\nevaluated using simulations of 2D fields and illustrated in thresholding of 3D\nfMRI brain activation maps and cosmological simulations on the 2-sphere.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:55:38 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 14:23:37 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Telschow", "Fabian", ""], ["Schwartzman", "Armin", ""], ["Cheng", "Dan", ""], ["Pranav", "Pratyush", ""]]}, {"id": "1908.02684", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee", "title": "Bayesian Structure Learning in Graphical Models using Shrinkage priors", "comments": "This is an extended abstract version of the ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of a high dimensional\nprecision matrix under sparsity assumptions. We propose to use a shrinkage\nprior, called the DL-graphical prior based on the Dirichlet-Laplace prior used\nfor the Gaussian mean problem. A posterior sampling scheme based on Gibbs\nsampling is also provided along with theoretical guarantees of the method by\nobtaining the posterior convergence rate of the precision matrix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:21:52 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Banerjee", "Sayantan", ""]]}, {"id": "1908.02718", "submitter": "Yan Shu", "authors": "Martin Mihelich, Charles Dognin, Yan Shu, Michael Blot", "title": "A Characterization of Mean Squared Error for Estimator with Bagging", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bagging can significantly improve the generalization performance of unstable\nmachine learning algorithms such as trees or neural networks. Though bagging is\nnow widely used in practice and many empirical studies have explored its\nbehavior, we still know little about the theoretical properties of bagged\npredictions. In this paper, we theoretically investigate how the bagging method\ncan reduce the Mean Squared Error (MSE) when applied on a statistical\nestimator. First, we prove that for any estimator, increasing the number of\nbagged estimators $N$ in the average can only reduce the MSE. This intuitive\nresult, observed empirically and discussed in the literature, has not yet been\nrigorously proved. Second, we focus on the standard estimator of variance\ncalled unbiased sample variance and we develop an exact analytical expression\nof the MSE for this estimator with bagging.\n  This allows us to rigorously discuss the number of iterations $N$ and the\nbatch size $m$ of the bagging method. From this expression, we state that only\nif the kurtosis of the distribution is greater than $\\frac{3}{2}$, the MSE of\nthe variance estimator can be reduced with bagging. This result is important\nbecause it demonstrates that for distribution with low kurtosis, bagging can\nonly deteriorate the performance of a statistical prediction. Finally, we\npropose a novel general-purpose algorithm to estimate with high precision the\nvariance of a sample.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 16:40:07 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Mihelich", "Martin", ""], ["Dognin", "Charles", ""], ["Shu", "Yan", ""], ["Blot", "Michael", ""]]}, {"id": "1908.03152", "submitter": "Kengo Kato", "authors": "Mingli Chen, Kengo Kato, Chenlei Leng", "title": "Analysis of Networks via the Sparse $\\beta$-Model", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of networks are increasingly available in a variety of\nareas, yet statistical models allowing for parameter estimates with desirable\nstatistical properties for sparse networks remain scarce. To address this, we\npropose the Sparse $\\beta$-Model (S$\\beta$M), a new network model that\ninterpolates the celebrated Erd\\H{o}s-R\\'enyi model and the $\\beta$-model that\nassigns one different parameter to each node. By a novel reparameterization of\nthe $\\beta$-model to distinguish global and local parameters, our S$\\beta$M can\ndrastically reduce the dimensionality of the $\\beta$-model by requiring some of\nthe local parameters to be zero. We derive the asymptotic distribution of the\nmaximum likelihood estimator of the S$\\beta$M when the support of the parameter\nvector is known. When the support is unknown, we formulate a penalized\nlikelihood approach with the $\\ell_0$-penalty. Remarkably, we show via a\nmonotonicity lemma that the seemingly combinatorial computational problem due\nto the $\\ell_0$-penalty can be overcome by assigning nonzero parameters to\nthose nodes with the largest degrees. We further show that a $\\beta$-min\ncondition guarantees our method to identify the true model and provide excess\nrisk bounds for the estimated parameters. The estimation procedure enjoys good\nfinite sample properties as shown by simulation studies. The usefulness of the\nS$\\beta$M is further illustrated via the analysis of a microfinance take-up\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 16:25:32 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 12:25:43 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 10:35:49 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Chen", "Mingli", ""], ["Kato", "Kengo", ""], ["Leng", "Chenlei", ""]]}, {"id": "1908.03305", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian, Diego Fern\\'andez", "title": "An Independence Test Based on Recurrence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new test of independence between random elements is presented in this\narticle. The test is based on a functional of the Cram\\'{e}r-von Mises type,\nwhich is applied to a $U$-process that is defined from the recurrence rates.\nTheorems of asymptotic distribution under $H_{0},$ and consistency under a wide\nclass of alternatives are obtained. The results under contiguous alternatives\nare also shown. The test has a very good behaviour under several alternatives,\nwhich shows that in many cases there is clearly larger power when compared to\nother tests that are widely used in literature. In addition, the new test could\nbe used for discrete or continuous time series.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 04:21:53 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kalemkerian", "Juan", ""], ["Fern\u00e1ndez", "Diego", ""]]}, {"id": "1908.03317", "submitter": "Kouakou Francois Domagni", "authors": "Francois Domagni, Samad Hedayat, Sinha Bikas", "title": "The general Nature of Saturated Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contemplate an experimental situation in a $2^k$-factorial experiment with\nacute resource crunch so that we need to conduct just a saturated design [SD] -\nwith the understanding that precision of the estimates cannot be estimated from\nthe data. It is known beforehand which effect(s)/interaction(s) are likely to\nbe negligible. We examine the flexibility to the extent that an experimenter\ncan make a choice of an SD in order to retain information on all the remaining\n[non-negligible] effects/interactions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 05:24:47 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 21:47:08 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Domagni", "Francois", ""], ["Hedayat", "Samad", ""], ["Bikas", "Sinha", ""]]}, {"id": "1908.03442", "submitter": "Rafael Caba\\~nas", "authors": "Andr\\'es R. Masegosa, Rafael Caba\\~nas, Helge Langseth, Thomas D.\n  Nielsen, Antonio Salmer\\'on", "title": "Probabilistic Models with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in statistical inference have significantly expanded the\ntoolbox of probabilistic modeling. Historically, probabilistic modeling has\nbeen constrained to (i) very restricted model classes where exact or\napproximate probabilistic inference were feasible, and (ii) small or\nmedium-sized data sets which fit within the main memory of the computer.\nHowever, developments in variational inference, a general form of approximate\nprobabilistic inference originated in statistical physics, are allowing\nprobabilistic modeling to overcome these restrictions: (i) Approximate\nprobabilistic inference is now possible over a broad class of probabilistic\nmodels containing a large number of parameters, and (ii) scalable inference\nmethods based on stochastic gradient descent and distributed computation\nengines allow to apply probabilistic modeling over massive data sets. One\nimportant practical consequence of these advances is the possibility to include\ndeep neural networks within a probabilistic model to capture complex non-linear\nstochastic relationships between random variables. These advances in\nconjunction with the release of novel probabilistic modeling toolboxes have\ngreatly expanded the scope of application of probabilistic models, and allow\nthese models to take advantage of the recent strides made by the deep learning\ncommunity. In this paper we review the main concepts, methods and tools needed\nto use deep neural networks within a probabilistic modeling framework.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 12:55:54 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 08:37:56 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 10:06:39 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Masegosa", "Andr\u00e9s R.", ""], ["Caba\u00f1as", "Rafael", ""], ["Langseth", "Helge", ""], ["Nielsen", "Thomas D.", ""], ["Salmer\u00f3n", "Antonio", ""]]}, {"id": "1908.03462", "submitter": "Johannes Lutzeyer", "authors": "J. F. Lutzeyer, A. T. Walden", "title": "Extending the Davis-Kahan theorem for comparing eigenvectors of two\n  symmetric matrices I: Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Davis-Kahan theorem can be used to bound the distance of the spaces\nspanned by the first $r$ eigenvectors of any two symmetric matrices. We extend\nthe Davis-Kahan theorem to apply to the comparison of the union of eigenspaces\nof any two symmetric matrices by making use of polynomial matrix transforms and\nin so doing, tighten the bound. The transform allows us to move requirements\npresent in the original Davis-Kahan theorem, from the eigenvalues of the\ncompared matrices on to the transformation parameters, with the latter being\nunder our control. We provide a proof of concept example, comparing the spaces\nspanned by the unnormalised and normalised graph Laplacian eigenvectors for\n$d$-regular graphs, in which the correct transform is automatically identified.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:08:16 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Lutzeyer", "J. F.", ""], ["Walden", "A. T.", ""]]}, {"id": "1908.03465", "submitter": "Johannes Lutzeyer", "authors": "J. F. Lutzeyer, A. T. Walden", "title": "Extending the Davis-Kahan theorem for comparing eigenvectors of two\n  symmetric matrices II: Computation and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extended Davis-Kahan theorem makes use of polynomial matrix\ntransformations to produce bounds at least as tight as the standard Davis-Kahan\ntheorem. The optimization problem of finding transformation parameters\nresulting in optimal bounds from the extended Davis-Kahan theorem is presented\nfor affine transformations. It is demonstrated how globally optimal bound\nvalues can be computed automatically using fractional programming theory. Two\ndifferent solution approaches, the Charnes-Cooper transformation and\nDinkelbach's algorithm are reviewed. Our implementation of the extended\nDavis--Kahan theorem is used to calculate bound values in three significant\nexamples. First, a pairwise comparison is made of the spaces spanned by the\neigenvectors of the graph shift operator matrices corresponding to different\nstochastic block model graphs. Second our bound is calculated on the distance\nof the spaces spanned by eigenvectors of the graph shift operators and their\ncorresponding generating matrices in the stochastic blockmodel, and, third, on\nthe sample and population covariance matrices in a spiked covariance model. Our\nextended bound values, using affine transformations, not only outperform the\nstandard Davis-Kahan bounds in all examples where both theorems apply, but also\ndemonstrate good performance in several cases where the standard Davis-Kahan\ntheorem cannot be used.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 14:09:59 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Lutzeyer", "J. F.", ""], ["Walden", "A. T.", ""]]}, {"id": "1908.03541", "submitter": "Jingwei Liu", "authors": "Jingwei Liu", "title": "Extension of Limit Theory with Deleting Items Partial Sum of Random\n  Variable Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deleting items theorems of weak law of large numbers (WLLN),strong law of\nlarge numbers (SLLN) and central limit theorem (CLT) are derived by\nsubstituting partial sum of random variable sequence with deleting items\npartial sum. We address the background of deleting items limit theory of random\nvariable sequence, discuss the classical limit theory of Chebyshev WLLN,\nBernoulli WLLN and Khinchine WLLN with standard mathematical analytical\ntechnique, then develop the deleting items theorems of WLLN, SLLN and CLT based\non convergence theorems and Slutsky's theorem. Our theorems extend the\nclassical limit theory of random variable sequence and provide the construction\nof some asymptotic bias estimators of sample expectation and variance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 17:07:10 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "1908.03606", "submitter": "Jana Jankov\\'a", "authors": "Jana Jankov\\'a, Rajen D. Shah, Peter B\\\"uhlmann, Richard J. Samworth", "title": "Goodness-of-fit testing in high-dimensional generalized linear models", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of tests to assess the goodness-of-fit of a\nhigh-dimensional generalized linear model. Our framework is flexible and may be\nused to construct an omnibus test or directed against testing specific\nnon-linearities and interaction effects, or for testing the significance of\ngroups of variables. The methodology is based on extracting left-over signal in\nthe residuals from an initial fit of a generalized linear model. This can be\nachieved by predicting this signal from the residuals using modern flexible\nregression or machine learning methods such as random forests or boosted trees.\nUnder the null hypothesis that the generalized linear model is correct, no\nsignal is left in the residuals and our test statistic has a Gaussian limiting\ndistribution, translating to asymptotic control of type I error. Under a local\nalternative, we establish a guarantee on the power of the test. We illustrate\nthe effectiveness of the methodology on simulated and real data examples by\ntesting goodness-of-fit in logistic regression models. Software implementing\nthe methodology is available in the R package `GRPtests'.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 19:30:59 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 20:41:03 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jankov\u00e1", "Jana", ""], ["Shah", "Rajen D.", ""], ["B\u00fchlmann", "Peter", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1908.03656", "submitter": "Caleb Kwon", "authors": "Caleb Kwon, Eric Mbakop", "title": "Estimation of the Number of Components of Non-Parametric Multivariate\n  Finite Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimator for the number of components (denoted by $M$) in\na K-variate non-parametric finite mixture model, where the analyst has repeated\nobservations of $K\\geq2$ variables that are independent given a finitely\nsupported unobserved variable. Under a mild assumption on the joint\ndistribution of the observed and latent variables, we show that an integral\noperator $T$, that is identified from the data, has rank equal to $M$. Using\nthis observation, and the fact that singular values are stable under\nperturbations, the estimator of $M$ that we propose is based on a thresholding\nrule which essentially counts the number of singular values of a consistent\nestimator of $T$ that are greater than a data-driven threshold. We prove that\nour estimator of $M$ is consistent, and establish non-asymptotic results which\nprovide finite sample performance guarantees for our estimator. We present a\nMonte Carlo study which shows that our estimator performs well for samples of\nmoderate size.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 00:24:57 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 02:14:06 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kwon", "Caleb", ""], ["Mbakop", "Eric", ""]]}, {"id": "1908.03676", "submitter": "Huiming Zhang", "authors": "Xiaowei Yang, Shuang Song, Huiming Zhang", "title": "Law of the Iterated Logarithm and Model Selection Consistency for GLMs\n  with Independent and Dependent Responses", "comments": "25 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the law of the iterated logarithm (LIL) for the maximum likelihood\nestimation of the parameters (as a convex optimization problem) in the\ngeneralized linear models with independent or weakly dependent ($\\rho$-mixing,\n$m$-dependent) responses under mild conditions. The LIL is useful to derive the\nasymptotic bounds for the discrepancy between the empirical process of the\nlog-likelihood function and the true log-likelihood. As the application of the\nLIL, the strong consistency of some penalized likelihood based model selection\ncriteria can be shown. Under some regularity conditions, the model selection\ncriterion will be helpful to select the simplest correct model almost surely\nwhen the penalty term increases with model dimension and the penalty term has\nan order higher than $O({\\rm{loglog}}n)$ but lower than $O(n)$. Simulation\nstudies are implemented to verify the selection consistency of BIC.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 03:09:31 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 10:49:36 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 18:22:30 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Xiaowei", ""], ["Song", "Shuang", ""], ["Zhang", "Huiming", ""]]}, {"id": "1908.04020", "submitter": "Xavier Bry", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG), Xavier Bry (IMAG)", "title": "Component-based regularisation of multivariate generalised linear mixed\n  models", "comments": "Journal of Computational and Graphical Statistics, Taylor & Francis,\n  In press", "journal-ref": null, "doi": "10.1080/10618600.2019.1598870", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the component-based regularisation of a multivariate Generalised\nLinear Mixed Model (GLMM) in the framework of grouped data. A set Y of random\nresponses is modelled with a multivariate GLMM, based on a set X of explanatory\nvariables, a set A of additional explanatory variables, and random effects to\nintroduce the within-group dependence of observations. Variables in X are\nassumed many and redundant so that regression demands regularisation. This is\nnot the case for A, which contains few and selected variables. Regularisation\nis performed building an appropriate number of orthogonal components that both\ncontribute to model Y and capture relevant structural information in X. To\nestimate the model, we propose to maximise a criterion specific to the\nSupervised Component-based Generalised Linear Regression (SCGLR) within an\nadaptation of Schall's algorithm. This extension of SCGLR is tested on both\nsimulated and real grouped data, and compared to ridge and LASSO\nregularisations. Supplementary material for this article is available online.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:34:06 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG"], ["Bry", "Xavier", "", "IMAG"]]}, {"id": "1908.04106", "submitter": "Holger Dette", "authors": "Holger Dette, Andrey Pepelyshev, Anatoly Zhigljavsky", "title": "Prediction in regression models with continuous observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting values of a random process or field\nsatisfying a linear model $y(x)=\\theta^\\top f(x) + \\varepsilon(x)$, where\nerrors $\\varepsilon(x)$ are correlated. This is a common problem in kriging,\nwhere the case of discrete observations is standard. By focussing on the case\nof continuous observations, we derive expressions for the best linear unbiased\npredictors and their mean squared error. Our results are also applicable in the\ncase where the derivatives of the process $y$ are available, and either a\nresponse or one of its derivatives need to be predicted. The theoretical\nresults are illustrated by several examples in particular for the popular\nMat\\'{e}rn $3/2$ kernel.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:04:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1908.04110", "submitter": "Constantin Weiser", "authors": "Michael Griebel, Florian Heiss, Jens Oettershagen, Constantin Weiser", "title": "Maximum Approximated Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical economic research frequently applies maximum likelihood estimation\nin cases where the likelihood function is analytically intractable. Most of the\ntheoretical literature focuses on maximum simulated likelihood (MSL)\nestimators, while empirical and simulation analyzes often find that alternative\napproximation methods such as quasi-Monte Carlo simulation, Gaussian\nquadrature, and integration on sparse grids behave considerably better\nnumerically. This paper generalizes the theoretical results widely known for\nMSL estimators to a general set of maximum approximated likelihood (MAL)\nestimators. We provide general conditions for both the model and the\napproximation approach to ensure consistency and asymptotic normality. We also\nshow specific examples and finite-sample simulation results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 12:22:52 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Griebel", "Michael", ""], ["Heiss", "Florian", ""], ["Oettershagen", "Jens", ""], ["Weiser", "Constantin", ""]]}, {"id": "1908.04145", "submitter": "Carsten Chong", "authors": "Carsten Chong", "title": "High-frequency analysis of parabolic stochastic PDEs with multiplicative\n  noise: Part I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic heat equation driven by a multiplicative Gaussian\nnoise that is white in time and spatially homogeneous in space. Assuming that\nthe spatial correlation function is given by a Riesz kernel of order $\\alpha\n\\in (0,1)$, we prove a central limit theorem for power variations and other\nrelated functionals of the solution. To our surprise, there is no asymptotic\nbias despite the low regularity of the noise coefficient in the multiplicative\ncase. We trace this circumstance back to cancellation effects between error\nterms arising naturally in second-order limit theorems for power variations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 13:33:46 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chong", "Carsten", ""]]}, {"id": "1908.04179", "submitter": "Steven Finch", "authors": "Steven Finch", "title": "Moments of Maximum: Segment of AR(1)", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{t}$ denote a stationary first-order autoregressive process. Consider\nfive contiguous observations (in time $t$) of the series (e.g., $X_{1}, ...,\nX_{5}$). Let $M$ denote the maximum of these. Let $\\rho$ be the lag-one serial\ncorrelation, which satisfies $|\\rho| < 1$. For what value of $\\rho$ is\n$\\mathbb{E}(M)$ maximized? How does $\\mathbb{V}(M)$ behave for increasing\n$\\rho$? Answers to these questions lie in Afonja (1972), suitably decoded.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 14:43:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Finch", "Steven", ""]]}, {"id": "1908.04233", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner", "title": "Geometrical Smeariness -- A new Phenomenon of Fr\\'echet Means", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, the central limit theorem (CLT) has been generalized to\nnon-Euclidean data spaces. Some years ago, it was found that for some random\nvariables on the circle, the sample Fr\\'echet mean fluctuates around the\npopulation mean asymptotically at a scale $n^{-\\tau}$ with exponent $\\tau <\n1/2$ with a non-normal distribution if the probability density at the antipodal\npoint of the mean is $\\frac{1}{2\\pi}$. The author and his collaborator recently\ndiscovered that $\\tau = 1/6$ for some random variables on higher dimensional\nspheres. In this article we show that, even more surprisingly, the phenomenon\non spheres of higher dimension is qualitatively different from that on the\ncircle, as it depends purely on geometrical properties of the space, namely its\ncurvature, and not on the density at the antipodal point. This gives rise to\nthe new concept of geometrical smeariness. In consequence, the sphere can be\ndeformed, say, by removing a neighborhood of the antipodal point of the mean\nand gluing a flat space there, with a smooth transition piece. This yields\nsmeariness on a manifold, which is diffeomorphic to Euclidean space. We give an\nexample family of random variables with 2-smeary mean, i.e. with $\\tau = 1/6$,\nwhose range has a hole containing the cut locus of the mean. The hole size\nexhibits a curse of dimensionality as it can increase with dimension,\nconverging to the whole hemisphere opposite a local Fr\\'echet mean. We observe\nsmeariness in simulated landmark shapes on Kendall pre-shape space and in real\ndata of geomagnetic north pole positions on the two-dimensional sphere.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:37:32 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 13:16:29 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 13:02:41 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Eltzner", "Benjamin", ""]]}, {"id": "1908.04328", "submitter": "Holger Dette", "authors": "Holger Dette, Subhra Sankar Dhar, Weichi Wu", "title": "Identifying shifts between two regression curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the problem whether two convex (concave) regression\nfunctions modelling the relation between a response and covariate in two\nsamples differ by a shift in the horizontal and/or vertical axis. We consider a\nnonparametric situation assuming only smoothness of the regression functions. A\ngraphical tool based on the derivatives of the regression functions and their\ninverses is proposed to answer this question and studied in several examples.\nWe also formalize this question in a corresponding hypothesis and develop a\nstatistical test. The asymptotic properties of the corresponding test statistic\nare investigated under the null hypothesis and local alternatives. In contrast\nto most of the literature on comparing shape invariant models, which requires\nindependent data the procedure is applicable for dependent and non-stationary\ndata. We also illustrate the finite sample properties of the new test by means\nof a small simulation study and a real data example.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 18:31:28 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Dette", "Holger", ""], ["Dhar", "Subhra Sankar", ""], ["Wu", "Weichi", ""]]}, {"id": "1908.04331", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Neil K. Chada and Emmanuel Delande", "title": "Elements of asymptotic theory with outer probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outer measures can be used for statistical inference in place of probability\nmeasures to bring flexibility in terms of model specification. The\ncorresponding statistical procedures such as Bayesian inference, estimators or\nhypothesis testing need to be analysed in order to understand their behaviour,\nand motivate their use. In this article, we consider a class of outer measures\nbased on the supremum of particular functions that we refer to as possibility\nfunctions. We then characterise the asymptotic behaviour of the corresponding\nBayesian posterior uncertainties, from which the properties of the\ncorresponding maximum a posteriori estimators can be deduced. These results are\nlargely based on versions of both the law of large numbers and the central\nlimit theorem that are adapted to possibility functions. Our motivation with\nouter measures is through the notion of uncertainty quantification, where\nverification of these procedures is of crucial importance. These introduced\nconcepts shed a new light on some standard concepts such as the Fisher\ninformation and sufficient statistics and naturally strengthen the link between\nthe frequentist and Bayesian approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 18:38:23 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 06:17:02 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 10:58:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Chada", "Neil K.", ""], ["Delande", "Emmanuel", ""]]}, {"id": "1908.04433", "submitter": "Hossein Taheri", "authors": "Hossein Taheri, Ramtin Pedarsani and Christos Thrampoulidis", "title": "Sharp Guarantees for Solving Random Equations with One-Bit Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG eess.SP math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of a wide class of convex optimization-based\nestimators for recovering a signal from corrupted one-bit measurements in\nhigh-dimensions. Our general result predicts sharply the performance of such\nestimators in the linear asymptotic regime when the measurement vectors have\nentries IID Gaussian. This includes, as a special case, the previously studied\nleast-squares estimator and various novel results for other popular estimators\nsuch as least-absolute deviations, hinge-loss and logistic-loss. Importantly,\nwe exploit the fact that our analysis holds for generic convex loss functions\nto prove a bound on the best achievable performance across the entire class of\nestimators. Numerical simulations corroborate our theoretical findings and\nsuggest they are accurate even for relatively small problem dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 22:51:06 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 19:54:10 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Taheri", "Hossein", ""], ["Pedarsani", "Ramtin", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "1908.04462", "submitter": "Ran Dai", "authors": "Ran Dai, Hyebin Song, Rina Foygel Barber, Garvesh Raskutti", "title": "The bias of isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bias of the isotonic regression estimator. While there is\nextensive work characterizing the mean squared error of the isotonic regression\nestimator, relatively little is known about the bias. In this paper, we provide\na sharp characterization, proving that the bias scales as $O(n^{-\\beta/3})$ up\nto log factors, where $1 \\leq \\beta \\leq 2$ is the exponent corresponding to\nH{\\\"o}lder smoothness of the underlying mean. Importantly, this result only\nrequires a strictly monotone mean and that the noise distribution has\nsubexponential tails, without relying on symmetric noise or other restrictive\nassumptions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:15:53 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 05:02:52 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dai", "Ran", ""], ["Song", "Hyebin", ""], ["Barber", "Rina Foygel", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1908.04468", "submitter": "Fred Zhang", "authors": "Zhixian Lei, Kyle Luh, Prayaag Venkat, Fred Zhang", "title": "A Fast Spectral Algorithm for Mean Estimation with Sub-Gaussian Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the algorithmic problem of estimating the mean of heavy-tailed\nrandom vector in $\\mathbb{R}^d$, given $n$ i.i.d. samples. The goal is to\ndesign an efficient estimator that attains the optimal sub-gaussian error\nbound, only assuming that the random vector has bounded mean and covariance.\nPolynomial-time solutions to this problem are known but have high runtime due\nto their use of semi-definite programming (SDP). Conceptually, it remains open\nwhether convex relaxation is truly necessary for this problem.\n  In this work, we show that it is possible to go beyond SDP and achieve better\ncomputational efficiency. In particular, we provide a spectral algorithm that\nachieves the optimal statistical performance and runs in time $\\widetilde\nO\\left(n^2 d \\right)$, improving upon the previous fastest runtime $\\widetilde\nO\\left(n^{3.5}+ n^2d\\right)$ by Cherapanamjeri el al. (COLT '19). Our algorithm\nis spectral in that it only requires (approximate) eigenvector computations,\nwhich can be implemented very efficiently by, for example, power iteration or\nthe Lanczos method.\n  At the core of our algorithm is a novel connection between the furthest\nhyperplane problem introduced by Karnin et al. (COLT '12) and a structural\nlemma on heavy-tailed distributions by Lugosi and Mendelson (Ann. Stat. '19).\nThis allows us to iteratively reduce the estimation error at a geometric rate\nusing only the information derived from the top singular vector of the data\nmatrix, leading to a significantly faster running time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:56:01 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 22:56:41 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Lei", "Zhixian", ""], ["Luh", "Kyle", ""], ["Venkat", "Prayaag", ""], ["Zhang", "Fred", ""]]}, {"id": "1908.04553", "submitter": "Charles Curry", "authors": "Stephen R Marsland, Robert I McLachlan, Charles Curry", "title": "Principal symmetric space analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel analogue of Euclidean PCA (principal component analysis)\nfor data taking values on a Riemannian symmetric space, using totally geodesic\nsubmanifolds as approximating lower dimnsional submanifolds. We illustrate the\ntechnique on n-spheres, Grassmannians, n-tori and polyspheres.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 09:09:44 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Marsland", "Stephen R", ""], ["McLachlan", "Robert I", ""], ["Curry", "Charles", ""]]}, {"id": "1908.04569", "submitter": "Timo Dimitriadis", "authors": "Timo Dimitriadis and Julie Schnaitmann", "title": "Forecast Encompassing Tests for the Expected Shortfall", "comments": "International Journal of Forecasting (2020+, forthcoming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new forecast encompassing tests for the risk measure Expected\nShortfall (ES). The ES currently receives much attention through its\nintroduction into the Basel III Accords, which stipulate its use as the primary\nmarket risk measure for the international banking regulation. We utilize joint\nloss functions for the pair ES and Value at Risk to set up three ES\nencompassing test variants. The tests are built on misspecification robust\nasymptotic theory and we investigate the finite sample properties of the tests\nin an extensive simulation study. We use the encompassing tests to illustrate\nthe potential of forecast combination methods for different financial assets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 10:26:03 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 16:18:01 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 13:17:05 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Dimitriadis", "Timo", ""], ["Schnaitmann", "Julie", ""]]}, {"id": "1908.04847", "submitter": "Badr-Eddine Ch\\'erief-Abdellatif", "authors": "Badr-Eddine Ch\\'erief-Abdellatif", "title": "Convergence Rates of Variational Inference in Sparse Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is becoming more and more popular for approximating\nintractable posterior distributions in Bayesian statistics and machine\nlearning. Meanwhile, a few recent works have provided theoretical justification\nand new insights on deep neural networks for estimating smooth functions in\nusual settings such as nonparametric regression. In this paper, we show that\nvariational inference for sparse deep learning retains the same generalization\nproperties than exact Bayesian inference. In particular, we highlight the\nconnection between estimation and approximation theories via the classical\nbias-variance trade-off and show that it leads to near-minimax rates of\nconvergence for H\\\"older smooth functions. Additionally, we show that the model\nselection framework over the neural network architecture via ELBO maximization\ndoes not overfit and adaptively achieves the optimal rate of convergence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:50:09 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 21:27:21 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""]]}, {"id": "1908.05255", "submitter": "Fang Han", "authors": "Yanqin Fan, Fang Han, Wei Li, Xiao-Hua Zhou", "title": "On rank estimators in increasing dimensions", "comments": "to appear in Journal of Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of rank estimators, including Han's maximum rank correlation (Han,\n1987) as a notable example, has been widely exploited in studying regression\nproblems. For these estimators, although the linear index is introduced for\nalleviating the impact of dimensionality, the effect of large dimension on\ninference is rarely studied. This paper fills this gap via studying the\nstatistical properties of a larger family of M-estimators, whose objective\nfunctions are formulated as U-processes and may be discontinuous in increasing\ndimension set-up where the number of parameters, $p_{n}$, in the model is\nallowed to increase with the sample size, $n$. First, we find that often in\nestimation, as $p_{n}/n\\rightarrow 0$, $(p_{n}/n)^{1/2}$ rate of convergence is\nobtainable. Second, we establish Bahadur-type bounds and study the validity of\nnormal approximation, which we find often requires a much stronger scaling\nrequirement than $p_{n}^{2}/n\\rightarrow 0.$ Third, we state conditions under\nwhich the numerical derivative estimator of asymptotic covariance matrix is\nconsistent, and show that the step size in implementing the covariance\nestimator has to be adjusted with respect to $p_{n}$. All theoretical results\nare further backed up by simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 17:35:07 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Fan", "Yanqin", ""], ["Han", "Fang", ""], ["Li", "Wei", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "1908.05355", "submitter": "Song Mei", "authors": "Song Mei, Andrea Montanari", "title": "The generalization error of random features regression: Precise\n  asymptotics and double descent curve", "comments": "We reorganized the proofs of the main theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods operate in regimes that defy the traditional\nstatistical mindset. Neural network architectures often contain more parameters\nthan training samples, and are so rich that they can interpolate the observed\nlabels, even if the latter are replaced by pure noise. Despite their huge\ncomplexity, the same architectures achieve small generalization error on real\ndata.\n  This phenomenon has been rationalized in terms of a so-called `double\ndescent' curve. As the model complexity increases, the test error follows the\nusual U-shaped curve at the beginning, first decreasing and then peaking around\nthe interpolation threshold (when the model achieves vanishing training error).\nHowever, it descends again as model complexity exceeds this threshold. The\nglobal minimum of the test error is found above the interpolation threshold,\noften in the extreme overparametrization regime in which the number of\nparameters is much larger than the number of samples. Far from being a peculiar\nproperty of deep neural networks, elements of this behavior have been\ndemonstrated in much simpler settings, including linear regression with random\ncovariates.\n  In this paper we consider the problem of learning an unknown function over\nthe $d$-dimensional sphere $\\mathbb S^{d-1}$, from $n$ i.i.d. samples\n$(\\boldsymbol x_i, y_i)\\in \\mathbb S^{d-1} \\times \\mathbb R$, $i\\le n$. We\nperform ridge regression on $N$ random features of the form $\\sigma(\\boldsymbol\nw_a^{\\mathsf T} \\boldsymbol x)$, $a\\le N$. This can be equivalently described\nas a two-layers neural network with random first-layer weights. We compute the\nprecise asymptotics of the test error, in the limit $N,n,d\\to \\infty$ with\n$N/d$ and $n/d$ fixed. This provides the first analytically tractable model\nthat captures all the features of the double descent phenomenon without\nassuming ad hoc misspecification structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 21:23:40 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 04:38:06 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 04:18:57 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 08:25:04 GMT"}, {"version": "v5", "created": "Fri, 11 Dec 2020 04:06:17 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mei", "Song", ""], ["Montanari", "Andrea", ""]]}, {"id": "1908.05368", "submitter": "Shuang Qiu", "authors": "Shuang Qiu, Xiaohan Wei, Zhuoran Yang", "title": "Robust One-Bit Recovery via ReLU Generative Networks: Near-Optimal\n  Statistical Rate and Global Landscape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robust one-bit compressed sensing problem whose goal is to\ndesign an algorithm that faithfully recovers any sparse target vector\n$\\theta_0\\in\\mathbb{R}^d$ \\textit{uniformly} via $m$ quantized noisy\nmeasurements. Specifically, we consider a new framework for this problem where\nthe sparsity is implicitly enforced via mapping a low dimensional\nrepresentation $x_0 \\in \\mathbb{R}^k$ through a known $n$-layer ReLU generative\nnetwork $G:\\mathbb{R}^k\\rightarrow\\mathbb{R}^d$ such that $\\theta_0 = G(x_0)$.\nSuch a framework poses low-dimensional priors on $\\theta_0$ without a known\nsparsity basis. We propose to recover the target $G(x_0)$ solving an\nunconstrained empirical risk minimization (ERM). Under a weak\n\\textit{sub-exponential measurement assumption}, we establish a joint\nstatistical and computational analysis. In particular, we prove that the ERM\nestimator in this new framework achieves a statistical rate of\n$m=\\widetilde{\\mathcal{O}}(kn \\log d /\\varepsilon^2)$ recovering any $G(x_0)$\nuniformly up to an error $\\varepsilon$. When the network is shallow (i.e., $n$\nis small), we show this rate matches the information-theoretic lower bound up\nto logarithm factors of $\\varepsilon^{-1}$. From the lens of computation, we\nprove that under proper conditions on the network weights, our proposed\nempirical risk, despite non-convexity, has no stationary point outside of small\nneighborhoods around the true representation $x_0$ and its negative multiple;\nfurthermore, we show that the global minimizer of the empirical risk stays\nwithin the neighborhood around $x_0$ rather than its negative multiple under\nfurther assumptions on the network weights.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 22:56:34 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 20:12:04 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 06:44:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qiu", "Shuang", ""], ["Wei", "Xiaohan", ""], ["Yang", "Zhuoran", ""]]}, {"id": "1908.05531", "submitter": "Alexander Kolnogorov", "authors": "Alexander Kolnogorov and Denis Grunev", "title": "Exponential two-armed bandit problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exponential two-armed bandit problem in which incomes are\ndescribed by exponential distribution densities. We develop Bayesian approach\nand present recursive equation for determination of Bayesian strategy and\nBayesian risk. In the limiting case as the control horizon goes to infinity, we\nobtain the second order partial differential equation in the domain of \"close\ndistributions\". Results are compared with Gaussian two-armed bandit. It turned\nout that exponential and Gaussian two-armed bandits have the same description\nin the limiting case. Since Gaussian two-armed bandit describes the batch\nprocessing, this means that in case of exponential two-armed bandit batch\nprocessing does not enlarge Bayesian risk in comparison with one-by-one optimal\nprocessing as the total number of processed data items goes to infinity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:32:47 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kolnogorov", "Alexander", ""], ["Grunev", "Denis", ""]]}, {"id": "1908.05607", "submitter": "Weixin Cai", "authors": "Mark J. van der Laan, David Benkeser and Weixin Cai", "title": "Efficient Estimation of Pathwise Differentiable Target Parameters with\n  the Undersmoothed Highly Adaptive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of a functional parameter of a realistically modeled\ndata distribution based on observing independent and identically distributed\nobservations. We define an $m$-th order Spline Highly Adaptive Lasso Minimum\nLoss Estimator (Spline HAL-MLE) of a functional parameter that is defined by\nminimizing the empirical risk function over an $m$-th order smoothness class of\nfunctions. We show that this $m$-th order smoothness class consists of all\nfunctions that can be represented as an infinitesimal linear combination of\ntensor products of $\\leq m$-th order spline-basis functions, and involves\nassuming $m$-derivatives in each coordinate. By selecting $m$ with\ncross-validation we obtain a Spline-HAL-MLE that is able to adapt to the\nunderlying unknown smoothness of the true function, while guaranteeing a rate\nof convergence faster than $n^{-1/4}$, as long as the true function is cadlag\n(right-continuous with left-hand limits) and has finite sectional variation\nnorm. The $m=0$-smoothness class consists of all cadlag functions with finite\nsectional variation norm and corresponds with the original HAL-MLE defined in\nvan der Laan (2015).\n  In this article we establish that this Spline-HAL-MLE yields an\nasymptotically efficient estimator of any smooth feature of the functional\nparameter under an easily verifiable global undersmoothing condition. A\nsufficient condition for the latter condition is that the minimum of the\nempirical mean of the selected basis functions is smaller than a constant times\n$n^{-1/2}$, which is not parameter specific and enforces the selection of the\n$L_1$-norm in the lasso to be large enough to include sparsely supported basis.\nWe demonstrate our general result for the $m=0$-HAL-MLE of the average\ntreatment effect and of the integral of the square of the data density. We also\npresent simulations for these two examples confirming the theory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:28:37 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 17:32:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Benkeser", "David", ""], ["Cai", "Weixin", ""]]}, {"id": "1908.05726", "submitter": "Wenpin Tang", "authors": "Wenpin Tang, Lu Zhang, Sudipto Banerjee", "title": "On identifiability and consistency of the nugget in Gaussian spatial\n  process models", "comments": "19 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial process models popular in geostatistics often represent the observed\ndata as the sum of a smooth underlying process and white noise. The variation\nin the white noise is attributed to measurement error, or micro-scale\nvariability, and is called the \"nugget\". We formally establish results on the\nidentifiability and consistency of the nugget in spatial models based upon the\nGaussian process within the framework of in-fill asymptotics, i.e. the sample\nsize increases within a sampling domain that is bounded. Our work extends\nresults in fixed domain asymptotics for spatial models without the nugget. More\nspecifically, we establish the identifiability of parameters in the Mat\\'ern\ncovariance function and the consistency of their maximum likelihood estimators\nin the presence of discontinuities due to the nugget. We also present\nsimulation studies to demonstrate the role of the identifiable quantities in\nspatial interpolation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 19:33:50 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 17:41:08 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 22:34:17 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2020 06:03:01 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Tang", "Wenpin", ""], ["Zhang", "Lu", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1908.05752", "submitter": "Andrii Babii", "authors": "Andrii Babii and Rohit Kumar", "title": "Isotonic Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation and inference for the isotonic regression\nat the boundary point, an object that is particularly interesting and required\nin the analysis of monotone regression discontinuity designs. We show that the\nisotonic regression is inconsistent in this setting and derive the asymptotic\ndistributions of boundary corrected estimators. Interestingly, the boundary\ncorrected estimators can be bootstrapped without subsampling or additional\nnonparametric smoothing which is not the case for the interior point. The Monte\nCarlo experiments indicate that shape restrictions can improve dramatically the\nfinite-sample performance of unrestricted estimators. Lastly, we apply the\nisotonic regression discontinuity designs to estimate the causal effect of\nincumbency in the U.S. House elections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:44:44 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 20:57:46 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 20:23:32 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 21:55:47 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 17:22:30 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2020 19:53:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""], ["Kumar", "Rohit", ""]]}, {"id": "1908.05818", "submitter": "Bharath Sriperumbudur", "authors": "Samory Kpotufe and Bharath K. Sriperumbudur", "title": "Gaussian Sketching yields a J-L Lemma in RKHS", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of the paper is to show that Gaussian sketching of a\nkernel-Gram matrix $\\boldsymbol K$ yields an operator whose counterpart in an\nRKHS $\\mathcal H$, is a \\emph{random projection} operator---in the spirit of\nJohnson-Lindenstrauss (J-L) lemma. To be precise, given a random matrix $Z$\nwith i.i.d. Gaussian entries, we show that a sketch $Z\\boldsymbol{K}$\ncorresponds to a particular random operator in (infinite-dimensional) Hilbert\nspace $\\mathcal H$ that maps functions $f \\in \\mathcal H$ to a low-dimensional\nspace $\\mathbb R^d$, while preserving a weighted RKHS inner-product of the form\n$\\langle f, g \\rangle_{\\Sigma} \\doteq \\langle f, \\Sigma^3 g \\rangle_{\\mathcal\nH}$, where $\\Sigma$ is the \\emph{covariance} operator induced by the data\ndistribution. In particular, under similar assumptions as in kernel PCA (KPCA),\nor kernel $k$-means (K-$k$-means), well-separated subsets of feature-space\n$\\{K(\\cdot, x): x \\in \\cal X\\}$ remain well-separated after such operation,\nwhich suggests similar benefits as in KPCA and/or K-$k$-means, albeit at the\nmuch cheaper cost of a random projection. In particular, our convergence rates\nsuggest that, given a large dataset $\\{X_i\\}_{i=1}^N$ of size $N$, we can build\nthe Gram matrix $\\boldsymbol K$ on a much smaller subsample of size $n\\ll N$,\nso that the sketch $Z\\boldsymbol K$ is very cheap to obtain and subsequently\napply as a projection operator on the original data $\\{X_i\\}_{i=1}^N$. We\nverify these insights empirically on synthetic data, and on real-world\nclustering applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 02:36:25 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 01:58:48 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Kpotufe", "Samory", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1908.05871", "submitter": "Peter Math\\'e", "authors": "Peter Math\\'e and M. Thamban Nair and Bernd Hofmann", "title": "Regularization of linear ill-posed problems involving multiplication\n  operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regularization of ill-posed equations involving multiplication\noperators when the multiplier function is positive almost everywhere and zero\nis an accumulation point of the range of this function. Such equations\nnaturally arise from equations based on non-compact self-adjoint operators in\nHilbert space, after applying unitary transformations arising out of the\nspectral theorem. For classical regularization theory, when noisy observations\nare given and the noise is deterministic and bounded, then non-compactness of\nthe ill-posed equations is a minor issue. However, for statistical ill-posed\nequations with non-compact operators less is known if the data are blurred by\nwhite noise. We develop a regularization theory with emphasis on this case. In\nthis context, we highlight several aspects, in particular we discuss the\nintrinsic degree of ill-posedness in terms of rearrangements of the multiplier\nfunction. Moreover, we address the required modifications of classical\nregularization schemes in order to be used for non-compact statistical\nproblems, and we also introduce the concept of the effective ill-posedness of\nthe operator equation under white noise. This study is concluded with\nprototypical examples for such equations, as these are deconvolution equations\nand certain final value problems in evolution equations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 07:12:36 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Math\u00e9", "Peter", ""], ["Nair", "M. Thamban", ""], ["Hofmann", "Bernd", ""]]}, {"id": "1908.05896", "submitter": "Ruby Chanchal", "authors": "Ruby Chanchal, Vaishali Gupta, Amit Kumar Misra", "title": "Stochastic Comparisons of Series and Parallel Systems with Topp-Leone\n  Generated Family of Distributions", "comments": "2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we stochastically compare the series and parallel systems\nhaving Topp Leone generated family of distributions. We consider that the\nlifetimes of the components of the systems have either the different shape\nparameters when the scale parameters are fixed or the different scale\nparameters when the shape parameters are fixed and established some ordering\nresults With the help of vector majorization technique.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:03:34 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Chanchal", "Ruby", ""], ["Gupta", "Vaishali", ""], ["Misra", "Amit Kumar", ""]]}, {"id": "1908.06130", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler", "title": "Average-Case Lower Bounds for Learning Sparse Mixtures, Robust\n  Estimation and Semirandom Adversaries", "comments": "Preliminary version (subsumed by expanded version at\n  arXiv:2005.08099), 65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops several average-case reduction techniques to show new\nhardness results for three central high-dimensional statistics problems,\nimplying a statistical-computational gap induced by robustness, a\ndetection-recovery gap and a universality principle for these gaps. A main\nfeature of our approach is to map to these problems via a common intermediate\nproblem that we introduce, which we call Imbalanced Sparse Gaussian Mixtures.\nWe assume the planted clique conjecture for a version of the planted clique\nproblem where the position of the planted clique is mildly constrained, and\nfrom this obtain the following computational lower bounds: (1) a $k$-to-$k^2$\nstatistical-computational gap for robust sparse mean estimation, providing the\nfirst average-case evidence for a conjecture of Li (2017) and Balakrishnan et\nal. (2017); (2) a tight lower bound for semirandom planted dense subgraph,\nwhich shows that a semirandom adversary shifts the detection threshold in\nplanted dense subgraph to the conjectured recovery threshold; and (3) a\nuniversality principle for $k$-to-$k^2$ gaps in a broad class of sparse mixture\nproblems that includes many natural formulations such as the spiked covariance\nmodel.\n  Our main approach is to introduce several average-case techniques to produce\nstructured and Gaussianized versions of an input graph problem, and then to\nrotate these high-dimensional Gaussians by matrices carefully constructed from\nhyperplanes in $\\mathbb{F}_r^t$. For our universality result, we introduce a\nnew method to perform an algorithmic change of measure tailored to sparse\nmixtures. We also provide evidence that the mild promise in our variant of\nplanted clique does not change the complexity of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 22:14:09 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 01:20:43 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""]]}, {"id": "1908.06208", "submitter": "Wenpin Tang", "authors": "Wenpin Tang, Yuting Ye", "title": "The existence of maximum likelihood estimate in high-dimensional binary\n  response generalized linear models", "comments": "23 pages, 7 figures, 1 table. This paper is published by\n  https://projecteuclid.org/euclid.ejs/1604044928", "journal-ref": "Electronic Journal of Statistics (2020) vol.14, no.2, 4028-4053", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent works on the high-dimensional logistic regression, we\nestablish that the existence of the maximum likelihood estimate exhibits a\nphase transition for a wide range of generalized linear models with binary\noutcome and elliptical covariates. This extends a previous result of Cand\\`es\nand Sur who proved the phase transition for the logistic regression with\nGaussian covariates. Our result reveals a rich structure in the phase\ntransition phenomenon, which is simply overlooked by Gaussianity. The main\ntools for deriving the result are data separation, convex geometry and\nstochastic approximation. We also conduct simulation studies to corroborate our\ntheoretical findings, and explore other features of the problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 00:04:49 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 06:06:21 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Tang", "Wenpin", ""], ["Ye", "Yuting", ""]]}, {"id": "1908.06431", "submitter": "Guan'ao Yan", "authors": "Jun Zhao, Guan'ao Yan and Yi Zhang", "title": "Semiparametric Expectile Regression for High-dimensional Heavy-tailed\n  and Heterogeneous Data", "comments": "arXiv admin note: text overlap with arXiv:1601.06000 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, high-dimensional heterogeneous data have attracted a lot of\nattention and discussion. Under heterogeneity, semiparametric regression is a\npopular choice to model data in statistics. In this paper, we take advantages\nof expectile regression in computation and analysis of heterogeneity, and\npropose the regularized partially linear additive expectile regression with\nnonconvex penalty, for example, SCAD or MCP for such high-dimensional\nheterogeneous data. We focus on a more realistic scenario: the regression error\nis heavy-tailed distributed and only has finite moments, which is violated with\nthe classical sub-gaussian distribution assumption and more common in practise.\nUnder some regular conditions, we show that with probability tending to one,\nthe oracle estimator is one of the local minima of our optimization problem.\nThe theoretical study indicates that the dimension cardinality of linear\ncovariates our procedure can handle with is essentially restricted by the\nmoment condition of the regression error. For computation, since the\ncorresponding optimization problem is nonconvex and nonsmooth, we derive a\ntwo-step algorithm to solve this problem. Finally, we demonstrate that the\nproposed method enjoys good performances in estimation accuracy and model\nselection through Monto Carlo simulation studies and a real data example.\nWhat's more, by taking different expectile weights $\\alpha$, we are able to\ndetect heterogeneity and explore the entire conditional distribution of the\nresponse variable, which indicates the usefulness of our proposed method for\nanalyzing high dimensional heterogeneous data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 12:27:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhao", "Jun", ""], ["Yan", "Guan'ao", ""], ["Zhang", "Yi", ""]]}, {"id": "1908.06456", "submitter": "Steffen Lauritzen", "authors": "Steffen Lauritzen", "title": "Harmonic Analysis of Symmetric Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note attempts to understand graph limits as defined by Lovasz and\nSzegedy (2006)} in terms of harmonic analysis on semigroups. This is done by\nrepresenting probability distributions of random exchangeable graphs as\nmixtures of characters on the semigroup of unlabeled graphs with node-disjoint\nunion, thereby providing an alternative derivation of de Finetti's theorem for\nrandom exchangeable graphs.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 15:02:40 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 09:11:08 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lauritzen", "Steffen", ""]]}, {"id": "1908.06459", "submitter": "Daniel Jerison", "authors": "Daniel C. Jerison", "title": "Quantitative convergence rates for reversible Markov chains via strong\n  random times", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X_t)$ be a discrete time Markov chain on a general state space. It is\nwell-known that if $(X_t)$ is aperiodic and satisfies a drift and minorization\ncondition, then it converges to its stationary distribution $\\pi$ at an\nexponential rate. We consider the problem of computing upper bounds for the\ndistance from stationarity in terms of the drift and minorization data.\n  Baxendale showed that these bounds improve significantly if one assumes that\n$(X_t)$ is reversible with nonnegative eigenvalues (i.e. its transition kernel\nis a self-adjoint operator on $L^2(\\pi)$ with spectrum contained in $[0,1]$).\nWe identify this phenomenon as a special case of a general principle: for a\nreversible chain with nonnegative eigenvalues, any strong random time gives\ndirect control over the convergence rate. We formulate this principle precisely\nand deduce from it a stronger version of Baxendale's result.\n  Our approach is fully quantitative and allows us to convert drift and\nminorization data into explicit convergence bounds. We show that these bounds\nare tighter than those of Rosenthal and Baxendale when applied to a\nwell-studied example.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 15:13:22 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Jerison", "Daniel C.", ""]]}, {"id": "1908.06496", "submitter": "Patric Bonnier", "authors": "Patric Bonnier, Harald Oberhauser", "title": "Signature Cumulants, Ordered Partitions, and Independence of Stochastic\n  Processes", "comments": "31 pages, 2 figures", "journal-ref": "Bernoulli, Volume 26, Number 4 (2020), 2727-2757", "doi": "10.3150/20-BEJ1205", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequence of so-called signature moments describes the laws of many\nstochastic processes in analogy with how the sequence of moments describes the\nlaws of vector-valued random variables. However, even for vector-valued random\nvariables, the sequence of cumulants is much better suited for many tasks than\nthe sequence of moments. This motivates us to study so-called signature\ncumulants. To do so, we develop an elementary combinatorial approach and show\nthat in the same way that cumulants relate to the lattice of partitions,\nsignature cumulants relate to the lattice of so-called \"ordered partitions\". We\nuse this to give a new characterisation of independence of multivariate\nstochastic processes; finally we construct a family of unbiased\nminimum-variance estimators of signature cumulants.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 18:31:44 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 12:37:12 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bonnier", "Patric", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1908.06600", "submitter": "Deepak Nag Ayyala", "authors": "Deepak Nag Ayyala", "title": "High dimensional statistical inference: theoretical development to data\n  analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is due to appear in the Handbook of Statistics, Vol. 43,\nElsevier/North-Holland, Amsterdam, edited by Arni S. R. Srinivasa Rao and C. R.\nRao.\n  In modern day analytics, there is ever growing need to develop statistical\nmodels to study high dimensional data. Between dimension reduction,\nasymptotics-driven methods and random projection based methods, there are\nseveral approaches developed so far. For high dimensional parametric models,\nestimation and hypothesis testing for mean and covariance matrices have been\nextensively studied. However, practical implementation of these methods are\nfairly limited and are primarily restricted to researchers involved in high\ndimensional inference. With several applied fields such as genomics,\nmetagenomics and social networking, high dimensional inference is a key\ncomponent of big data analytics. In this chapter, a comprehensive overview of\nhigh dimensional inference and its applications in data analytics is provided.\nKey theoretical developments and computational tools are presented, giving\nreaders an in-depth understanding of challenges in big data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:42:31 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ayyala", "Deepak Nag", ""]]}, {"id": "1908.06602", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Mar\\'ia F. Gil-Leyva, Rams\\'es H. Mena, Theodoros Nicoleris", "title": "Beta-Binomial stick-breaking non-parametric prior", "comments": "26 pages, 10 figures", "journal-ref": "Electron. J. Statist., Volume 14, Number 1 (2020), 1479-1507", "doi": "10.1214/20-EJS1694", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of nonparametric prior distributions, termed Beta-Binomial\nstick-breaking process, is proposed. By allowing the underlying length random\nvariables to be dependent through a Beta marginals Markov chain, an appealing\ndiscrete random probability measure arises. The chain's dependence parameter\ncontrols the ordering of the stick-breaking weights, and thus tunes the model's\nlabel-switching ability. Also, by tuning this parameter, the resulting class\ncontains the Dirichlet process and the Geometric process priors as particular\ncases, which is of interest for fast convergence of MCMC implementations. Some\nproperties of the model are discussed and a density estimation algorithm is\nproposed and tested with simulated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:48:11 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 01:42:28 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Gil-Leyva", "Mar\u00eda F.", ""], ["Mena", "Rams\u00e9s H.", ""], ["Nicoleris", "Theodoros", ""]]}, {"id": "1908.06735", "submitter": "Anne Philippe", "authors": "Mohamedou Ould Haye, Anne Philippe (LMJL), Caroline Robet (LMJL)", "title": "Inference for continuous-time long memory randomly sampled processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a continuous-time long memory stochastic process, a discrete-time\nrandomly sampled one is drawn. We investigate the second-order properties of\nthis process and establish some time-and frequency-domain asymptotic results.\nWe mainly focus on the case when the initial process is Gaussian. The challenge\nbeing that, although marginally remains Gaussian, the randomly sampled process\nwill no longer be jointly Gaussian.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:26:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Haye", "Mohamedou Ould", "", "LMJL"], ["Philippe", "Anne", "", "LMJL"], ["Robet", "Caroline", "", "LMJL"]]}, {"id": "1908.06852", "submitter": "Clement Benard", "authors": "Cl\\'ement B\\'enard (LPSM (UMR\\_8001)), G\\'erard Biau (LPSM\n  (UMR\\_8001)), S\\'ebastien da Veiga, Erwan Scornet (CMAP)", "title": "SIRUS: Stable and Interpretable RUle Set for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art learning algorithms, such as random forests or neural\nnetworks, are often qualified as \"black-boxes\" because of the high number and\ncomplexity of operations involved in their prediction mechanism. This lack of\ninterpretability is a strong limitation for applications involving critical\ndecisions, typically the analysis of production processes in the manufacturing\nindustry. In such critical contexts, models have to be interpretable, i.e.,\nsimple, stable, and predictive. To address this issue, we design SIRUS (Stable\nand Interpretable RUle Set), a new classification algorithm based on random\nforests, which takes the form of a short list of rules. While simple models are\nusually unstable with respect to data perturbation, SIRUS achieves a remarkable\nstability improvement over cutting-edge methods. Furthermore, SIRUS inherits a\npredictive accuracy close to random forests, combined with the simplicity of\ndecision trees. These properties are assessed both from a theoretical and\nempirical point of view, through extensive numerical experiments based on our\nR/C++ software implementation sirus available from CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:55:47 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 13:16:57 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 08:09:14 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 12:51:34 GMT"}, {"version": "v5", "created": "Wed, 16 Dec 2020 10:52:20 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["B\u00e9nard", "Cl\u00e9ment", "", "LPSM"], ["Biau", "G\u00e9rard", "", "LPSM"], ["da Veiga", "S\u00e9bastien", "", "CMAP"], ["Scornet", "Erwan", "", "CMAP"]]}, {"id": "1908.06907", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Probability Estimation with Truncated Inverse Binomial Sampling", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SY eess.SY stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a general theory of truncated inverse binomial\nsampling. In this theory, the fixed-size sampling and inverse binomial sampling\nare accommodated as special cases. In particular, the classical\nChernoff-Hoeffding bound is an immediate consequence of the theory. Moreover,\nwe propose a rigorous and efficient method for probability estimation, which is\nan adaptive Monte Carlo estimation method based on truncated inverse binomial\nsampling. Our proposed method of probability estimation can be orders of\nmagnitude more efficient as compared to existing methods in literature and\nwidely used software.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:08:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1908.06934", "submitter": "Guillaume Marrelec", "authors": "Guillaume Marrelec, Alain Giron", "title": "Cumulants of multiinformation density in the case of a multivariate\n  normal distribution", "comments": null, "journal-ref": "Statistics and Probability Letters 156, 108587 (2020)", "doi": "10.1016/j.spl.2019.108587", "report-no": null, "categories": "math.ST cs.IT math.IT stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalization of information density to a partitioning into $N\n\\geq 2$ subvectors. We calculate its cumulant-generating function and its\ncumulants, showing that these quantities are only a function of all the\nregression coefficients associated with the partitioning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:12:54 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 10:39:14 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Marrelec", "Guillaume", ""], ["Giron", "Alain", ""]]}, {"id": "1908.07021", "submitter": "Tobias Fritz", "authors": "Tobias Fritz", "title": "A synthetic approach to Markov kernels, conditional independence and\n  theorems on sufficient statistics", "comments": "98 pages. v6: fixed error in Section 7. v7: incorporates referee's\n  comments. v8: minor correction", "journal-ref": "Adv. Math. 370, 107239 (2020)", "doi": "10.1016/j.aim.2020.107239", "report-no": null, "categories": "math.ST cs.LO math.CT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Markov categories as a framework for synthetic probability and\nstatistics, following work of Golubtsov as well as Cho and Jacobs. This means\nthat we treat the following concepts in purely abstract categorical terms:\nconditioning and disintegration; various versions of conditional independence\nand its standard properties; conditional products; almost surely; sufficient\nstatistics; versions of theorems on sufficient statistics due to\nFisher--Neyman, Basu, and Bahadur.\n  Besides the conceptual clarity offered by our categorical setup, its main\nadvantage is that it provides a uniform treatment of various types of\nprobability theory, including discrete probability theory, measure-theoretic\nprobability with general measurable spaces, Gaussian probability, stochastic\nprocesses of either of these kinds, and many others.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:54:09 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 02:57:58 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:25:47 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 22:37:03 GMT"}, {"version": "v5", "created": "Mon, 24 Feb 2020 01:17:05 GMT"}, {"version": "v6", "created": "Tue, 31 Mar 2020 16:16:30 GMT"}, {"version": "v7", "created": "Tue, 28 Apr 2020 17:14:02 GMT"}, {"version": "v8", "created": "Sun, 31 May 2020 18:29:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fritz", "Tobias", ""]]}, {"id": "1908.07145", "submitter": "Atsushi Iwasaki", "authors": "Atsushi Iwasaki", "title": "Independent Randomness Tests based on the Orthogonalized Non-overlapping\n  Template Matching Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, randomness tests included in a test suite are not independent of\neach other. This renders it difficult to fix a rational criterion through the\nwhole test suite with an explicit significance level. In this paper, we focus\non the Non-overlapping Template Matching Test, which is a randomness test\nincluded in the NIST statistical test suite. The test uses a parameter called\n\"template\" and we can consider a test item for each template. We investigate\ndependency between two test items by deriving the joint probability density\nfunction of the two p-values and propose a transformation to make multi test\nitems independent of each other.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 03:34:56 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Iwasaki", "Atsushi", ""]]}, {"id": "1908.07186", "submitter": "Lancelot F. James", "authors": "Lanelot F. James", "title": "Stick-breaking Pitman-Yor processes given the species sampling size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random discrete distributions, say $F,$ known as species sampling models,\nrepresent a rich class of models for classification and clustering, in Bayesian\nstatistics and machine learning. They also arise in various areas of\nprobability and its applications. Jim Pitman, within the species sampling\ncontext, shows that mixed Poisson processes may be interpreted as the sample\nsize up till a given time or in terms of waiting times of appearance of\nindividuals to be classified. He notes connections to some recent work in the\nBayesian statistic/machine learning literature, with some more classical\nresults. We let $F:=F_{\\alpha,\\theta},$ be a Pitman-Yor process for $\\alpha\\in\n(0,1),$ and $\\theta>-\\alpha,$ with $\\alpha$-diversity equivalent in\ndistribution to $S^{-\\alpha}_{\\alpha,\\theta},$ and let\n$(N_{S_{\\alpha,\\theta}}(\\lambda),\\lambda\\ge 0)$ denote a mixed Poisson process\nwith rate $S_{\\alpha,\\theta}.$ In this paper we derive explicit stick-breaking\nrepresentations of $F_{\\alpha,\\theta}$ given\n$N_{S_{\\alpha,\\theta}}(\\lambda)=m.$ More precisely, if $(P_{\\ell})\\sim\n\\mathrm{PD}(\\alpha,\\theta)$, denotes a ranked sequence following the two\nparameter Poisson-Dirichlet distribution, we obtain explicit representations of\nthe sized biased permutation of $(P_{\\ell})|N_{S_{\\alpha,\\theta}}(\\lambda)=m.$\nDue to distributional results we shall develop in a more general context, it\nsuffices to consider the stable case $F_{\\alpha,0}|N_{S_{\\alpha}}(\\lambda)=m.$\nNotably, it follows that $F_{\\alpha,0}|N_{S_{\\alpha}}(\\lambda)=0,$ is\nequivalent in distribution to the popular normalized generalized gamma process.\nHence, we obtain explicit stick-breaking representations for the generalized\ngamma class as a special case.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:33:02 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["James", "Lanelot F.", ""]]}, {"id": "1908.07301", "submitter": "Jos\\'e Ant\\'onio Ferreira", "authors": "Jos\\'e A. Ferreira", "title": "Causality from the Point of View of Statistics", "comments": "86 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a basis for studying questions of cause and effect in statistics\nwhich subsumes and reconciles the models proposed by Pearl, Robins, Rubin and\nothers, and which, as far as mathematical notions and notation are concerned,\nis entirely conventional. In particular, we show that, contrary to what several\nauthors had thought, standard probability can be used to treat problems that\ninvolve notions of causality, and in a way not essentially different from the\nway it has been used in the area generally known (since the 1960s, at least) as\n'applied probability'. Conventional, elementary proofs are given of some of the\nmost important results obtained by the various schools of 'statistical\ncausality', and a variety of examples considered by those schools are worked\nout in detail. Pearl's 'calculus of intervention' is examined anew, and its\nfirst two rules are formulated and proved by means of elementary probability\nfor the first time since they were stated 25 years or so ago.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:29:57 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 08:19:22 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 11:46:02 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 11:02:23 GMT"}, {"version": "v5", "created": "Wed, 10 Mar 2021 15:42:40 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ferreira", "Jos\u00e9 A.", ""]]}, {"id": "1908.07444", "submitter": "Jaewhi Park", "authors": "Jinwoong Kwak, Ji Oon Lee, Jaewhi Park", "title": "Extremal eigenvalues of sample covariance matrices with general\n  population", "comments": "arXiv admin note: text overlap with arXiv:1310.7057", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the eigenvalues of sample covariance matrices of the form\n$\\mathcal{Q}=(\\Sigma^{1/2}X)(\\Sigma^{1/2}X)^*$. The sample $X$ is an $M\\times\nN$ rectangular random matrix with real independent entries and the population\ncovariance matrix $\\Sigma$ is a positive definite diagonal matrix independent\nof $X$. Assuming that the limiting spectral density of $\\Sigma$ exhibits convex\ndecay at the right edge of the spectrum, in the limit $M, N \\to \\infty$ with\n$N/M \\to d\\in(0,\\infty)$, we find a certain threshold $d_+$ such that for\n$d>d_+$ the limiting spectral distribution of $\\mathcal{Q}$ also exhibits\nconvex decay at the right edge of the spectrum. In this case, the largest\neigenvalues of $\\mathcal{Q}$ are determined by the order statistics of the\neigenvalues of $\\Sigma$, and in particular, the limiting distribution of the\nlargest eigenvalue of $\\mathcal{Q}$ is given by a Weibull distribution. In case\n$d<d_+$, we also prove that the limiting distribution of the largest eigenvalue\nof $\\caQ$ is Gaussian if the entries of $\\Sigma$ are i.i.d. random variables.\nWhile $\\Sigma$ is considered to be random mostly, the results also hold for\ndeterministic $\\Sigma$ with some additional assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 07:52:49 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 07:32:11 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 04:29:58 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 06:57:04 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Kwak", "Jinwoong", ""], ["Lee", "Ji Oon", ""], ["Park", "Jaewhi", ""]]}, {"id": "1908.07456", "submitter": "Eni Musta", "authors": "C\\'ecile Durot and Eni Musta", "title": "Results on standard estimators in the Cox model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Cox regression model and prove some properties of the maximum\npartial likelihood estimator $\\hat\\beta_n$ and of the the Breslow estimator\n$\\Lambda_n$. The asymptotic properties of these estimators have been widely\nstudied in the literature but we are not aware of a reference where it is shown\nthat they have uniformly bounded moments. These results are needed, for\nexample, when studying global errors of shape restricted estimators of the\nbaseline hazard function.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:57:34 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:19:48 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Durot", "C\u00e9cile", ""], ["Musta", "Eni", ""]]}, {"id": "1908.07460", "submitter": "Haolei Weng", "authors": "Jianqing Fan, Haolei Weng, Yifeng Zhou", "title": "Optimal estimation of functionals of high-dimensional mean and\n  covariance matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by portfolio allocation and linear discriminant analysis, we\nconsider estimating a functional $\\mathbf{\\mu}^T \\mathbf{\\Sigma}^{-1}\n\\mathbf{\\mu}$ involving both the mean vector $\\mathbf{\\mu}$ and covariance\nmatrix $\\mathbf{\\Sigma}$. We study the minimax estimation of the functional in\nthe high-dimensional setting where $\\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}$ is\nsparse. Akin to past works on functional estimation, we show that the optimal\nrate for estimating the functional undergoes a phase transition between regular\nparametric rate and some form of high-dimensional estimation rate. We further\nshow that the optimal rate is attained by a carefully designed plug-in\nestimator based on de-biasing, while a family of naive plug-in estimators are\nproved to fall short. We further generalize the estimation problem and\ntechniques that allow robust inputs of mean and covariance matrix estimators.\nExtensive numerical experiments lend further supports to our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 16:00:57 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:21:08 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Fan", "Jianqing", ""], ["Weng", "Haolei", ""], ["Zhou", "Yifeng", ""]]}, {"id": "1908.07585", "submitter": "Jun Yang", "authors": "Jun Yang and Shengyang Sun and Daniel M. Roy", "title": "Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher\n  Processes", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The developments of Rademacher complexity and PAC-Bayesian theory have been\nlargely independent. One exception is the PAC-Bayes theorem of Kakade,\nSridharan, and Tewari (2008), which is established via Rademacher complexity\ntheory by viewing Gibbs classifiers as linear operators. The goal of this paper\nis to extend this bridge between Rademacher complexity and state-of-the-art\nPAC-Bayesian theory. We first demonstrate that one can match the fast rate of\nCatoni's PAC-Bayes bounds (Catoni, 2007) using shifted Rademacher processes\n(Wegkamp, 2003; Lecu\\'{e} and Mitchell, 2012; Zhivotovskiy and Hanneke, 2018).\nWe then derive a new fast-rate PAC-Bayes bound in terms of the \"flatness\" of\nthe empirical risk surface on which the posterior concentrates. Our analysis\nestablishes a new framework for deriving fast-rate PAC-Bayes bounds and yields\nnew insights on PAC-Bayesian theory.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:46:14 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 16:12:50 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Yang", "Jun", ""], ["Sun", "Shengyang", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1908.07636", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "How to gamble with non-stationary $\\mathcal{X}$-armed bandits and have\n  no regrets", "comments": "The algorithm is optimized, the theoretical result is more detailed\n  now", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In $\\mathcal{X}$-armed bandit problem an agent sequentially interacts with\nenvironment which yields a reward based on the vector input the agent provides.\nThe agent's goal is to maximise the sum of these rewards across some number of\ntime steps. The problem and its variations have been a subject of numerous\nstudies, suggesting sub-linear and some times optimal strategies. The given\npaper introduces a novel variation of the problem. We consider an environment,\nwhich can abruptly change its behaviour an unknown number of times. To that end\nwe propose a novel strategy and prove it attains sub-linear cumulative regret.\nMoreover, in case of highly smooth relation between an action and the\ncorresponding reward, the method is nearly optimal. The theoretical result are\nsupported by experimental study.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:33:02 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 08:52:27 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 13:04:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "1908.07645", "submitter": "R W R Darling Ph. D.", "authors": "Jacob D. Baron, R. W. R. Darling", "title": "K-Nearest Neighbor Approximation Via the Friend-of-a-Friend Principle", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose $V$ is an $n$-element set where for each $x \\in V$, the elements of\n$V \\setminus \\{x\\}$ are ranked by their similarity to $x$. The $K$-nearest\nneighbor graph is a directed graph including an arc from each $x$ to the $K$\npoints of $V \\setminus \\{x\\}$ most similar to $x$. Constructive approximation\nto this graph using far fewer than $n^2$ comparisons is important for the\nanalysis of large high-dimensional data sets. $K$-Nearest Neighbor Descent is a\nparameter-free heuristic where a sequence of graph approximations is\nconstructed, in which second neighbors in one approximation are proposed as\nneighbors in the next. Run times in a test case fit an $O(n K^2 \\log{n})$\npattern. This bound is rigorously justified for a similar algorithm, using\nrange queries, when applied to a homogeneous Poisson process in suitable\ndimension. However the basic algorithm fails to achieve subquadratic complexity\non sets whose similarity rankings arise from a ``generic'' linear order on the\n$\\binom{n}{2}$ inter-point distances in a metric space.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 23:43:24 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 17:16:25 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 21:02:21 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2020 17:43:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Baron", "Jacob D.", ""], ["Darling", "R. W. R.", ""]]}, {"id": "1908.08243", "submitter": "Bernhard Klar", "authors": "Andreas Eberl, Bernhard Klar", "title": "Expectile based measures of skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, quite a few measures have been proposed for quantifying\nthe deviation of a probability distribution from symmetry. The most popular of\nthese skewness measures are based on the third centralized moment and on\nquantiles. However, there are major drawbacks in using these quantities. These\ninclude a strong emphasis on the distributional tails and a poor asymptotic\nbehaviour for the (empirical) moment based measure as well as difficult\nstatistical inference and strange behaviour for discrete distributions for\nquantile based measures.\n  Therefore, in this paper, we introduce skewness measures based on or\nconnected with expectiles. Since expectiles can be seen as smoothed versions of\nquantiles, they preserve the advantages over the moment based measure while not\nexhibiting most of the disadvantages of quantile based measures. We introduce\ncorresponding empirical counterparts and derive asymptotic properties. Finally,\nwe conduct a simulation study, comparing the newly introduced measures with\nestablished ones, and evaluating the performance of the respective estimators.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:08:03 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Eberl", "Andreas", ""], ["Klar", "Bernhard", ""]]}, {"id": "1908.08264", "submitter": "Nikolaus Schweizer", "authors": "Christian Bender and Nikolaus Schweizer", "title": "`Regression Anytime' with Brute-Force SVD Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.OC math.PR q-fin.CP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new least-squares Monte Carlo algorithm for the approximation of\nconditional expectations in the presence of stochastic derivative weights. The\nalgorithm can serve as a building block for solving dynamic programming\nequations, which arise, e.g., in non-linear option pricing problems or in\nprobabilistic discretization schemes for fully non-linear parabolic partial\ndifferential equations. Our algorithm can be generically applied when the\nunderlying dynamics stem from an Euler approximation to a stochastic\ndifferential equation. A built-in variance reduction ensures that the\nconvergence in the number of samples to the true regression function takes\nplace at an arbitrarily fast polynomial rate, if the problem under\nconsideration is smooth enough.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 09:10:45 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 16:14:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bender", "Christian", ""], ["Schweizer", "Nikolaus", ""]]}, {"id": "1908.08320", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid", "title": "Spatial and Spatiotemporal GARCH Models -- A Unified Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-series analyses, particularly for finance, generalized autoregressive\nconditional heteroscedasticity (GARCH) models are widely applied statistical\ntools for modelling volatility clusters (i.e., periods of increased or\ndecreased risk). In contrast, it has not been considered to be of critical\nimportance until now to model spatial dependence in the conditional second\nmoments. Only a few models have been proposed for modelling local clusters of\nincreased risks. In this paper, we introduce novel spatial GARCH and\nexponential GARCH processes in a unified spatial and spatiotemporal GARCH-type\nmodel, which also covers all previously proposed spatial ARCH models as well as\ntime-series GARCH models. For this common modelling framework, estimators are\nderived based on nonlinear least squares and on the maximum-likelihood\napproach. In addition to the theoretical contributions of this paper, we\nsuggest a model selection strategy that is verified by a series of Monte Carlo\nsimulation studies. Eventually, the use of the unified model is demonstrated by\nan empirical example that focuses on real estate prices from 1995 to 2014\nacross the ZIP-Code areas of Berlin. A spatial autoregressive model is applied\nto the data to illustrate how locally varying model uncertainties can be\ncaptured by the spatial GARCH-type models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:29:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:33:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1908.08558", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "Conformal prediction with localization", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method called localized conformal prediction, where we can\nperform conformal inference using only a local region around a new test sample\nto construct its confidence interval. Localized conformal inference is a\nnatural extension to conformal inference. It generalizes the method of\nconformal prediction to the case where we can break the data exchangeability,\nso as to give the test sample a special role. To our knowledge, this is the\nfirst work that introduces such a localization to the framework of conformal\nprediction. We prove that our proposal can also have assumption-free and finite\nsample coverage guarantees, and we compare the behaviors of localized conformal\nprediction and conformal prediction in simulations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 18:37:27 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 20:13:13 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 03:54:21 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "1908.08670", "submitter": "Ningning Xia", "authors": "Moming Wang, Ningning Xia, You Zhou", "title": "On the estimation of high-dimensional integrated covariance matrix based\n  on high-frequency data with multiple transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the mechanism of recording, the presence of multiple transactions at\neach recording time becomes a common feature for high-frequency data in\nfinancial market. Using random matrix theory, this paper considers the\nestimation of integrated covariance (ICV) matrices of high-dimensional\ndiffusion processes based on multiple high-frequency observations. We start by\nstudying the estimator, the time-variation adjusted realized covariance (TVA)\nmatrix, proposed in Zheng and Li (2011) without microstructure noise. We show\nthat in the high-dimensional case, for a class C of diffusion processes, the\nlimiting spectral distribution (LSD) of averaged TVA depends not only on that\nof ICV, but also on the numbers of multiple transactions at each recording\ntime. However, in practice, the observed prices are always contaminated by the\nmarket microstructure noise. Thus the limiting behavior of pre-averaging\naveraged TVA matrices is studied based on the noisy multiple observations. We\nshow that for processes in class C, the pre-averaging averaged TVA has\ndesirable properties that it eliminates the effects of microstructure noise and\nmultiple transactions, and its LSD depends solely on that of the ICV matrix.\nFurther, three types of nonlinear shrinkage estimators of ICV are proposed\nbased on high-frequency noisy multiple observations. Simulation studies support\nour theoretical results and show the finite sample performance of the proposed\nestimators. At last, the high-frequency portfolio strategies are evaluated\nunder these estimators in real data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 05:30:05 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 09:20:47 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Moming", ""], ["Xia", "Ningning", ""], ["Zhou", "You", ""]]}, {"id": "1908.08791", "submitter": "Micha{\\l} Kos", "authors": "Micha{\\l} Kos and Ma{\\l}gorzata Bogdan", "title": "On the asymptotic properties of SLOPE", "comments": "43 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted L-One Penalized Estimator (SLOPE) is a relatively new convex\noptimization procedure for selecting predictors in large data bases. Contrary\nto LASSO, SLOPE has been proved to be asymptotically minimax in the context of\nsparse high-dimensional generalized linear models. Additionally, in case when\nthe design matrix is orthogonal, SLOPE with the sequence of tuning parameters\n$\\lambda^{BH}$, corresponding to the sequence of decaying thresholds for the\nBenjamini-Hochberg multiple testing correction, provably controls False\nDiscovery Rate in the multiple regression model. In this article we provide new\nasymptotic results on the properties of SLOPE when the elements of the design\nmatrix are iid random variables from the Gaussian distribution. Specifically,\nwe provide the conditions, under which the asymptotic FDR of SLOPE based on the\nsequence $\\lambda^{BH}$ converges to zero and the power converges to 1. We\nillustrate our theoretical asymptotic results with extensive simulation study.\nWe also provide precise formulas describing FDR of SLOPE under different loss\nfunctions, which sets the stage for future results on the model selection\nproperties of SLOPE and its extensions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 12:46:14 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 08:35:17 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kos", "Micha\u0142", ""], ["Bogdan", "Ma\u0142gorzata", ""]]}, {"id": "1908.08864", "submitter": "Hengrui Luo", "authors": "Hengrui Luo, Giovanni Nattino, Matthew T. Pratola", "title": "Sparse Additive Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel model for Gaussian process (GP) regression\nin the fully Bayesian setting. Motivated by the ideas of sparsification,\nlocalization and Bayesian additive modeling, our model is built around a\nrecursive partitioning (RP) scheme. Within each RP partition, a sparse GP (SGP)\nregression model is fitted. A Bayesian additive framework then combines\nmultiple layers of partitioned SGPs, capturing both global trends and local\nrefinements with efficient computations. The model addresses both the problem\nof efficiency in fitting a full Gaussian process regression model and the\nproblem of prediction performance associated with a single SGP. Our approach\nmitigates the issue of pseudo-input selection and avoids the need for complex\ninter-block correlations in existing methods. The crucial trade-off becomes\nchoosing between many simpler local model components or fewer complex global\nmodel components, which the practitioner can sensibly tune. Implementation is\nvia a Metropolis-Hasting Markov chain Monte-Carlo algorithm with Bayesian\nback-fitting. We compare our model against popular alternatives on simulated\nand real datasets, and find the performance is competitive, while the fully\nBayesian procedure enables the quantification of model uncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:17:41 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 16:03:48 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Luo", "Hengrui", ""], ["Nattino", "Giovanni", ""], ["Pratola", "Matthew T.", ""]]}, {"id": "1908.08868", "submitter": "Simon Mak", "authors": "Liang Ding, Simon Mak, C. F. Jeff Wu", "title": "BdryGP: a new Gaussian process model for incorporating boundary\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are widely used as surrogate models for emulating\ncomputer code, which simulate complex physical phenomena. In many problems,\nadditional boundary information (i.e., the behavior of the phenomena along\ninput boundaries) is known beforehand, either from governing physics or\nscientific knowledge. While there has been recent work on incorporating\nboundary information within GPs, such models do not provide theoretical\ninsights on improved convergence rates. To this end, we propose a new GP model,\ncalled BdryGP, for incorporating boundary information. We show that BdryGP not\nonly has improved convergence rates over existing GP models (which do not\nincorporate boundaries), but is also more resistant to the\n\"curse-of-dimensionality\" in nonparametric regression. Our proofs make use of a\nnovel connection between GP interpolation and finite-element modeling.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:31:18 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ding", "Liang", ""], ["Mak", "Simon", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1908.08880", "submitter": "Andressa Cerqueira", "authors": "Andressa Cerqueira and Nancy L. Garcia", "title": "Graphical Construction of Spatial Gibbs Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Spatial Gibbs Random Graphs Model that incorporates the\ninterplay between the statistics of the graph and the underlying space where\nthe vertices are located. We propose a graphical construction of a model with\nvertices located in a finite subset of $\\mathbb{Z}^{2}$ that penalizes edges\nbetween distant vertices as well as other structures such as stars or\ntriangles. We prove the existence and uniqueness of a measure defined on graphs\nwith vertices in $\\mathbb{Z}^{2}$ as the limit along the measures over graphs\nwith finite vertex set. Moreover, a perfect simulation algorithm is obtained in\norder to sample a subgraph from the measure defined on graphs with vertex set\n$\\mathbb{Z}^{2}$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:48:33 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Cerqueira", "Andressa", ""], ["Garcia", "Nancy L.", ""]]}, {"id": "1908.09012", "submitter": "Bruce Watson", "authors": "Wen-Chi Kuo, David F. Rodda and Bruce A. Watson", "title": "The H\\'ajek-R\\'enyi-Chow maximal inequality and a strong law of large\n  numbers in Riesz spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the H\\'ajek-R\\'enyi-Chow maximal inequality for\nsubmartingales to $L^p$ type Riesz spaces with conditional expectation\noperators. As applications we obtain a submartingale convergence theorem and a\nstrong law of large numbers in Riesz spaces. Along the way we develop a Riesz\nspace variant of the Clarkson's inequality for $1\\le p\\le 2$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 19:41:18 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kuo", "Wen-Chi", ""], ["Rodda", "David F.", ""], ["Watson", "Bruce A.", ""]]}, {"id": "1908.09114", "submitter": "Yoichi Miyata", "authors": "Yoichi Miyata, Takayuki Shiohama and Toshihiro Abe", "title": "Identifiability of asymmetric circular and cylindrical distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifiability of statistical models is a fundamental and essential\ncondition that is required to prove the consistency of maximum likelihood\nestimators. However, the identifiability of the skew families of distributions\non the circle and cylinder for estimating model parameters has not been fully\ninvestigated in the literature. In this paper, a new method to prove the\nidentifiability of asymmetric circular and cylindrical distributions is\nproposed. Consequently, we prove the identifiability of general sine-skewed\ncircular distributions, including the sine-skewed von Mises and sine-skewed\nwrapped Cauchy distributions and a M\\\"{o}bius transformed distribution, which\ncan be regarded as asymmetric distributions on the unit circle. Moreover, we\nprove the identifiability of some cylindrical distributions wherein the\nmarginal distribution of a circular random variable is represented by a\nsine-skewed distribution. This study facilitates the future research on the\napplicability of our method to finite mixtures of asymmetric circular and\ncylindrical distributions for demonstrating generic identifiability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 09:34:12 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 03:19:44 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 12:45:28 GMT"}, {"version": "v4", "created": "Sun, 16 Aug 2020 07:45:39 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Miyata", "Yoichi", ""], ["Shiohama", "Takayuki", ""], ["Abe", "Toshihiro", ""]]}, {"id": "1908.09326", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin", "title": "Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky\n  Decomposition", "comments": "19 pages, 1 figure", "journal-ref": "SIAM Journal on Matrix Analysis and Applications, 2019, 40(4):\n  1353--1370", "doi": "10.1137/18M1221084", "report-no": null, "categories": "math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Riemannian metric, termed Log-Cholesky metric, on the\nmanifold of symmetric positive definite (SPD) matrices via Cholesky\ndecomposition. We first construct a Lie group structure and a bi-invariant\nmetric on Cholesky space, the collection of lower triangular matrices whose\ndiagonal elements are all positive. Such group structure and metric are then\npushed forward to the space of SPD matrices via the inverse of Cholesky\ndecomposition that is a bijective map between Cholesky space and SPD matrix\nspace. This new Riemannian metric and Lie group structure fully circumvent\nswelling effect, in the sense that the determinant of the Fr\\'echet average of\na set of SPD matrices under the presented metric, called Log-Cholesky average,\nis between the minimum and the maximum of the determinants of the original SPD\nmatrices. Comparing to existing metrics such as the affine-invariant metric and\nLog-Euclidean metric, the presented metric is simpler, more computationally\nefficient and numerically stabler. In particular, parallel transport along\ngeodesics under Log-Cholesky metric is given in a closed and easy-to-compute\nform.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 13:29:17 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lin", "Zhenhua", ""]]}, {"id": "1908.09429", "submitter": "Xin Tong Thomson", "authors": "X. T. Tong and M. Morzfeld and Y. M. Marzouk", "title": "MALA-within-Gibbs samplers for high-dimensional distributions with\n  sparse conditional structure", "comments": "38 ages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) samplers are numerical methods for drawing\nsamples from a given target probability distribution. We discuss one particular\nMCMC sampler, the MALA-within-Gibbs sampler, from the theoretical and practical\nperspectives. We first show that the acceptance ratio and step size of this\nsampler are independent of the overall problem dimension when (i) the target\ndistribution has sparse conditional structure, and (ii) this structure is\nreflected in the partial updating strategy of MALA-within-Gibbs. If, in\naddition, the target density is block-wise log-concave, then the sampler's\nconvergence rate is independent of dimension. From a practical perspective, we\nexpect that MALA-within-Gibbs is useful for solving high-dimensional Bayesian\ninference problems where the posterior exhibits sparse conditional structure at\nleast approximately. In this context, a partitioning of the state that\ncorrectly reflects the sparse conditional structure must be found, and we\nillustrate this process in two numerical examples. We also discuss trade-offs\nbetween the block size used for partial updating and computational requirements\nthat may increase with the number of blocks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:39:32 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:36:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Tong", "X. T.", ""], ["Morzfeld", "M.", ""], ["Marzouk", "Y. M.", ""]]}, {"id": "1908.09440", "submitter": "Mason A. Porter", "authors": "Jane Carlen, Jaume de Dios Pont, Cassidy Mentus, Shyr-Shea Chang,\n  Stephanie Wang, Mason A. Porter", "title": "Role Detection in Bicycle-Sharing Networks Using Multilayer Stochastic\n  Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST nlin.AO physics.soc-ph stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban spatial networks are complex systems with interdependent roles of\nneighborhoods and methods of transportation between them. In this paper, we\nclassify docking stations in bicycle-sharing networks to gain insight into the\nspatial delineations of three major United States cities from human mobility\ndynamics. We propose novel time-dependent stochastic block models, with\ndegree-heterogeneous blocks and either mixed or discrete block membership,\nwhich (1) detect the roles served by bicycle-sharing docking stations and (2)\ndescribe the traffic within and between blocks of stations over the course of a\nday. Our models produce concise descriptions of daily bicycle-sharing usage\npatterns in urban environments. They successfully uncover work and home\ndistricts, and they also reveal dynamics of such districts that are particular\nto each city. When we look for more than two roles, we uncover blocks with\nexpected uses, such as leisure activity, as well as previously unknown\nstructures. Our time-dependent SBMs also reveal how the functional roles of\nbicycle-sharing stations are influenced by surrounding public transportation\ninfrastructure. Our work has direct application to the design and maintenance\nof bicycle-sharing systems, and it can be applied more broadly to community\ndetection in temporal and multilayer networks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 02:35:25 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Carlen", "Jane", ""], ["Pont", "Jaume de Dios", ""], ["Mentus", "Cassidy", ""], ["Chang", "Shyr-Shea", ""], ["Wang", "Stephanie", ""], ["Porter", "Mason A.", ""]]}, {"id": "1908.09617", "submitter": "Piotr Zwiernik", "authors": "Majid M. Al-Sadoon and Piotr Zwiernik", "title": "The Identification Problem for Linear Rational Expectations Models", "comments": "JEL Classification: C10, C22, C32", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the identification of stationary solutions to\nlinear rational expectations models from the second moments of observable data.\nObservational equivalence is characterized and necessary and sufficient\nconditions are provided for: (i) identification under affine restrictions, (ii)\ngeneric identification under affine restrictions of analytically parametrized\nmodels, and (iii) local identification under non-linear restrictions. The\nresults strongly resemble the classical theory for VARMA models although\nsignificant points of departure are also documented.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 12:11:24 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Al-Sadoon", "Majid M.", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1908.09959", "submitter": "Aukosh Jagannath", "authors": "David Gamarnik, Aukosh Jagannath, Subhabrata Sen", "title": "The Overlap Gap Property in Principal Submatrix Recovery", "comments": "42 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study support recovery for a $k \\times k$ principal submatrix with\nelevated mean $\\lambda/N$, hidden in an $N\\times N$ symmetric mean zero\nGaussian matrix. Here $\\lambda>0$ is a universal constant, and we assume $k = N\n\\rho$ for some constant $\\rho \\in (0,1)$. We establish that {there exists a\nconstant $C>0$ such that} the MLE recovers a constant proportion of the hidden\nsubmatrix if $\\lambda {\\geq C} \\sqrt{\\frac{1}{\\rho} \\log \\frac{1}{\\rho}}$,\n{while such recovery is information theoretically impossible if $\\lambda = o(\n\\sqrt{\\frac{1}{\\rho} \\log \\frac{1}{\\rho}} )$}. The MLE is computationally\nintractable in general, and in fact, for $\\rho>0$ sufficiently small, this\nproblem is conjectured to exhibit a \\emph{statistical-computational gap}. To\nprovide rigorous evidence for this, we study the likelihood landscape for this\nproblem, and establish that for some $\\varepsilon>0$ and $\\sqrt{\\frac{1}{\\rho}\n\\log \\frac{1}{\\rho} } \\ll \\lambda \\ll \\frac{1}{\\rho^{1/2 + \\varepsilon}}$, the\nproblem exhibits a variant of the \\emph{Overlap-Gap-Property (OGP)}. As a\ndirect consequence, we establish that a family of local MCMC based algorithms\ndo not achieve optimal recovery. Finally, we establish that for $\\lambda >\n1/\\rho$, a simple spectral method recovers a constant proportion of the hidden\nsubmatrix.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 23:46:07 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 03:09:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gamarnik", "David", ""], ["Jagannath", "Aukosh", ""], ["Sen", "Subhabrata", ""]]}, {"id": "1908.10181", "submitter": "Ahmed Sani", "authors": "Ahmed Sani and loubna Karbil", "title": "Copulas and Preserver Problems", "comments": "Part of a project on probability in educationnal system", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserver problems concern the characterization of operators on general\nspaces that leave invariant some categories of subsets or ratios. The most\nknown in the mathematical literature are those of linear preserver problems\n(LPP) which date back to the ninth century. Here, we treat the preserver\nproblem in the recent and emerging field of copulas. Precisely, we prove that\n\\emph{copula property} is preserved uniquely under increasing transformations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:20:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Sani", "Ahmed", ""], ["Karbil", "loubna", ""]]}, {"id": "1908.10191", "submitter": "Charl Pretorius", "authors": "Sangyeol Lee, Simos G. Meintanis, Charl Pretorius", "title": "Fourier-type monitoring procedures for strict stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model-free monitoring procedures for strict stationarity of a\ngiven time series. The new criteria are formulated as L2-type statistics\nincorporating the empirical characteristic function. Asymptotic as well as\nMonte Carlo results are presented. The new methods are also employed in order\nto test for possible stationarity breaks in time-series data from the financial\nsector.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:36:08 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Lee", "Sangyeol", ""], ["Meintanis", "Simos G.", ""], ["Pretorius", "Charl", ""]]}, {"id": "1908.10292", "submitter": "Alexander Rakhlin", "authors": "Tengyuan Liang and Alexander Rakhlin and Xiyu Zhai", "title": "On the Multiple Descent of Minimum-Norm Interpolants and Restricted\n  Lower Isometry of Kernels", "comments": null, "journal-ref": "Proceedings of the 33rd Conference on Learning Theory 125 (2020)\n  2683-2711", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the risk of minimum-norm interpolants of data in Reproducing Kernel\nHilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape\nfor the various scalings of $d = n^{\\alpha}$, $\\alpha\\in(0,1)$, for the input\ndimension $d$ and sample size $n$. Empirical evidence supports our finding that\nminimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in\nsample size; furthermore, locations of the peaks in our experiments match our\ntheoretical predictions. Since gradient flow on appropriately initialized wide\nneural networks converges to a minimum-norm interpolant with respect to a\ncertain kernel, our analysis also yields novel estimation and generalization\nguarantees for these over-parametrized models.\n  At the heart of our analysis is a study of spectral properties of the random\nkernel matrix restricted to a filtration of eigen-spaces of the population\ncovariance operator, and may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:05:50 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 15:22:45 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""], ["Zhai", "Xiyu", ""]]}, {"id": "1908.10324", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang", "title": "On the Minimax Optimality of Estimating the Wasserstein Metric", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimax optimal rate for estimating the Wasserstein-$1$ metric\nbetween two unknown probability measures based on $n$ i.i.d. empirical samples\nfrom them. We show that estimating the Wasserstein metric itself between\nprobability measures, is not significantly easier than estimating the\nprobability measures under the Wasserstein metric. We prove that the minimax\noptimal rates for these two problems are multiplicatively equivalent, up to a\n$\\log \\log (n)/\\log (n)$ factor.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:56:35 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Liang", "Tengyuan", ""]]}, {"id": "1908.10401", "submitter": "Martin Wendler", "authors": "Alfredas Ra\\v{c}kauskas, Martin Wendler", "title": "Convergence of U-Processes in H\\\"older Spaces with Application to Robust\n  Detection of a Changed Segment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect a changed segment (so called epidemic changes) in a time series,\nvariants of the CUSUM statistic are frequently used. However, they are\nsensitive to outliers in the data and do not perform well for heavy tailed\ndata, especially when short segments get a high weight in the test statistic.\nWe will present a robust test statistic for epidemic changes based on the\nWilcoxon statistic. To study their asymptotic behavior, we prove functional\nlimit theorems for U-processes in H\\\"older spaces. We also study the finite\nsample behavior via simulations and apply the statistic to a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:36:30 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 09:57:44 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ra\u010dkauskas", "Alfredas", ""], ["Wendler", "Martin", ""]]}, {"id": "1908.10478", "submitter": "Tetsuya Kaji", "authors": "Tetsuya Kaji", "title": "Theory of Weak Identification in Semiparametric Models", "comments": "39 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide general formulation of weak identification in semiparametric\nmodels and an efficiency concept. Weak identification occurs when a parameter\nis weakly regular, i.e., when it is locally homogeneous of degree zero. When\nthis happens, consistent or equivariant estimation is shown to be impossible.\nWe then show that there exists an underlying regular parameter that fully\ncharacterizes the weakly regular parameter. While this parameter is not unique,\nconcepts of sufficiency and minimality help pin down a desirable one. If\nestimation of minimal sufficient underlying parameters is inefficient, it\nintroduces noise in the corresponding estimation of weakly regular parameters,\nwhence we can improve the estimators by local asymptotic Rao-Blackwellization.\nWe call an estimator weakly efficient if it does not admit such improvement.\nNew weakly efficient estimators are presented in linear IV and nonlinear\nregression models. Simulation of a linear IV model demonstrates how 2SLS and\noptimal IV estimators are improved.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 21:53:03 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 21:07:08 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 23:25:36 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kaji", "Tetsuya", ""]]}, {"id": "1908.10572", "submitter": "Toru Imai", "authors": "Toru Imai", "title": "On the overestimation of widely applicable Bayesian information\n  criterion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely applicable Bayesian information criterion (Watanabe, 2013) is\napplicable for both regular and singular models in the model selection problem.\nThis criterion tends to overestimate the log marginal likelihood. We identify\nan overestimating term of a widely applicable Bayesian information criterion.\nAdjustment of the term gives an asymptotically unbiased estimator of the\nleading two terms of asymptotic expansion of the log marginal likelihood. In\nnumerical experiments on regular and singular models, the adjustment resulted\nin smaller bias than the original criterion.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:04:39 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Imai", "Toru", ""]]}, {"id": "1908.10628", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Michal Pe\\v{s}ta", "title": "Changepoint in Linear Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear relations, containing measurement errors in input and output data, are\nconsidered. Parameters of these so-called errors-in-variables models can change\nat some unknown moment. The aim is to test whether such an unknown change has\noccurred or not. For instance, detecting a change in trend for a randomly\nspaced time series is a special case of the investigated framework. The\ndesigned changepoint tests are shown to be consistent and involve neither\nnuisance parameters nor tuning constants, which makes the testing procedures\neffortlessly applicable. A changepoint estimator is also introduced and its\nconsistency is proved. A boundary issue is avoided, meaning that the\nchangepoint can be detected when being close to the extremities of the\nobservation regime. As a theoretical basis for the developed methods, a weak\ninvariance principle for the smallest singular value of the data matrix is\nprovided, assuming weakly dependent and non-stationary errors. The results are\npresented in a simulation study, which demonstrates computational efficiency of\nthe techniques. The completely data-driven tests are illustrated through a\ncalibration problem, however, the methodology can be applied to other areas\nsuch as clinical measurements, dietary assessment, computational psychometrics,\nor environmental toxicology as manifested in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 10:27:48 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 23:20:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Pe\u0161ta", "Michal", ""]]}, {"id": "1908.10744", "submitter": "Jonathan Scarlett", "authors": "Zhaoqiang Liu and Jonathan Scarlett", "title": "Information-Theoretic Lower Bounds for Compressive Sensing with\n  Generative Models", "comments": "To appear in IEEE Journal on Selected Areas in Information Theory. In\n  this version, Theorem 7 was updated to allow for general depth/width scalings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that for compressive sensing, significantly fewer\nmeasurements may be required if the sparsity assumption is replaced by the\nassumption the unknown vector lies near the range of a suitably-chosen\ngenerative model. In particular, in (Bora {\\em et al.}, 2017) it was shown\nroughly $O(k\\log L)$ random Gaussian measurements suffice for accurate recovery\nwhen the generative model is an $L$-Lipschitz function with bounded\n$k$-dimensional inputs, and $O(kd \\log w)$ measurements suffice when the\ngenerative model is a $k$-input ReLU network with depth $d$ and width $w$. In\nthis paper, we establish corresponding algorithm-independent lower bounds on\nthe sample complexity using tools from minimax statistical analysis. In\naccordance with the above upper bounds, our results are summarized as follows:\n(i) We construct an $L$-Lipschitz generative model capable of generating\ngroup-sparse signals, and show that the resulting necessary number of\nmeasurements is $\\Omega(k \\log L)$; (ii) Using similar ideas, we construct ReLU\nnetworks with high depth and/or high depth for which the necessary number of\nmeasurements scales as $\\Omega\\big( kd \\frac{\\log w}{\\log n}\\big)$ (with output\ndimension $n$), and in some cases $\\Omega(kd \\log w)$. As a result, we\nestablish that the scaling laws derived in (Bora {\\em et al.}, 2017) are\noptimal or near-optimal in the absence of further assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 14:24:03 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 06:15:58 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Liu", "Zhaoqiang", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "1908.10761", "submitter": "Matthieu Lerasle", "authors": "Matthieu Lerasle", "title": "Lecture Notes: Selected topics on robust statistical learning theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes gather recent results on robust statistical learning theory. The\ngoal is to stress the main principles underlying the construction and\ntheoretical analysis of these estimators rather than provide an exhaustive\naccount on this rapidly growing field. The notes are the basis of lectures\ngiven at the conference StatMathAppli 2019.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 15:00:51 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Lerasle", "Matthieu", ""]]}, {"id": "1908.10935", "submitter": "Yihong Wu", "authors": "Yihong Wu and Harrison H. Zhou", "title": "Randomly initialized EM algorithm for two-component Gaussian mixture\n  achieves near optimality in $O(\\sqrt{n})$ iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the classical EM algorithm for parameter estimation in the\nsymmetric two-component Gaussian mixtures in $d$ dimensions. We show that, even\nin the absence of any separation between components, provided that the sample\nsize satisfies $n=\\Omega(d \\log^3 d)$, the randomly initialized EM algorithm\nconverges to an estimate in at most $O(\\sqrt{n})$ iterations with high\nprobability, which is at most $O((\\frac{d \\log^3 n}{n})^{1/4})$ in Euclidean\ndistance from the true parameter and within logarithmic factors of the minimax\nrate of $(\\frac{d}{n})^{1/4}$. Both the nonparametric statistical rate and the\nsublinear convergence rate are direct consequences of the zero Fisher\ninformation in the worst case. Refined pointwise guarantees beyond worst-case\nanalysis and convergence to the MLE are also shown under mild conditions.\n  This improves the previous result of Balakrishnan et al \\cite{BWY17} which\nrequires strong conditions on both the separation of the components and the\nquality of the initialization, and that of Daskalakis et al \\cite{DTZ17} which\nrequires sample splitting and restarting the EM iteration.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 20:44:19 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Wu", "Yihong", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1908.11048", "submitter": "Hyowon An", "authors": "Hyowon An, Kai Zhang, Hannu Oja and J. S. Marron", "title": "Variable screening based on Gaussian Centered L-moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in big data is identification of important variables.\nIn this paper, we propose methods of discovering variables with non-standard\nunivariate marginal distributions. The conventional moments-based summary\nstatistics can be well-adopted for that purpose, but their sensitivity to\noutliers can lead to selection based on a few outliers rather than\ndistributional shape such as bimodality. To address this type of\nnon-robustness, we consider the L-moments. Using these in practice, however,\nhas a limitation because they do not take zero values at the Gaussian\ndistributions to which the shape of a marginal distribution is most naturally\ncompared. As a remedy, we propose Gaussian Centered L-moments which share\nadvantages of the L-moments but have zeros at the Gaussian distributions. The\nstrength of Gaussian Centered L-moments over other conventional moments is\nshown in theoretical and practical aspects such as their performances in\nscreening important genes in cancer genetics data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 04:50:43 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["An", "Hyowon", ""], ["Zhang", "Kai", ""], ["Oja", "Hannu", ""], ["Marron", "J. S.", ""]]}, {"id": "1908.11070", "submitter": "Olivier Collier", "authors": "Olivier Collier and La\\\"etitia Comminges", "title": "Minimax optimal estimators for general additive functional estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we observe a sparse mean vector through Gaussian noise and we\naim at estimating some additive functional of the mean in the minimax sense.\nMore precisely, we generalize the results of (Collier et al., 2017, 2019) to a\nvery large class of functionals. The optimal minimax rate is shown to depend on\nthe polynomial approximation rate of the marginal functional, and optimal\nestimators achieving this rate are built.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:04:03 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Collier", "Olivier", ""], ["Comminges", "La\u00ebtitia", ""]]}, {"id": "1908.11133", "submitter": "Sophie Langer", "authors": "Michael Kohler and Sophie Langer", "title": "On the rate of convergence of fully connected very deep neural network\n  regression estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in nonparametric regression show that deep learning, i.e.,\nneural network estimates with many hidden layers, are able to circumvent the\nso-called curse of dimensionality in case that suitable restrictions on the\nstructure of the regression function hold. One key feature of the neural\nnetworks used in these results is that their network architecture has a further\nconstraint, namely the network sparsity. In this paper we show that we can get\nsimilar results also for least squares estimates based on simple fully\nconnected neural networks with ReLU activation functions. Here either the\nnumber of neurons per hidden layer is fixed and the number of hidden layers\ntends to infinity suitably fast for sample size tending to infinity, or the\nnumber of hidden layers is bounded by some logarithmic factor in the sample\nsize and the number of neurons per hidden layer tends to infinity suitably fast\nfor sample size tending to infinity. The proof is based on new approximation\nresults concerning deep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:01:20 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:25:19 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 08:40:19 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 12:45:47 GMT"}, {"version": "v5", "created": "Tue, 29 Sep 2020 15:22:58 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Kohler", "Michael", ""], ["Langer", "Sophie", ""]]}, {"id": "1908.11140", "submitter": "Sophie Langer", "authors": "Michael Kohler, Adam Krzyzak and Sophie Langer", "title": "Estimation of a function of low local dimensionality by deep neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) achieve impressive results for complicated tasks\nlike object detection on images and speech recognition. Motivated by this\npractical success, there is now a strong interest in showing good theoretical\nproperties of DNNs. To describe for which tasks DNNs perform well and when they\nfail, it is a key challenge to understand their performance. The aim of this\npaper is to contribute to the current statistical theory of DNNs. We apply DNNs\non high dimensional data and we show that the least squares regression\nestimates using DNNs are able to achieve dimensionality reduction in case that\nthe regression function has locally low dimensionality. Consequently, the rate\nof convergence of the estimate does not depend on its input dimension $d$, but\non its local dimension $d^*$ and the DNNs are able to circumvent the curse of\ndimensionality in case that $d^*$ is much smaller than $d$. In our simulation\nstudy we provide numerical experiments to support our theoretical result and we\ncompare our estimate with other conventional nonparametric regression\nestimates. The performance of our estimates is also validated in experiments\nwith real data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:24:10 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 18:11:26 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 12:10:43 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kohler", "Michael", ""], ["Krzyzak", "Adam", ""], ["Langer", "Sophie", ""]]}, {"id": "1908.11303", "submitter": "Renato Pelessoni", "authors": "Chiara Corsato, Renato Pelessoni and Paolo Vicig", "title": "Nearly-Linear uncertainty measures", "comments": "45 pages", "journal-ref": "International Journal of Approximate Reasoning, 114, pp. 1-28,\n  November 2019", "doi": "10.1016/j.ijar.2019.08.001", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several easy to understand and computationally tractable imprecise\nprobability models, like the Pari-Mutuel model, are derived from a given\nprobability measure P_0. In this paper we investigate a family of such models,\ncalled Nearly-Linear (NL). They generalise a number of well-known models, while\npreserving a simple mathematical structure. In fact, they are linear affine\ntransformations of P_0 as long as the transformation returns a value in [0,1] .\nWe study the properties of NL measures that are (at least) capacities, and show\nthat they can be partitioned into three major subfamilies. We investigate their\nconsistency, which ranges from 2-coherence, the minimal condition satisfied by\nall, to coherence, and the kind of beliefs they can represent. There is a\nvariety of different situations that NL models can incorporate, from\ngeneralisations of the Pari-Mutuel model, the {\\epsilon}-contamination model\nand other models to conflicting attitudes of an agent towards low/high\nP_0-probability events (both prudential and imprudent at the same time), or to\nsymmetry judgments. The consistency properties vary with the beliefs\nrepresented, but not strictly: some conflicting and partly irrational moods may\nbe compatible with coherence. In a final part, we compare NL models with their\nclosest, but only partly overlapping, models, neo-additive capacities and\nprobability intervals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:37:01 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Corsato", "Chiara", ""], ["Pelessoni", "Renato", ""], ["Vicig", "Paolo", ""]]}, {"id": "1908.11405", "submitter": "James P. Crutchfield", "authors": "P. M. Riechers and J. P. Crutchfield", "title": "Fraudulent White Noise: Flat power spectra belie arbitrarily complex\n  processes", "comments": "31 pages, 11 figures; Supplementary Material 24 pages, 5 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/swn.htm", "journal-ref": "Phys. Rev. Research 3, 013170 (2021)", "doi": "10.1103/PhysRevResearch.3.013170", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power spectral densities are a common, convenient, and powerful way to\nanalyze signals. So much so that they are now broadly deployed across the\nsciences and engineering---from quantum physics to cosmology, and from\ncrystallography to neuroscience to speech recognition. The features they reveal\nnot only identify prominent signal-frequencies but also hint at mechanisms that\ngenerate correlation and lead to resonance. Despite their near-centuries-long\nrun of successes in signal analysis, here we show that flat power spectra can\nbe generated by highly complex processes, effectively hiding all inherent\nstructure in complex signals. Historically, this circumstance has been widely\nmisinterpreted, being taken as the renowned signature of \"structureless\" white\nnoise---the benchmark of randomness. We argue, in contrast, to the extent that\nmost real-world complex systems exhibit correlations beyond pairwise statistics\ntheir structures evade power spectra and other pairwise statistical measures.\nAs concrete physical examples, we demonstrate that fraudulent white noise hides\nthe predictable structure of both entangled quantum systems and chaotic\ncrystals.\n  To make these words of warning operational, we present constructive results\nthat explore how this situation comes about and the high toll it takes in\nunderstanding complex mechanisms. First, we give the closed-form solution for\nthe power spectrum of a very broad class of structurally-complex signal\ngenerators. Second, we demonstrate the close relationship between eigen-spectra\nof evolution operators and power spectra. Third, we characterize the minimal\ngenerative structure implied by any power spectrum. Fourth, we show how to\nconstruct arbitrarily complex processes with flat power spectra. Finally,\nleveraging this diagnosis of the problem, we point the way to developing more\nincisive tools for discovering structure in complex signals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 18:15:08 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 20:44:07 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Riechers", "P. M.", ""], ["Crutchfield", "J. P.", ""]]}, {"id": "1908.11548", "submitter": "Thomas Whitaker Mr", "authors": "Thomas Whitaker, Boris Beranger and Scott A. Sisson", "title": "Composite likelihood methods for histogram-valued random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data analysis has been proposed as a technique for summarising large\nand complex datasets into a much smaller and tractable number of distributions\n-- such as random rectangles or histograms -- each describing a portion of the\nlarger dataset. Recent work has developed likelihood-based methods that permit\nfitting models for the underlying data while only observing the distributional\nsummaries. However, while powerful, when working with random histograms this\napproach rapidly becomes computationally intractable as the dimension of the\nunderlying data increases. We introduce a composite-likelihood variation of\nthis likelihood-based approach for the analysis of random histograms in $K$\ndimensions, through the construction of lower-dimensional marginal histograms.\nThe performance of this approach is examined through simulated and real data\nanalysis of max-stable models for spatial extremes using millions of observed\ndatapoints in more than $K=100$ dimensions. Large computational savings are\navailable compared to existing model fitting approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 06:20:49 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 05:39:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Whitaker", "Thomas", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}]