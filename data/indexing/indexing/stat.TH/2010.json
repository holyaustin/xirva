[{"id": "2010.00064", "submitter": "Ayush Jain", "authors": "Ayush Jain, Alon Orlitsky", "title": "Linear-Sample Learning of Low-Rank Distributions", "comments": "Accepted for Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many latent-variable applications, including community detection,\ncollaborative filtering, genomic analysis, and NLP, model data as generated by\nlow-rank matrices. Yet despite considerable research, except for very special\ncases, the number of samples required to efficiently recover the underlying\nmatrices has not been known. We determine the onset of learning in several\ncommon latent-variable settings. For all of them, we show that learning\n$k\\times k$, rank-$r$, matrices to normalized $L_{1}$ distance $\\epsilon$\nrequires $\\Omega(\\frac{kr}{\\epsilon^2})$ samples, and propose an algorithm that\nuses ${\\cal O}(\\frac{kr}{\\epsilon^2}\\log^2\\frac r\\epsilon)$ samples, a number\nlinear in the high dimension, and nearly linear in the, typically low, rank.\nThe algorithm improves on existing spectral techniques and runs in polynomial\ntime. The proofs establish new results on the rapid convergence of the spectral\ndistance between the model and observation matrices, and may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 19:10:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Jain", "Ayush", ""], ["Orlitsky", "Alon", ""]]}, {"id": "2010.00137", "submitter": "Holden Lee", "authors": "Rong Ge, Holden Lee, Jianfeng Lu, Andrej Risteski", "title": "Efficient sampling from the Bingham distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a algorithm for exact sampling from the Bingham distribution\n$p(x)\\propto \\exp(x^\\top A x)$ on the sphere $\\mathcal S^{d-1}$ with expected\nruntime of $\\operatorname{poly}(d, \\lambda_{\\max}(A)-\\lambda_{\\min}(A))$. The\nalgorithm is based on rejection sampling, where the proposal distribution is a\npolynomial approximation of the pdf, and can be sampled from by explicitly\nevaluating integrals of polynomials over the sphere. Our algorithm gives exact\nsamples, assuming exact computation of an inverse function of a polynomial.\nThis is in contrast with Markov Chain Monte Carlo algorithms, which are not\nknown to enjoy rapid mixing on this problem, and only give approximate samples.\n  As a direct application, we use this to sample from the posterior\ndistribution of a rank-1 matrix inference problem in polynomial time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 22:48:03 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Holden", ""], ["Lu", "Jianfeng", ""], ["Risteski", "Andrej", ""]]}, {"id": "2010.00297", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Universal time-series forecasting with mixture predictors", "comments": "This is the author's version of the book published by Springer under\n  the same name. The final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-030-54304-4 . Further updates and corrections\n  may be made here", "journal-ref": null, "doi": "10.1007/978-3-030-54304-4", "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book is devoted to the problem of sequential probability forecasting,\nthat is, predicting the probabilities of the next outcome of a growing sequence\nof observations given the past. This problem is considered in a very general\nsetting that unifies commonly used probabilistic and non-probabilistic\nsettings, trying to make as few as possible assumptions on the mechanism\ngenerating the observations. A common form that arises in various formulations\nof this problem is that of mixture predictors, which are formed as a\ncombination of a finite or infinite set of other predictors attempting to\ncombine their predictive powers. The main subject of this book are such mixture\npredictors, and the main results demonstrate the universality of this method in\na very general probabilistic setting, but also show some of its limitations.\nWhile the problems considered are motivated by practical applications,\ninvolving, for example, financial, biological or behavioural data, this\nmotivation is left implicit and all the results exposed are theoretical.\n  The book targets graduate students and researchers interested in the problem\nof sequential prediction, and, more generally, in theoretical analysis of\nproblems in machine learning and non-parametric statistics, as well as\nmathematical and philosophical foundations of these fields.\n  The material in this volume is presented in a way that presumes familiarity\nwith basic concepts of probability and statistics, up to and including\nprobability distributions over spaces of infinite sequences. Familiarity with\nthe literature on learning or stochastic processes is not required.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:56:23 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "2010.00408", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Badr-Eddine Ch\\'erief-Abdellatif, Alexis Derumigny,\n  Jean-David Fermanian", "title": "Estimation of copulas via Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust inference for parametric copula models.\nEstimation using Canonical Maximum Likelihood might be unstable, especially in\nthe presence of outliers. We propose to use a procedure based on the Maximum\nMean Discrepancy (MMD) principle. We derive non-asymptotic oracle inequalities,\nconsistency and asymptotic normality of this new estimator. In particular, the\noracle inequality holds without any assumption on the copula family, and can be\napplied in the presence of outliers or under misspecification. Moreover, in our\nMMD framework, the statistical inference of copula models for which there\nexists no density with respect to the Lebesgue measure on $[0,1]^d$, as the\nMarshall-Olkin copula, becomes feasible. A simulation study shows the\nrobustness of our new procedures, especially compared to pseudo-maximum\nlikelihood estimation. An R package implementing the MMD estimator for copula\nmodels is available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:50:17 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Alquier", "Pierre", ""], ["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "2010.00636", "submitter": "Roi Weiss", "authors": "L\\'aszl\\'o Gy\\\"orfi and Roi Weiss", "title": "Universal consistency and rates of convergence of multiclass prototype\n  algorithms in metric spaces", "comments": "To appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study universal consistency and convergence rates of simple\nnearest-neighbor prototype rules for the problem of multiclass classification\nin metric paces. We first show that a novel data-dependent partitioning rule,\nnamed Proto-NN, is universally consistent in any metric space that admits a\nuniversally consistent rule. Proto-NN is a significant simplification of\nOptiNet, a recently proposed compression-based algorithm that, to date, was the\nonly algorithm known to be universally consistent in such a general setting.\nPractically, Proto-NN is simpler to implement and enjoys reduced computational\ncomplexity.\n  We then proceed to study convergence rates of the excess error probability.\nWe first obtain rates for the standard $k$-NN rule under a margin condition and\na new generalized-Lipschitz condition. The latter is an extension of a recently\nproposed modified-Lipschitz condition from $\\mathbb R^d$ to metric spaces.\nSimilarly to the modified-Lipschitz condition, the new condition avoids any\nboundness assumptions on the data distribution. While obtaining rates for\nProto-NN is left open, we show that a second prototype rule that hybridizes\nbetween $k$-NN and Proto-NN achieves the same rates as $k$-NN while enjoying\nsimilar computational advantages as Proto-NN. However, as $k$-NN, this hybrid\nrule is not consistent in general.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:23:22 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:43:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Weiss", "Roi", ""]]}, {"id": "2010.01390", "submitter": "Anuran Makur", "authors": "Anuran Makur and Elchanan Mossel and Yury Polyanskiy", "title": "Broadcasting on Two-Dimensional Regular Grids", "comments": "52 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a specialization of the problem of broadcasting on directed acyclic\ngraphs, namely, broadcasting on 2D regular grids. Consider a 2D regular grid\nwith source vertex $X$ at layer $0$ and $k+1$ vertices at layer $k\\geq 1$,\nwhich are at distance $k$ from $X$. Every vertex of the 2D regular grid has\noutdegree $2$, the vertices at the boundary have indegree $1$, and all other\nvertices have indegree $2$. At time $0$, $X$ is given a random bit. At time\n$k\\geq 1$, each vertex in layer $k$ receives transmitted bits from its parents\nin layer $k-1$, where the bits pass through binary symmetric channels with\nnoise level $\\delta\\in(0,1/2)$. Then, each vertex combines its received bits\nusing a common Boolean processing function to produce an output bit. The\nobjective is to recover $X$ with probability of error better than $1/2$ from\nall vertices at layer $k$ as $k \\rightarrow \\infty$. Besides their natural\ninterpretation in communication networks, such broadcasting processes can be\nconstrued as 1D probabilistic cellular automata (PCA) with boundary conditions\nthat limit the number of sites at each time $k$ to $k+1$. We conjecture that it\nis impossible to propagate information in a 2D regular grid regardless of the\nnoise level and the choice of processing function. In this paper, we make\nprogress towards establishing this conjecture, and prove using ideas from\npercolation and coding theory that recovery of $X$ is impossible for any\n$\\delta$ provided that all vertices use either AND or XOR processing functions.\nFurthermore, we propose a martingale-based approach that establishes the\nimpossibility of recovering $X$ for any $\\delta$ when all NAND processing\nfunctions are used if certain supermartingales can be rigorously constructed.\nWe also provide numerical evidence for the existence of these supermartingales\nby computing explicit examples for different values of $\\delta$ via linear\nprogramming.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:01:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Makur", "Anuran", ""], ["Mossel", "Elchanan", ""], ["Polyanskiy", "Yury", ""]]}, {"id": "2010.01598", "submitter": "Bernd Funovits", "authors": "Wolfgang Scherrer and Bernd Funovits", "title": "All-Pass Functions for Mirroring Pairs of Complex-Conjugated Roots of\n  Rational Matrix Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct rational all-pass matrix functions with real-valued coefficients\nfor mirroring pairs of complex-conjugated determinantal roots of a rational\nmatrix. This problem appears, for example, when proving the spectral\nfactorization theorem, or, more recently, in the literature on possibly\nnon-invertible or possibly non-causal vector autoregressive moving average\n(VARMA) models. In general, it is not obvious whether the all-pass matrix\nfunction (and as a consequence the all-pass transformed rational matrix with\ninitally real-valued coefficients) which mirrors complex-conjugated roots at\nthe unit circle has real-valued coefficients. Naive constructions result in\nall-pass functions with complex-valued coefficients which implies that the\nreal-valued parameter space (usually relevant for estimation) is left.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 15:12:19 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 13:22:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Scherrer", "Wolfgang", ""], ["Funovits", "Bernd", ""]]}, {"id": "2010.01654", "submitter": "Ning Ning", "authors": "Ning Ning", "title": "Multivariate Quantile Bayesian Structural Time Series (MQBSTS) Model", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the multivariate quantile Bayesian structural time\nseries (MQBSTS) model for the joint quantile time series forecast, which is the\nfirst such model for correlated multivariate time series to the author's best\nknowledge. The MQBSTS model also enables quantile based feature selection in\nits regression component where each time series has its own pool of\ncontemporaneous external time series predictors, which is the first time that a\nfully data-driven quantile feature selection technique applicable to time\nseries data to the author's best knowledge. Different from most machine\nlearning algorithms, the MQBSTS model has very few hyper-parameters to tune,\nrequires small datasets to train, converges fast, and is executable on ordinary\npersonal computers. Extensive examinations on simulated data and empirical data\nconfirmed that the MQBSTS model has superior performance in feature selection,\nparameter estimation, and forecast.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:56:34 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ning", "Ning", ""]]}, {"id": "2010.01705", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Vasilis Kontonis, Christos Tzamos,\n  Nikos Zarifis", "title": "A Polynomial Time Algorithm for Learning Halfspaces with Tsybakov Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of PAC learning homogeneous halfspaces in the presence\nof Tsybakov noise. In the Tsybakov noise model, the label of every sample is\nindependently flipped with an adversarially controlled probability that can be\narbitrarily close to $1/2$ for a fraction of the samples. {\\em We give the\nfirst polynomial-time algorithm for this fundamental learning problem.} Our\nalgorithm learns the true halfspace within any desired accuracy $\\epsilon$ and\nsucceeds under a broad family of well-behaved distributions including\nlog-concave distributions. Prior to our work, the only previous algorithm for\nthis problem required quasi-polynomial runtime in $1/\\epsilon$.\n  Our algorithm employs a recently developed reduction \\cite{DKTZ20b} from\nlearning to certifying the non-optimality of a candidate halfspace. This prior\nwork developed a quasi-polynomial time certificate algorithm based on\npolynomial regression. {\\em The main technical contribution of the current\npaper is the first polynomial-time certificate algorithm.} Starting from a\nnon-trivial warm-start, our algorithm performs a novel \"win-win\" iterative\nprocess which, at each step, either finds a valid certificate or improves the\nangle between the current halfspace and the true one. Our warm-start algorithm\nfor isotropic log-concave distributions involves a number of analytic tools\nthat may be of broader interest. These include a new efficient method for\nreweighting the distribution in order to recenter it and a novel\ncharacterization of the spectrum of the degree-$2$ Chow parameters.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:19:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2010.01758", "submitter": "Claudia Solis-Lemus", "authors": "Claudia Solis-Lemus, Arrigo Coen, Cecile Ane", "title": "On the Identifiability of Phylogenetic Networks under a Pseudolikelihood\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree of Life is the graphical structure that represents the evolutionary\nprocess from single-cell organisms at the origin of life to the vast\nbiodiversity we see today. Reconstructing this tree from genomic sequences is\nchallenging due to the variety of biological forces that shape the signal in\nthe data, and many of those processes like incomplete lineage sorting and\nhybridization can produce confounding information. Here, we present the\nmathematical version of the identifiability proofs of phylogenetic networks\nunder the pseudolikelihood model in SNaQ. We establish that the ability to\ndetect different hybridization events depends on the number of nodes on the\nhybridization blob, with small blobs (corresponding to closely related species)\nbeing the hardest to be detected. Our work focuses on level-1 networks, but\nraises attention to the importance of identifiability studies on phylogenetic\ninference methods for broader classes of networks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 03:28:25 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Solis-Lemus", "Claudia", ""], ["Coen", "Arrigo", ""], ["Ane", "Cecile", ""]]}, {"id": "2010.01768", "submitter": "Nabarun Deb", "authors": "Nabarun Deb, Promit Ghosal and Bodhisattva Sen", "title": "Measuring Association on Topological Spaces Using Kernels and Geometric\n  Graphs", "comments": "66 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a class of simple, nonparametric, yet\ninterpretable measures of association between two random variables $X$ and $Y$\ntaking values in general topological spaces. These nonparametric measures --\ndefined using the theory of reproducing kernel Hilbert spaces -- capture the\nstrength of dependence between $X$ and $Y$ and have the property that they are\n0 if and only if the variables are independent and 1 if and only if one\nvariable is a measurable function of the other. Further, these population\nmeasures can be consistently estimated using the general framework of graph\nfunctionals which include $k$-nearest neighbor graphs and minimum spanning\ntrees. Moreover, a sub-class of these estimators are also shown to adapt to the\nintrinsic dimensionality of the underlying distribution. Some of these\nempirical measures can also be computed in near linear time. Under the\nhypothesis of independence between $X$ and $Y$, these empirical measures\n(properly normalized) have a standard normal limiting distribution. Thus, these\nmeasures can also be readily used to test the hypothesis of mutual independence\nbetween $X$ and $Y$. In fact, as far as we are aware, these are the only\nprocedures that possess all the above mentioned desirable properties.\nFurthermore, when restricting to Euclidean spaces, we can make these sample\nmeasures of association finite-sample distribution-free, under the hypothesis\nof independence, by using multivariate ranks defined via the theory of optimal\ntransport. The recent correlation coefficient proposed in Dette et al. (2013),\nChatterjee (2019), and Azadkia and Chatterjee (2019) can be seen as a special\ncase of this general class of measures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:25:09 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 17:21:06 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Deb", "Nabarun", ""], ["Ghosal", "Promit", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2010.01837", "submitter": "Jad Beyhum", "authors": "Jad Beyhum (ORSTAT), Eric Gautier (TSE)", "title": "Factor and factor loading augmented estimators for panel regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers linear panel data models where the dependence of the\nregressors and the unobservables is modelled through a factor structure. The\nasymptotic setting is such that the number of time periods and the sample size\nboth go to infinity. Non-strong factors are allowed and the number of factors\ncan grow to infinity with the sample size. We study a class of two-step\nestimators of the regression coefficients. In the first step, factors and\nfactor loadings are estimated. Then, the second step corresponds to the panel\nregression of the outcome on the regressors and the estimates of the factors\nand the factor loadings from the first step. Different methods can be used in\nthe first step while the second step is unique. We derive sufficient conditions\non the first-step estimator and the data generating process under which the\ntwo-step estimator is asymptotically normal. Assumptions under which using an\napproach based on principal components analysis in the first step yields an\nasymptotically normal estimator are also given. The two-step procedure exhibits\ngood finite sample properties in simulations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:49:33 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:28:46 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Beyhum", "Jad", "", "ORSTAT"], ["Gautier", "Eric", "", "TSE"]]}, {"id": "2010.01851", "submitter": "David Holzm\\\"uller", "authors": "David Holzm\\\"uller", "title": "On the Universality of the Double Descent Peak in Ridgeless Regression", "comments": "Accepted at ICLR 2021. 9 pages + 34 pages appendix. Changes in v5:\n  Added link to repository with generated data. Experimental results can be\n  reproduced using the code at\n  https://github.com/dholzmueller/universal_double_descent", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a non-asymptotic distribution-independent lower bound for the\nexpected mean squared generalization error caused by label noise in ridgeless\nlinear regression. Our lower bound generalizes a similar known result to the\noverparameterized (interpolating) regime. In contrast to most previous works,\nour analysis applies to a broad class of input distributions with almost surely\nfull-rank feature matrices, which allows us to cover various types of\ndeterministic or random feature maps. Our lower bound is asymptotically sharp\nand implies that in the presence of label noise, ridgeless linear regression\ndoes not perform well around the interpolation threshold for any of these\nfeature maps. We analyze the imposed assumptions in detail and provide a theory\nfor analytic (random) feature maps. Using this theory, we can show that our\nassumptions are satisfied for input distributions with a (Lebesgue) density and\nfeature maps given by random deep neural networks with analytic activation\nfunctions like sigmoid, tanh, softplus or GELU. As further examples, we show\nthat feature maps from random Fourier features and polynomial kernels also\nsatisfy our assumptions. We complement our theory with further experimental and\nanalytic results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:30:25 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:09:03 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 13:56:02 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 17:15:33 GMT"}, {"version": "v5", "created": "Thu, 25 Mar 2021 10:33:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Holzm\u00fcller", "David", ""]]}, {"id": "2010.02126", "submitter": "Cheng Li", "authors": "Cheng Li", "title": "Bayesian Fixed-domain Asymptotics: Bernstein-von Mises Theorem for\n  Covariance Parameters in a Gaussian Process Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models typically contain finite dimensional parameters in\nthe covariance functions that need to be estimated from the data. We study the\nBayesian fixed-domain asymptotic properties of the covariance parameters in a\nGaussian process with an isotropic Matern covariance function. Under\nfixed-domain asymptotics, it is well known that when the dimension of data is\nless than or equal to three, the microergodic parameter can be consistently\nestimated with asymptotic normality while the variance parameter and the range\n(or length-scale) parameter cannot. Motivated by the frequentist theory, we\nprove a Bernstein-von Mises theorem for the covariance parameters in isotropic\nMatern covariance functions. We show that under fixed-domain asymptotics, the\njoint posterior distribution of the microergodic parameter and the range\nparameter can be factored independently into the product of their marginal\nposteriors as the sample size goes to infinity. The posterior of the\nmicroergodic parameter converges in total variation norm to a normal\ndistribution with shrinking variance, while the posterior of the range\nparameter does not necessarily converge to any degenerate distribution in\ngeneral. Our theory allows unbounded prior support for the range parameter.\nFurthermore, we propose a new property called the posterior asymptotic\nefficiency in linear prediction, and show that the Bayesian kriging predictor\nat a new spatial location with covariance parameters randomly drawn from their\nposterior has the same prediction mean squared error as if the true parameters\nwere known. In the special case of one-dimensional Ornstein-Uhlenbeck process,\nwe derive an explicit form for the limiting posterior distribution of the range\nparameter and an explicit posterior convergence rate for the posterior\nasymptotic efficiency. We verify these asymptotic results in numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:14:13 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 00:10:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Li", "Cheng", ""]]}, {"id": "2010.02177", "submitter": "Simeng Wang", "authors": "Yan Pautrat, Simeng Wang", "title": "Ke Li's lemma for quantum hypothesis testing in general von Neumann\n  algebras", "comments": "13 pages. Remarks and comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.OA math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lemma stated by Ke Li in [arXiv:1208.1400] has been used in e.g.\n[arXiv:1510.04682,arXiv:1706.04590,arXiv:1612.01464,arXiv:1308.6503,arXiv:1602.08898]\nfor various tasks in quantum hypothesis testing, data compression with quantum\nside information or quantum key distribution. This lemma was originally proven\nin finite dimension, with a direct extension to type I von Neumann algebras.\nHere we show that the use of modular theory allows to give more transparent\nmeaning to the objects constructed by the lemma, and to prove it for general\nvon Neumann algebras. This yields immediate generalizations of e.g.\n[arXiv:1510.04682].\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:24:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:30:22 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Pautrat", "Yan", ""], ["Wang", "Simeng", ""]]}, {"id": "2010.02288", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Marten Wegkamp", "title": "Detecting approximate replicate components of a high-dimensional random\n  vector with latent structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional feature vectors are likely to contain sets of measurements\nthat are approximate replicates of one another. In complex applications, or\nautomated data collection, these feature sets are not known a priori, and need\nto be determined. This work proposes a class of latent factor models on the\nobserved high-dimensional random vector $X \\in \\mathbb{R}^p$, for defining,\nidentifying and estimating the index set of its approximately replicate\ncomponents. The model class is parametrized by a $p \\times K$ loading matrix\n$A$ that contains a hidden sub-matrix whose rows can be partitioned into groups\nof parallel vectors. Under this model class, a set of approximate replicate\ncomponents of $X$ corresponds to a set of parallel rows in $A$: these entries\nof $X$ are, up to scale and additive error, the same linear combination of the\n$K$ latent factors; the value of $K$ is itself unknown. The problem of finding\napproximate replicates in $X$ reduces to identifying, and estimating, the\nlocation of the hidden sub-matrix within $A$, and of the partition of its row\nindex set $H$. Both $H$ and its partiton can be fully characterized in terms of\na new family of criteria based on the correlation matrix of $X$, and their\nidentifiability, as well as that of the unknown latent dimension $K$, are\nobtained as consequences. The constructive nature of the identifiability\narguments enables computationally efficient procedures, with consistency\nguarantees. When $A$ has the errors-in-variable parametrization, the difficulty\nof the problem is elevated. The task becomes that of separating out groups of\nparallel rows that are proportional to canonical basis vectors from other dense\nparallel rows in $A$. This is met under a scale assumption, via a principled\nway of selecting the target row indices, guided by the succesive maximization\nof Schur complements of appropriate covariance matrices.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:03:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2010.02304", "submitter": "Wenshuo Wang", "authors": "Wenshuo Wang and Lucas Janson", "title": "A Power Analysis of the Conditional Randomization Test and Knockoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific problems, researchers try to relate a response variable\n$Y$ to a set of potential explanatory variables $X = (X_1,\\dots,X_p)$, and\nstart by trying to identify variables that contribute to this relationship. In\nstatistical terms, this goal can be posed as trying to identify $X_j$'s upon\nwhich $Y$ is conditionally dependent. Sometimes it is of value to\nsimultaneously test for each $j$, which is more commonly known as variable\nselection. The conditional randomization test (CRT) and model-X knockoffs are\ntwo recently proposed methods that respectively perform conditional\nindependence testing and variable selection by, for each $X_j$, computing any\ntest statistic on the data and assessing that test statistic's significance by\ncomparing it to test statistics computed on synthetic variables generated using\nknowledge of $X$'s distribution. Our main contribution is to analyze their\npower in a high-dimensional linear model where the ratio of the dimension $p$\nand the sample size $n$ converge to a positive constant. We give explicit\nexpressions of the asymptotic power of the CRT, variable selection with CRT\n$p$-values, and model-X knockoffs, each with a test statistic based on either\nthe marginal covariance, the least squares coefficient, or the lasso. One\nuseful application of our analysis is the direct theoretical comparison of the\nasymptotic powers of variable selection with CRT $p$-values and model-X\nknockoffs; in the instances with independent covariates that we consider, the\nCRT provably dominates knockoffs. We also analyze the power gain from using\nunlabeled data in the CRT when limited knowledge of $X$'s distribution is\navailable, and the power of the CRT when samples are collected retrospectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:49:30 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Wenshuo", ""], ["Janson", "Lucas", ""]]}, {"id": "2010.02326", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen and Xiaoou Li", "title": "Determining the Number of Factors in High-dimensional Generalised Latent\n  Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a generalisation of the classical linear factor model, generalised latent\nfactor models are a useful tool for analysing multivariate data of different\ntypes, including binary choices and counts. In this paper, we propose an\ninformation criterion to determine the number of factors in generalised latent\nfactor models. The consistency of the proposed information criterion is\nestablished under a high-dimensional setting where both the sample size and the\nnumber of manifest variables grow to infinity and data may have many missing\nvalues. To establish this consistency result, an error bound is established for\nthe parameter estimates that improves the existing results and may be of\nindependent theoretical interest. Simulation shows that the proposed criterion\nhas good finite sample performance. An application to Eysenck's personality\nquestionnaire confirms the three-factor structure of this personality survey.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:36:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "2010.02425", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen", "title": "Improving Nonparametric Density Estimation with Tensor Decompositions", "comments": "20 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While nonparametric density estimators often perform well on low dimensional\ndata, their performance can suffer when applied to higher dimensional data,\nowing presumably to the curse of dimensionality. One technique for avoiding\nthis is to assume no dependence between features and that the data are sampled\nfrom a separable density. This allows one to estimate each marginal\ndistribution independently thereby avoiding the slow rates associated with\nestimating the full joint density. This is a strategy employed in naive Bayes\nmodels and is analogous to estimating a rank-one tensor. In this paper we\ninvestigate whether these improvements can be extended to other simplified\ndependence assumptions which we model via nonnegative tensor decompositions. In\nour central theoretical results we prove that restricting estimation to\nlow-rank nonnegative PARAFAC or Tucker decompositions removes the\ndimensionality exponent on bin width rates for multidimensional histograms.\nThese results are validated experimentally with high statistical significance\nvia direct application of existing nonnegative tensor factorization to\nhistogram estimators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:39:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Vandermeulen", "Robert A.", ""]]}, {"id": "2010.02476", "submitter": "Gregory Rice", "authors": "Lajos Horv\\'ath and Gregory Rice", "title": "Limit results for $L^p$ functionals of weighted CUSUM processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative sum (CUSUM) process is often used in change point analysis to\ndetect changes in the mean of sequentially observed data. We provide a full\ndescription of the asymptotic distribution of $L^p, 1\\leq p <\\infty$,\nfunctionals of the weighted CUSUM process for time series under general\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:51:07 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Horv\u00e1th", "Lajos", ""], ["Rice", "Gregory", ""]]}, {"id": "2010.02482", "submitter": "Anru R. Zhang", "authors": "Yuchen Zhou and Anru R. Zhang and Lili Zheng and Yazhen Wang", "title": "Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies a general framework for high-order tensor SVD. We propose\na new computationally efficient algorithm, tensor-train orthogonal iteration\n(TTOI), that aims to estimate the low tensor-train rank structure from the\nnoisy high-order tensor observation. The proposed TTOI consists of\ninitialization via TT-SVD (Oseledets, 2011) and new iterative backward/forward\nupdates. We develop the general upper bound on estimation error for TTOI with\nthe support of several new representation lemmas on tensor matricizations. By\ndeveloping a matching information-theoretic lower bound, we also prove that\nTTOI achieves the minimax optimality under the spiked tensor model. The merits\nof the proposed TTOI are illustrated through applications to estimation and\ndimension reduction of high-order Markov processes, numerical studies, and a\nreal data example on New York City taxi travel records. The software of the\nproposed algorithm is available online.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:18:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhou", "Yuchen", ""], ["Zhang", "Anru R.", ""], ["Zheng", "Lili", ""], ["Wang", "Yazhen", ""]]}, {"id": "2010.02888", "submitter": "Kavya Ravichandran", "authors": "Maryam Aliakbarpour, Amartya Shankha Biswas, Kavya Ravichandran,\n  Ronitt Rubinfeld", "title": "Testing Tail Weight of a Distribution Via Hazard Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the shape of a distribution of data is of interest to people in\na great variety of fields, as it may affect the types of algorithms used for\nthat data. Given samples from a distribution, we seek to understand how many\nelements appear infrequently, that is, to characterize the tail of the\ndistribution. We develop an algorithm based on a careful bucketing scheme that\ndistinguishes heavy-tailed distributions from non-heavy-tailed ones via a\ndefinition based on the hazard rate under some natural smoothness and ordering\nassumptions. We verify our theoretical results empirically.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:13:14 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Biswas", "Amartya Shankha", ""], ["Ravichandran", "Kavya", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2010.03104", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Alexander Rakhlin and David Simchi-Levi and\n  Yunzong Xu", "title": "Instance-Dependent Complexity of Contextual Bandits and Reinforcement\n  Learning: A Disagreement-Based Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical multi-armed bandit problem, instance-dependent algorithms\nattain improved performance on \"easy\" problems with a gap between the best and\nsecond-best arm. Are similar guarantees possible for contextual bandits? While\npositive results are known for certain special cases, there is no general\ntheory characterizing when and how instance-dependent regret bounds for\ncontextual bandits can be achieved for rich, general classes of policies. We\nintroduce a family of complexity measures that are both sufficient and\nnecessary to obtain instance-dependent regret bounds. We then introduce new\noracle-efficient algorithms which adapt to the gap whenever possible, while\nalso attaining the minimax rate in the worst case. Finally, we provide\nstructural results that tie together a number of complexity measures previously\nproposed throughout contextual bandits, reinforcement learning, and active\nlearning and elucidate their role in determining the optimal instance-dependent\nregret. In a large-scale empirical evaluation, we find that our approach often\ngives superior results for challenging exploration problems.\n  Turning our focus to reinforcement learning with function approximation, we\ndevelop new oracle-efficient algorithms for reinforcement learning with rich\nobservations that obtain optimal gap-dependent sample complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 01:33:06 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Simchi-Levi", "David", ""], ["Xu", "Yunzong", ""]]}, {"id": "2010.03141", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura", "title": "Bayesian Shrinkage Approaches to Unbalanced Problems of Estimation and\n  Prediction on the Basis of Negative Multinomial Samples", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we treat estimation and prediction problems where negative\nmultinomial variables are observed and in particular consider unbalanced\nsettings. First, the problem of estimating multiple negative multinomial\nparameter vectors under the standardized squared error loss is treated and a\nnew empirical Bayes estimator which dominates the UMVU estimator under suitable\nconditions is derived. Second, we consider estimation of the joint predictive\ndensity of several multinomial tables under the Kullback-Leibler divergence and\nobtain a sufficient condition under which the Bayesian predictive density with\nrespect to a hierarchical shrinkage prior dominates the Bayesian predictive\ndensity with respect to the Jeffreys prior. Third, our proposed Bayesian\nestimator and predictive density give risk improvements in simulations.\nFinally, the problem of estimating the joint predictive density of negative\nmultinomial variables is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:54:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hamura", "Yasuyuki", ""]]}, {"id": "2010.03145", "submitter": "Qiyang Han", "authors": "Qiyang Han, Bodhisattva Sen, Yandi Shen", "title": "High dimensional asymptotics of likelihood ratio tests in the Gaussian\n  sequence model under convex constraints", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Gaussian sequence model $Y=\\mu+\\xi$, we study the likelihood ratio\ntest (LRT) for testing $H_0: \\mu=\\mu_0$ versus $H_1: \\mu \\in K$, where $\\mu_0\n\\in K$, and $K$ is a closed convex set in $\\mathbb{R}^n$. In particular, we\nshow that under the null hypothesis, normal approximation holds for the\nlog-likelihood ratio statistic for a general pair $(\\mu_0,K)$, in the high\ndimensional regime where the estimation error of the associated least squares\nestimator diverges in an appropriate sense. The normal approximation further\nleads to a precise characterization of the power behavior of the LRT in the\nhigh dimensional regime. These characterizations show that the power behavior\nof the LRT is in general non-uniform with respect to the Euclidean metric, and\nillustrate the conservative nature of existing minimax optimality and\nsub-optimality results for the LRT. A variety of examples, including testing in\nthe orthant/circular cone, isotonic regression, Lasso, and testing parametric\nassumptions versus shape-constrained alternatives, are worked out to\ndemonstrate the versatility of the developed theory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:02:53 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 20:08:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Qiyang", ""], ["Sen", "Bodhisattva", ""], ["Shen", "Yandi", ""]]}, {"id": "2010.03243", "submitter": "Justyna Wr\\'oblewska", "authors": "Justyna Wr\\'oblewska", "title": "A note on some extensions of the matrix angular central Gaussian\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the notion of the matrix angular central distribution\n(MACG) to the complex case. We start by considering the normally distributed\nrandom complex matrix ($Z$) and show that is the orientation\n($H_Z=Z(Z'Z)^{-1}$) has complex MACG (CMACG) distribution. Then we discuss the\ndistribution of the orientation of the linear transformation of the random\nmatrix which orientation part has CMACG distribution. Finally, we discuss the\nfamily of distributions which lead to the CMACG distribution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:52:02 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wr\u00f3blewska", "Justyna", ""]]}, {"id": "2010.03334", "submitter": "Ilia Negri", "authors": "Ilia Negri, Yoichi Nishiyama", "title": "Change point detection based on method of moment estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A change point detection procedure using the method of moment estimators is\nproposed. The test statistics is based on a suitable $Z$-process. The\nasymptotic behavior of this process is established under both the null and the\nalternative hypothesis and the consistency of the test is also proved. An\nestimator for the change point is proposed and its consistency is derived. Some\nexamples of this method applied to a parametric family of random variables are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:07:45 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Negri", "Ilia", ""], ["Nishiyama", "Yoichi", ""]]}, {"id": "2010.03382", "submitter": "Hugo Kruiniger Dr", "authors": "Hugo Kruiniger", "title": "Further results on the estimation of dynamic panel logit models with\n  fixed effects", "comments": "12 pages in total; extended version (section 2 added and appendix\n  extended)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kitazawa (2013, 2016) showed that the common parameters in the panel logit\nAR(1) model with strictly exogenous covariates and fixed effects are estimable\nat the root-n rate using the Generalized Method of Moments. Honor\\'e and\nWeidner (2020) extended his results in various directions: they found\nadditional moment conditions for the logit AR(1) model and also considered\nestimation of logit AR(p) models with p>1. In this note we prove a conjecture\nin their paper and show that 2^{T}-2T of their moment functions for the logit\nAR(1) model are linearly independent and span the set of valid moment\nfunctions, which is a 2^{T}-2T -dimensional linear subspace of the 2^{T}\n-dimensional vector space of real valued functions over the outcomes y element\nof {0,1}^{T}. We also prove that when p=2 and T element of {3,4,5}, there are,\nrespectively, 2^{T}-4(T-1) and 2^{T}-(3T-2) linearly independent moment\nfunctions for the panel logit AR(2) models with and without covariates.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:51:19 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 16:03:25 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 23:24:11 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kruiniger", "Hugo", ""]]}, {"id": "2010.03390", "submitter": "Yifan Cui", "authors": "Yifan Cui and Eric Tchetgen Tchetgen", "title": "On a necessary and sufficient identification condition of optimal\n  treatment regimes with an instrumental variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding is a threat to causal inference and individualized\ndecision making. Similar to Cui and Tchetgen Tchetgen (2020); Qiu et al.\n(2020); Han (2020a), we consider the problem of identification of optimal\nindividualized treatment regimes with a valid instrumental variable. Han\n(2020a) provided an alternative identifying condition of optimal treatment\nregimes using the conditional Wald estimand of Cui and Tchetgen Tchetgen\n(2020); Qiu et al. (2020) when treatment assignment is subject to endogeneity\nand a valid binary instrumental variable is available. In this note, we provide\na necessary and sufficient condition for identification of optimal treatment\nregimes using the conditional Wald estimand. Our novel condition is necessarily\nimplied by those of Cui and Tchetgen Tchetgen (2020); Qiu et al. (2020); Han\n(2020a) and may continue to hold in a variety of potential settings not covered\nby prior results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:03:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cui", "Yifan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2010.03460", "submitter": "Marco Mondelli", "authors": "Marco Mondelli and Ramji Venkataramanan", "title": "Approximate Message Passing with Spectral Initialization for Generalized\n  Linear Models", "comments": "38 pages, 5 figures, AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a signal from measurements obtained via\na generalized linear model. We focus on estimators based on approximate message\npassing (AMP), a family of iterative algorithms with many appealing features:\nthe performance of AMP in the high-dimensional limit can be succinctly\ncharacterized under suitable model assumptions; AMP can also be tailored to the\nempirical distribution of the signal entries, and for a wide class of\nestimation problems, AMP is conjectured to be optimal among all polynomial-time\nalgorithms.\n  However, a major issue of AMP is that in many models (such as phase\nretrieval), it requires an initialization correlated with the ground-truth\nsignal and independent from the measurement matrix. Assuming that such an\ninitialization is available is typically not realistic. In this paper, we solve\nthis problem by proposing an AMP algorithm initialized with a spectral\nestimator. With such an initialization, the standard AMP analysis fails since\nthe spectral estimator depends in a complicated way on the design matrix. Our\nmain contribution is a rigorous characterization of the performance of AMP with\nspectral initialization in the high-dimensional limit. The key technical idea\nis to define and analyze a two-phase artificial AMP algorithm that first\nproduces the spectral estimator, and then closely approximates the iterates of\nthe true AMP. We also provide numerical results that demonstrate the validity\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:52:35 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 09:43:24 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Mondelli", "Marco", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "2010.03513", "submitter": "Seonghyun Jeong", "authors": "Seonghyun Jeong", "title": "Posterior contraction in group sparse logit models for categorical\n  responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies posterior contraction rates in multi-category logit models\nwith priors incorporating group sparse structures. We consider a general class\nof logit models that includes the well-known multinomial logit models as a\nspecial case. Group sparsity is useful when predictor variables are naturally\nclustered and particularly useful for variable selection in the multinomial\nlogit models. We provide a unified platform for posterior contraction rates of\ngroup-sparse logit models that include binary logistic regression under\nindividual sparsity. No size restriction is directly imposed on the true signal\nin this study. In addition to establishing the first-ever contraction\nproperties for multi-category logit models under group sparsity, this work also\nrefines recent findings on the Bayesian theory of binary logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:33:07 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 15:16:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jeong", "Seonghyun", ""]]}, {"id": "2010.03729", "submitter": "Jason Miller", "authors": "Jason Miller, Sui Tang, Ming Zhong, Mauro Maggioni", "title": "Learning Theory for Inferring Interaction Kernels in Second-Order\n  Interacting Agent Systems", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the complex interactions of systems of particles or agents is a\nfundamental scientific and mathematical problem that is studied in diverse\nfields, ranging from physics and biology, to economics and machine learning. In\nthis work, we describe a very general second-order, heterogeneous,\nmultivariable, interacting agent model, with an environment, that encompasses a\nwide variety of known systems. We describe an inference framework that uses\nnonparametric regression and approximation theory based techniques to\nefficiently derive estimators of the interaction kernels which drive these\ndynamical systems. We develop a complete learning theory which establishes\nstrong consistency and optimal nonparametric min-max rates of convergence for\nthe estimators, as well as provably accurate predicted trajectories. The\nestimators exploit the structure of the equations in order to overcome the\ncurse of dimensionality and we describe a fundamental coercivity condition on\nthe inverse problem which ensures that the kernels can be learned and relates\nto the minimal singular value of the learning matrix. The numerical algorithm\npresented to build the estimators is parallelizable, performs well on\nhigh-dimensional problems, and is demonstrated on complex dynamical systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 02:07:53 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Miller", "Jason", ""], ["Tang", "Sui", ""], ["Zhong", "Ming", ""], ["Maggioni", "Mauro", ""]]}, {"id": "2010.03799", "submitter": "Dylan Foster", "authors": "Zakaria Mhammedi and Dylan J. Foster and Max Simchowitz and Dipendra\n  Misra and Wen Sun and Akshay Krishnamurthy and Alexander Rakhlin and John\n  Langford", "title": "Learning the Linear Quadratic Regulator from Nonlinear Observations", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem setting for continuous control called the LQR with\nRich Observations, or RichLQR. In our setting, the environment is summarized by\na low-dimensional continuous latent state with linear dynamics and quadratic\ncosts, but the agent operates on high-dimensional, nonlinear observations such\nas images from a camera. To enable sample-efficient learning, we assume that\nthe learner has access to a class of decoder functions (e.g., neural networks)\nthat is flexible enough to capture the mapping from observations to latent\nstates. We introduce a new algorithm, RichID, which learns a near-optimal\npolicy for the RichLQR with sample complexity scaling only with the dimension\nof the latent state space and the capacity of the decoder function class.\nRichID is oracle-efficient and accesses the decoder class only through calls to\na least-squares regression oracle. Our results constitute the first provable\nsample complexity guarantee for continuous control with an unknown nonlinearity\nin the system model and general function approximation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:02:47 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Mhammedi", "Zakaria", ""], ["Foster", "Dylan J.", ""], ["Simchowitz", "Max", ""], ["Misra", "Dipendra", ""], ["Sun", "Wen", ""], ["Krishnamurthy", "Akshay", ""], ["Rakhlin", "Alexander", ""], ["Langford", "John", ""]]}, {"id": "2010.03832", "submitter": "Olivier Wintenberger", "authors": "Marco Oesting, Olivier Wintenberger (LPSM UMR 8001)", "title": "Estmiation of the Spectral Measure from Convex Combinations of Regularly\n  Varying Random Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremal dependence structure of a regularly varying random vector X is\nfully described by its limiting spectral measure. In this paper, we investigate\nhow to recover characteristics of the measure, such as extremal coefficients,\nfrom the extremal behaviour of convex combinations of components of X. Our\nconsiderations result in a class of new estimators of moments of the\ncorresponding combinations for the spectral vector. We show asymp-totic\nnormality by means of a functional limit theorem and, focusing on the\nestimation of extremal coefficients, we verify that the minimal asymptotic\nvariance can be achieved by a plug-in estimator.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:20:24 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Oesting", "Marco", "", "LPSM UMR 8001"], ["Wintenberger", "Olivier", "", "LPSM UMR 8001"]]}, {"id": "2010.03891", "submitter": "Rasmus Erlemann", "authors": "Rasmus Erlemann and Bo Henry Lindqvist", "title": "Conditional Goodness-of-Fit Tests for Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of testing goodness-of-fit for discrete\ndistributions, where we focus on the geometric distribution. We define new\nlikelihood-based goodness-of-fit tests using the beta-geometric distribution\nand the type I discrete Weibull distribution as alternative distributions. The\ntests are compared in a simulation study, where also the classical\ngoodness-of-fit tests are considered for comparison. Throughout the paper we\nconsider conditional testing given a minimal sufficient statistic under the\nnull hypothesis, which enables the calculation of exact p-values. For this\npurpose, a new method is developed for drawing conditional samples from the\ngeometric distribution and the negative binomial distribution. We also explain\nbriefly how the conditional approach can be modified for the binomial, negative\nbinomial and Poisson distributions. It is finally noted that the simulation\nmethod may be extended to other discrete distributions having the same\nsufficient statistic, by using the Metropolis-Hastings algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 10:36:58 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Erlemann", "Rasmus", ""], ["Lindqvist", "Bo Henry", ""]]}, {"id": "2010.04114", "submitter": "Yang Li", "authors": "Yang Li, Jinqiao Duan and Xianbin Liu", "title": "A Machine Learning Framework for Computing the Most Probable Paths of\n  Stochastic Dynamical Systems", "comments": "22 pages, 13 figures", "journal-ref": "Phys. Rev. E 103, 012124 (2021)", "doi": "10.1103/PhysRevE.103.012124", "report-no": null, "categories": "math.DS math.PR math.ST nlin.CD physics.comp-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of transition phenomena between metastable states induced by\nnoise plays a fundamental role in a broad range of nonlinear systems. The\ncomputation of the most probable paths is a key issue to understand the\nmechanism of transition behaviors. Shooting method is a common technique for\nthis purpose to solve the Euler-Lagrange equation for the associated action\nfunctional, while losing its efficacy in high-dimensional systems. In the\npresent work, we develop a machine learning framework to compute the most\nprobable paths in the sense of Onsager-Machlup action functional theory.\nSpecifically, we reformulate the boundary value problem of Hamiltonian system\nand design a neural network to remedy the shortcomings of shooting method. The\nsuccessful applications of our algorithms to several prototypical examples\ndemonstrate its efficacy and accuracy for stochastic systems with both\n(Gaussian) Brownian noise and (non-Gaussian) L\\'evy noise. This novel approach\nis effective in exploring the internal mechanisms of rare events triggered by\nrandom fluctuations in various scientific fields.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 20:01:37 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 02:36:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Li", "Yang", ""], ["Duan", "Jinqiao", ""], ["Liu", "Xianbin", ""]]}, {"id": "2010.04218", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Adaptive spectral density estimation by model selection under local\n  differential privacy", "comments": "33 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study spectral density estimation under local differential privacy.\nAnonymization is achieved through truncation followed by Laplace perturbation.\nWe select our estimator from a set of candidate estimators by a penalized\ncontrast criterion. This estimator is shown to attain nearly the same rate of\nconvergence as the best estimator from the candidate set. A key ingredient of\nthe proof are recent results on concentration of quadratic forms in terms of\nsub-exponential random variables obtained in arXiv:1903.05964. We illustrate\nour findings in a small simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 19:00:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "2010.04345", "submitter": "Chao Gao", "authors": "Chao Gao and Anderson Y. Zhang", "title": "Exact Minimax Estimation for Phase Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the phase synchronization problem with measurements\n$Y=z^*z^{*H}+\\sigma W\\in\\mathbb{C}^{n\\times n}$, where $z^*$ is an\n$n$-dimensional complex unit-modulus vector and $W$ is a complex-valued\nGaussian random matrix. It is assumed that each entry $Y_{jk}$ is observed with\nprobability $p$. We prove that the minimax lower bound of estimating $z^*$\nunder the squared $\\ell_2$ loss is $(1-o(1))\\frac{\\sigma^2}{2p}$. We also show\nthat both generalized power method and maximum likelihood estimator achieve the\nerror bound $(1+o(1))\\frac{\\sigma^2}{2p}$. Thus, $\\frac{\\sigma^2}{2p}$ is the\nexact asymptotic minimax error of the problem. Our upper bound analysis\ninvolves a precise characterization of the statistical property of the power\niteration. The lower bound is derived through an application of van Trees'\ninequality.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:20:33 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 03:07:14 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2010.04347", "submitter": "Mohammed Zafar Anis", "authors": "M Z Anis", "title": "The Unit-Gompertz Distribution: Characterizations and Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Mazucheli et al. (2019) introduced the unit-Gompertz (UG)\ndistribution and studied some of its properties. In a complementary work, Anis\nand De (2020) corrected some of the subtle errors in the original paper and\nstudied some other interesting properties of this new distribution. However, to\nthe best of our knowledge no charcterization results on this distribution have\nappeared in the literature. This is addressed in the present paper using\ntruncated moments; and some more properties are investigated.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 03:29:44 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Anis", "M Z", ""]]}, {"id": "2010.04399", "submitter": "Amaury Durand", "authors": "Amaury Durand (EDF R and D TREE, LTCI), Fran\\c{c}ois Roueff", "title": "Hilbert valued fractionally integrated autoregressive moving average\n  processes with long memory operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractionally integrated autoregressive moving average processes have been\nwidely and successfully used to model univariate time series exhibiting long\nrange dependence. Vector and functional extensions of these processes have also\nbeen considered more recently. Here we rely on a spectral domain approach to\nextend this class of models in the form of a general Hilbert valued processes.\nIn this framework, the usual univariate long memory parameter d is replaced by\na long memory operator D acting on the Hilbert space. Our approach is compared\nto processes defined in the time domain that were previously introduced for\nmodeling long range dependence in the context of functional time series.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 07:10:54 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Durand", "Amaury", "", "EDF R and D TREE, LTCI"], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "2010.04485", "submitter": "Daniel Nevo", "authors": "Daniel Nevo and Malka Gorfine", "title": "Causal inference for semi-competing risks data", "comments": "35 pages, 3 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging challenge for time-to-event data is studying semi-competing\nrisks, namely when two event times are of interest: a non-terminal event time\n(e.g. age at disease diagnosis), and a terminal event time (e.g. age at death).\nThe non-terminal event is observed only if it precedes the terminal event,\nwhich may occur before or after the non-terminal event. Studying treatment or\nintervention effects on the dual event times is complicated because for some\nunits, the non-terminal event may occur under one treatment value but not under\nthe other. Until recently, existing approaches (e.g., the survivor average\ncausal effect) generally disregarded the time-to-event nature of both outcomes.\nMore recent research focused on principal strata effects within time-varying\npopulations under Bayesian approaches. In this paper, we propose alternative\nnon time-varying estimands, based on a single stratification of the population.\nWe present a novel assumption utilizing the time-to-event nature of the data,\nwhich is weaker than the often-invoked monotonicity assumption. We derive\nresults on partial identifiability, suggest a sensitivity analysis approach,\nand give conditions under which full identification is possible. Finally, we\npresent non-parametric and semi-parametric estimation methods for\nright-censored data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:19:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Nevo", "Daniel", ""], ["Gorfine", "Malka", ""]]}, {"id": "2010.04507", "submitter": "Subrata Chakraborty", "authors": "Seng Huat Ong, Subrata Chakraborty, Aniket Biswas", "title": "A new generalization of the geometric distribution using Azzalini's\n  mechanism: properties and application", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skewing mechanism of Azzalini for continuous distributions is used for\nthe first time to derive a new generalization of the geometric distribution.\nVarious structural properties of the proposed distribution are investigated.\nCharacterizations, including a new result for the geometric distribution, in\nterms of the proposed model are established. Extensive simulation experiment is\ndone to evaluate performance of the maximum likelihood estimation method.\nLikelihood ratio test for the necessity of additional skewing parameter is\nderived and corresponding simulation based power study is also reported. Two\nreal life count datasets are analyzed with the proposed model and compared with\nsome recently introduced two-parameter count models. The findings clearly\nindicate the superiority of the proposed model over the existing ones in\nmodelling real life count data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:38:06 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Ong", "Seng Huat", ""], ["Chakraborty", "Subrata", ""], ["Biswas", "Aniket", ""]]}, {"id": "2010.04596", "submitter": "Sophie Langer", "authors": "Sophie Langer", "title": "Approximating smooth functions by deep neural networks with sigmoid\n  activation function", "comments": "arXiv admin note: text overlap with arXiv:1908.11133", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of deep neural networks (DNNs) with sigmoid activation\nfunction. Recently, it was shown that DNNs approximate any $d$-dimensional,\nsmooth function on a compact set with a rate of order $W^{-p/d}$, where $W$ is\nthe number of nonzero weights in the network and $p$ is the smoothness of the\nfunction. Unfortunately, these rates only hold for a special class of sparsely\nconnected DNNs. We ask ourselves if we can show the same approximation rate for\na simpler and more general class, i.e., DNNs which are only defined by its\nwidth and depth. In this article we show that DNNs with fixed depth and a width\nof order $M^d$ achieve an approximation rate of $M^{-2p}$. As a conclusion we\nquantitatively characterize the approximation power of DNNs in terms of the\noverall weights $W_0$ in the network and show an approximation rate of\n$W_0^{-p/d}$. This more general result finally helps us to understand which\nnetwork topology guarantees a special target accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:29:31 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Langer", "Sophie", ""]]}, {"id": "2010.04610", "submitter": "Mikko Pakkanen", "authors": "Anine E. Bolko, Kim Christensen, Mikko S. Pakkanen, Bezirgen Veliyev", "title": "Roughness in spot variance? A GMM approach for estimation of fractional\n  log-normal stochastic volatility models using realized measures", "comments": "42 pages, 2 figures, v2: updated numerical methods and other minor\n  improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a generalized method of moments approach for joint\nestimation of the parameters of a fractional log-normal stochastic volatility\nmodel. We show that with an arbitrary Hurst exponent an estimator based on\nintegrated variance is consistent. Moreover, under stronger conditions we also\nderive a central limit theorem. These results stand even when integrated\nvariance is replaced with a realized measure of volatility calculated from\ndiscrete high-frequency data. However, in practice a realized estimator\ncontains sampling error, the effect of which is to skew the fractal coefficient\ntoward \"roughness\". We construct an analytical approach to control this error.\nIn a simulation study, we demonstrate convincing small sample properties of our\napproach based both on integrated and realized variance over the entire memory\nspectrum. We show that the bias correction attenuates any systematic deviance\nin the estimated parameters. Our procedure is applied to empirical\nhigh-frequency data from numerous leading equity indexes. With our robust\napproach the Hurst index is estimated around 0.05, confirming roughness in\nintegrated variance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:39:34 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 16:16:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bolko", "Anine E.", ""], ["Christensen", "Kim", ""], ["Pakkanen", "Mikko S.", ""], ["Veliyev", "Bezirgen", ""]]}, {"id": "2010.04703", "submitter": "Bryan Graham", "authors": "Bryan S. Graham", "title": "Sparse network asymptotics for logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a bipartite network where $N$ consumers choose to buy or not to buy\n$M$ different products. This paper considers the properties of the logistic\nregression of the $N\\times M$ array of i-buys-j purchase decisions,\n$\\left[Y_{ij}\\right]_{1\\leq i\\leq N,1\\leq j\\leq M}$, onto known functions of\nconsumer and product attributes under asymptotic sequences where (i) both $N$\nand $M$ grow large and (ii) the average number of products purchased per\nconsumer is finite in the limit. This latter assumption implies that the\nnetwork of purchases is sparse: only a (very) small fraction of all possible\npurchases are actually made (concordant with many real-world settings). Under\nsparse network asymptotics, the first and last terms in an extended\nHoeffding-type variance decomposition of the score of the logit composite\nlog-likelihood are of equal order. In contrast, under dense network\nasymptotics, the last term is asymptotically negligible. Asymptotic normality\nof the logistic regression coefficients is shown using a martingale central\nlimit theorem (CLT) for triangular arrays. Unlike in the dense case, the\nnormality result derived here also holds under degeneracy of the network\ngraphon. Relatedly, when there happens to be no dyadic dependence in the\ndataset in hand, it specializes to recently derived results on the behavior of\nlogistic regression with rare events and iid data. Sparse network asymptotics\nmay lead to better inference in practice since they suggest variance estimators\nwhich (i) incorporate additional sources of sampling variation and (ii) are\nvalid under varying degrees of dyadic dependence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:46:29 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Graham", "Bryan S.", ""]]}, {"id": "2010.04778", "submitter": "Konrad Kulakowski", "authors": "Konrad Ku{\\l}akowski and Ji\\v{r}\\'i Mazurek and Micha{\\l} Strada", "title": "On the similarity between ranking vectors in the pairwise comparison\n  method", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many priority deriving methods for pairwise comparison matrices. It\nis known that when these matrices are consistent all these methods result in\nthe same priority vector. However, when they are inconsistent, the results may\nvary. The presented work formulates an estimation of the difference between\npriority vectors in the two most popular ranking methods: the eigenvalue method\nand the geometric mean method. The estimation provided refers to the\ninconsistency of the pairwise comparison matrix. Theoretical considerations are\naccompanied by Montecarlo experiments showing the discrepancy between the\nvalues of both methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 19:47:01 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ku\u0142akowski", "Konrad", ""], ["Mazurek", "Ji\u0159\u00ed", ""], ["Strada", "Micha\u0142", ""]]}, {"id": "2010.04855", "submitter": "Rahul Singh", "authors": "Rahul Singh, Liyuan Xu, Arthur Gretton", "title": "Reproducing Kernel Methods for Nonparametric and Semiparametric\n  Treatment Effects", "comments": "Formerly \"Kernel Methods for Policy Evaluation: Treatment Effects,\n  Mediation Analysis, and Off-Policy Planning\" (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of reproducing kernel ridge estimators for nonparametric\nand semiparametric policy evaluation. The framework includes (i) treatment\neffects of the population, of subpopulations, and of alternative populations;\n(ii) the decomposition of a total effect into a direct effect and an indirect\neffect (mediated by a particular mechanism); and (iii) effects of sequences of\ntreatments. Treatment and covariates may be discrete or continuous, and low,\nhigh, or infinite dimensional. We consider estimation of means, increments, and\ndistributions of counterfactual outcomes. Each estimator is an inner product in\na reproducing kernel Hilbert space (RKHS), with a one line, closed form\nsolution. For the nonparametric case, we prove uniform consistency and provide\nfinite sample rates of convergence. For the semiparametric case, we prove root\nn consistency, Gaussian approximation, and semiparametric efficiency by finite\nsample arguments. We evaluate our estimators in simulations then estimate\ncontinuous, heterogeneous, incremental, and mediated treatment effects of the\nUS Jobs Corps training program for disadvantaged youth.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 00:53:11 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 15:29:08 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 16:14:00 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Singh", "Rahul", ""], ["Xu", "Liyuan", ""], ["Gretton", "Arthur", ""]]}, {"id": "2010.04890", "submitter": "Yuanlu Bai", "authors": "Yuanlu Bai, Zhiyuan Huang, Henry Lam, Ding Zhao", "title": "Rare-Event Simulation for Neural Network and Random Forest Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study rare-event simulation for a class of problems where the target\nhitting sets of interest are defined via modern machine learning tools such as\nneural networks and random forests. This problem is motivated from fast\nemerging studies on the safety evaluation of intelligent systems, robustness\nquantification of learning models, and other potential applications to\nlarge-scale simulation in which machine learning tools can be used to\napproximate complex rare-event set boundaries. We investigate an importance\nsampling scheme that integrates the dominating point machinery in large\ndeviations and sequential mixed integer programming to locate the underlying\ndominating points. Our approach works for a range of neural network\narchitectures including fully connected layers, rectified linear units,\nnormalization, pooling and convolutional layers, and random forests built from\nstandard decision trees. We provide efficiency guarantees and numerical\ndemonstration of our approach using a classification model in the UCI Machine\nLearning Repository.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 03:27:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bai", "Yuanlu", ""], ["Huang", "Zhiyuan", ""], ["Lam", "Henry", ""], ["Zhao", "Ding", ""]]}, {"id": "2010.05008", "submitter": "Lihu Xu", "authors": "Peng Chen, Xinghu Jin, Xiang Li, Lihu Xu", "title": "A generalized Catoni's ${\\rm M}$-estimator under finite {$\\alpha$-th\n  moment assumption} with $\\alpha \\in (1,2)$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the { ${\\rm M}$-estimator} put forward by Catoni in his seminal\npaper [C12] to the case in which samples can have finite $\\alpha$-th moment\nwith $\\alpha \\in (1,2)$ rather than finite variance, our approach is by\nslightly modifying the influence function $\\varphi$ therein. The choice of the\nnew influence function is inspired by the Taylor-like expansion developed in\n[C-N-X]. We obtain a deviation bound of the estimator, as $\\alpha \\rightarrow\n2$, this bound is the same as that in [C12]. Experiment shows that our\ngeneralized ${\\rm M}$-estimator performs better than the empirical mean\nestimator, the smaller the $\\alpha$ is, the better the performance will be. As\nan application, we study an $\\ell_{1}$ regression considered by Zhang et al.\n[Z-Z] who assumed that samples have finite variance, and relax their assumption\nto be finite {$\\alpha$-th} moment with $\\alpha \\in (1,2)$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 14:08:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Peng", ""], ["Jin", "Xinghu", ""], ["Li", "Xiang", ""], ["Xu", "Lihu", ""]]}, {"id": "2010.05146", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "On the Le Cam distance between Poisson and Gaussian experiments and the\n  asymptotic properties of Szasz estimators", "comments": "19 pages, 1 figure", "journal-ref": "J. Math. Anal. Appl. 499 (2021), no. 1, 1-18", "doi": "10.1016/j.jmaa.2021.125033", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove a local limit theorem for the ratio of the Poisson\ndistribution to the Gaussian distribution with the same mean and variance,\nusing only elementary methods (Taylor expansions and Stirling's formula). We\nthen apply the result to derive an upper bound on the Le Cam distance between\nPoisson and Gaussian experiments, which gives a complete proof of the sketch\nprovided in the unpublished set of lecture notes by Pollard (2010), who uses a\ndifferent approach. We also use the local limit theorem to derive the\nasymptotics of the variance for Bernstein c.d.f. and density estimators with\nPoisson weights on the positive half-line (also called Szasz estimators). The\npropagation of errors in the literature due to the incorrect estimate in Lemma\n2 (iv) of Leblanc (2012) is addressed in the Appendix.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 03:11:11 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 15:49:31 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 21:47:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2010.05170", "submitter": "Licong Lin", "authors": "Licong Lin, Edgar Dobriban", "title": "What causes the test error? Going beyond bias-variance via ANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning methods are often overparametrized, allowing\nadaptation to the data at a fine level. This can seem puzzling; in the worst\ncase, such models do not need to generalize. This puzzle inspired a great\namount of work, arguing when overparametrization reduces test error, in a\nphenomenon called \"double descent\". Recent work aimed to understand in greater\ndepth why overparametrization is helpful for generalization. This leads to\ndiscovering the unimodality of variance as a function of the level of\nparametrization, and to decomposing the variance into that arising from label\nnoise, initialization, and randomness in the training data to understand the\nsources of the error.\n  In this work we develop a deeper understanding of this area. Specifically, we\npropose using the analysis of variance (ANOVA) to decompose the variance in the\ntest error in a symmetric way, for studying the generalization performance of\ncertain two-layer linear and non-linear networks. The advantage of the analysis\nof variance is that it reveals the effects of initialization, label noise, and\ntraining data more clearly than prior approaches. Moreover, we also study the\nmonotonicity and unimodality of the variance components. While prior work\nstudied the unimodality of the overall variance, we study the properties of\neach term in variance decomposition.\n  One key insight is that in typical settings, the interaction between training\nsamples and initialization can dominate the variance; surprisingly being larger\nthan their marginal effect. Also, we characterize \"phase transitions\" where the\nvariance changes from unimodal to monotone. On a technical level, we leverage\nadvanced deterministic equivalent techniques for Haar random matrices, that --\nto our knowledge -- have not yet been used in the area. We also verify our\nresults in numerical simulations and on empirical data examples.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 05:21:13 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:55:36 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 06:46:33 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lin", "Licong", ""], ["Dobriban", "Edgar", ""]]}, {"id": "2010.05220", "submitter": "Erin Gabriel", "authors": "Erin E. Gabriel, Arvid Sj\\\"olander, Michael C. Sachs", "title": "Nonparametric bounds for causal effects in imperfect randomized\n  experiments", "comments": "35 pages, 5 figures, includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonignorable missingness and noncompliance can occur even in well-designed\nrandomized experiments making the intervention effect that the experiment was\ndesigned to estimate nonidentifiable. Nonparametric causal bounds provide a way\nto narrow the range of possible values for a nonidentifiable causal effect with\nminimal assumptions. We derive novel bounds for the causal risk difference for\na binary outcome and intervention in randomized experiments with nonignorable\nmissingness caused by a variety of mechanisms and with or without\nnoncompliance. We illustrate the use of the proposed bounds in our motivating\ndata example of peanut consumption on the development of peanut allergies in\ninfants.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 10:51:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gabriel", "Erin E.", ""], ["Sj\u00f6lander", "Arvid", ""], ["Sachs", "Michael C.", ""]]}, {"id": "2010.05306", "submitter": "Elina Robeva", "authors": "Yiheng Liu, Elina Robeva, and Huanqing Wang", "title": "Learning Linear Non-Gaussian Graphical Models with Multidirected Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method to learn the underlying acyclic mixed\ngraph of a linear non-Gaussian structural equation model given observational\ndata. We build on an algorithm proposed by Wang and Drton, and we show that one\ncan augment the hidden variable structure of the recovered model by learning\n{\\em multidirected edges} rather than only directed and bidirected ones.\nMultidirected edges appear when more than two of the observed variables have a\nhidden common cause. We detect the presence of such hidden causes by looking at\nhigher order cumulants and exploiting the multi-trek rule. Our method recovers\nthe correct structure when the underlying graph is a bow-free acyclic mixed\ngraph with potential multi-directed edges.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 18:10:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Liu", "Yiheng", ""], ["Robeva", "Elina", ""], ["Wang", "Huanqing", ""]]}, {"id": "2010.05321", "submitter": "Viet Anh Nguyen", "authors": "Viet Anh Nguyen and Xuhui Zhang and Jose Blanchet and Angelos\n  Georghiou", "title": "Distributionally Robust Parametric Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parameter estimation problem of a probabilistic generative\nmodel prescribed using a natural exponential family of distributions. For this\nproblem, the typical maximum likelihood estimator usually overfits under\nlimited training sample size, is sensitive to noise and may perform poorly on\ndownstream predictive tasks. To mitigate these issues, we propose a\ndistributionally robust maximum likelihood estimator that minimizes the\nworst-case expected log-loss uniformly over a parametric Kullback-Leibler ball\naround a parametric nominal distribution. Leveraging the analytical expression\nof the Kullback-Leibler divergence between two distributions in the same\nnatural exponential family, we show that the min-max estimation problem is\ntractable in a broad setting, including the robust training of generalized\nlinear models. Our novel robust estimator also enjoys statistical consistency\nand delivers promising empirical results in both regression and classification\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:05:49 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Zhang", "Xuhui", ""], ["Blanchet", "Jose", ""], ["Georghiou", "Angelos", ""]]}, {"id": "2010.05373", "submitter": "Viet Anh Nguyen", "authors": "Viet Anh Nguyen and Fan Zhang and Jose Blanchet and Erick Delage and\n  Yinyu Ye", "title": "Distributionally Robust Local Non-parametric Conditional Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional estimation given specific covariate values (i.e., local\nconditional estimation or functional estimation) is ubiquitously useful with\napplications in engineering, social and natural sciences. Existing data-driven\nnon-parametric estimators mostly focus on structured homogeneous data (e.g.,\nweakly independent and stationary data), thus they are sensitive to adversarial\nnoise and may perform poorly under a low sample size. To alleviate these\nissues, we propose a new distributionally robust estimator that generates\nnon-parametric local estimates by minimizing the worst-case conditional\nexpected loss over all adversarial distributions in a Wasserstein ambiguity\nset. We show that despite being generally intractable, the local estimator can\nbe efficiently found via convex optimization under broadly applicable settings,\nand it is robust to the corruption and heterogeneity of the data. Experiments\nwith synthetic and MNIST datasets show the competitive performance of this new\nclass of estimators.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 00:11:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Zhang", "Fan", ""], ["Blanchet", "Jose", ""], ["Delage", "Erick", ""], ["Ye", "Yinyu", ""]]}, {"id": "2010.05780", "submitter": "Chad M. Topaz", "authors": "Lu Xian, Henry Adams, Chad M. Topaz, Lori Ziegelmeier", "title": "Capturing Dynamics of Time-Varying Data via Topology", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.AT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to understanding complex data is to study its shape through the\nlens of algebraic topology. While the early development of topological data\nanalysis focused primarily on static data, in recent years, theoretical and\napplied studies have turned to data that varies in time. A time-varying\ncollection of metric spaces as formed, for example, by a moving school of fish\nor flock of birds, can contain a vast amount of information. There is often a\nneed to simplify or summarize the dynamic behavior. We provide an introduction\nto topological summaries of time-varying metric spaces including vineyards\n[19], crocker plots [56], and multiparameter rank functions [37]. We then\nintroduce a new tool to summarize time-varying metric spaces: a crocker stack.\nCrocker stacks are convenient for visualization, amenable to machine learning,\nand satisfy a desirable continuity property which we prove. We demonstrate the\nutility of crocker stacks for a parameter identification task involving an\ninfluential model of biological aggregations [58]. Altogether, we aim to bring\nthe broader applied mathematics community up-to-date on topological summaries\nof time-varying metric spaces.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 20:07:40 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 14:10:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Xian", "Lu", ""], ["Adams", "Henry", ""], ["Topaz", "Chad M.", ""], ["Ziegelmeier", "Lori", ""]]}, {"id": "2010.05899", "submitter": "Paria Rashidinejad", "authors": "Paria Rashidinejad, Jiantao Jiao, Stuart Russell", "title": "SLIP: Learning to Predict in Unknown Dynamical Systems with Long-Term\n  Memory", "comments": "47 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and practical (polynomial time) algorithm for online\nprediction in unknown and partially observed linear dynamical systems (LDS)\nunder stochastic noise. When the system parameters are known, the optimal\nlinear predictor is the Kalman filter. However, the performance of existing\npredictive models is poor in important classes of LDS that are only marginally\nstable and exhibit long-term forecast memory. We tackle this problem through\nbounding the generalized Kolmogorov width of the Kalman filter model by\nspectral methods and conducting tight convex relaxation. We provide a\nfinite-sample analysis, showing that our algorithm competes with Kalman filter\nin hindsight with only logarithmic regret. Our regret analysis relies on\nMendelson's small-ball method, providing sharp error bounds without\nconcentration, boundedness, or exponential forgetting assumptions. We also give\nexperimental results demonstrating that our algorithm outperforms\nstate-of-the-art methods. Our theoretical and experimental results shed light\non the conditions required for efficient probably approximately correct (PAC)\nlearning of the Kalman filter from partially observed data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:50:21 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Rashidinejad", "Paria", ""], ["Jiao", "Jiantao", ""], ["Russell", "Stuart", ""]]}, {"id": "2010.06103", "submitter": "Qianqian Zhu", "authors": "Hua Liu, Songhua Tan and Qianqian Zhu", "title": "Quasi-maximum Likelihood Inference for Linear Double Autoregressive\n  Models", "comments": "5 table and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the quasi-maximum likelihood inference including\nestimation, model selection and diagnostic checking for linear double\nautoregressive (DAR) models, where all asymptotic properties are established\nunder only fractional moment of the observed process. We propose a Gaussian\nquasi-maximum likelihood estimator (G-QMLE) and an exponential quasi-maximum\nlikelihood estimator (E-QMLE) for the linear DAR model, and establish the\nconsistency and asymptotic normality for both estimators. Based on the G-QMLE\nand E-QMLE, two Bayesian information criteria are proposed for model selection,\nand two mixed portmanteau tests are constructed to check the adequacy of fitted\nmodels. Moreover, we compare the proposed G-QMLE and E-QMLE with the existing\ndoubly weighted quantile regression estimator in terms of the asymptotic\nefficiency and numerical performance. Simulation studies illustrate the\nfinite-sample performance of the proposed inference tools, and a real example\non the Bitcoin return series shows the usefulness of the proposed inference\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:19:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Hua", ""], ["Tan", "Songhua", ""], ["Zhu", "Qianqian", ""]]}, {"id": "2010.06153", "submitter": "Thomas Fung", "authors": "Thomas Fung and Eugene Seneta", "title": "Tail asymptotics for the bivariate equi-skew Variance-Gamma distribution", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We derive the asymptotic rate of decay to zero of the tail dependence of the\nbivariate skew Variance Gamma (VG) distribution under the equal-skewness\ncondition, as an explicit regularly varying function. Our development is in\nterms of a slightly more general bivariate skew Generalized Hyperbolic (GH)\ndistribution. Our initial reduction of the bivariate problem to a univariate\none is motivated by our earlier study of tail dependence rate for the bivariate\nskew normal distribution\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:53:37 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fung", "Thomas", ""], ["Seneta", "Eugene", ""]]}, {"id": "2010.06168", "submitter": "Sophie Langer Dr.", "authors": "Sophie Langer", "title": "Analysis of the rate of convergence of fully connected deep neural\n  network regression estimates with smooth activation function", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.11133", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article contributes to the current statistical theory of deep neural\nnetworks (DNNs). It was shown that DNNs are able to circumvent the so--called\ncurse of dimensionality in case that suitable restrictions on the structure of\nthe regression function hold. In most of those results the tuning parameter is\nthe sparsity of the network, which describes the number of non-zero weights in\nthe network. This constraint seemed to be the key factor for the good rate of\nconvergence results. Recently, the assumption was disproved. In particular, it\nwas shown that simple fully connected DNNs can achieve the same rate of\nconvergence. Those fully connected DNNs are based on the unbounded ReLU\nactivation function. In this article we extend the results to smooth activation\nfunctions, i.e., to the sigmoid activation function. It is shown that\nestimators based on fully connected DNNs with sigmoid activation function also\nachieve the minimax rates of convergence (up to $\\ln n$-factors). In our result\nthe number of hidden layers is fixed, the number of neurons per layer tends to\ninfinity for sample size tending to infinity and a bound for the weights in the\nnetwork is given.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:06:26 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Langer", "Sophie", ""]]}, {"id": "2010.06284", "submitter": "Vitalii Makogin", "authors": "Mehmet Siddik Cadirci, Dafydd Evans, Nikolai Leonenko, Vitalii Makogin", "title": "Entropy-based test for generalized Gaussian distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide the proof of $L^2$ consistency for the $k$th\nnearest neighbour distance estimator of the Shannon entropy for an arbitrary\nfixed $k\\geq 1.$ We construct the non-parametric test of goodness-of-fit for a\nclass of introduced generalized multivariate Gaussian distributions based on a\nmaximum entropy principle. The theoretical results are followed by numerical\nstudies on simulated samples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:56:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cadirci", "Mehmet Siddik", ""], ["Evans", "Dafydd", ""], ["Leonenko", "Nikolai", ""], ["Makogin", "Vitalii", ""]]}, {"id": "2010.06340", "submitter": "Randolf Altmeyer", "authors": "Randolf Altmeyer, Till Bretschneider, Josef Jan\\'ak, Markus Rei{\\ss}", "title": "Parameter Estimation in an SPDE Model for Cell Repolarisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a concrete setting where stochastic partial differential equations (SPDEs)\nare able to model real phenomena, we propose a stochastic Meinhardt model for\ncell repolarisation and study how parameter estimation techniques developed for\nsimple linear SPDE models apply in this situation. We establish the existence\nof mild SPDE solutions and we investigate the impact of the driving noise\nprocess on pattern formation in the solution. We then pursue estimation of the\ndiffusion term and show asymptotic normality for our estimator as the space\nresolution becomes finer. The finite sample performance is investigated for\nsynthetic and real data resembling experimental findings for cell orientation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:53:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Altmeyer", "Randolf", ""], ["Bretschneider", "Till", ""], ["Jan\u00e1k", "Josef", ""], ["Rei\u00df", "Markus", ""]]}, {"id": "2010.06373", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "Generalized Rescaled Polya urn and its statistical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Generalized Rescaled Polya (GRP) urn, that provides a\ngenerative model for a chi-squared test of goodness of fit for the long-term\nprobabilities of clustered data, with independence between clusters and\ncorrelation, due to a reinforcement mechanism, inside each cluster. We apply\nthe proposed test to a data set of Twitter posts about COVID-19 pandemic: in a\nfew words, for a classical chi-squared test the data result strongly\nsignificant for the rejection of the null hypothesis (the daily long-run\nsentiment rate remains constant), but, taking into account the correlation\namong data, the introduced test leads to a different conclusion. Beside the\nstatistical application, we point out that the GRP urn is a simple variant of\nthe standard Eggenberger-Polya urn, that, with suitable choices of the\nparameters, shows \"local\" reinforcement, almost sure convergence of the\nempirical mean to a deterministic limit and different asymptotic behaviours of\nthe predictive mean. Moreover, the study of this model provides the opportunity\nto analyze stochastic approximation dynamics, that are unusual in the related\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:30:02 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:14:54 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 10:14:38 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "2010.06420", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat, Fabien Panloup, Cl\\'ement Pellegrini", "title": "On the cost of Bayesian posterior mean strategy for log-concave models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of computing Bayesian estimators\nusing Langevin Monte-Carlo type approximation. The novelty of this paper is to\nconsider together the statistical and numerical counterparts (in a general\nlog-concave setting). More precisely, we address the following question: given\n$n$ observations in $\\mathbb{R}^q$ distributed under an unknown probability\n$\\mathbb{P}_{\\theta^\\star}$ with $\\theta^\\star \\in \\mathbb{R}^d$ , what is the\noptimal numerical strategy and its cost for the approximation of $\\theta^\\star$\nwith the Bayesian posterior mean? To answer this question, we establish some\nquantitative statistical bounds related to the underlying Poincar\\'e constant\nof the model and establish new results about the numerical approximation of\nGibbs measures by Cesaro averages of Euler schemes of (over-damped) Langevin\ndiffusions. These last results include in particular some quantitative controls\nin the weakly convex case based on new bounds on the solution of the related\nPoisson equation of the diffusion.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 04:51:10 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Panloup", "Fabien", ""], ["Pellegrini", "Cl\u00e9ment", ""]]}, {"id": "2010.06562", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Ziteng Sun, and Himanshu Tyagi", "title": "Unified lower bounds for interactive high-dimensional estimation under\n  information constraints", "comments": "Significant improvements: handle sparse parameter estimation,\n  simplify and generalize arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of distributed parameter estimation using interactive\nprotocols subject to local information constraints such as bandwidth\nlimitations, local differential privacy, and restricted measurements. We\nprovide a unified framework enabling us to derive a variety of (tight) minimax\nlower bounds for different parametric families of distributions, both\ncontinuous and discrete, under any $\\ell_p$ loss. Our lower bound framework is\nversatile and yields \"plug-and-play\" bounds that are widely applicable to a\nlarge range of estimation problems. In particular, our approach recovers bounds\nobtained using data processing inequalities and Cram\\'er--Rao bounds, two other\nalternative approaches for proving lower bounds in our setting of interest.\nFurther, for the families considered, we complement our lower bounds with\nmatching upper bounds.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:25:19 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:04:18 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 18:49:21 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 13:54:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "2010.06768", "submitter": "Jeffrey P. Spence", "authors": "Jeffrey P. Spence", "title": "Flexible mean field variational inference using mixtures of\n  non-overlapping exponential families", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse models are desirable for many applications across diverse domains as\nthey can perform automatic variable selection, aid interpretability, and\nprovide regularization. When fitting sparse models in a Bayesian framework,\nhowever, analytically obtaining a posterior distribution over the parameters of\ninterest is intractable for all but the simplest cases. As a result\npractitioners must rely on either sampling algorithms such as Markov chain\nMonte Carlo or variational methods to obtain an approximate posterior. Mean\nfield variational inference is a particularly simple and popular framework that\nis often amenable to analytically deriving closed-form parameter updates. When\nall distributions in the model are members of exponential families and are\nconditionally conjugate, optimization schemes can often be derived by hand.\nYet, I show that using standard mean field variational inference can fail to\nproduce sensible results for models with sparsity-inducing priors, such as the\nspike-and-slab. Fortunately, such pathological behavior can be remedied as I\nshow that mixtures of exponential family distributions with non-overlapping\nsupport form an exponential family. In particular, any mixture of a diffuse\nexponential family and a point mass at zero to model sparsity forms an\nexponential family. Furthermore, specific choices of these distributions\nmaintain conditional conjugacy. I use two applications to motivate these\nresults: one from statistical genetics that has connections to generalized\nleast squares with a spike-and-slab prior on the regression coefficients; and\nsparse probabilistic principal component analysis. The theoretical results\npresented here are broadly applicable beyond these two examples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 01:46:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Spence", "Jeffrey P.", ""]]}, {"id": "2010.06790", "submitter": "Mingzhou Xu", "authors": "Mingzhou Xu, Yunzheng Ding, Yongzheng Zhou (School of Information\n  Engineering, Jingdezhen Ceramic Institute Jingdezhen, China)", "title": "Central Limit Theorem and Moderate deviation for nonhomogenenous Markov\n  chains", "comments": "8 pages", "journal-ref": "Journal of Mathemtatics (PRC) , Vol. 39, No.1, 137-146, 2019", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our purpose is to prove central limit theorem for countable nonhomogeneous\nMarkov chain under the condition of uniform convergence of transition\nprobability matrices for countable nonhomogeneous Markov chain in Ces\\`aro\nsense. Furthermore, we obtain a corresponding moderate deviation theorem for\ncountable nonhomogeneous Markov chain by G\\\"artner-Ellis theorem and\nexponential equivalent method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 03:15:45 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Xu", "Mingzhou", "", "School of Information\n  Engineering, Jingdezhen Ceramic Institute Jingdezhen, China"], ["Ding", "Yunzheng", "", "School of Information\n  Engineering, Jingdezhen Ceramic Institute Jingdezhen, China"], ["Zhou", "Yongzheng", "", "School of Information\n  Engineering, Jingdezhen Ceramic Institute Jingdezhen, China"]]}, {"id": "2010.06851", "submitter": "Kangqiang Li", "authors": "Kangqiang Li, Han Bao, Songqiao Tang and Lixin Zhang", "title": "Robust covariance estimation for distributed principal component\n  analysis", "comments": "28 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fan et al. [$\\mathit{Annals}$ $\\mathit{of}$ $\\mathit{Statistics}$\n$\\textbf{47}$(6) (2019) 3009-3031] proposed a distributed principal component\nanalysis (PCA) algorithm to significantly reduce the communication cost between\nmultiple servers. In this paper, we robustify their distributed algorithm by\nusing robust covariance matrix estimators respectively proposed by Minsker\n[$\\mathit{Annals}$ $\\mathit{of}$ $\\mathit{Statistics}$ $\\textbf{46}$(6A) (2018)\n2871-2903] and Ke et al. [$\\mathit{Statistical}$ $\\mathit{Science}$\n$\\textbf{34}$(3) (2019) 454-471] instead of the sample covariance matrix. We\nextend the deviation bound of robust covariance estimators with bounded fourth\nmoments to the case of the heavy-tailed distribution under only bounded\n$2+\\epsilon$ moments assumption. The theoretical results show that after the\nshrinkage or truncation treatment for the sample covariance matrix, the\nstatistical error rate of the final estimator produced by the robust algorithm\nis the same as that of sub-Gaussian tails, when $\\epsilon \\geq 2$ and the\nsampling distribution is symmetric innovation. While $2 > \\epsilon >0$, the\nrate with respect to the sample size of each server is slower than that of the\nbounded fourth moment assumption. Extensive numerical results support the\ntheoretical analysis, and indicate that the algorithm performs better than the\noriginal distributed algorithm and is robust to heavy-tailed data and outliers.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:29:39 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 05:00:30 GMT"}, {"version": "v3", "created": "Sun, 7 Mar 2021 05:12:02 GMT"}, {"version": "v4", "created": "Sun, 28 Mar 2021 11:16:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Kangqiang", ""], ["Bao", "Han", ""], ["Tang", "Songqiao", ""], ["Zhang", "Lixin", ""]]}, {"id": "2010.06968", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen", "title": "Stochastic modelling of Gaussian processes by improper linear\n  functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches to stochastic processes exist, noting that key properties\nsuch as measurability and continuity are not trivially satisfied. We introduce\na new theory for Gaussian processes using improper linear functionals. Using a\ncollection of i.i.d. standard normal variables, we define Gaussian white noise\nand discuss its properties. This is extended to general Gaussian processes on\nHilbert space, where the variance is allowed to be any suitable operator. Our\nmain focus is $L^2$ spaces, and we discuss criteria for Gaussian processes to\nbe continuous in this setting. Finally, we outline a framework for statistical\ninference using the presented theory with focus on the special case of\n$L^2[0,1]$. We introduce the Fredholm determinant into the functional\nlog-likelihood. We demonstrate that the naive functional log-likelihood is not\nconsistent with the multivariate likelihood. A correction term is introduced,\nand we prove an asymptotical result.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:21:49 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Olsen", "Niels Lundtorp", ""]]}, {"id": "2010.06999", "submitter": "Sebastian M\\\"uller", "authors": "Abraham Gutierrez, Sebastian M\\\"uller", "title": "Estimations of means and variances in a Markov linear model", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression models and ANOVA are probably the most frequently\napplied methods of all statistical analyses. We study the case where the\npredictors are qualitative variables, and the response variable is\nquantitative. In this case, we propose an alternative to the classic approaches\nthat does not assume homoscedasticity but assumes that a Markov chain can\ndescribe the covariates' correlations. This approach transforms the dependent\ncovariates using a change of measure to independent covariates. The transformed\nestimates allow a pairwise comparison of the mean and variance of the\ncontribution of different values of the covariates. We show that under standard\nmoment conditions, the estimators are asymptotically normally distributed. We\ntest our method with data from simulations and apply it to several classic data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:24:08 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 09:13:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gutierrez", "Abraham", ""], ["M\u00fcller", "Sebastian", ""]]}, {"id": "2010.07065", "submitter": "Rasmus Erlemann", "authors": "Bo Henry Lindqvist, Rasmus Erlemann, Gunnar Taraldsen", "title": "Conditional Monte Carlo revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Monte Carlo refers to sampling from the conditional distribution\nof a random vector X given the value T(X) = t for a function T(X). Classical\nconditional Monte Carlo methods were designed for estimating conditional\nexpectations of functions of X by sampling from unconditional distributions\nobtained by certain weighting schemes. The basic ingredients were the use of\nimportance sampling and change of variables. In the present paper we\nreformulate the problem by introducing an artificial parametric model,\nrepresenting the conditional distribution of X given T(X)=t within this new\nmodel. The key is to provide the parameter of the artificial model by a\ndistribution. The approach is illustrated by several examples, which are\nparticularly chosen to illustrate conditional sampling in cases where such\nsampling is not straightforward. A simulation study and an application to\ngoodness-of-fit testing of real data are also given.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:11:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lindqvist", "Bo Henry", ""], ["Erlemann", "Rasmus", ""], ["Taraldsen", "Gunnar", ""]]}, {"id": "2010.07072", "submitter": "Rasmus Erlemann", "authors": "Rasmus Erlemann, Richard Lockhart, Rihan Yao", "title": "Cramer-von Mises tests for Change Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two nonparametric tests of the hypothesis that a sequence of\nindependent observations is identically distributed against the alternative\nthat at a single change point the distribution changes. The tests are based on\nthe Cramer-von Mises two-sample test computed at every possible change point.\nOne test uses the largest such test statistic over all possible change points;\nthe other averages over all possible change points. Large sample theory for the\naverage statistic is shown to provide useful p-values much more quickly than\nbootstrapping, particularly in long sequences. Power is analyzed for contiguous\nalternatives. The average statistic is shown to have limiting power larger than\nits level for such alternative sequences. Evidence is presented that this is\nnot true for the maximal statistic. Asymptotic methods and bootstrapping are\nused for constructing the test distribution. Performance of the tests is\nchecked with a Monte Carlo power study for various alternative distributions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:21:12 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Erlemann", "Rasmus", ""], ["Lockhart", "Richard", ""], ["Yao", "Rihan", ""]]}, {"id": "2010.07140", "submitter": "James Lucas", "authors": "James Lucas, Mengye Ren, Irene Kameni, Toniann Pitassi, Richard Zemel", "title": "Theoretical bounds on estimation error for meta-learning", "comments": "12 pages in main paper,22 pages in appendix,4 figures total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have traditionally been developed under the\nassumption that the training and test distributions match exactly. However,\nrecent success in few-shot learning and related problems are encouraging signs\nthat these models can be adapted to more realistic settings where train and\ntest distributions differ. Unfortunately, there is severely limited theoretical\nsupport for these algorithms and little is known about the difficulty of these\nproblems. In this work, we provide novel information-theoretic lower-bounds on\nminimax rates of convergence for algorithms that are trained on data from\nmultiple sources and tested on novel data. Our bounds depend intuitively on the\ninformation shared between sources of data, and characterize the difficulty of\nlearning in this setting for arbitrary algorithms. We demonstrate these bounds\non a hierarchical Bayesian model of meta-learning, computing both upper and\nlower bounds on parameter estimation via maximum-a-posteriori inference.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:57:21 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lucas", "James", ""], ["Ren", "Mengye", ""], ["Kameni", "Irene", ""], ["Pitassi", "Toniann", ""], ["Zemel", "Richard", ""]]}, {"id": "2010.07242", "submitter": "Nan Wu", "authors": "David B Dunson, Hau-Tieng Wu and Nan Wu", "title": "Diffusion Based Gaussian Processes on Restricted Domains", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonparametric regression and spatial process modeling, it is common for\nthe inputs to fall in a restricted subset of Euclidean space. For example, the\nlocations at which spatial data are collected may be restricted to a narrow\nnon-linear subset, such as near the edge of a lake. Typical kernel-based\nmethods that do not take into account the intrinsic geometric of the domain\nacross which observations are collected may produce sub-optimal results. In\nthis article, we focus on solving this problem in the context of Gaussian\nprocess (GP) models, proposing a new class of diffusion-based GPs (DB-GPs),\nwhich learn a covariance that respects the geometry of the input domain. We use\nthe term `diffusion-based' as the idea is to measure intrinsic distances\nbetween inputs in a restricted domain via a diffusion process. As the heat\nkernel is intractable computationally, we approximate the covariance using\nfinitely-many eigenpairs of the Graph Laplacian (GL). Our proposed algorithm\nhas the same order of computational complexity as current GP algorithms using\nsimple covariance kernels. We provide substantial theoretical support for the\nDB-GP methodology, and illustrate performance gains through toy examples,\nsimulation studies, and applications to ecology data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:01:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dunson", "David B", ""], ["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "2010.07416", "submitter": "Tom\\'a\\v{s} Gonda", "authors": "Tobias Fritz, Tom\\'a\\v{s} Gonda, Paolo Perrone, Eigil Fjeldgren\n  Rischel", "title": "Representable Markov Categories and Comparison of Statistical\n  Experiments in Categorical Probability", "comments": "69 pages, color used in text and diagrams. v2: Measure-theoretic\n  formulation of BSS Theorem added to the introduction plus minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LO math.CT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov categories are a recent categorical approach to the mathematical\nfoundations of probability and statistics. Here, this approach is advanced by\nstating and proving equivalent conditions for second-order stochastic\ndominance, a widely used way of comparing probability distributions by their\nspread. Furthermore, we lay foundation for the theory of comparing statistical\nexperiments within Markov categories by stating and proving the classical\nBlackwell-Sherman-Stein Theorem. Our version not only offers new insight into\nthe proof, but its abstract nature also makes the result more general,\nautomatically specializing to the standard Blackwell-Sherman-Stein Theorem in\nmeasure-theoretic probability as well as a Bayesian version that involves\nprior-dependent garbling. Along the way, we define and characterize\nrepresentable Markov categories, within which one can talk about Markov kernels\nto or from spaces of distributions. We do so by exploring the relation between\nMarkov categories and Kleisli categories of probability monads.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 21:54:57 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 03:05:33 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Fritz", "Tobias", ""], ["Gonda", "Tom\u00e1\u0161", ""], ["Perrone", "Paolo", ""], ["Rischel", "Eigil Fjeldgren", ""]]}, {"id": "2010.07420", "submitter": "Marie Cottrell", "authors": "Marie Cottrell, Cynthia Faure, J\\'er\\^ome Lacaille, Madalina Olteanu", "title": "Anomaly Detection for Bivariate Signals", "comments": null, "journal-ref": "Rojas I., Joya G., Catala A. (eds). Advances in Computational\n  Intelligence, part 1, IWANN 2019, vol 11506, Springer, Cham, pp.162-173,\n  2019, Lecture Notes in Computer Science,,", "doi": "10.1007/978-3-030-20521-8_14", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anomaly detection problem for univariate or multivariate time series is a\ncritical question in many practical applications as industrial processes\ncontrol, biological measures, engine monitoring, supervision of all kinds of\nbehavior. In this paper we propose a simple and empirical approach to detect\nanomalies in the behavior of multivariate time series. The approach is based on\nthe empirical estimation of the conditional quantiles of the data, which\nprovides upper and lower bounds for the confidence tubes. The method is tested\non artificial data and its effectiveness has been proven in a real framework\nsuch as that of the monitoring of aircraft engines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 22:27:29 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cottrell", "Marie", ""], ["Faure", "Cynthia", ""], ["Lacaille", "J\u00e9r\u00f4me", ""], ["Olteanu", "Madalina", ""]]}, {"id": "2010.07501", "submitter": "Mingzhou Xu", "authors": "Mingzhou Xu and Kun Cheng (School of Information Engineering,\n  Jingdezhen Ceramic Institute, Jingdezhen, China)", "title": "Moderate deviations for empirical measures for nonhomogeneous Markov\n  chains", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that moderate deviations for empirical measures for countable\nnonhomogeneous Markov chains hold under the assumption of uniform convergence\nof transition probability matrices for countable nonhomogeneous Markov chains\nin Ces\\`aro sense.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:00:26 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Xu", "Mingzhou", "", "School of Information Engineering,\n  Jingdezhen Ceramic Institute, Jingdezhen, China"], ["Cheng", "Kun", "", "School of Information Engineering,\n  Jingdezhen Ceramic Institute, Jingdezhen, China"]]}, {"id": "2010.07596", "submitter": "Yury Kutoyants", "authors": "Yury A. Kutoyants", "title": "Hidden Markov Model Where Higher Noise Makes Smaller Errors", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parameter estimation in a partially observed\nlinear Gaussian system with small noises in the state and observation\nequations. We describe asymptotic properties of the MLE and Bayes estimators in\nthe setting with state and observation noises of possibly unequal intensities.\nIt is shown that both estimators are consistent, asymptotically normal with\nconvergent moments and asymptotically efficient. This model has an unusual\nfeature: larger noise in the state equation yields smaller estimation error.\nThe proofs are based on asymptotic analysis of the Kalman-Bucy filter and the\nassociated Riccati equation in particular.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:40:07 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kutoyants", "Yury A.", ""]]}, {"id": "2010.07603", "submitter": "Yury Kutoyants", "authors": "Yury A. Kutoyants", "title": "Quadratic Variation Estimation of Hidden Markov Process and Related\n  Problems", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partially observed linear Gaussian system of stochastic differential\nequations with low noise in observations is considered. A kernel-type\nestimators are used for estimation of the quadratic variation of the derivative\nof the limit of the observed process. Then this estimator is used for\nnonparametric estimation of the integral of the square of volatility of\nunobservable component. This estimator is also used for construction of\nsubstitution estimators in the case where the drift in observable component and\nthe volatility of the state component depend on some unknown parameter. Then\nthis substitution estimator and Fisher-score device allows us to introduce the\nOne-step MLE-process and adaptive Kalman-Bucy filter.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:58:24 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kutoyants", "Yury A.", ""]]}, {"id": "2010.07612", "submitter": "Yury Kutoyants", "authors": "O.V. Chernoyarov, A.S. Dabye, F.N. Diop, Yu.A. Kutoyants", "title": "On Non Asymptotic Expansion of the MME in the Case of Poisson\n  Observations", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of parameter estimation by observations of inhomogeneous Poisson\nprocesses is considered. The method of moments estimator is studied and its\nstochastic expansion is obtained. This stochastic expansion is then used to\nobtain the expansion of the moments of this estimator and the expansion of the\ndistribution function. The stochastic expansion, expansion of the moments and\nthe expansion of distribution function are non asymptotic in nature. Several\nexamples are considered.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:14:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chernoyarov", "O. V.", ""], ["Dabye", "A. S.", ""], ["Diop", "F. N.", ""], ["Kutoyants", "Yu. A.", ""]]}, {"id": "2010.07618", "submitter": "Yury Kutoyants", "authors": "Oleg V. Chernoyarov, Yury A. Kutoyants", "title": "Approximation of BSDE with Hidden Forward Equation and Unknown\n  Volatility", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper the problem of approximating the solution of BSDE is\nconsidered in the case where the solution of forward equation is observed in\nthe presence of small Gaussian noise. We suppose that the volatility of the\nforward equation depends on an unknown parameter. This approximation is made in\nseveral steps. First we obtain a preliminary estimator of the unknown\nparameter, then using Kalman-Bucy filtration equations and Fisher-score device\nwe construct an one-step MLE-process of this parameter. The solution of BSDE is\napproximated by means of the solution of PDE and the One-step MLE-process. The\nerror of approximation is described in different metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:29:25 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chernoyarov", "Oleg V.", ""], ["Kutoyants", "Yury A.", ""]]}, {"id": "2010.07624", "submitter": "Yury Kutoyants", "authors": "Yury A. Kutoyants", "title": "On Multi-step Estimation of Delay for SDE", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of delay estimation by the observations of the\nsolutions of several SDEs. It is known that the MLE for these models are\nconsistent and asymptotically normal, but the likelihood ratio functions are\nnot differentiable w.r.t. parameter and therefore numerical calculation of the\nMLEs has certain difficulties. We propose One-step and Two-step MLE, whose\ncalculation has no such problems and provide estimator asymptotically\nequivalent to the MLE. These constructions are realized in two or three steps.\nFirst we construct preliminary estimators which are consistent and\nasymptotically normal, but not asymptotically efficient. Then we use these\nestimators and modified Fisher-score device to obtain One-step and Two-step\nMLEs. We suppose that its numerical realization is much more simple. Stochastic\nPantograph equation is introduced and related statistical problems are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:39:00 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Kutoyants", "Yury A.", ""]]}, {"id": "2010.08007", "submitter": "Shashank Singh", "authors": "Shashank Singh", "title": "Continuum-Armed Bandits: A Function Space Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continuum-armed bandits (a.k.a., black-box or $0^{th}$-order optimization)\ninvolves optimizing an unknown objective function given an oracle that\nevaluates the function at a query point, with the goal of using as few query\npoints as possible. In the most well-studied case, the objective function is\nassumed to be Lipschitz continuous and minimax rates of simple and cumulative\nregrets are known in both noiseless and noisy settings. This paper studies\ncontinuum-armed bandits under more general smoothness conditions, namely Besov\nsmoothness conditions, on the objective function. In both noiseless and noisy\nconditions, we derive minimax rates under simple and cumulative regrets. Our\nresults show that minimax rates over objective functions in a Besov space are\nidentical to minimax rates over objective functions in the smallest H\\\"older\nspace into which the Besov space embeds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 20:21:44 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:50:03 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 20:22:25 GMT"}, {"version": "v4", "created": "Sun, 21 Mar 2021 22:22:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Singh", "Shashank", ""]]}, {"id": "2010.08022", "submitter": "Tim Barfoot", "authors": "Timothy D Barfoot", "title": "Fundamental Linear Algebra Problem of Gaussian Inference", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underlying many Bayesian inference techniques that seek to approximate the\nposterior as a Gaussian distribution is a fundamental linear algebra problem\nthat must be solved for both the mean and key entries of the covariance. Even\nwhen the true posterior is not Gaussian (e.g., in the case of nonlinear\nmeasurement functions) we can use variational schemes that repeatedly solve\nthis linear algebra problem at each iteration. In most cases, the question is\nnot whether a solution to this problem exists, but rather how we can exploit\nproblem-specific structure to find it efficiently. Our contribution is to\nclearly state the Fundamental Linear Algebra Problem of Gaussian Inference\n(FLAPOGI) and to provide a novel presentation (using Kronecker algebra) of the\nnot-so-well-known result of Takahashi et al. (1973) that makes it possible to\nsolve for key entries of the covariance matrix. We first provide a global\nsolution and then a local version that can be implemented using local message\npassing amongst a collection of agents calculating in parallel. Contrary to\nbelief propagation, our local scheme is guaranteed to converge in both the mean\nand desired covariance quantities to the global solution even when the\nunderlying factor graph is loopy; in the case of synchronous updates, we\nprovide a bound on the number of iterations required for convergence. Compared\nto belief propagation, this guaranteed convergence comes at the cost of\nadditional storage, calculations, and communication links in the case of loops;\nhowever, we show how these can be automatically constructed on the fly using\nonly local information.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:09:17 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Barfoot", "Timothy D", ""]]}, {"id": "2010.08039", "submitter": "Philipp Harms", "authors": "Philipp Harms, Peter W. Michor, Xavier Pennec and Stefan Sommer", "title": "Geometry of Sample Spaces", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.MG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics, independent, identically distributed random samples do not\ncarry a natural ordering, and their statistics are typically invariant with\nrespect to permutations of their order. Thus, an $n$-sample in a space $M$ can\nbe considered as an element of the quotient space of $M^n$ modulo the\npermutation group. The present paper takes this definition of sample space and\nthe related concept of orbit types as a starting point for developing a\ngeometric perspective on statistics. We aim at deriving a general mathematical\nsetting for studying the behavior of empirical and population means in spaces\nranging from smooth Riemannian manifolds to general stratified spaces.\n  We fully describe the orbifold and path-metric structure of the sample space\nwhen $M$ is a manifold or path-metric space, respectively. These results are\nnon-trivial even when $M$ is Euclidean. We show that the infinite sample space\nexists in a Gromov-Hausdorff type sense and coincides with the Wasserstein\nspace of probability distributions on $M$. We exhibit Fr\\'echet means and\n$k$-means as metric projections onto 1-skeleta or $k$-skeleta in Wasserstein\nspace, and we define a new and more general notion of polymeans. This geometric\ncharacterization via metric projections applies equally to sample and\npopulation means, and we use it to establish asymptotic properties of polymeans\nsuch as consistency and asymptotic normality.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:56:14 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Harms", "Philipp", ""], ["Michor", "Peter W.", ""], ["Pennec", "Xavier", ""], ["Sommer", "Stefan", ""]]}, {"id": "2010.08071", "submitter": "Nikolas Siapoutis", "authors": "Nikolas Siapoutis, Donald Richards and Bharath K. Sriperumbudur", "title": "Mean Shrinkage Estimation for High-Dimensional Diagonal Natural\n  Exponential Families", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimators have been studied widely in statistics and have profound\nimpact in many applications. In this paper, we study simultaneous estimation of\nthe mean parameters of random observations from a diagonal multivariate natural\nexponential family. More broadly, we study distributions for which the diagonal\nentries of the covariance matrix are certain quadratic functions of the mean\nparameter. We propose two classes of semi-parametric shrinkage estimators for\nthe mean vector and construct unbiased estimators of the corresponding risk.\nFurther, we establish the asymptotic consistency and convergence rates for\nthese shrinkage estimators under squared error loss as both $n$, the sample\nsize, and $p$, the dimension, tend to infinity. Finally, we consider the\ndiagonal multivariate natural exponential families, which have been classified\nas consisting of the normal, Poisson, gamma, multinomial, negative multinomial,\nand hybrid classes of distributions. We deduce consistency of our estimators in\nthe case of the normal, gamma, and negative multinomial distributions if $p\nn^{-1/3}\\log^{4/3}{n} \\rightarrow 0$ as $n,p \\rightarrow \\infty$, and for\nPoisson and multinomial distributions if $pn^{-1/2} \\rightarrow 0$ as $n,p\n\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:36:16 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Siapoutis", "Nikolas", ""], ["Richards", "Donald", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "2010.08082", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo", "title": "Nonparametric iterated-logarithm extensions of the sequential\n  generalized likelihood ratio test", "comments": "53 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nonparametric extension of the sequential generalized likelihood\nratio (GLR) test and corresponding time-uniform confidence sequences for the\nmean of a univariate distribution. By utilizing a geometric interpretation of\nthe GLR statistic, we derive a simple analytic upper bound on the probability\nthat it exceeds any prespecified boundary; these are intractable to approximate\nvia simulations due to infinite horizon of the tests and the composite\nnonparametric nulls under consideration. Using time-uniform boundary-crossing\ninequalities, we carry out a unified nonasymptotic analysis of expected sample\nsizes of one-sided and open-ended tests over nonparametric classes of\ndistributions (including sub-Gaussian, sub-exponential, sub-gamma, and\nexponential families). Finally, we present a flexible and practical method to\nconstruct time-uniform confidence sequences that are easily tunable to be\nuniformly close to the pointwise Chernoff bound over any target time interval.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 00:58:58 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:18:22 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 02:34:18 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 03:13:34 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Ramdas", "Aaditya", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2010.08097", "submitter": "Lam Ho", "authors": "Vu Dinh, Lam Si Tung Ho", "title": "Consistent Feature Selection for Analytic Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important steps toward interpretability and explainability of\nneural network models is feature selection, which aims to identify the subset\nof relevant features. Theoretical results in the field have mostly focused on\nthe prediction aspect of the problem with virtually no work on feature\nselection consistency for deep neural networks due to the model's severe\nnonlinearity and unidentifiability. This lack of theoretical foundation casts\ndoubt on the applicability of deep learning to contexts where correct\ninterpretations of the features play a central role.\n  In this work, we investigate the problem of feature selection for analytic\ndeep networks. We prove that for a wide class of networks, including deep\nfeed-forward neural networks, convolutional neural networks, and a major\nsub-class of residual neural networks, the Adaptive Group Lasso selection\nprocedure with Group Lasso as the base estimator is selection-consistent. The\nwork provides further evidence that Group Lasso might be inefficient for\nfeature selection with neural networks and advocates the use of Adaptive Group\nLasso over the popular Group Lasso.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 01:59:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Dinh", "Vu", ""], ["Ho", "Lam Si Tung", ""]]}, {"id": "2010.08127", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Behnam Neyshabur, Hanie Sedghi", "title": "The Deep Bootstrap Framework: Good Online Learners are Good Offline\n  Generalizers", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for reasoning about generalization in deep\nlearning. The core idea is to couple the Real World, where optimizers take\nstochastic gradient steps on the empirical loss, to an Ideal World, where\noptimizers take steps on the population loss. This leads to an alternate\ndecomposition of test error into: (1) the Ideal World test error plus (2) the\ngap between the two worlds. If the gap (2) is universally small, this reduces\nthe problem of generalization in offline learning to the problem of\noptimization in online learning. We then give empirical evidence that this gap\nbetween worlds can be small in realistic deep learning settings, in particular\nsupervised image classification. For example, CNNs generalize better than MLPs\non image distributions in the Real World, but this is \"because\" they optimize\nfaster on the population loss in the Ideal World. This suggests our framework\nis a useful tool for understanding generalization in deep learning, and lays a\nfoundation for future research in the area.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:07:49 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 03:24:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nakkiran", "Preetum", ""], ["Neyshabur", "Behnam", ""], ["Sedghi", "Hanie", ""]]}, {"id": "2010.08132", "submitter": "Yucong Ma", "authors": "Zheng Tracy Ke, Jun S. Liu, Yucong Ma", "title": "Power of FDR Control Methods: The Impact of Ranking Algorithm, Tampered\n  Design, and Symmetric Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the power of FDR control methods for high-dimensional variable selections\nhas been mostly evaluated empirically, we focus here on theoretical power\nanalyses of two recent such methods, the knockoff filter and the Gaussian\nmirror. We adopt the Rare/Weak signal model, popular in multiple testing and\nvariable selection literature, and characterize the rate of convergence of the\nnumber of false positives and the number of false negatives of FDR control\nmethods for particular classes of designs.\n  Our analyses lead to several noteworthy discoveries. First, the choice of the\nsymmetric statistic in FDR control methods crucially affects the power. Second,\nwith a proper symmetric statistic, the operation of adding \"noise\" to achieve\nFDR control yields almost no loss of power compared with its prototype, at\nleast for some special classes of designs. Third, the knockoff filter and\nGaussian mirror have comparable power for orthogonal designs, but they behave\ndifferently for non-orthogonal designs. We study the block-wise diagonal\ndesigns and show that the knockoff filter has a higher power when the\nregression coefficient vector is extremely sparse, and the Gaussian mirror has\na higher power when the coefficient vector is moderately sparse.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:30:41 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 22:50:22 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Liu", "Jun S.", ""], ["Ma", "Yucong", ""]]}, {"id": "2010.08148", "submitter": "Yiming Xu", "authors": "Braxton Osting, Dong Wang, Yiming Xu and Dominique Zosso", "title": "Consistency of archetypal analysis", "comments": "30 pages, 9 figures; add some details to the proof of Lemma 2.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal analysis is an unsupervised learning method that uses a convex\npolytope to summarize multivariate data. For fixed $k$, the method finds a\nconvex polytope with $k$ vertices, called archetype points, such that the\npolytope is contained in the convex hull of the data and the mean squared\ndistance between the data and the polytope is minimal. In this paper, we prove\na consistency result that shows if the data is independently sampled from a\nprobability measure with bounded support, then the archetype points converge to\na solution of the continuum version of the problem, of which we identify and\nestablish several properties. We also obtain the convergence rate of the\noptimal objective values under appropriate assumptions on the distribution. If\nthe data is independently sampled from a distribution with unbounded support,\nwe also prove a consistency result for a modified method that penalizes the\ndispersion of the archetype points. Our analysis is supported by detailed\ncomputational experiments of the archetype points for data sampled from the\nuniform distribution in a disk, the normal distribution, an annular\ndistribution, and a Gaussian mixture model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:07:26 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:11:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Osting", "Braxton", ""], ["Wang", "Dong", ""], ["Xu", "Yiming", ""], ["Zosso", "Dominique", ""]]}, {"id": "2010.08236", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Wesley Tansey, Yanzhen Chen", "title": "Quantile regression with deep ReLU Networks: Estimators and minimax\n  rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is the task of estimating a specified percentile\nresponse, such as the median, from a collection of known covariates. We study\nquantile regression with rectified linear unit (ReLU) neural networks as the\nchosen model class. We derive an upper bound on the expected mean squared error\nof a ReLU network used to estimate any quantile conditional on a set of\ncovariates. This upper bound only depends on the best possible approximation\nerror, the number of layers in the network, and the number of nodes per layer.\nWe further show upper bounds that are tight for two large classes of functions:\ncompositions of H\\\"older functions and members of a Besov space. These tight\nbounds imply ReLU networks with quantile regression achieve minimax rates for\nbroad collections of function types. Unlike existing work, the theoretical\nresults hold under minimal assumptions and apply to general error\ndistributions, including heavy-tailed distributions. Empirical simulations on a\nsuite of synthetic response functions demonstrate the theoretical results\ntranslate to practical implementations of ReLU networks. Overall, the\ntheoretical and empirical results provide insight into the strong performance\nof ReLU neural networks for quantile regression across a broad range of\nfunction classes and error distributions. All code for this paper is publicly\navailable at https://github.com/tansey/quantile-regression.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:34:04 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:20:07 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 11:39:50 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 04:53:49 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2020 02:40:16 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Tansey", "Wesley", ""], ["Chen", "Yanzhen", ""]]}, {"id": "2010.08384", "submitter": "Alessandro De Gregorio", "authors": "Alessandro De Gregorio, Francesco Iafrate", "title": "Regularized Bridge-type estimation with multiple penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to introduce an adaptive penalized estimator for\nidentifying the true reduced parametric model under the sparsity assumption. In\nparticular, we deal with the framework where the unpenalized estimator of the\nstructural parameters needs simultaneously multiple rates of convergence (i.e.\nthe so-called mixed-rates asymptotic behavior). We introduce a Bridge-type\nestimator by taking into account penalty functions involving $\\ell^q$ norms\n$(0<q\\leq 1)$. We prove that the proposed regularized estimator satisfies the\noracle properties.\n  Our approach is useful for the estimation of stochastic differential\nequations in the parametric sparse setting. More precisely, under the high\nfrequency observation scheme, we apply our methodology to an ergodic diffusion\nand introduce a procedure for the selection of the tuning parameters.\nFurthermore, the paper contains a simulation study as well as a real data\nprediction in order to assess about the performance of the proposed Bridge\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:37:31 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 19:28:23 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["De Gregorio", "Alessandro", ""], ["Iafrate", "Francesco", ""]]}, {"id": "2010.08444", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno and Claudio Agostinelli and Luca Greco", "title": "Robust Estimation for Multivariate Wrapped Models", "comments": "18 pages, 4 figures. METRON (2021)", "journal-ref": "Saraceno, G., Agostinelli, C. & Greco, L. Robust estimation for\n  multivariate wrapped models. METRON (2021)", "doi": "10.1007/s40300-021-00214-9", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted likelihood technique for robust estimation of a multivariate\nWrapped Normal distribution for data points scattered on a p-dimensional torus\nis proposed. The occurrence of outliers in the sample at hand can badly\ncompromise inference for standard techniques such as maximum likelihood method.\nTherefore, there is the need to handle such model inadequacies in the fitting\nprocess by a robust technique and an effective down-weighting of observations\nnot following the assumed model. Furthermore, the employ of a robust method\ncould help in situations of hidden and unexpected substructures in the data.\nHere, it is suggested to build a set of data-dependent weights based on the\nPearson residuals and solve the corresponding weighted likelihood estimating\nequations. In particular, robust estimation is carried out by using a\nClassification EM algorithm whose M-step is enhanced by the computation of\nweights based on current parameters' values. The finite sample behavior of the\nproposed method has been investigated by a Monte Carlo numerical studies and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:15:48 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Agostinelli", "Claudio", ""], ["Greco", "Luca", ""]]}, {"id": "2010.08463", "submitter": "Andrii Babii", "authors": "Andrii Babii and Xi Chen and Eric Ghysels and Rohit Kumar", "title": "Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory\n  and an Application to Racial Justice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of asymmetries in prediction problems arising in economics has\nbeen recognized for a long time. In this paper, we focus on binary choice\nproblems in a data-rich environment with general loss functions. In contrast to\nthe asymmetric regression problems, the binary choice with general loss\nfunctions and high-dimensional datasets is challenging and not well understood.\nEconometricians have studied binary choice problems for a long time, but the\nliterature does not offer computationally attractive solutions in data-rich\nenvironments. In contrast, the machine learning literature has many\ncomputationally attractive algorithms that form the basis for much of the\nautomated procedures that are implemented in practice, but it is focused on\nsymmetric loss functions that are independent of individual characteristics.\nOne of the main contributions of our paper is to show that the theoretically\nvalid predictions of binary outcomes with arbitrary loss functions can be\nachieved via a very simple reweighting of the logistic regression, or other\nstate-of-the-art machine learning techniques, such as boosting or (deep) neural\nnetworks. We apply our analysis to racial justice in pretrial detention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:01:20 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:14:42 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 18:32:22 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Babii", "Andrii", ""], ["Chen", "Xi", ""], ["Ghysels", "Eric", ""], ["Kumar", "Rohit", ""]]}, {"id": "2010.08479", "submitter": "Phil Long", "authors": "Peter L. Bartlett and Philip M. Long", "title": "Failures of model-dependent generalization bounds for least-norm\n  interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider bounds on the generalization performance of the least-norm linear\nregressor, in the over-parameterized regime where it can interpolate the data.\nWe describe a sense in which any generalization bound of a type that is\ncommonly proved in statistical learning theory must sometimes be very loose\nwhen applied to analyze the least-norm interpolant. In particular, for a\nvariety of natural joint distributions on training examples, any valid\ngeneralization bound that depends only on the output of the learning algorithm,\nthe number of training examples, and the confidence parameter, and that\nsatisfies a mild condition (substantially weaker than monotonicity in sample\nsize), must sometimes be very loose -- it can be bounded below by a constant\nwhen the true excess risk goes to zero.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:30:05 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 23:12:30 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 17:05:24 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bartlett", "Peter L.", ""], ["Long", "Philip M.", ""]]}, {"id": "2010.08611", "submitter": "F. Richard Guo", "authors": "F. Richard Guo, Emilija Perkovi\\'c", "title": "Minimal enumeration of all possible total effects in a Markov\n  equivalence class", "comments": "Corrected Figure 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, when a total causal effect of interest is not\nidentified, the set of all possible effects can be reported instead. This\ntypically occurs when the underlying causal DAG is only known up to a Markov\nequivalence class, or a refinement thereof due to background knowledge. As\nsuch, the class of possible causal DAGs is represented by a maximally oriented\npartially directed acyclic graph (MPDAG), which contains both directed and\nundirected edges. We characterize the minimal additional edge orientations\nrequired to identify a given total effect. A recursive algorithm is then\ndeveloped to enumerate subclasses of DAGs, such that the total effect in each\nsubclass is identified as a distinct functional of the observed distribution.\nThis resolves an issue with existing methods, which often report possible total\neffects with duplicates, namely those that are numerically distinct due to\nsampling variability but are in fact causally identical.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:07:20 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 05:49:07 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 00:45:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Guo", "F. Richard", ""], ["Perkovi\u0107", "Emilija", ""]]}, {"id": "2010.08838", "submitter": "Harold Chiang", "authors": "Harold D. Chiang and Bing Yang Tan", "title": "Empirical likelihood and uniform convergence rates for dyadic kernel\n  density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the asymptotic properties of and improved inference\nmethods for kernel density estimation (KDE) for dyadic data. We first establish\nnovel uniform convergence rates for dyadic KDE under general assumptions. As\nthe existing analytic variance estimator is known to behave unreliably in\nfinite samples, we propose a modified jackknife empirical likelihood procedure\nfor inference. The proposed test statistic is self-normalised and no variance\nestimator is required. In addition, it is asymptotically pivotal regardless of\npresence of dyadic clustering. The results are extended to cover the\npractically relevant case of incomplete dyadic network data. Simulations show\nthat this jackknife empirical likelihood-based inference procedure delivers\nprecise coverage probabilities even under modest sample sizes and with\nincomplete dyadic data. Finally, we illustrate the method by studying airport\ncongestion.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 18:10:23 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 00:56:05 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Chiang", "Harold D.", ""], ["Tan", "Bing Yang", ""]]}, {"id": "2010.08870", "submitter": "Xiaotian Xie", "authors": "Xiaotian Xie, Dimitrios Katselis, Carolyn L. Beck and R. Srikant", "title": "On the Consistency of Maximum Likelihood Estimators for Causal Network\n  Identification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying parameters of a particular class of\nMarkov chains, called Bernoulli Autoregressive (BAR) processes. The structure\nof any BAR model is encoded by a directed graph. Incoming edges to a node in\nthe graph indicate that the state of the node at a particular time instant is\ninfluenced by the states of the corresponding parental nodes in the previous\ntime instant. The associated edge weights determine the corresponding level of\ninfluence from each parental node. In the simplest setup, the Bernoulli\nparameter of a particular node's state variable is a convex combination of the\nparental node states in the previous time instant and an additional Bernoulli\nnoise random variable. This paper focuses on the problem of edge weight\nidentification using Maximum Likelihood (ML) estimation and proves that the ML\nestimator is strongly consistent for two variants of the BAR model. We\nadditionally derive closed-form estimators for the aforementioned two variants\nand prove their strong consistency.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:25:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xie", "Xiaotian", ""], ["Katselis", "Dimitrios", ""], ["Beck", "Carolyn L.", ""], ["Srikant", "R.", ""]]}, {"id": "2010.08876", "submitter": "Xinran Li", "authors": "Xinran Li and Xiao-Li Meng", "title": "A Multi-resolution Theory for Approximating Infinite-$p$-Zero-$n$:\n  Transitional Inference, Individualized Predictions, and a World Without\n  Bias-Variance Trade-off", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transitional inference is an empiricism concept, rooted and practiced in\nclinical medicine since ancient Greece. Knowledge and experiences gained from\ntreating one entity are applied to treat a related but distinctively different\none. This notion of \"transition to the similar\" renders individualized\ntreatments an operational meaning, yet its theoretical foundation defies the\nfamiliar inductive inference framework. The uniqueness of entities is the\nresult of potentially an infinite number of attributes (hence $p=\\infty$),\nwhich entails zero direct training sample size (i.e., $n=0$) because genuine\nguinea pigs do not exist. However, the literature on wavelets and on sieve\nmethods suggests a principled approximation theory for transitional inference\nvia a multi-resolution (MR) perspective, where we use the resolution level to\nindex the degree of approximation to ultimate individuality. MR inference seeks\na primary resolution indexing an indirect training sample, which provides\nenough matched attributes to increase the relevance of the results to the\ntarget individuals and yet still accumulate sufficient indirect sample sizes\nfor robust estimation. Theoretically, MR inference relies on an infinite-term\nANOVA-type decomposition, providing an alternative way to model sparsity via\nthe decay rate of the resolution bias as a function of the primary resolution\nlevel. Unexpectedly, this decomposition reveals a world without variance when\nthe outcome is a deterministic function of potentially infinitely many\npredictors. In this deterministic world, the optimal resolution prefers\nover-fitting in the traditional sense when the resolution bias decays\nsufficiently rapidly. Furthermore, there can be many \"descents\" in the\nprediction error curve, when the contributions of predictors are inhomogeneous\nand the ordering of their importance does not align with the order of their\ninclusion in prediction.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:56:59 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 05:14:32 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 08:20:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Xinran", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "2010.09237", "submitter": "Nicolas Schreuder", "authors": "Nicolas Schreuder and Victor-Emmanuel Brunel and Arnak Dalalyan", "title": "Statistical guarantees for generative models without domination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a convenient framework for studying (adversarial)\ngenerative models from a statistical perspective. It consists in modeling the\ngenerative device as a smooth transformation of the unit hypercube of a\ndimension that is much smaller than that of the ambient space and measuring the\nquality of the generative model by means of an integral probability metric. In\nthe particular case of integral probability metric defined through a smoothness\nclass, we establish a risk bound quantifying the role of various parameters. In\nparticular, it clearly shows the impact of dimension reduction on the error of\nthe generative model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:09:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Schreuder", "Nicolas", ""], ["Brunel", "Victor-Emmanuel", ""], ["Dalalyan", "Arnak", ""]]}, {"id": "2010.09267", "submitter": "Touboul Adrien", "authors": "Julien Reygner (CERMICS, GdR MASCOT-NUM), Adrien Touboul (CERMICS, IRT\n  SystemX)", "title": "Reweighting samples under covariate shift using a Wasserstein distance\n  criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering two random variables with different laws to which we only have\naccess through finite size iid samples, we address how to reweight the first\nsample so that its empirical distribution converges towards the true law of the\nsecond sample as the size of both samples goes to infinity. We study an optimal\nreweighting that minimizes the Wasserstein distance between the empirical\nmeasures of the two samples, and leads to an expression of the weights in terms\nof Nearest Neighbors. The consistency and some asymptotic convergence rates in\nterms of expected Wasserstein distance are derived, and do not need the\nassumption of absolute continuity of one random variable with respect to the\nother. These results have some application in Uncertainty Quantification for\ndecoupled estimation and in the bound of the generalization error for the\nNearest Neighbor Regression under covariate shift.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:23:55 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Reygner", "Julien", "", "CERMICS, GdR MASCOT-NUM"], ["Touboul", "Adrien", "", "CERMICS, IRT\n  SystemX"]]}, {"id": "2010.09478", "submitter": "Rahul Singh", "authors": "Rahul Singh, Fang Liu, Yin Sun and Ness Shroff", "title": "Multi-Armed Bandits with Dependent Arms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the classical multi-armed bandit problem (MABP) which\nwe call as Multi-Armed Bandits with dependent arms. More specifically, multiple\narms are grouped together to form a cluster, and the reward distributions of\narms belonging to the same cluster are known functions of an unknown parameter\nthat is a characteristic of the cluster. Thus, pulling an arm $i$ not only\nreveals information about its own reward distribution, but also about all those\narms that share the same cluster with arm $i$. This \"correlation\" amongst the\narms complicates the exploration-exploitation trade-off that is encountered in\nthe MABP because the observation dependencies allow us to test simultaneously\nmultiple hypotheses regarding the optimality of an arm. We develop learning\nalgorithms based on the UCB principle which utilize these additional side\nobservations appropriately while performing exploration-exploitation trade-off.\nWe show that the regret of our algorithms grows as $O(K\\log T)$, where $K$ is\nthe number of clusters. In contrast, for an algorithm such as the vanilla UCB\nthat is optimal for the classical MABP and does not utilize these dependencies,\nthe regret scales as $O(M\\log T)$ where $M$ is the number of arms.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:00:19 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:06:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Singh", "Rahul", ""], ["Liu", "Fang", ""], ["Sun", "Yin", ""], ["Shroff", "Ness", ""]]}, {"id": "2010.09589", "submitter": "Mengkun Du", "authors": "Mengkun Du and Lan Wu", "title": "A factor-adjusted multiple testing of general alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor-adjusted multiple testing is used for handling strong correlated\ntests. Since most of previous works control the false discovery rate under\nsparse alternatives, we develop a two-step method, namely the AdaFAT, for any\ntrue false proportion. In this paper, the proposed procedure is adjusted by\nlatent factor loadings. Under the existence of explanatory variables, a uniform\nconvergence rate of the estimated factor loadings is given. We also show that\nthe power of AdaFAT goes to one along with the controlled false discovery rate.\nThe performance of the proposed procedure is examined through simulations\ncalibrated by China A-share market.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:14:04 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 13:44:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Du", "Mengkun", ""], ["Wu", "Lan", ""]]}, {"id": "2010.09597", "submitter": "Quanquan Gu", "authors": "Difan Zou and Pan Xu and Quanquan Gu", "title": "Faster Convergence of Stochastic Gradient Langevin Dynamics for\n  Non-Log-Concave Sampling", "comments": "44 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new convergence analysis of stochastic gradient Langevin\ndynamics (SGLD) for sampling from a class of distributions that can be\nnon-log-concave. At the core of our approach is a novel conductance analysis of\nSGLD using an auxiliary time-reversible Markov Chain. Under certain conditions\non the target distribution, we prove that $\\tilde O(d^4\\epsilon^{-2})$\nstochastic gradient evaluations suffice to guarantee $\\epsilon$-sampling error\nin terms of the total variation distance, where $d$ is the problem dimension.\nThis improves existing results on the convergence rate of SGLD (Raginsky et\nal., 2017; Xu et al., 2018). We further show that provided an additional\nHessian Lipschitz condition on the log-density function, SGLD is guaranteed to\nachieve $\\epsilon$-sampling error within $\\tilde O(d^{15/4}\\epsilon^{-3/2})$\nstochastic gradient evaluations. Our proof technique provides a new way to\nstudy the convergence of Langevin-based algorithms and sheds some light on the\ndesign of fast stochastic gradient-based sampling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:23:18 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 07:15:34 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zou", "Difan", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "2010.09656", "submitter": "Philip Etter", "authors": "Philip A. Etter, Lexing Ying", "title": "Operator Augmentation for Noisy Elliptic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the computational sciences, one must often estimate model parameters from\ndata subject to noise and uncertainty, leading to inaccurate results. In order\nto improve the accuracy of models with noisy parameters, we consider the\nproblem of reducing error in an elliptic linear system with the operator\ncorrupted by noise. We assume the noise preserves positive definiteness, but\notherwise, we make no additional assumptions the structure of the noise. Under\nthese assumptions, we propose the operator augmentation framework, a collection\nof easy-to-implement algorithms that augment a noisy inverse operator by\nsubtracting an additional auxiliary term. In a similar fashion to the\nJames-Stein estimator, this has the effect of drawing the noisy inverse\noperator closer to the ground truth, and hence reduces error. We develop\nbootstrap Monte Carlo algorithms to estimate the required augmentation\nmagnitude for optimal error reduction in the noisy system. To improve the\ntractability of these algorithms, we propose several approximate polynomial\nexpansions for the operator inverse, and prove desirable convergence and\nmonotonicity properties for these expansions. We also prove theorems that\nquantify the error reduction obtained by operator augmentation. In addition to\ntheoretical results, we provide a set of numerical experiments on four\ndifferent graph and grid Laplacian systems that all demonstrate effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:55:00 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:11:57 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 20:05:04 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 20:22:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Etter", "Philip A.", ""], ["Ying", "Lexing", ""]]}, {"id": "2010.09686", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith and Aaditya Ramdas", "title": "Estimating means of bounded random variables by betting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives confidence intervals (CI) and time-uniform confidence\nsequences (CS) for the classical problem of estimating an unknown mean from\nbounded observations. We present a general approach for deriving concentration\nbounds, that can be seen as a generalization (and improvement) of the\ncelebrated Chernoff method. At its heart, it is based on deriving a new class\nof composite nonnegative martingales, with strong connections to betting and\nthe method of mixtures. We show how to extend these ideas to sampling without\nreplacement, another heavily studied problem. In all cases, our bounds are\nadaptive to the unknown variance, and empirically vastly outperform competing\napproaches based on Hoeffding or empirical Bernstein inequalities and their\nrecent supermartingale generalizations. In short, we establish a new\nstate-of-the-art for four fundamental problems: CSs and CIs for bounded means,\nwith and without replacement.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:22:03 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 01:29:19 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 00:57:37 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:56:49 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2010.09830", "submitter": "Zexun Chen", "authors": "Zexun Chen, Jun Fan, Kuo Wang", "title": "Remarks on multivariate Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes occupy one of the leading places in modern statistics and\nprobability theory due to their importance and a wealth of strong results. The\ncommon use of Gaussian processes is in connection with problems related to\nestimation, detection, and many statistical or machine learning models. With\nthe fast development of Gaussian process applications, it is necessary to\nconsolidate the fundamentals of vector-valued stochastic processes, in\nparticular multivariate Gaussian processes, which is the essential theory for\nmany applied problems with multiple correlated responses. In this paper, we\npropose a precise definition of multivariate Gaussian processes based on\nGaussian measures on vector-valued function spaces, and provide an existence\nproof. In addition, several fundamental properties of multivariate Gaussian\nprocesses, such as strict stationarity and independence, are introduced. We\nfurther derive multivariate Brownian motion including It\\^o lemma as a special\ncase of a multivariate Gaussian process, and present a brief introduction to\nmultivariate Gaussian process regression as a useful statistical learning\nmethod for multi-output prediction problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:10:17 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 22:52:27 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 17:04:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Chen", "Zexun", ""], ["Fan", "Jun", ""], ["Wang", "Kuo", ""]]}, {"id": "2010.09906", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and He Jiang", "title": "On the Consistency of Metric and Non-Metric K-medoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the consistency of K-medoids in the context of metric spaces. We\nstart by proving that K-medoids is asymptotically equivalent to K-means\nrestricted to the support of the underlying distribution under general\nconditions, including a wide selection of loss functions. This asymptotic\nequivalence, in turn, enables us to apply the work of Parna (1986) on the\nconsistency of K-means. This general approach applies also to non-metric\nsettings where only an ordering of the dissimilarities is available. We\nconsider two types of ordinal information: one where all quadruple comparisons\nare available; and one where only triple comparisons are available. We provide\nsome numerical experiments to illustrate our theory.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:46:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Jiang", "He", ""]]}, {"id": "2010.09937", "submitter": "Marcin Pitera", "authors": "Marcin Pitera and Thorsten Schmidt", "title": "Unbiased estimation and backtesting of risk in the context of heavy\n  tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the estimation of risk is an important question in the daily business\nof banks and insurances, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk and expected shortfall frameworks. Motivated by this finding, we\npropose a new algorithm for bias correction and show how to apply it for\ngeneralized Pareto distributions. In particular, we consider value-at-risk and\nexpected shortfall plug-in estimators, and show that the application of our\nalgorithm leads to gain in efficiency when heavy tails exist in the data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:37:52 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 17:34:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pitera", "Marcin", ""], ["Schmidt", "Thorsten", ""]]}, {"id": "2010.10194", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Lorenz Haubner, Axel Munk, Peter B\\\"uhlmann", "title": "Optimistic search strategy: Change point detection for large-scale data\n  via adaptive logarithmic queries", "comments": "extended Table 1; added Model II and Lemma 5.3; added further minor\n  explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classical and ever reviving topic, change point detection is often\nformulated as a search for the maximum of a gain function describing improved\nfits when segmenting the data. Searching through all candidate split points on\nthe grid for finding the best one requires $O(T)$ evaluations of the gain\nfunction for an interval with $T$ observations. If each evaluation is\ncomputationally demanding (e.g. in high-dimensional models), this can become\ninfeasible. Instead, we propose optimistic search strategies with $O(\\log T)$\nevaluations exploiting specific structure of the gain function.\n  Towards solid understanding of our strategies, we investigate in detail the\nclassical univariate Gaussian change in mean setup. For some of our proposals\nwe prove asymptotic minimax optimality for single and multiple change point\nscenarios. Our search strategies generalize far beyond the theoretically\nanalyzed univariate setup. We illustrate, as an example, massive computational\nspeedup in change point detection for high-dimensional Gaussian graphical\nmodels. More generally, we demonstrate empirically that our optimistic search\nmethods lead to competitive estimation performance while heavily reducing\nrun-time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:09:52 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 23:50:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["Haubner", "Lorenz", ""], ["Munk", "Axel", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2010.10436", "submitter": "Lorenz Richter", "authors": "Lorenz Richter, Ayman Boustati, Nikolas N\\\"usken, Francisco J. R.\n  Ruiz, \\\"Omer Deniz Akyildiz", "title": "VarGrad: A Low-Variance Gradient Estimator for Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the properties of an unbiased gradient estimator of the ELBO for\nvariational inference, based on the score function method with leave-one-out\ncontrol variates. We show that this gradient estimator can be obtained using a\nnew loss, defined as the variance of the log-ratio between the exact posterior\nand the variational approximation, which we call the $\\textit{log-variance\nloss}$. Under certain conditions, the gradient of the log-variance loss equals\nthe gradient of the (negative) ELBO. We show theoretically that this gradient\nestimator, which we call $\\textit{VarGrad}$ due to its connection to the\nlog-variance loss, exhibits lower variance than the score function method in\ncertain settings, and that the leave-one-out control variate coefficients are\nclose to the optimal ones. We empirically demonstrate that VarGrad offers a\nfavourable variance versus computation trade-off compared to other\nstate-of-the-art estimators on a discrete VAE.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:46:01 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 10:27:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Richter", "Lorenz", ""], ["Boustati", "Ayman", ""], ["N\u00fcsken", "Nikolas", ""], ["Ruiz", "Francisco J. R.", ""], ["Akyildiz", "\u00d6mer Deniz", ""]]}, {"id": "2010.10598", "submitter": "Paul Doukhan M.", "authors": "Victor De la Pena and Paul Doukhan and Yahia Salhi", "title": "A Dynamic Taylor's Law", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taylor's power law (or fluctuation scaling) states that on comparable\npopulations, the variance of each sample is approximately proportional to a\npower of the mean of the population. It has been shown to hold by empirical\nobservations in a broad class of disciplines including demography, biology,\neconomics, physics and mathematics.\n  In particular, it has been observed in the problems involving population\ndynamics, market trading, thermodynamics and number theory.\n  For this many authors consider panel data in order to obtain laws of large\nnumbers and the possibility to fit those expressions; essentially we aim at\nconsidering ergodic behaviors without independence. Thus we restrict the study\nto stationary time series and we develop different Taylor exponents in this\nsetting.\n  From a theoretic point of view, there has been a growing interest on the\nstudy of the behavior of such a phenomenon. Most of these works focused on the\nso-called static Taylor related to independent samples. In this paper, we\nintroduce a dynamic Taylor's law for dependent samples using self-normalised\nexpressions involving Bernstein blocks. A central limit theorem (CLT) is proved\nunder either weak dependence or strong mixing assumptions for the marginal\nprocess. The limit behavior of such a new index involves the series of\ncovariances unlike the classic framework where the limit behavior involves the\nmarginal variance. We also provide an asymptotic result for for a\ngoodness-of-fit testing suited to check whether the corresponding dynamical\nTaylor's law holds in empirical studies. Moreover, we also obtain a consistent\nestimation of the Taylor's exponent.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:16:44 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["De la Pena", "Victor", ""], ["Doukhan", "Paul", ""], ["Salhi", "Yahia", ""]]}, {"id": "2010.10918", "submitter": "Vitalii Makogin", "authors": "Nikolai Leonenko, Vitalii Makogin, Mehmet Siddik Cadirci", "title": "The entropy based goodness of fit tests for generalized von Mises-Fisher\n  distributions and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some new classes of unimodal rotational invariant directional\ndistributions, which generalize von Mises-Fisher distribution. We propose three\ntypes of distributions, one of which represents axial data. For each new type\nwe provide formulae and short computational study of parameter estimators by\nthe method of moments and the method of maximum likelihood. The main goal of\nthe paper is to develop the goodness of fit test to detect that sample entries\nfollow one of the introduced generalized von Mises--Fisher distribution based\non the maximum entropy principle. We use $k$th nearest neighbour distances\nestimator of Shannon entropy and prove its $L^2$-consistency. We examine the\nbehaviour of the test statistics, find critical values and compute power of the\ntest on simulated samples. We apply the goodness of fit test to local fiber\ndirections in a glass fibre reinforced composite material and detect the\nsamples which follow axial generalized von Mises--Fisher distribution.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:58:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Leonenko", "Nikolai", ""], ["Makogin", "Vitalii", ""], ["Cadirci", "Mehmet Siddik", ""]]}, {"id": "2010.11213", "submitter": "Mahdi Soltanolkotabi", "authors": "Adel Javanmard and Mahdi Soltanolkotabi", "title": "Precise Statistical Analysis of Classification Accuracies for\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the wide empirical success of modern machine learning algorithms and\nmodels in a multitude of applications, they are known to be highly susceptible\nto seemingly small indiscernible perturbations to the input data known as\nadversarial attacks. A variety of recent adversarial training procedures have\nbeen proposed to remedy this issue. Despite the success of such procedures at\nincreasing accuracy on adversarially perturbed inputs or robust accuracy, these\ntechniques often reduce accuracy on natural unperturbed inputs or standard\naccuracy. Complicating matters further the effect and trend of adversarial\ntraining procedures on standard and robust accuracy is rather counter intuitive\nand radically dependent on a variety of factors including the perceived form of\nthe perturbation during training, size/quality of data, model\noverparameterization, etc. In this paper we focus on binary classification\nproblems where the data is generated according to the mixture of two Gaussians\nwith general anisotropic covariance matrices and derive a precise\ncharacterization of the standard and robust accuracy for a class of minimax\nadversarially trained models. We consider a general norm-based adversarial\nmodel, where the adversary can add perturbations of bounded $\\ell_p$ norm to\neach input data, for an arbitrary $p\\ge 1$. Our comprehensive analysis allows\nus to theoretically explain several intriguing empirical phenomena and provide\na precise understanding of the role of different problem parameters on standard\nand robust accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 18:00:53 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Javanmard", "Adel", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2010.11285", "submitter": "Liyan Xie", "authors": "Liyan Xie and Yao Xie", "title": "Sequential Change Detection by Optimal Weighted $\\ell_2$ Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new non-parametric statistic, called the weighed $\\ell_2$\ndivergence, based on empirical distributions for sequential change detection.\nWe start by constructing the weighed $\\ell_2$ divergence as a fundamental\nbuilding block for two-sample tests and change detection. The proposed\nstatistic is proved to attain the optimal sample complexity in the offline\nsetting. We then study the sequential change detection using the weighed\n$\\ell_2$ divergence and characterize the fundamental performance metrics,\nincluding the average run length (ARL) and the expected detection delay (EDD).\nWe also present practical algorithms to find the optimal projection to handle\nhigh-dimensional data and the optimal weights, which is critical to quick\ndetection since, in such settings, there are not many post-change samples.\nSimulation results and real data examples are provided to validate the good\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 19:51:23 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 04:29:19 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Xie", "Liyan", ""], ["Xie", "Yao", ""]]}, {"id": "2010.11406", "submitter": "Jon Lee", "authors": "Luze Xu, Marcia Fampa, Jon Lee", "title": "1-norm minimization and minimum-rank structured sparsity for symmetric\n  and ah-symmetric generalized inverses: rank one and two", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.05744", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized inverses are important in statistics and other areas of applied\nmatrix algebra. A \\emph{generalized inverse} of a real matrix $A$ is a matrix\n$H$ that satisfies the Moore-Penrose (M-P) property $AHA=A$. If $H$ also\nsatisfies the M-P property $HAH=H$, then it is called \\emph{reflexive}.\nReflexivity of a generalized inverse is equivalent to minimum rank, a highly\ndesirable property. We consider aspects of symmetry related to the calculation\nof various \\emph{sparse} reflexive generalized inverses of $A$. As is common,\nwe use (vector) 1-norm minimization for both inducing sparsity and for keeping\nthe magnitude of entries under control.\n  When $A$ is symmetric, a symmetric $H$ is highly desirable, but generally\nsuch a restriction on $H$ will not lead to a 1-norm minimizing reflexive\ngeneralized inverse. We investigate a block construction method to produce a\nsymmetric reflexive generalized inverse that is structured and has guaranteed\nsparsity. Letting the rank of $A$ be $r$, we establish that the 1-norm\nminimizing generalized inverse of this type is a 1-norm minimizing symmetric\ngeneralized inverse when (i) $r=1$ and when (ii) $r=2$ and $A$ is nonnegative.\n  Another aspect of symmetry that we consider relates to another M-P property:\n$H$ is \\emph{ah-symmetric} if $AH$ is symmetric. The ah-symmetry property is\nsufficient for a generalized inverse to be used to solve the least-squares\nproblem $\\min\\{\\|Ax-b\\|_2:~x\\in\\mathbb{R}^n\\}$ using $H$, via $x:=Hb$. We\ninvestigate a column block construction method to produce an ah-symmetric\nreflexive generalized inverse that is structured and has guaranteed sparsity.\nWe establish that the 1-norm minimizing ah-symmetric generalized inverse of\nthis type is a 1-norm minimizing ah-symmetric generalized inverse when (i)\n$r=1$ and when (ii) $r=2$ and $A$ satisfies a technical condition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:43:06 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Xu", "Luze", ""], ["Fampa", "Marcia", ""], ["Lee", "Jon", ""]]}, {"id": "2010.11417", "submitter": "Daisuke Nagakura", "authors": "Daisuke Nagakura", "title": "Positive definiteness of the asymptotic covariance matrix of OLS\n  estimators in parsimonious regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Ghysels, Hill, and Motegi (2020) proposed a test for examining\nwhether a large number of coefficients in linear regression models is zero. The\ntest is called the max test. The test statistic is calculated by first running\nmultiple ordinary least squares (OLS) regressions, each including only one of\nkey regressors, whose coefficients are supposed to be zero under the null, and\nthen taking the maximum value of the squared OLS coefficient estimates of those\nkey regressors. They called these regressions parsimonious regressions. This\npaper answers a question raised in their Remark 2.4; whether the asymptotic\ncovariance matrix of the OLS estimators in the parsimonious regressions is\ngenerally positive definite. The paper shows that it is generally positive\ndefinite, and the result may be utilized to facilitate the calculation of the\nsimulated p value necessary for implementing the max test.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:48:20 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 02:22:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nagakura", "Daisuke", ""]]}, {"id": "2010.11470", "submitter": "Nicolas Verzelen", "authors": "Nicolas Verzelen, Magalie Fromont, Matthieu Lerasle, and Patricia\n  Reynaud-Bouret", "title": "Optimal Change-Point Detection and Localization", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a times series ${\\bf Y}$ in $\\mathbb{R}^n$, with a piece-wise contant\nmean and independent components, the twin problems of change-point detection\nand change-point localization respectively amount to detecting the existence of\ntimes where the mean varies and estimating the positions of those\nchange-points. In this work, we tightly characterize optimal rates for both\nproblems and uncover the phase transition phenomenon from a global testing\nproblem to a local estimation problem. Introducing a suitable definition of the\nenergy of a change-point, we first establish in the single change-point setting\nthat the optimal detection threshold is $\\sqrt{2\\log\\log(n)}$. When the energy\nis just above the detection threshold, then the problem of localizing the\nchange-point becomes purely parametric: it only depends on the difference in\nmeans and not on the position of the change-point anymore. Interestingly, for\nmost change-point positions, it is possible to detect and localize them at a\nmuch smaller energy level. In the multiple change-point setting, we establish\nthe energy detection threshold and show similarly that the optimal localization\nerror of a specific change-point becomes purely parametric. Along the way,\ntight optimal rates for Hausdorff and $l_1$ estimation losses of the vector of\nall change-points positions are also established. Two procedures achieving\nthese optimal rates are introduced. The first one is a least-squares estimator\nwith a new multiscale penalty that favours well spread change-points. The\nsecond one is a two-step multiscale post-processing procedure whose\ncomputational complexity can be as low as $O(n\\log(n))$. Notably, these two\nprocedures accommodate with the presence of possibly many low-energy and\ntherefore undetectable change-points and are still able to detect and localize\nhigh-energy change-points even with the presence of those nuisance parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:26:01 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 23:57:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Verzelen", "Nicolas", ""], ["Fromont", "Magalie", ""], ["Lerasle", "Matthieu", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "2010.11518", "submitter": "Cl\\'ement Chadebec", "authors": "Cl\\'ement Chadebec (CRC, Universit\\'e de Paris), Cl\\'ement Mantoux\n  (ARAMIS) and St\\'ephanie Allassonni\\`ere (CRC, Universit\\'e de Paris)", "title": "Geometry-Aware Hamiltonian Variational Auto-Encoder", "comments": "44 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoders (VAEs) have proven to be a well suited tool for\nperforming dimensionality reduction by extracting latent variables lying in a\npotentially much smaller dimensional space than the data. Their ability to\ncapture meaningful information from the data can be easily apprehended when\nconsidering their capability to generate new realistic samples or perform\npotentially meaningful interpolations in a much smaller space. However, such\ngenerative models may perform poorly when trained on small data sets which are\nabundant in many real-life fields such as medicine. This may, among others,\ncome from the lack of structure of the latent space, the geometry of which is\noften under-considered. We thus propose in this paper to see the latent space\nas a Riemannian manifold endowed with a parametrized metric learned at the same\ntime as the encoder and decoder networks. This metric is then used in what we\ncalled the Riemannian Hamiltonian VAE which extends the Hamiltonian VAE\nintroduced by arXiv:1805.11328 to better exploit the underlying geometry of the\nlatent space. We argue that such latent space modelling provides useful\ninformation about its underlying structure leading to far more meaningful\ninterpolations, more realistic data-generation and more reliable clustering.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:26:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chadebec", "Cl\u00e9ment", "", "CRC, Universit\u00e9 de Paris"], ["Mantoux", "Cl\u00e9ment", "", "ARAMIS"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC, Universit\u00e9 de Paris"]]}, {"id": "2010.11537", "submitter": "Nikita Zhivotovskiy", "authors": "Luc Devroye, Silvio Lattanzi, Gabor Lugosi, Nikita Zhivotovskiy", "title": "On Mean Estimation for Heteroscedastic Random Variables", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the common mean $\\mu$ of $n$ independent\nsymmetric random variables with different and unknown standard deviations\n$\\sigma_1 \\le \\sigma_2 \\le \\cdots \\le\\sigma_n$. We show that, under some mild\nregularity assumptions on the distribution, there is a fully adaptive estimator\n$\\widehat{\\mu}$ such that it is invariant to permutations of the elements of\nthe sample and satisfies that, up to logarithmic factors, with high\nprobability, \\[ |\\widehat{\\mu} - \\mu| \\lesssim \\min\\left\\{\\sigma_{m^*},\n\\frac{\\sqrt{n}}{\\sum_{i = \\sqrt{n}}^n \\sigma_i^{-1}} \\right\\}~, \\] where the\nindex $m^* \\lesssim \\sqrt{n}$ satisfies $m^* \\approx \\sqrt{\\sigma_{m^*}\\sum_{i\n= m^*}^n\\sigma_i^{-1}}$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:56:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Devroye", "Luc", ""], ["Lattanzi", "Silvio", ""], ["Lugosi", "Gabor", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2010.11665", "submitter": "Kolyan Ray", "authors": "Kolyan Ray, Botond Szabo, Gabriel Clara", "title": "Spike and slab variational Bayes for high dimensional logistic\n  regression", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a popular scalable alternative to Markov chain\nMonte Carlo for Bayesian inference. We study a mean-field spike and slab VB\napproximation of widely used Bayesian model selection priors in sparse\nhigh-dimensional logistic regression. We provide non-asymptotic theoretical\nguarantees for the VB posterior in both $\\ell_2$ and prediction loss for a\nsparse truth, giving optimal (minimax) convergence rates. Since the VB\nalgorithm does not depend on the unknown truth to achieve optimality, our\nresults shed light on effective prior choices. We confirm the improved\nperformance of our VB algorithm over common sparse VB approaches in a numerical\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:49:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""], ["Clara", "Gabriel", ""]]}, {"id": "2010.11914", "submitter": "Tobias Boege", "authors": "Tobias Boege", "title": "Gaussoids are two-antecedental approximations of Gaussian conditional\n  independence structures", "comments": "18 pages; v2: fixed typo in Prop. 3.3, generalized Lemma 4.1, added\n  Remark 4.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gaussoid axioms are conditional independence inference rules which\ncharacterize regular Gaussian CI structures over a three-element ground set. It\nis known that no finite set of inference rules completely describes regular\nGaussian CI as the ground set grows. In this article we show that the gaussoid\naxioms logically imply every inference rule of at most two antecedents which is\nvalid for regular Gaussians over any ground set. The proof is accomplished by\nexhibiting for each inclusion-minimal gaussoid extension of at most two CI\nstatements a regular Gaussian realization. Moreover we prove that all those\ngaussoids have rational positive-definite realizations inside every\n$\\varepsilon$-ball around the identity matrix. For the proof we introduce the\nconcept of algebraic Gaussians over arbitrary fields and of positive Gaussians\nover ordered fields and obtain the same two-antecedental completeness of the\ngaussoid axioms for algebraic and positive Gaussians over all fields of\ncharacteristic zero as a byproduct.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:47:48 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:04:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Boege", "Tobias", ""]]}, {"id": "2010.11921", "submitter": "Gabor Lugosi", "authors": "Gabor Lugosi and Shahar Mendelson", "title": "Multivariate mean estimation with direction-dependent accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean of a random vector based on\n$N$ independent, identically distributed observations. We prove the existence\nof an estimator that has a near-optimal error in all directions in which the\nvariance of the one dimensional marginal of the random vector is not too small:\nwith probability $1-\\delta$, the procedure returns $\\wh{\\mu}_N$ which satisfies\nthat for every direction $u \\in S^{d-1}$, \\[ \\inr{\\wh{\\mu}_N - \\mu, u}\\le\n\\frac{C}{\\sqrt{N}} \\left( \\sigma(u)\\sqrt{\\log(1/\\delta)} + \\left(\\E\\|X-\\EXP\nX\\|_2^2\\right)^{1/2} \\right)~, \\] where $\\sigma^2(u) = \\var(\\inr{X,u})$ and $C$\nis a constant. To achieve this, we require only slightly more than the\nexistence of the covariance matrix, in the form of a certain moment-equivalence\nassumption.\n  The proof relies on novel bounds for the ratio of empirical and true\nprobabilities that hold uniformly over certain classes of random variables.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:52:45 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lugosi", "Gabor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "2010.12000", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis, Themis Gouleakis, Christos Tzamos, Manolis\n  Zampetakis", "title": "Computationally and Statistically Efficient Truncated Regression", "comments": "Accepted for presentation at the Conference on Learning Theory (COLT)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a computationally and statistically efficient estimator for the\nclassical problem of truncated linear regression, where the dependent variable\n$y = w^T x + \\epsilon$ and its corresponding vector of covariates $x \\in R^k$\nare only revealed if the dependent variable falls in some subset $S \\subseteq\nR$; otherwise the existence of the pair $(x, y)$ is hidden. This problem has\nremained a challenge since the early works of [Tobin 1958, Amemiya 1973,\nHausman and Wise 1977], its applications are abundant, and its history dates\nback even further to the work of Galton, Pearson, Lee, and Fisher. While\nconsistent estimators of the regression coefficients have been identified, the\nerror rates are not well-understood, especially in high dimensions.\n  Under a thickness assumption about the covariance matrix of the covariates in\nthe revealed sample, we provide a computationally efficient estimator for the\ncoefficient vector $w$ from $n$ revealed samples that attains $l_2$ error\n$\\tilde{O}(\\sqrt{k/n})$. Our estimator uses Projected Stochastic Gradient\nDescent (PSGD) without replacement on the negative log-likelihood of the\ntruncated sample. For the statistically efficient estimation we only need\noracle access to the set $S$.In order to achieve computational efficiency we\nneed to assume that $S$ is a union of a finite number of intervals but still\ncan be complicated. PSGD without replacement must be restricted to an\nappropriately defined convex cone to guarantee that the negative log-likelihood\nis strongly convex, which in turn is established using concentration of\nmatrices on variables with sub-exponential tails. We perform experiments on\nsimulated data to illustrate the accuracy of our estimator.\n  As a corollary, we show that SGD learns the parameters of single-layer neural\nnetworks with noisy activation functions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:31:30 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Gouleakis", "Themis", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "2010.12094", "submitter": "Michael Fauss", "authors": "Michael Fau{\\ss} and H. Vincent Poor", "title": "Fading Boundaries: On a Nonparametric Variant of the Kiefer--Weiss\n  Problem", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric variant of the Kiefer--Weiss problem is proposed and\ninvestigated. In analogy to the classical Kiefer--Weiss problem, the objective\nis to minimize the maximum expected sample size of a sequential test. However,\ninstead of taking the maximum over a parametric family of distributions, it is\ntaken over all distributions defined on the given sample space. Two optimality\nconditions are stated, one necessary and one sufficient. The latter is based on\nexisting results on a more general minimax problem in sequential detection.\nThese results are specialized and made explicit in this paper. It is shown that\nthe nonparametric Kiefer--Weiss test is distinctly different from its\nparametric counterpart and admits non-standard, arguably counterintuitive\nproperties. In particular, it can be nontruncated and critically depends on its\nstopping rules being randomized. These properties are illustrated numerically\nusing the example of coin flipping, that is, testing the success probability of\na Bernoulli random variable.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:39:15 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Fau\u00df", "Michael", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2010.12101", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Julien Clancy, Thibaut Le Gouic, Philippe Rigollet,\n  George Stepaniants, Austin J. Stromme", "title": "Fast and Smooth Interpolation on Wasserstein Space", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for smoothly interpolating probability measures using\nthe geometry of optimal transport. To that end, we reduce this problem to the\nclassical Euclidean setting, allowing us to directly leverage the extensive\ntoolbox of spline interpolation. Unlike previous approaches to measure-valued\nsplines, our interpolated curves (i) have a clear interpretation as governing\nparticle flows, which is natural for applications, and (ii) come with the first\napproximation guarantees on Wasserstein space. Finally, we demonstrate the\nbroad applicability of our interpolation methodology by fitting surfaces of\nmeasures using thin-plate splines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:55:48 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chewi", "Sinho", ""], ["Clancy", "Julien", ""], ["Gouic", "Thibaut Le", ""], ["Rigollet", "Philippe", ""], ["Stepaniants", "George", ""], ["Stromme", "Austin J.", ""]]}, {"id": "2010.12222", "submitter": "Gabriel Frisch", "authors": "Gabriel Frisch (Heudiasyc), Jean-Benoist L\\'eger (Heudiasyc), Yves\n  Grandvalet (Heudiasyc)", "title": "Learning from missing data with the Latent Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data can be informative. Ignoring this information can lead to\nmisleading conclusions when the data model does not allow information to be\nextracted from the missing data. We propose a co-clustering model, based on the\nLatent Block Model, that aims to take advantage of this nonignorable\nnonresponses, also known as Missing Not At Random data (MNAR). A variational\nexpectation-maximization algorithm is derived to perform inference and a model\nselection criterion is presented. We assess the proposed approach on a\nsimulation study, before using our model on the voting records from the lower\nhouse of the French Parliament, where our analysis brings out relevant groups\nof MPs and texts, together with a sensible interpretation of the behavior of\nnon-voters.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:11:43 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Frisch", "Gabriel", "", "Heudiasyc"], ["L\u00e9ger", "Jean-Benoist", "", "Heudiasyc"], ["Grandvalet", "Yves", "", "Heudiasyc"]]}, {"id": "2010.12236", "submitter": "Solenne Gaucher", "authors": "Solenne Gaucher (LMO)", "title": "Finite Continuum-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a situation where an agent has $T$ ressources to be allocated to\na larger number $N$ of actions. Each action can be completed at most once and\nresults in a stochastic reward with unknown mean. The goal of the agent is to\nmaximize her cumulative reward. Non trivial strategies are possible when side\ninformation on the actions is available, for example in the form of covariates.\nFocusing on a nonparametric setting, where the mean reward is an unknown\nfunction of a one-dimensional covariate, we propose an optimal strategy for\nthis problem. Under natural assumptions on the reward function, we prove that\nthe optimal regret scales as $O(T^{1/3})$ up to poly-logarithmic factors when\nthe budget $T$ is proportional to the number of actions $N$. When $T$ becomes\nsmall compared to $N$, a smooth transition occurs. When the ratio $T/N$\ndecreases from a constant to $N^{-1/3}$, the regret increases progressively up\nto the $O(T^{1/2})$ rate encountered in continuum-armed bandits.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 08:48:45 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 14:37:47 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gaucher", "Solenne", "", "LMO"]]}, {"id": "2010.12286", "submitter": "Masahiro Kobayashi", "authors": "Masahiro Kobayashi, Kazuho Watanabe", "title": "Unbiased Estimation Equation under $f$-Separable Bregman Distortion\n  Measures", "comments": null, "journal-ref": "2020 IEEE Information Theory Workshop (ITW), 2021, pp. 1-5", "doi": "10.1109/ITW46852.2021.9457678", "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss unbiased estimation equations in a class of objective function\nusing a monotonically increasing function $f$ and Bregman divergence. The\nchoice of the function $f$ gives desirable properties such as robustness\nagainst outliers. In order to obtain unbiased estimation equations,\nanalytically intractable integrals are generally required as bias correction\nterms. In this study, we clarify the combination of Bregman divergence,\nstatistical model, and function $f$ in which the bias correction term vanishes.\nFocusing on Mahalanobis and Itakura-Saito distances, we provide a\ngeneralization of fundamental existing results and characterize a class of\ndistributions of positive reals with a scale parameter, which includes the\ngamma distribution as a special case. We discuss the possibility of latent bias\nminimization when the proportion of outliers is large, which is induced by the\nextinction of the bias correction term.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:33:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kobayashi", "Masahiro", ""], ["Watanabe", "Kazuho", ""]]}, {"id": "2010.12299", "submitter": "Thibault Randrianarisoa", "authors": "Thibault Randrianarisoa", "title": "Smoothing and adaptation of shifted P\\'olya Tree ensembles", "comments": "28 pages for the main article; 26 pages for the supplementary\n  material; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, S. Arlot and R. Genuer have shown that a model of random forests\noutperforms its single-tree counterpart in the estimation of $\\alpha-$H\\\"older\nfunctions, $\\alpha\\leq2$. This backs up the idea that ensembles of tree\nestimators are smoother estimators than single trees. On the other hand, most\npositive optimality results on Bayesian tree-based methods assume that\n$\\alpha\\leq1$. Naturally, one wonders whether Bayesian counterparts of forest\nestimators are optimal on smoother classes, just like it has been observed for\nfrequentist estimators for $\\alpha\\leq 2$. We dwell on the problem of density\nestimation and introduce an ensemble estimator from the classical (truncated)\nP\\'olya tree construction in Bayesian nonparametrics. The resulting Bayesian\nforest estimator is shown to lead to optimal posterior contraction rates, up to\nlogarithmic terms, for the Hellinger and $L^1$ distances on probability density\nfunctions on $[0;1)$ for arbitrary H\\\"older regularity $\\alpha>0$. This\nimproves upon previous results for constructions related to the P\\'olya tree\nprior whose optimality was only proven in the case $\\alpha\\leq 1$. Also, we\nintroduce an adaptive version of this new prior in the sense that it does not\nrequire the knowledge of $\\alpha$ to be defined and attain optimality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:06:33 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Randrianarisoa", "Thibault", ""]]}, {"id": "2010.12300", "submitter": "Neil Walton", "authors": "Neil Walton, Yuqing Zhang", "title": "Perturbed Pricing", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple randomized rule for the optimization of prices in revenue\nmanagement with contextual information. It is known that the certainty\nequivalent pricing rule, albeit popular, is sub-optimal. We show that, by\nallowing a small amount of randomization around these certainty equivalent\nprices, the benefits of optimal pricing and low regret are achievable.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:12:32 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Walton", "Neil", ""], ["Zhang", "Yuqing", ""]]}, {"id": "2010.12372", "submitter": "Vladimir Fomichov", "authors": "V. Fomichov, J. Ivanovs", "title": "Detection of groups of concomitant extremes using clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing empirical evidence that the spherical $k$-means clustering\nperforms remarkably well in identification of groups of concomitant extremes in\nhigh dimensions, thereby leading to sparse models. We provide first theoretical\nresults supporting this approach, but also identify some pitfalls. Furthermore,\nwe develop a novel spherical $k$-principal-components clustering algorithm\nwhich is more appropriate for identification of concomitant extremes. Our main\nresult establishes a broadly satisfied sufficient condition guaranteeing the\nsuccess of this method, albeit in a rather basic setting. Finally, we\nillustrate in simulations that $k$-principal-components outperforms $k$-means\nin the difficult case of weak asymptotic dependence within the groups.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:01:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fomichov", "V.", ""], ["Ivanovs", "J.", ""]]}, {"id": "2010.12449", "submitter": "Stefanie Schwaar", "authors": "Stefanie Schwaar", "title": "A Data-driven Change-point Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The q-weighted CUSUM and their corresponding estimator are well known\nstatistics for change-point detection and estimation. They have the difficulty\nthat the performance is highly dependent on the location of the change. An\nadaptive estimator with data-driven weights is presented to overcome this\nproblem, and it is shown that the corresponding adaptive change-point tests are\nvalid.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:43:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Schwaar", "Stefanie", ""]]}, {"id": "2010.12514", "submitter": "James Johndrow", "authors": "James E. Johndrow, Natesh S. Pillai, Aaron Smith", "title": "No Free Lunch for Approximate MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that the performance of Markov chain Monte Carlo (MCMC)\ncan degrade quickly when targeting computationally expensive posterior\ndistributions, such as when the sample size is large. This has motivated the\nsearch for MCMC variants that scale well to large datasets. One general\napproach has been to look at only a subsample of the data at every step. In\nthis note, we point out that well-known MCMC convergence results often imply\nthat these \"subsampling\" MCMC algorithms cannot greatly improve performance. We\napply these generic results to realistic statistical problems and proposed\nalgorithms, and also discuss some design principles suggested by the results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:32:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Johndrow", "James E.", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "2010.12580", "submitter": "Hamid Eftekhari", "authors": "Hamid Eftekhari, Moulinath Banerjee, Ya'acov Ritov", "title": "Design of $c$-Optimal Experiments for High dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study random designs that minimize the asymptotic variance of a de-biased\nlasso estimator when a large pool of unlabeled data is available but measuring\nthe corresponding responses is costly. The optimal sampling distribution arises\nas the solution of a semidefinite program. The improvements in efficiency that\nresult from these optimal designs are demonstrated via simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:31:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Eftekhari", "Hamid", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2010.12664", "submitter": "Gholamali Aminian", "authors": "Gholamali Aminian, Laura Toni, Miguel R. D. Rodrigues", "title": "Jensen-Shannon Information Based Characterization of the Generalization\n  Error of Learning Algorithms", "comments": "Accepted in ITW 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization error bounds are critical to understanding the performance of\nmachine learning models. In this work, we propose a new information-theoretic\nbased generalization error upper bound applicable to supervised learning\nscenarios. We show that our general bound can specialize in various previous\nbounds. We also show that our general bound can be specialized under some\nconditions to a new bound involving the Jensen-Shannon information between a\nrandom variable modelling the set of training samples and another random\nvariable modelling the hypothesis. We also prove that our bound can be tighter\nthan mutual information-based bounds under some conditions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:53:07 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 15:27:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Aminian", "Gholamali", ""], ["Toni", "Laura", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "2010.12887", "submitter": "Jincheng Bai", "authors": "Jincheng Bai, Qifan Song, Guang Cheng", "title": "Nearly Optimal Variational Inference for High Dimensional Regression\n  with Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variational Bayesian (VB) procedure for high-dimensional linear\nmodel inferences with heavy tail shrinkage priors, such as student-t prior.\nTheoretically, we establish the consistency of the proposed VB method and prove\nthat under the proper choice of prior specifications, the contraction rate of\nthe VB posterior is nearly optimal. It justifies the validity of VB inference\nas an alternative of Markov Chain Monte Carlo (MCMC) sampling. Meanwhile,\ncomparing to conventional MCMC methods, the VB procedure achieves much higher\ncomputational efficiency, which greatly alleviates the computing burden for\nmodern machine learning applications such as massive data analysis. Through\nnumerical studies, we demonstrate that the proposed VB method leads to shorter\ncomputing time, higher estimation accuracy, and lower variable selection error\nthan competitive sparse Bayesian methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 12:10:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bai", "Jincheng", ""], ["Song", "Qifan", ""], ["Cheng", "Guang", ""]]}, {"id": "2010.12895", "submitter": "Tao Li", "authors": "Jiyanglin Li and Tao Li", "title": "Some Theoretical Results Concerning Time-varying Nonparametric\n  Regression with Local Stationary Regressors and Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With regard to a three-step estimation procedure, proposed without\ntheoretical discussion by Li and You in Journal of Applied Statistics and\nManagement, for a nonparametric regression model with time-varying regression\nfunction, local stationary regressors and time-varying AR(p) (tvAR(p)) error\nprocess , we established all necessary asymptotic properties for each of\nestimator. We derive the convergence rate and asymptotic normality of the\npreliminary estimation of nonparametric regression function, establish the\nasymptotic distribution of time-varying coefficient functions in the error\nterm, and present the asymptotic property of the refined estimation of\nnonparametric regression function. In addition, with regard to the ULASSO\nmethod for variable selection and constant coefficient detection for error term\nstructure, we show that the ULASSO estimator can identify the true error term\nstructure consistently. We conduct two simulation studies to illustrate the\nfinite sample performance of the estimators and validate our theoretical\ndiscussion on the properties of the estimators.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 13:17:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Jiyanglin", ""], ["Li", "Tao", ""]]}, {"id": "2010.13013", "submitter": "Sanath Kumar Krishnamurthy", "authors": "Sanath Kumar Krishnamurthy, Vitor Hadad, and Susan Athey", "title": "Tractable contextual bandits beyond realizability", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tractable contextual bandit algorithms often rely on the realizability\nassumption - i.e., that the true expected reward model belongs to a known\nclass, such as linear functions. In this work, we present a tractable bandit\nalgorithm that is not sensitive to the realizability assumption and\ncomputationally reduces to solving a constrained regression problem in every\nepoch. When realizability does not hold, our algorithm ensures the same\nguarantees on regret achieved by realizability-based algorithms under\nrealizability, up to an additive term that accounts for the misspecification\nerror. This extra term is proportional to T times a function of the mean\nsquared error between the best model in the class and the true model, where T\nis the total number of time-steps. Our work sheds light on the bias-variance\ntrade-off for tractable contextual bandits. This trade-off is not captured by\nalgorithms that assume realizability, since under this assumption there exists\nan estimator in the class that attains zero bias.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 01:36:04 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 00:08:36 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Krishnamurthy", "Sanath Kumar", ""], ["Hadad", "Vitor", ""], ["Athey", "Susan", ""]]}, {"id": "2010.13018", "submitter": "Takeyuki Sasai", "authors": "Takeyuki Sasai and Hironori Fujisawa", "title": "Adversarial Robust Low Rank Matrix Estimation: Compressed Sensing and\n  Matrix Completion", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust low rank matrix estimation when outputs are contaminated\nby adversary. Our method covers matrix compressed sensing (including lasso as a\npartial problem) and matrix completion. We attain fast convergence rates by\nusing convex estimators.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 02:32:07 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 08:45:37 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sasai", "Takeyuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "2010.13039", "submitter": "Debdeep Pati", "authors": "Indrajit Ghosh, Anirban Bhattacharya and Debdeep Pati", "title": "Statistical optimality and stability of tangent transform algorithms in\n  logit models", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic approach to finding variational approximation in an otherwise\nintractable non-conjugate model is to exploit the general principle of convex\nduality by minorizing the marginal likelihood that renders the problem\ntractable. While such approaches are popular in the context of variational\ninference in non-conjugate Bayesian models, theoretical guarantees on\nstatistical optimality and algorithmic convergence are lacking. Focusing on\nlogistic regression models, we provide mild conditions on the data generating\nprocess to derive non-asymptotic upper bounds to the risk incurred by the\nvariational optima. We demonstrate that these assumptions can be completely\nrelaxed if one considers a slight variation of the algorithm by raising the\nlikelihood to a fractional power. Next, we utilize the theory of dynamical\nsystems to provide convergence guarantees for such algorithms in logistic and\nmultinomial logit regression. In particular, we establish local asymptotic\nstability of the algorithm without any assumptions on the data-generating\nprocess. We explore a special case involving a semi-orthogonal design under\nwhich a global convergence is obtained. The theory is further illustrated using\nseveral numerical studies.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 05:15:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ghosh", "Indrajit", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "2010.13139", "submitter": "Maria Longobardi Prof.Dr.", "authors": "S. Tahmasebi, M. Longobardi, M.R. Kazemi, M. Alizadeh", "title": "Cumulative Tsallis Entropy for Maximum Ranked Set Sampling with Unequal\n  Samples", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2020.124763", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we consider the information content of maximum ranked set\nsampling procedure with unequal samples (MRSSU) in terms of Tsallis entropy\nwhich is a nonadditive generalization of Shannon entropy. We obtain several\nresults of Tsallis entropy including bounds, monotonic properties, stochastic\norders, and sharp bounds under some assumptions. We also compare the\nuncertainty and information content of MRSSU with its counterpart in the simple\nrandom sampling (SRS) data. Finally, we develop some characterization results\nin terms of cumulative Tsallis entropy and residual Tsallis entropy of MRSSU\nand SRS data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 15:35:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tahmasebi", "S.", ""], ["Longobardi", "M.", ""], ["Kazemi", "M. R.", ""], ["Alizadeh", "M.", ""]]}, {"id": "2010.13405", "submitter": "Francois Bachoc", "authors": "Fran\\c{c}ois Bachoc (IMT), Tommaso Cesari (TSE), S\\'ebastien\n  Gerchinovitz (IMT)", "title": "The sample complexity of level set approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating the level set of an unknown function by\nsequentially querying its values. We introduce a family of algorithms called\nBisect and Approximate through which we reduce the level set approximation\nproblem to a local function approximation problem. We then show how this\napproach leads to rate-optimal sample complexity guarantees for H{\\\"o}lder\nfunctions, and we investigate how such rates improve when additional smoothness\nor other structural assumptions hold true.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:02:23 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 09:09:29 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Cesari", "Tommaso", "", "TSE"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT"]]}, {"id": "2010.13410", "submitter": "Tetsuya Kawai", "authors": "Tetsuya Kawai, Masayuki Uchida", "title": "Adaptive testing method for ergodic diffusion processes based on high\n  frequency data", "comments": "40 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric tests for multidimensional ergodic diffusions based on\nhigh frequency data. We propose two-step testing method for diffusion\nparameters and drift parameters. To construct test statistics of the tests, we\nutilize the adaptive estimator and provide three types of test statistics:\nlikelihood ratio type test, Wald type test and Rao's score type test. It is\nproved that these test statistics converge in distribution to the chi-squared\ndistribution under null hypothesis and have consistency of the tests against\nalternatives. Moreover, these test statistics converge in distribution to the\nnon-central chi-squared distribution under local alternatives. We also give\nsome simulation studies of the behavior of the three types of test statistics.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 08:12:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kawai", "Tetsuya", ""], ["Uchida", "Masayuki", ""]]}, {"id": "2010.13531", "submitter": "Wenhao Zhan", "authors": "Wenhao Zhan", "title": "Strong Privacy and Utility Guarantee: Over-the-Air Statistical\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the privacy problem of statistical estimation from distributed\ndata, where users communicate to a central processor over a Gaussian\nmultiple-access channel(MAC). To avoid the inevitable sacrifice of data utility\nfor privacy in digital transmission schemes, we devise an over-the-air\nestimation strategy which utilizes the additive nature of MAC channel. Using\nthe mutual information between the channel outputs and users' data as the\nmetric, we obtain the privacy bounds for our scheme and validate that it can\nguarantee strong privacy without incurring larger estimation error. Further, to\nincrease the robustness of our methods, we adjust our primary schemes by adding\nGaussian noises locally and derive the corresponding minimax mean squared error\nunder conditional mutual information constraints. Comparing the performance of\nour methods to the digital ones, we show that the minimax error decreases by\n$O(\\frac{1}{n})$ in general, which suggests the advantages of over-the-air\nestimation for preserving data privacy and utility.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:41:13 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 08:45:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhan", "Wenhao", ""]]}, {"id": "2010.13599", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow and Cyrus Samii and Ye Wang", "title": "Design-Based Inference for Spatial Experiments with Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider design-based causal inference in settings where randomized\ntreatments have effects that bleed out into space in complex ways that overlap\nand in violation of the standard \"no interference\" assumption for many causal\ninference methods. We define a spatial \"average marginalized response,\" which\ncharacterizes how, in expectation, units of observation that are a specified\ndistance from an intervention point are affected by treatments at that point,\naveraging over effects emanating from other intervention points. We establish\nconditions for non-parametric identification, asymptotic distributions of\nestimators, and recovery of structural effects. We propose methods for both\nsample-theoretic and permutation-based inference. We provide illustrations\nusing randomized field experiments on forest conservation and health.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:15:31 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:14:02 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Wang", "Ye", ""]]}, {"id": "2010.13604", "submitter": "Stefan Stein", "authors": "Stefan Stein, Chenlei Leng", "title": "A Sparse $\\beta$-Model with Covariates for Networks", "comments": "73 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of networks are increasingly encountered in modern science\nand humanity. This paper concerns a new generative model, suitable for sparse\nnetworks commonly observed in practice, to capture degree heterogeneity and\nhomophily, two stylized features of a typical network. The former is achieved\nby differentially assigning parameters to individual nodes, while the latter is\nmaterialized by incorporating covariates. Similar models in the literature for\nheterogeneity often include as many nodal parameters as the number of nodes,\nleading to over-parametrization and, as a result, strong requirements on the\ndensity of the network. For parameter estimation, we propose the use of the\npenalized likelihood method with an $\\ell_1$ penalty on the nodal parameters,\ngiving rise to a convex optimization formulation which immediately connects our\nestimation procedure to the LASSO literature. We highlight the differences of\nour approach to the LASSO method for logistic regression, emphasizing the\nfeasibility of our model to conduct inference for sparse networks, study the\nfinite-sample error bounds on the excess risk and the $\\ell_1$-error of the\nresulting estimator, and develop a central limit theorem for the parameter\nassociated with the covariates. Simulation and data analysis corroborate the\ndeveloped theory. As a by-product of our main theory, we study what we call the\nErd\\H{o}s-R\\'{e}nyi model with covariates and develop the associated\nstatistical inference for sparse networks, which can be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:19:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Stein", "Stefan", ""], ["Leng", "Chenlei", ""]]}, {"id": "2010.13679", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier, Olivier Collier, Laetitia Comminges, Alexandre\n  B. Tsybakov, Yuhao Wang", "title": "Estimation of the $l_2$-norm and testing in sparse linear regression\n  with unknown variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the related problems of estimating the $l_2$-norm and the squared\n$l_2$-norm in sparse linear regression with unknown variance, as well as the\nproblem of testing the hypothesis that the regression parameter is null under\nsparse alternatives with $l_2$ separation. We establish the minimax optimal\nrates of estimation (respectively, testing) in these three problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:58:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Collier", "Olivier", ""], ["Comminges", "Laetitia", ""], ["Tsybakov", "Alexandre B.", ""], ["Wang", "Yuhao", ""]]}, {"id": "2010.13687", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser, Yuming Zhang", "title": "A General Approach for Simulation-based Bias Correction in High\n  Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in statistical analysis lies in controlling the bias\nof estimators due to the ever-increasing data size and model complexity.\nApproximate numerical methods and data features like censoring and\nmisclassification often result in analytical and/or computational challenges\nwhen implementing standard estimators. As a consequence, consistent estimators\nmay be difficult to obtain, especially in complex and/or high dimensional\nsettings. In this paper, we study the properties of a general simulation-based\nestimation framework that allows to construct bias corrected consistent\nestimators. We show that the considered approach leads, under more general\nconditions, to stronger bias correction properties compared to alternative\nmethods. Besides its bias correction advantages, the considered method can be\nused as a simple strategy to construct consistent estimators in settings where\nalternative methods may be challenging to apply. Moreover, the considered\nframework can be easily implemented and is computationally efficient. These\ntheoretical results are highlighted with simulation studies of various commonly\nused models, including the negative binomial regression (with and without\ncensoring) and the logistic regression (with and without misclassification\nerrors). Additional numerical illustrations are provided in the supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:07:01 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 20:37:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""], ["Zhang", "Yuming", ""]]}, {"id": "2010.13904", "submitter": "Muxuan Liang", "authors": "Muxuan Liang and Menggang Yu", "title": "Relative Contrast Estimation and Inference for Treatment Recommendation", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there are resource constraints, it is important to rank or estimate\ntreatment benefits according to patient characteristics. This facilitates\nprioritization of assigning different treatments. Most existing literature on\nindividualized treatment rules targets absolute conditional treatment effect\ndifferences as the metric for benefits. However, there can be settings where\nrelative differences may better represent such benefits. In this paper, we\nconsider modeling such relative differences that form scale-invariant contrasts\nbetween conditional treatment effects. We show that all scale-invariant\ncontrasts are monotonic transformations of each other. Therefore we posit a\nsingle index model for a particular relative contrast. Identifiability of the\nmodel is enforced via an intuitive $l_2$ norm constraint on index parameters.\nWe then derive estimating equations and efficient scores via semiparametric\nefficiency theory. Based on the efficient score and its variant, we propose a\ntwo-step approach that consists of minimizing a doubly robust loss function and\na subsequent one-step efficiency augmentation procedure to achieve efficiency\nbound. Careful theoretical and numerical studies are provided to show the\nsuperiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:14:38 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 09:05:42 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 21:24:21 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liang", "Muxuan", ""], ["Yu", "Menggang", ""]]}, {"id": "2010.13953", "submitter": "Ziyu Xu", "authors": "Ziyu Xu, Aaditya Ramdas", "title": "Dynamic Algorithms for Online Multiple Testing", "comments": "32 pages, 15 figures. Will be published in Mathematical and\n  Scientific Machine Learning 2021 (PMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new algorithms for online multiple testing that provably control\nfalse discovery exceedance (FDX) while achieving orders of magnitude more power\nthan previous methods. This statistical advance is enabled by the development\nof new algorithmic ideas: earlier algorithms are more \"static\" while our new\nones allow for the dynamical adjustment of testing levels based on the amount\nof wealth the algorithm has accumulated. We demonstrate that our algorithms\nachieve higher power in a variety of synthetic experiments. We also prove that\nSupLORD can provide error control for both FDR and FDX, and controls FDR at\nstopping times. Stopping times are particularly important as they permit the\nexperimenter to end the experiment arbitrarily early while maintaining desired\ncontrol of the FDR. SupLORD is the first non-trivial algorithm, to our\nknowledge, that can control FDR at stopping times in the online setting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:41:54 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 22:25:47 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 15:43:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xu", "Ziyu", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2010.14010", "submitter": "Ruodu Wang", "authors": "Ruodu Wang", "title": "Testing with p*-values: Between p-values and e-values", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of p*-values (p*-variables), which generalizes\np-values (p-variables) in several senses. The new notion has four natural\ninterpretations: probabilistic, operational, Bayesian, and frequentist. The\nsimplest interpretation of a p*-value is the average of several p-values. We\nshow that there are four equivalent definitions of p*-values. The randomized\np*-test is proposed, which is a randomized version of the simple p-test.\nAdmissible calibrators of p*-values to and from p-values and e-values are\nobtained with nice mathematical forms, revealing the role of p*-values as a\nbridge between p-values and e-values. The notion of p*-values becomes useful in\nmany situations even if one is only interested in p-values and e-values. In\nparticular, tests based on p*-values can be applied to improve several classic\nmethods for p-values and e-values.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 02:46:12 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 18:50:56 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 07:34:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Ruodu", ""]]}, {"id": "2010.14056", "submitter": "Sean Plummer", "authors": "Sean Plummer, Shuang Zhou, Anirban Bhattacharya, David Dunson, Debdeep\n  Pati", "title": "Statistical Guarantees for Transformation Based Models with Applications\n  to Implicit Variational Inference", "comments": "First two authors contributed equally to this work. arXiv admin note:\n  text overlap with arXiv:1701.07572", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation-based methods have been an attractive approach in\nnon-parametric inference for problems such as unconditional and conditional\ndensity estimation due to their unique hierarchical structure that models the\ndata as flexible transformation of a set of common latent variables. More\nrecently, transformation-based models have been used in variational inference\n(VI) to construct flexible implicit families of variational distributions.\nHowever, their use in both non-parametric inference and variational inference\nlacks theoretical justification. We provide theoretical justification for the\nuse of non-linear latent variable models (NL-LVMs) in non-parametric inference\nby showing that the support of the transformation induced prior in the space of\ndensities is sufficiently large in the $L_1$ sense. We also show that, when a\nGaussian process (GP) prior is placed on the transformation function, the\nposterior concentrates at the optimal rate up to a logarithmic factor. Adopting\nthe flexibility demonstrated in the non-parametric setting, we use the NL-LVM\nto construct an implicit family of variational distributions, deemed GP-IVI. We\ndelineate sufficient conditions under which GP-IVI achieves optimal risk bounds\nand approximates the true posterior in the sense of the Kullback-Leibler\ndivergence. To the best of our knowledge, this is the first work on providing\ntheoretical guarantees for implicit variational inference.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:06:29 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 20:02:43 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Plummer", "Sean", ""], ["Zhou", "Shuang", ""], ["Bhattacharya", "Anirban", ""], ["Dunson", "David", ""], ["Pati", "Debdeep", ""]]}, {"id": "2010.14146", "submitter": "Timo Dimitriadis", "authors": "Timo Dimitriadis and Tobias Fissler and Johanna F. Ziegel", "title": "The Efficiency Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation via M- and Z-estimation is broadly considered to be\nequally powerful in semiparametric models for one-dimensional functionals. This\nis due to the fact that, under sufficient regularity conditions, there is a\none-to-one relation between the corresponding objective functions - strictly\nconsistent loss functions and oriented strict identification functions - via\nintegration and differentiation. When dealing with multivariate functionals\nsuch as multiple moments, quantiles, or the pair (Value at Risk, Expected\nShortfall), this one-to-one relation fails due to integrability conditions: Not\nevery identification function possesses an antiderivative. The most important\nimplication of this failure is an efficiency gap: The most efficient\nZ-estimator often outperforms the most efficient M-estimator, implying that the\nsemiparametric efficiency bound cannot be attained by the M-estimator in these\ncases. We show that this phenomenon arises for pairs of quantiles at different\nlevels and for the pair (Value at Risk, Expected Shortfall), where we\nillustrate the gap through extensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 09:12:16 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Dimitriadis", "Timo", ""], ["Fissler", "Tobias", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "2010.14170", "submitter": "Lidan He", "authors": "Xinwei Feng, Lidan He and Zhi Liu", "title": "Large Deviation principles of Realized Laplace Transform of Volatility", "comments": "20pages, 2figures", "journal-ref": null, "doi": null, "report-no": "JOTP-D-19-00236R1", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under scenario of high frequency data, consistent estimator of realized\nLaplace transform of volatility is proposed by \\citet{TT2012a} and related\ncentral limit theorem has been well established. In this paper, we investigate\nthe asymptotic tail behaviour of the empirical realized Laplace transform of\nvolatility (ERLTV). We establish both large deviation principle and moderate\ndeviation principle for the ERLTV. The good rate function for the large\ndeviation principle is well defined in the whole real space, which indicates a\nlimit for the normalized logarithmic tail probability of the ERLTV. Moreover,\nwe also derive the function-level large and moderate deviation principles for\nERLTV.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:07:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Feng", "Xinwei", ""], ["He", "Lidan", ""], ["Liu", "Zhi", ""]]}, {"id": "2010.14224", "submitter": "Camilla Cal\\`i", "authors": "Jorge Navarro, Camilla Cal\\`i, Maria Longobardi, Fabrizio Durante", "title": "Distortion Representations of Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The univariate distorted distribution were introduced in risk theory to\nrepresent changes (distortions) in the expected distributions of some risks.\nLater they were also applied to represent distributions of order statistics,\ncoherent systems, proportional hazard rate (PHR) and proportional reversed\nhazard rate (PRHR) models, etc. In this paper we extend this concept to the\nmultivariate setup. We show that, in some cases, they are a valid alternative\nto the copula representations especially when the marginal distributions may\nnot be easily handled. Several relevant examples illustrate the applications of\nsuch representations in statistical modeling. They include the study of paired\n(dependent) ordered data, joint residual lifetimes, order statistics and\ncoherent systems.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 17:47:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Navarro", "Jorge", ""], ["Cal\u00ec", "Camilla", ""], ["Longobardi", "Maria", ""], ["Durante", "Fabrizio", ""]]}, {"id": "2010.14449", "submitter": "Dennis Shen", "authors": "Anish Agarwal, Devavrat Shah, Dennis Shen", "title": "On Principal Component Regression in a High-Dimensional\n  Error-in-Variables Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the classical method of principal component regression (PCR) in a\nhigh-dimensional error-in-variables setting. Here, the observed covariates are\nnot only noisy and contain missing values, but the number of covariates can\nalso exceed the sample size. Under suitable conditions, we establish that PCR\nidentifies the unique linear model parameter with minimum $\\ell_2$-norm, and\nderive non-asymptotic $\\ell_2$-rates of convergence that show its consistency.\nFurthermore, we develop an algorithm for out-of-sample predictions in the\npresence of corrupted data that uses PCR as a key subroutine, and provide its\nnon-asymptotic prediction performance guarantees. Notably, our results do not\nrequire the out-of-samples covariates to follow the same distribution as that\nof the in-sample covariates, but rather that they obey a simple linear\nalgebraic constraint. We provide simulations that illustrate our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:07:36 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 17:50:20 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 23:12:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Agarwal", "Anish", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""]]}, {"id": "2010.14460", "submitter": "Sebastian Roch", "authors": "Wai-Tong Louis Fan and Brandon Legried and Sebastien Roch", "title": "Impossibility of phylogeny reconstruction from $k$-mer counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider phylogeny estimation under a two-state model of sequence\nevolution by site substitution on a tree. In the asymptotic regime where the\nsequence lengths tend to infinity, we show that for any fixed $k$ no\nstatistically consistent phylogeny estimation is possible from $k$-mer counts\nof the leaf sequences alone. Formally, we establish that the joint leaf\ndistributions of $k$-mer counts on two distinct trees have total variation\ndistance bounded away from $1$ as the sequence length tends to infinity. That\nis, the two distributions cannot be distinguished with probability going to one\nin that asymptotic regime. Our results are information-theoretic: they imply an\nimpossibility result for any reconstruction method using only $k$-mer counts at\nthe leaves.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:22:35 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Fan", "Wai-Tong Louis", ""], ["Legried", "Brandon", ""], ["Roch", "Sebastien", ""]]}, {"id": "2010.14555", "submitter": "Peng Ding", "authors": "Anqi Zhao, Peng Ding", "title": "Covariate-adjusted Fisher randomization tests for the average treatment\n  effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's randomization test (FRT) delivers exact $p$-values under the strong\nnull hypothesis of no treatment effect on any units whatsoever and allows for\nflexible covariate adjustment to improve the power. Of interest is whether the\nprocedure could also be valid for testing the weak null hypothesis of zero\naverage treatment effect. Towards this end, we evaluate two general strategies\nfor FRT with covariate-adjusted test statistics: that based on the residuals\nfrom an outcome model with only the covariates, and that based on the output\nfrom an outcome model with both the treatment and the covariates. Based on\ntheory and simulation, we recommend using the ordinary least squares (OLS) fit\nof the observed outcome on the treatment, centered covariates, and their\ninteractions for covariate adjustment, and conducting FRT with the robust\n$t$-value of the treatment as the test statistic. The resulting FRT is\nfinite-sample exact for the strong null hypothesis, asymptotically valid for\nthe weak null hypothesis, and more powerful than the unadjusted analog under\nalternatives, all irrespective of whether the linear model is correctly\nspecified or not. We develop the theory for complete randomization, cluster\nrandomization, stratified randomization, and rerandomization, respectively, and\ngive a recommendation for the test procedure and test statistic under each\ndesign. We first focus on the finite-population perspective and then extend the\nresult to the super-population perspective, highlighting the difference in\nstandard errors. Motivated by the similarity in procedure, we also evaluate the\ndesign-based properties of five existing permutation tests originally for\nlinear models and show the superiority of the proposed FRT for testing the\ntreatment effects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:51:43 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 20:41:13 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 22:39:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2010.14638", "submitter": "Yabo Niu", "authors": "Yabo Niu, Nilabja Guha, Debkumar De, Anindya Bhadra, Veerabhadran\n  Baladandayuthapani, Bani K. Mallick", "title": "Bayesian Variable Selection in Multivariate Nonlinear Regression with\n  Graph Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGMs) are well-established tools for probabilistic\nexploration of dependence structures using precision matrices. We develop a\nBayesian method to incorporate covariate information in this GGMs setup in a\nnonlinear seemingly unrelated regression framework. We propose a joint\npredictor and graph selection model and develop an efficient collapsed Gibbs\nsampler algorithm to search the joint model space. Furthermore, we investigate\nits theoretical variable selection properties. We demonstrate our method on a\nvariety of simulated data, concluding with a real data set from the TCPA\nproject.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 21:57:07 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Niu", "Yabo", ""], ["Guha", "Nilabja", ""], ["De", "Debkumar", ""], ["Bhadra", "Anindya", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2010.14694", "submitter": "Max Farrell", "authors": "Max H. Farrell and Tengyuan Liang and Sanjog Misra", "title": "Deep Learning for Individual Heterogeneity: An Automatic Inference\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methodology for estimation and inference using machine learning to\nenrich economic models. Our framework takes a standard economic model and\nrecasts the parameters as fully flexible nonparametric functions, to capture\nthe rich heterogeneity based on potentially high dimensional or complex\nobservable characteristics. These \"parameter functions\" retain the\ninterpretability, economic meaning, and discipline of classical parameters.\nDeep learning is particularly well-suited to structured modeling of\nheterogeneity in economics. We show how to design the network architecture to\nmatch the structure of the economic model, delivering novel methodology that\nmoves deep learning beyond prediction. We prove convergence rates for the\nestimated parameter functions. These functions are the key inputs into the\nfinite-dimensional parameter of inferential interest. We obtain inference based\non a novel influence function calculation that covers any second-stage\nparameter and any machine-learning-enriched model that uses a smooth\nper-observation loss function. No additional derivations are required. The\nscore can be taken directly to data, using automatic differentiation if needed.\nThe researcher need only define the original model and define the parameter of\ninterest. A key insight is that we need not write down the influence function\nin order to evaluate it on the data. Our framework gives new results for a host\nof contexts, covering such diverse examples as price elasticities,\nwillingness-to-pay, and surplus measures in binary or multinomial choice\nmodels, effects of continuous treatment variables, fractional outcome models,\ncount data, heterogeneous production functions, and more. We apply our\nmethodology to a large scale advertising experiment for short-term loans. We\nshow how economically meaningful estimates and inferences can be made that\nwould be unavailable without our results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 01:41:47 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 19:34:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Farrell", "Max H.", ""], ["Liang", "Tengyuan", ""], ["Misra", "Sanjog", ""]]}, {"id": "2010.14715", "submitter": "Stilian Stoev", "authors": "Jinqi Shen, Stilian Stoev, Tailen Hsing", "title": "Tangent fields, intrinsic stationarity, and self-similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the local structure of continuous random fields on\n$\\mathbb R^d$ taking values in a complete separable linear metric space\n${\\mathbb V}$. Extending the seminal work of Falconer (2002), we show that the\ngeneralized $k$-th order increment tangent fields are self-similar and almost\neverywhere intrinsically stationary in the sense of Matheron (1973). These\nresults motivate the further study of the structure of ${\\mathbb V}$-valued\nintrinsic random functions of order $k$ (IRF$_k$, $k=0,1,\\cdots$). To this end,\nwe focus on the special case where ${\\mathbb V}$ is a Hilbert space. Building\non the work of Sasvari (2009) and Berschneider (2012), we establish the\nspectral characterization of all second order ${\\mathbb V}$-valued IRF$_k$'s,\nextending the classic Matheron theory. Using these results, we further\ncharacterize the class of Gaussian, operator self-similar ${\\mathbb V}$-valued\nIRF$_k$'s, extending some results of Dobrushin (1979) and Didier, Meerschaert,\nand Pipiras (2018), among others. These processes are ${\\mathbb V}$-valued\ncounterparts to $k$-order fractional Brownian fields and are characterized by\ntheir self-similarity operator exponent as well as a finite trace class\noperator valued spectral measure. We conclude with several examples motivating\nfuture applications to probability and statistics.\n  In a technical Supplement of independent interest, we provide a unified\ntreatment of the spectral theory for second-order stationary and intrinsically\nstationary processes taking values in a separable Hilbert space. We give the\nproofs of the Bochner-Neeb and Bochner-Schwartz theorems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:49:33 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 19:12:11 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 02:34:25 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shen", "Jinqi", ""], ["Stoev", "Stilian", ""], ["Hsing", "Tailen", ""]]}, {"id": "2010.14761", "submitter": "Enrico Maria Malatesta", "authors": "Carlo Baldassi, Enrico M. Malatesta, Matteo Negri, Riccardo Zecchina", "title": "Wide flat minima and optimal generalization in classifying\n  high-dimensional Gaussian mixtures", "comments": "19 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2006.07897", "journal-ref": null, "doi": "10.1088/1742-5468/abcd31", "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the connection between minimizers with good generalizing\nproperties and high local entropy regions of a threshold-linear classifier in\nGaussian mixtures with the mean squared error loss function. We show that there\nexist configurations that achieve the Bayes-optimal generalization error, even\nin the case of unbalanced clusters. We explore analytically the error-counting\nloss landscape in the vicinity of a Bayes-optimal solution, and show that the\ncloser we get to such configurations, the higher the local entropy, implying\nthat the Bayes-optimal solution lays inside a wide flat region. We also\nconsider the algorithmically relevant case of targeting wide flat minima of the\n(differentiable) mean squared error loss. Our analytical and numerical results\nshow not only that in the balanced case the dependence on the norm of the\nweights is mild, but also, in the unbalanced case, that the performances can be\nimproved.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:32:03 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 16:06:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Baldassi", "Carlo", ""], ["Malatesta", "Enrico M.", ""], ["Negri", "Matteo", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "2010.15009", "submitter": "Debashis Ghosh", "authors": "Youngjoo Cho, Debashis Ghosh", "title": "Bridging linearity-based and kernel-based sufficient dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a lot of interest in sufficient dimension reduction (SDR)\nmethodologies as well as nonlinear extensions in the statistics literature. In\nthis note, we use classical results regarding metric spaces and positive\ndefinite functions to link linear SDR procedures to their nonlinear\ncounterparts.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:36:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cho", "Youngjoo", ""], ["Ghosh", "Debashis", ""]]}, {"id": "2010.15063", "submitter": "Shuting Shen", "authors": "Shuting Shen, Junwei Lu", "title": "Combinatorial-Probabilistic Trade-Off: Community Properties Test in the\n  Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an inferential framework testing the general\ncommunity combinatorial properties of the stochastic block model. Instead of\nestimating the community assignments, we aim to test the hypothesis on whether\na certain community property is satisfied. For instance, we propose to test\nwhether a given set of nodes belong to the same community or whether different\nnetwork communities have the same size. We propose a general inference\nframework that can be applied to all symmetric community properties. To ease\nthe challenges caused by the combinatorial nature of communities properties, we\ndevelop a novel shadowing bootstrap testing method. By utilizing the symmetry,\nour method can find a shadowing representative of the true assignment and the\nnumber of assignments to be tested in the alternative can be largely reduced.\nIn theory, we introduce a combinatorial distance between two community classes\nand show a combinatorial-probabilistic trade-off phenomenon in the community\nproperties test. Our test is honest as long as the product of combinatorial\ndistance between two communities and the probabilistic distance between two\nassignment probabilities is sufficiently large. On the other hand, we shows\nthat such trade-off also exists in the information-theoretic lower bound of the\ncommunity property test. We also implement numerical experiments on both the\nsynthetic data and the protein interaction application to show the validity of\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:16:14 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:38:39 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Shen", "Shuting", ""], ["Lu", "Junwei", ""]]}, {"id": "2010.15351", "submitter": "Yves Isma\\\"el Ngounou Bakam", "authors": "Yves I. Ngounou Bakam and Denys Pommeret", "title": "Nonparametric estimation of copulas and copula densities by orthogonal\n  projections", "comments": "42 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study nonparametric estimators of copulas and copula\ndensities. We first focus our study on a density copula estimator based on a\npolynomial orthogonal projection of the joint density. A new copula estimator\nis then deduced. Its asymptotic properties are studied: we provide a large\nfunctional class for which this construction is optimal in the minimax and\nmaxiset sense and we propose a method selection for the smoothing parameter. An\nintensive simulation study shows the very good performance of both copulas and\ncopula densities estimators which we compare to a large panel of competitors. A\nreal dataset in actuarial science illustrates this approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 04:24:31 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Bakam", "Yves I. Ngounou", ""], ["Pommeret", "Denys", ""]]}, {"id": "2010.15515", "submitter": "Manuele Leonelli", "authors": "Christiane G\\\"orgen, Manuele Leonelli, Orlando Marigliano", "title": "The curved exponential family of a staged tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Staged tree models are a discrete generalization of Bayesian networks. We\nshow that these form curved exponential families and derive their natural\nparameters, sufficient statistic, and cumulant-generating function as functions\nof their graphical representation. We give necessary graphical criteria for\nclassifying regular subfamilies and discuss implications for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:10:14 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 21:43:00 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["G\u00f6rgen", "Christiane", ""], ["Leonelli", "Manuele", ""], ["Marigliano", "Orlando", ""]]}, {"id": "2010.15530", "submitter": "A. Daniel Carnerero", "authors": "A. Daniel Carnerero, Daniel R. Ramirez and Teodoro Alamo", "title": "Probabilistic interval predictor based on dissimilarity functions", "comments": "8 pages, 4 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new method to obtain probabilistic interval predictions\nof a dynamical system. The method uses stored past system measurements to\nestimate the future evolution of the system. The proposed method relies on the\nuse of dissimilarity functions to estimate the conditional probability density\nfunction of the outputs. A family of empirical probability density functions,\nparameterized by means of two parameters, is introduced. It is shown that the\nthe proposed family encompasses the multivariable normal probability density\nfunction as a particular case. We show that the proposed method constitutes a\ngeneralization of classical estimation methods. A cross-validation scheme is\nused to tune the two parameters on which the methodology relies. In order to\nprove the effectiveness of the methodology presented, some numerical examples\nand comparisons are provided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:54:33 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Carnerero", "A. Daniel", ""], ["Ramirez", "Daniel R.", ""], ["Alamo", "Teodoro", ""]]}, {"id": "2010.15539", "submitter": "Bal\\'azs Gerencs\\'er", "authors": "Bal\\'azs Gerencs\\'er, Andrea Ottolini", "title": "Rates of convergence for Gibbs sampling in the analysis of almost\n  exchangeable data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by de Finetti's representation theorem for almost exchangeable\narrays, we want to sample $\\mathbf p \\in [0,1]^d$ from a distribution with\ndensity proportional to $\\exp(-A^2\\sum_{i<j}c_{ij}(p_i-p_j)^2)$, where $A$ is\nlarge and $c_{ij}$'s are non-negative weights. We analyze the rate of\nconvergence of a coordinate Gibbs sampler used to simulate from these measures.\nWe show that for every non-zero fixed matrix $C=(c_{ij})$, and large enough\n$A$, mixing happens in $\\Theta(A^2)$ steps in a suitable Wasserstein distance.\nThe upper and lower bounds are explicit and depend on the matrix $C$ through\nfew relevant spectral parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:15:51 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 18:47:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gerencs\u00e9r", "Bal\u00e1zs", ""], ["Ottolini", "Andrea", ""]]}, {"id": "2010.15658", "submitter": "Ekkehard Schnoor", "authors": "Arash Behboodi, Holger Rauhut, Ekkehard Schnoor", "title": "Compressive Sensing and Neural Networks from a Statistical Learning\n  Perspective", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various iterative reconstruction algorithms for inverse problems can be\nunfolded as neural networks. Empirically, this approach has often led to\nimproved results, but theoretical guarantees are still scarce. While some\nprogress on generalization properties of neural networks have been made, great\nchallenges remain. In this chapter, we discuss and combine these topics to\npresent a generalization error analysis for a class of neural networks suitable\nfor sparse reconstruction from few linear measurements. The hypothesis class\nconsidered is inspired by the classical iterative soft-thresholding algorithm\n(ISTA). The neural networks in this class are obtained by unfolding iterations\nof ISTA and learning some of the weights. Based on training samples, we aim at\nlearning the optimal network parameters via empirical risk minimization and\nthereby the optimal network that reconstructs signals from their compressive\nlinear measurements. In particular, we may learn a sparsity basis that is\nshared by all of the iterations/layers and thereby obtain a new approach for\ndictionary learning. For this class of networks, we present a generalization\nbound, which is based on bounding the Rademacher complexity of hypothesis\nclasses consisting of such deep networks via Dudley's integral. Remarkably,\nunder realistic conditions, the generalization error scales only\nlogarithmically in the number of layers, and at most linear in number of\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:05:43 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 11:26:50 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 12:38:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Behboodi", "Arash", ""], ["Rauhut", "Holger", ""], ["Schnoor", "Ekkehard", ""]]}, {"id": "2010.15659", "submitter": "Tobias Freidling", "authors": "Tobias Freidling, Benjamin Poignard, H\\'ector Climente-Gonz\\'alez,\n  Makoto Yamada", "title": "Post-selection inference with HSIC-Lasso", "comments": "Changes to previous version: * Incorporating comments and remarks\n  from reviewers * Evaluation of power of the proposed method * Summarising\n  behaviour for different hyper-parameters in one paragraph, instead of several\n  figures * Pseudocode of the algorithm * Additional, in-depth experiment on\n  real-world data", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting influential features in non-linear and/or high-dimensional data is\na challenging and increasingly important task in machine learning. Variable\nselection methods have thus been gaining much attention as well as\npost-selection inference. Indeed, the selected features can be significantly\nflawed when the selection procedure is not accounted for. We propose a\nselective inference procedure using the so-called model-free \"HSIC-Lasso\" based\non the framework of truncated Gaussians combined with the polyhedral lemma. We\nthen develop an algorithm, which allows for low computational costs and\nprovides a selection of the regularisation parameter. The performance of our\nmethod is illustrated by both artificial and real-world data based experiments,\nwhich emphasise a tight control of the type-I error, even for small sample\nsizes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:10:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 11:19:29 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Freidling", "Tobias", ""], ["Poignard", "Benjamin", ""], ["Climente-Gonz\u00e1lez", "H\u00e9ctor", ""], ["Yamada", "Makoto", ""]]}, {"id": "2010.15690", "submitter": "Ludovic Arnould", "authors": "Ludovic Arnould (LPSM (UMR\\_8001)), Claire Boyer (LPSM (UMR\\_8001)),\n  Erwan Scornet (CMAP)", "title": "Analyzing the tree-layer structure of Deep Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests on the one hand, and neural networks on the other hand, have\nmet great success in the machine learning community for their predictive\nperformance. Combinations of both have been proposed in the literature, notably\nleading to the so-called deep forests (DF) (Zhou \\& Feng,2019). In this paper,\nour aim is not to benchmark DF performances but to investigate instead their\nunderlying mechanisms. Additionally, DF architecture can be generally\nsimplified into more simple and computationally efficient shallow forest\nnetworks. Despite some instability, the latter may outperform standard\npredictive tree-based methods. We exhibit a theoretical framework in which a\nshallow tree network is shown to enhance the performance of classical decision\ntrees. In such a setting, we provide tight theoretical lower and upper bounds\non its excess risk. These theoretical results show the interest of tree-network\narchitectures for well-structured data provided that the first layer, acting as\na data encoder, is rich enough.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:32:03 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 13:47:48 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Arnould", "Ludovic", "", "LPSM"], ["Boyer", "Claire", "", "LPSM"], ["Scornet", "Erwan", "", "CMAP"]]}, {"id": "2010.15764", "submitter": "Yuansi Chen", "authors": "Yuansi Chen, Peter B\\\"uhlmann", "title": "Domain adaptation under structural causal models", "comments": "75 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain adaptation (DA) arises as an important problem in statistical machine\nlearning when the source data used to train a model is different from the\ntarget data used to test the model. Recent advances in DA have mainly been\napplication-driven and have largely relied on the idea of a common subspace for\nsource and target data. To understand the empirical successes and failures of\nDA methods, we propose a theoretical framework via structural causal models\nthat enables analysis and comparison of the prediction performance of DA\nmethods. This framework also allows us to itemize the assumptions needed for\nthe DA methods to have a low target error. Additionally, with insights from our\ntheory, we propose a new DA method called CIRM that outperforms existing DA\nmethods when both the covariates and label distributions are perturbed in the\ntarget data. We complement the theoretical analysis with extensive simulations\nto show the necessity of the devised assumptions. Reproducible synthetic and\nreal data experiments are also provided to illustrate the strengths and\nweaknesses of DA methods when parts of the assumptions of our theory are\nviolated.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:09:34 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chen", "Yuansi", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2010.15817", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis and Panagiotis Lolas", "title": "$\\sigma$-Ridge: group regularized ridge regression via empirical Bayes\n  noise level cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features in predictive models are not exchangeable, yet common supervised\nmodels treat them as such. Here we study ridge regression when the analyst can\npartition the features into $K$ groups based on external side-information. For\nexample, in high-throughput biology, features may represent gene expression,\nprotein abundance or clinical data and so each feature group represents a\ndistinct modality. The analyst's goal is to choose optimal regularization\nparameters $\\lambda = (\\lambda_1, \\dotsc, \\lambda_K)$ -- one for each group. In\nthis work, we study the impact of $\\lambda$ on the predictive risk of\ngroup-regularized ridge regression by deriving limiting risk formulae under a\nhigh-dimensional random effects model with $p\\asymp n$ as $n \\to \\infty$.\nFurthermore, we propose a data-driven method for choosing $\\lambda$ that\nattains the optimal asymptotic risk: The key idea is to interpret the residual\nnoise variance $\\sigma^2$, as a regularization parameter to be chosen through\ncross-validation. An empirical Bayes construction maps the one-dimensional\nparameter $\\sigma$ to the $K$-dimensional vector of regularization parameters,\ni.e., $\\sigma \\mapsto \\widehat{\\lambda}(\\sigma)$. Beyond its theoretical\noptimality, the proposed method is practical and runs as fast as\ncross-validated ridge regression without feature groups ($K=1$).\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:52:45 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:05:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Lolas", "Panagiotis", ""]]}, {"id": "2010.15861", "submitter": "Benjamin Robinson", "authors": "Joseph Wells, Mary Cook, Karleigh Pine, Benjamin D. Robinson", "title": "Fisher-Rao distance on the covariance cone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher-Rao geodesic distance on the statistical manifold consisting of\nzero-mean p-dimensional multivariate Gaussians appears without proof in several\nplaces (such as Steven Smith's \"Covariance, Subspace, and Intrinsic Cramer-Rao\nBounds\"). In this paper, we give a proof using basic Riemannian geometry.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:06:28 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wells", "Joseph", ""], ["Cook", "Mary", ""], ["Pine", "Karleigh", ""], ["Robinson", "Benjamin D.", ""]]}, {"id": "2010.15917", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "An improvement of Tusn\\'ady's inequality in the bulk", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a non-asymptotic generalization of the refined continuity correction\nfor the Binomial distribution found in Cressie (1978), which we then use to\nimprove the versions of Tusn\\'ady's inequality from Massart (2002) and Carter &\nPollard (2004) in the bulk.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:04:34 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 05:51:40 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2010.15950", "submitter": "Jochem Oorschot", "authors": "Jochem Oorschot and Chen Zhou", "title": "All Block Maxima method for estimating the extreme value index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block maxima (BM) approach in extreme value analysis fits a sample of\nblock maxima to the Generalized Extreme Value (GEV) distribution. We consider\nall potential blocks from a sample, which leads to the All Block Maxima (ABM)\nestimator. Different from existing estimators based on the BM approach, the ABM\nestimator is permutation invariant. We show the asymptotic behavior of the ABM\nestimator, which has the lowest asymptotic variance among all estimators using\nthe BM approach. Simulation studies justify our asymptotic theories. A key step\nin establishing the asymptotic theory for the ABM estimator is to obtain\nasymptotic expansions for the tail empirical process based on higher order\nstatistics with weights.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 21:18:17 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Oorschot", "Jochem", ""], ["Zhou", "Chen", ""]]}, {"id": "2010.16186", "submitter": "Ioannis Kosmidis", "authors": "Ruggero Bellio, Ioannis Kosmidis, Alessandra Salvan, Nicola Sartori", "title": "Parametric bootstrap inference for stratified models with\n  high-dimensional nuisance specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference about a scalar parameter of interest typically relies on the\nasymptotic normality of common likelihood pivots, such as the signed likelihood\nroot, the score and Wald statistics. Nevertheless, the resulting inferential\nprocedures have been known to perform poorly when the dimension of the nuisance\nparameter is large relative to the sample size and when the information about\nthe parameters is limited. In such cases, the use of asymptotic normality of\nanalytical modifications of the signed likelihood root is known to recover\ninferential performance. It is proved here that parametric bootstrap of\nstandard likelihood pivots results in as accurate inferences as analytical\nmodifications of the signed likelihood root do in stratified models with\nstratum specific nuisance parameters. We focus on the challenging case where\nthe number of strata increases as fast or faster than the stratum samples size.\nIt is also shown that this equivalence holds regardless of whether constrained\nor unconstrained bootstrap is used. This is in contrast to when the dimension\nof the parameter space is fixed relative to the sample size, where constrained\nbootstrap is known to correct inference to higher-order than unconstrained\nbootstrap does. Large scale simulation experiments support the theoretical\nfindings and demonstrate the excellent performance of bootstrap in some extreme\nmodelling scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:49:29 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Bellio", "Ruggero", ""], ["Kosmidis", "Ioannis", ""], ["Salvan", "Alessandra", ""], ["Sartori", "Nicola", ""]]}, {"id": "2010.16212", "submitter": "Kwangjun Ahn", "authors": "Kwangjun Ahn, Sinho Chewi", "title": "Efficient constrained sampling via the mirror-Langevin algorithm", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new discretization of the mirror-Langevin diffusion and give a\ncrisp proof of its convergence. Our analysis uses relative convexity/smoothness\nand self-concordance, ideas which originated in convex optimization, together\nwith a new result in optimal transport that generalizes the displacement\nconvexity of the entropy. Unlike prior works, our result both (1) requires much\nweaker assumptions on the mirror map and the target distribution, and (2) has\nvanishing bias as the step size tends to zero. In particular, for the task of\nsampling from a log-concave distribution supported on a compact set, our\ntheoretical results are significantly better than the existing guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 11:54:24 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ahn", "Kwangjun", ""], ["Chewi", "Sinho", ""]]}, {"id": "2010.16338", "submitter": "Peter Cameron", "authors": "R. A. Bailey, Peter J. Cameron, Michael Kinyon, Cheryl E. Praeger", "title": "Diagonal groups and arcs over groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.GR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an earlier paper by three of the present authors and Csaba Schneider, it\nwas shown that, for $m\\ge2$, a set of $m+1$ partitions of a set $\\Omega$, any\n$m$ of which are the minimal non-trivial elements of a Cartesian lattice,\neither form a Latin square (if $m=2$), or generate a join-semilattice of\ndimension $m$ associated with a diagonal group over a base group $G$.\n  In this paper we investigate what happens if we have $m+r$ partitions with\n$r\\geq 2$, any $m$ of which are minimal elements of a Cartesian lattice. If\n$m=2$, this is just a set of mutually orthogonal Latin squares. We consider the\ncase where all these squares are isotopic to Cayley tables of groups, and give\nan example to show the groups need not be all isomorphic. For $m>2$, things are\nmore restricted. Any $m+1$ of the partitions generate a join-semilattice\nadmitting a diagonal group over a group $G$. It may be that the groups are all\nisomorphic, though we cannot prove this. Under an extra hypothesis, we show\nthat $G$ must be abelian and must have three fixed-point-free automorphisms\nwhose product is the identity. Under this hypothesis, such a structure gives an\northogonal array, and conversely in some cases.\n  If the group is cyclic of prime order $p$, then the structure corresponds\nexactly to an arc of cardinality $m+r$ in the $(m-1)$-dimensional projective\nspace over the field with $p$ elements, so all known results about arcs are\napplicable. More generally, arcs over a finite field of order $q$ give examples\nwhere $G$ is the elementary abelian group of order $q$. These examples can be\nlifted to non-elementary abelian groups using $p$-adic techniques.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:56:11 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:47:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bailey", "R. A.", ""], ["Cameron", "Peter J.", ""], ["Kinyon", "Michael", ""], ["Praeger", "Cheryl E.", ""]]}, {"id": "2010.16360", "submitter": "Nuno Picado", "authors": "Nuno Picado, Paulo Eduardo Oliveira", "title": "Denoising and Interior Detection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{M}$ be a compact manifold of $\\mathbb{R}^d$. The goal of this\npaper is to decide, based on a sample of points, whether the interior of\n$\\mathcal{M}$ is empty or not. We divide this work in two main parts. Firstly,\nunder a dependent sample which may or may not contain some noise within, we\ncharacterize asymptotic properties of an interior detection test based on a\nsuitable control of the dependence. Afterwards, we drop the dependence and\nconsider a model where the points sampled from the manifold are mixed with some\npoints sampled from a different measure (noisy observations). We study the\nbehaviour with respect to the amount of noisy observations, introducing a\nmethodology to identify true manifold points, characterizing convergence\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:34:34 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Picado", "Nuno", ""], ["Oliveira", "Paulo Eduardo", ""]]}]